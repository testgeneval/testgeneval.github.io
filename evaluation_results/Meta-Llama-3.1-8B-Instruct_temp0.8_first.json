{"raw_generations": {"sample_200": ["    def test_message_creation(self):\n        message = EmailMessage(\n            subject='Test Email',\n            body='This is a test email',\n            from_email='test@example.com',\n            to=['recipient@example.com'],\n            attachments=[('file.txt', b'Hello, world!', 'text/plain')],\n        )\n        self.assertMessageHasHeaders(message.message(), {'Subject', 'From', 'To'})\n", "    def test_safe_mime_text(self):\n        # Test that SafeMIMEText forbids multi-line headers\n        msg = SafeMIMEText('Hello\\nWorld')\n        self.assertRaises(BadHeaderError, lambda: msg.__setitem__('Subject', msg))\n\n        # Test that SafeMIMEText allows single-line headers\n        msg = SafeMIMEText('Hello World')\n        msg['Subject'] = 'Test'\n        self.assertEqual(msg['Subject'], 'Test')\n\n        # Test that SafeMIMEText allows headers with non-ASCII characters\n        msg = SafeMIMEText('Hello \u00e0\u00e8\u00ef\u00f2\u00f9')\n        msg['Subject'] = 'Test'\n        self.assertEqual(msg['Subject'], 'Test')\n\n        # Test that SafeMIMEText forbids multi-line values\n        msg = SafeMIMEText('Hello')\n        self.assertRaises(BadHeaderError, lambda: msg.__setitem__('Subject', 'Hello\\nWorld'))\n\n        # Test that SafeMIMEText allows single-line values\n        msg = SafeMIMEText('Hello')\n        msg['Subject'] = 'Hello World'\n        self.assertEqual(msg['Subject'], 'Hello World')\n\n        # Test that SafeMIMEText allows non-ASCII values\n        msg = SafeMIMEText('Hello \u00e0\u00e8\u00ef\u00f2\u00f9')\n        msg['Subject'] = 'Hello \u00e0\u00e8\u00ef\u00f2\u00f9'\n        self.assertEqual(msg['Subject'], 'Hello \u00e0\u00e8\u00ef\u00f2\u00f9')\n", "    def test_multipart_message_with_attachment(self):\n        msg = EmailMessage(\n            subject='Test message',\n            body='Hello, world!',\n            from_email='from@example.com',\n            to=['to@example.com'],\n            attachments=[\n                ('file.txt', b'Hello, world!', 'text/plain'),\n            ],\n        )\n        self.assertIsInstance(msg.message(), SafeMIMEMultipart)\n        self.assertIn('Content-Type', msg.message().keys())\n        self.assertIn('Content-Disposition', msg.message()['Content-Type'].values())\n", "    def test_email_message_headers(self):\n        \"\"\"\n        Test EmailMessage has the expected headers.\n        \"\"\"\n        email = EmailMessage('Subject', 'Body', 'from@example.com', 'to@example.com')\n        message = email.message()\n        expected_headers = {\n            ('Subject', 'Subject'),\n            ('From', 'from@example.com'),\n            ('To', 'to@example.com'),\n            ('Date', email._date),\n        }\n        self.assertMessageHasHeaders(message, expected_headers)\n", "    def test_email_message_with_bad_header(self):\n        msg = EmailMessage(\n            subject='Test Email',\n            body='Hello',\n            from_email='from@example.com',\n            to=['to@example.com'],\n        )\n        with self.assertRaises(BadHeaderError):\n            msg['Subject'] = '\\nHeader Value'\n"], "sample_1007": ["def test_subfactorial_negative_input():\n    x = Symbol('x')\n    assert subfactorial(-1) == 1\n    assert subfactorial(-2) == 0\n    assert subfactorial(-3) == -1\n    assert subfactorial(-4) == 0\n    assert subfactorial(-5) == 1\n", "def test_FallingFactorial_at_zero():\n    from sympy import FallingFactorial as ff\n    assert ff(0, 0).is_positive\n    assert ff(0, 0).is_integer\n    assert ff(0, 0).is_even\n    assert ff(0, 0).is_nonnegative\n", "def test_factorial_even_integer():\n    assert factorial(2).is_even\n", "def test_falling_factorial_eval_is_odd():\n    from sympy import ff\n    # Test cases for even values of x\n    assert ff(4, 1)._eval_is_odd() == False\n    assert ff(4, 2)._eval_is_odd() == True\n    assert ff(4, 4)._eval_is_odd() == False\n    assert ff(4, 0)._eval_is_odd() == False\n    # Test cases for odd values of x\n    assert ff(3, 1)._eval_is_odd() == True\n    assert ff(3, 2)._eval_is_odd() == False\n    assert ff(3, 3)._eval_is_odd() == True\n    assert ff(3, 0)._eval_is_odd() == False\n", "def test_FallingFactorial_non_integer_arg():\n    n, k = symbols('n k', integer=True)\n    ff_1 = ff(n, k)\n    assert str(ff_1) == 'FallingFactorial(n, k)'\n    \n    ff_2 = ff(2.5, 3)\n    expected = \"FallingFactorial(2.5, 3)\"\n    assert str(ff_2) == expected\n    \n    # Test the _eval method\n    result = ff(2.5, 3).evalf()\n    expected = 4.256998491753147\n    assert result.evalf() == expected\n    \n    # Test that FallingFactorial is immutable\n    ff_3 = ff(n, k)\n    result = ff_3.args = (2, 3)\n    assert result == (2, 3)\n    \n    # Test that a non-integer argument to FallingFactorial raises an error\n    from sympy.functions.combinatorial.factorials import FallingFactorial\n    with raises(ValueError):\n        FallingFactorial(n, 3)\n        \n    # Test that a negative argument to FallingFactorial raises an error\n    with raises(ValueError):\n        ff(-2, 3)\n"], "sample_55": ["    def test_AdminForm_readonly_fields(self):\n        response = self.client.get(reverse('admin:article_article_changelist'))\n        form = response.context['adminform']\n        self.assertIsInstance(form, AdminForm)\n        self.assertEqual(form.readonly_fields, ())\n", "    def test_admin_form_fields_extraction(self):\n        response = self.client.get(reverse('admin:admin_views_actor_changelist'))\n        self.assertEqual(len(self.get_admin_form_fields(response)), 2)\n        self.assertEqual(self.get_admin_form_fields(response)[0].field['name'], 'id')\n        self.assertEqual(self.get_admin_form_fields(response)[1].field['name'], 'name')\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        clsInlineAdminFormSet = InlineAdminFormSet(\n            inline=BookAdmin,\n            formset=BookAdmin.formset,\n            fieldsets=BookAdmin.fieldsets,\n            prepopulated_fields=BookAdmin.prepopulated_fields,\n            model_admin=BookAdmin,\n        )\n        clsInlineFormSet = InlineAdminFormSet(\n            inline=ChapterAdmin,\n            formset=ChapterAdmin.formset,\n            fieldsets=ChapterAdmin.fieldsets,\n            prepopulated_fields=ChapterAdmin.prepopulated_fields,\n            model_admin=ChapterAdmin,\n        )\n", "    def test_inline_formset_gets_data_from_instance(self):\n        response = self.client.get(reverse('admin:admin_ordered_admin_method_changelist'))\n        admin_formset = response.context['inline_admin_formset']\n        self.assertIsNotNone(admin_formset)\n        self.assertEqual(admin_formset.inline_formset_data(), json.dumps({\n            'name': '#admin_ordered_admin_method_fieldsets',\n            'options': {\n                'prefix': 'admin_ordered_admin_method_fieldsets',\n                'addText': 'Add another field',\n                'deleteText': 'Remove',\n            }\n        }))\n", "    def test_inline_formset_errors(self):\n        # Create some objects\n        s = Section.objects.create(name='Test section')\n        a = Article.objects.create(content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=s)\n        b = Article.objects.create(content='<p>Oldest content</p>', date=datetime.datetime(2000, 3, 18, 11, 54, 58), section=s)\n\n        # Create a POST request to the change view with some invalid data\n        data = {\n            \"name\": \"Test section\",\n            \"article_set-TOTAL_FORMS\": \"2\",\n            \"article_set-INITIAL_FORMS\": \"2\",\n            \"article_set-MAX_NUM_FORMS\": \"0\",\n            \"article_set-0-id\": a.pk,\n            \"article_set-0-content\": \"&lt;p&gt;Middle content&lt;/p&gt;\",\n            \"article_set-0-date_0\": \"2008-03-18\",\n            \"article_set-0-date_1\": \"11:54:58\",\n            \"article_set-0-section\": s.pk,\n            \"article_set-1-id\": b.pk,\n            \"article_set-1-content\": \"&lt;p&gt;Oldest content&lt;/p&gt;\",\n            \"article_set-1-date_0\": \"2000-03-18\",\n            \"article_set-1-date_1\": \"11:54:58\",\n            \"article_set-1-section\": 999,  # invalid section ID\n        }\n\n        response = self.client.post(reverse('admin:article_article_changelist'), data, follow=True)\n\n        # Check that the response contains the correct error messages\n        self.assertContentBefore(response, 'Article', 'article_set-1-section')\n        self.assertContentBefore(response, 'This field is required.', 'Invalid data in article_set-1-section')\n        self.assertContentBefore(response, 'Select a valid choice.', 'article_set-1-section')\n"], "sample_744": ["def test_PowerTransformer_zero_lambda():\n    pt = PowerTransformer(method='box-cox', standardize=True, copy=True)\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    with assert_warns_message(DataConversionWarning,\n                             lambda: pt.fit_transform(X)):\n        pt.fit(X)\n    X_transformed = pt.transform(X)\n    X_inverse = pt.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse, X)\n", "def test_PowerTransformer_inverse_transform_with_zero_lambdas():\n    pt = PowerTransformer(method='box-cox', standardize=True, copy=True)\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    pt.fit(X)\n\n    X_transformed = pt.transform(X)\n\n    # Check if inverse transform raises an error when lambda is zero\n    with pytest.raises(NotFittedError):\n        pt.inverse_transform(X_transformed)\n\n    pt.fit(X, np.array([0, 0, 0]))\n    X_transformed = pt.transform(X)\n\n    # Check if inverse transform returns the original data when lambda is zero\n    X_inverse_transformed = pt.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse_transformed, X)\n\n    # Check if inverse transform raises an error when lambda is not zero\n    with pytest.raises(NotFittedError):\n        pt.fit(X, np.array([1, 1, 1]))\n        pt.inverse_transform(X_transformed)\n\n    # Check if inverse transform returns the original data when lambda is not zero\n    pt.fit(X, np.array([0, 0, 0]))\n    X_inverse_transformed = pt.inverse_transform(X_transformed)\n    assert_array_equal(X_inverse_transformed, X)\n", "def test_RobustScaler_max_abs_norm():\n    # test that max_abs_norm can be used instead of with_std with RobustScaler\n    # and that the result is equal to the original\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 5)\n    scaler1 = RobustScaler(with_centering=True, with_scaling=True)\n    scaler2 = RobustScaler(with_centering=True, with_scaling=False, max_abs_norm=True)\n    scaler1.fit(X)\n    scaler2.fit(X)\n    X_scaled1 = scaler1.transform(X)\n    X_scaled2 = scaler2.transform(X)\n    assert_array_almost_equal(X_scaled1, X_scaled2, decimal=5)\n", "    def test_quantile_transform_skewed_data(self):\n        # Test the quantile transform on skewed data\n        X = np.array([[1.0, 5.0, 10.0],\n                      [2.0, 4.0, 10.0],\n                      [3.0, 3.0, 10.0]])\n        q = QuantileTransformer()\n        q.fit(X)\n        X_transformed = q.transform(X)\n        assert_array_almost_equal(X_transformed, np.array([[0., 0.5, 1.],\n                                                          [0., 0.25, 1.],\n                                                          [0., 0., 1.]]))\n", "def test_power_transform():\n    # smoke tests\n    pt = PowerTransformer()\n    X = np.array([1, 2, 3])\n    with pytest.raises(ValueError):\n        pt.fit_transform(X)\n\n    # check box-cox\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([1, 2, 3])\n    pt.fit(X)\n    X_transformed = pt.transform(X)\n    X_inverse = pt.inverse_transform(X_transformed)\n    assert_allclose(X, X_inverse)\n\n    # check different method\n    pt = PowerTransformer(method='not_box-cox')\n    with pytest.raises(ValueError):\n        pt.fit(X)\n\n    # check standardize\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([1, 2, 3])\n    pt.fit(X)\n    X_transformed = pt.transform(X)\n    X_inverse = pt.inverse_transform(X_transformed)\n    X_scaled = StandardScaler().fit_transform(X)\n    X_scaled_transformed = pt.transform(X_scaled)\n    X_scaled_inverse = pt.inverse_transform(X_scaled_transformed)\n    assert_allclose(X_inverse, X_scaled_inverse)\n\n    # check copy\n    pt = PowerTransformer(method='box-cox', copy=False)\n    X = np.array([1, 2, 3])\n    X_copy = X.copy()\n    pt.fit(X)\n    pt.transform(X)\n    assert_allclose(X, X_copy)\n\n    # check method='box-cox' for zero-mean, unit-variance data\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([1, 2, 3])\n    pt.fit(X)\n    X_transformed = pt.transform(X)\n    assert_allclose(X_transformed.mean(), 0)\n    assert_allclose(X_transformed.std(), 1)\n\n    # check method='box-cox' for non-zero-mean, non-unit-variance data\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([1, 2, 4])\n    pt.fit(X)\n    X_transformed = pt.transform(X)\n    assert_not_allclose(X_transformed.mean(), 0)\n    assert_not_allclose(X_transformed.std(), 1)\n\n    # test inverse transform\n    pt = PowerTransformer(method='box-cox')\n    X = np.array([1, 2, 3])\n    X_transformed = pt.transform(X)\n    X_inverse = pt"], "sample_908": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1060": ["def test_PythonCodePrinter_print_Infinity():\n    x = oo\n    printer = PythonCodePrinter(settings={'fully_qualified_modules': False})\n    assert printer.doprint(x) == 'inf'\n", "def test_SparseMatrix_printing():\n    sm = SparseMatrix([[1, 2], [3, 4]])\n    print(sm)\n    printer = SciPyPrinter()\n    assert printer.doprint(sm) == \"scipy.sparse.coo_matrix([1, 2, 3, 4], ((0, 0), (0, 1), (1, 0), (1, 1)), shape=(2, 2))\"\n", "def test_SparseMatrix():\n    # Test SciPyPrinter\n    sp = SciPyPrinter()\n    mat = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    assert sp.doprint(mat) == \"scipy.sparse.coo_matrix([1, 2, 3], ((0, 0), (1, 1), (2, 2)), shape=(3, 3))\"\n    assert sp.doprint(SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3], (0, 1, 2, 1), (0, 1, 2, 2)], [(0, 0), (1, 1), (2, 2)])) == \"scipy.sparse.coo_matrix([1, 2, 3], ((0, 0), (1, 1), (2, 2), (0, 1, 2, 1), (0, 1, 2, 2)), shape=(3, 3))\"\n\n    # Test NumPyPrinter\n    np = NumPyPrinter()\n    assert np.doprint(mat) == \"coo_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\"\n\n    # Test MpmathPrinter\n    mp = MpmathPrinter()\n    assert mp.doprint(mat) == \"mpmath.matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\"\n\n    # Test PythonCodePrinter\n    p = PythonCodePrinter()\n    assert p.doprint(mat) == \"[[1, 0, 0], [0, 2, 0], [0, 0, 3]]\"\n", "def test_PythonCodePrinter():\n    # Test abstract class methods\n    with raises(AttributeError):\n        AbstractPythonCodePrinter()\n    \n    expr = 5\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == '5'\n    \n    expr = symbols('x')\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == 'x'\n\n    # Test printing of None and infinity\n    expr = zoo\n    printer = PythonCodePrinter()\n    assert printer._print_NaN(expr) == 'float(\\'nan\\')'\n    \n    expr = -oo\n    printer = PythonCodePrinter()\n    assert printer._print_Infinity(expr) == 'float(\\'-inf\\')'\n    \n    expr = oo\n    printer = PythonCodePrinter()\n    assert printer._print_Infinity(expr) == 'float(\\'inf\\')'\n\n    # Test printing of numbers\n    expr = Rational(1, 2)\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == '1/2'\n\n    expr = 5.5\n    printer = MpmathPrinter()\n    assert printer._print(expr) == 'mpmath.mpf(5.5)'\n\n    # Test printing of symbols\n    expr = symbols('x')\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == 'x'\n\n    # Test printing of basic functions\n    expr = sin(x)\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == 'sin(x)'\n\n    expr = exp(x)\n    printer = PythonCodePrinter()\n    assert printer._print(expr) == 'exp(x)'\n\n    # Test printing of built-in functions\n    expr = piecewise(x > 0, x, -x)\n    printer = PythonCodePrinter()\n    assert printer._print_Piecewise(expr) == '(x if x > 0 else -x)'\n\n    # Test printing of relational operators\n    expr = Eq(x, 1)\n    printer = PythonCodePrinter()\n    assert printer._print_Relational(expr) == '(x == 1)'\n\n    expr = Le(x, 1)\n    printer = PythonCodePrinter()\n    assert printer._print_Relational(expr) == '(x <= 1)'\n\n    expr = Gt(x, 1)\n    printer = PythonCodePrinter()\n    assert printer._print_Relational(expr) == '(x > 1)'\n\n    # Test printing of logical", "def test_SciPyPrinter():\n    printer = SciPyPrinter()\n    f = FunctionDefinition('f', (x, y), pi)\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  pi\"\n\n    f = FunctionDefinition('f', (x, y), x + y)\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  x + y\"\n\n    f = FunctionDefinition('f', (x, y), None)\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  None\"\n\n    f = FunctionDefinition('f', (x, y), FunctionDefinition('g', (z,), z**2))\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  def g(z):\\n    z**2\"\n\n    f = FunctionDefinition('f', (x, y), (x + y) + z)\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  (x + y) + z\"\n\n    f = FunctionDefinition('f', (x, y), Eq(x + y, 2))\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  Eq(x + y, 2)\"\n\n    f = FunctionDefinition('f', (x, y), Mod(x + y, 3))\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  Mod(x + y, 3)\"\n\n    # Testing that issue 17065 is fixed\n    f = FunctionDefinition('f', (x, y), FunctionDefinition('g', (z,), sin(z)))\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  def g(z):\\n    sin(z)\"\n\n    f = FunctionDefinition('f', (x, y), FunctionDefinition('g', (z,), FunctionDefinition('h', (w,), w**2)))\n    assert printer._print_FunctionDefinition(f) == \"def f(x, y):\\n  def g(z):\\n    def h(w):\\n      w**2\"\n\n    # Testing that issue 17416 is fixed\n    f = FunctionDefinition('f', (x, y), And(x"], "sample_693": ["def test_make_xunit_fixture():\n    class TestClass:\n            pass\n\n            pass\n\n    fixture = _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, scope=\"function\", pass_self=True)\n    assert fixture is not None\n\n    fixture = _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, scope=\"function\", pass_self=False)\n    assert fixture is not None\n\n    fixture = _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, scope=\"session\", pass_self=True)\n    assert fixture is None\n\n    fixture = _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, scope=\"package\", pass_self=True)\n    assert fixture is None\n\n    class TestClassWithCleanup:\n            pass\n\n            pass\n\n            pass\n\n    fixture = _make_xunit_fixture(TestClassWithCleanup, \"setup_method\", \"teardown_method\", \"cleanup\", scope=\"function\", pass_self=True)\n    assert fixture is not None\n\n    fixture = _make_xunit_fixture(TestClassWithCleanup, \"setup_method\", \"teardown_method\", \"cleanup\", scope=\"session\", pass_self=True)\n    assert fixture is None\n\n    # Test with invalid scope\n    with pytest.raises(ValueError):\n        _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, scope=\"invalid_scope\", pass_self=True)\n\n    # Test with invalid setup_name\n    with pytest.raises(ValueError):\n        _make_xunit_fixture(TestClass, \"invalid_setup_name\", \"teardown_method\", None, scope=\"function\", pass_self=True)\n\n    # Test with invalid teardown_name\n    with pytest.raises(ValueError):\n        _make_xunit_fixture(TestClass, \"setup_method\", \"invalid_teardown_name\", None, scope=\"function\", pass_self=True)\n\n    # Test with invalid cleanup_name\n    with pytest.raises(ValueError):\n        _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", \"invalid_cleanup_name\", scope=\"function\", pass_self=True)\n", "def test_make_xunit_fixture(tmpdir):\n    class TestClass:\n            pass\n\n            pass\n\n    fixture = _make_xunit_fixture(\n        TestClass, \"setup_method\", \"teardown_method\", None, scope=\"function\", pass_self=True\n    )\n    assert fixture is not None\n\n    # Test the fixture's name\n    assert fixture.name.startswith(\"_unittest_setup_method_fixture\")\n\n    # Test the fixture's scope\n    assert fixture.scope == \"function\"\n\n    # Test the fixture's autouse flag\n    assert fixture.autouse\n\n    # Test the fixture's function\n    assert callable(fixture.function)\n\n    # Test the fixture's setup and teardown functions\n    assert fixture.setup is TestClass.setup_method\n    assert fixture.teardown is TestClass.teardown_method\n\n    # Test the fixture's cleanup function\n    assert fixture.cleanup is None\n", "def test_check_testcase_implements_trial_reporter(tmpdir: str) -> None:\n    \"\"\"Test check_testcase_implements_trial_reporter function.\"\"\"\n    test_dir = tmpdir / \"test_dir\"\n    test_file = test_dir / \"test_file.py\"\n    test_file.write_text(\n        \"\"\"\n        import pytest\n        from twisted.trial.unittest import TestCase\n        class MyTestCase(TestCase):\n                pass\n        \"\"\"\n    )\n    test_file.parent.mkdir()\n    test_file.write_text(\"import pytest\")\n    test_file.write_text(\"import twisted.trial.unittest\")\n    test_file.write_text(\n        \"\"\"\n        from twisted.trial.itrial import IReporter\n        from zope.interface import classImplements\n        classImplements(MyTestCase, IReporter)\n        \"\"\"\n    )\n    result = pytest.main([str(test_file), \"-p\", \"no:twisted\"])\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"AssertionError\" in str(result.out)\n    assert \"TestCaseFunction\" in str(result.out)\n\n    test_file.write_text(\n        \"\"\"\n        from twisted.trial.unittest import TestCase\n        class MyTestCase(TestCase):\n            __unittest_skip__ = True\n        \"\"\"\n    )\n    result = pytest.main([str(test_file), \"-p\", \"no:twisted\"])\n    assert result.ret == ExitCode.SKIP\n\n    test_file.write_text(\n        \"\"\"\n        from twisted.trial.unittest import TestCase\n        class MyTestCase(TestCase):\n                super().__init__()\n                self.__unittest_skip__ = True\n        \"\"\"\n    )\n    result = pytest.main([str(test_file), \"-p\", \"no:twisted\"])\n    assert result.ret == ExitCode.TESTS_FAILED\n    assert \"AssertionError\" in str(result.out)\n    assert \"TestCaseFunction\" in str(result.out)\n", "def test_check_testcase_implements_trial_reporter(tmpdir: str) -> None:\n    \"\"\"\n    Test that check_testcase_implements_trial_reporter correctly marks an item as a trial reporter.\n    \"\"\"\n    # Create a test class that implements IReporter\n    class MyTestCase(unittest.TestCase):\n        class _TrialReporter:\n                pass\n\n        classImplements(MyTestCase, twisted.trial.itrial.IReporter)\n\n            pass\n\n    # Create a Pytester\n    pytester = Pytester(tmpdir)\n\n    # Create a test item for MyTestCase\n    item = pytest.Item.from_parent(pytester, name=\"test_my_test\", parent=MyTestCase)\n\n    # Run pytest_runtest_protocol with the test item\n    with pytest.raises(SystemExit) as excinfo:\n        pytest_runtest_protocol(item)\n\n    # Assert that the item is a trial reporter\n    assert item._is_trial_reporter\n", "def test_make_xunit_fixture():\n    \"\"\"Test _make_xunit_fixture creation.\"\"\"\n    # Create a test class with setup and teardown methods.\n    class TestClass:\n            pass\n\n            pass\n\n            pass\n\n            pass\n\n    # Create a test class without setup and teardown methods.\n    class TestClassNoSetup:\n        pass\n\n    # Test that _make_xunit_fixture returns None when no setup and teardown methods are defined.\n    assert _make_xunit_fixture(TestClassNoSetup, \"setup_class\", \"teardown_class\", \"cleanup\", \"class\", False) is None\n\n    # Test that _make_xunit_fixture returns None when setup method is not callable.\n    assert _make_xunit_fixture(TestClass, \"setup_class\", \"non_callable_teardown\", \"cleanup\", \"class\", False) is None\n\n    # Test that _make_xunit_fixture returns None when teardown method is not callable.\n    assert _make_xunit_fixture(TestClass, \"callable_setup\", \"teardown_class\", \"cleanup\", \"class\", False) is None\n\n    # Test that _make_xunit_fixture creates a fixture when setup and teardown methods are defined.\n    fixture = _make_xunit_fixture(TestClass, \"setup_class\", \"teardown_class\", \"cleanup\", \"class\", False)\n    assert fixture is not None\n\n    # Test that the fixture is a generator.\n    assert isinstance(fixture, type(()))\n\n    # Test that the fixture has the correct scope and name.\n    assert fixture.scope == \"class\"\n    assert fixture.name == \"_unittest_setup_class_fixture_TestClass\"\n\n    # Test that the fixture is autouse.\n    assert fixture.autouse == True\n\n    # Test that the fixture has the correct arguments.\n    assert fixture.args == []\n    assert fixture.kwargs == {}\n\n    # Test that the fixture is called with the correct arguments.\n    with pytest.raises(SystemExit):\n        fixture._pytestconfig = pytest.config\n        next(fixture)  # type: ignore[operator]\n\n    # Test that the fixture is called with the correct fixture request.\n    request = pytest.config._fixturemanager.getfixtureinfo(name=\"test_function\", func=None)\n    fixture.request = request\n    with pytest.raises(SystemExit):\n        next(fixture)  # type: ignore[operator]\n\n    # Test that the setup method is called.\n    def setup_called = False\n"], "sample_488": ["    def test_set_installed_apps(self):\n        # Check that set_installed_apps() raises an exception if the app registry isn't ready yet.\n        with self.assertRaises(AppRegistryNotReady):\n            apps.set_installed_apps([])\n\n        # Check that set_installed_apps() raises a ValueError if the installed apps isn't a list.\n        with self.assertRaises(ValueError):\n            apps.set_installed_apps('not a list')\n\n        # Check that set_installed_apps() raises a ValueError if the installed apps contains duplicates.\n        apps.set_installed_apps(SOME_INSTALLED_APPS)\n        with self.assertRaises(ValueError):\n            apps.set_installed_apps(SOME_INSTALLED_APPS + ['django.contrib.admin'])\n\n        # Check that set_installed_apps() resets the app registry.\n        apps.set_installed_apps([])\n        self.assertEqual(list(apps.get_app_configs()), [])\n\n        # Check that set_installed_apps() calls the ready() method of the app config.\n        class MockAppConfig(AppConfig):\n            name = 'test_app'\n                self.ready_called = True\n\n        apps.set_installed_apps([MockAppConfig])\n        self.assertTrue(MockAppConfig.ready_called)\n\n        # Check that set_installed_apps() updates the app registry.\n        apps.set_installed_apps(SOME_INSTALLED_APPS)\n        self.assertEqual(len(apps.get_app_configs()), 5)\n", "    def test_set_installed_apps_then_unset_installed_apps(self):\n        apps.set_installed_apps([\"apps.apps.MyAdmin\", \"django.contrib.auth\"])\n        self.assertTrue(apps.is_installed(\"MyAdmin\"))\n        self.assertFalse(apps.is_installed(\"MyAuth\"))\n        apps.unset_installed_apps()\n        self.assertFalse(apps.is_installed(\"MyAdmin\"))\n        self.assertTrue(apps.is_installed(\"MyAuth\"))\n", "    def test_set_available_apps_and_get_app_config(self):\n        apps.set_available_apps([\"apps.apps.MyAdmin\", \"django.contrib.contenttypes\"])\n        self.assertEqual(len(apps.get_app_configs()), 2)\n        self.assertTrue(apps.get_app_config(\"admin\"))\n        self.assertTrue(apps.get_app_config(\"contenttypes\"))\n", "    def test_set_installed_apps_and_unset_installed_apps(self):\n        apps.clear_cache()\n        self.assertIsNone(apps.get_app_config('one_config_app'))\n\n        # Test with some installed apps\n        installed_apps = [\n            'django.contrib.contenttypes',\n            'django.contrib.sessions',\n            'django.contrib.messages',\n            'django.contrib.staticfiles',\n        ]\n        apps.set_installed_apps(installed_apps)\n        self.assertEqual(apps.get_app_configs(), [\n            apps.get_app_config('contenttypes'),\n            apps.get_app_config('sessions'),\n            apps.get_app_config('messages'),\n            apps.get_app_config('staticfiles'),\n        ])\n\n        # Test with no installed apps\n        apps.unset_installed_apps()\n        self.assertEqual(apps.get_app_configs(), [])\n\n        # Test with some installed apps\n        installed_apps = [\n            'one_config_app',\n            'two_configs_one_default_app',\n        ]\n        apps.set_installed_apps(installed_apps)\n        self.assertEqual(apps.get_app_configs(), [\n            apps.get_app_config('one_config_app'),\n            apps.get_app_config('two_config'),\n        ])\n\n        # Test that set_installed_apps() can be called multiple times\n        apps.set_installed_apps(installed_apps)\n        self.assertEqual(apps.get_app_configs(), [\n            apps.get_app_config('one_config_app'),\n            apps.get_app_config('two_config'),\n        ])\n\n        # Test that set_installed_apps() can be called with a different list of installed apps\n        installed_apps = [\n            'django.contrib.contenttypes',\n            'two_configs_one_default_app',\n        ]\n        apps.set_installed_apps(installed_apps)\n        self.assertEqual(apps.get_app_configs(), [\n            apps.get_app_config('contenttypes'),\n            apps.get_app_config('two_config'),\n        ])\n\n        # Test that set_installed_apps() raises an exception when the app registry isn't ready\n        apps.unset_installed_apps()\n        with self.assertRaises(AppRegistryNotReady):\n            apps.set_installed_apps(installed_apps)\n\n        # Test that set_installed_apps() raises a ValueError when the list of installed apps isn't a subset of installed apps\n        installed_apps = [\n            'non_existent_app',\n        ]\n        with self.assertRaises(ValueError):\n            apps.set_installed_apps(installed_apps)\n", "    def test_set_installed_apps(self):\n        # Test that set_installed_apps() triggers a full population of the registry.\n        new_apps.set_installed_apps(SOME_INSTALLED_APPS)\n        self.assertTrue(new_apps.apps.apps_ready)\n        self.assertTrue(new_apps.apps.models_ready)\n        self.assertTrue(new_apps.apps.ready)\n        self.assertEqual(len(new_apps.apps.app_configs), 8)\n\n        # Test that unset_installed_apps() resets the registry back to its previous state.\n        new_apps.unset_installed_apps()\n        self.assertFalse(new_apps.apps.apps_ready)\n        self.assertFalse(new_apps.apps.models_ready)\n        self.assertFalse(new_apps.apps.ready)\n        self.assertEqual(len(new_apps.apps.app_configs), 4)\n"], "sample_572": ["    def test_univariate_integration(self, x):\n        kde = KDE()\n        density, support = kde(x)\n        assert_array_almost_equal(density.sum(), 1)\n", "    def test_univariate_stat(self, x):\n        h = Histogram(stat=\"count\")\n        hist, bin_edges = h(x)\n        assert len(hist) == len(bin_edges) - 1\n", "    def test_bivariate_define_support(self, rng):\n        x1 = rng.normal(0, 1, 100)\n        x2 = rng.normal(0, 5, 100)\n        kde = KDE(bw_method='silverman', bw_adjust=1, gridsize=50, cut=2, clip=None, cumulative=False)\n        support = kde.define_support(x1, x2)\n        assert isinstance(support, (tuple, list))\n        assert len(support) == 2\n        assert len(support[0]) == 50\n        assert len(support[1]) == 50\n", "    def test_univariate(self, x):\n        hist = Histogram(stat=\"count\", bins=10)\n        hist_def = hist.define_bin_params(x)\n        hist_def = dict(bins=np.linspace(x.min(), x.max(), 10))\n        x_hist, bin_edges = hist(x)\n        expected = np.histogram(x, **hist_def)\n        assert_array_equal(x_hist, expected[0])\n", "    def test_estimate_aggregator_single_value_estimator(self, rng, x):\n        estimator = np.mean\n        errorbar = \"sd\"\n        aggregator = EstimateAggregator(estimator, errorbar=errorbar)\n        estimate = aggregator(x, \"value\")\n        assert_array_almost_equal(estimate[\"value\"], np.mean(x))\n        assert_array_almost_equal(estimate[\"value\"], np.mean(x))\n"], "sample_416": ["    def test_settings_to_cmd_args_env_env_variables(self):\n        # Test when all env variables are provided.\n        settings_dict = {\n            \"OPTIONS\": {\n                \"passfile\": \"/path/to/passfile\",\n                \"service\": \"test_service\",\n                \"sslmode\": \"require\",\n                \"sslrootcert\": \"/path/to/sslrootcert\",\n                \"sslcert\": \"/path/to/sslcert\",\n                \"sslkey\": \"/path/to/sslkey\"\n            },\n            \"PASSWORD\": \"test_password\"\n        }\n        expected_env = {\n            \"PGPASSWORD\": \"test_password\",\n            \"PGSERVICE\": \"test_service\",\n            \"PGSSLMODE\": \"require\",\n            \"PGSSLROOTCERT\": \"/path/to/sslrootcert\",\n            \"PGSSLCERT\": \"/path/to/sslcert\",\n            \"PGSSLKEY\": \"/path/to/sslkey\",\n            \"PGPASSFILE\": \"/path/to/passfile\"\n        }\n        args, env = self.settings_to_cmd_args_env(settings_dict)\n        self.assertEqual(env, expected_env)\n", "    def test_sslmode_invalid_values(self):\n        with self.assertRaises(TypeError):\n            DatabaseClient.settings_to_cmd_args_env(\n                {\"OPTIONS\": {\"sslmode\": \"invalid\"}},\n                parameters=[\"--help\"]\n            )\n", "    def test_settings_to_cmd_args_env_with_service(self):\n        settings_dict = {\n            \"NAME\": \"test_db\",\n            \"HOST\": \"localhost\",\n            \"USER\": \"test_user\",\n            \"PASSWORD\": \"test_password\",\n            \"OPTIONS\": {\n                \"service\": \"my_service\"\n            }\n        }\n        parameters = [\"--echo-queries\"]\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, [\"psql\", \"-U\", \"test_user\", \"-h\", \"localhost\", \"-p\", \"5432\", \"test_db\", \"--echo-queries\"])\n        self.assertEqual(env, {\"PGSERVICE\": \"my_service\", \"PGPASSWORD\": \"test_password\"})\n", "    def test_runshell_signal_restore(self):\n        # Test that the SIGINT handler is restored after runshell returns.\n            raise KeyboardInterrupt\n        with mock.patch.object(signal, 'getsignal') as mock_getsignal:\n            mock_getsignal.return_value = sigint_handler\n            with self.assertRaises(KeyboardInterrupt):\n                client = DatabaseClient()\n                client.runshell([])\n            self.assertEqual(mock_getsignal.call_args_list, [\n                mock.call(signal.SIGINT),\n                mock.call(signal.SIGINT, signal.SIG_IGN),\n                mock.call(signal.SIGINT, signal.SIGINT),\n            ])\n", "    def test_settings_to_cmd_args_env_default_database(self):\n        settings_dict = {\"OPTIONS\": {\"service\": \"my_service\"}}\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, [])\n        self.assertEqual(args, [\"psql\", \"-U\", \"postgres\", \"-h\", \"127.0.0.1\", \"-p\", \"5432\", \"postgres\"])\n        self.assertIsNone(env)\n"], "sample_1114": ["def test_ImageSet_iterable():\n    z = Symbol('z')\n    z2 = Symbol('z', integer=True)\n    ImageSet1 = ImageSet(Lambda(z, z**2), S.Naturals)\n    ImageSet2 = ImageSet(Lambda(z2, z2**2), S.Naturals)\n    ImageSet1_iterable = iter(ImageSet1)\n    ImageSet2_iterable = iter(ImageSet2)\n    ImageSet1_list = list(ImageSet1_iterable)\n    ImageSet2_list = list(ImageSet2_iterable)\n\n    assert len(ImageSet1_list) == 23\n    assert len(ImageSet2_list) == 23\n    assert ImageSet1_list[0] == 1\n    assert ImageSet2_list[0] == 1\n\n    ImageSet1_iterable = iter(ImageSet1)\n    ImageSet2_iterable = iter(ImageSet2)\n    ImageSet1_set = set(ImageSet1_iterable)\n    ImageSet2_set = set(ImageSet2_iterable)\n\n    assert len(ImageSet1_set) == 23\n    assert len(ImageSet2_set) == 23\n    assert ImageSet1_set == {1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529}\n    assert ImageSet2_set == {1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529}\n", "def test_Rationals():\n    from sympy import S, Rational\n    assert S.Rationals.is_subset(S.Reals)\n    assert S.Rationals.is_superset(S.Integers)\n    assert S.Rationals._contains(1)\n    assert S.Rationals._contains(1.5)\n    assert not S.Rationals._contains(pi)\n    assert S.Rationals.contains(1)\n    assert not S.Rationals.contains(1.5)\n    assert not S.Rationals.contains(pi)\n", "def test_ImageSet():\n    # Test ImageSet with FiniteSet\n    from sympy.sets.sets import imageset, FiniteSet\n    from sympy.sets.fancysets import ImageSet, Range\n    r = Range(1, 5)\n    x = Symbol('x', integer=True)\n    image_set = ImageSet(Lambda(x, x**2), r)\n    expected = FiniteSet(1, 4, 9, 16)\n    assert imageset(image_set) == expected\n\n    # Test ImageSet with Range\n    r = Range(1, 5)\n    x = Symbol('x', integer=True)\n    image_set = ImageSet(Lambda(x, x**2), r)\n    expected = FiniteSet(1, 4, 9, 16)\n    assert imageset(image_set) == expected\n\n    # Test ImageSet with Symbolic Range\n    n = Symbol('n', integer=True)\n    image_set = ImageSet(Lambda(n, n**2), Range(n, n + 10, 2))\n    expected = FiniteSet(0, 4, 16, 36, 64, 100)\n    assert imageset(image_set) == expected\n\n    # Test ImageSet with ComplexRegion\n    r = Range(-1, 1)\n    theta = Range(0, 2*pi)\n    complex_region = ComplexRegion(r * theta, polar=True)\n    x = Symbol('x', cls=Dummy)\n    image_set = ImageSet(Lambda((x, y), x + y*I), complex_region)\n    expected = Union(FiniteSet(-2 - 2*I, -2, -1 - 2*I, -1, 0 - 2*I, 0, 1 - 2*I, 1, 2 - 2*I, 2),\n                    FiniteSet(-2 + 2*I, -2, -1 + 2*I, -1, 0 + 2*I, 0, 1 + 2*I, 1, 2 + 2*I, 2))\n    assert imageset(image_set) == expected\n\n    # Test ImageSet with FiniteSet and Lambda\n    from sympy.sets.sets import imageset\n    image_set = ImageSet(Lambda((x, y), x + y*I), FiniteSet(1, 2) * FiniteSet(3, 4))\n", "def test_Range():\n    r = Range(2, 5)\n    assert r == Range(2, 5, 1)\n    assert r.as_relational(x) == And(Eq(floor(x), x), 2 <= x, x < 5)\n", "def test_ImageSet_preimage():\n    from sympy.solvers.solveset import _solveset_multi\n    from sympy import And, Eq, Lambda, S, Interval, Union, ImageSet, FiniteSet\n    x, y = symbols('x, y')\n\n    # Test an ImageSet of FiniteSets\n    s = ImageSet(Lambda((x, y), x**2 + y**2), FiniteSet((1, 2), (3, 4)))\n    assert s == FiniteSet(6, 10)\n\n    # Test an ImageSet with non-matching arguments\n        return Eq(x**2 + y, 4)\n    s = ImageSet(Lambda((x, y), f(x, y)), FiniteSet((1, 2), (3, 4)))\n    assert s == ImageSet(Lambda((x, y), f(x, y)), FiniteSet((1, 2), (3, 4)))\n\n    # Test an ImageSet with negative ranges\n    s = ImageSet(Lambda((x, y), x**2 + y**2), Range(-10, 10))\n    assert s == FiniteSet(0)\n\n    # Test an ImageSet with ranges\n    s = ImageSet(Lambda((x, y), x**2 + y**2), Range(-10, 10))\n    assert s == FiniteSet(0)\n\n    # Test an ImageSet with intervals\n    s = ImageSet(Lambda((x, y), x**2 + y**2), Interval(-10, 10))\n    assert s == FiniteSet(0)\n\n    # Test an ImageSet with Union of ranges\n    s = ImageSet(Lambda((x, y), x**2 + y**2), Union(Range(-10, 0), Range(0, 10)))\n    assert s == FiniteSet(0)\n\n    # Test an ImageSet with Union of intervals\n    s = ImageSet(Lambda((x, y), x**2 + y**2), Union(Interval(-10, 0), Interval(0, 10)))\n    assert s == FiniteSet(0)\n\n    # Test an ImageSet with a Lambda function with multiple arguments\n    s = ImageSet(Lambda((x, y), x**2 + y**2), FiniteSet((1, 2), (3, 4)))\n"], "sample_5": ["def test_parameter_bounds(model):\n    model_instance = model()\n    for parameter in model_instance.parameters.values():\n        if parameter.bounds is not None:\n            # Test that getting and setting bounds work correctly\n            min_value, max_value = parameter.bounds\n            assert min_value == parameter.min\n            assert max_value == parameter.max\n            parameter.min = 0\n            assert parameter.bounds == (0, parameter.max)\n            parameter.max = 5\n            assert parameter.bounds == (parameter.min, 5)\n            # Test that setting bounds with min and max simultaneously raises an error\n            with pytest.raises(ValueError):\n                parameter.bounds = (1, 2, 3)\n            # Test that setting bounds with min and max with different units raises an error\n            with pytest.raises(TypeError):\n                parameter.bounds = (1 * u.m, 2 * u.cm)\n", "def test_parameter_bounds(model, tmp_path):\n    \"\"\"\n    Test parameter bounds\n    \"\"\"\n    model = model()\n    param = model.parameters[0]\n\n    # Test setting bounds directly\n    param.bounds = (10, 20)\n    assert param.min == 10\n    assert param.max == 20\n\n    # Test setting min and max separately\n    param.min = 15\n    param.max = 25\n    assert param.bounds == (15, 25)\n\n    # Test setting bounds with a non-numeric value\n    with pytest.raises(TypeError):\n        param.bounds = ('a', 'b')\n\n    # Test setting min with a non-numeric value\n    with pytest.raises(TypeError):\n        param.min = 'a'\n\n    # Test setting max with a non-numeric value\n    with pytest.raises(TypeError):\n        param.max = 'a'\n\n    # Test setting bounds with a Quantity value\n    param.bounds = (10 * u.m, 20 * u.m)\n    assert param.min == 10\n    assert param.max == 20\n    assert param.bounds == (10, 20)\n\n    # Test setting min with a Quantity value\n    param.min = 15 * u.m\n    param.max = 25 * u.m\n    assert param.bounds == (15, 25)\n\n    # Test setting max with a Quantity value\n    param.min = 15 * u.m\n    param.max = 25 * u.m\n    assert param.bounds == (15, 25)\n\n    # Test setting bounds with a single value\n    param.bounds = (10,)\n    assert param.min == 10\n    assert param.max == None\n\n    # Test setting min with a single value\n    param.min = 15\n    assert param.bounds == (15, None)\n\n    # Test setting max with a single value\n    param.max = 25\n    assert param.bounds == (None, 25)\n\n    # Test setting bounds with a non-numeric value\n    with pytest.raises(TypeError):\n        param.bounds = ('a',)\n\n    # Test setting min with a non-numeric value\n    with pytest.raises(TypeError):\n        param.min = 'a'\n\n    # Test setting max with a non-numeric value\n    with pytest.raises(TypeError):\n        param.max = 'a'\n\n    # Test setting bounds with a Quantity value\n    param.bounds = (10 * u.m,)\n", "def test_parameter_value_with_quantity_input(mpl_plot_tester, model):\n    \"\"\"\n    Test that the value of a parameter can be set to a Quantity object\n    and that the unit is converted correctly.\n    \"\"\"\n    # create a model instance\n    model_instance = model(**{param: value for param, value in model['parameters'].items()})\n    # set the unit of one of the parameters to a quantity\n    model_instance.parameters['amplitude'].unit = u.kg\n    # test that the value of the parameter can be set to a Quantity object\n    model_instance.parameters['amplitude'].value = 5 * u.kg\n    assert_quantity_allclose(model_instance.parameters['amplitude'].value, 5 * u.kg, rtol=1e-9)\n    assert model_instance.parameters['amplitude'].unit == u.kg\n", "def test_poly_models_fitting(model):\n    class FakeModel:\n            self.parameters = {param.name: param for param in model.parameters.values()}\n\n            return inputs\n\n    fitter = LevMarLSQFitter()\n    model_instance = model(**{name: Quantity(value) for name, value in model.parameters.items()})\n    model_instance.model = FakeModel()\n\n    fitter.fit(model_instance, np.array([3*u.m, 36*u.one]), model_instance)\n\n    assert_quantity_allclose(model_instance.parameters['c0'].value, 3)\n    assert_quantity_allclose(model_instance.parameters['c1'].value, 2/u.m)\n    assert_quantity_allclose(model_instance.parameters['c2'].value, 3/u.m**2)\n\n    assert model_instance.parameters['c0'].fixed\n    assert not model_instance.parameters['c1'].fixed\n    assert not model_instance.parameters['c2'].fixed\n", "def test_parameter_bounds_on_polynomial_models(model):\n    \"\"\"Check that polynomial models handle bounds correctly.\"\"\"\n\n    # Set bounds\n    param = model.parameters['degree']\n    param.bounds = (0, 5)\n    assert param.min == 0\n    assert param.max == 5\n    assert param.bounds == (0, 5)\n    param.bounds = (0, 10)\n    assert param.min == 0\n    assert param.max == 10\n    assert param.bounds == (0, 10)\n\n    # Test bounds setter with min and max\n    param.min = 1\n    assert param.bounds == (1, 10)\n    param.max = 5\n    assert param.bounds == (1, 5)\n    with pytest.raises(ValueError):\n        param.min = 5\n        param.max = 1\n\n    # Test bounds setter with min and bounds\n    param.min = 1\n    param.bounds = (1, 5)\n    assert param.bounds == (1, 5)\n\n    # Test bounds setter with max and bounds\n    param.max = 5\n    param.bounds = (1, 5)\n    assert param.bounds == (1, 5)\n\n    # Test bounds setter with bounds\n    param.bounds = (1, 5)\n    assert param.bounds == (1, 5)\n\n    # Test bounds setter with invalid bounds\n    param.bounds = (-1, 1)\n    assert param.bounds == (-1, 1)\n    param.bounds = (1, -1)\n    assert param.bounds == (1, -1)\n\n    # Test min setter with invalid value\n    param.min = 'a'\n    assert param.min == 1\n    param.min = 1.5\n    assert param.min == 1\n\n    # Test max setter with invalid value\n    param.max = 'a'\n    assert param.max == 5\n    param.max = 6.5\n    assert param.max == 5\n\n    # Test bounds setter with invalid values\n    param.bounds = ('a', 5)\n    assert param.bounds == (0, 5)\n    param.bounds = (1, 'a')\n    assert param.bounds == (1, 5)\n"], "sample_1029": ["def test_Fraction_field_repr():\n    # Test that srepr delivers the expected string\n    # and that the condition eval(srepr(expr))==expr holds.\n    assert srepr(field.QQ) == \"FracField(QQ, Reals, order=lex)\"\n    assert eval(srepr(field.QQ), ENV) == field.QQ\n\n    # Test a different field\n    assert srepr(field.QQ('t', order=lex)) == \"FracField(QQ('t'), Reals, order=lex)\"\n    assert eval(srepr(field.QQ('t', order=lex)), ENV) == field.QQ('t', order=lex)\n", "def test_srepr_SparseMatrix():\n    \"\"\"Test srepr for SparseMatrix.\"\"\"\n    M = Matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    sM = srepr(M)\n    assert srepr(M) == sM\n    assert eval(sM, ENV) == M\n", "def test_MonogenicFiniteExtension():\n    \"\"\"\n    Test the MonogenicFiniteExtension in srepr\n    \"\"\"\n    x = symbols('x')\n    ext = FiniteExtension(x**2 + 1, ZZ)\n    sT(ext, \"FiniteExtension(x**2 + 1, ZZ)\")\n\n", "compilation error", "compilation error"], "sample_738": ["    def test_hashing_vectorizer_fit_transform(self):\n        v = HashingVectorizer(n_features=100)\n        X = v.fit_transform([\"the dog is very good\", \"the cat is good\"])\n        assert_array_equal(X.shape, (2, 100))\n", "def test_strip_accents_ascii_return_type():\n    s = \"\u00e9\"\n    result = strip_accents_ascii(s)\n    assert isinstance(result, str)\n\n", "def test_hashing_vectorizer():\n    # Test that a HashingVectorizer instance can be fitted\n    X = HashingVectorizer().fit_transform([\"hello world\", \"hello\"])\n    assert X.shape[1] == 2 ** 20\n\n    # Test that the output of the transform method is the same when\n    # the input is the same\n    X1 = HashingVectorizer().fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer().fit_transform([\"hello world\", \"hello\"])\n    assert_array_equal(X1.toarray(), X2.toarray())\n\n    # Test that the output of the transform method is different when the\n    # input is different\n    X1 = HashingVectorizer().fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer().fit_transform([\"hello world\", \"foo\"])\n    assert_not_equal(X1.toarray(), X2.toarray())\n\n    # Test that the binary parameter works as expected\n    X1 = HashingVectorizer(binary=True).fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer(binary=False).fit_transform([\"hello world\", \"hello\"])\n    assert_array_equal(X1.toarray(), X2.toarray())\n\n    # Test that the norm parameter works as expected\n    X1 = HashingVectorizer(norm='l1').fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer(norm='l2').fit_transform([\"hello world\", \"hello\"])\n    assert_not_equal(X1.toarray(), X2.toarray())\n\n    # Test that the alternate_sign parameter works as expected\n    X1 = HashingVectorizer(alternate_sign=True).fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer(alternate_sign=False).fit_transform([\"hello world\", \"hello\"])\n    assert_not_equal(X1.toarray(), X2.toarray())\n\n    # Test that the non_negative parameter works as expected\n    X1 = HashingVectorizer(non_negative=True).fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer(non_negative=False).fit_transform([\"hello world\", \"hello\"])\n    assert_not_equal(X1.toarray(), X2.toarray())\n\n    # Test that the dtype parameter works as expected\n    X1 = HashingVectorizer(dtype=np.float32).fit_transform([\"hello world\", \"hello\"])\n    X2 = HashingVectorizer(dtype=np", "def test_TfidfVectorizer_stop_words():\n    v = TfidfVectorizer(stop_words='english')\n    # Test that the stop words attribute is populated correctly\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(set(v.stop_words_), ENGLISH_STOP_WORDS)\n\n    # Test that the vocabulary attribute is populated correctly\n    assert_equal(set(v.vocabulary_.keys()), set(uppercase(doc).split()\n                                             for doc in ALL_FOOD_DOCS))\n\n    # Test that the fit_transform method correctly ignores stop words\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(X.toarray().sum(axis=1), np.array([5, 3, 5, 3, 2, 2]))\n\n    # Test that the transform method correctly ignores stop words\n    X = v.fit_transform(ALL_FOOD_DOCS)\n    Y = v.transform(ALL_FOOD_DOCS)\n    assert_array_equal(X.toarray(), Y.toarray())\n", "def test_tfidfvectorizer_doc_features_names():\n    \"\"\"Check that the document feature names of the `TfidfVectorizer` are\n    correct when the `vocabulary_` attribute is set.\"\"\"\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_in(\"burger\", vectorizer.get_feature_names())\n    assert_in(\"burger\", vectorizer.vocabulary_)\n    assert_in(\"burger\", vectorizer._tfidf.vocabulary_)\n"], "sample_272": ["    def test_migration_plan_empty_targets(self):\n        executor = MigrationExecutor(connection=connection)\n        with self.assertRaises(InvalidMigrationPlan):\n            executor.migration_plan(targets=())\n", "    def test_mutate_state_preserve(self):\n        migration = self.create_migration(\"migrations\", \"0001_initial\")\n        migration.initial = True\n        state = self._create_project_state()\n        state2 = migration.mutate_state(state, preserve=True)\n        self.assertNotEqual(state, state2)\n        self.assertEqual(state.apps, state2.apps)\n", "    def test_detect_soft_applied(self):\n        migration = self.get_migration(\"0001_initial\")\n        project_state = ProjectState()\n        is_applied, _ = migration.executor.detect_soft_applied(project_state, migration)\n        self.assertFalse(is_applied)\n\n        # Create the tables and fields from the initial migration\n        migration.executor.apply_migration(project_state, migration)\n        is_applied, _ = migration.executor.detect_soft_applied(project_state, migration)\n        self.assertTrue(is_applied)\n", "def test_migrate_unmigrated_apps(self):\n    class Migration(migrations.Migration):\n        dependencies = []\n        operations = [\n            migrations.CreateModel(\n                name='testmodel',\n                fields=[],\n                options={'managed': True},\n            ),\n        ]\n\n    with override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"}):\n        executor = MigrationExecutor(connection)\n        state = ProjectState(real_apps=list(executor.loader.unmigrated_apps))\n        plan = executor.migration_plan((('migrations', None),))\n        self.assertEqual(len(plan), 1)\n        state = executor.migrate(plan=plan, state=state)\n        self.assertTrue(executor.recorder.applied_migrations())\n        self.assertFalse(executor.recorder.unapplied_migrations())\n", "    def test_migrate_with_fake_initial(self):\n        executor = MigrationExecutor(connection, progress_callback=lambda x, y, z: None)\n        state = ProjectState(real_apps=list(executor.loader.unmigrated_apps))\n        migration = executor.loader.applied_migrations[\"test_migrations.0002_auto_20210101_1200\"]\n        # Make sure the initial migration is not applied yet\n        self.assertTrue(migration not in executor.loader.applied_migrations)\n        # Migrate up to the initial migration with fake_initial=True\n        state = executor.migrate([(migration.app_label, migration.name)], fake_initial=True)\n        # Check that the initial migration is not recorded\n        self.assertTrue(migration not in executor.recorder.applied_migrations())\n        # Migrate up to the initial migration with fake_initial=False\n        state = executor.migrate([(migration.app_label, migration.name)], fake_initial=False)\n        # Check that the initial migration is now recorded\n        self.assertIn(migration, executor.recorder.applied_migrations())\n"], "sample_234": ["def test_difference(self):\n    q1 = Number.objects.filter(num__gt=F('other_num')).values_list('num')\n    q2 = Number.objects.filter(num__lt=F('other_num')).values_list('num')\n    expected_numbers = [1, 2, 3, 8, 9]\n    self.assertNumbersEqual(q1.difference(q2), expected_numbers, ordered=True)\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_union_with_raw_query(self):\n        queryset = Number.objects.raw('SELECT * FROM myapp_number')\n        union_queryset = queryset.union(Number.objects.raw('SELECT * FROM myapp_number'))\n        with self.assertRaises(NotImplementedError):\n            union_queryset.values('num')\n", "def test_raw_queryset_iterator(self):\n    Number.objects.raw('SELECT num FROM myapp_number WHERE num > 5')\n    self.assertEqual(list(Number.objects.raw('SELECT num FROM myapp_number WHERE num > 5'))[:2], [6, 7])\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num > num')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num < num')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num != num')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num IS NULL AND num IS NOT NULL')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num = NULL')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num = NOT NULL')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num = 5 OR num = NOT 5')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num = 5 OR num = 5')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num != 5 AND num != 5')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num != 5 AND num != 5 AND num != 5')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num IN (5, NOT 5, 5)')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num IN (5, 5)')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num NOT IN (5, 5)')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num IN (5, 5, 5)')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num NOT IN (5, 5, 5)')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects.raw('SELECT num FROM myapp_number WHERE num BETWEEN 5 AND NOT 5')\n\n    with self.assertRaises(DatabaseError):\n        Number.objects", "    def test_update_updates_correct_fields(self):\n        Number.objects.create(num=1)\n        Number.objects.create(num=2)\n        num = 5\n        Number.objects.update(num=num)\n        self.assertEqual(Number.objects.count(), 2)\n        self.assertEqual(Number.objects.get(num=1).num, num)\n        self.assertEqual(Number.objects.get(num=2).num, num)\n"], "sample_312": ["    def test_add_method_with_squash_false(self):\n        node = Node()\n        data = Node([('a', 1), ('b', 2)])\n        result = node.add(data, 'DEFAULT', squash=False)\n        self.assertEqual(result, data)\n        self.assertEqual(node.children, data.children)\n", "    def test_add_squash_false(self):\n        \"\"\"Test add with squash=False.\"\"\"\n        node = Node()\n        child_node = Node([('a', 1)])\n        result = node.add(child_node, 'OR', squash=False)\n        self.assertEqual(node.children, [child_node])\n        self.assertEqual(result, child_node)\n", "    def test_add_method_with_squash_false(self):\n        \"\"\"Verify add method works when squash is set to False.\"\"\"\n        node3 = Node([self.node1])\n        node4 = Node(connector='OR', negated=True)\n        result = node3.add(node4, 'AND', squash=False)\n        self.assertEqual(len(node3.children), 2)\n        self.assertIn(node4, node3.children)\n", "    def test_negate_node(self):\n        # Test that negating a node with children works as expected\n        self.node1.negate()\n        self.assertTrue(self.node1.negated)\n        self.assertTrue(self.node1)\n        \n        # Test that negating a node with no children works as expected\n        self.assertFalse(self.node2)\n        self.node2.negate()\n        self.assertTrue(self.node2)\n\n        # Test that negating a node with a child works as expected\n        node3 = Node([self.node1])\n        self.assertTrue(node3)\n        node3.negate()\n        self.assertFalse(node3)\n", "    def test_negate(self):\n        self.node1.negate()\n        self.assertTrue(self.node1.negated)\n"], "sample_584": ["def test_check_shape_tile_ids():\n    tile_ids = [(), (1,), (1, 2), (1, 2, 3)]\n    with raises_regex(ValueError, r\"The supplied objects do not form a hypercube \"\n                             r\"because sub-lists do not have consistent \"\n                             r\"depths\"):\n        _check_shape_tile_ids(tile_ids)\n\n    tile_ids = [(1,), (1,), (1, 2), (1, 2)]\n    with raises_regex(ValueError, r\"The supplied objects do not form a hypercube \"\n                             r\"because sub-lists do not have consistent \"\n                             r\"lengths along dimension0\"):\n        _check_shape_tile_ids(tile_ids)\n\n    tile_ids = [(1, 2), (1, 2), (1, 2), (1, 2)]\n    _check_shape_tile_ids(tile_ids)\n\n    tile_ids = [(1,), (1, 2), (1, 2), (1, 2)]\n    _check_shape_tile_ids(tile_ids)\n", "def test_infer_concat_order_from_coords_on_single_dataset():\n    ds0 = Dataset({'x': [1, 2, 3]})\n    ds1 = Dataset({'x': [2, 3, 4]})\n    with raises_regex(ValueError, \"Could not find any dimension coordinates to use to order the datasets for concatenation\"):\n        _infer_concat_order_from_coords([ds0, ds1])\n", "def test_concat_dim_with_none():\n    \"\"\"Test _combine_all_along_first_dim with concat_dim=None.\"\"\"\n    arrays = [np.arange(6), np.arange(6)]\n    combined_ids = _combine_all_along_first_dim(\n        {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'no_conflicts')\n    assert_equal(combined_ids, {(): _combine_1d([arrays[0], arrays[1]], None,\n                                                'no_conflicts', 'all',\n                                                'different')})\n\n    arrays = [np.arange(6), np.arange(6)]\n    combined_ids = _combine_all_along_first_dim(\n        {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'no_conflicts',\n        fill_value=np.nan)\n    assert_equal(combined_ids, {(): _combine_1d([arrays[0], arrays[1]], None,\n                                                'no_conflicts', 'all',\n                                                'different', np.nan)})\n\n    arrays = [np.arange(6), np.arange(6)]\n    combined_ids = _combine_all_along_first_dim(\n        {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'equals')\n    assert_equal(combined_ids, {(): _combine_1d([arrays[0], arrays[1]], None,\n                                                'equals', 'all', 'different')})\n\n    arrays = [np.arange(6), np.arange(6)]\n    combined_ids = _combine_all_along_first_dim(\n        {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'identical')\n    assert_equal(combined_ids, {(): _combine_1d([arrays[0], arrays[1]], None,\n                                                'identical', 'all', 'different')})\n\n    arrays = [np.arange(6), np.arange(6)]\n    with pytest.raises(ValueError):\n        _combine_all_along_first_dim(\n            {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'broadcast_equals')\n\n    arrays = [np.arange(6), np.arange(6)]\n    with pytest.raises(ValueError):\n        _combine_all_along_first_dim(\n            {(): arrays[0], (1,): arrays[1]}, None, 'all', 'different', 'no_conflicts',\n           ", "    def test_infer_concat_order_from_positions_simple(self):\n        # Tests whether _infer_concat_order_from_positions correctly identifies the\n        # order of tile_ids based on a simple nested list structure.\n        a = DataArray([1, 2, 3], dims=['x'])\n        b = DataArray([4, 5, 6], dims=['x'])\n        c = DataArray([7, 8, 9], dims=['x'])\n        data = [a, [b, c]]\n        tile_ids = _infer_concat_order_from_positions(data)\n        expected_tile_ids = OrderedDict([((), a), ((0,), b), ((1,), c)])\n        assert_combined_tile_ids_equal(tile_ids, expected_tile_ids)\n", "def test_manual_combine_with_empty_dataset():\n    # Create a dataset with an empty data variable\n    ds = Dataset()\n    ds['x'] = DataArray(np.array([1, 2, 3]))\n    \n    # Create a nested list of datasets\n    nested_ds = [ds, ds]\n    \n    # Try to manually combine the datasets\n    with raises_regex(ValueError, r'Empty dataset'):\n        combine_nested(nested_ds, concat_dim='x')\n\n"], "sample_1138": ["def test_TR5_odd_even():\n    from sympy.simplify.fu import TR5\n    from sympy.abc import x\n    from sympy import sin\n    assert TR5(sin(x)**3) == sin(x)**3\n    assert TR5(sin(x)**5) == (1 - cos(x)**2)**2*sin(x)**2\n    assert TR5(sin(x)**7) == (1 - cos(x)**2)**4*sin(x)**2\n", "def test_TR14():\n    # test TR14 on different types of arguments\n    assert TR14((cos(x) - 1)*(cos(x) + 1)) == sin(x)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)) == -cos(x)**2\n    assert TR14((cos(x) - 1)*(cos(x) + 1)*(cos(y) + 1)) == -sin(x)**2*cos(y)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)*(sin(y) - 1)) == cos(x)**2*cos(y)**2\n    assert TR14((cos(x) - 1)*(cos(x) + 1)*(sin(y) + 1)) == -sin(x)**2*cos(y)**2\n    assert TR14((sin(x) - 1)*(sin(x) + 1)*(cos(y) - 1)) == cos(x)**2*cos(y)**2\n\n    # test TR14 with non-trigonometric arguments\n    assert TR14((x - 1)*(x + 1)) == (x - 1)*(x + 1)\n    assert TR14((x - 1)*(y + 1)) == (x - 1)*(y + 1)\n    assert TR14((x - 1)*(x + 1)*(y - 1)) == (x - 1)*(x + 1)*(y - 1)\n\n    # test TR14 with rational coefficients\n    assert TR14(2*(cos(x) - 1)*(cos(x) + 1)) == -2*sin(x)**2\n    assert TR14(3*(sin(x) - 1)*(sin(x) + 1)) == -9*cos(x)**2\n    assert TR14(4*(cos(x) - 1)*(cos(x) + 1)*(sin(y) + 1)) == -16*sin(x)**2*cos(y)**2\n\n    # test TR14 with negative coefficients\n    assert TR14(-(cos(x) - 1)*(cos(x) + 1)) == -sin(x)**2\n    assert TR14(-(sin(x) - 1)*(sin(x) + 1)) == cos(x)**2\n    assert TR14(-(cos(x) - 1)*(cos(x) + 1)*(sin", "def test_process_common_addends():\n    # Test that the function can find and combine common addends in the following example\n    rv = Add(x + 2*y, x + 3*y)\n        if a.is_Add:\n            return a.func(a.args[0] + a.args[1])\n        return a\n    assert process_common_addends(rv, do) == 5*y + x\n    # Test that the function can find and combine common addends with different coefficients\n    rv = Add(x + 2*y, 2*x + 3*y)\n    assert process_common_addends(rv, do) == 5*y + 3*x\n    # Test that the function can find and combine common addends with rational coefficients\n    rv = Add(x + 2*y, 2*x + 3/2*y)\n    assert process_common_addends(rv, do) == 5*y + 5*x\n    # Test that the function can find and combine common addends with negative coefficients\n    rv = Add(-x + 2*y, 2*x - 3*y)\n    assert process_common_addends(rv, do) == 0\n    # Test that the function can find and combine common addends with no common factor\n    rv = Add(x + y, x + z)\n    assert process_common_addends(rv, do) == x + y + x + z\n    # Test that the function can find and combine common addends with a key2 function\n    rv = Add(x + y, x + y)\n        return a.is_positive\n    assert process_common_addends(rv, do, key2=key2) == 2*x + 2*y\n    # Test that the function can find and combine common addends with no common factor and key2 function\n    rv = Add(x + y, x + z)\n    assert process_common_addends(rv, do, key2=key2) == x + y + x + z\n", "def test_tr9():\n    from sympy import cos, sin, pi\n\n    # simple cases\n    assert TR9(cos(1) + cos(2)) == 2*cos(1/2)*cos(3/2)\n    assert TR9(sin(1) + sin(2)) == 2*sin(1/2)*cos(1/2)\n    assert TR9(cos(1) + cos(2) + cos(3)) == 2*cos(1/2)*cos(3/2) + cos(3/2)*cos(5/2)\n    assert TR9(sin(1) + sin(2) + sin(3)) == 2*sin(1/2)*cos(1/2) - cos(1/2)*cos(3/2) + sin(3/2)*cos(1/2)\n\n    # when no change is made by TR9, no re-arrangement will be made\n    assert TR9(cos(3) + cos(3)*cos(2)) == cos(3) + cos(2)*cos(3)\n\n    # change by TR9 but not by TR10\n    assert TR9(cos(1) + 2*sin(1) + 2*sin(2)) == cos(1) + 4*sin(3/2)*cos(1/2)\n\n    # no change by TR9\n    assert TR9(cos(3) + 2*sin(3) + 2*sin(2)) == cos(3) + 2*sin(3) + 2*sin(2)\n\n    # two args\n    assert TR9(cos(x) + sin(y)) == sin(x + y) + sin(x - y)\n\n    # test that factoring of common term is attempted\n    assert TR9(cos(3) + cos(3)*cos(2)) == cos(3) + cos(2)*cos(3)\n\n    # 4 args\n    assert TR9(cos(x) + sin(y) + cos(z) + sin(a)) == (sin(x + y) + sin(x - y))/2 + (sin(z + a) + sin(z - a))/2\n\n    # 5 args\n    assert TR9(cos(x) + sin(y) + cos(z) + sin(a) + cos(b)) == (\n        sin(x + y) + sin(x - y", "def test_process_common_addends():\n    from sympy.core import S\n    from sympy.abc import x\n\n    # test that TR12 can make use of process_common_addends\n    e = (2*sin(x + 1) + sin(x - 1))/(sin(x + 1)*sin(x - 1))\n    rv = TR12(e)\n    assert rv == 4\n\n    # test that process_common_addends can make use of TR12\n    e = sin(2*x + 1) + sin(2*x - 1)\n    rv = TR12(process_common_addends(e, TR12))\n    assert rv == 4*sin(x)\n\n    # test the key function\n    f = lambda x: x.count(x)\n    rv = process_common_addends(e, TR12, key2=f)\n    assert rv == e\n\n    # test that the keys are correctly sorted\n    e = Add(sin(2*x + 1), sin(3*x + 1))\n    rv = process_common_addends(e, TR12)\n    assert rv.args[0] == sin(2*x + 1)\n\n    # test that process_common_addends can handle the case when only\n    # key2 is supplied\n    e = Add(S(1)*sin(x + 1), S(2)*sin(x - 1))\n    rv = process_common_addends(e, TR12, key2=lambda x: x.as_numer_denom()[1])\n    assert rv.args[0] == S(2)*sin(x - 1)\n\n    # test that process_common_addends can handle the case when only key1 is supplied\n    e = Add(S(2)*sin(x + 1), S(-1)*sin(x - 1))\n    rv = process_common_addends(e, TR12)\n    assert rv.args[0] == S(2)*sin(x + 1)\n\n    # test that process_common_addends can handle the case when neither key1 nor key2 is supplied\n    e = Add(sin(x + 1), sin(x - 1))\n    try:\n        process_common_addends(e, TR12)\n        assert False, \"should raise an exception\"\n    except ValueError:\n        pass\n\n    # test that process_common_addends can handle the case when there are no\n    # common addends\n    e = Add(sin(x + 1), sin(x"], "sample_329": ["    def test_deconstructable_instance(self):\n        serializer = serializer_factory(DeconstructibleInstances())\n        self.assertEqual(serializer.serialize(), (\"DeconstructibleInstances()\", {'import custom_migration_operations.tests'}))\n", "    def test_serializer_factory_model_field(self):\n        field = models.CharField(max_length=255)\n        self.assertEqual(serializer_factory(field), ModelFieldSerializer(field))\n", "    def test_register_unregister(self):\n        # Test registering a serializer\n        class TestSerializer(BaseSerializer):\n                return 'test', set()\n\n        Serializer.register(float, TestSerializer)\n        self.assertIn(float, Serializer._registry)\n        self.assertEqual(Serializer._registry[float], TestSerializer)\n\n        # Test unregistering a serializer\n        Serializer.unregister(float)\n        self.assertNotIn(float, Serializer._registry)\n", "    def test_float_serialize(self):\n        self.assertEqual(serializer_factory(float(1.5)).serialize(), ('float(1.5)', set()))\n", "    def test_FunctorialSerializer(self):\n        instance = functools.partial(math.sin, 10)\n        serializer = serializer_factory(instance)\n        self.assertEqual(serializer.serialize(), ('functools.partial(sin, 10)', {'import math', 'import functools'}))\n"], "sample_1170": ["def test_MatPow_printing():\n    M = Matrix([[1, 2], [3, 4]])\n    p = StrPrinter()\n    assert p._print_MatPow(M**2) == 'Matrix([[1, 2], [3, 4]]**2)'\n", "def test_StrPrinter_MatMul():\n    p = StrPrinter()\n    assert p.doprint(Matrix([[1, 2], [3, 4]])) == 'Matrix([[1, 2], [3, 4]])'\n    assert p.doprint(Matrix([[1, 2], [3, 4]] * 3)) == 'Matrix([[1, 2], [3, 4]])**3'\n    assert p.doprint(Matrix([[1, 2], [3, 4]]) * Matrix([[5, 6], [7, 8]])) == 'Matrix([[1, 2], [3, 4]] * [[5, 6], [7, 8]])'\n", "def test_StrPrinter():\n    p = StrPrinter({'order': 'none'})\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) + Matrix([[1, 2], [3, 4]]) == \"0\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) - Matrix([[1, 2], [3, 4]]) == \"0\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) * Matrix([[1, 2], [3, 4]]) == \"8\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) / Matrix([[1, 2], [3, 4]]) == \"1\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) / 2 == \"[[0.5, 1.0], [1.5, 2.0]]\"\n\n    assert p._print(1 / Matrix([[1, 2], [3, 4]]) == \"[0.125, -0.25]\"\n\n    assert p._print(2 * Matrix([[1, 2], [3, 4]]) == \"[[2, 4], [6, 8]]\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) + 2 == \"[[3, 4], [5, 6]]\"\n\n    assert p._print(2 + Matrix([[1, 2], [3, 4]]) == \"[[3, 4], [5, 6]]\"\n\n    assert p._print(Matrix([[1, 2], [3, 4]]) * (2 + Matrix([[1, 2], [3, 4]])) == \"16\"\n\n    assert p._print((2 + Matrix([[1, 2], [3, 4]]) * 2) * Matrix([[1, 2], [3, 4]]) == \"32\"\n\n    assert p._print((Matrix([[1, 2], [3, 4]]) * 2) * (2 + Matrix([[1, 2], [3, 4]])) == \"32\"\n\n    assert p._print(1 * Matrix([[1, 2], [3, 4]]) == \"[[1, 2], [3, 4]]\"\n\n", "def test_str_printer_permutation_cyclic():\n    from sympy.combinatorics import Permutation\n    p = Permutation([1, 2, 3, 4, 5])\n    assert StrPrinter(settings={\"perm_cyclic\": True}).doprint(p) == \"(1,2,3)\"\n    assert StrPrinter(settings={\"perm_cyclic\": False}).doprint(p) == \"Permutation([1, 2, 3, 4, 5])\"\n", "def test_strprinter_unevaluated_mul():\n    # test printer for unevaluated Mul with negative Rational factors\n    expr = -x * (y + 2) * (z + 3)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"-x*(y+2)*(z+3)\"\n    assert printer._print(expr.func(*expr.args)) == \"-x*y*z - 2*x*z - 2*x*y - 4\"\n\n    # test printer for unevaluated Mul with negative Rational factors\n    expr = x * (y + 2) / (z + 3)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"(x*(y+2))/(z+3)\"\n    assert printer._print(expr.func(*expr.args)) == \"x*y/z + 2*x/z + 2*x*y/(z+3)\"\n\n    # test printer for unevaluated Mul with negative Rational factors\n    expr = -x * (y + 2) / (z + 3)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"-(x*(y+2))/(z+3)\"\n    assert printer._print(expr.func(*expr.args)) == \"-x*y/z - 2*x/z - 2*x*y/(z+3)\"\n\n    # test printer for unevaluated Mul with non-Rational factors\n    expr = x * (y + 2) * sin(z)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"x*(y+2)*sin(z)\"\n\n    # test printer for unevaluated Mul with non-Rational factors\n    expr = x * (y + 2) / (z + 3)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"(x*(y+2))/(z+3)\"\n\n    # test printer for unevaluated Mul with a single non-Rational factor\n    expr = x * (y + 2)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"x*(y+2)\"\n\n    # test printer for unevaluated Mul with a single Rational factor\n    expr = 2 * (y + 2)\n    printer = StrPrinter()\n    assert printer._print(expr) == \"2*(y+2)\"\n\n    # test printer for unevaluated Mul with a negative Rational factor\n    expr = -2 * (y +"], "sample_18": ["    def setup_class(cls):\n        cls.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        cls.q.info.name = \"v\"\n        cls.q.info.description = \"air speed of a african swallow\"\n        cls.q2 = u.Quantity(np.arange(1.0, 5.0), \"km/s\")\n        cls.q2.info.name = \"v\"\n        cls.q2.info.description = \"air speed of a african swallow\"\n", "    def teardown_class(self):\n        self.q = None\n", "    def setup_class(cls):\n        cls.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        cls.q.info.name = \"v\"\n        cls.q.info.description = \"air speed of a african swallow\"\n", "    def setup_class(cls):\n        cls.q = u.Quantity(np.arange(1.0, 5.0), \"m/s\")\n        cls.q.info.name = \"v\"\n        cls.q.info.description = \"air speed of a african swallow\"\n", "compilation error"], "sample_184": ["    def test_get_deferred_fields(self):\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255, default=DEFERRED)\n            field3 = models.CharField(max_length=255, default=DEFERRED)\n\n        obj = TestModel()\n        self.assertEqual(obj.get_deferred_fields(), {'field2', 'field3'})\n", "    def test_index_together_fields(self):\n        # Test that index_together fields are checked correctly\n        # when creating a model.\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = [('field1', 'field2')]\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 0)\n\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = ['field1']\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 0)\n\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = [(1, 'field2')]\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = ['field1', 'field2', 'field3']\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = [['field1', 'field2']]\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n", "    def test_check_single_primary_key(self):\n        class Model(models.Model):\n            pk1 = models.AutoField(primary_key=True)\n            pk2 = models.AutoField(primary_key=True)\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E026')\n", "    def setUpTestData(cls):\n        from invalid_models_tests.models import Parent, Child\n        Parent.objects.create(id=1)\n        Child.objects.create(parent_id=1)\n", "    def test_save_base(self):\n        class TestModel(models.Model):\n            id = models.AutoField(primary_key=True)\n            name = models.CharField(max_length=255)\n\n        with transaction.atomic():\n            with self.assertRaises(ValueError):\n                TestModel.objects.create(name='test')\n\n        with transaction.atomic():\n            test_model = TestModel.objects.create(name='test')\n            test_model.save(using='other')\n            self.assertEqual(test_model._state.db, 'other')\n\n        with transaction.atomic():\n            test_model.save(using='default', update_fields=['name'])\n            self.assertEqual(test_model._state.db, 'default')\n\n        with transaction.atomic():\n            test_model.save(using='other', update_fields=['name'])\n            test_model._state.db = 'default'\n            self.assertEqual(test_model._state.db, 'default')\n\n        with transaction.atomic():\n            test_model.refresh_from_db(using='other', fields=['name'])\n            self.assertEqual(test_model._state.db, 'other')\n\n        with transaction.atomic():\n            test_model.save(using='other', update_fields=['id'])\n            self.assertEqual(test_model._state.db, 'other')\n"], "sample_39": ["    def test_naxis0(self):\n        with pytest.raises(_wcs.NoWcsKeywordsFoundError):\n            validate(\"\")\n", "def test_calc_footprint(self):\n    # Create a WCS object from a sample header\n    header = fits.Header()\n    header['NAXIS1'] = 100\n    header['NAXIS2'] = 100\n    header['CTYPE1'] = 'RA---TAN'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['CRVAL1'] = 12.0\n    header['CRVAL2'] = 23.0\n    header['CRPIX1'] = 50\n    header['CRPIX2'] = 50\n    header['CDELT1'] = 0.01\n    header['CDELT2'] = 0.01\n    wcs = wcs.WCS(header)\n\n    # Calculate the footprint of the image on the sky\n    corners = np.array([[1, 1],\n                        [1, 100],\n                        [100, 100],\n                        [100, 1]], dtype=np.float64)\n    footprint = wcs.calc_footprint(header=header, undistort=True, axes=[100, 100])\n\n    # Check that the result is an array of shape (4, 2)\n    assert footprint.shape == (4, 2)\n\n    # Check that the result is within the expected range\n    assert np.all(np.abs(footprint) < 2)\n\n    # Test that the result is correct when the center of the pixel is used\n    footprint_center = wcs.calc_footprint(header=header, undistort=True, axes=[100, 100], center=True)\n    assert_array_almost_equal(footprint_center, footprint)\n\n    # Test that the result is correct when the corner of the pixel is used\n    footprint_corner = wcs.calc_footprint(header=header, undistort=True, axes=[100, 100], center=False)\n    assert_array_almost_equal(footprint_corner, footprint)\n\n    # Test that the result is None when the image size is not provided\n    no_axes = wcs.calc_footprint(header=header, undistort=True)\n    assert no_axes is None\n\n    # Test that the result is None when the header is None\n    no_header = wcs.calc_footprint(header=None, undistort=True)\n    assert no_header is None\n", "    def setup(self):\n        # ...\n", "    def setup(self):\n        # get the list of the hdr files that we want to test\n        self._file_list = list(get_pkg_data_filenames(\"maps\", pattern=\"*.hdr\"))\n", "    def test_no_warnings(self):\n        filename = get_pkg_data_filename('testwcs', 'testwcs_nohdr.fits')\n        validate(filename)\n"], "sample_45": ["    def test_trunc_second(self):\n        dt = datetime(2022, 1, 1, 12, 30, 0)\n        model = self.create_model(dt, dt + timedelta(days=1))\n        qs = DTModel.objects.annotate(truncated_second=TruncSecond('start_datetime'))\n        self.assertEqual(qs.first().truncated_second, dt.replace(second=0, microsecond=0))\n", "    def test_extract_week_with_timezone_aware_date(self):\n        dt = datetime(2022, 1, 3, tzinfo=timezone.make_aware(timedelta()))\n        obj = DTModel.objects.create(\n            start_date=dt.date(),\n            start_datetime=dt,\n        )\n        self.assertEqual(ExtractWeek('start_date')(obj), 53)\n        self.assertEqual(ExtractWeek('start_datetime')(obj), 53)\n", "    def create_model(self, start_datetime, end_datetime):\n        return DTModel.objects.create(\n            name=start_datetime.isoformat() if start_datetime else 'None',\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            start_date=start_datetime.date() if start_datetime else None,\n            end_date=end_datetime.date() if end_datetime else None,\n            start_time=start_datetime.time() if start_datetime else None,\n            end_time=end_datetime.time() if end_datetime else None,\n            duration=(end_datetime - start_datetime) if start_datetime and end_datetime else None,\n        )\n", "    def test_extract_time_in_timezone(self):\n        # Test ExtractHour, ExtractMinute, ExtractSecond lookups in a timezone-aware datetime\n        dt = datetime(2022, 1, 1, 12, 30, 0, tzinfo=pytz.timezone('US/Pacific'))\n        dt_pytz = dt.astimezone(pytz.utc)\n        self.assertEqual(dt.hour, ExtractHour('start_datetime').f(dt_pytz))\n        self.assertEqual(dt.minute, ExtractMinute('start_datetime').f(dt_pytz))\n        self.assertEqual(dt.second, ExtractSecond('start_datetime').f(dt_pytz))\n", "    def create_model(self, start_datetime, end_datetime, tzinfo=None):\n        return DTModel.objects.create(\n            name=start_datetime.isoformat() if start_datetime else 'None',\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            start_date=start_datetime.date() if start_datetime else None,\n            end_date=end_datetime.date() if end_datetime else None,\n            start_time=start_datetime.time() if start_datetime else None,\n            end_time=end_datetime.time() if end_datetime else None,\n            duration=(end_datetime - start_datetime) if start_datetime and end_datetime else None,\n            tzinfo=tzinfo,\n        )\n"], "sample_686": ["    def test_deprecated_messages(self, pytester):\n        # Test that deprecated messages are correctly created\n        assert isinstance(FUNCARGNAMES, PytestDeprecationWarning)\n        assert isinstance(FILLFUNCARGS, PytestDeprecationWarning)\n        assert isinstance(RESULT_LOG, PytestDeprecationWarning)\n        assert isinstance(FIXTURE_POSITIONAL_ARGUMENTS, PytestDeprecationWarning)\n        assert isinstance(NODE_USE_FROM_PARENT, UnformattedWarning)\n        assert isinstance(JUNIT_XML_DEFAULT_FAMILY, PytestDeprecationWarning)\n        assert isinstance(COLLECT_DIRECTORY_HOOK, PytestDeprecationWarning)\n        assert isinstance(PYTEST_COLLECT_MODULE, UnformattedWarning)\n        assert isinstance(TERMINALWRITER_WRITER, PytestDeprecationWarning)\n        assert isinstance(MINUS_K_DASH, PytestDeprecationWarning)\n        assert isinstance(MINUS_K_COLON, PytestDeprecationWarning)\n        assert isinstance(WARNING_CAPTURED_HOOK, PytestDeprecationWarning)\n        assert isinstance(FSCOLLECTOR_GETHOOKPROXY_ISINITPATH, PytestDeprecationWarning)\n\n        # Test that deprecated messages are correctly formatted\n        assert FILLFUNCARGS.message == \"The `_fillfuncargs` function is deprecated, use \" \\\n                                     \"function._request._fillfixtures() instead if you cannot avoid reaching into internals.\"\n        assert RESULT_LOG.message == \"--result-log is deprecated, please try the new pytest-reportlog plugin.\\n\" \\\n                                  \"See https://docs.pytest.org/en/stable/deprecations.html#result-log-result-log for more information.\"\n        assert NODE_USE_FROM_PARENT.message.format(name=\"Node\") == \"Direct construction of Node has been deprecated, please use Node.from_parent.\\n\" \\\n                                                                \"See https://docs.pytest.org/en/stable/deprecations.html#node-construction-changed-to-node-from-parent for more information.\"\n        assert PYTEST_COLLECT_MODULE.message.format(name=\"CollectModule\") == \"pytest.collect.CollectModule was moved to pytest.CollectModule\\n\" \\\n                                                                       \"Please update to the new name.\"\n\n        # Test that the deprecated messages are raised when used\n        with pytest.raises(DeprecationWarning):\n            deprecated.deprecated(FILLFUNCARGS)\n\n        with pytest.raises(DeprecationWarning):\n            deprecated.deprecated(RESULT_LOG)\n\n        class TestNode(nodes.Node):\n            pass\n\n        with pytest.raises(DeprecationWarning", "def test_MINUS_K_DASH_deprecation_warning(capsys):\n    with pytest.warns(PytestDeprecationWarning, match=\"The `-k '-expr'` syntax to -k is deprecated.\\nUse `-k 'not expr'` instead.\"):\n        pytest.main(['-k', '-expr'])\n", "def test_node_use_from_parent():\n    # Set up a Testdir\n    td = Testdir()\n\n    # Create a node\n    node = nodes.Node(id=\"test_node\")\n\n    # Test that direct construction raises a warning\n    with pytest.warns(UnformattedWarning) as record:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"default\")\n            node_use_from_parent = NODE_USE_FROM_PARENT\n            node.__class__ = type(node)\n            assert node_use_from_parent.node.__class__ == node.__class__\n\n    assert len(record.list) == 1\n    warning_message = str(record.list[0].message)\n    assert f\"{node.__class__.__name__}\" in warning_message\n    assert \"from_parent\" in warning_message\n\n    # Test that construction using from_parent does not raise a warning\n    with pytest.warns(UnformattedWarning) as record:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"default\")\n            node_from_parent = nodes.Node.from_parent(id=\"test_node\")\n            assert node_from_parent.__class__ == node.__class__\n\n    assert len(record.list) == 0\n", "def test_FSCOLLECTOR_GETHOOKPROXY_ISINITPATH(tmpdir, capsys):\n    # Arrange\n    pytestconfig = pytest.Config()\n    collector = mock.Mock(spec=nodes.FSCollector)\n    collector.session = pytestconfig\n    # Act\n    with warnings.catch_warnings(record=True) as w:\n        collector.gethookproxy()\n        collector.isinitpath()\n    # Assert\n    assert len(w) == 2\n    assert isinstance(w[0].message, deprecated.PytestDeprecationWarning)\n    assert str(w[0].message).startswith(\"The gethookproxy() method is deprecated;\")\n    assert isinstance(w[1].message, deprecated.PytestDeprecationWarning)\n    assert str(w[1].message).startswith(\"The isinitpath() method is deprecated;\")\n", "def test_MINUS_K_COLON():\n    \"\"\"Test the MINUS_K_COLON warning is triggered correctly\"\"\"\n    with pytest.warns(PytestDeprecationWarning) as record:\n        pytest.main([\"-k\", \"MINUS_K_COLON:\"], plugins=None)\n    assert len(record.list) == 1\n    warning = record.list[0]\n    assert str(warning.message) == MINUS_K_COLON.message\n"], "sample_391": ["    def test_rename_model_reduces_to_rename_model(self):\n        model_name = \"myapp_mymodel\"\n        new_name = \"myapp_newmodel\"\n        operations = [\n            operations.CreateModel(\n                name=model_name,\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n            ),\n            operations.RenameModel(old_name=model_name, new_name=new_name),\n        ]\n        expected = [\n            operations.CreateModel(\n                name=new_name,\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=255)),\n                ],\n            )\n        ]\n        self.assertOptimizesTo(operations, expected)\n", "    def test_add_index_operations(self):\n        model = models.Model()\n        model.add_index = operations.AddIndex(\n            model._meta.object_name, models.Index(fields=[model._meta.get_field(\"field1\")])\n        )\n        operations = [\n            operations.AddIndex(model, model.add_index),\n        ]\n        self.assertOptimizesTo(operations, [])\n", "    def test_rename_model_operation_reduction(self):\n        operations = [\n            operations.RenameModel(\"old_model\", \"new_model\"),\n            operations.CreateModel(\"old_model\", [(\"id\", models.AutoField(primary_key=True))]),\n            operations.AlterModelManagers(\"old_model\", [(\"objects\", EmptyManager())]),\n            operations.AlterModelOptions(\"old_model\", {\"default_manager_name\": \"old_manager\"}),\n        ]\n        self.assertOptimizesTo(\n            operations, [operations.CreateModel(\"new_model\", [(\"id\", models.AutoField(primary_key=True))])]\n        )\n", "    def test_create_model_reduce_on_duplicate_field(self):\n        operations = [\n            CreateModel(\n                \"test_model\",\n                [(\"id\", models.AutoField(primary_key=True)),\n                 (\"field1\", models.CharField(max_length=255)),\n                 (\"field1\", models.CharField(max_length=255))],\n                options={\"managed\": False},\n            ),\n        ]\n        self.assertDoesNotOptimize(operations, iterations=1)\n        self.assertOptimizesTo(operations, [\n            CreateModel(\n                \"test_model\",\n                [(\"id\", models.AutoField(primary_key=True)),\n                 (\"field1\", models.CharField(max_length=255)),\n                 (\"field2\", models.CharField(max_length=255)),\n                 ],\n                options={\"managed\": False},\n            )\n        ])\n", "    def test_add_and_remove_index_optimizes(self):\n        op = operations.CreateModel(\n            \"myapp_mymodel\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"my_field\", models.CharField(max_length=255)),\n            ],\n        )\n        ops = [op]\n        expected = [\n            \"CreateIndex\",\n            \"Rename table for myapp_mymodel to myapp_mymodel\",\n            \"AlterIndex myapp_mymymodel_0 on field(s) my_field of model myapp_mymodel\",\n            \"RemoveIndex myapp_mymymodel_0 on myapp_mymymodel\",\n            \"Rename table for myapp_mymymodel to myapp_mymymodel\",\n        ]\n        self.assertOptimizesTo(ops, expected, exact=3)\n"], "sample_688": ["def test_on_rm_rf_error(tmp_path):\n    \"\"\"Test on_rm_rf_error.\"\"\"\n    lock_path = tmp_path / \"lock\"\n    os.mkdir(lock_path)\n    with pytest.raises(OSError):\n        on_rm_rf_error(os.remove, str(lock_path), OSError(\"Permission denied\"))\n", "def test_remove_lock_file(tmpdir):\n    \"\"\"Ensure a lock file is removed when it is outdated.\"\"\"\n    lock_path = tmpdir.join(\"lock\")\n    lock_path.write_text(\"test\")\n    mtime = lock_path.stat().st_mtime\n    assert ensure_deletable(lock_path, mtime - LOCK_TIMEOUT - 1) == True\n    lock_path.unlink()\n    assert not lock_path.exists()\n", "def test_parse_num_negative_value():\n    \"\"\"Test parsing of negative value.\"\"\"\n    assert parse_num(\"-1\") == -1\n    assert parse_num(\"-abc\") == -1\n    assert parse_num(\"abc\") == -1\n\n", "def test_make_numbered_dir_cleanup(tmp_path):\n    \"\"\"Check that make_numbered_dir_with_cleanup cleans up after itself.\"\"\"\n    root = tmp_path / \"root\"\n    root.mkdir()\n    dir1 = make_numbered_dir_with_cleanup(root, \"dir\", keep=0, lock_timeout=LOCK_TIMEOUT)\n    assert not cleanup_candidates(root, \"dir\", 1)\n    dir2 = make_numbered_dir_with_cleanup(root, \"dir\", keep=1, lock_timeout=LOCK_TIMEOUT)\n    assert list(cleanup_candidates(root, \"dir\", 1)) == [dir1]\n    assert list(cleanup_candidates(root, \"dir\", 0)) == [dir1, dir2]\n    atexit.register(cleanup_numbered_dir, root, \"dir\", 1, LOCK_TIMEOUT - 1)\n    os._exit(0)  # trigger cleanup\n", "def test_sanity_import_path(tmp_path):\n    \"\"\"Test import_path() without sys.path mutations.\"\"\"\n    py_path = tmp_path / \"test_import_path.py\"\n    py_path.write_text(\"import pytest\\n\")\n    mod = import_path(py_path)\n    assert mod is not None\n"], "sample_888": ["def test_isolation_forest_fit_without_random_state():\n    # Test if IsolationForest can be fit without setting a random_state\n    X = np.array([[1, 2], [3, 4]])\n    iforest = IsolationForest(random_state=None)\n    with pytest.warns(UserWarning):\n        iforest.fit(X)\n    assert iforest.random_state is not None\n    assert isinstance(iforest.random_state, np.random.RandomState)\n", "def test_isolation_forest_n_estimators():\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=3, random_state=42)\n    clf = IsolationForest(n_estimators=10, random_state=42).fit(X)\n    clf.predict(X)\n    assert clf.n_estimators_ == 10\n", "def test_decision_function_single_tree():\n    # Create an instance of IsolationForest with a single tree\n    iforest = IsolationForest(n_estimators=1, random_state=0)\n    X = np.array([[1, 2], [3, 4]])\n    iforest.fit(X)\n\n    # Check that decision_function returns the correct scores\n    assert_array_almost_equal(iforest.decision_function(X), [-1, -1])\n\n    # Check that decision_function raises an error when not fitted\n    with pytest.raises(ValueError):\n        IsolationForest(n_estimators=1).decision_function(X)\n", "def test_isolation_forest_predict_equal_samples():\n    # Verify that predict returns the correct value when using equal samples\n    X = np.array([[1, 2], [3, 4]])\n    clf = IsolationForest(n_estimators=10)\n    clf.fit(X)\n    assert_array_equal(clf.predict(X), [1, 1])\n", "def test_fit_zero_samples():\n    \"\"\"Ensure that no errors are raised when the number of samples is zero.\"\"\"\n    X = np.array([]).reshape(0, 4)\n    with pytest.raises(ValueError):\n        IsolationForest().fit(X)\n"], "sample_1148": ["def test_matrix_element_equality():\n    from sympy import MatrixSymbol, Eq, MatrixElement\n\n    A = MatrixSymbol('A', 3, 3)\n    a = MatrixElement(A, 0, 0)\n    b = MatrixElement(A, 0, 0)\n    c = MatrixElement(A, 1, 1)\n    d = MatrixElement(A, 1, 2)\n\n    assert Eq(a, b) == True\n    assert Eq(a, c) == False\n    assert Eq(a, d) == False\n\n    assert Eq(a, a) == True\n    assert Eq(b, b) == True\n    assert Eq(c, c) == True\n    assert Eq(d, d) == True\n\n    # Test with non-zero matrices\n    A = MatrixSymbol('A', 2, 2)\n    A[0, 0] = 1\n    A[0, 1] = 2\n    A[1, 0] = 3\n    A[1, 1] = 4\n\n    a = MatrixElement(A, 0, 0)\n    b = MatrixElement(A, 0, 1)\n    c = MatrixElement(A, 1, 0)\n    d = MatrixElement(A, 1, 1)\n\n    assert Eq(a, b) == False\n    assert Eq(a, c) == False\n    assert Eq(a, d) == False\n    assert Eq(b, c) == False\n    assert Eq(b, d) == False\n    assert Eq(c, d) == False\n", "def test_transpose_properties():\n    n, m, l = symbols('n m l', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', n, n)\n\n    assert A.T.is_Transpose\n    assert B.T.is_Transpose\n    assert A.T.is_Symmetric == False\n    assert C.T.is_Symmetric\n\n    assert A.shape == (n, m)\n    assert A.T.shape == (m, n)\n    assert B.shape == (m, n)\n    assert B.T.shape == (n, m)\n\n    assert A.T == A.T\n    assert A.T.conjugate() == A.conjugate().T\n    assert B.T.conjugate() == B.conjugate().T\n    assert C.T.conjugate() == C.conjugate().T\n\n    assert A.T._eval_conjugate() == A._eval_conjugate().T\n    assert B.T._eval_conjugate() == B._eval_conjugate().T\n    assert C.T._eval_conjugate() == C._eval_conjugate().T\n", "def test_matrix_derivative_transpose():\n    n = symbols('n')\n    M = MatrixSymbol('M', n, n)\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n    expr = X*M.transpose()*Y\n    assert MatrixExpr._eval_derivative(expr, M) == ZeroMatrix(n, n)\n", "def test_matrix_derivative():\n    from sympy import diff, MatrixSymbol, Matrix\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    x = symbols('x')\n\n    # Test simple case\n    eq = A * B\n    result = diff(eq, x)\n    expected = Matrix([0, 0])\n    assert result == expected\n\n    # Test case where the matrix is the result of a function\n    eq = MatrixSymbol('f', 2, 2) * (A + B)\n    result = diff(eq, x)\n    expected = Matrix([0, 0])\n    assert result == expected\n\n    # Test case where the matrix is the result of an exponential\n    eq = (A + B) ** x\n    result = diff(eq, x)\n    expected = Matrix([0, 0])\n    assert result == expected\n\n    # Test case where the matrix is the result of a trigonometric function\n    eq = sin(A + B)\n    result = diff(eq, x)\n    expected = cos(A + B) * diff(A + B, x)\n    assert result == expected\n\n    # Test case where the matrix is the result of a sum\n    eq = A + B\n    result = diff(eq, x)\n    expected = Matrix([0, 0])\n    assert result == expected\n", "def test_matrix_symbol_substitution():\n    from sympy import sin, cos, tan, sqrt, cbrt, exp\n    from sympy import Symbol\n    x, y = symbols('x y')\n\n    # Test that a MatrixSymbol can be substituted by a matrix\n    A = MatrixSymbol('A', 3, 3)\n    A_sub = A.subs(A, sin(x)*Identity(3) + cos(x)*eye(3))\n    assert A_sub.equals(sin(x)*eye(3) + cos(x)*eye(3))\n\n    # Test that a MatrixSymbol cannot be substituted by a non-matrix expression\n    B = MatrixSymbol('B', 3, 3)\n    with raises(SympifyError):\n        B.subs(B, x)\n\n    # Test that a MatrixSymbol can be substituted by a matrix expression\n    C = MatrixSymbol('C', 3, 3)\n    C_sub = C.subs(C, sin(x)*Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    assert C_sub.equals(sin(x)*Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n\n    # Test that a MatrixSymbol cannot be substituted by a non-matrix expression\n    D = MatrixSymbol('D', 3, 3)\n    with raises(SympifyError):\n        D.subs(D, x)\n\n    # Test that a MatrixSymbol can be substituted by a matrix expression with KroneckerDelta\n    E = MatrixSymbol('E', 3, 3)\n    E_sub = E.subs(E, sin(x)*KroneckerDelta(x, y)*eye(3) + cos(x)*eye(3))\n    assert E_sub.equals(sin(x)*KroneckerDelta(x, y)*eye(3) + cos(x)*eye(3))\n\n    # Test that a MatrixSymbol cannot be substituted by a non-matrix expression with KroneckerDelta\n    F = MatrixSymbol('F', 3, 3)\n    with raises(SympifyError):\n        F.subs(F, x)\n\n    # Test that a MatrixSymbol can be substituted by a matrix expression with MatMul\n    G = MatrixSymbol('G', 3, 3)\n    G_sub = G.subs(G, sin(x)*"], "sample_802": ["def test_feature_union_multiple_transformers_with_n_jobs():\n    from sklearn.datasets import load_iris\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.decomposition import PCA\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.pipeline import FeatureUnion\n\n    X, y = load_iris(return_X_y=True)\n    scaler = StandardScaler()\n    pca = PCA(n_components=2)\n    feat_union = FeatureUnion([(scaler.name, scaler), (pca.name, pca)])\n\n    feat_union.fit(X, y)\n    X_t = feat_union.transform(X)\n\n    # Test with 2 threads\n    feat_union_n_jobs = FeatureUnion([(scaler.name, scaler), (pca.name, pca)],\n                                    n_jobs=2)\n    X_t_n_jobs = feat_union_n_jobs.transform(X)\n\n    assert_array_equal(X_t, X_t_n_jobs)\n\n    # Test with 4 threads\n    feat_union_n_jobs = FeatureUnion([(scaler.name, scaler), (pca.name, pca)],\n                                    n_jobs=4)\n    X_t_n_jobs = feat_union_n_jobs.transform(X)\n\n    assert_array_equal(X_t, X_t_n_jobs)\n\n    # Test with 1 thread (default)\n    feat_union_n_jobs = FeatureUnion([(scaler.name, scaler), (pca.name, pca)],\n                                    n_jobs=1)\n    X_t_n_jobs = feat_union_n_jobs.transform(X)\n\n    assert_array_equal(X_t, X_t_n_jobs)\n", "def test_pipeline_fit_predict_with_invalid_final_estimator():\n    \"\"\"Test pipeline with an estimator that does not have fit_predict\"\"\"\n    X = np.random.rand(10, 5)\n    y = np.random.randint(0, 2, 10)\n\n    # Test with a pipeline that ends with an estimator that does not have fit_predict\n    pipeline = make_pipeline(SelectKBest(f_classif, k=1), LogisticRegression())\n    assert_raise_message(\n        TypeError,\n        \"Last step of Pipeline should implement fit_predict. 'None' (type \"\n        \"<class 'NoneType'>) doesn't\",\n        pipeline.fit_predict, X, y)\n\n    # Test with a pipeline that ends with a string 'passthrough'\n    pipeline = make_pipeline(SelectKBest(f_classif, k=1), 'passthrough')\n    assert_raise_message(\n        TypeError,\n        \"Last step of Pipeline should implement fit_predict. 'passthrough' \"\n        \"(type <class 'str'>) doesn't\",\n        pipeline.fit_predict, X, y)\n\n    # Test with a pipeline that ends with an estimator that does not have fit_predict\n    pipeline = make_pipeline(SelectKBest(f_classif, k=1), DummyTransf())\n    assert_raise_message(\n        TypeError,\n        \"Last step of Pipeline should implement fit_predict. 'None' (type \"\n        \"<class 'DummyTransf'>) doesn't\",\n        pipeline.fit_predict, X, y)\n", "def test_inverse_transform():\n    \"\"\"Test inverse_transform method.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    X_transformed = Pipeline([('step1', DummyTransf())]).fit_transform(X)\n    assert_array_equal(Pipeline([('step1', DummyTransf())]).inverse_transform(X_transformed), X)\n", "def test_Pipeline_backward_compatibility():\n    # Check that a pipeline can be cloned and fitted\n    pipeline = Pipeline([('estimator', DummyEstimatorParams())])\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.named_steps == pipeline.named_steps\n\n    # Check that cloning a pipeline with a joblib.Memory object doesn't raise\n    # an error\n    memory = Memory(location=mkdtemp())\n    pipeline = Pipeline([('estimator', DummyEstimatorParams())], memory=memory)\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.memory.location is None\n\n    # Check that a pipeline with no estimators is cloned correctly\n    pipeline = Pipeline()\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.steps == []\n\n    # Check that a pipeline with a list of estimators is cloned correctly\n    pipeline = Pipeline([('estimator1', DummyEstimatorParams()), ('estimator2', DummyEstimatorParams())])\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.steps == [('estimator1', pipeline.steps[0][1]), ('estimator2', pipeline.steps[1][1])]\n\n    # Check that a pipeline with a joblib.Memory object is cloned correctly\n    memory = Memory(location=mkdtemp())\n    pipeline = Pipeline([('estimator1', DummyEstimatorParams()), ('estimator2', DummyEstimatorParams())], memory=memory)\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.memory.location is None\n\n    # Check that a pipeline with a list of estimators and a joblib.Memory object is cloned correctly\n    pipeline = Pipeline([('estimator1', DummyEstimatorParams()), ('estimator2', DummyEstimatorParams())], memory=memory)\n    pipeline_clone = clone(pipeline)\n    assert pipeline_clone.memory.location is None\n\n    # Check that a pipeline with a estimator that raises an exception during fit is cloned correctly\n    class ExceptionRaiser(BaseEstimator):\n            raise Exception('Test exception')\n\n    pipeline = Pipeline([('estimator1', ExceptionRaiser())])\n    pipeline_clone = clone(pipeline)\n    assert isinstance(pipeline_clone.steps[0][1], ExceptionRaiser)\n", "    def test_fit_predict_binary_classifiers(self):\n        \"\"\"Test binary classifiers with multiclass Pipeline\"\"\"\n        X, y = load_iris(return_X_y=True)\n        y = np.where(y == 0, 1, y)\n        y = np.where(y == 1, 2, y)\n\n        # Fit a pipeline with a binary classifier\n        pipeline = make_pipeline(SVC(probability=True, random_state=42), SVC(probability=True, random_state=42))\n        pipeline.fit(X, y)\n\n        # Test that fit_predict raises a ValueError\n        with assert_raises(ValueError):\n            pipeline.fit_predict(X, y)\n"], "sample_1089": ["def test_factor_terms_lcm():\n    from sympy import lcm\n    A, B, C = symbols('A B C', commutative=False)\n    expr = lcm(A + B, C + A)\n    expected = lcm(A + B, C + A)\n    assert factor_terms(expr) == expected\n", "def test_decompose_power():\n    assert decompose_power(S(4)) == (4, 1)\n    assert decompose_power(S(2)**3) == (2, 3)\n    assert decompose_power(x) == (x, 1)\n    assert decompose_power(x**2) == (x, 2)\n    assert decompose_power(x**2*y) == (x**y, 2)\n    assert decompose_power(x**(2*y/3)) == (x**(y/3), 2)\n\n    assert decompose_power(S(-4)) == (4, -1)\n    assert decompose_power(-x) == (-x, 1)\n    assert decompose_power(-x**2) == (-x, 2)\n    assert decompose_power(-x**2*y) == (-x**y, 2)\n    assert decompose_power(-x**(2*y/3)) == (-x**(y/3), 2)\n\n    assert decompose_power(S(4).pow(3)) == (4, 3)\n    assert decompose_power(x.pow(2)) == (x, 2)\n    assert decompose_power(x.pow(2)*y) == (x**y, 2)\n    assert decompose_power(x.pow(2*y/3)) == (x**(y/3), 2)\n\n    assert decompose_power(S(-4).pow(3)) == (4, -3)\n    assert decompose_power(-x.pow(2)) == (-x, 2)\n    assert decompose_power(-x.pow(2)*y) == (-x**y, 2)\n    assert decompose_power(-x.pow(2*y/3)) == (-x**(y/3), 2)\n", "def test_factor_nc_with_multiple_nc_objects_and_nc_symbols():\n    from sympy.abc import x, y, z\n    A, B, C = symbols('A,B,C', commutative=False)\n\n    expr = (x + A)*(x + B)*(x + C)\n    expected = (x + A)*(x + B)*(x + C)\n    assert factor_nc(expr) == expected\n\n    expr = A**2 * B**2 + A**2 * C**2 + B**2 * C**2\n    expected = (A**2 + B**2 + C**2)*(A**2 + B**2 + C**2)\n    assert factor_nc(expr) == expected\n\n    expr = (x + A)**2 + (x + B)**2 + (x + C)**2\n    expected = (x + A + B + C)**2 + (x + A - B - C)**2 + (x - A + B - C)**2\n    assert factor_nc(expr) == expected\n\n    expr = A**2 + B**2 + C**2\n    expected = A**2 + B**2 + C**2\n    assert factor_nc(expr) == expected\n\n    expr = A**2 + B**2 + A*B*C\n    expected = A**2 + B**2 + A*B*C\n    assert factor_nc(expr) == expected\n\n    expr = (A + B)**2 + (A + C)**2\n    expected = (A + B + C)**2 + (A - B + C)**2\n    assert factor_nc(expr) == expected\n\n    expr = A**2 + B**2 + C**2 + A*B*C\n    expected = A**2 + B**2 + C**2 + A*B*C\n    assert factor_nc(expr) == expected\n", "def test_mask_nc():\n    from sympy import Symbol\n    from sympy.core.exprtools import _mask_nc\n    from sympy.abc import x, y\n\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n\n    eq = A**2 - x**2 - y**2\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (y**2 - x**2)\n    assert nc_symbols == [A]\n\n    eq = x*A + y*B\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (y*B + x*A)\n    assert nc_symbols == [A, B]\n\n    eq = A**2 + B**2\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A**2 + B**2)\n    assert nc_symbols == [A, B]\n\n    eq = A*B + B*A\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (2*B*A)\n    assert nc_symbols == [A, B]\n\n    eq = A*B + A - B\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A*(B + 1) - B)\n    assert nc_symbols == [A, B]\n\n    eq = A*B + A - B + A\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A*(B + 2) - B)\n    assert nc_symbols == [A, B]\n\n    eq = A**2 + 2*A + 1\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A**2 + 2*A + 1)\n    assert nc_symbols == []\n\n    eq = A**2 - 2*A - 1\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A**2 - 2*A - 1)\n    assert nc_symbols == []\n\n    eq = A**2 + B**2\n    expr, rep, nc_symbols = _mask_nc(eq)\n    assert expr == (A**2 + B**2)\n    assert nc_symbols == [A, B]\n", "def test_mask_nc_replaces_No_with_Dummy():\n    from sympy import NO, CreateFermion, AnnihilateFermion\n    NO = NO\n    CreateFermion = CreateFermion\n    AnnihilateFermion = AnnihilateFermion\n\n    eq = NO(CreateFermion(x)*AnnihilateFermion(y))\n    expr, rep, _ = _mask_nc(eq)\n    assert expr == CreateFermion(_d0)*AnnihilateFermion(_d1)\n    assert rep == {_d0: NO, _d1: CreateFermion(x)*AnnihilateFermion(y)}\n    assert _mask_nc(eq, 'd') == (CreateFermion(_d0)*AnnihilateFermion(_d1),\n                                 {_d0: NO, _d1: CreateFermion(x)*AnnihilateFermion(y)}, [_d0, _d1])\n"], "sample_647": ["def test_warn_explicit_for():\n        pass\n\n    pytest.warn_explicit_for(test_func, PytestConfigWarning())\n\n    # Check if the warning was emitted correctly\n    with pytest.warns(PytestConfigWarning):\n        test_func()\n", "def test_warn_explicit_for(capsys):\n    # Test warn_explicit_for function with different types of messages\n        pass\n\n    warn_explicit_for(test_function, PytestWarning(\"Test message\"))\n    captured = capsys.readouterr()\n    assert \"Test message\" in captured.out\n\n    # Test warn_explicit_for function with a warning that has a format string\n        pass\n\n    warn_explicit_for(\n        test_function,\n        UnformattedWarning(\n            category=PytestWarning, template=\"Test message with param: {param}\"\n        ),\n    )\n    captured = capsys.readouterr()\n    assert \"Test message with param:\" in captured.out\n\n    # Test warn_explicit_for function with a warning that has a format string and keyword arguments\n        pass\n\n    warn_explicit_for(\n        test_function,\n        UnformattedWarning(\n            category=PytestWarning,\n            template=\"Test message with param: {param}\",\n        ),\n    )\n    captured = capsys.readouterr()\n    assert \"Test message with param:\" in captured.out\n\n    # Test warn_explicit_for function with a deprecated warning\n        pass\n\n    warn_explicit_for(test_function, PytestDeprecationWarning(\"Test message\"))\n    captured = capsys.readouterr()\n    assert \"Test message\" in captured.out\n\n    # Test warn_explicit_for function with an experimental warning\n        pass\n\n    warn_explicit_for(test_function, PytestExperimentalApiWarning(\"Test message\"))\n    captured = capsys.readouterr()\n    assert \"Test message\" in captured.out\n", "def test_warn_explicit_for():\n    \"\"\"Test warn_explicit_for function with different types of warnings.\"\"\"\n    # Test with a custom warning\n    warning_class = PytestWarning\n    message = \"Custom warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Custom warning message\"):\n        pass\n\n    # Test with a deprecation warning\n    warning_class = PytestDeprecationWarning\n    message = \"Deprecation warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Deprecation warning message\"):\n        pass\n\n    # Test with an experimental api warning\n    warning_class = PytestExperimentalApiWarning\n    message = \"Experimental API warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Experimental API warning message\"):\n        pass\n\n    # Test with an unhandled coroutine warning\n    warning_class = PytestUnhandledCoroutineWarning\n    message = \"Unhandled coroutine warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Unhandled coroutine warning message\"):\n        pass\n\n    # Test with a filename and lineno\n    warning_class = PytestWarning\n    message = \"Custom warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Custom warning message\"):\n        pass\n\n    # Test with module and registry\n    warning_class = PytestWarning\n    message = \"Custom warning message\"\n    method = type(\"TestMethod\", (), {})\n    warn_explicit_for(method.__init__, message)\n    with pytest.warns(warning_class, match=\"Custom warning message\"):\n        pass\n", "def test_warn_explicit_for():\n    import warnings\n\n        pass\n\n    warning = PytestWarning(\"Test warning\")\n    warn_explicit_for(test_func, warning)\n\n    with pytest.warns(warning):\n        warnings.warn_explicit_for = None  # Simulate warning being emitted\n        test_func()\n", "def test_warning_class_is_subclass_of_warning(warning_class: Type[Warning]) -> None:\n    assert issubclass(warning_class, Warning)\n\n"], "sample_359": ["    def test_create_index(self):\n        with CaptureQueriesContext() as capture_queries:\n            with self.assertNumQueries(1):\n                self.apply_operation(AddIndex('unicode_model', models.Index(fields=['field1', 'field2'], name='test_index')))\n\n            self.assertEqual(capture_queries.captured_queries, [])\n\n            self.assertModelExists('unicode_model')\n            self.assertModelIndexExists('unicode_model', 'test_index')\n\n            self.apply_operation(RemoveIndex('unicode_model', 'test_index'))\n\n            self.assertModelExists('unicode_model')\n            self.assertNotModelIndexExists('unicode_model', 'test_index')\n", "    def test_model_operation_references_model(self):\n        with self.assertRaises(ValueError):\n            CreateModel('test', [('test_field', models.IntegerField())]).references_model('Test', 'testapp')\n        with self.assertRaises(ValueError):\n            CreateModel('test', [('test_field', models.IntegerField())]).references_model('OtherModel', 'testapp')\n\n        model_operation = CreateModel('Test', [('test_field', models.IntegerField())])\n        self.assertTrue(model_operation.references_model('Test', 'testapp'))\n        self.assertFalse(model_operation.references_model('OtherModel', 'testapp'))\n\n        model_operation = RenameModel('OldTest', 'NewTest')\n        self.assertTrue(model_operation.references_model('OldTest', 'testapp'))\n        self.assertTrue(model_operation.references_model('NewTest', 'testapp'))\n", "    def test_alter_model_managers(self):\n        Migration = migrations.Migration\n        new_manager = models.Manager()\n        self.assertApply(\n            [\n                migrations.RunPython(\n                    lambda apps, schema_editor: [\n                        UnicodeModel.objects.create(\n                            name='Test Name',\n                            description='Test description',\n                            is_active=True\n                        )\n                    ],\n                    lambda apps, schema_editor: [\n                        unicode_model := UnicodeModel.objects.get(name='Test Name')\n                        unicode_model.delete()\n                    ],\n                ),\n                AlterModelManagers('unicode_model', [\n                    ('objects', new_manager),\n                    ('inactive', FoodManager()),\n                ]),\n            ],\n            operations=[\n                CreateModel(\n                    name='unicode_model',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=255)),\n                        ('description', models.TextField()),\n                        ('is_active', models.BooleanField(default=True)),\n                    ],\n                    options={\n                        'abstract': False,\n                        'db_table': 'django_test_unicode_model',\n                        'default_permissions': [],\n                        'managed': True,\n                        'ordering': [],\n                        'permissions': [],\n                        'verbose_name': 'Unicode Model',\n                        'verbose_name_plural': 'Unicode Models',\n                    },\n                    bases=(models.Model,),\n                    managers=[\n                        ('objects', FoodQuerySet.__class__()),\n                        ('inactive', FoodManager()),\n                    ],\n                ),\n                AlterModelManagers('unicode_model', [\n                    ('objects', new_manager),\n                    ('inactive', FoodManager()),\n                ]),\n            ],\n            expected_state={\n                'models': {\n                    'unicode_model': {\n                        'fields': [\n                            ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                            ('name', models.CharField(max_length=255)),\n                            ('description', models.TextField()),\n                            ('is_active', models.BooleanField(default=True)),\n                        ],\n                        'options': {\n                            'abstract': False,\n                            'db_table': 'django_test_unicode_model',\n                            'default_permissions': [],\n                            'managed': True,\n                            'ordering': [],\n                            'permissions': [],\n                            'verbose_name': 'Unicode Model',\n                            'verbose_name_plural': 'Unicode Models',\n                        },\n                        'managers': [\n                            ('objects', new_manager),\n                            ('inactive', FoodManager()),\n                        ],\n                    },\n                },\n                'operations': [\n                    CreateModel(\n                        name='unicode_model", "    def test_alter_options_with_default_manager(self):\n        with CaptureQueriesContext() as queries:\n            with transaction.atomic():\n                with self.assertRaises(IntegrityError):\n                    Migration.from_state(ProjectState({'app': [ModelState(\n                        'app', 'model', [(b'manager1', models.Manager()), (b'manager2', models.Manager())],\n                        {'default_manager': 'manager1', 'base_manager': 'manager2'}\n                    )]})).apply.ProjectState({'app': [ModelState(\n                        'app', 'model', [(b'manager1', models.Manager()), (b'manager2', models.Manager())],\n                        {'default_manager': 'manager3', 'base_manager': 'manager2'}\n                    )]}))\n            self.assertEqual(queries.captured.queries[:3], [\n                {\n                    'sql': \"SELECT COUNT(*) FROM django_migrations\",\n                    'params': (),\n                    'kind': 'select',\n                    'using': 'default',\n                },\n                {\n                    'sql': \"SELECT COUNT(*) FROM django_migrations\",\n                    'params': (),\n                    'kind': 'select',\n                    'using': 'default',\n                },\n                {\n                    'sql': \"INSERT INTO app_model (model, migration) VALUES (%s, %s)\",\n                    'params': (b'model', b'migration'),\n                    'kind': 'insert',\n                    'using': 'default',\n                },\n            ])\n            self.assertEqual(queries.captured.queries[3:], [\n                {\n                    'sql': \"INSERT INTO app_model_options (model, option, option_value) VALUES (%s, %s, %s)\",\n                    'params': (b'model', b'default_manager', b'manager3'),\n                    'kind': 'insert',\n                    'using': 'default',\n                },\n                {\n                    'sql': \"UPDATE app_model_options SET option_value = %s WHERE option = %s AND model = %s\",\n                    'params': (None, b'base_manager', b'model'),\n                    'kind': 'update',\n                    'using': 'default',\n                },\n                {\n                    'sql': \"UPDATE app_model_options SET option_value = %s WHERE option = %s AND model = %s\",\n                    'params': (None, b'default_manager', b'model'),\n                    'kind': 'update',\n                    'using': 'default',\n                },\n            ])\n", "    def test_renames_model(self):\n        # Create the models\n        with self.settings(MODELS_str=(\n            \"from django.db import models\\n\"\n            \"class Model1(models.Model):\\n\"\n            \"    pass\\n\"\n            \"class Model2(models.Model):\\n\"\n            \"    class Meta:\\n\"\n            \"        proxy = True\\n\"\n            \"        verbose_name = 'Proxy model'\\n\"\n            \"    pass\\n\"\n            \"class Model3(models.Model):\\n\"\n            \"    pass\"\n        )):\n            with transaction.atomic():\n                self.apply_operation(RenameModel('model2', 'new_model2'))\n        self.assertNotIn('model2', self.project_state.models)\n        self.assertIn('new_model2', self.project_state.models)\n\n        # Make sure the table was renamed\n        self.assertTableRenamed(\n            'django.test.models', 'new_model2', 'new_model2',\n            'django.test.models', 'model2', 'model2'\n        )\n\n        # Test that the proxy relation is still there\n        self.assertProxyRelation('django.test.models', 'new_model2', 'Model1')\n\n        # Make sure the new table has the same relations as the old one\n        self.assertModelsHaveSameRelations(\n            'django.test.models', 'new_model2', 'django.test.models', 'model2'\n        )\n\n        # Make sure the new model has the same managers as the old one\n        self.assertModelsHaveSameManagers(\n            'django.test.models', 'new_model2', 'django.test.models', 'model2'\n        )\n\n        # Make sure the new model has the same fields as the old one\n        self.assertModelsHaveSameFields(\n            'django.test.models', 'new_model2', 'django.test.models', 'model2'\n        )\n"], "sample_14": ["def test_latitude_construction_from_longitude():\n    # Test that an attempt to create a Latitude from a Longitude raises a TypeError\n    with pytest.raises(TypeError):\n        Latitude(Longitude(90 * u.deg))\n\n    # Test that an attempt to create a Latitude from a Longitude with a valid Longitude\n    # raises a TypeError\n    with pytest.raises(TypeError):\n        Latitude(Longitude(180 * u.deg))\n\n    # Test that a Latitude can be created from a valid Latitude\n    lat = Latitude(45 * u.deg)\n    assert isinstance(lat, Latitude)\n\n    # Test that a Latitude can be created from a valid string\n    lat = Latitude('45d')\n    assert isinstance(lat, Latitude)\n\n    # Test that a Latitude can be created from a valid Quantity\n    lat = Latitude(45 * u.deg)\n    assert isinstance(lat, Latitude)\n\n    # Test that a Latitude can be created from an array of valid Latitudes\n    lat = Latitude([45 * u.deg, 90 * u.deg])\n    assert isinstance(lat, Latitude)\n\n    # Test that a Latitude can be created from an array of valid strings\n    lat = Latitude(['45d', '90d'])\n    assert isinstance(lat, Latitude)\n", "    def test_wrap_at_wrap_angle(self):\n        a = Angle([350, 10, 20] * u.deg)\n        expected = [10, 10, 20]\n        result = a.wrap_at('0d')\n        assert_array_equal(result.value, expected)\n", "def test_latitude_scalar():\n    \"\"\"Test the initialization of a Latitude with a scalar value.\"\"\"\n    lat = Latitude(45 * u.deg)\n    assert_allclose(lat.value, 45)\n    assert lat.unit == u.deg\n    assert isinstance(lat, Latitude)\n", "def test_Longitude_wrap_at():\n    a = Longitude([300, 400, 500] * u.deg)\n    a.wrap_at(180 * u.deg)\n    assert_array_equal(a.value, np.array([60, 160, 260]) * u.deg)\n    assert_array_equal(a.unit, u.deg)\n    a = Longitude([300, 400, 500] * u.deg)\n    a.wrap_at(180 * u.deg, inplace=True)\n    assert_array_equal(a.value, np.array([60, 160, 260]) * u.deg)\n    assert_array_equal(a.unit, u.deg)\n    a = Longitude([300, 400, 500] * u.deg)\n    a.wrap_angle = 180 * u.deg\n    a._wrap_at(180 * u.deg)\n    assert_array_equal(a.value, np.array([60, 160, 260]) * u.deg)\n    assert_array_equal(a.unit, u.deg)\n", "def test_Latitude__setitem__raises_type_error():\n    lat = Latitude([-30, 30] * u.deg)\n    with pytest.raises(TypeError):\n        lat['angle'] = Longitude(180 * u.deg)\n"], "sample_465": ["    def test_get_list_display(self):\n        model_admin = ModelAdmin(Band, self.site)\n        self.assertEqual(model_admin.get_list_display(MockRequest()), ('__str__',))\n", "    def test_modelform_defines_fields(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = [\"name\", \"sign_date\"]\n\n        form = MyModelForm()\n        self.assertTrue(modelform_defines_fields(form))\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(\n            name=\"The Doors\",\n            bio=\"\",\n            sign_date=date(1965, 1, 1),\n        )\n        cls.song = Song.objects.create(\n            title=\"Light My Fire\",\n            album=\"The Doors\",\n            release_date=date(1967, 5, 8),\n            band=cls.band,\n        )\n        cls.concert = Concert.objects.create(\n            name=\"The Doors at the Hollywood Bowl\",\n            date=date(1968, 9, 14),\n        )\n        cls.concert.song = [cls.song]\n        cls.concert.save()\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(\n            name=\"The Doors\",\n            bio=\"\",\n            sign_date=date(1965, 1, 1),\n        )\n        cls.user = User.objects.create(\n            username=\"john\",\n            password=\"password123\",\n            email=\"john@example.com\",\n            is_staff=True,\n            is_superuser=True,\n        )\n", "    def setUpTestData(cls):\n        cls.band = Band.objects.create(\n            name=\"The Doors\",\n            bio=\"\",\n            sign_date=date(1965, 1, 1),\n        )\n        cls.song = Song.objects.create(\n            title=\"Light My Fire\",\n            duration=5,\n            band=cls.band,\n        )\n        cls.concert = Concert.objects.create(\n            title=\"The Doors concert\",\n            date=date(1967, 3, 11),\n            band=cls.band,\n        )\n        cls.inline = TabularInline(\n            Song,\n            cls.site,\n            fk_name=\"band\",\n            extra=2,\n            can_delete=True,\n            classes=\"collapse\",\n        )\n"], "sample_273": ["    def test_check_default_primary_key(self):\n        with modify_settings(DEFAULT_AUTO_FIELD={'value': 'django.db.models.AutoField'}):\n            with self.assertRaisesRegex(\n                Error,\n                r\"Auto-created primary key used when not defining a primary key type, by default 'django\\.db\\.models\\.AutoField'\"\n            ):\n                class Model(models.Model):\n                    pass\n", "    def test_save_base_raw(self):\n        # Arrange\n        class Model(models.Model):\n            class Meta:\n                db_table = 'test_save_base_raw'\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        instance = Model.objects.create(field1='value1', field2='value2')\n\n        # Act\n        instance.save(raw=True)\n\n        # Assert\n        self.assertEqual(Model.objects.get(field1='value1', field2='value2').field1, 'value1')\n        self.assertEqual(Model.objects.get(field1='value1', field2='value2').field2, 'value2')\n        self.assertEqual(instance.field1, 'value1')\n        self.assertEqual(instance.field2, 'value2')\n", "    def test_model_unpickle(self):\n        class TestModel(models.Model):\n            pass\n\n        model_id = TestModel\n\n        unpickled_model = model_unpickle(model_id)\n        self.assertEqual(unpickled_model, TestModel)\n        self.assertIsNotNone(unpickled_model.__dict__)\n        self.assertIsInstance(unpickled_model, type(TestModel))\n", "    def test_auto_field_auto_created(self):\n        class MyModel(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        errors = MyModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(\n            errors[0].hint,\n            \"Configure the DEFAULT_AUTO_FIELD setting or the \"\n            \"django.db.backends.sqlite3.DatabaseWrapper.default_auto_field attribute \"\n            \"to point to a subclass of AutoField, e.g. 'django.db.models.BigAutoField'.\"\n        )\n", "    def test_model_custom_ordering(self):\n        class MyModel(models.Model):\n            class Meta:\n                ordering = ['-id', 'name']\n\n        my_model = MyModel.objects.create(id=3, name='c')\n        MyModel.objects.create(id=2, name='b')\n        MyModel.objects.create(id=1, name='a')\n\n        my_model.refresh_from_db()\n\n        self.assertEqual(my_model.get_next_in_order(), MyModel.objects.get(id=2))\n        self.assertEqual(my_model.get_previous_in_order(), MyModel.objects.get(id=4))\n"], "sample_1050": ["def test_print_MatrixBase():\n    M = MatrixSymbol('M', 3, 3)\n    printer = PythonCodePrinter()\n    assert printer._print_MatrixBase(M) == 'numpy.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])'\n", "def test_print_Relational():\n    assert MpmathPrinter()._print_Relational(Eq(x, 0)) == \"mpmath.eq(x, 0)\"\n    assert MpmathPrinter()._print_Relational(Le(x, 0)) == \"mpmath.le(x, 0)\"\n    assert MpmathPrinter()._print_Relational(Gt(x, 0)) == \"mpmath.gt(x, 0)\"\n", "def test_PythonCodePrinter_BinaryOp_Fold():\n    from sympy import symbols, sin, cos\n    x = symbols('x')\n    expr = sin(x + x)\n    expected = '((math.sin(x) + math.sin(x))'\n    assert pycode(expr) == expected\n", "def test_pymath_printer():\n    from sympy.printing.pycode import pycode\n    from sympy import exp, log, log2, log1p\n\n    expr = log(exp(x))\n    result = pycode(expr, fully_qualified_modules=False)\n    assert result == \"math.log(math.exp(x))\"\n\n    expr = log2(exp(x))\n    result = pycode(expr, fully_qualified_modules=False)\n    assert result == \"math.log(2)*math.log(math.exp(x))\"\n\n    expr = log1p(exp(x))\n    result = pycode(expr, fully_qualified_modules=False)\n    assert result == \"math.log(1 + math.exp(x))\"\n", "def test_printing_numpy_select():\n    assert str(NumPyPrinter().doprint(Piecewise((1, z>0), (2, True))) == \"numpy.select([z>0, True], [1, 2], default=numpy.nan)\")\n    assert str(NumPyPrinter().doprint(Piecewise((1, z>0), (2, z>1), (3, True))) == \"numpy.select([z>0, z>1, True], [1, 2, 3], default=numpy.nan)\")\n\n"], "sample_793": ["def test_average_path_length():\n    # Test that average_path_length returns correct values\n    assert_allclose(_average_path_length([1]), [0.])\n    assert_allclose(_average_path_length([2]), [1.0])\n    assert_allclose(_average_path_length([3]), [1.4427])\n\n    # Test that average_path_length raises an error for negative values\n    with pytest.raises(ValueError):\n        _average_path_length([-1])\n\n    # Test that average_path_length raises an error for zero values\n    with pytest.raises(ValueError):\n        _average_path_length([0])\n\n    # Test that average_path_length raises an error for non-integer values\n    with pytest.raises(TypeError):\n        _average_path_length([1.5])\n\n    # Test that average_path_length returns the correct shape\n    assert _average_path_length([1, 2, 3]).shape == (3,)\n\n    # Test that average_path_length returns the correct values for multiple leaves\n    assert_allclose(_average_path_length([1, 2, 3]), [0., 1.0, 1.4427])\n\n    # Test that average_path_length returns the correct values for a large number of leaves\n    assert_allclose(_average_path_length(np.random.randint(1, 100, size=100)), np.random.uniform(0, 2, size=100))\n", "def test_fit_predict_isolation_forest_max_samples_auto():\n    # Test that IsolationForest with max_samples='auto' does not raise an exception\n    X = np.random.rand(100, 10)\n    isof = IsolationForest(max_samples='auto')\n    isof.fit(X)\n    isof.predict(X)\n\n    # Test that IsolationForest with max_samples='auto' does not raise an exception\n    # when n_samples > 256\n    X = np.random.rand(257, 10)\n    isof = IsolationForest(max_samples='auto')\n    isof.fit(X)\n    isof.predict(X)\n\n    # Test that IsolationForest with max_samples='auto' does not raise an exception\n    # when n_samples <= 256\n    X = np.random.rand(256, 10)\n    isof = IsolationForest(max_samples='auto')\n    isof.fit(X)\n    isof.predict(X)\n", "def test_isolation_forest_behaviour_consistency_behaviour_new(behaviour):\n    \"\"\"Test that decision_function and score_samples are consistent when\n    behaviour='new'.\n    \"\"\"\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n    clf = IsolationForest(n_estimators=10, behaviour=behaviour, random_state=42)\n    clf.fit(X, y)\n    check_is_fitted(clf, ['estimators_'])\n    decision_function = clf.decision_function(X)\n    score_samples = clf.score_samples(X)\n    assert_allclose(decision_function, score_samples)\n", "def test_isolation_forest_max_samples_warning():\n    \"\"\"Test the Isolation Forest with max_samples > n_samples.\n\n    Check that a warning is emitted when max_samples is set to a value larger\n    than the total number of samples.\n    \"\"\"\n    X = np.random.rand(100, 10)\n    iforest = IsolationForest(max_samples=150, random_state=0)\n    with pytest.warns(UserWarning) as record:\n        iforest.fit(X)\n        assert len(record) == 1\n        assert \"max_samples (%s) is greater than the \" % iforest.max_samples_ in record[0].message.args[0]\n", "def test_behaviour_param():\n    \"\"\"Test the behaviour of IsolationForest with respect to the 'behaviour' parameter.\n\n    It checks that:\n    - 'behaviour=\"new\"' is the default behaviour and its threshold is 0.\n    - 'behaviour=\"old\"' is deprecated and its threshold is deprecated.\n    - 'behaviour' is not a valid parameter.\n\n    \"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n\n    # Test that 'behaviour=\"new\"' is the default behaviour and its threshold is 0\n    iforest = IsolationForest()\n    assert iforest.behaviour == 'new'\n    assert iforest.threshold_ == 0\n\n    # Test that 'behaviour=\"old\"' is deprecated and its threshold is deprecated\n    with assert_warns_regex(DeprecationWarning, message=\"behaviour='old' is deprecated\"):\n        iforest = IsolationForest(behaviour='old')\n    assert iforest.behaviour == 'old'\n\n    # Test that 'behaviour' is not a valid parameter\n    with assert_raises_regex(ValueError, \"behaviour is not a valid parameter\"):\n        iforest = IsolationForest(behaviour='invalid')\n"], "sample_52": ["    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n", "    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n        cls.a1 = Author.objects.create(first_name='Author', last_name='One')\n        cls.a2 = Author.objects.create(first_name='Author', last_name='Two')\n", "    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n\n        cls.af1 = Author.objects.create(name='Author 1', email='author1@example.com')\n        cls.af2 = Author.objects.create(name='Author 2', email='author2@example.com')\n        cls.af3 = Author.objects.create(name='Author 3', email='author3@example.com')\n\n        cls.b1 = Book.objects.create(title='Book 1', category=cls.c1, author=cls.af1)\n        cls.b2 = Book.objects.create(title='Book 2', category=cls.c2, author=cls.af2)\n        cls.b3 = Book.objects.create(title='Book 3', category=cls.c3, author=cls.af3)\n", "    def test_model_choice_field_invalid_choice(self):\n        form = forms.Form({\n            'category': 'non-existent-category',\n        })\n        field = forms.ModelChoiceField(Category.objects.all(), required=False)\n        with self.assertRaises(ValidationError):\n            field.clean('non-existent-category')\n", "    def test_modelchoicefield_valid_choice(self):\n        article = Article.objects.create(\n            title='Test',\n            category=self.c1,\n            author=Author.objects.create(first_name='Test', last_name='User'),\n            writer=Writer.objects.create(first_name='Test', last_name='Writer'),\n            published_date=datetime.date.today()\n        )\n        field = ModelChoiceField(Category.objects.all())\n        self.assertEqual(field.to_python(field.prepare_value(article.category)), article.category)\n        self.assertEqual(field.to_python(article.category.pk), article.category)\n"], "sample_726": ["    def test_fit_transform_output_matches_transform(self):\n        lb = LabelBinarizer()\n        lb.fit_transform([1, 2, 3, 4, 5])\n        assert_array_equal(lb.transform([1, 2, 3, 4, 5]), lb.classes_)\n", "def test_MultiLabelBinarizer_fit_transform():\n    y = [(1, 2), (3,)]\n    mlb = MultiLabelBinarizer(sparse_output=False)\n    result = mlb.fit_transform(y)\n    expected = np.array([[1, 1, 0],\n                        [0, 0, 1]])\n    assert_array_equal(toarray(result), toarray(expected))\n", "def test_LabelBinarizer_fit_transform():\n    lb = LabelBinarizer(pos_label=2, neg_label=1)\n    y = np.array([1, 2, 1, 2, 0])\n    expected = np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1],\n                         [1, 0, 0]])\n    assert_array_equal(lb.fit_transform(y), expected)\n    assert_array_equal(lb.classes_, np.array([0, 1, 2]))\n", "def test_LabelEncoder_unique_duplicate_labels():\n    le = LabelEncoder()\n    y = [1, 2, 3, 2, 4]\n    le.fit(y)\n    y_expected = np.array([0, 1, 2, 1, 3])\n    assert_array_equal(le.transform(y), y_expected)\n\n    # Test LabelEncoder with non-numeric labels\n    le = LabelEncoder()\n    y = ['a', 'b', 'c', 'b', 'd']\n    le.fit(y)\n    y_expected = np.array([0, 1, 2, 1, 3])\n    assert_array_equal(le.transform(y), y_expected)\n\n    # Test LabelEncoder with empty input\n    le = LabelEncoder()\n    y = []\n    assert_array_equal(le.transform(y), [])\n\n    # Test LabelEncoder with non-hashable labels\n    le = LabelEncoder()\n    y = [[1, 2], [3, 4]]  # This should raise a ValueError\n    assert_raises(ValueError, le.fit, y)\n    assert_raises(ValueError, le.transform, y)\n\n    # Test LabelEncoder with non-comparable labels\n    le = LabelEncoder()\n    y = [1, 2, 3, 'a']  # This should raise a ValueError\n    assert_raises(ValueError, le.fit, y)\n    assert_raises(ValueError, le.transform, y)\n", "def test_label_binarize_one_vs_all_sparse_output():\n    # Test label_binarize with one-vs-all sparse output\n    lb = LabelBinarizer(sparse_output=True)\n    X = np.array([1, 2, 3])\n    classes = np.array([1, 2, 3])\n    result = lb.fit_transform(X)\n    expected = csr_matrix((X.shape[0], classes.shape[0]), dtype=int)\n    assert_array_equal(result.toarray(), expected.toarray())\n"], "sample_1028": ["def test_Mod_eval():\n    x = symbols('x')\n    m = Mod(x**2 + 2*x + 1, 4)\n    assert m.evalf() == 1\n    assert m.subs(x, 3) == 1\n    assert m.subs(x, 4) == 0\n    assert m.subs(x, -3) == 1\n    assert m.subs(x, -4) == 0\n\n    m = Mod(x**2 + 2*x + 2, 4)\n    assert m.evalf() == 2\n    assert m.subs(x, 3) == 2\n    assert m.subs(x, 4) == 0\n    assert m.subs(x, -3) == 2\n    assert m.subs(x, -4) == 0\n\n    m = Mod(x**2 + 2*x, 4)\n    assert m.evalf() == 2\n    assert m.subs(x, 3) == 1\n    assert m.subs(x, 4) == 0\n    assert m.subs(x, -3) == 3\n    assert m.subs(x, -4) == 0\n\n    m = Mod(x**2 - 2*x, 4)\n    assert m.evalf() == 2\n    assert m.subs(x, 3) == 3\n    assert m.subs(x, 4) == 0\n    assert m.subs(x, -3) == 1\n    assert m.subs(x, -4) == 0\n\n    m = Mod(0, 4)\n    assert m.evalf() == 0\n    assert m.subs(x, 3) == 0\n    assert m.subs(x, 4) == 0\n    assert m.subs(x, -3) == 0\n    assert m.subs(x, -4) == 0\n\n    m = Mod(1, 4)\n    assert m.evalf() == 1\n    assert m.subs(x, 3) == 1\n    assert m.subs(x, 4) == 1\n    assert m.subs(x, -3) == 1\n    assert m.subs(x, -4) == 1\n\n    m = Mod(2, 4)\n    assert m.evalf", "def test_modulus_with_zero_as_first_argument():\n    from sympy.core.numbers import nan\n    x = symbols('x')\n    assert Mod(x, 0) == nan\n    assert Mod(0, 0) == nan\n    assert Mod(nan, 0) == nan\n    assert Mod(nan, nan) == nan\n    assert Mod(5, 0) == nan\n    assert Mod(0, 5) == nan\n    assert Mod(nan, 5) == nan\n    assert Mod(5, nan) == nan\n", "def test_Mod_large():\n    x = Symbol('x')\n    assert Mod(12345678901234567890*x, 1234567890123456789).is_integer\n    assert Mod(12345678901234567890, 1234567890123456789).is_integer\n    assert Mod(1234567890123456789, 1234567890123456789).is_integer\n\n    # Test with larger numbers\n    large_number = 10**100\n    assert Mod(large_number, large_number).is_zero\n    assert Mod(2*large_number, large_number).is_integer\n    assert Mod(large_number + 1, large_number).is_integer\n    assert Mod(large_number - 1, large_number).is_integer\n", "def test_Mod_divided_Pow():\n    from sympy import symbols, sqrt, Mod\n\n    x, y = symbols('x,y')\n    assert Mod(x**2, y) == Mod(x**2, y)\n    assert Mod(x**2, sqrt(y)) == Mod(x**2, sqrt(y))\n", "    def test_Mod_of_complex(self):\n        from sympy import I, Mod\n        from sympy.abc import x\n        from sympy import cos, sin\n        expr = Mod(cos(x) + I * sin(x), x)\n        self.assertEqual(expr.simplify().args[1], 1)\n"], "sample_441": ["    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashForm(self.u1)\n        self.assertTrue(form.is_valid())\n        self.assertIn(\"No password set.\", form.fields[\"password\"].widget.attrs[\"summary\"][0][\"label\"])\n", "    def test_read_only_password_hash_widget(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertTrue(form.fields[\"password\"].widget.read_only)\n", "    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashField()\n        self.assertEqual(form.widget.template_name, 'auth/widgets/read_only_password_hash.html')\n        self.assertTrue(form.widget.read_only)\n", "    def test_widgets_render(self):\n        form = UserChangeForm(instance=self.u1)\n        widget = form.fields[\"password\"].widget\n        html = widget.render(\"password\", \"hashed_password\")\n        self.assertIn(\"password\", html)\n        self.assertIn(\"hashed_password\", html)\n", "    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashField()\n        self.assertTrue(form.widget.read_only)\n        self.assertEqual(form.widget.template_name, \"auth/widgets/read_only_password_hash.html\")\n"], "sample_521": ["def test_text3d_with_no_zdir():\n    fig, axs = plt.subplots()\n    ax = axs.add_subplot(111, projection='3d')\n    text = art3d.Text3D('Hello, world!', 0, 0, 0, zdir=None)\n    text.draw(ax)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 1)\n    return fig, axs\n", "def test_poly3dcollection_facecolor():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a simple 3D PolyCollection\n    verts = [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]]\n    poly_col = art3d.Poly3DCollection([verts])\n\n    # Set a simple face color\n    poly_col.set_facecolor('red')\n\n    # Check the face color is correctly set\n    assert poly_col.get_facecolor() == np.array([1., 0., 0., 1.])\n\n    # Check the face color is correctly set after projection\n    poly_col.set_3d_properties()\n    assert np.allclose(poly_col.get_facecolor(), np.array([1., 0., 0., 1.]))\n", "def test_set_3d_properties_line3d():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    z = np.cos(x)\n    line = art3d.Line3D(x, y, z, color='r')\n    ax.add_line(line)\n    line.set_3d_properties(z=5, zdir='x')\n    assert np.allclose(line._verts3d[:, 2], 5)\n\n", "def test_line_collection_2d_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.sin(x)\n    line = LineCollection([[[x, y]], [x, y]], colors='red')\n    ax.add_collection(line)\n    line_3d = line_collection_2d_to_3d(line, z=5, zdir='z')\n    ax.set_xlim(-1, 3)\n    ax.set_ylim(-1, 1)\n    ax.set_zlim(-1, 6)\n    ax.set_title('Line collection 2D to 3D')\n    assert line_3d is not None\n", "def test_rotate_axes():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot([0, 1], [0, 1], [0, 1])\n    patch = art3d.Patch3D(PathPatch(Circle((0, 0), 1, resolution=100),\n                                  facecolor='blue', edgecolor='red'))\n    ax.add_patch(patch)\n    ax.set_zlim(-1, 1)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n\n    # Test rotation around x-axis\n    ax.set_yticks([])\n    ax.set_xticks([])\n    ax.set_zticks([])\n    patch.set_3d_properties(0, '-x')\n    ax.view_init(elev=30)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    # Test rotation around y-axis\n    ax.view_init(elev=30, azim=-120)\n    patch.set_3d_properties(0, '-y')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    # Test rotation around z-axis\n    ax.view_init(elev=30, azim=-120)\n    patch.set_3d_properties(0, '-')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    # Test original orientation\n    ax.view_init(elev=30)\n    patch.set_3d_properties(0)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    # Check if all patches are correctly rotated\n    for artist in [ax.collections[0], ax.patches[0]]:\n        assert artist.get_path().vertices.shape == (100, 2)\n\n    # Test rotation around z-axis with a different z-position\n    ax.view_init(elev=30)\n    patch.set_3d_properties(1)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    assert patch.get_path().vertices.shape == (100, 2)\n\n    # Test rotation around x-axis with a different z-position\n    ax.view_init(elev=30, azim=-120)\n    patch.set_3d_properties(1, '-x"], "sample_490": ["    def test_create_sql(self):\n        constraint = UniqueConstraint(\n            name=\"test_unique_constraint\",\n            fields=[\"field1\", \"field2\"],\n            deferrable=Deferrable.DEFERRED,\n            violation_error_code=\"test_violation_code\",\n            violation_error_message=\"Test violation message\",\n        )\n        self.assertEqual(\n            constraint.create_sql(Product, connection),\n            (\n                \"ALTER TABLE testapp_product \"\n                \"ADD CONSTRAINT test_unique_constraint \"\n                \"UNIQUE DEFERRABLE DEFERRED (field1, field2)\"\n                \" USING INDEX TABLESPACE pg_default\"\n            ),\n        )\n", "    def test_check_constraint_deprecation_warning(self):\n        with ignore_warnings(record=True) as w:\n            CheckConstraint(check=F('field') > 10)\n        self.assertTrue(len(w.record) == 1)\n        self.assertTrue(\n            \"Passing positional arguments to CheckConstraint is deprecated.\" in str(w.record[0].message)\n        )\n", "    def test_base_constraint_init(self):\n        # Test that BaseConstraint requires a name argument\n        with self.assertRaises(TypeError):\n            BaseConstraint()\n\n        # Test that BaseConstraint issues a deprecation warning for positional arguments\n        with ignore_warnings(record=True):\n            BaseConstraint(name='test_constraint')\n\n        # Test that BaseConstraint allows keyword arguments for name, violation_error_code, and violation_error_message\n        constraint = BaseConstraint(name='test_constraint')\n        self.assertEqual(constraint.name, 'test_constraint')\n        self.assertIsNone(constraint.violation_error_code)\n        self.assertEqual(constraint.violation_error_message, BaseConstraint.default_violation_error_message)\n\n        constraint = BaseConstraint(name='test_constraint', violation_error_code='code', violation_error_message='message')\n        self.assertEqual(constraint.name, 'test_constraint')\n        self.assertEqual(constraint.violation_error_code, 'code')\n        self.assertEqual(constraint.violation_error_message, 'message')\n", "    def test_unique_constraint_with_expressions(self):\n        unique_constraint = UniqueConstraint(\n            expressions=[F(\"field1\"), F(\"field2\")], fields=(), name=\"test_unique_constraint\"\n        )\n        with self.assertRaises(ValueError):\n            unique_constraint.validate(Product, Product())\n        product = Product(field1=\"value1\", field2=\"value2\")\n        with self.assertRaises(ValidationError):\n            unique_constraint.validate(Product, product)\n", "    def test_unique_constraint_with_condition(self):\n        model = UniqueConstraintConditionProduct\n        instance = model(product=1, product_attribute='attribute1')\n        constraint = UniqueConstraint(\n            condition=Q(product__gt=0),\n            fields=('product',)\n        )\n        with self.assertRaises(ValidationError):\n            instance.full_clean()\n        instance.product = 1\n        with self.assertRaises(ValidationError):\n            instance.full_clean()\n        instance.product = 2\n        instance.full_clean()\n"], "sample_141": ["    def test_deserialize_object(self):\n        serializer = serializers.get_serializer(self.serializer_name)()\n        data = \"\"\"[\n            {\n                \"pk\": 1,\n                \"model\": \"serializers.category\",\n                \"fields\": {\"name\": \"Reference\"}\n            }\n        ]\"\"\"\n        deserializer = serializers.get_deserializer(self.serializer_name)(data)\n        obj = next(deserializer)\n        self.assertEqual(obj.__class__.__name__, 'Category')\n        self.assertEqual(obj.name, 'Reference')\n", "    def test_deserialization_with_deferred_fields(self):\n        serializer = serializers.ModelSerializer()\n        stream = serializers.StringIO('{\"id\": 1, \"name\": \"John\", \"categories\": DEFER_FIELD}')\n        deserializer = serializers.Deserializer(stream)\n        obj = serializer.deserialize(deserializer)\n        self.assertEqual(obj.id, 1)\n        self.assertEqual(obj.name, 'John')\n        self.assertEqual(obj.categories, None)\n", "    def test_deserialize_object(self):\n        # Test deserialization of a simple object\n        data = {'name': 'Test', 'age': 30}\n        deserializer = serializers.Deserializer('{\"pk\": 1, \"model\": \"serializers.testmodel\", \"fields\": ' +\n                                            json.dumps(data)}')\n        deserialized_obj = next(deserializer)\n        self.assertEqual(deserialized_obj.object.name, 'Test')\n        self.assertEqual(deserialized_obj.object.age, 30)\n", "    def test_deserialize_object_with_non_existent_primary_key(self):\n        model_data = {\n            \"model\": \"serializers.category\",\n            \"fields\": {\"name\": \"Non-fiction\"}\n        }\n        deserializer = serializers.get_serializer(\"json\").deserialize(model_data, option=AUTO)\n        self.assertIsInstance(deserializer, serializers.DeserializedObject)\n        self.assertIsNone(deserializer.object.pk)\n        self.assertIsNone(deserializer.m2m_data)\n", "    def test_deserialize_m2m_values_deferred(self):\n        category = models.ForeignKey('self', related_name='categories')\n        book = models.Model()\n        book.add_to_class('categories', models.ManyToManyField(category))\n        serializer = serializers.get_serializer(self.serializer_name)()\n        serialized_data = [\n            {\n                \"model\": \"serializers.book\",\n                \"fields\": {\"title\": \"Book 1\", \"categories\": \"[1, 2]\"},\n                \"pk\": 1\n            },\n            {\n                \"model\": \"serializers.category\",\n                \"fields\": {\"name\": \"Reference\"},\n                \"pk\": 1\n            },\n            {\n                \"model\": \"serializers.category\",\n                \"fields\": {\"name\": \"Non-fiction\"},\n                \"pk\": 2\n            }\n        ]\n        deserializer = serializers.get_deserializer(self.serializer_name)\n        deserializer.load(serialized_data)\n        self.assertEqual(deserializer deserialized_objects[0].object.categories.all().count(), 2)\n"], "sample_626": ["    def test_getitem_with_tuple(self):\n        index = pd.Index([1, 2, 3])\n        adapter = PandasIndexingAdapter(index)\n        result = adapter[1]\n        assert isinstance(result, np.int64)\n        assert result == 2\n", "def test_as_array_method():\n    # Check that passing a single value to as_array raises TypeError\n    with pytest.raises(TypeError):\n        Variable(np.array([1, 2, 3])).as_array()\n\n    # Check that passing multiple values to as_array raises ValueError\n    with pytest.raises(ValueError):\n        Variable(np.array([[1, 2], [3, 4]])).as_array(1, 2)\n\n    # Check that as_array method returns a numpy array\n    var = Variable(np.array([1, 2, 3]))\n    assert isinstance(var.as_array(), np.ndarray)\n\n    # Check that as_array method returns a numpy array of the same dtype\n    var = Variable(np.array([1, 2, 3], dtype=np.float32))\n    assert var.as_array().dtype == np.float32\n\n    # Check that as_array method returns a numpy array of the same shape\n    var = Variable(np.array([[1, 2], [3, 4]]))\n    assert var.as_array().shape == (2, 2)\n", "    def test_indexer_0d(self):\n        da = xr.DataArray([1, 2, 3])\n        indexer = BasicIndexer((0,))\n        result = da[indexer]\n        assert result == 1\n", "def test_pandas_multiindex_assignment():\n    da = xr.DataArray(np.arange(6), dims=[\"x\", \"y\"])\n    da.coords[\"z\"] = xr.DataArray([1, 2, 3, 4, 5, 6], dims=[\"x\"])\n    da.coords[\"w\"] = xr.DataArray([1, 1, 1, 1, 1, 1], dims=[\"x\"])\n\n    multiindex = pd.MultiIndex.from_tuples([(1, 1), (1, 2), (2, 1), (2, 2)])\n    da.coords[\"z\"].encoding[\"dtype\"] = \"int32\"\n    da[\"y\"] = xr.DataArray(\n        np.arange(6), coords={\"z\": multiindex}, dims=[\"x\", \"y\"]\n    )\n\n    da[\"y\"].values[2] = 10\n\n    expected_value = np.full(6, fill_value=10, dtype=\"int32\")\n    expected_value[3] = 5\n\n    assert np.array_equal(da[\"y\"].values, expected_value)\n    assert np.array_equal(da[\"z\"].values, multiindex)\n    assert np.array_equal(da[\"w\"].values, np.full(6, fill_value=1, dtype=\"int64\"))\n", "def test_explicit_indexing_array_api_adapter():\n    ds = xr.Dataset(\n        {\n            \"x\": ([\"x\"], np.arange(3)),\n            \"y\": ([\"x\", \"y\"], np.arange(3 * 4).reshape(3, 4)),\n        },\n        coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"d\", \"e\", \"f\", \"g\"]},\n    )\n\n    # Test integer index\n    assert (\n        ds[\"y\"].isel(x=1).values == np.array([[ 7,  8,  9]])\n    ).all()\n\n    # Test slice index\n    assert (\n        ds[\"y\"].isel(x=slice(1, 3)).values == np.array([[14, 15, 16], [17, 18, 19]])\n    ).all()\n\n    # Test that masking works with integer index\n    masked_data = np.ma.array(\n        [[1, 2, 3], [4, 5, 6], [7, 8, 9]], mask=[[0, 0, 0], [1, 0, 1], [0, 0, 0]]\n    )\n    ds_masked = xr.Dataset({\"x\": ([\"x\"], np.arange(3)), \"y\": ([\"x\", \"y\"], masked_data)})\n    assert (\n        ds_masked[\"y\"].isel(x=1).values == np.array([[4, 5, 6]])\n    ).all()\n\n    # Test that masking works with slice index\n    assert (\n        ds_masked[\"y\"].isel(x=slice(1, 3)).values == np.array(\n            [[5, 6, 6], [7, 8, 9]]\n        )\n    ).all()\n\n    # Test that masking works with vectorized index\n    masked_data = np.ma.array(\n        np.arange(12).reshape((2, 3, 2)), mask=[[0, 0, 0], [1, 1, 1]]\n    )\n    ds_masked = xr.Dataset(\n        {\"x\": ([\"x\", \"y\"], np.arange(2 * 3)), \"y\": ([\"x\", \"y\", \"z\"], masked_data)}\n    )\n    assert (\n        ds_masked[\"y\"].isel(x=1).isel(z=0).values == np.array([5, "], "sample_204": ["    def test_migrations_module(self):\n        # Test that migrations_module returns the correct module path.\n        app_label = 'myapp'\n        module_name, explicit = MigrationLoader.migrations_module(app_label)\n        self.assertEqual(module_name, 'myapp.migrations')\n        self.assertFalse(explicit)\n\n        # Test that migrations_module returns the correct module path when specified in settings.\n        with modify_settings(MIGRATION_MODULES={'myapp': 'myapp.migrations_custom'}):\n            module_name, explicit = MigrationLoader.migrations_module(app_label)\n            self.assertEqual(module_name, 'myapp.migrations_custom')\n            self.assertTrue(explicit)\n", "    def setUp(self):\n        super().setUp()\n        self.loader = MigrationLoader(connection=connections['default'])\n", "    def test_detect_conflicts(self):\n        \"\"\"\n        Test that detect_conflicts() identifies apps with multiple leaf migrations.\n        \"\"\"\n        from django.db.migrations.loader import MigrationLoader\n\n        # Set up a test migration loader\n        loader = MigrationLoader(connection=connection)\n\n        # Create two apps with multiple leaf migrations\n        app_config1 = apps.get_app_config('app1')\n        app_config2 = apps.get_app_config('app2')\n\n        # Create a migration for each app\n        migration1 = app_config1.create_model('test_migration')\n        migration2 = app_config2.create_model('test_migration')\n\n        # Add the migrations to the loader\n        loader.graph.add_node((app_config1.label, migration1.name), migration1)\n        loader.graph.add_node((app_config2.label, migration2.name), migration2)\n\n        # Assert that the loader detects conflicts\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'app1': ['test_migration'], 'app2': ['test_migration']})\n", "    def test_build_graph_with_inconsistent_migration_history(self):\n        # Create a migration that is not applied, but has an applied dependency\n        with self.settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n            },\n            'other': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': ':memory:',\n            },\n        }):\n            # Create a recorder for the default database\n            recorder = MigrationRecorder(connections['default'])\n            # Create a migration that is not applied\n            recorder.applied_migrations = [('app1', '0001_initial')]\n            # Create a migration that is applied, but depends on the not applied migration\n            recorder.applied_migrations.append(('app2', '0001_initial'))\n\n        # Create a loader for the default database\n        loader = MigrationLoader(connections['default'])\n        # Build the graph\n        loader.build_graph()\n        # Check that an InconsistentMigrationHistory is raised\n        with self.assertRaises(InconsistentMigrationHistory):\n            loader.check_consistent_history(connections['default'])\n", "    def test_build_graph(self):\n        self.assertTrue(self.loader.build_graph())\n"], "sample_984": ["def test_StrPrinter_print_Poly():\n    p = StrPrinter(settings={\"sympy_integers\": True})\n    assert p.doprint(Poly(x**2 + 2*x + 1, x)) == 'S(1)*x**2 + S(2)*x + S(1)'\n", "def test_printing_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(M) == \"[[1, 2], [3, 4]]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert sstr(M) == \"[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert sstr(M) == \"[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\"\n\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(M, order='none') == \"[[1, 2], [3, 4]]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert sstr(M, order='none') == \"[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert sstr(M, order='lex') == \"[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\"\n\n    M = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert sstr(M, order='grlex') == \"[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\"\n", "def test_print_CRootOf():\n    p = StrPrinter()\n    assert p.doprint(CRootOf(x, 1)) == \"CRootOf(x, 1)\"\n    assert p.doprint(CRootOf(x, 2)) == \"CRootOf(x, 2)\"\n    assert p.doprint(CRootOf(x, 3)) == \"CRootOf(x, 3)\"\n\n    assert p.doprint(CRootOf(x**2 + 1, 1)) == \"CRootOf(x**2 + 1, 1)\"\n    assert p.doprint(CRootOf(x**2 + 1, 2)) == \"CRootOf(x**2 + 1, 2)\"\n    assert p.doprint(CRootOf(x**2 + 1, 3)) == \"CRootOf(x**2 + 1, 3)\"\n\n    assert p.doprint(CRootOf(x**2 + 1, 4)) == \"CRootOf(x**2 + 1, 4)\"\n", "def test_strprinter_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(M) == \"[1 2]\\n[3 4]\"\n    assert sstr(M, full_prec=True) == \"[1.0 2.0]\\n[3.0 4.0]\"\n    assert sstr(M, full_prec=False) == \"[1 2]\\n[3 4]\"\n    assert sstr(M, order='none') == \"[[1, 2], [3, 4]]\"\n", "def test_str_printer_division():\n    p = StrPrinter({'order': 'none', 'full_prec': True})\n    assert p._print(Rational(10, 3)) == '10/3'\n"], "sample_422": ["    def test_get_queryset(self):\n        descriptor = ReverseManyToOneDescriptor(Author._meta.get_field(\"books\"))\n        manager = descriptor.__get__(self.author1)\n        self.assertIsInstance(manager, RelatedManager)\n        self.assertEqual(manager.instance, self.author1)\n        self.assertEqual(manager.model, Book)\n        self.assertEqual(manager.field, Author._meta.get_field(\"books\"))\n        self.assertEqual(manager.query_field_name, \"books\")\n        self.assertEqual(manager.prefetch_cache_name, \"books\")\n        self.assertEqual(manager.source_field_name, \"author_id\")\n        self.assertEqual(manager.target_field_name, \"book_id\")\n        self.assertEqual(manager.symmetrical, False)\n        self.assertEqual(manager.through, Book.authors.through)\n        self.assertEqual(manager.reverse, False)\n", "    def test_get_queryset(self):\n        instance = Author.objects.get(name=\"Charlotte\")\n        descriptor = instance.books\n        self.assertIsInstance(descriptor, Book.objects)\n", "    def test_add(self):\n        author1 = Author.objects.get(name=\"Charlotte\")\n        author2 = Author.objects.get(name=\"Anne\")\n        reader1 = Reader.objects.get(name=\"Amy\")\n        book = Book.objects.get(title=\"Wuthering Heights\")\n\n        # Test add single object\n        self.assertEqual(book.authors.count(), 1)\n        reader1.books_read.add(book)\n        self.assertEqual(book.authors.count(), 2)\n        self.assertIn(author1, book.authors.all())\n        self.assertIn(reader1, book.books_read.all())\n\n        # Test add multiple objects\n        reader2 = Reader.objects.get(name=\"Belinda\")\n        reader1.books_read.add(book, book2)\n        self.assertEqual(book.authors.count(), 2)\n        self.assertEqual(reader1.books_read.count(), 2)\n\n        # Test adding a non-model instance\n        with self.assertRaises(TypeError):\n            reader1.books_read.add(\"non-model-instance\")\n\n        # Test adding a non-existent model instance\n        with self.assertRaises(ObjectDoesNotExist):\n            reader1.books_read.add(Author.objects.create())\n", "    def test_prefetch_related_with_symmetrical_m2m_relation(self):\n        queryset = Bookmark.objects.prefetch_related('readers', 'books')\n        bookmarks = list(queryset)\n        self.assertEqual(len(bookmarks), 2)\n        self.assertEqual(queryset.count(), 2)\n\n        self.assertEqual(len(bookmarks[0].readers.all()), 1)\n        self.assertEqual(bookmarks[0].readers.all()[0].name, 'Amy')\n        self.assertEqual(len(bookmarks[1].readers.all()), 1)\n        self.assertEqual(bookmarks[1].readers.all()[0].name, 'Belinda')\n\n        self.assertEqual(len(bookmarks[0].books.all()), 2)\n        self.assertIn(bookmarks[0].books.all()[0].title, ['Poems', 'Sense and Sensibility'])\n        self.assertEqual(len(bookmarks[1].books.all()), 1)\n        self.assertEqual(bookmarks[1].books.all()[0].title, 'Jane Eyre')\n", "    def test_many_to_many_descriptor_prefetching(self):\n        # Test that ManyToManyDescriptor uses prefetch_related for related objects.\n        with CaptureQueriesContext() as queries:\n            Author.objects.prefetch_related(\"books\").get(id=1)\n            self.assertEqual(len(queries), 1)\n            self.assertIn(\"FROM\", queries[0])\n            self.assertIn(\"JOIN\", queries[0])\n            self.assertIn(\"FROM\", queries[0])\n"], "sample_1100": ["def test_pow_as_base_exp():\n    assert Pow(2, 3).as_base_exp() == (2, 3)\n    assert Pow(2, 3 + 4).as_base_exp() == (2, 3 + 4)\n    assert Pow(2 + 3, 3).as_base_exp() == (2 + 3, 3)\n    assert Pow(2 + 3, 3 + 4).as_base_exp() == (2 + 3, 3 + 4)\n", "def test_pow_evalf():\n    x = symbols('x')\n    assert same_and_same_prec(Pow(x, 2).evalf(3), 0.5*(x+0.5)**2)\n    assert same_and_same_prec(Pow(x, 3).evalf(3), 0.5*(x+0.5)**3)\n    assert same_and_same_prec(Pow(x, Rational(1,3)).evalf(3), (x+0.5)**(1/3))\n    assert same_and_same_prec(Pow(x, -1).evalf(3), 1/(x+0.5))\n    assert same_and_same_prec(Pow(x, -2).evalf(3), 1/(x+0.5)**2)\n    assert same_and_same_prec(Pow(x, -3).evalf(3), 1/(x+0.5)**3)\n    assert same_and_same_prec(Pow(x, -Rational(1,3)).evalf(3), (x+0.5)**(-1/3))\n", "    def test_Pow_with_zero_base_and_nonzero_exponent(self):\n        x = sympify('x')\n        assert Pow(x, 0) == S.One\n        assert Pow(x, -0) == S.One\n        assert Pow(x, oo) == S.Infinity\n        assert Pow(x, -oo) == S.Zero\n", "def test_Pow_as_numer_denom():\n    from sympy import Pow, exp, sin, Rational, zoo, oo\n    from sympy.core.numbers import One, Zero\n    from sympy.core.expr import unchanged\n    from sympy.core.sympify import SympifyError\n    from sympy.testing.pytest import raises\n\n        # Test with positive exponent\n        assert Pow(3, 2).as_numer_denom() == (3, 2)\n        # Test with negative exponent\n        assert Pow(3, -2).as_numer_denom() == (3, -2)\n        # Test with non-integer exponent\n        assert Pow(3, Rational(3, 2)).as_numer_denom() == (3**(3/2), 1)\n        # Test with non-integer exponent and non-integer base\n        assert Pow(exp(3), Rational(3, 2)).as_numer_denom() == (exp(3)**(3/2), 1)\n        # Test with oo exponent\n        assert Pow(3, oo).as_numer_denom() == (1, 3**oo)\n        # Test with oo base and non-oo exponent\n        assert Pow(oo, 3).as_numer_denom() == (1, oo**3)\n        # Test with zoo exponent\n        assert Pow(3, zoo).as_numer_denom() == (1, 3**zoo)\n        # Test with zoo base and non-zoo exponent\n        assert Pow(zoo, 3).as_numer_denom() == (1, zoo**3)\n        # Test with infinite base and oo exponent\n        assert Pow(oo, oo).as_numer_denom() == (1, oo**oo)\n        # Test with finite base and oo exponent\n        assert Pow(3, oo).as_numer_denom() == (1, 3**oo)\n        # Test with finite base and zoo exponent\n        assert Pow(3, zoo).as_numer_denom() == (1, 3**zoo)\n        # Test with negative base and oo exponent\n        assert Pow(-3, oo).as_numer_denom() == (-1, -3**oo)\n        # Test with negative base and zoo exponent\n        assert Pow(-3, zoo).as_numer_denom() == (-1, -3**zoo)\n       ", "def test_isqrt():\n    x = symbols('x')\n    assert isqrt(16) == 4\n    assert isqrt(25) == 5\n    assert isqrt(30) == 5\n    assert isqrt(36) == 6\n    assert isqrt(49) == 7\n    assert isqrt(64) == 8\n    assert isqrt(81) == 9\n    assert isqrt(100) == 10\n    assert isqrt(-1) == 1\n    assert isqrt(-2) == 1\n    assert isqrt(-3) == 1\n    assert isqrt(-4) == 2\n\n    assert raises(ValueError, lambda: isqrt(-5))\n    assert raises(ValueError, lambda: isqrt(0))\n    assert raises(ValueError, lambda: isqrt(-0))\n    assert raises(ValueError, lambda: isqrt(-0.5))\n    assert raises(ValueError, lambda: isqrt(-1.0))\n    assert raises(ValueError, lambda: isqrt(-1.0))  # test float inputs\n\n    # large inputs\n    assert isqrt(2**52 + 2**27) == 2**26\n\n    # edge cases\n    assert isqrt(1) == 1\n    assert isqrt(2**100) == 2**50\n    assert isqrt(0.0) == 0\n    assert isqrt(0) == 0\n    assert isqrt(1.0) == 1\n    assert isqrt(1) == 1\n\n    # sanity check that it uses the C function when it can\n    assert isqrt(3.0) == 1\n\n    # verify that isqrt(x) == isqrt(-(-x)) for positive numbers\n    assert isqrt(16) == isqrt(-(-16))\n    assert isqrt(25) == isqrt(-(-25))\n    assert isqrt(36) == isqrt(-(-36))\n    assert isqrt(49) == isqrt(-(-49))\n    assert isqrt(64) == isqrt(-(-64))\n    assert isqrt(81) == isqrt(-(-81))\n    assert isqrt(100) == isqrt(-(-100))\n"], "sample_226": ["    def test_destroy_test_db_keeps_original_database_name(self):\n        original_name = 'original_database_name'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = original_name\n        destroyer = BaseDatabaseCreation(test_connection)\n        destroyer.destroy_test_db(old_database_name=original_name, keepdb=True)\n        self.assertEqual(test_connection.settings_dict['NAME'], original_name)\n", "    def test_clone_test_db_keeps_migrations(self):\n        # Arrange\n        connection = get_connection_copy()\n        connection.settings_dict['TEST']['MIGRATE'] = True\n        connection.settings_dict['NAME'] = 'test_db'\n\n        db_creation = BaseDatabaseCreation(connection)\n\n        # Act\n        db_creation.clone_test_db(suffix='clone')\n\n        # Assert\n        self.assertEqual(connection.settings_dict['NAME'], 'test_db_clone')\n", "    def test_create_test_db_migrate(self):\n        # Test that migrations are applied to the test database\n        connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(connection)\n        db_creation.create_test_db(verbosity=0, autoclobber=False, serialize=False)\n        # Check if the migrations are applied to the test database\n        self.assertTrue(connection.settings_dict['TEST']['MIGRATE'])\n", "    def test_serialize_db_to_string(self):\n        # Arrange\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = False\n        test_connection.create_test_db()\n        test_connection.cursor().execute('''\n            CREATE TABLE test_object (\n                id serial PRIMARY KEY,\n                name character varying(255)\n            )\n        ''')\n\n        obj1 = Object.objects.create(name='obj1')\n        obj2 = Object.objects.create(name='obj2')\n        ObjectReference.objects.create(reference=obj1)\n        ObjectSelfReference.objects.create(object=obj1, reference=obj2)\n\n        # Act\n        serialized_data = test_connection.serialize_db_to_string()\n\n        # Assert\n        self.assertNotEqual(serialized_data, '')\n        self.assertEqual(len(serialized_data.splitlines()), 3)\n", "    def test_clone_test_db_raises_not_implemented_error(self):\n        # Test that _clone_test_db raises a NotImplementedError when called directly.\n        connection = get_connection_copy()\n        base_db_creation = BaseDatabaseCreation(connection)\n        with self.assertRaises(NotImplementedError):\n            base_db_creation._clone_test_db(suffix='clone_suffix')\n"], "sample_727": ["def _check_statistics_axis_none(X, X_true,\n                               strategy, statistics, missing_values):\n    \"\"\"Utility function for testing imputation for a given strategy.\n\n    Test:\n        - with dense and sparse arrays\n\n    Check that:\n        - the statistics (mean, median, mode) are correct\n        - the missing values are imputed correctly\"\"\"\n\n    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n              \"axis = None, sparse = {0}\" % (strategy, missing_values)\n\n    assert_ae = assert_array_equal\n    if X.dtype.kind == 'f' or X_true.dtype.kind == 'f':\n        assert_ae = assert_array_almost_equal\n\n    # Normal matrix\n    imputer = Imputer(missing_values, strategy=strategy, axis=None)\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(False))\n    assert_ae(X_trans, X_true, err_msg=err_msg.format(False))\n\n    # Sparse matrix\n    imputer = Imputer(missing_values, strategy=strategy, axis=None)\n    imputer.fit(sparse.csc_matrix(X))\n    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n\n    if sparse.issparse(X_trans):\n        X_trans = X_trans.toarray()\n\n    assert_ae(imputer.statistics_, statistics,\n              err_msg=err_msg.format(True))\n\n", "def test_imputer_invalid_strategy():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    imputer = Imputer(strategy=\"invalid\")\n    assert_raises(ValueError, imputer.fit, X)\n    imputer = Imputer(strategy=\"invalid\", axis=1)\n    assert_raises(ValueError, imputer.fit, X.transpose())\n", "def test_imputer_0():\n    \"\"\"Test case with 0.0 values as missing_values and axis = 0.\n\n    Check that:\n        - the statistics (mean, median, mode) are correct\n        - the missing values are imputed correctly\n    \"\"\"\n    X = np.array([[1, 2, np.nan, 4],\n                  [np.nan, 3, 5, 6],\n                  [7, 8, np.nan, 9]])\n\n    X_true = np.array([[1, 2, 2, 4],\n                       [1, 3, 5, 6],\n                       [7, 8, 8, 9]])\n\n    for strategy in ['mean', 'median', 'most_frequent']:\n        imputer = Imputer(missing_values=0.0, strategy=strategy, axis=0)\n        _check_statistics(X, X_true, strategy, imputer.statistics_, 0.0)\n\n", "def test_imputer_copy(self):\n    \"\"\"Test copy parameter of the Imputer transformer.\n\n    Test that:\n        - Imputer returns a new copy when copy is True\n        - Imputer does not return a copy when copy is False, except in some\n          edge cases (see comments in the Imputer class)\n\n    \"\"\"\n    X = np.array([[1, 2, np.nan], [4, 5, 6]])\n\n    # Test that a new copy is returned when copy is True\n    imputer = Imputer(strategy='mean', copy=True)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(id(imputer.statistics_), id(np.array([1., 2., 3.]))\n\n    # Test that a new copy is returned when X is not an array of floating values\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.uint8)\n    imputer = Imputer(strategy='mean', copy=True)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(id(imputer.statistics_), id(np.array([1., 2., 3.])))\n\n    # Test that a new copy is returned when X is sparse and missing_values=0\n    X = sparse.csc_matrix([[1, 2, 0], [4, 5, 6]])\n    imputer = Imputer(missing_values=0, copy=True)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(id(imputer.statistics_), id(np.array([1., 2., 3.]))\n\n    # Test that a new copy is not returned when axis=0 and X is encoded as a CSR\n    # matrix (this is an edge case, as CSR matrices do not support\n    # in-place modification)\n    X = sparse.csr_matrix([[1, 2, 0], [4, 5, 6]])\n    imputer = Imputer(strategy='mean', copy=False)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(id(imputer.statistics_), id(np.array([1., 2., 3.]))\n\n    # Test that a new copy is not returned when axis=1 and X is encoded as a CSC\n   ", "def _check_statistics_axis_zero_sparse_zero_mismatches_other_values(self):\n    \"\"\"Utility function for testing imputation for a given strategy along axis=0\n    when there are missing values equal to 0.\n\n    Test that the missing values are imputed correctly\n    \"\"\"\n\n    X = sparse.csr_matrix([[1, 2, 0], [0, 3, 4], [5, 0, 6]])\n    X_true = np.array([[1, 2, 0], [2, 3, 4], [5, 4, 6]])\n    statistics = np.array([2., 3., 5.])\n\n    imputer = Imputer(missing_values=0, strategy=\"mean\", axis=0)\n    imputer.fit(X)\n\n    X_trans = imputer.transform(X)\n\n    assert_array_equal(X_trans.toarray(), X_true)\n\n    # Test that all zero columns are discarded\n    X = sparse.csr_matrix([[0, 0, 0], [0, 0, 0]])\n    imputer.fit(X)\n    assert_raises(ValueError, imputer.transform, X)\n\n"], "sample_855": ["def test_stratified_strategy_predict_proba():\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y = np.array([[1, 0],\n                  [2, 0],\n                  [1, 0],\n                  [1, 3]])\n    est = DummyClassifier(strategy='stratified')\n    est.fit(X, y)\n    _check_predict_proba(est, X, y)\n", "def test_dummy_regressor_score():\n    # Test that DummyRegressor returns 1.0 when the strategy is \"mean\" and\n    # all target values are equal.\n    X = np.array([[0], [0], [0], [0]])\n    y = np.full((4, 1), 5)\n    clf = DummyRegressor(strategy=\"mean\")\n    clf.fit(X, y)\n    assert_almost_equal(clf.score(X, y), 1.0)\n\n    # Test that DummyRegressor returns 1.0 when the strategy is \"constant\" and\n    # the constant is equal to all target values.\n    X = np.array([[0], [0], [0], [0]])\n    y = np.full((4, 1), 5)\n    clf = DummyRegressor(strategy=\"constant\", constant=5)\n    clf.fit(X, y)\n    assert_almost_equal(clf.score(X, y), 1.0)\n\n    # Test that DummyRegressor returns 1.0 when the strategy is \"median\" and\n    # all target values are equal.\n    X = np.array([[0], [0], [0], [0]])\n    y = np.full((4, 1), 5)\n    clf = DummyRegressor(strategy=\"median\")\n    clf.fit(X, y)\n    assert_almost_equal(clf.score(X, y), 1.0)\n\n    # Test that DummyRegressor returns -inf when the strategy is \"quantile\" and\n    # the constant is not equal to all target values.\n    X = np.array([[0], [0], [0], [0]])\n    y = np.array([[5], [5], [5], [4]])\n    clf = DummyRegressor(strategy=\"quantile\", quantile=0.5)\n    assert_warns_message(UserWarning, clf.fit, X, y)\n    assert_almost_equal(clf.score(X, y), -np.inf)\n", "def test_dummy_regressor_quantile_at_edge_cases():\n    # Test that quantile strategy works at edge cases\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    # Mean\n    y = np.array([[1], [1], [1], [1]])\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.0)\n    est.fit(X, y)\n    assert_array_almost_equal(est.constant_, np.array([[1]]))\n\n    # Median\n    y = np.array([[2], [1], [1], [1]])\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.5)\n    est.fit(X, y)\n    assert_array_almost_equal(est.constant_, np.array([[2]]))\n\n    # Maximum\n    y = np.array([[1], [2], [2], [2]])\n    est = DummyRegressor(strategy=\"quantile\", quantile=1.0)\n    est.fit(X, y)\n    assert_array_almost_equal(est.constant_, np.array([[2]]))\n\n    # Multiple outputs\n    y = np.array([[1, 2], [1, 1], [1, 1], [1, 2]])\n    est = DummyRegressor(strategy=\"quantile\", quantile=0.5, n_outputs=2)\n    est.fit(X, y)\n    assert_array_almost_equal(est.constant_, np.array([[1, 1]]))\n", "def _check_score_regressor(estimator):\n    X = np.array([[0], [0], [0], [0]])  # ignored\n    y_learn = np.array([1, 2, 1, 1])\n    y_test = np.array([3, 4, 5, 6])\n    est = clone(estimator)\n    est.fit(X, y_learn)\n    score_learn = est.score(X, y_learn)\n    score_test = est.score(X, y_test)\n    assert score_learn == 1.0  # perfect score on training set\n    assert score_test < 1.0  # not perfect score on test set\n\n", "def _check_stratified_vs_most_frequent_same_dist(dist, clf_stratified, clf_most_frequent):\n    n_samples = 10\n    X = np.random.rand(n_samples, 2)\n    y = np.random.choice(dist, size=n_samples)\n\n    # Stratified\n    clf_stratified.fit(X, y)\n    y_pred_stratified = clf_stratified.predict(X)\n\n    # Most frequent\n    clf_most_frequent.fit(X, y)\n    y_pred_most_frequent = clf_most_frequent.predict(X)\n\n    assert_array_equal(y_pred_stratified, y_pred_most_frequent)\n\n"], "sample_953": ["def test_generate_makefile_with_no_make_mode():\n    old_input = qs.term_input\n    qs.term_input = mock_input({\n        'Project name': 'My Project',\n        'Author name(s)': 'John Doe',\n        'Project version': '1.0',\n        'Project release': '1.0',\n        'Document language': 'en',\n        'Source file suffix': '.rst',\n        'Master document name': 'index',\n        'Makefile? (y/n)': 'n',\n        'Create Windows command file? (y/n)': 'n',\n    })\n\n    try:\n        with pytest.raises(SystemExit) as exc:\n            qs.main(['-p', 'My Project', '-a', 'John Doe', '-v', '1.0', '-r', '1.0',\n                     '-l', 'en', '--suffix', '.rst', '--master', 'index',\n                     '--makefile', 'n', '--batchfile', 'n'])\n        assert exc.value.code == 0\n        assert path.exists('Makefile') is False\n        assert path.exists('make.bat') is False\n    finally:\n        qs.term_input = old_input\n", "def test_quickstart_ask_user_defaults():\n    \"\"\"Test the ask_user function with default values.\"\"\"\n    input_mock = mock_input({\n        'Root path for the documentation': 'test',\n        'Please enter a valid path name.': 'test',\n        'Please enter some text.': 'test',\n        'Please enter a file suffix, e.g. \\'.rst\\' or \\'.txt\\'.': '.rst',\n        'Project name': 'test',\n        'Author name(s)': 'Test',\n        'Please enter a file name, or rename the existing file and press Enter': 'test',\n        'Create Makefile? (y/n)': 'y',\n        'Create Windows command file? (y/n)': 'y',\n        'Please enter one of [automatically insert docstrings from modules, automatically test code snippets in doctest blocks, link between Sphinx documentation of different projects, write \\\\\" +\n        'todo\\\\\" entries that can be shown or hidden on build, checks for documentation coverage, include math, rendered as PNG or SVG images, include math, rendered in the browser by MathJax, conditional inclusion of content based on config values, include links to the source code of documented Python objects, create .nojekyll file to publish the document on GitHub pages].': 'automatically insert docstrings from modules',\n        'Please enter one of [automatically insert docstrings from modules, automatically test code snippets in doctest blocks, link between Sphinx documentation of different projects, write \\\\\" +\n        'todo\\\\\" entries that can be shown or hidden on build, checks for documentation coverage, include math, rendered as PNG or SVG images, include math, rendered in the browser by MathJax, conditional inclusion of content based on config values, include links to the source code of documented Python objects, create .nojekyll file to publish the document on GitHub pages]:': 'automatically test code snippets in doctest blocks',\n        'Note: imgmath and mathjax cannot be enabled at the same time. imgmath has been deselected.': '',\n    })\n    qs.term_input = input_mock\n    d = {}\n    qs.ask_user(d)\n    assert d['path'] == 'test'\n    assert d['sep'] is False\n    assert d['dot'] == '_'\n    assert d['project'] == 'test'\n    assert d['author'] == 'Test'\n    assert d['version'] == ''\n    assert d['release'] == ''\n    assert d['language'] is None", "def test_quickstart_valid_dir():\n    \"\"\"\n    Test that the valid_dir() function returns True for a valid directory.\n\n    :return: None\n    \"\"\"\n    # Create a test directory\n    tmpdir = path.join(path.dirname(__file__), 'test_quickstart_valid_dir')\n    os.mkdir(tmpdir)\n\n    # Set up the environment\n    os.environ['PATH'] = pathsep.join([tmpdir, os.environ['PATH']])\n\n    # Set up the input function to return a valid directory\n    qs.term_input = mock_input({'project': 'Test Project', 'author': 'Author',\n                               'version': '1.0', 'release': '1.0', 'language': None,\n                               'suffix': '.rst', 'master': 'index'})\n    qs.ask_user({'path': tmpdir})\n\n    # Test the valid_dir() function\n    assert qs.valid_dir({'path': tmpdir}) is True\n\n    # Remove the test directory\n    os.rmdir(tmpdir)\n\n", "def test_generate_valid_dir():\n    d = {'path': 'valid_dir'}\n    assert qs.valid_dir(d)\n    d['path'] = 'non_existent_dir'\n    assert valid_dir(d) == False\n    d['path'] = path.exists('non_existent_dir') and path.isdir('non_existent_dir')\n    assert valid_dir(d)\n    d['path'] = 'valid_dir'\n    d['sep'] = True\n    d['dot'] = 'custom'\n    reserved_names = [\n        'conf.py',\n        d['dot'] + 'static',\n        d['dot'] + 'templates',\n        d['master'] + d['suffix'],\n    ]\n    assert set(reserved_names) & set(os.listdir('valid_dir')) == set([])\n    os.makedirs('conf.py', exist_ok=True)\n    assert valid_dir(d) == False\n    os.rmdir('conf.py')\n    os.makedirs(d['dot'] + 'static', exist_ok=True)\n    assert valid_dir(d) == False\n    os.rmdir(d['dot'] + 'static')\n    os.makedirs(d['dot'] + 'templates', exist_ok=True)\n    assert valid_dir(d) == False\n    os.rmdir(d['dot'] + 'templates')\n    os.makedirs(d['master'] + d['suffix'], exist_ok=True)\n    assert valid_dir(d) == False\n    os.remove(d['master'] + d['suffix'])\n    assert valid_dir(d)\n    d['sep'] = False\n    assert valid_dir(d)\n", "def test_ask_user_overwrite_conf_file():\n    \"\"\"Ensure that ask_user raises an error when an existing conf.py file is found.\"\"\"\n    qs.ensuredir = lambda x: x  # patch ensuredir to do nothing\n    qs.conf_path = 'conf.py'  # mock conf_path to be an existing file\n    with pytest.raises(qs.ValidationError) as err:\n        qs.ask_user(DEFAULTS)\n    assert str(err.value) == 'Please enter a valid path name.'\n\n"], "sample_1062": ["def test_process_common_addends():\n    x, y = symbols('x y')\n    expr1 = 2*x + 3*y + x + y\n    expr2 = 2*x + 3*y + y + x\n    assert process_common_addends(expr1, lambda x: x.expand()) == 5*x + 5*y\n    assert process_common_addends(expr2, lambda x: x.expand()) == 5*x + 5*y\n    expr3 = 2*x + 3*y + x + 3*y\n    assert process_common_addends(expr3, lambda x: x.expand()) == 4*x + 6*y\n", "def test_fu_negative_powers():\n    from sympy import pi\n\n    # test fu with negative powers\n    expr1 = 1/sin(x)**2\n    expected1 = -cot(x)**2\n    assert fu(expr1) == expected1\n\n    expr2 = 1/cos(x)**2\n    expected2 = -tan(x)**2\n    assert fu(expr2) == expected2\n\n    expr3 = 1/cos(x)**3\n    expected3 = -tan(x)**2*tan(x)\n    assert fu(expr3) == expected3\n\n    expr4 = 1/sin(x)**3\n    expected4 = -cot(x)**2*cot(x)\n    assert fu(expr4) == expected4\n\n    # test fu with negative powers and other transformations\n    expr5 = (1 - 1/sin(x)**2) * cos(y)\n    expected5 = -cot(x)**2*cos(y)\n    assert fu(expr5) == expected5\n\n    expr6 = (1 - 1/cos(x)**2) * sin(y)\n    expected6 = -tan(x)**2*sin(y)\n    assert fu(expr6) == expected6\n\n    # test that fu can handle negative powers of sec and csc\n    expr7 = sec(x)**(-3)\n    expected7 = csc(x)**3\n    assert fu(expr7) == expected7\n\n    expr8 = csc(x)**(-3)\n    expected8 = sec(x)**3\n    assert fu(expr8) == expected8\n", "def test_TR10i():\n    from sympy import sin, cos\n    from sympy.abc import x, y\n    from sympy.simplify.fu import TR10i\n    # Test with negative arguments\n    assert TR10i(sin(-x)*cos(y)) == -sin(x)*cos(y)\n    assert TR10i(cos(-x)*sin(y)) == cos(x)*sin(y)\n    # Test with even powers\n    assert TR10i(cos(x)**2*cos(y)**2) == cos(x)*cos(y)\n    assert TR10i(sin(x)**2*sin(y)**2) == sin(x)*sin(y)\n    assert TR10i(cos(x)**4*sin(y)**2) == cos(x)**2*sin(y)\n    assert TR10i(sin(x)**2*cos(y)**4) == sin(x)*cos(y)**2\n    # Test with odd powers\n    assert TR10i(cos(x)*cos(y)) == cos(x + y)\n    assert TR10i(sin(x)*sin(y)) == sin(x + y)\n    assert TR10i(cos(x)**3*cos(y)) == cos(2*x + y)\n    assert TR10i(sin(x)**3*sin(y)) == sin(3*x + y)\n    assert TR10i(cos(x)*sin(y)**3) == cos(x + 3*y)\n    # Test with arguments that cannot be combined\n    assert TR10i(cos(x)*sin(y) + sin(x)*cos(y)) == sin(x)*cos(y) + sin(y)*cos(x)\n    assert TR10i(cos(x)*sin(y) + sin(x)) == sin(x)*cos(y) + sin(y)*cos(x) + sin(x)\n    assert TR10i(sin(x)*sin(y) + sin(x)*cos(y)) == sin(x)*cos(y) + sin(x + y)\n    assert TR10i(sin(x)*cos(y) + cos(x)*sin(y)) == sin(x)*sin(y) + sin(x + y)\n    assert TR10i(sin(x)**2*sin(y) + sin(x)*sin(y)) == sin(x)*sin(y)*(sin(y) + 1)\n    # Test with different orders of arguments\n    assert TR10i(sin(y)*sin(x) + sin(x)*cos(y)) == sin(x)*sin(y) + sin(x + y)\n    assert TR10i(sin(y)*cos(x) +", "def test_TR15_negative_powers():\n    from sympy.simplify.fu import TR15\n    from sympy.abc import x, y, z\n    expr1 = sin(x)**-2\n    expr2 = cos(x)**-2\n    expr3 = (sin(x) + sin(y))**-2\n    assert TR15(expr1) == expr1\n    assert TR15(expr2) == 1/cot(x)**2\n    assert TR15(expr3) == 1/(sin(x + y)**2 + 2*sin(x)*sin(y))**2\n", "def test_TRpower():\n    # check all integer powers are reduced\n    assert TRpower(x**3) == -cos(2*x)/8 + 3/8\n    assert TRpower(x**4) == cos(2*x)**2 - 1\n    assert TRpower(x**5) == -cos(2*x)/16 + 5/16\n    assert TRpower(x**6) == -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + 5/16\n    # check odd powers are reduced to cos(kx)*sin(lx) form\n    assert TRpower(x**7) == -cos(2*x)/32 - 7*cos(4*x)/32 + 35*cos(6*x)/32 + 7/32\n    assert TRpower(x**9) == 9*cos(2*x)/512 - 21*cos(4*x)/256 + 105*cos(6*x)/512 - 35*cos(8*x)/512 + 63/512\n    # check even powers are reduced to cos(kx)**2 form\n    assert TRpower(x**8) == -cos(4*x)**2/8 + 1\n    assert TRpower(x**10) == -cos(2*x)**2/16 + 10*cos(4*x)**2/16 - 1\n    # check powers of products are reduced\n    assert TRpower(x**2*cos(2*x)) == (3*sin(x)/2 - sin(3*x)/2)*cos(2*x)\n    # check rational powers\n    assert TRpower(x**3/2) == -cos(x)/2 - 1/2\n    # check non-integer powers don't get reduced\n    assert TRpower(x**3.5) == x**3.5\n    # check all powers of sin and cos are reduced\n    assert TRpower(sin(x)**6) == -15*cos(2*x)/32 + 3*cos(4*x)/16 - cos(6*x)/32 + 5/16\n    assert TRpower(sin(x)**7) == -cos(2*x)/32 - 7*cos(4*x)/32 + 35*cos(6*x)/32 + 7/32\n    assert"], "sample_300": ["    def test_filtering_on_summary_annotation(self):\n        q = Query(Ranking)\n        q.add_annotation(Func('COUNT', 'id'), 'num_ranks', is_summary=True)\n        with self.assertRaises(FieldError):\n            q.build_filter(('num_ranks', 10), can_reuse=None, allow_joins=False, check_filterable=False)\n", "    def test_set_annotation_mask(self):\n        query = Query(model=Author)\n        query.annotations = {'name': F('name')}\n        query.annotation_select_mask = {'name'}\n        self.assertEqual(query.annotation_select, {'name': F('name')})\n        query.set_annotation_mask(None)\n        self.assertIsNone(query.annotation_select_mask)\n        self.assertEqual(query.annotation_select, {})\n        query.set_annotation_mask({'name', 'age'})\n        self.assertEqual(query.annotation_select, {'name': F('name')})\n        query.annotation_select_mask = {'name', 'email'}\n        self.assertEqual(query.annotation_select, {'name': F('name'), 'email': F('email')})\n", "    def test_count_aggregation(self):\n        query = Query(Ranking)\n        self.assertEqual(query.get_count('default'), 0)\n        Ranking.objects.create(name='test')\n        self.assertEqual(query.get_count('default'), 1)\n", "    def test__lookup_joins(self):\n        query = Query(Author)\n        query.add_filter('name__exact', 'John')\n        self.assertEqual(query._lookup_joins, [query.get_initial_alias()])\n", "    def test_join_promotion(self):\n        # Test that join promotion works correctly.\n        q = Query(Author)\n        q.add_filter(('name', 'John'))\n        q.add_filter(('name', 'John'))\n        q.add_filter(('age', '25'))\n        q.add_filter(('age', '25'))\n\n        # The two 'name' filters should promote the join to INNER.\n        self.assertEqual(q.alias_map['name'].join_type, INNER)\n\n        # The 'age' filters should promote the join to LOUTER.\n        self.assertEqual(q.alias_map['age'].join_type, LOUTER)\n\n        # Check that join promotion works when the query has a subquery.\n        subq = Query(Author)\n        subq.add_filter(('name', 'John'))\n        subq = subq.exists()\n        q = Query(Author)\n        q.add_filter(('name', 'John'))\n        q.add_q(subq)\n\n        # The subquery's join should be promoted to INNER.\n        self.assertEqual(subq.alias_map['name'].join_type, INNER)\n\n        # The outer query's join should be promoted to LOUTER.\n        self.assertEqual(q.alias_map['name'].join_type, LOUTER)\n"], "sample_1045": ["def test_Rational_1_3():\n    assert Rational(1, 3) == Rational(1, 3)\n    assert Rational(1, 3) != Rational(2, 3)\n    assert Rational(2, 6) == Rational(1, 3)\n    assert Rational(3, 9) == Rational(1, 3)\n    assert Rational(1, 3) + 1 == Rational(4, 3)\n    assert Rational(1, 3) - 1 == Rational(-2, 3)\n    assert Rational(1, 3) * 2 == Rational(2, 3)\n    assert Rational(1, 3) / 2 == Rational(1, 6)\n", "def test_fuzzy_not():\n    assert fuzzy_not(S.False)\n    assert fuzzy_not(S.True)\n    assert fuzzy_not(S.Zero)\n    assert fuzzy_not(S.One)\n    assert fuzzy_not(nan)\n    assert fuzzy_not(S.Infinity)\n    assert fuzzy_not(S.NegativeInfinity)\n    assert fuzzy_not(S.ComplexInfinity)\n    assert fuzzy_not(S.NaN)\n    assert fuzzy_not(EulerGamma)\n    assert fuzzy_not(Catalan)\n    assert fuzzy_not(GoldenRatio)\n    assert fuzzy_not(TribonacciConstant)\n    assert fuzzy_not(ImaginaryUnit)\n    assert fuzzy_not(Pi)\n    assert fuzzy_not(S.Exp1)\n    assert fuzzy_not(S.Zero)\n    assert fuzzy_not(S.One)\n    assert fuzzy_not(nan)\n    assert fuzzy_not(S.Infinity)\n    assert fuzzy_not(S.NegativeInfinity)\n    assert fuzzy_not(S.ComplexInfinity)\n    assert fuzzy_not(S.NaN)\n    assert fuzzy_not(Exp1)\n    assert fuzzy_not(Pi)\n", "    def test_gcd_euclid(self):\n        assert igcd(10, 15) == 5\n        assert igcd(15, 10) == 5\n        assert igcd(0, 10) == 10\n        assert igcd(10, 0) == 10\n        assert igcd(0, 0) == 0\n        assert igcd(-10, -15) == 5\n        assert igcd(-10, 15) == 5\n        assert igcd(-15, 10) == 5\n", "def test_Float_representation():\n    # Test the constructor can handle the usual cases\n    assert Float(3) == Float('3')\n    assert Float(3.0) == Float('3.0')\n    # Test the constructor can handle the automatic conversion from int\n    assert Float(3) == Float('3')\n    assert Float(3.0) == Float('3.0')\n    # Test the constructor can handle the conversion of floats with high precision\n    assert Float('1.23456789') == Float(1.23456789)\n    # Test the constructor can handle the conversion of floats to Float with low\n    # precision\n    assert Float('1e-3', 3) == Float(0.001)\n    # Test the constructor can handle the conversion of floats to Float with very\n    # low precision\n    assert Float('1e-300', 1) == Float(1e-300)\n    # Test the constructor can handle the conversion of floats with very high\n    # precision\n    assert Float('1e-300', 300) == Float(1e-300)\n    # Test the constructor can handle the conversion of floats with NaN\n    assert Float(nan) == Float('nan')\n    # Test the constructor can handle the conversion of floats with positive infinity\n    assert Float(oo) == Float('inf')\n    # Test the constructor can handle the conversion of floats with negative infinity\n    assert Float(-oo) == Float('-inf')\n    # Test the constructor can handle the conversion of floats with very high\n    # precision and NaN\n    assert Float('nan', 300) == Float('nan')\n    # Test the constructor can handle the conversion of floats with very high\n    # precision and positive infinity\n    assert Float('inf', 300) == Float('inf')\n    # Test the constructor can handle the conversion of floats with very high\n    # precision and negative infinity\n    assert Float('-inf', 300) == Float('-inf')\n    # Test the constructor can handle the conversion of floats with very high\n    # precision and very large exponent\n    assert Float('1e300', 300) == Float('inf')\n    # Test the constructor can handle the conversion of floats with very high\n    # precision and very small exponent\n    assert Float('1e-300', 300) == Float('inf')\n    # Test the constructor can handle the conversion of floats with", "    def test_init(self):\n        f = Float(0, 100)\n        assert f == Float(0)\n"], "sample_1071": ["def test_quantity_simplify_complex_expression():\n    from sympy.physics.units.util import quantity_simplify\n    from sympy.physics.units.prefixes import kilo, milli\n    from sympy.physics.units import meter, second, meter_per_second\n\n    # Test simplification of expressions involving prefixes\n    expr = kilo*meter + milli*meter\n    expected = 1000*meter + meter\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions involving quantities of the same dimension\n    expr = meter + meter_per_second\n    expected = meter + meter/second\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions involving quantities of different dimensions\n    expr = kilo*meter + kilo*second\n    expected = kilo*(meter + second)\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions involving powers of quantities\n    expr = (kilo*meter)**2 + meter\n    expected = kilo**2*meter**2 + meter\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions involving mixed units\n    expr = kilo*meter + 2*second\n    expected = kilo*meter + 2*second\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions with no prefixes\n    expr = meter + second\n    expected = meter + second\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions with only one quantity\n    expr = kilo*meter\n    expected = kilo*meter\n    assert quantity_simplify(expr) == expected\n\n    # Test simplification of expressions with a single prefix\n    expr = milli*meter\n    expected = 0.001*meter\n    assert quantity_simplify(expr) == expected\n", "def test_convert_to_mixed_units():\n    from sympy.physics.units import meter, gram, second, day\n    from sympy.physics.units import convert_to\n    expr = meter/gram\n    result = convert_to(expr, [meter, second])\n    assert result == second**2/meter, NS(str(result))\n    result = convert_to(expr, day)\n    assert result == meter/day, NS(str(result))\n    result = convert_to(expr, meter)\n    assert result == expr, NS(str(result))\n    result = convert_to(expr, gram)\n    assert result == expr, NS(str(result))\n", "def test_quantity_simplify_with_negative_prefixes():\n    from sympy.physics.units.prefixes import kilo, milli\n    from sympy.physics.units import quantity_simplify\n\n    expr = kilo * (milli * meter)\n    expected_expr = 0.001 * meter\n    assert quantity_simplify(expr) == expected_expr\n\n", "def test_quantity_simplify_prefixes():\n    \"\"\"Test quantity_simplify with prefixes.\"\"\"\n    from sympy.physics.units.prefixes import kilo, centi, milli, micro, nano, pico\n    from sympy.physics.units import Quantity\n    # test units with prefixes\n    expr = kilo*centimeter\n    assert quantity_simplify(expr) == Quantity('100*centimeter')\n\n    expr = kilo * centi * meter\n    assert quantity_simplify(expr) == Quantity('100*meter')\n\n    expr = kilo * meter\n    assert quantity_simplify(expr) == Quantity('1000*meter')\n\n    expr = milli * meter\n    assert quantity_simplify(expr) == Quantity('0.001*meter')\n\n    expr = micro * meter\n    assert quantity_simplify(expr) == Quantity('1.0*10**-6*meter')\n\n    expr = nano * meter\n    assert quantity_simplify(expr) == Quantity('1.0*10**-9*meter')\n\n    expr = pico * meter\n    assert quantity_simplify(expr) == Quantity('1.0*10**-12*meter')\n", "def test_quantity_simplify_multiple_units():\n    from sympy.physics.units import convert_to, quantity_simplify\n    from sympy.physics.units import kilogram, meter, second, meter/second\n\n    expr = 3*2*4*kilogram*(meter/second)\n    expected = 24*kilogram*(meter/second)\n    assert quantity_simplify(convert_to(expr, [kilogram, meter, second])) == expected\n"], "sample_467": ["    def test_omitted_from_data_with_year_and_month(self):\n        form = Form({\n            'year': '2009',\n            'month': '12',\n            'day': '',\n        })\n        self.assertTrue(self.widget.value_omitted_from_data(form.data, form.files, 'date'))\n", "    def test_clear_checkbox_name(self):\n        self.assertEqual(self.widget.clear_checkbox_name(\"foo\"), \"foo-clear\")\n", "    def test_init(self):\n        widget = self.widget\n        self.assertIsInstance(widget.widgets, tuple)\n        self.assertEqual(len(widget.widgets), 2)\n        self.assertIsInstance(widget.widgets[0], DateInput)\n        self.assertIsInstance(widget.widgets[1], TimeInput)\n", "    def test_format_value_year_none(self):\n        widget = self.widget\n        value = {\"year\": \"\", \"month\": \"January\", \"day\": \"15\"}\n        expected = {\"year\": \"\", \"month\": \"January\", \"day\": \"15\"}\n        self.assertEqual(widget.format_value(value), expected)\n", "    def test_rendering(self):\n        date = date(2022, 12, 25)\n        time = date.time()\n        expected_html = \"\"\"\n            <div>\n                <input id=\"id_year\" type=\"text\" name=\"year\" value=\"2022\" required>\n                <input id=\"id_month\" type=\"text\" name=\"month\" value=\"12\" required>\n                <input id=\"id_day\" type=\"text\" name=\"day\" value=\"25\" required>\n                <input id=\"id_time\" type=\"time\" name=\"time\" value=\"00:00:00\" required>\n            </div>\n        \"\"\"\n        self.assertEqual(self.widget.render(\"test\", date, {}), expected_html)\n"], "sample_593": ["def variable():\n    arr = np.array([[1, 2, 3], [4, 5, 6]])\n    return xr.Variable((\"x\", \"y\"), arr)\n\n", "def test_inline_variable_array_repr():\n    da = xr.DataArray(np.random.randn(4, 6))\n    expected_repr = \"<numpy.ndarray> (4, 6)\"\n    assert inline_variable_array_repr(da, 35) == expected_repr\n", "def test_short_data_repr_html_scalar():\n    da = xr.DataArray(5, dims=[\"x\"])\n    assert \"5\" in short_data_repr_html(da)\n    assert \"<pre>\" not in short_data_repr_html(da)\n", "def test_collapsible_section():\n    # Test that collapsible section summary is rendered correctly\n    section = collapsible_section(\"Test Section\", \"This is some details\", \"\", 5)\n    assert \"Test Section\" in section\n    assert \"This is some details\" in section\n    assert \"5\" in section\n\n    # Test that collapsible section is disabled when no items\n    section = collapsible_section(\"Test Section\", \"\", \"\", 0)\n    assert \"disabled\" in section\n\n    # Test that collapsible section is collapsed when items exceeds max items to collapse\n    section = collapsible_section(\"Test Section\", \"This is some details\", \"\", 5)\n    assert \"checked\" in section\n\n    # Test that collapsible section is not collapsed when items is less than or equal to max items to collapse\n    section = collapsible_section(\"Test Section\", \"This is some details\", \"\", 5, collapsed=False)\n    assert \"checked\" not in section\n\n    # Test that collapsible section is not collapsed when disabled\n    section = collapsible_section(\"Test Section\", \"This is some details\", \"\", 5, enabled=False)\n    assert \"checked\" not in section\n", "def test_summarize_coord_multiindex():\n    # test that the icon is rendered correctly\n    ds = xr.Dataset({\"x\": multiindex})\n    coord = ds.coords[\"x\"]\n    summary = summarize_coord(\"x\", coord)\n    assert \"<svg\" in summary\n    assert \"xr-multiindex\" in summary\n\n    # test that the icon is rendered correctly when coords have multiple levels\n    ds = xr.Dataset({\"x\": xr.DataArray(multiindex, dims=[\"level_1\", \"level_2\"])})\n    coord = ds.coords[\"x\"]\n    summary = summarize_coord(\"x\", coord)\n    assert \"<svg\" in summary\n    assert \"xr-multiindex\" in summary\n\n    # test that the icon is not rendered when is_index is False\n    ds = xr.Dataset({\"x\": multiindex})\n    coord = ds.coords[\"x\"]\n    summary = summarize_coord(\"x\", coord, is_index=False)\n    assert \"<svg\" not in summary\n\n    # test that coords are rendered correctly\n    ds = xr.Dataset({\"x\": multiindex})\n    summary = summarize_coord(\"x\", ds.coords[\"x\"])\n    assert \"<dt><span>level_1 :</span></dt>\" in summary\n    assert \"<dt><span>level_2 :</span></dt>\" in summary\n"], "sample_712": ["def test_deprecation_warning_n_values():\n    # Test deprecation warning for n_values\n    X = np.array([[1, 2], [3, 4]])\n    encoder = OneHotEncoder(n_values='auto')\n    with warnings.catch_warnings(record=True) as w:\n        encoder.fit(X)\n        assert len(w) == 1\n        assert issubclass(w[0].category, DeprecationWarning)\n        assert \"Passing 'n_values' is deprecated in version 0.20\" in str(w[0].message)\n    # Test deprecation warning for n_values with categorical_features\n    encoder = OneHotEncoder(n_values='auto', categorical_features=[0, 1])\n    with warnings.catch_warnings(record=True) as w:\n        encoder.fit(X)\n        assert len(w) == 1\n        assert issubclass(w[0].category, DeprecationWarning)\n        assert \"Passing 'n_values' is deprecated in version 0.20\" in str(w[0].message)\n", "def test_OneHotEncoder_sparse_output():\n    \"\"\"Test OneHotEncoder output sparse matrix.\"\"\"\n    # Create a OneHotEncoder object\n    encoder = OneHotEncoder(sparse=True)\n\n    # Create a dataset with two features\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n\n    # Fit the encoder to the dataset\n    encoder.fit(X)\n\n    # Transform the dataset\n    encoded_X = encoder.transform(X)\n\n    # Check that the output is a sparse matrix\n    assert isinstance(encoded_X, sparse.spmatrix)\n\n    # Check that the sparse matrix has the correct shape\n    expected_shape = (len(X), encoder.categories_[0].shape[0] + encoder.categories_[1].shape[0])\n    assert encoded_X.shape == expected_shape\n\n    # Check that the sparse matrix has the correct non-zero values\n    expected_values = np.array([[1, 0, 1, 0, 0],\n                               [0, 1, 0, 0, 1],\n                               [0, 1, 0, 0, 1]])\n    assert_array_equal(toarray(encoded_X), expected_values)\n", "def test_OneHotEncoder_categories(categories, expected_categories):\n    encoder = OneHotEncoder(categories=categories)\n    encoder.fit([['a', 1]])\n    assert encoder.categories_ == expected_categories\n", "def test_handle_unknown_equal_error():\n    encoder = OneHotEncoder(handle_unknown='error')\n    X = [['Male'], ['Female']]\n    with pytest.raises(ValueError):\n        encoder.fit_transform(X)\n", "def test_ordinal_encoder_inv_transform_no_sparse():\n    X = np.array([[0, 2], [1, 0]], dtype=object)\n    enc = OrdinalEncoder()\n    enc.fit(X)\n    X_inv = enc.inverse_transform(X)\n    expected_result = np.array([['Female', 2], ['Male', 1]], dtype=object)\n    assert_array_equal(X_inv, expected_result)\n"], "sample_108": ["    def test_reverse_dict(self):\n        resolver = resolve('/')\n        reverse_dict = resolver.reverse_dict\n        self.assertIn('base64', reverse_dict)\n        self.assertIn('subpatterns-base64', reverse_dict['base64'])\n        self.assertIn('namespaced-base64', reverse_dict['base64']['namespaced-base64'])\n\n        self.assertEqual(reverse_dict['base64'].getlist('base64')[0], ('aGVsbG8=', '', {}))\n        self.assertEqual(reverse_dict['base64']['subpatterns-base64'].getlist('subpatterns/d29ybGQ=')[0], ('d29ybGQ=', '', included_kwargs))\n        self.assertEqual(reverse_dict['base64']['namespaced-base64'].getlist('namespaced/d29ybGQ=')[0], ('d29ybGQ=', '', included_kwargs))\n", "    def test_resolver_match(self):\n        urlconf = 'urlpatterns.path_urls'\n        resolver = get_resolver(urlconf=urlconf)\n        match = resolver.resolve('/')\n        self.assertEqual(match.func, empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {})\n", "    def test_reverse_lazy_url(self):\n        view = lambda request: empty_view\n        url = reverse('lazy_url', args=['hello', 'world'])\n        self.assertEqual(resolve(url).func, view)\n        self.assertEqual(resolve(url).kwargs, {'view': view})\n", "    def test_reverse_dict(self):\n        resolver = resolve('/base64/aGVsbG8=/')\n        self.assertEqual(resolver.reverse_dict['base64'], [\n            [('url', 'base64', {'value': b'hello'}, {'value': b'hello'}),\n             ('url', 'subpattern-base64', {'value': b'hello'}, {'value': b'hello'})],\n        ])\n", "    def test_resolve_with_urlconf_module(self):\n        urlconf = 'urlpatterns.urlconf'\n        resolver = get_resolver(urlconf=urlconf)\n        self.assertEqual(resolver.urlconf_name, urlconf)\n"], "sample_531": ["def test_subplots_adjust():\n    # The test with the correct parameters\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(121)\n    ax3 = fig.add_subplot(122)\n    assert fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9,\n                             wspace=0.2, hspace=0.2) is None\n    assert fig.get_layout_engine().get_info() == {\n        'w_pad': 0.2, 'h_pad': 0.2, 'wspace': 0.2, 'hspace': 0.2}\n\n    # The test with some of the parameters set to None\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(121)\n    ax3 = fig.add_subplot(122)\n    fig.subplots_adjust(left=None, right=0.9, bottom=0.1, top=0.9,\n                       wspace=None, hspace=None)\n    assert fig.get_layout_engine().get_info() == {\n        'w_pad': 0.0, 'h_pad': 0.0, 'wspace': 0.0, 'hspace': 0.0}\n\n    # The test with some of the parameters set to None and some to the default\n    # value\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(121)\n    ax3 = fig.add_subplot(122)\n    fig.subplots_adjust(left=None, right=0.9, bottom=0.1, top=0.9,\n                       wspace=None, hspace=0.2)\n    assert fig.get_layout_engine().get_info() == {\n        'w_pad': 0.0, 'h_pad': 0.2, 'wspace': 0.0, 'hspace': 0.2}\n\n    # The test with a custom figure with some of the parameters set to None\n    # and some to the default value\n    fig = plt.figure(figsize=(10, 5))\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(121)\n    ax3 = fig.add_subplot(122)\n    fig.subplots_adjust(left=None, right=0.9", "def test_tight_layout_adjusted_to_bbox_inches():\n    # Create a figure with a tight layout\n    fig = Figure(figsize=(6, 4))\n    fig.tight_layout()\n    # Set the bbox_inches to a value smaller than the figure size\n    fig.bbox_inches = (1, 1, 2, 2)\n    # Assert that the tight layout is adjusted accordingly\n    assert fig.tight_layout(rect=(0, 0, 1, 1))\n", "def test_figure_dpi(dpi, expected_dpi):\n    fig = Figure(dpi=dpi)\n    assert fig.dpi == expected_dpi\n    fig.set_dpi(300)\n    assert fig.dpi == 300\n", "def test_figure_subplot_polar():\n    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n    # Add some data\n    theta = np.linspace(0, 2*np.pi, 100)\n    ax.plot(theta, np.sin(theta))\n    # Check if the subplot has the correct projection\n    assert ax.get_projection_name() == 'polar'\n    # Check if the figure has a polar axes\n    assert fig._axstack.current().get_projection_name() == 'polar'\n\n    # Create a new figure with a polar subplot\n    fig2, ax2 = plt.subplots(subplot_kw={'projection': 'polar'})\n    # Add some data\n    theta2 = np.linspace(0, 2*np.pi, 100)\n    ax2.plot(theta2, np.sin(theta2))\n    # Check if the figure has a polar axes\n    assert fig2._axstack.current().get_projection_name() == 'polar'\n\n    # Try to create a non-polar subplot in a polar figure\n    with pytest.raises(ValueError):\n        plt.subplots(subplot_kw={'projection': 'rectilinear'})\n\n    # Try to create a polar subplot in a non-polar figure\n    assert fig._axstack.current().get_projection_name() != 'polar'\n", "def test_tight_layout_engine(tmp_path, tight_layout_engine):\n    \"\"\"\n    Ensure tight layout engine works when set on a figure.\n    \"\"\"\n    # Create a figure with a tight layout engine\n    fig = plt.figure(tight_layout=tight_layout_engine)\n\n    # Verify that the tight layout engine is applied\n    assert fig.get_layout_engine().name == \"tight\" if tight_layout_engine else None\n\n    # Close the figure\n    plt.close(fig)\n"], "sample_928": ["    def test_heading_empty_string(self):\n        env = Environment()\n        result = heading(env, '')\n        assert result == ''\n", "def test_heading():\n    env = Environment()\n    env.filters['heading'] = heading\n    result = env.from_string('{{ heading(\"Hello World\", 1) }}').render()\n    assert result.startswith(\"Hello World\")\n    assert result.endswith(\"=\" * 12)\n    env.filters['heading'] = heading\n    result = env.from_string('{{ heading(\"Hello World\", 2) }}').render()\n    assert result.startswith(\"Hello World\")\n    assert result.endswith(\"-\" * 10)\n    env.filters['heading'] = heading\n    result = env.from_string('{{ heading(\"Hello World\", 3) }}').render()\n    assert result.startswith(\"Hello World\")\n    assert result.endswith(\"~\" * 8)\n", "def test_heading_environmentfilter_language():\n    env = Environment()\n    env.filters.update({\n        'language': lambda: 'ja',\n    })\n    result = heading(env, 'Heading')\n    expected = 'Heading==========\\n'\n    assert result == expected\n", "def test_default_role():\n    # Test default_role function\n    with default_role('test_doc', 'nonexistent_role'):\n        assert not roles.role('nonexistent_role', english, 0, Reporter('', 4, 4))\n    with default_role('test_doc', 'uri'):\n        assert roles.role('uri', english, 0, Reporter('', 4, 4))\n", "def test_heading():\n    env = Environment()\n    env.language = 'en'\n    assert heading(env, 'Hello') == \"Hello=\\n====================================================\\n\"\n    assert heading(env, 'Hello', level=3) == \"Hello...\\n....\\n\"\n\n    env.language = 'ja'\n    assert heading(env, 'Hello') == \"Hello=\\n=====================================\\n\"\n    assert heading(env, 'Hello', level=3) == \"Hello...\\n....\\n\"\n\n    env.language = 'zh_CN'\n    assert heading(env, 'Hello') == \"Hello=\\n=====================================\\n\"\n    assert heading(env, 'Hello', level=3) == \"Hello...\\n....\\n\"\n\n    env.language = 'fr'\n    assert heading(env, 'Bonjour') == \"Bonjour=\\n=====================================================\\n\"\n    assert heading(env, 'Bonjour', level=3) == \"Bonjour...\\n....\\n\"\n"], "sample_590": ["def test_dataset_concat_compat_equals():\n    da1 = DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = DataArray([4, 5, 6], dims=[\"x\"])\n    assert_identical(\n        concat([da1, da2], \"x\", data_vars=\"all\", coords=\"all\", compat=\"equals\"),\n        DataArray([1, 2, 3, 4, 5, 6], dims=[\"x\"]),\n    )\n", "def test_concat_dataset_with_labeled_concat_dim():\n    da1 = DataArray([1, 2, 3], dims=['a'])\n    da2 = DataArray([4, 5, 6], dims=['a'])\n    da3 = DataArray([7, 8, 9], dims=['a'])\n\n    da4 = DataArray([10, 11, 12], dims=['b'])\n    da5 = DataArray([13, 14, 15], dims=['b'])\n\n    ds1 = Dataset({'a': da1})\n    ds2 = Dataset({'a': da2})\n    ds3 = Dataset({'a': da3})\n\n    ds4 = Dataset({'b': da4})\n    ds5 = Dataset({'b': da5})\n\n    result = concat([ds1, ds2, ds3], dim='c', coords='all', positions=[0, 1, 2])\n    expected = Dataset({\n        'a': DataArray([1, 2, 3, 4, 5, 6, 7, 8, 9], dims=['c', 'a']),\n        'c': DataArray([0, 1, 2], dims=['c'])\n    })\n    assert_equal(result, expected)\n\n    result = concat([ds4, ds5], dim='c', coords='all', positions=[0, 1, 2])\n    expected = Dataset({\n        'b': DataArray([10, 11, 12, 13, 14, 15], dims=['c', 'b']),\n        'c': DataArray([0, 1, 2], dims=['c'])\n    })\n    assert_equal(result, expected)\n\n    result = concat([ds1, ds4], dim='c', coords='all', positions=[0, 1, 2])\n    expected = Dataset({\n        'a': DataArray([1, 2, 3, np.nan, np.nan, np.nan], dims=['c', 'a']),\n        'b': DataArray([np.nan, np.nan, np.nan, 10, 11, 12], dims=['c', 'b']),\n        'c': DataArray([0, 1, 2], dims=['c'])\n    })\n    assert_equal(result, expected)\n", "def test_concat_compat_equals():\n    da1 = DataArray([1, 2, 3], dims=['x'])\n    da2 = DataArray([4, 5, 6], dims=['x'])\n    da1.name = 'var1'\n    da2.name = 'var1'\n\n    with pytest.raises(ValueError):\n        concat([da1, da2], compat='equals')\n", "def test_concat_dim_coord_coord_variable():\n    da1 = DataArray([1, 2, 3], dims=['x'])\n    da2 = DataArray([4, 5, 6], dims=['x'])\n    ds1 = Dataset({'x': da1})\n    ds2 = Dataset({'x': da1, 'concat_dim': da2})\n\n    result = concat([ds1, ds2], dim='concat_dim', data_vars='minimal', coords='minimal')\n    assert_array_equal(result.coords, {'concat_dim': [4, 5, 6]})\n\n    result = concat([ds1, ds2], dim='concat_dim', data_vars='minimal', coords='all')\n    assert_array_equal(result.coords, {'concat_dim': [4, 5, 6], 'x': [1, 2, 3]})\n", "def test_concat_compat_no_conflicts():\n    \"\"\"Ensure that when compat='no_conflicts', all non-null values are kept.\"\"\"\n    # Create two datasets with overlapping values\n    x = np.arange(3)\n    y = np.arange(3, 6)\n    ds1 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 1))})\n    ds2 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 2))})\n\n    # Concatenate them with compat='no_conflicts'\n    expected_ds = Dataset(\n        {\n            \"x\": (\"x\", np.arange(6)),\n            \"y\": (\n                \"x\",\n                np.array([1, 2, 1, 2, 1, 2]),\n            ),\n        }\n    )\n\n    assert_equal(concat(ds1, ds2, compat=\"no_conflicts\"), expected_ds)\n\n    # Now check that non-null values from the first dataset are kept\n    ds1 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 1))})\n    ds2 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 2))})\n    ds2[\"y\"].data[-1] = np.nan  # Make last value null in second dataset\n\n    expected_ds = Dataset(\n        {\n            \"x\": (\"x\", np.arange(3)),\n            \"y\": (\"x\", np.array([1, 2, 1])),\n        }\n    )\n\n    assert_equal(concat(ds1, ds2, compat=\"no_conflicts\"), expected_ds)\n\n    # And non-null values from the second dataset are also kept\n    ds1 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 2))})\n    ds2 = Dataset({\"x\": (\"x\", x), \"y\": (\"x\", np.full_like(x, 1))})\n    ds2[\"y\"].data[0] = np.nan  # Make first value null in second dataset\n\n    expected_ds = Dataset(\n        {\n            \"x\": (\"x\", np.arange(3)),\n            \"y\": (\"x\", np.array([2, 2, 1])),\n        }\n    )\n\n    assert_equal(concat(ds1, ds2, compat=\"no_conflicts\"), expected_ds)\n\n    # Now"], "sample_550": ["def test_warn_toolclasses(axes_class, expected_warning):\n    with pytest.warns(UserWarning, match=re.escape(expected_warning)):\n        axes_class.__new__(axes_class)\n", "    def test_xlim(self, plt):\n        ax = plt.subplot(1, 1, 1)\n        ax.set_xlim(0, 1)\n        assert ax.get_xlim() == (0, 1)\n        assert ax.set_xlim(0, 1) == (0, 1)\n        ax.set_xlim(0, 2)\n        assert ax.get_xlim() == (0, 2)\n        assert ax.set_xlim(0, 1) == (0, 1)\n        ax.set_xlim(2, 1)\n        assert ax.get_xlim() == (1, 2)\n", "    def test_masking(self, image_comparison):\n        x = np.linspace(0, 2*np.pi, 100)\n        y = np.sin(x)\n\n        fig, ax = plt.subplots()\n        ax.plot(x, y, color='blue', alpha=0.5)\n\n        ax.fill_between(x, 0, np.cos(x), where=np.cos(x) > 0, color='red')\n        ax.fill_between(x, 0, np.cos(x), where=np.cos(x) < 0, color='green')\n\n        ax.set_xlim(0, np.pi)\n        ax.set_ylim(-1.1, 1.1)\n\n        ax.masked_where(np.cos(x) <= 0, np.cos(x))\n        ax.masked_where(np.cos(x) >= 0, np.cos(x))\n", "def test_grid_no_ticks():\n    # Arrange\n    fig, ax = plt.subplots()\n    ax.grid(True)\n\n    # Act and Assert\n    ax.grid(False)\n    assert ax.grid() is False\n", "def test_clear():\n    # set up a figure\n    fig, ax = plt.subplots()\n    \n    # set some properties on the axes\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\n    \n    # set some properties on the axis\n    ax.xaxis.set_label_position('top')\n    ax.yaxis.set_label_position('right')\n    \n    # verify that the clear method removes all these properties\n    ax.clear()\n    \n    assert ax.get_xlabel() is None\n    assert ax.get_ylabel() is None\n    assert ax.get_title() is None\n    assert ax.xaxis.get_label_position() == 'bottom'\n    assert ax.yaxis.get_label_position() == 'left'\n"], "sample_1151": ["def test_modulus_with_negative_divisor():\n    x, y = symbols('x,y')\n    assert Mod(x, -y).args == (x, -y)\n    assert Mod(-x, -y).args == (x, y)\n", "def test_Mod_with_negative_dividend_and_divisor():\n    from sympy.core.numbers import nan\n    from sympy.testing.randtest import verify_numerically\n    from . import Mod\n\n    x = symbols('x')\n\n    # Test with negative dividend and positive divisor\n    mod_expr1 = Mod(-5, 3)\n    expected_result = -2\n    assert verify_numerically(mod_expr1, expected_result)\n\n    # Test with negative dividend and negative divisor\n    mod_expr2 = Mod(-5, -3)\n    expected_result = -2\n    assert verify_numerically(mod_expr2, expected_result)\n\n    # Test with negative dividend and zero divisor (should raise an error)\n    mod_expr3 = Mod(-5, 0)\n    with raises(ZeroDivisionError):\n        mod_expr3\n\n    # Test with NaN dividend and positive divisor\n    mod_expr4 = Mod(nan, 3)\n    expected_result = nan\n    assert verify_numerically(mod_expr4, expected_result)\n\n    # Test with NaN dividend and negative divisor\n    mod_expr5 = Mod(nan, -3)\n    expected_result = nan\n    assert verify_numerically(mod_expr5, expected_result)\n\n    # Test with NaN dividend and zero divisor\n    mod_expr6 = Mod(nan, 0)\n    expected_result = nan\n    assert verify_numerically(mod_expr6, expected_result)\n", "def test_modulus_with_zero():\n    from sympy import symbols, Mod, S\n    a, x, y = symbols('a,x,y')\n    assert Mod(0, x).is_zero\n    assert Mod(x, 0).is_zero\n    assert Mod(0, 0).is_zero\n    with raises(ZeroDivisionError):\n        Mod(x, 0)\n    assert Mod(1, 1) == 1\n    assert Mod(2, 1) == 1\n    assert Mod(-1, 1) == 0\n    assert Mod(-2, 1) == 0\n    assert Mod(x, 1) == x\n    assert Mod(x, x) == 0\n    assert Mod(x, -1) == x\n    assert Mod(x, -x) == 0\n", "def test_Mod_as_Mul():\n    from sympy.abc import x, y, z\n    # Test that Mod(a*b, c) is not converted to a*b % c\n    from sympy import Mod\n    from sympy import Mul\n    assert Mod(x*y, z).func is Mod, \"Mod(x*y, z) should not be converted to x*y % z\"\n    assert Mod(x*y, z).count(Mul) == 1, \"Mod(x*y, z) should not be converted to a Mul\"\n", "def test_Mod_modulo_by_zero():\n    from sympy.abc import x, y\n    Mod(x, 0)\n"], "sample_1099": ["def test_partial_derivative_multiple_variables_multiple_contracted():\n    A = TensorHead(\"A\", [L])\n    i, j, k, m = tensor_indices(\"i j k m\", L)\n    partial_A_ij = PartialDerivative(A(i, j), i)\n    partial_A_jm = PartialDerivative(A(i, j), m)\n    result = partial_A_ij * partial_A_jm\n\n    expected_result = PartialDerivative(A(i, j, k), i)\n    expected_result = expected_result.doit()\n\n    assert result == expected_result\n", "def test_PD_doit():\n    # Test doit method with single variable\n    i, j = tensor_indices(\"i j\", L)\n    expr = PartialDerivative(A(i), j)\n    result = expr.doit()\n    expected = A(i, j)\n    assert result == expected\n\n    # Test doit method with multiple variables\n    i, j, k = tensor_indices(\"i j k\", L)\n    expr = PartialDerivative(A(i), (j, k))\n    result = expr.doit()\n    expected = A(i, j, k)\n    assert result == expected\n\n    # Test doit method with nested PartialDerivative\n    i, j, k = tensor_indices(\"i j k\", L)\n    expr = PartialDerivative(PartialDerivative(A(i), j), k)\n    result = expr.doit()\n    expected = A(i, j, k)\n    assert result == expected\n\n    # Test doit method with multiple variables and mixed type of indices\n    i, j, k, m, m1, m2 = tensor_indices(\"i j k m m1 m2\", L)\n    expr = PartialDerivative(A(i, j), (m, m1))\n    result = expr.doit()\n    expected = A(i, j, m, m1)\n    assert result == expected\n\n    # Test doit method with a tensorhead with multiple free indices\n    i, j, k = tensor_indices(\"i j k\", L)\n    expr = PartialDerivative(A(i, j, k), j)\n    result = expr.doit()\n    expected = PartialDerivative(A(i, L_0, k), L_0)\n    assert result == expected\n", "def test_partial_derivative_multiple_variables_real_number_coefficients():\n    x, y, z = symbols('x y z')\n    A = TensorHead('A', [L])\n    B = TensorHead('B', [L])\n    C = TensorHead('C', [L])\n\n    # Create a tensor expression with a real number coefficient\n    expr = 3 * A(i) * B(j) * C(k)\n    pd_expr = PartialDerivative(expr, x)\n\n    assert pd_expr.doit().free_symbols == {x, y, z}\n    assert pd_expr.doit().expr.free_symbols == {x, y, z}\n    assert pd_expr.get_free_indices() == [i, j, k]\n\n    # Test that the doit method correctly handles multiple variables\n    pd_expr = PartialDerivative(expr, (x, y))\n    assert pd_expr.doit().free_symbols == {x, y, z}\n    assert pd_expr.doit().expr.free_symbols == {x, y, z}\n    assert pd_expr.get_free_indices() == [i, j, k]\n\n    # Test that the doit method correctly handles multiple variables with one of them being a symbolic expression\n    pd_expr = PartialDerivative(expr, (x, y*z))\n    assert pd_expr.doit().free_symbols == {x, y, z}\n    assert pd_expr.doit().expr.free_symbols == {x, y, z}\n    assert pd_expr.get_free_indices() == [i, j, k]\n\n    # Test that the doit method correctly handles multiple variables with one of them being a symbolic expression that is a product\n    pd_expr = PartialDerivative(expr, (x*(y*z), y))\n    assert pd_expr.doit().free_symbols == {x, y, z}\n    assert pd_expr.doit().expr.free_symbols == {x, y, z}\n    assert pd_expr.get_free_indices() == [i, j, k]\n\n    # Test that the doit method correctly handles multiple variables with one of them being a symbolic expression that is a power\n    pd_expr = PartialDerivative(expr, (x**2, y))\n    assert pd_expr.doit().free_symbols == {x, y, z}\n    assert pd_expr.doit().expr.free_symbols == {x, y, z}\n    assert pd_expr.get_free_indices() == [i, j, k]\n\n    # Test that the doit method correctly handles multiple variables with one of them", "def test_partial_derivative_contraction():\n    x, y, z = symbols('x y z')\n    A, B, C, D = tensor_heads(\"A B C D\", [L])\n\n    # Test case 1: partial derivative of a tensor with a free index\n    # that is contracted with another tensor\n    PD = PartialDerivative(A(i), y)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 2: partial derivative of a tensor with a dummy index\n    # that is contracted with another tensor\n    PD = PartialDerivative(A(k), k)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 3: partial derivative of a tensor with a dummy index\n    # that is contracted with a tensor head\n    PD = PartialDerivative(A(k), k)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 4: partial derivative of a tensor with a free index\n    # that is contracted with a tensor head\n    PD = PartialDerivative(A(i), i)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 5: partial derivative of a tensor with a free index\n    # that is contracted with a tensor with a free index\n    PD = PartialDerivative(A(i), i)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 6: partial derivative of a tensor with a dummy index\n    # that is contracted with a tensor with a free index\n    PD = PartialDerivative(A(k), i)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 7: partial derivative of a tensor with a dummy index\n    # that is contracted with a tensor with a dummy index\n    PD = PartialDerivative(A(k), k)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 8: partial derivative of a tensor with a dummy index\n    # that is contracted with a tensor with a dummy index and a free index\n    PD = PartialDerivative(A(k), i)\n    assert PD.expr.free_symbols == {x, y, z}\n\n    # Test case 9: partial derivative of a tensor with a free index\n    # that is contracted with a tensor with a dummy", "def test_PD_get_indices_contraction():\n    \"\"\"\n    Test that get_indices returns the correct indices after contraction.\n\n    \"\"\"\n    # Test with indices of different valence\n    i, j = tensor_indices(\"i j\", L)\n    expr = PartialDerivative(A(i), A(j))\n    assert expr.get_indices() == [i, -j]\n\n    # Test with contractions\n    expr = PartialDerivative(A(i), A(i))\n    assert expr.get_indices() == [L_0, -L_0]\n\n    # Test with multiple contractions\n    expr = PartialDerivative(A(i), A(j)) + PartialDerivative(A(i), A(j))\n    assert expr.get_indices() == [i, -j]\n\n    # Test with mixed contractions\n    expr = PartialDerivative(A(i), A(i)) + PartialDerivative(A(i), A(j))\n    assert expr.get_indices() == [L_0, -j]\n\n    # Test with repeated indices\n    expr = PartialDerivative(A(i), A(j)) * PartialDerivative(A(i), A(j))\n    assert expr.get_indices() == [i, -j]\n\n    # Test with indices of different valence in variables\n    i, j, k = tensor_indices(\"i j k\", L)\n    H = TensorHead(\"H\", [L, L, L])\n    expr = PartialDerivative(H(i, j, k), H(i, j, k))\n    assert expr.get_indices() == [i, -i, j, -j, k, -k]\n\n    # Test with non-tensor variables\n    x = symbols('x')\n    expr = PartialDerivative(A(i), x)\n    assert expr.get_indices() == [i]\n\n    # Test with complex numbers\n    expr = PartialDerivative(A(i), 3.5 * A(j))\n    assert expr.get_indices() == [i]\n"], "sample_863": ["def test_PipelineCacheWithPassthroughTransformer():\n    # Create a pipeline with a passthrough transformer\n    pipeline = Pipeline([('passthrough', 'passthrough'), ('estimator', LogisticRegression())])\n    # Fit the pipeline\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    pipeline.fit(X, y)\n    # Check that the passthrough transformer is not cloned\n    assert pipeline.steps[0][1] is 'passthrough'\n    # Check that the estimator is cloned\n    assert pipeline.steps[1][1] is not LogisticRegression()\n    # Test that we can't fit a passthrough transformer\n    with assert_raise_message(ValueError, \"Last step of Pipeline should implement fit \"\n                                        \"or be the string 'passthrough'. \"\n                                        \"'passthrough' (type <class 'str'>) \"\n                                        \"doesn't\", pipeline.fit, 'passthrough'):\n        pipeline.fit_transform(X, y)\n", "    def test_cache_pipeline(self):\n        # Create a pipeline with caching\n        pipeline = make_pipeline(StandardScaler(), DummyEstimatorParams())\n        cache_dir = mkdtemp()\n        pipeline.memory = cache_dir\n        X = np.array([[1, 2], [3, 4]])\n\n        # Fit the pipeline\n        pipeline.fit(X)\n\n        # Check that the transformer is cached\n        with open(joblib.Memory(cache_dir).cachedir + \"/scaler.pkl\", 'rb') as f:\n            cached_scaler = joblib.load(f)\n\n        # Check that the cached transformer is the same as the original\n        assert_allclose(pipeline.steps[0][1].mean_, cached_scaler.mean_)\n\n        # Check that the pipeline is still working after the cache is cleared\n        joblib.Memory(cache_dir).clear()\n        assert_array_equal(pipeline.transform(X), pipeline.transform(X))\n", "def test_Pipeline_copy_attributes():\n    pipeline = Pipeline([('est', DummyEstimatorParams())])\n    new_pipeline = pipeline.copy()\n    assert new_pipeline.steps == pipeline.steps\n    assert new_pipeline.named_steps == pipeline.named_steps\n    assert new_pipeline._estimator_type == pipeline._estimator_type\n    assert new_pipeline._final_estimator == pipeline._final_estimator\n\n    # Check that a copy can be modified independently\n    new_pipeline['est'].got_attribute = True\n    assert not pipeline['est'].got_attribute\n\n", "def test_Pipeline_fit_transform_untransformed_X():\n    \"\"\"Test fit_transform of a pipeline with an estimator that does not\n    expect X to be pre-transformed.\"\"\"\n    X = np.array([1, 2, 3])\n    y = np.array([1, 2, 3])\n\n    p = Pipeline(steps=[('transformer', Transf()), ('estimator', LinearRegression())])\n    Xt, fit_params = p._fit(X, y, **{})\n    assert_array_equal(Xt, X)\n    assert fit_params == {}\n", "def test_Pipeline_transform():\n    \"\"\"Test that transform raises a ValueError when the final estimator\n    is not a callable.\n\n    \"\"\"\n    p = Pipeline([('foo', TransfFitParams())])\n    with assert_raise_message(ValueError,\n                            \"Last step of Pipeline should implement fit \"\n                            \"or be the string 'passthrough'. \"\n                            \"'TransfFitParams' (type <class 'sklearn.pipeline.tests.test_pipeline.TransfFitParams'>)\"\n                            \" doesn't\"):\n        p.transform([[1, 2]])\n"], "sample_206": ["    def test_field_file_init(self):\n        from django.core.files.storage import default_storage\n        from .models import Document\n\n        document = Document()\n        field = Document._meta.get_field('file')\n        field_file = field.attr_class(document, field, None)\n        self.assertEqual(field_file.storage, default_storage)\n        self.assertIsNone(field_file.name)\n        self.assertTrue(field_file._committed)\n", "    def test_field_file_hashing(self):\n        field = models.FileField(upload_to='uploads/')\n        instance = models.Model()\n        field_name = field.name\n        field_value = field_value = FieldFile(instance, field, 'file_name')\n        self.assertNotEqual(hash(field_value), hash('file_name'))\n", "    def test_field_file_size(self):\n        # Create a temporary file with a specific size\n        with tempfile.NamedTemporaryFile(mode='wb') as f:\n            f.write(b'x' * 1024 * 1024)  # 1MB\n            f.flush()\n\n            # Create a model instance with a FileField\n            doc = Document(file=temp.File(f.name))\n\n            # Check that the size of the file attribute matches the actual size\n            self.assertEqual(doc.file.size, 1024 * 1024)\n\n            # Save the instance to the database\n            doc.save()\n\n            # Check that the size of the file attribute still matches the actual size\n            self.assertEqual(doc.file.size, 1024 * 1024)\n\n            # Delete the file from the storage\n            doc.file.delete(save=False)\n\n            # Check that the size of the file attribute is None\n            self.assertIsNone(doc.file.size)\n\n            # Save the instance to the database again\n            doc.save()\n\n            # Check that the size of the file attribute is still None\n            self.assertIsNone(doc.file.size)\n", "    def test_field_file_close_file(self):\n        file = FieldFile(None, models.FileField(), 'test.txt')\n        self.assertFalse(file.closed)\n        file.close()\n        self.assertTrue(file.closed)\n", "    def test_field_file_close(self):\n        with tempfile.NamedTemporaryFile() as f:\n            file = FieldFile(None, None, 'tempfile')\n            file.file = open(f.name, 'wb')\n            file.save('tempfile', file.file, save=False)\n            file.close()\n            self.assertFalse(file.closed)\n            file.close()\n            self.assertTrue(file.closed)\n"], "sample_532": ["def test_contour_labeler_zorder():\n    \"\"\"Check that contour labels have the correct zorder\"\"\"\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    cs = ax.contour(X, Y, Z, [0, 1, 2])\n    cs.clabel(levels=[0, 1], inline_spacing=0, inline=False)\n    for i, text in enumerate(cs.labelTexts):\n        assert text.get_zorder() == (2 + i + 1)\n", "def test_contour_levels():\n    x = np.linspace(0, 10, 10)\n    y = np.linspace(0, 10, 10)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n\n    cs = axs[0].contour(X, Y, Z, levels=5)\n    axs[0].set_title(\"Default levels\")\n\n    cs = axs[1].contour(X, Y, Z, levels=[1, 2, 3])\n    axs[1].set_title(\"Custom levels\")\n\n    for ax in axs:\n        ax.set_aspect('equal')\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n\n    plt.show()\n", "def test_contour_find_nearest_contour(filled, origin):\n    # Create a sample contour\n    if filled:\n        fig, ax = plt.subplots()\n        ax.contourf(np.random.rand(10, 10), origin=origin)\n    else:\n        fig, ax = plt.subplots()\n        ax.contour(np.random.rand(10, 10), origin=origin)\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    # Get the contour set\n    if filled:\n        contour_set = fig.gca().contourf\n    else:\n        contour_set = fig.gca().contour\n\n    # Get the reference point\n    x, y = 0.5, 0.5\n\n    # Find the nearest contour point\n    conmin, segmin, imin, xmin, ymin, d2min = contour_set.find_nearest_contour(x, y)\n\n    # Check if the contour point is in the correct contour\n    contour = contour_set.collections[conmin]\n    path = contour.get_paths()[segmin]\n    assert_array_almost_equal_nulp(xmin, path.vertices[imin, 0])\n    assert_array_almost_equal_nulp(ymin, path.vertices[imin, 1])\n\n    # Check if the contour is the closest to the reference point\n    d2min_other = np.inf\n    for i in range(len(contour_set.collections)):\n        for j in range(len(contour_set.collections[i].get_paths())):\n            path = contour_set.collections[i].get_paths()[j]\n            d2 = np.sum((path.vertices - [x, y]) ** 2)\n            if d2 < d2min_other:\n                d2min_other = d2\n                conmin_other, segmin_other, imin_other, xmin_other, ymin_other, _ = contour_set.find_nearest_contour(x, y, i, True)\n                assert_array_almost_equal_nulp(xmin_other, path.vertices[imin_other, 0])\n                assert_array_almost_equal_nulp(ymin_other, path.vertices[imin_other, 1])\n\n    # Check if the distance to the closest contour point is smaller than to other contour points\n    for i in range(len(contour_set.collections)):\n        for j in range(len(contour_set.collections[i", "def test_contour_labeler_inline_spacing():\n    x, y = np.meshgrid(np.linspace(0, 10, 10), np.linspace(0, 10, 10))\n    z = np.sin(x) * np.cos(y)\n    levels = [0, 1, 2]\n    with rc_context(rc={'font.size': 12}):\n        fig, ax = plt.subplots()\n        cs = ax.contour(x, y, z, levels, cmap='viridis')\n        cs.clabel(levels=levels, inline=True, inline_spacing=2)\n\n        # Test that the labels are correctly spaced\n        for text in cs.labelTexts:\n            assert text.get_window_extent().width > 10\n\n        # Test that the labels are correctly placed\n        for i, text in enumerate(cs.labelTexts):\n            x, y = text.get_position()\n            assert 0.5 < x < 1.5\n            assert 0.5 < y < 1.5\n            if i == 0:\n                assert 0.7 < x < 1.3\n                assert 1.7 < y < 2.3\n            elif i == 1:\n                assert 4.7 < x < 5.3\n                assert 1.7 < y < 2.3\n            elif i == 2:\n                assert 8.7 < x < 9.3\n                assert 1.7 < y < 2.3\n\n        # Test that the contour is removed where the label is placed\n        for collection in cs.collections:\n            for path in collection.get_paths():\n                assert path.vertices.shape[0] == 2\n\n        # Test that the labels are correctly rotated\n        for text in cs.labelTexts:\n            assert 90 < text.get_rotation() < 270\n\n        # Test that the labels are correctly formatted\n        for text in cs.labelTexts:\n            assert text.get_text() in ['0.0', '1.0', '2.0']\n", "def test_contour_labeler_add_label_near():\n    # Create a test data\n    x, y = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 10, 100))\n    z = np.sin(np.sqrt(x**2 + y**2))\n\n    fig, ax = plt.subplots()\n    cs = ax.contourf(x, y, z)\n\n    # Test that a label is added near a contour point\n    cs.clabel(inline=False)\n    label = cs.labelTexts[0]\n    assert label.get_text() == \"1.000\"\n\n    # Test that a label is added near a contour point at a different location\n    cs.add_label_near(x=5, y=5, inline=False)\n    label = cs.labelTexts[1]\n    assert label.get_text() == \"1.000\"\n\n    # Test that a label is not added too close to another label\n    cs.add_label_near(x=5, y=5, inline=False)\n    with pytest.raises(ValueError):\n        cs.add_label_near(x=5.01, y=5, inline=False)\n\n    # Test that a label is not added too close to the edge of the figure\n    cs.add_label_near(x=10.1, y=5, inline=False)\n    with pytest.raises(ValueError):\n        cs.add_label_near(x=10.1, y=5, inline=False)\n\n    # Test that a label can be added near a contour point with a custom format\n    cs.labelFmt = ticker.FuncFormatter(lambda x, p: f\"{x:.2f}\")\n    cs.clabel(inline=False)\n    label = cs.labelTexts[0]\n    assert label.get_text() == \"1.00\"\n\n    # Test that a label can be added near a contour point with a custom format and\n    # a specified rotation\n    cs.add_label_near(x=5, y=5, inline=False, rotation=45)\n    label = cs.labelTexts[-1]\n    assert label.get_rotation() == 45\n\n    # Test that a label can be added near a contour point with a custom format and\n    # a specified rotation, and that the rotation is updated when the axes aspect\n    # ratio changes\n    with rc_context(rc={'image.origin': 'upper'}):\n        cs.add_label_near(x=5, y=5, inline=False, rotation=45)\n"], "sample_566": ["def test_subfigure_tight_layout():\n    fig = Figure(figsize=(8, 6))\n    fig.set_constrained_layout(True)\n    fig.add_subplot(1, 2, 1)\n    fig.add_subplot(1, 2, 2)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        fig.tight_layout()\n    fig2 = Figure(figsize=(8, 6))\n    fig2.set_constrained_layout(True)\n    fig2.add_subplot(1, 2, 1)\n    fig2.add_subplot(1, 2, 2)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        fig2.tight_layout(rect=[0.1, 0.1, 0.8, 0.8])\n    assert fig.get_constrained_layout()\n    assert not fig2.get_constrained_layout()\n", "def test_constrained_layout_engine():\n    fig, axs = plt.subplots(3, 3, figsize=(10, 6))\n\n    # Case 1: Constrained layout with default parameters\n    for i in range(3):\n        for j in range(3):\n            axs[i, j].plot([1, 2, 3], [4, 5, 6])\n\n    fig.tight_layout()\n\n    # Case 2: Constrained layout with custom parameters\n    fig2, axs2 = plt.subplots(3, 3, figsize=(10, 6))\n    for i in range(3):\n        for j in range(3):\n            axs2[i, j].plot([1, 2, 3], [4, 5, 6])\n    fig2.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8)\n    fig2.tight_layout()\n\n    # Case 3: Constrained layout with custom parameters and labels\n    fig3, axs3 = plt.subplots(3, 3, figsize=(10, 6))\n    for i in range(3):\n        for j in range(3):\n            axs3[i, j].plot([1, 2, 3], [4, 5, 6])\n            axs3[i, j].set_title(f'Row {i}, Col {j}')\n\n    fig3.tight_layout()\n\n    # Case 4: Tight layout\n    fig4, axs4 = plt.subplots(3, 3, figsize=(10, 6))\n    for i in range(3):\n        for j in range(3):\n            axs4[i, j].plot([1, 2, 3], [4, 5, 6])\n    fig4.tight_layout()\n\n    # Case 5: Constrained layout with custom parameters and tight layout\n    fig5, axs5 = plt.subplots(3, 3, figsize=(10, 6))\n    for i in range(3):\n        for j in range(3):\n            axs5[i, j].plot([1, 2, 3], [4, 5, 6])\n    fig5.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8)\n    fig5.tight_layout()\n", "def test_constrained_layout_pads():\n    # Create a figure with a constrained layout\n    fig, ax = plt.subplots(2, 2, figsize=(8, 8), constrained_layout=True)\n\n    # Create some data\n    x = np.linspace(0, 2 * np.pi, 400)\n    y1 = np.sin(x)\n    y2 = np.cos(x)\n\n    # Plot the data\n    ax[0, 0].plot(x, y1)\n    ax[0, 1].plot(x, y2)\n    ax[1, 0].plot(x, y1)\n    ax[1, 1].plot(x, y2)\n\n    # Save the figure to a file\n    with io.BytesIO() as f:\n        plt.savefig(f, format='png')\n        f.seek(0)\n\n    # Read the image\n    img = Image.open(f)\n\n    # Check that the layout is constrained\n    assert img.size == (800, 800)\n\n    # Check that the figure size is correct\n    assert fig.get_size_inches() == (8, 8)\n\n    # Check that the layout engine is constrained\n    assert fig.get_layout_engine().get() == {'w_pad': 3.0 / 72.0,\n                                            'h_pad': 3.0 / 72.0,\n                                            'wspace': 0.2,\n                                            'hspace': 0.2}\n\n    # Check that the constrained layout pads are correct\n    w_pad, h_pad, wspace, hspace = fig.get_constrained_layout_pads(relative=True)\n    assert w_pad == 3.0 / 72.0\n    assert h_pad == 3.0 / 72.0\n    assert wspace == 0.2\n    assert hspace == 0.2\n\n    # Check that the figure can be saved to a file\n    with io.BytesIO() as f:\n        fig.savefig(f, format='png')\n        f.seek(0)\n        img = Image.open(f)\n        assert img.size == (800, 800)\n\n    # Check that the figure can be saved to a file with a custom size\n    fig.set_size_inches((10, 10))\n    with io.BytesIO() as f:\n        fig.savefig(f, format='png')\n        f.seek(0)\n        img = Image.open(f)\n        assert img", "def test_figure_subplot_mosaic():\n    # Test the figure.subplot_mosaic method\n    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\n    # Create a mosaic with 2x2 subplots\n    subplotspec = gridspec.GridSpec(2, 2)\n    ax1, ax2, ax3, ax4 = ax[:].subplots(1, 4)\n\n    # Add some data to the subplots\n    ax1.plot([1, 2, 3])\n    ax2.plot([4, 5, 6])\n    ax3.plot([7, 8, 9])\n    ax4.plot([10, 11, 12])\n\n    # Use the subplot_mosaic method to create a mosaic with the same subplots\n    mosaic = [['A', 'B'],\n              ['C', 'D']]\n    new_axes = fig.subplot_mosaic(mosaic)\n    assert set(new_axes.values()) == {ax1, ax2, ax3, ax4}\n\n    # Check that the subplots are correctly labeled\n    assert new_axes['A'] == ax1\n    assert new_axes['B'] == ax2\n    assert new_axes['C'] == ax3\n    assert new_axes['D'] == ax4\n\n    # Check that the figure layout is correct\n    assert fig.get_constrained_layout()\n    assert fig.get_layout_engine() == ConstrainedLayoutEngine()\n\n    # Clean up\n    plt.close(fig)\n", "def test_figure_pickle():\n    # Test that we can pickle a Figure object.\n    fig = Figure()\n    # Make sure that this Figure has an Axes associated with it.\n    fig.add_subplot(111)\n    state = pickle.dumps(fig)\n    newfig = pickle.loads(state)\n    assert len(newfig.axes) == 1  # Axes was present, so should still be present\n    newfig.clear()\n    newfig.add_subplot(111)\n    assert len(newfig.axes) == 1  # Axes should be present after clear and add\n"], "sample_990": ["def test_hyperbolic_functions():\n    from sympy.functions.elementary.hyperbolic import sinh, cosh, tanh, coth, \\\n        csch, sech, asinh, acosh, atanh, acoth, asech, acsch\n\n    x = symbols('x')\n    y = symbols('y')\n\n    assert (sinh(x + y)).args[0] == x + y\n    assert (cosh(x + y)).args[0] == x + y\n    assert (tanh(x + y)).args[0] == x + y\n    assert (coth(x + y)).args[0] == x + y\n    assert (csch(x + y)).args[0] == x + y\n    assert (sech(x + y)).args[0] == x + y\n    assert (asinh(x + y)).args[0] == x + y\n    assert (acosh(x + y)).args[0] == x + y\n    assert (atanh(x + y)).args[0] == x + y\n    assert (acoth(x + y)).args[0] == x + y\n    assert (asech(x + y)).args[0] == x + y\n    assert (acsch(x + y)).args[0] == x + y\n\n    assert (sinh(x + y + z)).args[0] == x + y + z\n    assert (cosh(x + y + z)).args[0] == x + y + z\n    assert (tanh(x + y + z)).args[0] == x + y + z\n    assert (coth(x + y + z)).args[0] == x + y + z\n    assert (csch(x + y + z)).args[0] == x + y + z\n    assert (sech(x + y + z)).args[0] == x + y + z\n    assert (asinh(x + y + z)).args[0] == x + y + z\n    assert (acosh(x + y + z)).args[0] == x + y + z\n    assert (atanh(x + y + z)).args[0] == x + y + z\n    assert (acoth(x + y + z)).args[0] == x + y + z\n   ", "def test_sech_derivative():\n    x = symbols('x')\n    sech = sech(x)\n    assert sech.diff(x) == -tanh(x)*sech(x)\n\n", "def test_tanh_complex():\n    assert tanh(x).is_complex and tanh(x).is_real_imag and tanh(x).as_real_imag()[1] == 0\n", "def test_csch():\n    x = Symbol('x')\n    assert csch(x).fdiff(1) == -coth(x)*csch(x)\n    assert csch(x).fdiff(2) == -csch(x)*coth(x)**2 + csch(x)**3\n    assert csch(x).taylor_term(1, x) == 2 * (-2) * 1 * x\n    assert csch(1).taylor_term(1, x) == 2 * (-2) * 1 * x\n    assert csch(1).taylor_term(3, x) == 2 * (-2) * 1 * x**3\n    assert csch(2).taylor_term(3, x) == 2 * (-2) * 1 * x**3\n", "    def test_sech(self):\n        assert sech(0).evalf() == 1.0\n        assert sech(1).evalf() == 0.52109532202\n        assert sech(-1).evalf() == 0.52109532202\n        assert sech(2).evalf() == 0.09004736013\n"], "sample_831": ["def test_export_text_multiclass_unbalanced():\n    # Make a classifier with unbalanced classes\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    y2 = [[-1, 1], [-1, 1], [-1, 1], [1, 2], [1, 2], [1, 3]]\n    w = [1, 1, 1, .5, .5, .5]\n    y_degraded = [1, 1, 1, 1, 1, 1]\n    X2 = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y3 = [-1, -1, -1, 1, 1, 1]\n    w2 = [1, 1, 1, 1, 1, 1]\n    y4 = [[-1, 1], [-1, 1], [-1, 1], [1, 2], [1, 2], [1, 3]]\n    X3 = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y5 = [[-1, 1], [-1, 1], [-1, 1], [1, 2], [1, 2], [1, 3]]\n    X4 = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y6 = [[-1, 1], [-1, 1], [-1, 1], [1, 2], [1, 2], [1, 3]]\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n    clf.fit(X2, y3)\n    clf.fit(X3, y5)\n    clf.fit(X4, y", "def test_export_text_fails_when_tree_is_not_fitted():\n    decision_tree = DecisionTreeClassifier()\n    assert_raises_regex(NotFittedError,\n                        \"DecisionTreeClassifier is not fitted yet.\",\n                        export_text, decision_tree)\n", "def test_export_graphviz_unfitted_decision_tree():\n    # No error should be raised\n    from sklearn.tree import DecisionTreeClassifier\n    decision_tree = DecisionTreeClassifier()\n    assert_export_graphviz_unfitted(decision_tree)\n", "def test_export_graphviz_proportion():\n    \"\"\"Tests that the proportion is displayed correctly in export_graphviz\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = [1, 0]\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n    # test that the proportion is displayed as a fraction\n    proportion = export_graphviz(clf, max_depth=2, proportion=True)\n    assert_in('0.50', proportion)\n    # test that the proportion is displayed as a percentage when show_weights=True\n    proportion = export_graphviz(clf, max_depth=2, proportion=True,\n                                 show_weights=True)\n    assert_in('50.00%', proportion)\n", "def test_export_graphviz_binary_tree_with_proportion():\n    \"\"\"Test export graphviz for a binary tree with proportion=True\"\"\"\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    clf = DecisionTreeClassifier(random_state=0, max_depth=2)\n    clf.fit(X, y)\n\n    graph = export_graphviz(clf, proportion=True)\n    assert search(r'(\\d+\\.\\d+)', graph) is not None\n    assert search(r'(\\d+\\.\\d+%)"], "sample_8": ["    def test_addition(self):\n        masked_array = Masked(self.a, mask=self.mask_a)\n        assert_masked_equal(masked_array + 1, Masked(self.a + 1))\n        assert_masked_equal(masked_array + self.b, Masked(self.a + self.b))\n        assert_masked_equal(masked_array + masked_array, Masked(self.a + self.a))\n        assert_masked_equal(masked_array + Masked(self.b, mask=self.mask_b),\n                           Masked(self.a + self.b))\n        assert_masked_equal(masked_array + Masked(masked_array, mask=self.mask_a),\n                           Masked(self.a + self.a))\n", "compilation error", "    def test_masked_ufuncs(self, func, data, mask):\n        # Test that the masked array has the correct mask\n        masked_data = Masked(data, mask)\n        result = func(masked_data)\n        assert_masked_equal(result, Masked(func(data[~mask]), mask[~mask]))\n\n        # Test that the result is not masked\n        assert not result.mask.any()\n\n        # Test that the result is correct for the whole array\n        result = func(masked_data[~mask])\n        assert_array_equal(result, func(data[~mask]))\n\n   ", "    def test_masked_array_masking(self):\n        a = Masked(self.a, mask=self.mask_a)\n        assert_masked_equal(a, self.a)\n        assert_masked_equal(a.mask, self.mask_a)\n        assert_masked_equal(a.filled(0), self.a)\n        assert_masked_equal(a.filled(-1), self.a + -1)\n        assert_masked_equal(a.filled(np.nan), self.a)\n        assert_masked_equal(a.filled(None), self.a)\n        assert_masked_equal(a.filled(np.ma.masked), self.a)\n        assert_masked_equal(a.filled(0, copy=False), self.a)\n        assert_masked_equal(a.filled(-1, copy=False), self.a + -1)\n        assert_masked_equal(a.filled(np.nan, copy=False), self.a)\n        assert_masked_equal(a.filled(None, copy=False), self.a)\n        assert_masked_equal(a.filled(np.ma.masked, copy=False), self.a)\n\n        # Fill with a scalar array\n        assert_masked_equal(a.filled(self.a[0]), self.a)\n\n        # Fill with a scalar mask\n        assert_masked_equal(a.filled(~self.mask_a), self.a)\n        assert_masked_equal(a.filled(np.array([True, False, False])), self.a)\n\n        # Fill with an array of the wrong shape\n        with pytest.raises(ValueError):\n            a.filled(self.a[0, None])\n\n        # Fill with an array of the wrong dtype\n        with pytest.raises(TypeError):\n            a.filled(self.b)\n\n        # Fill with None in an incompatible shape\n        with pytest.raises(TypeError):\n            a.filled(None, copy=False)\n\n        # Fill with a Masked array in an incompatible shape\n        with pytest.raises(ValueError):\n            a.filled(Masked(self.a[0]), copy=False)\n", "    def test_copy(self):\n        masked_a = Masked(self.a, self.mask_a)\n        masked_b = masked_a.copy()\n        assert_masked_equal(masked_a, masked_b)\n"], "sample_914": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_161": ["    def test_related_field_db_type(self):\n        class Model(models.Model):\n            class Meta:\n                managed = True\n            field = models.IntegerField()\n\n        class Related(models.Model):\n            class Meta:\n                managed = True\n            foreign_key = models.ForeignKey(Model, on_delete=models.CASCADE)\n\n        self.assertEqual(RelatedField(field.remote_field).db_type(connection=self._get_connection()), None)\n", "    def test_check_many_to_many_field_without_unique_together(self):\n        class Parent(models.Model):\n            class Meta:\n                unique_together = [('field', 'other_field')]\n            field = models.CharField(max_length=10)\n\n        class Child(models.Model):\n            parent = models.ManyToManyField(Parent)\n\n        with self.assertWarns(DjangoWarning, msg='Setting unique=True on a ForeignKey has the same effect as using a OneToOneField.'):\n            Parent._meta.check()\n", "    def test_foreign_object_check(self):\n        # Create a model with a foreign object\n        class Model(models.Model):\n            class Meta:\n                managed = True\n            foreign = models.ForeignKey('self')\n\n        # Check if the check is raised\n        with self.assertWarns(DjangoWarning, msg=\"ManyToManyFields cannot be unique.\"):\n            check = Model._check_unique()\n            self.assertEqual(len(check), 1)\n\n        # Check if the check is not raised\n        class Model(models.Model):\n            class Meta:\n                managed = True\n            foreign = models.ForeignKey('self', unique=True)\n\n        check = Model._check_unique()\n        self.assertEqual(len(check), 0)\n", "    def test_m2m_ignored_options(self):\n        class Model(models.Model):\n            field = models.ManyToManyField('self')\n\n        with override_settings(DEBUG=False):\n            # The debug setting should have no effect on many-to-many fields.\n            checks = Model._default_manager.db_connection._check_models(Model)\n            self.assertEqual(checks, [])\n\n        with override_settings(DEBUG=True):\n            # The debug setting should also have no effect on many-to-many fields.\n            checks = Model._default_manager.db_connection._check_models(Model)\n            self.assertEqual(checks, [])\n", "    def setUp(self):\n        self.app_config = apps.get_app_config('invalid_models_tests')\n        self.app_config.apps_ready = True\n        self.app_config.apps.load_app_settings()\n"], "sample_504": ["def test_colorbar_extension_length():\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate((0.2, 0.3, 0.4)):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*3 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing='uniform')\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                            bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n\n", "def test_colorbar_extension_length():\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=0.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate((0.2, 0.3, 0.5)):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*3 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing='uniform')\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_extension_length_proportional_auto():\n    \"\"\"\n    Test colorbar extension length with proportional spacing and auto extension\n    fraction.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        # Create a subplot.\n        cax = fig.add_subplot(12, 1, i*3 + 3 + 1)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendfrac='auto',\n                 orientation='horizontal', spacing='proportional')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_extension_length_proportional_auto():\n    \"\"\"\n    Check for proportional spacing and \"auto\" extendfrac.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        # Create a subplot.\n        cax = fig.add_subplot(12, 1, i*3 + 1)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendfrac='auto',\n                 orientation='horizontal', spacing='proportional')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n\n", "def test_colorbar_alpha():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10))\n    cb = Colorbar(ax, im, alpha=0.5)\n    assert cb.alpha == 0.5\n    assert cb.solids.get_edgecolor() == 'face'\n    assert cb.solids.get_alpha() == 0.5\n    cb.remove()\n    cb2 = Colorbar(ax, im, alpha=np.array([0.5, 0.3, 0.7]))\n    assert cb2.alpha is None\n    assert cb2.solids.get_edgecolor() == 'face'\n    assert cb2.solids.get_alpha() is None\n    cb2.remove()\n"], "sample_1171": ["compilation error", "def test_ImageSet():\n    from sympy import symbols\n    n, m = symbols('n m')\n    ImageSet_Lambda = ImageSet(Lambda((n, m), n**2 + m**2), Range(1, 10), Range(1, 10))\n    assert ImageSet_Lambda.as_relational(x) == ImageSet_Lambda.lamda(x)\n    assert ImageSet_Lambda.lamda(2) == 5\n    assert ImageSet_Lambda.lamda(3) == 10\n    assert ImageSet_Lambda.lamda(1/2) == 5/4\n    assert ImageSet_Lambda.lamda(3/2) == 22/4\n", "    def test_Range_negative_step(self):\n        from sympy import oo\n        r = Range(10, 0, -3)\n        self.assertEqual(r.start, 10)\n        self.assertEqual(r.stop, 0)\n        self.assertEqual(r.step, -3)\n        self.assertTrue(r.contains(4))\n        self.assertFalse(r.contains(6))\n", "def test_ImageSet_lambda_signature():\n    x, y = symbols('x y')\n    lam = Lambda(x, y**2)\n    sets = S.Naturals\n    image_set = ImageSet(lam, sets)\n    assert image_set.lamda(x) == 1\n    assert image_set.lamda(y) == 1\n    assert image_set.lamda(x+1) == 4\n    assert image_set.lamda(x-1) == 0\n", "def test_ImageSet_union():\n    r = Range(1, 4)\n    s = FiniteSet(1, 2, 3, 4)\n    u = ImageSet(Lambda(x, x**2), r)\n    assert u.intersect(s).is_FiniteSet\n    u = ImageSet(Lambda(x, x**2), Union(r, s))\n    assert u.is_Union\n    assert u.args[0].is_ImageSet\n    assert u.args[0].base_sets[0].is_Range\n    assert u.args[0].base_sets[0].args[0].is_Range\n    assert u.args[0].base_sets[0].args[1].is_Range\n    assert u.args[1].is_ImageSet\n    assert u.args[1].base_sets[0].is_FiniteSet\n"], "sample_472": ["    def test_get_page(self):\n        \"\"\"Test the Paginator's get_page method.\"\"\"\n        article_list = Article.objects.all()[:10]\n        paginator = Paginator(article_list, 2)\n\n        # Test case 1: Valid page number\n        page = paginator.get_page(1)\n        self.assertIsInstance(page, Paginator.Page)\n        self.assertEqual(page.number, 1)\n\n        # Test case 2: Invalid page number (not an integer)\n        with self.assertRaises(Paginator.InvalidPage) as cm:\n            paginator.get_page('a')\n        self.assertEqual(cm.exception.args[0], 'That page number is not an integer')\n\n        # Test case 3: Invalid page number (out of range)\n        with self.assertRaises(Paginator.EmptyPage) as cm:\n            paginator.get_page(paginator.num_pages + 1)\n        self.assertEqual(cm.exception.args[0], 'That page contains no results')\n\n        # Test case 4: Empty first page\n        article_list = Article.objects.all()[:0]\n        paginator = Paginator(article_list, 2, allow_empty_first_page=False)\n        with self.assertRaises(Paginator.EmptyPage) as cm:\n            paginator.get_page(1)\n        self.assertEqual(cm.exception.args[0], 'That page contains no results')\n", "    def test_empty_list(self):\n        paginator = Paginator([], per_page=10)\n        self.check_paginator([0, 0, []], paginator)\n", "    def test_invalid_page_number_float(self):\n        paginator = Paginator(Article.objects.all(), 10)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.get_page(3.5)\n", "    def test_invalid_page_with_ellipsis_range(self):\n        # Test that a Paginator with an ellipsis range raises a PageNotAnInteger\n        # exception when given an invalid page number\n        paginator = Paginator(\n            object_list=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n            per_page=2,\n            orphans=0,\n            allow_empty_first_page=True,\n        )\n        with self.assertRaises(PageNotAnInteger):\n            list(paginator.get_elided_page_range(0))\n", "    def test_get_page(self):\n        article = Article.objects.create()\n        article_list = [article] * 11\n        paginator = Paginator(article_list, 5)\n\n        # Test that get_page() returns a Page object\n        page = paginator.get_page(1)\n        self.assertIsInstance(page, Paginator.Page)\n\n        # Test that get_page() returns a valid page when number is not an integer\n        page = paginator.get_page(2.5)\n        self.assertEqual(page.number, 1)\n\n        # Test that get_page() returns a valid page when number is out of range\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(3)\n\n        # Test that get_page() returns a valid page when object_list is empty\n        empty_article_list = Article.objects.none()\n        paginator = Paginator(empty_article_list, 5)\n        page = paginator.get_page(1)\n        self.assertIsInstance(page, Paginator.Page)\n        self.assertEqual(page.object_list, [])\n"], "sample_898": ["def test_coverage_error_multilabel():\n    X, y = make_multilabel_classification(n_samples=20, n_features=5,\n                                         n_classes=3, n_labels=2,\n                                         random_state=1)\n    y_pred = np.zeros((20, 3))\n    y_pred[np.arange(20), y] = 1\n\n    score = coverage_error(y, y_pred)\n    assert_greater(score, 0)\n    assert_less(score, 2)\n", "def test_coverage_error_binary():\n    X, y = make_multilabel_classification(n_samples=20, n_features=10,\n                                          n_classes=2, n_labels=2,\n                                          random_state=42)\n    sample_weight = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\n    score = coverage_error(y, X, sample_weight=sample_weight)\n    assert_greater(score, 0.0)\n\n    score = coverage_error(y, X, sample_weight=None)\n    assert_greater(score, 0.0)\n\n    y = np.zeros((20, 2))\n    score = coverage_error(y, X, sample_weight=None)\n    assert_greater(score, 0.0)\n\n    y = np.array([[1, 0], [0, 1], [1, 0], [0, 1]])\n    score = coverage_error(y, X, sample_weight=None)\n    assert_greater(score, 0.0)\n\n    y = np.array([[1, 1], [0, 0], [1, 1], [0, 0]])\n    score = coverage_error(y, X, sample_weight=None)\n    assert_greater(score, 0.0)\n\n    sample_weight = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n                              0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n    score = coverage_error(y, X, sample_weight=sample_weight)\n    assert_greater(score, 0.0)\n\n    sample_weight = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0", "def test_label_ranking_loss_binary():\n    # We can only use binary data for label ranking loss\n    X, y = make_multilabel_classification(n_samples=10, n_features=10,\n                                          n_classes=2, n_labels=1,\n                                          random_state=1)\n    y_score = np.random.rand(10, 2)\n    # Test that a ValueError is raised for multiclass data\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for non-binary labels\n    y = np.array([[1, 1, 1], [0, 0, 0]])\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for multilabel data\n    X, y = make_multilabel_classification(n_samples=10, n_features=10,\n                                          n_classes=2, n_labels=2,\n                                          random_state=1)\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for sparse data\n    y = csr_matrix(y)\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for non-binary score\n    y = np.array([[1, 0, 0], [0, 0, 1]])\n    y_score = np.random.rand(10, 3)\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for non-binary score (sparse)\n    y = csr_matrix(y)\n    y_score = np.random.rand(10, 3)\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for non-binary score (multilabel)\n    X, y = make_multilabel_classification(n_samples=10, n_features=10,\n                                          n_classes=3, n_labels=2,\n                                          random_state=1)\n    with assert_raise_message(ValueError, \"Data is not binary\"):\n        label_ranking_loss(y, y_score)\n\n    # Test that a ValueError is raised for non-binary score (sparse multilabel)\n    y", "def test_coverage_error_multilabel():\n    \"\"\"Test that the coverage error is correctly computed in multilabel case.\"\"\"\n    X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                         n_classes=3, n_labels=2,\n                                         random_state=42)\n    lb = LabelBinarizer().fit_transform(y)\n    y_score = np.array([[0.1, 0.3, 0.6], [0.2, 0.4, 0.4],\n                        [0.5, 0.3, 0.2], [0.1, 0.6, 0.3],\n                        [0.2, 0.3, 0.5], [0.4, 0.3, 0.3],\n                        [0.2, 0.6, 0.2], [0.3, 0.2, 0.5],\n                        [0.4, 0.5, 0.1], [0.1, 0.4, 0.5]])\n    score = coverage_error(lb, y_score)\n    assert_greater(score, 0, precision=1e-2)\n    # Best value is average number of labels per sample\n    assert_greater(score, 0.5, precision=1e-2)\n", "    def test_label_ranking_average_precision_score(self):\n        # Test that all labels are assigned a score\n        y_true = np.array([[1, 0, 1], [1, 0, 1]])\n        y_score = np.array([[0.9, 0.5, 0.8], [0.9, 0.5, 0.8]])\n        expected_score = 0.75  # (1/2 + 0/1 + 0/1) / 2 = 0.75\n        assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), expected_score)\n"], "sample_985": ["def test_min_max_infinity():\n    from sympy import Max, Min, S\n    assert Max(S.Infinity, S.Infinity) == S.Infinity\n    assert Min(S.Infinity, S.Infinity) == S.Infinity\n", "    def test_min_max_args(self):\n        x, y = symbols('x y')\n        assert Min(x, y) == Min(x, y)\n        assert Min(y, x) == Min(x, y)\n        assert Min(x, x) == x\n", "def test_sqrt():\n    x = Basic('x')\n    assert sqrt(x) == S.sqrt(x)\n    assert (sqrt(x)**2).simplify() == x\n", "    def test_max(self):\n        from sympy import max, oo\n        self.assertTrue(Max(1, 2, 3) == max(1, 2, 3))\n        self.assertTrue(Max(x, 2, 3) == Max(x, 2, 3))\n        self.assertTrue(Max(x, x, 3) == x)\n        self.assertTrue(Max(x, 2, x) == x)\n", "def test_Mean_IdentityFunction():\n    from sympy import Mean\n    x = symbols('x')\n    assert Mean(1, 2, 3, 4, 5) == (Sum([1, 2, 3, 4, 5], generate=False)/5)\n    assert Mean(x, x, x) == x\n    assert Mean(x**2, x**2) == x**2\n"], "sample_942": ["def test_parse_annotation():\n    env = Mock(spec=['ref_context'])\n    env.ref_context['py:module'] = 'module_name'\n    env.ref_context['py:class'] = 'class_name'\n    annotation = 'List[str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List[str]'\n\n    annotation = 'List[str, int]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4\n    assert isinstance(result[0], addnodes.desc_sig_punctuation)\n    assert result[0].astext() == '['\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == 'List'\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == ', '\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == 'str, int'\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == ']'\n\n    annotation = 'Dict[str, List[int]]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 6\n    assert isinstance(result[0], addnodes.desc_sig_punctuation)\n    assert result[0].astext() == '['\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == 'Dict'\n    assert isinstance(result[2], addnodes.desc_sig_punctuation)\n    assert result[2].astext() == ', '\n    assert isinstance(result[3], nodes.Text)\n    assert result[3].astext() == 'str'\n    assert isinstance(result[4], addnodes.desc_sig_punctuation)\n    assert result[4].astext() == ', '\n    assert isinstance(result[5], addnodes.desc_sig_punctuation)\n    assert result[5].astext() == ']'\n\n    annotation = 'Union[int, str]'\n    result = _parse_annotation(annotation, env)\n    assert len(result) == 4\n    assert isinstance(result[0], addnodes.desc_sig_punctuation)\n    assert result[0].astext() == '['\n    assert isinstance(result[1], nodes.Text)\n    assert result[1].astext() == 'Union'\n    assert isinstance(result[2", "def test_pydomain_parse_annotation():\n    env = Mock()\n    env.ref_context.get.return_value = None\n    env.ref_context.setdefault.return_value = None\n\n    annotation = \"List[int]\"\n    expected = [\n        nodes.Text(\"List\"),\n        addnodes.desc_sig_punctuation('['),\n        nodes.Text('int'),\n        addnodes.desc_sig_punctuation(']')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = \"None\"\n    result = _parse_annotation(annotation, env)\n    assert result == [type_to_xref(annotation, env)]\n\n    annotation = \"Union[int, str]\"\n    expected = [\n        nodes.Text(\"Union\"),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"int\"),\n        addnodes.desc_sig_punctuation(\", \"),\n        nodes.Text(\"str\"),\n        addnodes.desc_sig_punctuation(\"]\")\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = \"Dict[str, int]\"\n    expected = [\n        nodes.Text(\"Dict\"),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"str\"),\n        addnodes.desc_sig_punctuation(\", \"),\n        nodes.Text(\"int\"),\n        addnodes.desc_sig_punctuation(\"]\")\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = \"Tuple[int, str]\"\n    expected = [\n        nodes.Text(\"Tuple\"),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"int\"),\n        addnodes.desc_sig_punctuation(\", \"),\n        nodes.Text(\"str\"),\n        addnodes.desc_sig_punctuation(\"]\")\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = \"List[Dict[str, int]]\"\n    expected = [\n        nodes.Text(\"List\"),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"Dict\"),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"str\"),\n        addnodes.desc_sig_punctuation(\", \"),\n        nodes.Text(\"int\"),\n        addnodes.desc_sig_punctuation(\"]\"),\n        addnodes.desc_sig_punctuation(\"]\")\n    ]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = \"typing.Union[int, str]\"\n    expected = [\n        type_to_xref(\"typing.Union\", env),\n        addnodes.desc_sig_punctuation(\"[\"),\n        nodes.Text(\"int\"),\n        addnodes", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context.get.return_value = 'module_name'\n\n    # Test simple case\n    annotation = 'int'\n    expected = ['class', 'int']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test list annotation\n    annotation = '[int]'\n    expected = ['class', 'int']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test tuple annotation\n    annotation = '(int, str)'\n    expected = ['class', 'int', 'class', 'str']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test Ellipsis annotation\n    annotation = '...'\n    expected = ['class', '...']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test unsupported syntax\n    annotation = 'invalid syntax'\n    with pytest.raises(SyntaxError):\n        _parse_annotation(annotation, env)\n\n    # Test None annotation\n    annotation = 'None'\n    expected = ['obj', 'None']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test module context\n    annotation = 'module.module'\n    expected = ['class', 'module']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    # Test complex annotation\n    annotation = '[int, str]'\n    expected = ['class', 'int', 'class', 'str']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    # Test that type annotation parsing works correctly\n    env = Mock(spec=['ref_context'])\n    env.ref_context.get.return_value = 'module'\n    env.ref_context.get.side_effect = lambda x: 'module' if x == 'py:module' else None\n\n    # Test simple type annotation\n    annotation = 'int'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, nodes.Text(annotation))\n\n    # Test nested type annotation\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation, env)\n    expected = [addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('List'),\n                addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('int'),\n                addnodes.desc_sig_punctuation('', ']'),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert_node(result, *expected)\n\n    # Test type annotation with ellipsis\n    annotation = '... | int'\n    result = _parse_annotation(annotation, env)\n    expected = [addnodes.desc_sig_punctuation('', \"...\"),\n                nodes.Text(\"|\"),\n                nodes.Text('int')]\n    assert_node(result, *expected)\n\n    # Test type annotation with subscript\n    annotation = 'List[int][0]'\n    result = _parse_annotation(annotation, env)\n    expected = [addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('List'),\n                addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('int'),\n                addnodes.desc_sig_punctuation('', ']'),\n                addnodes.desc_sig_punctuation('', '['),\n                nodes.Text('0'),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert_node(result, *expected)\n\n    # Test type annotation with tuple\n    annotation = '(int, str)'\n    result = _parse_annotation(annotation, env)\n    expected = [addnodes.desc_sig_punctuation('', '('),\n                nodes.Text('int'),\n                addnodes.desc_sig_punctuation('', ','),\n                nodes.Text('str'),\n                addnodes.desc_sig_punctuation('', ')')]\n    assert_node(result, *expected)\n\n    # Test type annotation with invalid syntax\n    annotation = 'invalid syntax'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, type_to_xref(annotation, env))\n", "def test_parse_arglist_syntax_error():\n    # Test that an empty argument list raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function()')\n\n    # Test that an argument list with unbalanced brackets raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function([,])')\n\n    # Test that an argument list with a closing bracket without an opening bracket raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function]()')\n\n    # Test that an argument list with an opening bracket without a closing bracket raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function([')\n\n    # Test that an argument list with a closing bracket after an opening bracket but without an argument in between raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function([])')\n\n    # Test that an argument list with a comma without an argument before it raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(),')\n\n    # Test that an argument list with an argument that contains a comma but is not enclosed in brackets raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(arg1, arg2)')\n\n    # Test that an argument list with a default value without an argument name raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(arg1=1)')\n\n    # Test that an argument list with a default value without a type annotation raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(arg1: int = 1)')\n\n    # Test that an argument list with a default value with a type annotation and a default value raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(arg1: int = 1)')\n\n    # Test that an argument list with a variable positional argument without a type annotation raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(*arg1)')\n\n    # Test that an argument list with a variable keyword argument without a type annotation raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(**arg1)')\n\n    # Test that an argument list with a variable positional argument and a variable keyword argument without type annotations raises a SyntaxError\n    with pytest.raises(SyntaxError):\n        parse('my_function(*arg1, **arg2)')\n\n    # Test"], "sample_818": ["def test_spectral_clustering_assign_labels_kmeans_with_empty_data():\n    \"\"\"Check that a warning is raised when assigning labels to empty data.\"\"\"\n    with pytest.warns(UserWarning):\n        clustering = SpectralClustering(assign_labels='kmeans')\n        clustering.fit([])\n\n", "def test_fit_on_sparse_matrix():\n    from scipy.sparse import csr_matrix\n    X = csr_matrix(np.array([[1, 1], [2, 1], [1, 0],\n                            [4, 7], [3, 5], [3, 6]]))\n    clustering = SpectralClustering(n_clusters=2, affinity='precomputed',\n                                   random_state=0).fit(X)\n    labels = clustering.labels_\n    expected_labels = np.array([1, 1, 1, 0, 0, 0])\n    assert_array_equal(labels, expected_labels)\n", "def test_eigen_solver(eigen_solver):\n    X, _ = make_blobs(n_samples=100, centers=5, n_features=2, random_state=42)\n    sc = SpectralClustering(n_clusters=5, eigen_solver=eigen_solver)\n    sc.fit(X)\n    assert sc.labels_.shape[0] == X.shape[0]\n", "def test_spectral_clustering_eigen_solver_is_none(eigen_solver):\n    # Test that eigen_solver is None\n    clustering = SpectralClustering(eigen_solver=eigen_solver)\n    with pytest.warns(UserWarning):\n        clustering.fit(np.array([[1, 2], [3, 4]]))\n    assert clustering.eigen_solver is None\n", "def test_discretize_similarity():\n    \"\"\"Test discretize function with similarity matrix\"\"\"\n    rng = np.random.RandomState(0)\n    n_samples, n_clusters = 100, 3\n    vectors = rng.rand(n_samples, n_clusters)\n\n    # Create a similarity matrix\n    similarity_matrix = rbf_kernel(vectors)\n    similarity_matrix = (similarity_matrix + similarity_matrix.T) / 2\n\n    labels = discretize(similarity_matrix)\n    assert_equal(len(set(labels)), n_clusters)\n"], "sample_435": ["    def test_read_only_password_hash_widget(self):\n        form = UserChangeForm(instance=User.objects.get(username=\"testclient\"))\n        self.assertTrue(form.fields[\"password\"].widget.read_only)\n", "    def test_password_reset_form_email_context(self):\n        form = PasswordResetForm(data={\"email\": \"testclient@example.com\"})\n        form.save()\n        self.assertEqual(len(mail.outbox), 1)\n        email_message = mail.outbox[0]\n        self.assertEqual(email_message.subject, \"Password reset email\")\n        self.assertIn(\"testclient@example.com\", email_message.body)\n        self.assertIn(\"testclient@example.com\", email_message.alternatives[0][1])\n", "    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashField()\n        self.assertTrue(form.disabled)\n        self.assertFalse(form.readonly)\n        self.assertIsInstance(form.widget, ReadOnlyPasswordHashWidget)\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_user(username=\"testclient\", password=\"password\")\n        cls.u2 = User.objects.create_user(username=\"inactive\", password=\"password\", is_active=False)\n        cls.u3 = User.objects.create_user(username=\"staff\", password=\"password\")\n        cls.u4 = User.objects.create(username=\"empty_password\", password=\"\")\n        cls.u5 = User.objects.create(username=\"unmanageable_password\", password=\"$\")\n        cls.u6 = User.objects.create(username=\"unknown_password\", password=\"foo$bar\")\n", "    def test_read_only_password_hash_widget(self):\n        form = ReadOnlyPasswordHashField()\n        context = form.get_context(\"password\", \"pbkdf2_sha256$180000$cR1IyUWkEwVv$z+cUy08j19g3sIuSbJ2DlWqOrzKQ9H9f8fMMjldCms=\", {})\n        self.assertEqual(context[\"summary\"][0][\"label\"], \"No password set.\")\n"], "sample_1136": ["def test_from_QQ_gmpy():\n    # Test conversion from GMPY mpq object\n    from sympy.polys.domains import QQ_gmpy\n    a = QQ_gmpy(1, 2)\n    K = EX()\n    result = K.from_QQ_gmpy(K, a, QQ_gmpy)\n    assert result.ex == a\n", "def test_expression_domain_numer_denom():\n    K = EX\n    a = K(1/2)\n    assert a.as_expr().as_numer_denom() == (1, 2)\n    assert a.numer().as_expr() == 1\n    assert a.denom().as_expr() == 2\n\n    b = K(1/3)\n    assert b.as_expr().as_numer_denom() == (1, 3)\n    assert b.numer().as_expr() == 1\n    assert b.denom().as_expr() == 3\n\n    c = K(1/6)\n    assert c.as_expr().as_numer_denom() == (1, 6)\n    assert c.numer().as_expr() == 1\n    assert c.denom().as_expr() == 6\n", "def test_expression_domain_operations():\n    # Test the is_positive method\n    domain = EX\n    assert domain.is_positive(domain.one)  # Should return True\n    assert not domain.is_positive(domain.zero)  # Should return False\n\n    # Test the is_negative method\n    assert not domain.is_negative(domain.one)  # Should return False\n    assert domain.is_negative(domain.zero)  # Should return True\n\n    # Test the is_nonpositive method\n    assert not domain.is_nonpositive(domain.one)  # Should return False\n    assert domain.is_nonpositive(domain.zero)  # Should return True\n\n    # Test the is_nonnegative method\n    assert domain.is_nonnegative(domain.one)  # Should return True\n    assert not domain.is_nonnegative(domain.zero)  # Should return False\n\n    # Test the numer and denom methods\n    a = EX(1/2)\n    assert domain.numer(a) == EX(1)  # Should return the numerator\n    assert domain.denom(a) == EX(2)  # Should return the denominator\n\n    # Test the gcd method\n    assert domain.gcd(domain.one, domain.zero) == domain.one  # Should return 1\n    assert domain.gcd(domain.zero, domain.zero) == domain.zero  # Should return 0\n\n    # Test the lcm method\n    assert domain.lcm(domain.one, domain.zero) == domain.zero  # Should return 0\n    assert domain.lcm(domain.zero, domain.zero) == domain.zero  # Should return 0\n\n    # Test the gcd_list and lcm_list methods\n    a = EX(6)\n    b = EX(12)\n    assert domain.gcd_list([a, b]) == EX(6)  # Should return the gcd of the elements\n    assert domain.lcm_list([a, b]) == EX(12)  # Should return the lcm of the elements\n\n    # Test the to_sympy and from_sympy methods\n    assert domain.to_sympy(domain.one) == S(1)  # Should return the SymPy equivalent\n    assert domain.from_sympy(S(1)) == domain.one  # Should return the EX equivalent\n\n    # Test the from_ZZ_python method\n    assert domain.from_ZZ_python(domain, 5, ZZ) == domain.one  # Should return the EX equivalent\n\n    # Test the from_QQ_python method\n    assert domain.from_QQ_python", "def test_from_sympy():\n    from sympy.polys import Poly, ZZ\n    from sympy.abc import x, y\n    from sympy import cos\n\n    e = EX(x**2 + 2*x + 1)\n    f = e.from_sympy(Poly(x**2 + 2*x + 1, x))\n    assert e.ex == f.ex\n\n    g = e.from_sympy(cos(x))\n    assert g.ex == cos(x)\n\n    e = EX(x*y + x + y)\n    f = e.from_sympy(Poly(x*y + x + y, x, y))\n    assert e.ex == f.ex\n\n    with raises(TypeError):\n        e.from_sympy(123)\n    with raises(TypeError):\n        e.from_sympy([x**2, x])\n", "def test_expression_domain_operations():\n    # Test __abs__\n    K = EX()\n    assert K.is_nonnegative(K.one)\n    assert not K.is_negative(K.one)\n    assert K.is_positive(K(one))\n\n    # Test is_nonpositive and is_nonnegative\n    assert K.is_nonpositive(K(-1))\n    assert K.is_nonnegative(K(1))\n    assert not K.is_positive(K(-1))\n    assert not K.is_negative(K(1))\n\n    # Test numer and denom\n    f = K(1/2)\n    assert K.numer(f) == K(1)\n    assert K.denom(f) == K(2)\n\n    # Test gcd and lcm\n    f = K(6)\n    g = K(12)\n    assert K.gcd(f, g) == K(6)\n    assert K.lcm(f, g) == K(12)\n\n    # Test negative and zero\n    assert K(-K(1)) == K(-1)\n    assert K.zero == K(0)\n\n    # Test comparison\n    f = K(1)\n    g = K(1)\n    h = K(2)\n    assert f == g\n    assert f != h\n\n    # Test exponentiation\n    assert K(2)**K(3) == K(8)\n\n    # Test division\n    assert K(4) / K(2) == K(2)\n    assert K(4) / K(0) == NotImplemented\n\n    # Test real and imaginary parts\n    assert K.to_sympy(K(3)).is_real\n    assert K.to_sympy(K(3)).is_real == True\n\n    # Test to_python conversion\n    assert K.from_python(1) == K(1)\n    assert K.from_python(1.0) == NotImplemented\n\n    # Test from_sympy conversion\n    assert K.from_sympy(K(3)) == K(3)\n    assert K.from_sympy(3) == K(3)\n\n    # Test other conversion methods\n    assert K.from_ZZ_python(K, 1, ZZ) == K(1)\n    assert K.from_QQ_python(K, 1, QQ) == K(1)\n    assert K.from_ZZ_gmpy(K, 1, ZZ) == K(1)\n    assert K.from_QQ_gmpy(K, 1, QQ) == K(1)\n    assert K.from_G"], "sample_705": ["def test_Pytester_syspathinsert(tmp_path: Path) -> None:\n    \"\"\"Test that sys.path is correctly updated.\"\"\"\n    from _pytest.pytester import Pytester\n\n    tester = Pytester(request=None, tmp_path_factory=tmp_path, _ispytest=True)\n    original_sys_path = sys.path[:]\n    tester.syspathinsert(str(tmp_path))\n    assert tmp_path in sys.path\n    assert original_sys_path != sys.path\n", "def test_linecomp():\n    \"\"\"Test LineComp assert_contains_lines.\"\"\"\n    pytester = pytester_mod.Pytester()\n    tester = LineComp()\n    tester.stringio.write(\"line1\\nline2\\nline3\")\n    tester.stringio.seek(0)\n    tester.assert_contains_lines([\"line1\", \"line2\"])\n    tester.stringio.write(\"line4\\nline5\")\n    tester.stringio.seek(0)\n    tester.assert_contains_lines([\"line1\", \"line2\", \"line4\", \"line5\"])\n    tester.stringio.write(\"line6\\nline7\")\n    tester.stringio.seek(0)\n    tester.assert_contains_lines([\"line1\", \"line2\", \"line4\", \"line5\", \"line6\", \"line7\"])\n    tester.stringio.write(\"line8\\nline9\")\n    tester.stringio.seek(0)\n    tester.assert_contains_lines([\"line1\", \"line2\", \"line4\", \"line5\", \"line6\", \"line7\", \"line8\", \"line9\"])\n", "def test_pytester_runpytest_subprocess(tmp_path):\n    \"\"\"Ensure runpytest_subprocess doesn't change cwd.\"\"\"\n    cwd = Path.cwd()\n    pytester = Pytester(tmp_path, _ispytest=False)\n    pytester.runpytest_subprocess()\n    assert Path.cwd() == cwd\n", "def test_runpython_c(tmp_path: Path) -> None:\n    pytester = Pytester(tmp_path, _ispytest=False)\n    pytester.mkdir(\"subdir\")\n    pytester.syspathinsert()\n    pytester.makepyfile(\n        __main__=\"\n        import sys\n        import os\n        import subprocess\n        print(os.getcwd())\n        sys.path.append('subdir')\n        subprocess.run(['python', '-c', 'import os; print(os.getcwd())'], check=True)\n        \"\n    )\n    result = pytester.runpython_c(\"import os; print(os.getcwd())\")\n    assert result.outlines == [\"subdir\"]  # new directory is in sys.path\n    assert result.errlines == []  # no errors\n", "def test_python_c_run(tmp_path):\n    # check that runpython_c doesn't fail on utf8\n    (tmp_path / \"hello.py\").write_text(\"print('hello')\")\n    tmp_path.joinpath(\"hello.py\").chmod(0o755)\n    p = Pytester._instance(tmp_path).runpython_c(\"python hello.py\")\n\n    # check that runpython_c outputs\n    assert p.outlines == ['hello\\n']\n\n    # check that runpython_c fails if the command is invalid\n    (tmp_path / \"hello.py\").write_text(\"invalid command\")\n    tmp_path.joinpath(\"hello.py\").chmod(0o755)\n    p = Pytester._instance(tmp_path).runpython_c(\"python hello.py\")\n    assert p.ret == ExitCode.CAPTURED\n\n    # check that runpython_c works with a shebang\n    (tmp_path / \"hello.py\").write_text(\"#!/usr/bin/env python\\nprint('hello')\")\n    tmp_path.joinpath(\"hello.py\").chmod(0o755)\n    p = Pytester._instance(tmp_path).runpython_c(\"hello.py\")\n    assert p.outlines == ['hello\\n']\n\n    # check that runpython_c works with a non-utf8 filename\n    (tmp_path / \"hello.txt\").write_text(\"print('hello')\")\n    tmp_path.joinpath(\"hello.txt\").chmod(0o755)\n    p = Pytester._instance(tmp_path).runpython_c(\"python hello.txt\")\n    assert p.outlines == ['hello\\n']\n\n    # check that runpython_c works when the python interpreter is not found\n    (tmp_path / \"hello.py\").write_text(\"print('hello')\")\n    tmp_path.joinpath(\"hello.py\").chmod(0o755)\n    p = Pytester._instance(tmp_path).runpython_c(\"python123 hello.py\")\n    assert p.ret == ExitCode.CAPTURED\n    assert p.errlines == ['Command \"python123\" not found.']\n"], "sample_1047": ["def test_assumptions_inherited_from_bases():\n    # Create a class that inherits from Basic and has some assumptions\n    class MySym(Basic):\n        default_assumptions = StdFactKB({'commutative': True})\n\n    # Check that assumptions are inherited correctly\n    x = Symbol('x', commutative=True, base=MySym)\n    assert x.is_commutative\n    assert x.is_complex\n    assert x.is_real\n    assert x.is_algebraic\n    assert x.is_transcendental\n\n    # Check that assumptions are not inherited if they are not in the default assumptions\n    x = Symbol('x', base=MySym)\n    assert not x.is_commutative\n    assert x.is_complex\n    assert x.is_real\n    assert x.is_algebraic\n    assert x.is_transcendental\n", "def test_InconsistentAssumptions():\n    assert raises(InconsistentAssumptions, Rational(1, 2).is_integer)\n    assert raises(InconsistentAssumptions, Rational(1, 2).is_odd)\n    assert raises(InconsistentAssumptions, Rational(1, 2).is_even)\n    assert raises(InconsistentAssumptions, Rational(1, 2).is_rational)\n    assert raises(InconsistentAssumptions, Rational(1, 2).is_real)\n", "def test_ask_transcendental():\n    assert _ask('transcendental', I) == True\n    assert _ask('transcendental', Rational(1)) == False\n    assert _ask('transcendental', sqrt(pi)) == True\n    assert _ask('transcendental', sin(pi)) == True\n    assert _ask('transcendental', asin(sin(pi))) == True\n    assert _ask('transcendental', log(pi)) == True\n    assert _ask('transcendental', exp(1)) == True\n    assert _ask('transcendental', log(sqrt(pi))) == True\n    assert _ask('transcendental', exp(log(sqrt(pi)))) == True\n", "def test_infinite():\n    x = Symbol('x', finite=True)\n    assert x.is_finite\n    assert not x.is_infinite\n    with raises(InconsistentAssumptions):\n        x = Symbol('x', finite=False, infinite=True)\n", "def test_infinite():\n    assert I.is_infinite is None\n    assert sqrt(3).is_infinite is None\n    assert (3 + 4*I).is_infinite is None\n    assert simplify(1 + pi).is_infinite\n    assert simplify(1 + sqrt(2)).is_infinite\n    assert simplify(log(2)).is_infinite\n    assert simplify(exp(I*pi)).is_infinite\n    assert simplify(sin(1)).is_infinite is None\n    assert simplify(asin(1)).is_infinite is None\n    assert simplify(factorial(2)).is_infinite is None\n    assert simplify(Mod(3, 4)).is_infinite is None\n"], "sample_1193": ["def test_closest_points():\n    p1 = Point2D(1, 1)\n    p2 = Point2D(2, 2)\n    p3 = Point2D(3, 3)\n    p4 = Point2D(0, 0)\n    points = [p1, p2, p3, p4]\n    expected = {(p1, p2)}\n    assert closest_points(*points) == expected\n\n", "def test_idiff_trigonometric_function():\n    x = Symbol('x')\n    y = Symbol('y')\n    eq = sin(x) - x**2\n    result = idiff(eq, y, x)\n    expected_result = 1\n    assert result.equals(expected_result)\n", "def test_idiff():\n    from sympy.abc import x, y, z\n    from sympy.geometry import circle\n    from sympy.geometry.util import idiff\n    c = circle((0, 0), 5)\n    assert idiff(c, y, x) == -x/(5*y)\n    assert idiff(c, (y, z), x) == (Derivative(z, x) - x/(5*y))/z\n", "def test_idiff_nonlinear():\n    x, y = Symbol('x'), Symbol('y')\n    assert idiff(x**2 + y**2 - 4, y, x) == -x/y\n    assert idiff(x**2 + y**2 - 4, y, x, 2) == (-x**2 - y**2)/y**3\n    assert idiff(exp(x) + y, y, x) == -exp(x)\n    assert idiff(exp(x) + y, [y, x], x) == -exp(x)\n", "def test_idiff_custom_variable():\n    x, y = Symbol('x'), Symbol('y')\n    # Test with a simple function\n    eq = y**2 + x**2 - 4\n    expected = -x/y\n    assert idiff(eq, y, x) == expected\n\n    # Test with a more complex function\n    eq = y**2 + x**2 - 4*x*y\n    expected = (x**2 - 4*y)/(2*y**2 - 2*x)\n    assert idiff(eq, y, x) == expected\n\n    # Test with multiple dependent variables\n    eq = x**2 + y**2 + z**2 - 6*z - 8\n    z = Symbol('z')\n    expected = -z\n    assert idiff(eq, [y, z], x) == expected\n\n    # Test with a non-symbolic dependent variable\n    eq = x**2 + y**2 + z**2 - 6*z - 8\n    expected = -z\n    assert idiff(eq, [Function('y')(x), Function('z')(x)], x) == expected\n\n    # Test with a non-function dependent variable\n    eq = x**2 + y**2 + z**2 - 6*z - 8\n    expected = -z\n    assert idiff(eq, [y, z], x) == expected\n\n    # Test with a symbolic variable in the function\n    eq = x**2 + y**2 - 4*y + x*z - z\n    expected = -z\n    assert idiff(eq, y, x) == expected\n\n    # Test with a symbolic variable in the derivative\n    eq = y**2 + x**2 - 4*x*y\n    expected = (x**2 - 4*y)/(2*y**2 - 2*x)\n    assert idiff(eq, y, x) == expected\n\n    # Test with a higher-order derivative\n    eq = x**2 + y**2 - 4*x*y\n    expected = (x**2 - 4*y)/(2*y**2 - 2*x)\n    assert idiff(eq, y, x, 2) == expected\n"], "sample_666": ["    def test_reset(self, caplog):\n        handler = LogCaptureHandler()\n        handler.setLevel(logging.INFO)\n        handler.emit(logging.LogRecord(\n            \"name\", logging.INFO, \"filename\", 123, \"message\"\n        ))\n        assert len(handler.records) == 1\n        handler.reset()\n        assert len(handler.records) == 0\n", "    def test_get_log_level_for_setting(self, log_level, expected_level):\n        # Test get_log_level_for_setting function\n        config = Config()\n        config.getoption = lambda *args, **kwargs: log_level\n        config.getini = lambda *args, **kwargs: log_level\n        assert get_log_level_for_setting(config, \"log_level\") == expected_level\n", "    def test_log_capture_handler_reset(self, caplog):\n        with caplog.at_level(logging.INFO):\n            caplog.handler.records = [logging.LogRecord(\"test\", logging.INFO, \"test.py\", 1, \"test\")]\n            caplog.handler.reset()\n        assert caplog.records == []\n", "    def test_log_auto_indent(self, caplog, capsys, auto_indent):\n        # Given\n        plugin = LoggingPlugin(pytest.config)\n        plugin._setup_cli_logging()\n        plugin.formatter = logging.Formatter(\"%(asctime)s %(levelname)s %(message)s\")\n        plugin.log_auto_indent = auto_indent\n        with caplog.at_level(logging.INFO):\n            plugin.live_logs_context()  # Enable live logging\n            print(\"Hello world!\")\n            print(\"This is a long message that spans multiple lines.\")\n            print(\"This is another long message that spans multiple lines.\")\n\n        # When\n        text = caplog.text\n\n        # Then\n        assert \"Hello world!\" in text\n        assert \"This is a long message that spans multiple lines.\" in text\n        assert \"This is another long message that spans multiple lines.\" in text\n        if auto_indent == \"on\":\n            assert text.count(\"\\n\") == 5\n            assert text.count(\"  \") == 2\n        elif auto_indent == \"off\":\n            assert text.count(\"\\n\") == 4\n            assert text.count(\"  \") == 0\n        else:\n            assert text.count(\"\\n\") == 5\n            assert text.count(\"  \") == auto_indent\n", "    def test_catch_log_with_formatter(self, caplog, method, fmt, date_fmt):\n        log_level = logging.INFO\n        logger = logging.getLogger()\n        logger.setLevel(log_level)\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(fmt, date_fmt)\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n        with catching_logs(handler, level=log_level):\n            logger.info(\"Test log with %s format and %s date format\", fmt, date_fmt)\n\n        assert len(caplog.records) == 1\n        record = caplog.records[0]\n        assert record.levelno == log_level\n        assert record.getMessage() == \"Test log with \" + fmt + \" format and \" + date_fmt + \" date format\"\n"], "sample_1115": ["def testTensorIndexType_data_deprecation():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    Lorentz.data = 1\n    assert _tensor_data_substitution_dict[Lorentz] is not None\n", "def test_get_index_structure():\n    from sympy import symbols, TensorIndexType, TensorIndex, tensor_indices, TensorHead\n    from sympy.tensor.tensor import get_index_structure\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensor_heads\n\n    L = TensorIndexType('Lorentz', dummy_name='L')\n    i, j, k = tensor_indices('i,j,k', L)\n    A = TensorHead('A', [L, L, L], TensorSymmetry.fully_symmetric(3))\n\n    # Test when tensor is a TensExpr\n    assert get_index_structure(A(i, j, k)) == _IndexStructure.from_indices(A(i, j, k))\n\n    # Test when tensor is a TensorHead\n    assert get_index_structure(A) == _IndexStructure.from_indices(i, j, k)\n\n    # Test when tensor is a TensorIndex\n    assert get_index_structure(i) == _IndexStructure.from_indices(i)\n\n    # Test when tensor is a Symbol\n    assert get_index_structure(i.args[0]) == _IndexStructure.from_indices(i)\n\n    # Test when tensor is a TensorIndexType\n    assert get_index_structure(L) == _IndexStructure.from_indices()\n\n    # Test when tensor is a TensAdd\n    t = A(i, j) + A(j, k)\n    assert get_index_structure(t) == _IndexStructure.from_indices(i, j, k)\n\n    # Test when tensor is a TensMul\n    t = A(i, j) * A(j, k)\n    assert get_index_structure(t) == _IndexStructure.from_indices(i, j, k)\n\n    # Test when tensor is a TensorElement\n    te = TensorElement(A(i, j), {i: 2})\n    assert get_index_structure(te) == _IndexStructure.from_indices(j)\n", "def test_TensorIndexType_data_set():\n    # Test that setting a TensorIndexType's data raises an error\n    # if the data is not a valid ndarray.\n    with raises(ValueError):\n        TensorIndexType('Lorentz').data = 1\n\n    # Test that setting a TensorIndexType's data creates a new ndarray\n    # instead of modifying the old one.\n    TensorIndexType('Lorentz').data = Array([1, 2, 3])\n    assert TensorIndexType('Lorentz').data is not Array([1, 2, 3])\n\n    # Test that setting a TensorIndexType's data with a matrix\n    # creates a new ndarray with the correct shape.\n    TensorIndexType('Lorentz').data = Matrix([[1, 2], [3, 4]])\n    assert TensorIndexType('Lorentz').data.shape == (2, 2)\n\n    # Test that setting a TensorIndexType's data with a vector\n    # creates a new ndarray with the correct shape.\n    TensorIndexType('Lorentz').data = Matrix([1, 2, 3])\n    assert TensorIndexType('Lorentz').data.shape == (3,)\n\n    # Test that setting a TensorIndexType's data with a scalar\n    # creates a new ndarray with the correct shape.\n    TensorIndexType('Lorentz').data = 1\n    assert TensorIndexType('Lorentz').data.shape == (1,)\n\n    # Test that setting a TensorIndexType's data with an invalid shape\n    # raises an error.\n    with raises(ValueError):\n        TensorIndexType('Lorentz').data = Matrix([[1, 2, 3], [4, 5, 6]])\n\n    # Test that setting a TensorIndexType's data with a non-array value\n    # raises an error.\n    with raises(ValueError):\n        TensorIndexType('Lorentz').data = 1\n\n    # Test that setting a TensorIndexType's data with a value that has\n    # the wrong dimension raises an error.\n    with raises(ValueError):\n        TensorIndexType('Lorentz').data = Array([1, 2, 3])\n\n    # Test that setting a TensorIndexType's data with a value that has\n    # a different dimension than the TensorIndexType's dim raises an error.\n    with raises(ValueError):\n       ", "def test_perm2tensor():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    m0, m1, m2 = tensor_indices('m0,m1,m2', Lorentz)\n    g = Lorentz.metric\n    p, q = tensor_heads('p,q', [Lorentz])\n    t = p(m0)*q(m1)*g(-m0, -m2)\n    t_canon_bp = t.canon_bp()\n    g_canon_bp = canon_bp(g)\n    t_perm2tensor = t_perm2tensor(t_canon_bp, Permutation([0, 1, 2, 3]), True)\n    assert t_perm2tensor.equals(t_perm2tensor)\n    assert t_perm2tensor.equals(p(m1)*q(m0)*g(-m1, -m2))\n", "def test_TensorIndexType_init():\n    # TODO: remove test when TensorIndexType is fixed\n    with warns_deprecated_sympy():\n        try:\n            TensorIndexType('wrong')\n        except ValueError:\n            pass\n        else:\n            raise AssertionError(\"TensorIndexType should raise an error for invalid name\")\n"], "sample_466": ["    def test_operation_writer_serialization_of_enum_instance(self):\n        enum_instance = TextEnum.A\n        operation_writer = OperationWriter(lambda: None)\n        rendered_string, _ = operation_writer.serialize(enum_instance)\n        self.assertIn(\"TextEnum\", rendered_string)\n        self.assertIn(\"A\", rendered_string)\n", "    def test_serialize_operation_with_text_enum(self):\n        operation = migrations.CreateModel(\n            name='TestModel',\n            fields=[\n                migrations.Field(name='field1', type='IntegerField'),\n                migrations.Field(name='field2', type='TextField'),\n            ],\n            options={\n                'managed': True,\n                'db_table': 'appname_testmodel',\n            },\n            bases=(TextEnum,),\n            app_label='appname',\n            app_config=None,\n            state_operations=None,\n        )\n        writer = OperationWriter(operation)\n        self.assertEqual(\n            writer.render(),\n            'migrations.operations.CreateModel(\\n    name=\\'TestModel\\',\\n    fields=[\\n        migrations.Field(name=\\'field1\\', type=\\'IntegerField\\'),\\n        migrations.Field(name=\\'field2\\', type=\\'TextField\\'),\\n    ],\\n    options={'managed': True, \\'db_table': \\'appname_testmodel\\'},\\n    bases=(\\'appname.TextEnum\\',),\\n    app_label=\\'appname\\',\\n    app_config=None,\\n    state_operations=None,\\n),'\n        )\n", "    def test_operation_writer_serialize(self):\n        operation = custom_migration_operations.operations.AddField(\n            field=models.CharField(max_length=10)\n        )\n        writer = OperationWriter(operation)\n        self.assertIn(\"migrations.AddField\", writer.render())\n        self.assertIn(\"max_length=10\", writer.render())\n", "    def test_deconstructible_instances(self):\n        instance = DeconstructibleInstances()\n        writer = OperationWriter(instance.deconstruct)\n        rendered = writer.render()\n        self.assertIn(\"DeconstructibleInstances\", rendered)\n        self.assertIn(\"[]\", rendered)\n        self.assertIn(\"{}\", rendered)\n", "    def test_serialize_operations(self):\n        operation = custom_migration_operations.operations.CreateModel(\n            name=\"TestModel\", fields={\"name\": models.CharField(max_length=255)}\n        )\n        writer = OperationWriter(operation)\n        rendered, imports = writer.serialize()\n        expected_imports = [\n            \"from django.db import models\",\n            \"from custom_migration_operations import operations\",\n        ]\n        self.assertEqual(\n            writer.render(), expected_imports[0] + \"\\n\" + rendered + \"\\n\" + expected_imports[1]\n        )\n        self.assertEqual(imports, set(expected_imports))\n"], "sample_486": ["def test_inlineformset_factory_formfield_callback(self):\n    class TestForm(ModelForm):\n        class Meta:\n            model = ChildRelatedViaAK\n            fields = (\"name\", \"parent\")\n\n    class TestFormSet(BaseInlineFormSet):\n        form = TestForm\n        fk_name = \"parent\"\n\n    formset = inlineformset_factory(\n        AutoPKParent, ChildRelatedViaAK, form=TestForm, fk_name=\"parent\", formfield_callback=lambda x: x\n    )\n    self.assertEqual(formset.form.formfield_callback, lambda x: x)\n    self.assertEqual(formset.form._meta.model, ChildRelatedViaAK)\n", "    def test_inlineformset_factory_without_fk_name(self):\n        with self.assertRaises(ValueError):\n            inlineformset_factory(ParentWithUUIDAlternateKey, UUIDPKChild)\n", "    def test_inlineformset_factory_model_with_multiple_foreign_keys(self):\n        from .models import (\n            ChildWithMultipleFK,\n            ParentModel,\n        )\n\n        ChildWithMultipleFK.objects.create(parent1_id=1, parent2_id=2)\n        ParentModel.objects.create()\n        formset = inlineformset_factory(\n            ParentModel,\n            ChildWithMultipleFK,\n            fk_name=\"fk1\",\n            fields=(\"fk1\", \"fk2\"),\n        )(data={\"fk1-0-fk1\": \"1\", \"fk2-0-fk2\": \"1\"}, queryset=ChildWithMultipleFK.objects.all())\n        formset.save(commit=False)\n        self.assertEqual(formset.forms[0].instance.parent1_id, 1)\n        self.assertEqual(formset.forms[0].instance.parent2_id, 2)\n        self.assertEqual(formset.forms[0].cleaned_data[\"fk1\"], 1)\n        self.assertEqual(formset.forms[0].cleaned_data[\"fk2\"], 1)\n", "    def test_model_choice_field_choices(self):\n        from .forms import ParentModelChoiceFieldForm\n        form = ParentModelChoiceFieldForm({'parent': '1'})\n        self.assertEqual(form.fields['parent'].choices, [\n            (1, 'Parent object 1'),\n            (2, 'Parent object 2'),\n        ])\n", "    def test_inline_formset_factory_fk_name_specified(self):\n        formset = inlineformset_factory(\n            ParentWithUUIDAlternateKey,\n            UUIDPKChild,\n            fk_name=\"uuid_foo\",\n        )\n        self.assertEqual(formset.fk.name, \"uuid_foo\")\n"], "sample_403": ["    def test_alter_model_options_on_model(self):\n        with transaction.atomic():\n            with self.assertRaises(IntegrityError):\n                AlterModelOptions(\"test_model\", {\"get_latest_by\": \"fake_field\"}).apply(\n                    Migration, connection\n                )\n", "    def test_alter_field(self):\n        class Migration(migrations.Migration):\n            dependencies = [(\"tests\", \"0001_initial\")]\n            operations = [\n                migrations.AlterField(\n                    model_name=\"unicodemodel\",\n                    name=\"name\",\n                    field=models.CharField(max_length=10),\n                )\n            ]\n\n        with transaction.atomic(), CaptureQueriesContext() as capture:\n            Migration.apply(capture, project_state=ProjectState.from_apps(None))\n            self.assertEqual(capture.captured_queries, [])\n\n        with transaction.atomic(), CaptureQueriesContext() as capture:\n            Migration.apply(capture, project_state=ProjectState.from_apps(None))\n            self.assertEqual(len(capture.captured_queries), 1)\n\n        with transaction.atomic(), CaptureQueriesContext() as capture:\n            Migration.apply(capture, project_state=ProjectState.from_apps(None), reverse=True)\n            self.assertEqual(capture.captured_queries, [])\n\n        model = UnicodeModel.objects.get(name=\"foo\")\n        with self.assertRaises(FieldDoesNotExist):\n            UnicodeModel._meta.get_field(\"name\")\n", "    def test_alter_model_options_with_valid_options(self):\n        operation = AlterModelOptions(\n            name=\"test_model\",\n            options={\"default_manager_name\": \"CustomManager\", \"managed\": False},\n        )\n        self.assertEqual(operation.describe(), \"Change Meta options on test_model\")\n        self.assertEqual(operation.state_forwards(app_label=\"app\", state=ModelState()), [\n            {\"model\": \"app\", \"name\": \"test_model\", \"fields\": [], \"options\": {\"default_manager_name\": \"CustomManager\"}, \"bases\": (models.Model,), \"managers\": []}\n        ])\n        self.assertEqual(\n            operation.database_forwards(app_label=\"app\", schema_editor=models.SchemaEditorFake(), from_state=ModelState(), to_state=ModelState()),\n            None\n        )\n        self.assertEqual(\n            operation.database_backwards(app_label=\"app\", schema_editor=models.SchemaEditorFake(), from_state=ModelState(), to_state=ModelState()),\n            None\n        )\n", "    def test_add_index(self):\n        initial_state = ModelState(\n            \"app\", \"model\", [], {}, (models.Model,), []\n        )\n        operation = AddIndex(\"model\", models.Index(fields=[\"field1\", \"field2\"]))\n        with CaptureQueriesContext() as queries:\n            self._apply_operation(operation, initial_state)\n        self.assertEqual(\n            initial_state.get_indexes(),\n            {\"index\": [\"field1\", \"field2\"]},\n        )\n        self.assertEqual(\n            queries.captured_queries,\n            [\"CREATE INDEX index ON app_model (field1, field2)\"],\n        )\n\n        # Test deconstruct method\n        self.assertEqual(operation.deconstruct(), (\"AddIndex\", [], {\"model_name\": \"model\", \"index\": models.Index(fields=[\"field1\", \"field2\"])}))\n\n        # Test that reduce method works correctly\n        reduced_operation = operation.reduce(DeleteModel(\"model\"), \"app\")\n        self.assertEqual(reduced_operation, [])\n", "    def test_alter_default_tablespace(self):\n        # Act\n        with transaction.atomic():\n            with connection.schema_editor(using='default') as schema_editor:\n                tablespace = 'new_tbs'\n                operation = AlterModelOptions(\n                    'unicode',\n                    {\n                        'db_tablespace': tablespace,\n                    }\n                )\n                operation.database_forwards(\n                    'myapp',\n                    schema_editor,\n                    ModelState(models=StateDict({'unicode': unicode_model_state()})),\n                    ModelState(models=StateDict({'unicode': unicode_model_state(tablespace=tablespace)})),\n                )\n\n        # Assert\n        with transaction.atomic():\n            self.assertEqual(\n                unicode_model._meta.db_tablespace,\n                tablespace\n            )\n\n        # Revert\n        with transaction.atomic():\n            with connection.schema_editor(using='default') as schema_editor:\n                operation.database_backwards(\n                    'myapp',\n                    schema_editor,\n                    ModelState(models=StateDict({'unicode': unicode_model_state(tablespace=tablespace)})),\n                    ModelState(models=StateDict({'unicode': unicode_model_state()})),\n                )\n\n        # Assert\n        self.assertEqual(\n            unicode_model._meta.db_tablespace,\n            'my_tbs'\n        )\n"], "sample_1140": ["def test_print_Norm():\n    n = Norm(x**2 + y**2)\n    assert pretty(n) == \"sqrt(x**2 + y**2)\"\n", "def test_subfactorial():\n    n = sp.Symbol('n')\n    # Test the pretty-printing of subfactorial functions\n    assert pretty(sp.subfactorial(n)) == prettyForm(\"subfactorial(n)\")\n    assert pretty(sp.subfactorial(2*n)) == prettyForm(\"subfactorial(2*n)\")\n    assert pretty(sp.subfactorial(n+1)) == prettyForm(\"subfactorial(n+1)\")\n\n    # Test that the subfactorial function is not modified by pretty-printing\n    assert pretty(sp.subfactorial(n).func) == sp.subfactorial\n    assert pretty(sp.subfactorial(2*n).func) == sp.subfactorial\n    assert pretty(sp.subfactorial(n+1).func) == sp.subfactorial\n\n    # Test that the subfactorial function is printed correctly when it's part of a larger expression\n    assert pretty(n*sp.subfactorial(n)) == prettyForm(\"n*subfactorial(n)\")\n    assert pretty(n*sp.subfactorial(2*n)) == prettyForm(\"n*subfactorial(2*n)\")\n    assert pretty(n*sp.subfactorial(n+1)) == prettyForm(\"n*subfactorial(n+1)\")\n\n    # Test that the subfactorial function is not modified when it's part of a larger expression\n    assert pretty((n*sp.subfactorial(n)).func) == sp.subfactorial\n    assert pretty((n*sp.subfactorial(2*n)).func) == sp.subfactorial\n    assert pretty((n*sp.subfactorial(n+1)).func) == sp.subfactorial\n", "def test_ImageSet():\n    from sympy import Symbol, Eq, And, ImageSet, cos\n    x = Symbol('x')\n    s1 = Symbol('s1')\n    s2 = Symbol('s2')\n    r = Symbol('r')\n    u = Eq(s1 + s2, 0)\n    assert pretty(ImageSet(s1, u)) == pretty(s1, u)\n    assert pretty(ImageSet(s1, u, ImageSet(s2, u))) == pretty(s1, u, pretty(s2, u))\n    assert pretty(ImageSet(s1, u, ImageSet(s2, u), ImageSet(r, u))) == pretty(s1, u, pretty(s2, u), pretty(r, u))\n    assert pretty(ImageSet(s1, u, ImageSet(s2, u), ImageSet(r, u), ImageSet(cos(x), u))) == pretty(s1, u, pretty(s2, u), pretty(r, u), pretty(cos(x), u))\n    # Test with more than one variable\n    assert pretty(ImageSet(s1, And(u, Eq(s2, 0)))) == pretty(s1, pretty(s2, 0))\n    assert pretty(ImageSet(s1, u, ImageSet(s2, And(u, Eq(s2, 0))))) == pretty(s1, pretty(s2, pretty(s2, 0)))\n    assert pretty(ImageSet(s1, u, ImageSet(s2, And(u, Eq(s2, 0))), ImageSet(r, And(u, Eq(s2, 0))))) == pretty(s1, pretty(s2, pretty(s2, 0)), pretty(r, pretty(s2, 0)))\n    assert pretty(ImageSet(s1, And(u, Eq(s2, 0)), ImageSet(s2, And(u, Eq(s2, 0))))) == pretty(s1, pretty(s2, pretty(s2, 0)), pretty(s2, pretty(s2, 0)))\n    assert pretty(ImageSet(s1, And(u, Eq(s2, 0)), ImageSet(s2, And(u, Eq(s2, 0))), ImageSet(r, And(u, Eq(s2, 0))))) == pretty(s1, pretty(s2, pretty(s2, 0)), pretty(s2, pretty(s2, 0)), pretty(r, pretty(s2, 0)))\n", "compilation error", "def test_rational_powers():\n    from sympy import Rational\n    assert upretty(Rational(1, 2**3)) == '1/(2**3)'\n    assert upretty(Rational(1, 2**3).as_numer_denom()[0] / (2**3)) == '1/8'\n"], "sample_682": ["    def test_evaluate_condition_string(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif('1 > 2', reason=\"one is not greater than two\")\n                assert False\n            \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        assert \"SKIPPED\" in result.stdout.str()\n", "    def test_evaluate_condition(self, pytester: Testdir):\n        item = pytester.makeitem(\"test_function\")\n        mark = pytest.mark.skip(reason=\"test skipped\")\n        item._markers = [mark]\n\n        result, reason = evaluate_condition(item, mark, \"1 == 2\")\n        assert result is False\n        assert reason == \"condition: 1 == 2\"\n\n        result, reason = evaluate_condition(item, mark, \"1 == 1\")\n        assert result is False\n        assert reason == \"condition: 1 == 1\"\n\n        item._markers = []\n        result, reason = evaluate_condition(item, mark, \"1 == 1\")\n        assert result is False\n        assert reason == \"you need to specify reason=STRING when using booleans as conditions.\"\n", "    def test_evaluate_condition_empty_string(self, testdir: Testdir) -> None:\n        # Evaluate an empty string condition.\n        item = testdir.makeitem(\"test_func\")\n        mark = Mark.fromname(\"skipif\", args=[], kwargs={\"condition\": \"\"})\n        result, reason = evaluate_condition(item, mark, \"\")\n        assert result is False\n        assert reason == \"\"\n", "def test_evaluate_condition_string_condition(monkeypatch, testdir: Testdir):\n    # Patch the config to make runxfail always True\n    monkeypatch.setattr(testdir.getini(\"xfail_strict\"), True)\n    testdir.makeini(\"\"\"\n    [pytest]\n    runxfail = True\n    xfail_strict = False\n    \"\"\")\n    \n    item = testdir.pytest_runtest_setup(pytest.Item(\"test_func\"))\n    \n    condition = \"1 + 1 == 2\"\n    mark = Mark(name=\"xfail\", kwargs={\"reason\": \"condition\", \"condition\": condition})\n    result, reason = evaluate_condition(item, mark, condition)\n    assert result == True\n    assert reason == \"condition: 1 + 1 == 2\"\n\n    condition = \"1 + 1 == 3\"\n    mark = Mark(name=\"xfail\", kwargs={\"reason\": \"condition\", \"condition\": condition})\n    result, reason = evaluate_condition(item, mark, condition)\n    assert result == True\n    assert reason == \"condition: 1 + 1 == 3\"\n\n    condition = \"1 + 1 == '2'\"\n    mark = Mark(name=\"xfail\", kwargs={\"reason\": \"condition\", \"condition\": condition})\n    result, reason = evaluate_condition(item, mark, condition)\n    assert result == False\n    assert reason == \"Error evaluating xfail condition: you need to specify reason=STRING when using booleans as conditions.\"\n\n", "def test_evaluate_condition_empty_string():\n    \"\"\"Test that an empty string condition raises an error.\"\"\"\n    item = Item(\"test_name\", None)\n    mark = Mark(\"skipif\", {\"reason\": \"test reason\"})\n    with pytest.raises(ValueError):\n        evaluate_condition(item, mark, \"\")\n"], "sample_679": ["    def test_invalid_condition_syntax(self):\n        # Arrange\n        item = mock.Mock(spec=Item)\n        item.iter_markers.return_value = [Mark(kwargs={\"condition\": \"1 +\", \"reason\": \"error in expr\"})]\n        evaluator = MarkEvaluator(item, \"invalid_condition_syntax\")\n\n        # Act and Assert\n        assert evaluator.istrue() is False\n        assert evaluator.getexplanation() == \"condition: 1 +\"\n        assert evaluator.exc is not None\n", "def test_MarkEvaluator_get_explanation():\n    item = mock.Mock()\n    item.iter_markers.return_value = [mock.Mock(kwargs={\"reason\": \"test reason\"})]\n    evaluator = MarkEvaluator(item, \"test_mark\")\n    assert evaluator.getexplanation() == \"test reason\"\n", "def test_wasvalid_with_exception(self):\n    # Arrange\n    item = mock.Mock(spec=Item)\n    evaluator = MarkEvaluator(item, \"test_mark\")\n\n    # Act\n    evaluator._mark = mock.Mock(spec=Mark)\n    evaluator._mark.kwargs = {\"reason\": \"reason\"}\n\n    # Assert\n    assert evaluator.wasvalid() is True\n    evaluator.exc = (Exception, Exception(), None)\n    assert evaluator.wasvalid() is False\n", "def test_invalid_conditionraises():\n    item = mock.Mock(spec=Item)\n    item.iter_markers.return_value = [\n        Mark(name=\"invalidraise\", args=[\"True\"], kwargs={\"reason\": \"invalid reason\"})\n    ]\n    evaluator = MarkEvaluator(item, \"invalidraise\")\n    assert evaluator.wasvalid() == False\n    assert evaluator.invalidraise(Exception) is False\n    assert evaluator.getexplanation() == \"condition: True\"\n", "def test_mark_evaluator_invalidraises():\n    mark = Mark(\"raises\", {\"reason\": \"Mock exception\", \"reason\": \"ValueError\"})\n    mark_generator = MarkGenerator()\n    collector = Collector.fromconfig({}, mark_generator)\n    item = Node(collector=collector, config=None, name=\"test\")\n    evaluator = MarkEvaluator(item, \"raises\")\n    assert evaluator.wasvalid() is True\n    assert evaluator.invalidraise(ValueError(\"Mock exception\")) is False\n    assert evaluator.invalidraise(ValueError(\"Mock exception\")) is True\n"], "sample_343": ["    def test_get_content_type_cache(self):\n        post = Post.objects.create()\n        self.assertEqual(GenericForeignKey(ct_field='content_type', fk_field='object_id').get_content_type(post), ContentType.objects.get_for_model(post))\n", "    def test_generic_foreign_key_check_object_id_field(self):\n        # Test that _check_object_id_field correctly raises an error if the\n        # object_id field does not exist.\n        class TestModel(models.Model):\n            generic_fk = GenericForeignKey('content_type', 'non_existent_field')\n        with self.assertRaises(FieldDoesNotExist):\n            TestModel._meta.get_field('non_existent_field')\n        with self.assertRaises(checks.Error):\n            GenericForeignKey('content_type', 'non_existent_field').check()\n\n        # Test that _check_object_id_field does not raise an error if the object_id\n        # field exists.\n        class TestModel(models.Model):\n            object_id_field = models.IntegerField()\n            generic_fk = GenericForeignKey('content_type', 'object_id_field')\n        TestModel._meta.get_field('object_id_field')\n        self.assertEqual(GenericForeignKey('content_type', 'object_id_field').check(), [])\n", "    def test_generic_foreign_key_initialization(self):\n        class Post(models.Model):\n            content_type = GenericForeignKey(ct_field='content_type', fk_field='object_id')\n            object_id = models.IntegerField()\n\n        # Test initialization with default arguments\n        post = Post()\n        self.assertEqual(post.content_type.ct_field, 'content_type')\n        self.assertEqual(post.content_type.fk_field, 'object_id')\n        self.assertTrue(post.content_type.for_concrete_model)\n\n        # Test initialization with custom arguments\n        post = Post()\n        post.content_type = GenericForeignKey('custom_ct_field', 'custom_fk_field', for_concrete_model=False)\n        self.assertEqual(post.content_type.ct_field, 'custom_ct_field')\n        self.assertEqual(post.content_type.fk_field, 'custom_fk_field')\n        self.assertFalse(post.content_type.for_concrete_model)\n", "    def test_generic_foreign_key_get_content_type_with_object(self):\n        question = Question.objects.get(id=1)\n        gfk = question.content_type\n        self.assertEqual(gfk.model, ContentType)\n        self.assertEqual(gfk.pk, 1)\n", "    def test_get_content_type_with_invalid_arguments(self):\n        with self.assertRaises(Exception):\n            GenericForeignKey().get_content_type()\n"], "sample_1059": ["def test_differentiation():\n    from sympy.abc import n, a, b, m, alpha, x\n    from sympy import diff\n\n    # Test Jacobi polynomial differentiation\n    jacobi_poly = jacobi(n, a, b, x)\n    assert unchanged(diff(jacobi_poly, n), 2 * jacobi(n - 1, a + 1, b + 1, x))\n    assert unchanged(diff(jacobi_poly, a), Sum(1 / (a + b + n + k + 1) * (jacobi(n, a, b, x) + f2*jacobi(k, a, b, x)), (k, 0, n - 1)))\n    assert unchanged(diff(jacobi_poly, b), Sum(1 / (a + b + n + k + 1) * (jacobi(n, a, b, x) + f2*jacobi(k, a, b, x)), (k, 0, n - 1)))\n    assert unchanged(diff(jacobi_poly, x), (a/2 + b/2 + n/2 + 1/2)*jacobi(n - 1, a + 1, b + 1, x))\n\n    # Test Gegenbauer polynomial differentiation\n    gegenbauer_poly = gegenbauer(n, a, x)\n    assert unchanged(diff(gegenbauer_poly, n), 2 * (1 + (-1)**(n - k)) * (k + a) / ((k + n + 2*a) * (n - k)) * gegenbauer(k, a, x) + 2*(k + 1) / ((k + 2*a) * (2*k + 2*a + 1)) + 2 / (k + n + 2*a) * gegenbauer(n, a, x))\n    assert unchanged(diff(gegenbauer_poly, a), 2 * (1 + (-1)**(n - k)) * (k + a) / ((k + n + 2*a) * (n - k)) * gegenbauer(k, a, x) + 2*(k + 1) / ((k + 2*a) * (2*k + 2*a + 1)) + 2 / (k +", "def test_chebyshevu_finite_difference():\n    from sympy.abc import n, x\n    f = chebyshevu(n, x)\n    diff_f = f.diff(x)\n    for i in range(2, 6):\n        diff_f = diff_f.diff(x)\n        assert diff_f == diff(diff_f, x), f\"diff of chebyshevu order {i} failed\"\n", "def test_gegenbauer_root():\n    n = 1\n    k = 0\n    assert chebyshevu_root(n, k) == cos(S.Pi*k/(n + 1))\n", "def test_legendre_diff_at_x_zero():\n    x = Symbol('x')\n    n = Symbol('n')\n    result = diff(legendre(n, x), x)\n    assert result == n*(x*legendre(n, x) - legendre(n - 1, x))/(x**2 - 1)\n    result = result.subs(x, 0)\n    expected = (n/2)*(-1)**(n + 1)/(n - 1)\n    assert result == expected\n", "def test_jacobi_poly():\n    from sympy.polys.orthopolys import jacobi_poly\n    a, b = Symbol('a'), Symbol('b')\n    x = Symbol('x')\n    n = 3\n    assert jacobi(n, a, b, x) == jacobi_poly(n, a, b, x)\n    # test we get an error if n is not an integer\n    assert raises(ValueError, lambda: jacobi(n + 0.5, a, b, x))\n    # test we get an error if a or b is not a number\n    assert raises(ValueError, lambda: jacobi(n, a, b.symbols('c'), x))\n    # test we get an error if the argument is too large\n    assert raises(ValueError, lambda: jacobi(n, 1.1, b, x))\n"], "sample_142": ["    def test_model_form_construct_instance(self):\n        album = Album.objects.create(title='Test Album', artist='Test Artist')\n        form = SongForm(instance=album)\n        instance = construct_instance(form, album)\n        self.assertEqual(instance.title, album.title)\n        self.assertEqual(instance.artist, album.artist)\n", "    def test_fields_for_model(self):\n        form = SongForm()\n        fields = fields_for_model(form._meta.model)\n        self.assertIn('title', fields)\n        self.assertIn('artist', fields)\n        self.assertNotIn('album', fields)\n", "    def test_model_form_initial_data(self):\n        form = SongForm(data={'title': 'Test song'})\n        self.assertEqual(form.initial, {'title': form.instance.title})\n", "    def test_converts_model_instance_to_dict(self):\n        song = Song.objects.create(title='My Song')\n        data = model_to_dict(song)\n        self.assertEqual(data['title'], 'My Song')\n", "    def test_fields_for_model_returns_none_for_empty_formfields(self):\n        class EmptyModel:\n            class Meta:\n                pass\n        form = EmptyModel()\n        self.assertEqual(fields_for_model(form, fields=[]), {})\n"], "sample_124": ["    def test_clean(self):\n        field = MultiValueField(fields=(CharField(), CharField()))\n        data = ['hello', 'world']\n        self.assertEqual(field.clean(data), ['hello', 'world'])\n", "    def test_compress(self):\n        field = MultiValueField(fields=(IntegerField(), IntegerField()))\n        self.assertIsNone(field.compress([]))\n        self.assertEqual(field.compress([1, 2]), (1, 2))\n        self.assertEqual(field.compress([1, 2, 3]), (1, 2))\n", "    def test_CharField_to_python_with_strip(self):\n        field = CharField(strip=True)\n        self.assertEqual(field.to_python('   Hello World   '), 'Hello World')\n", "    def test_compress_method(self):\n        field = MultiValueField(fields=(IntegerField(), IntegerField()))\n        with self.assertRaises(NotImplementedError):\n            field.compress(['1', '2'])\n", "    def test_MultiValueField_compress(self):\n        # Test that MultiValueField.compress() is called correctly\n        class MyMultiValueField(MultiValueField):\n            fields = (IntegerField(), IntegerField())\n\n                return data_list[0] + data_list[1]\n\n        form = MyMultiValueField()\n        data = [1, 2]\n        self.assertEqual(form.compress(data), 3)\n"], "sample_1011": ["def test_octave_code_Indexed():\n    from sympy import IndexedBase, Idx\n    from sympy.codegen.ast import Assignment\n    from sympy.printing.codeprinter import CodePrinter\n    A = IndexedBase('A')\n    i = Idx('i', 3)\n    e = A[i]\n    printer = CodePrinter(settings={'contract': True})\n    code = printer._doprint_loops(e, e)\n    assert code == \"for i = 1:3\\n    A(i) = A(i);\\nend\"\n", "def test_octave_code_piecewise_with_non_default_condition():\n    from sympy import Piecewise, symbols\n    from sympy.printing import octave_code as mcode\n\n    x = symbols('x')\n\n    pw = Piecewise((x + 1, x > 0), (x - 1, x < 0), (x, True))\n    code = mcode(pw, assign_to='tau')\n    expected = 'tau = ((x > 0).*(x + 1) + ((x < 0).*(x - 1) + (~(x > 0) & ~(x < 0)).*(x));'\n    assert code == expected\n", "def test_octave_code_complex_number_imaginary_part():\n    assert octave_code(2 + 3j).replace('i', 'j') == '2 + 3j'\n", "def test_octave_code_hadamard_product():\n    x, y = symbols('x y')\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n\n    # HadamardProduct with MatrixSymbols is supported in octave_code\n    expected = \"A .* B\"\n    result = octave_code(HadamardProduct(A, B), user_functions={\"HadamardProduct\": \".*\"})\n    assert result == expected\n\n    # HadamardProduct between a HadamardProduct and a MatrixSymbol is supported\n    C = HadamardProduct(A, B)\n    expected = \"((A .* B) .* B)\"\n    result = octave_code(HadamardProduct(C, B), user_functions={\"HadamardProduct\": \".*\"})\n    assert result == expected\n\n    # HadamardProduct between a MatrixSymbol and a HadamardProduct is supported\n    expected = \"(A .* (B .* B))\"\n    result = octave_code(HadamardProduct(A, C), user_functions={\"HadamardProduct\": \".*\"})\n    assert result == expected\n\n    # HadamardProduct with non-MatrixSymbols is supported, but uses '.*'\n    expected = \"(x .* y)\"\n    result = octave_code(HadamardProduct(x, y), user_functions={\"HadamardProduct\": \".*\"})\n    assert result == expected\n", "def test_OctaveCodePrinter_MatPow_non_integer():\n    from sympy import Matrix, MatrixSymbol, S\n    A = MatrixSymbol('A', 2, 2)\n    assert octave_code(A**S(1/2)) == 'A^(1/2)'\n\n"], "sample_186": ["    def test_check_admin_app(self):\n        app_configs = [checks.AppConfig('django.contrib.auth')]\n        errors = check_admin_app(app_configs)\n        self.assertEqual(errors, [])\n", "    def test_check_fields(self):\n        # Test case for _check_fields method in BaseModelAdminChecks class\n        admin = admin.ModelAdmin()\n        admin.fields = ['id', 'title']\n        errors = admin.check().check(admin)\n        self.assertEqual(len(errors), 0)\n\n        admin.fields = ['id', 'title', 'id']  # Duplicate field\n        errors = admin.check().check(admin)\n        self.assertEqual(len(errors), 1)\n\n        admin.fields = ['id', 'title', 'invalid_field']  # Invalid field\n        errors = admin.check().check(admin)\n        self.assertEqual(len(errors), 1)\n", "    def test_save_as_is_boolean(self):\n        self.assertEqual(ValidFields.save_as, False)\n        ValidFields.save_as = True\n        self.assertTrue(ValidFields.save_as)\n        with self.assertRaises(ValueError):\n            ValidFields.save_as = 'invalid'\n", "    def test_admin_form_subclassing(self):\n        # Test that form subclasses BaseModelForm\n        with self.assertRaises(ValueError):\n            class MyAdmin(admin.ModelAdmin):\n                form = ValidFields\n", "    def test_inline_max_num_and_extra(self):\n        # Test that max_num and extra are validated when they are used together\n        class TestInline(admin.StackedInline):\n            model = Album\n            fk_name = 'album'\n            max_num = 10\n            extra = 2\n\n        checks = checks.CheckSystem()\n        admin_checks = ModelAdminChecks()\n        errors = admin_checks.check(TestInline)\n        self.assertEqual(len(errors), 0)\n\n        # Test that max_num is validated when extra is not provided\n        TestInline.extra = None\n        errors = admin_checks.check(TestInline)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E203')\n\n        # Test that extra is validated when max_num is not provided\n        TestInline.max_num = None\n        errors = admin_checks.check(TestInline)\n        self.assertEqual(len(errors), 0)\n\n        # Test that max_num and extra are validated when they are used together with a formset\n        TestInline.formset = SongInlineFormSet\n        errors = admin_checks.check(TestInline)\n        self.assertEqual(len(errors), 0)\n"], "sample_409": ["    def test_blocktranslate_context(self):\n        template = Template(self.templates[\"i18n04\"])\n        context = Context({\"anton\": \"hello\"})\n        result = template.render(context)\n        self.assertIn(\"hello\", result)\n", "    def test_translate_tag(self):\n        self.assertEqual(\n            Template(\"{% load i18n %}{% translate 'Hello, %s!' as var %}\").render(Context()),\n            mark_safe('Hello, %s!')\n        )\n        self.assertEqual(\n            Template(\"{% load i18n %}{% translate 'Hello, %s!' noop %}\").render(Context()),\n            mark_safe('Hello, %s!')\n        )\n        self.assertEqual(\n            Template(\"{% load i18n %}{% translate 'Hello, {{ name }}!' %}\").render(\n                Context({\"name\": \"John\"})\n            ),\n            mark_safe('Hello, John!')\n        )\n        self.assertEqual(\n            Template(\"{% load i18n %}{% translate 'Hello, {{ name }}!' context 'greeting' %}\").render(\n                Context({\"name\": \"John\"})\n            ),\n            mark_safe('Hello, John!')\n        )\n", "    def test_language_tags(self):\n        self.assertTemplateDoesNotExist(\"i18n01\")\n        self.assertTemplateDoesNotExist(\"i18n02\")\n        self.assertTemplateDoesNotExist(\"i18n03\")\n        self.assertTemplateDoesNotExist(\"i18n04\")\n\n        context = Context({\"LANGUAGES\": [(\"en\", \"English\")]})\n        output = Template(\"i18n02\").render(context)\n        self.assertEqual(output, \"\")\n\n        output = Template(\"i18n03\").render(context)\n        self.assertEqual(output, \"\")\n\n        with translation.override(\"en\"):\n            output = Template(\"i18n04\").render(context)\n            self.assertEqual(output, \"\")\n\n        with translation.override(\"fr\"):\n            output = Template(\"i18n04\").render(context)\n            self.assertEqual(output, \"\")\n", "    def test_get_available_languages(self):\n        self.assertContainsHTML(self.get_rendered_template(), \"de, en\")\n        self.assertEqual(self.context[\"langs\"], [\n            (\"de\", \"German\"),\n            (\"en\", \"English\")\n        ])\n", "compilation error"], "sample_709": ["def test_sys_modules_snapshot_preserves_readline(caplog):\n    # Arrange\n    monkeypatch = MonkeyPatch()\n    monkeypatch.setenv(\"PY_COLORS\", \"0\")\n    sys_modules_snapshot = SysModulesSnapshot()\n\n    # Act\n    readline = sys.modules[\"readline\"]\n    sys_modules_snapshot.restore()\n    sys.modules.clear()\n    sys.modules[\"readline\"] = readline  # restore the original\n\n    # Assert\n    assert readline in sys.modules\n    assert \"readline\" not in sys_modules_snapshot.__saved\n", "def test_pytester_chdir(tmp_path: Path) -> None:\n    \"\"\"Check that the working directory is changed to the test directory when creating a Pytester instance.\"\"\"\n    pytester = Pytester(request=None, tmp_path_factory=lambda: tmp_path)\n    assert pytester.path == tmp_path\n", "def test_pytester_runpytest_subprocess_fail(tmp_path: Path, monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Check that pytester.runpytest_subprocess fails if the process exits with a non-zero status.\"\"\"\n    monkeypatch.setenv(\"PATH\", \"/bin\")\n    p = tmp_path / \"test.py\"\n    p.write_text(\"import sys; sys.exit(1)\")\n\n    pytester = Pytester(tmp_path, _ispytest=True)\n    result = pytester.runpytest_subprocess(\"-v\", p)\n    assert result.ret != 0\n    assert result.outlines == []\n    assert result.errlines == [\"Usage: test.py [options]\"]\n", "    def test_run_python_c_command(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import os\n            assert os.environ['FOO'] == 'bar'\n            \"\"\"\n        )\n        result = pytester.runpython(\"-c\", 'import os; os.environ[\"FOO\"] = \"bar\"')\n        result.assert_outcomes(passed=1)\n        assert result.stdout.fnmatch_lines([\"bar\"])\n", "def test_pytester_runpytest_inprocess(tmp_path):\n    p = Pytester(tmp_path)\n    rec = p.runpytest_inprocess()\n    assert rec.ret == ExitCode.OK\n    assert rec.outlines == []\n    assert rec.errlines == []\n    assert rec.duration > 0\n"], "sample_362": ["    def test_autodetector_m2m_through(self):\n        before_states = [\n            self.author_with_m2m,\n            self.contract,\n            self.publisher,\n            self.book,\n        ]\n        after_states = [\n            self.author_with_m2m,\n            self.contract_renamed,\n            self.publisher,\n            self.book,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', new_name='Writer')\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, name='publishers', new_name='deal')\n", "    def test_rename_field_alter_field_alter_column(self):\n        author_name_renamed = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"names\", models.CharField(max_length=200)),\n        ])\n        author_name_altered = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"names\", models.CharField(max_length=250)),\n        ])\n        changes = self.get_changes([self.author_name_renamed], [self.author_name_altered])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\", \"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"name\")\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"names\", field=models.CharField(max_length=250))\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n", "    def test_alter_unique_together_adding_field(self):\n        # Test that AlterUniqueTogether operations are correctly generated\n        # when adding a field to a model with an existing unique_together constraint.\n        model_states = [\n            self.author_with_book,\n            self.book_foo_together,\n        ]\n        before_states = [\n            self.author_with_book,\n            self.book,\n        ]\n        after_states = [\n            self.author_with_book,\n            self.book_foo_together_2,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={('author', 'title')})\n", "    def test_renamed_fields_with_nested_deconstructibles(self):\n        model_state = ModelState(\n            \"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=DeconstructibleObject(\n                    DeconstructibleObject(1),\n                    (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                    a=DeconstructibleObject('A'),\n                    b=DeconstructibleObject(B=DeconstructibleObject('c')),\n                ))),\n            ]\n        )\n        # from_state is author_name_deconstructible_1\n        # to_state is author_name_nested_deconstructible_1\n        from_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=DeconstructibleObject())),\n        ])\n        to_state = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=DeconstructibleObject(\n                DeconstructibleObject(1),\n                (DeconstructibleObject('t1'), DeconstructibleObject('t2'),),\n                a=DeconstructibleObject('A'),\n                b=DeconstructibleObject(B=DeconstructibleObject('c')),\n            ))),\n        ])\n        changes = MigrationAutodetector(from_state, to_state)._detect_changes()\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(changes[\"testapp\"][0].operations[0].__class__.__name__, \"RenameField\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].model_name_lower, \"author\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].name_lower, \"name\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].old_name, \"name\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].new_name, \"name\")\n", "    def test_add_index_with_null_field(self):\n        from django.db import models\n        before = ModelState(\"app\", \"Model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=100, null=True)),\n        ])\n        after = ModelState(\"app\", \"Model\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"field\", models.CharField(max_length=100, null=True)),\n        ])\n        after.options['indexes'] = [\n            models.Index(fields=[\"field\"]),\n        ]\n        changes = MigrationAutodetector(\n            before,\n            after,\n        ).changes()\n        self.assertNumberMigrations(changes, \"app\", 1)\n        self.assertOperationTypes(changes, \"app\", 0, [\"AddIndex\"])\n        self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"Model\", model_name_lower=\"model\")\n        self.assertOperationAttributes(changes, \"app\", 0, 0, field=\"field\", index=\"app.model_field_idx\")\n"], "sample_659": ["    def test_repr_excinfo(self):\n        # Test that repr_excinfo() produces a string that is identical to\n        # repr(self.value) when no tracebacks are available\n        excinfo = ExceptionInfo.for_later()\n        excinfo._excinfo = sys.exc_info()\n        reprstr = excinfo.getrepr().__str__()\n        reprstr_self = repr(excinfo.value).__str__()\n        assert reprstr == reprstr_self\n", "def test_filter_traceback_entry():\n    frame = pytest.Frame(pytest.Frame(pytest.Frame(pytest.Frame(object())).raw))\n    entry = pytest.TracebackEntry(frame._rawentry)\n    assert filter_traceback(entry) is True\n", "    def test_getreprcrash(self):\n        # GIVEN\n        excinfo = ExceptionInfo((TypeError, TypeError(), None))\n\n        # WHEN\n        result = excinfo._getreprcrash()\n\n        # THEN\n        assert isinstance(result, ReprFileLocation)\n        assert result.path == sys.exc_info()[0].__module__\n        assert result.lineno == 0\n        assert result.message == \"TypeError()\"\n", "    def test_filter_traceback(self):\n        frame = sys._getframe(0)\n        tb = sys.exc_info()[2]\n        entry = TracebackEntry(tb)\n        assert filter_traceback(entry) is False\n        entry = TracebackEntry(tb, excinfo=ExceptionInfo.from_current())\n        assert filter_traceback(entry) is True\n", "    def test_filter_traceback(self):\n        # Test filter_traceback removes traceback entries from internal libraries\n        with pytest.raises(Failed):\n            with dummy_context_manager():\n                pytest.warns(PytestDeprecationWarning)(self).warns(\"this is a warning\")\n        tb = sys.exc_info()[2]\n        entry = next(tb)\n        assert not filter_traceback(entry)\n\n        # Test filter_traceback doesn't remove traceback entries from dynamically generated code\n        with pytest.raises(Failed):\n            with dummy_context_manager():\n                code = getrawcode(lambda: 1 / 0)\n                code.exec_()\n        tb = sys.exc_info()[2]\n        entry = next(tb)\n        assert not filter_traceback(entry)\n        assert entry.path.basename.startswith(\"<\")\n"], "sample_74": ["    def test_runshell_db_connection_params_without_user(self):\n        dbinfo = {'database': 'test_database', 'host': 'localhost', 'port': 5432, 'sslmode': 'verify-full'}\n        _, pg_env = self._run_it(dbinfo)\n        self.assertFalse('PGUSER' in pg_env)\n", "    def test_runshell_db_with_missing_required_params(self):\n        \"\"\"\n        Test that runshell_db raises a ValueError when a required parameter is missing.\n        \"\"\"\n        conn_params = {'host': 'localhost'}\n        with self.assertRaises(ValueError):\n            DatabaseClient.runshell_db(conn_params)\n", "    def test_runshell_db_empty_conn_params(self):\n        \"\"\"\n        Test DatabaseClient.runshell_db() with empty conn_params.\n        \"\"\"\n        conn_params = {}\n        with self.assertRaises(ValueError):\n            DatabaseClient.runshell_db(conn_params)\n", "    def test_runshell_db_with_all_parameters(self):\n        \"\"\"\n        Test that the runshell_db method correctly handles all database connection parameters.\n        \"\"\"\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'test_database',\n            'user': 'test_user',\n            'password': 'test_password',\n            'sslmode': 'verify-full',\n            'sslrootcert': '/path/to/sslrootcert',\n            'sslcert': '/path/to/sslcert',\n            'sslkey': '/path/to/sslkey',\n        }\n        args, pg_env = self._run_it(dbinfo)\n\n        # Assert that the command line list is correct.\n        expected_args = [\n            'psql',\n            '-h', 'localhost',\n            '-p', '5432',\n            '-U', 'test_user',\n            '-d', 'test_database',\n            '-c', 'set client_min_messages TO ERROR',\n            '-c', 'set standard_conforming_strings TO on',\n            '-c', 'set timezone TO UTC',\n        ]\n        self.assertEqual(args, expected_args)\n\n        # Assert that the PG* environment variables are correct.\n        expected_pg_env = {\n            'PGPASSWORD': 'test_password',\n            'PGSSLMODE': 'verify-full',\n            'PGSSLROOTCERT': '/path/to/sslrootcert',\n            'PGSSLCERT': '/path/to/sslcert',\n            'PGSSLKEY': '/path/to/sslkey',\n        }\n        self.assertEqual(pg_env, expected_pg_env)\n", "    def test_runshell_db_with_nonexistent_host(self):\n        \"\"\"\n        Test that DatabaseClient.runshell_db() raises a subprocess.CalledProcessError when\n        the host parameter is nonexistent.\n        \"\"\"\n        dbinfo = {\n            'database': 'test',\n            'user': 'test',\n            'password': 'test',\n            'host': 'nonexistent-host',\n            'port': '5432',\n        }\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db(dbinfo)\n"], "sample_1180": ["def test_point3d_scale():\n    p = Point3D(1, 2, 3)\n    assert p.scale(2, 3).args == (2, 6, 3)\n    assert p.scale(2).args == (2, 6, 6)\n    assert p.scale(2, 3, 4).args == (2, 6, 12)\n", "def test_Point_project():\n    from sympy.geometry import Point\n\n    p1 = Point(1, 2, 3)\n    p2 = Point(4, 5, 6)\n    z = Point.origin\n    assert Point.project(p1, p2) == p1.midpoint(p2)\n", "def test_point_distance():\n    p1 = Point(1, 1)\n    p2 = Point(4, 5)\n    assert p1.distance(p2) == 5\n    p3 = Point(0, 0)\n    assert p1.distance(p3) == sqrt(26)\n\n    # test with sympy expressions\n    x, y = Symbol('x'), Symbol('y')\n    p1 = Point(x, y)\n    p2 = Point(0, 0)\n    assert p1.distance(p2) == sqrt(x**2 + y**2)\n\n    # test with non-Point object\n    with raises(TypeError):\n        p1.distance('point')\n\n    # test with invalid Point\n    with raises(TypeError):\n        p1.distance(Point2D([1, 2]))\n\n    # test with non-GeometryEntity\n    with raises(TypeError):\n        p1.distance(5)\n\n    p1 = Point(1, 2)\n    l = Line((3, 1), (2, 2))\n    assert p1.distance(l) == sqrt(2)\n\n    # test with invalid Line\n    with raises(TypeError):\n        p1.distance('line')\n\n    # test with non-GeometryEntity\n    with raises(TypeError):\n        p1.distance(5)\n", "def test_is_collinear_collinear_points():\n    x = Symbol('x')\n    p1 = Point2D(0, 0)\n    p2 = Point2D(x, x)\n    p3 = Point2D(0, 0)\n    assert p1.is_collinear(p2, p3)\n    assert p1.is_collinear(p2, p2)\n", "    def test__new__raises_TypeError_when_coords_is_not_sequence(self):\n        with raises(TypeError):\n            Point('invalid', evaluate=False)\n"], "sample_385": ["    def test_required_autocomplete_widget(self):\n        form = NotRequiredBandForm(data={\"band\": \"\"})\n        self.assertTrue(form.is_valid())\n        form = RequiredBandForm(data={\"band\": \"\"})\n        self.assertFalse(form.is_valid())\n", "    def test_get_related_url(self):\n        album = Album.objects.create(band=\"Test Band\", featuring=\"Test Featuring\")\n        video_stream = VideoStream.objects.create(release_event=album)\n        widget = ForeignKeyRawIdWidget(Album._meta.get_field(\"band\").rel, admin.site)\n        self.assertEqual(\n            widget.get_related_url(\n                (\"myapp\", \"Album\"), \"change\", video_stream.id\n            ),\n            \"/admin/myapp/album/%d/change/\" % video_stream.id,\n        )\n", "    def setUp(self):\n        translation.activate(\"en\")\n        self.album = Album.objects.create(band=Band.objects.create(name=\"Band\"), title=\"Album\")\n        self.release_event = ReleaseEvent.objects.create(title=\"Release Event\")\n        self.video_stream = VideoStream.objects.create(release_event=self.release_event)\n", "    def test_autocomplete_multiple_select_widget_with_required_field(self):\n        form = NotRequiredBandForm({\n            \"band\": \"\",\n        })\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors[\"band\"], [\"This field is required.\"])\n", "    def test_rendering_disabled(self):\n        # This test is to ensure that rendering of the autocomplete widget is disabled\n        # when the field is not required.\n        form = NotRequiredBandForm()\n        self.assertFalse(form[\"band\"].widget.media)\n        self.assertEqual(form[\"band\"].widget.optgroups(\"band\", None, None), [])\n"], "sample_631": ["    def test_unused_import_warning(self):\n        test_file = os.path.join(REGR_DATA_DIR, 'test_unused_import.py')\n        with set_config(ini='pylint.rc', options=['--no-redefined-builtin']):\n            linter = linter.Linter()\n            linter.infos = []\n            linter.warning('test_unused_import.py')\n            self.assertEqual(len(linter.infos), 1)\n", "    def test_no_variables_in_loop_used_outside_loop(self):\n        \"\"\"Check that no variables in loop are used outside the loop\"\"\"\n        # setup\n        test_script = \"\"\"\n            for x in range(10):\n                pass\n            print(x)\n        \"\"\"\n        # run\n        linter = linter()\n        linter.checker_registry.register(variables.VariablesChecker(linter))\n        messages = linter.check_tree(astroid.parse(test_script))\n        # check\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(messages[0].msg_id, \"undefined-loop-variable\")\n        self.assertEqual(messages[0].node, astroid.parse(test_script).body[1].body[0].body[0])\n", "    def test_unused_import_with_empty_namespace_package(self):\n        # pylint: disable=unused-import\n        \"\"\"Test unused import in a package.\"\"\"\n        self._run_test(\"\"\"\n            # -*- coding: utf-8 -*-\n            package = \"example\"\n            import os\n            unused_import = \"module\"\n        \"\"\", {\n            \"messages\": [\n                Message(\n                    \"unused-import\",\n                    line=4,\n                    col=0,\n                    args=(\"module\",),\n                    node=4,\n                ),\n            ],\n        })\n", "    def test_unused_variables_in_lambda(self):\n        \"\"\"Check that unused variables in a lambda are reported.\"\"\"\n        script = \"\"\"", "    def test_attribute_access_from_multiple_imports(self):\n        \"\"\"test attribute access from multiple imports with different module path\"\"\"\n        linter = linter.get_linter()\n        set_config(linter, {\n            \"load-plugins\": [\"variables\"],\n            \"init-import\": False,\n            \"dummy-variables-rgx\": r'^_[a-zA-Z]+$',\n            \"additional-builtins\": [],\n            \"callbacks\": [\"cb_\", \"_cb\"],\n            \"redefining-builtins-modules\": [\"six.moves\", \"past.builtins\", \"builtins\", \"io\"],\n            \"ignored-argument-names\": r'^_.*',\n            \"allow-global-unused-variables\": False,\n            \"allowed-redefined-builtins\": []\n        })\n\n        linter.register_checker(variables.VariablesChecker(linter))\n\n        test_script = \"\"\"\n            from six import moves\n            from past.builtins import some_built_in\n            from builtins import some_other_built_in\n            from io import io_module\n\n            moves.some_built_in\n            some_built_in.some_other_attribute\n            some_other_built_in.something_else\n            io_module.something\n        \"\"\"\n\n        linter.checker_instance._to_consume = [None]\n\n        with linter.run_on_text(test_script):\n            self.assertHasMessage(linter, \"no-name-in-module\", args=(\"some_built_in\", \"six\"))\n            self.assertHasMessage(linter, \"no-name-in-module\", args=(\"some_other_built_in\", \"builtins\"))\n            self.assertHasMessage(linter, \"no-name-in-module\", args=(\"something_else\", \"past.builtins\"))\n            self.assertHasMessage(linter, \"no-name-in-module\", args=(\"something\", \"io\"))\n            self.assertHasNoMessage(linter, \"no-name-in-module\", args=(\"moves\", \"six\"))\n            self.assertHasNoMessage(linter, \"no-name-in-module\", args=(\"builtins\", \"builtins\"))\n"], "sample_919": ["compilation error", "compilation error", "def test_class_with_bases():\n    ast = parse('class', '''\n    class A : public B, public C\n    {\n        int x;\n    };\n    ''')\n    idDict = {\n        1: 'cA0',\n        2: 'cB0I0E',\n        3: 'cC0I0E',\n        4: 'cA0I0E'\n    }\n    output = '''\n    class A : public B, public C\n    {\n        int x;\n    };\n    '''\n    check('class', '''\n    class A : public B, public C\n    {\n        int x;\n    };\n    ''', idDict, output)\n", "compilation error", "compilation error"], "sample_967": ["def test_install_mathjax_config():\n    \"\"\"Test install_mathjax configuration.\"\"\"\n    app = Sphinx('docs', 'source', 'build', 'conf.py', init_files=[])\n\n    # Test that it works when mathjax_path is not set\n    app.config.mathjax_path = None\n    with pytest.raises(ExtensionError):\n        install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_path is set\n    app.config.mathjax_path = MATHJAX_URL\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_options is not set\n    app.config.mathjax_options = None\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_options is set\n    app.config.mathjax_options = {'defer': 'defer'}\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_inline is not set\n    app.config.mathjax_inline = None\n    with pytest.raises(ExtensionError):\n        install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_inline is set\n    app.config.mathjax_inline = [r'\\(', r'\\)']\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_display is not set\n    app.config.mathjax_display = None\n    with pytest.raises(ExtensionError):\n        install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_display is set\n    app.config.mathjax_display = [r'\\[', r'\\]']\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_config is not set\n    app.config.mathjax_config = None\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax_config is set\n    app.config.mathjax_config = {'mathjax': {'message': 'Test'}}\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it works when mathjax2_config is not set\n    app.config.mathjax2_config = None\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    # Test that it", "def test_install_mathjax_config():\n    app = Sphinx()\n    config = {'mathjax_path': MATHJAX_URL, 'mathjax_config': {'extensions': ['asciimath2jax.js']}}\n    install_mathjax(app, 'index', 'index', config, None)\n    assert app.add_js_file.called_once_with(MATHJAX_URL)\n    assert app.add_js_file.called_once_with(None, 'text/x-mathjax-config', 'MathJax.Hub.Config({\"extensions\": [\"asciimath2jax.js\"]})')\n", "def test_mathjax_path(app, config_option, expected):\n    app.config.mathjax_path = config_option\n    app.builder.format = 'html'\n    app.builder.math_renderer_name = 'mathjax'\n    result = app.builder.config.mathjax_path\n    assert result == expected\n", "def test_install_mathjax_config(app_config, expected_error, caplog, capsys):\n    from sphinx.application import Sphinx\n    app = Sphinx(srcdir='.', outdir='build', confoverrides={'extensions': ['sphinx.ext.mathjax'], 'mathjax_path': 'invalid_url'})\n    app.builder.format = 'html'\n    app.builder.math_renderer_name = 'mathjax'\n    app.config.update(app_config)\n\n    install_mathjax(app, 'index', 'index.html', {}, None)\n    captured = capsys.readouterr()\n    assert expected_error in captured.out\n    assert 'WARNING' in caplog.text\n", "def test_install_mathjax_config_warning(caplog):\n    # Create an app with the mathjax extension and some config values\n    app = Sphinx(appname='test', srcdir='.', outdir='build', confoverrides={\n        'extensions': ['sphinx.ext.mathjax'],\n        'mathjax_path': MATHJAX_URL,\n        'mathjax3_config': {'tex': {'macro': 'test_macro'}},\n        'mathjax2_config': 'mathjax2_config'\n    })\n\n    # Add the 'html-page-context' event\n    event = {'pagename': 'index', 'templatename': 'index', 'context': {}, 'app': app}\n    install_mathjax(app, **event)\n\n    # Check that a warning is raised when using mathjax2_config with a CDN url\n    assert 'mathjax_config/mathjax2_config does not work for the current MathJax version, use mathjax3_config instead' in caplog.text\n\n    # Create a new app with the mathjax extension and some config values\n    app2 = Sphinx(appname='test', srcdir='.', outdir='build', confoverrides={\n        'extensions': ['sphinx.ext.mathjax'],\n        'mathjax_path': MATHJAX_URL,\n        'mathjax3_config': {'tex': {'macro': 'test_macro'}}\n    })\n\n    # Add the 'html-page-context' event\n    event = {'pagename': 'index', 'templatename': 'index', 'context': {}, 'app': app2}\n\n    # Check that no warning is raised when using mathjax3_config\n    with caplog.at_level(logging.WARNING):\n        install_mathjax(app2, **event)\n\n    assert 'mathjax_config/mathjax2_config does not work for the current MathJax version, use mathjax3_config instead' not in caplog.text\n"], "sample_318": ["    def test_resolve_empty_path(self):\n        resolver = get_resolver()\n        with self.assertRaises(Resolver404):\n            resolver.resolve('')\n", "    def test_resolve_with_nested_namespaces(self):\n        # Test that resolve() handles nested namespaces correctly.\n        urlconf = URLResolver(RegexPattern(r'^'), [\n            URLResolver(RegexPattern(r'^'), [\n                URLPattern(path('ns1/', name='ns1'), views.empty_view),\n                URLPattern(path('ns1/ns2/', name='ns1:ns2'), views.empty_view),\n            ]),\n            URLPattern(path('ns3/', name='ns3'), views.empty_view),\n        ])\n        resolver_match = urlconf.resolve('/ns1/ns2/42/')\n        self.assertEqual(resolver_match.url_name, 'ns1:ns2')\n        self.assertEqual(resolver_match.app_names, ['ns1', 'ns2'])\n        self.assertEqual(resolver_match.namespaces, ['ns1', 'ns2'])\n        self.assertEqual(resolver_match.route, 'ns1/ns2/')\n        self.assertEqual(resolver_match.kwargs, {'arg1': '42'})\n", "    def test_resolve_error_handler(self):\n        from django.urls import path\n\n        class Custom404View:\n                self.request = request\n\n        self.urlconf = path('', lambda request: HttpResponse('Hello'))\n        self.request_factory = RequestFactory()\n\n        resolver = get_resolver(self.urlconf)\n        self.assertEqual(resolver.resolve_error_handler(404), Custom404View)\n\n        custom_404_view = Custom404View(self.request_factory.get('/non-existent-page/'))\n        self.assertEqual(resolver.resolve('/non-existent-page/').func, custom_404_view)\n\n        with self.assertRaises(ViewDoesNotExist):\n            resolver.resolve('/non-existent-page/').func(self.request_factory.get('/non-existent-page/'))\n\n        self.assertEqual(resolver.resolve_error_handler(500), lambda request: HttpResponse('Internal Server Error'))\n", "    def test_resolve_with_nested_namespaces(self):\n        # Create a resolver for the outer URLconf\n        resolver = get_resolver()\n        # Create a resolver for the inner URLconf\n        inner_resolver = get_ns_resolver('namespace_urls', resolver, {'converter1': 'str', 'converter2': 'int'})\n        \n        # Test resolving a URL in the inner namespace\n        match = inner_resolver.resolve('/test3/inner/42/37/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '42', 'arg2': '37'})\n\n        # Test resolving a URL in the outer namespace\n        match = resolver.resolve('/ns-included1/test3/inner/42/37/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '42', 'arg2': '37'})\n\n        # Test resolving a URL that doesn't exist in the inner namespace\n        with self.assertRaises(Resolver404):\n            inner_resolver.resolve('/test3/outer/42/37/')\n\n        # Test resolving a URL that doesn't exist in the outer namespace\n        with self.assertRaises(Resolver404):\n            resolver.resolve('/ns-included1/test4/outer/42/37/')\n\n        # Test resolving a URL in the outer namespace with a nested namespace\n        match = resolver.resolve('/ns-included1/ns-included4/ns-included2/test3/inner/42/37/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '42', 'arg2': '37'})\n", "    def test_reverse_nested_app_namespace(self):\n        # Test case for reversing a URL with nested app and namespace\n        urlconf = get_urlconf()\n        resolver = get_resolver(urlconf)\n\n        # Define a URL pattern with nested app and namespace\n        @path('app/test/<str:arg1>/', name='nested-app-ns-test')\n            return HttpResponse(f\"Nested app ns test: {arg1}\")\n\n        # Test reversing the URL with namespace\n        namespace = 'app:namespace'\n        url = reverse(f'{namespace}:nested-app-ns-test', args=['test'])\n        self.assertEqual(url, f'/app/namespace/test/test/')\n\n        # Test reversing the URL with app and namespace\n        url = reverse(f'{namespace}:nested-app-ns-test', args=['test'])\n        self.assertEqual(url, f'/app/namespace/test/test/')\n"], "sample_555": ["def test_FancyArrowPatch_arrowstyle():\n    # Test that FancyArrowPatch's arrowstyle parameter is correctly passed to\n    # the ArrowStyle instance\n    arrowstyle = ArrowStyle(\"Fancy\")\n    fancy_arrow = FancyArrowPatch(posA=(0, 0), posB=(1, 1),\n                                 arrowstyle=arrowstyle)\n    assert isinstance(fancy_arrow.get_arrowstyle(), ArrowStyle)\n    assert fancy_arrow.get_arrowstyle() == arrowstyle\n", "def test_fancyarrow():\n    # Test FancyArrow with default parameters\n    fig, ax = plt.subplots()\n    fa = FancyArrow(0.1, 0.5, 0.5, 0, head_width=0.3, head_length=0.4)\n    ax.add_patch(fa)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_aspect('equal')\n    plt.show()\n\n    # Test FancyArrow with non-default parameters\n    fa = FancyArrow(0.1, 0.5, 0.5, 0, head_width=0.1, head_length=0.2,\n                    width=0.5, length_includes_head=False)\n    ax.add_patch(fa)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_aspect('equal')\n    plt.show()\n\n    # Test FancyArrow with head_width and head_length set to None\n    fa = FancyArrow(0.1, 0.5, 0.5, 0, head_width=None, head_length=None)\n    ax.add_patch(fa)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_aspect('equal')\n    plt.show()\n\n    # Test FancyArrow with negative head_width and head_length\n    fa = FancyArrow(0.1, 0.5, 0.5, 0, head_width=-0.1, head_length=-0.2)\n    ax.add_patch(fa)\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_aspect('equal')\n    plt.show()\n\n    # Test FancyArrow with invalid parameters\n    with pytest.raises(ValueError):\n        FancyArrow(0.1, 0.5, 0.5, 0, head_width='invalid')\n    with pytest.raises(ValueError):\n        FancyArrow(0.1, 0.5, 0.5, 0, head_length='invalid')\n    with pytest.raises(ValueError):\n        FancyArrow(0.1, 0.5, 0.5, 0, width='invalid')\n    with pytest.raises(ValueError):\n        FancyArrow(0.1, 0.5, 0.5, 0, length_includes_head='invalid')\n\n    # Test", "    def setup_method(self):\n        self.figure, self.ax = plt.subplots()\n", "def test_FancyArrowPatch_draw():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    arrow = FancyArrowPatch(posA=(0, 0), posB=(0.5, 0.5),\n                            arrowstyle='simple', connectionstyle='arc3',\n                            mutation_scale=0.5, mutation_aspect=None)\n    ax.add_patch(arrow)\n    ax.draw_artist(arrow)\n    img = ax.figure.canvas.renderer._renderer\n    bbox = arrow.get_window_extent(renderer=img)\n    rect = Rectangle(xy=(bbox.x0, bbox.y0), width=bbox.width,\n                    height=bbox.height, fill=False, edgecolor='k')\n    rect.draw(img)\n    draw_bbox(bbox, img, color='k', trans=None)\n    assert np.array_equal(img.get_image().tostring(), np.array([0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0", "    def test_edge_default(self):\n        patch = FancyArrowPatch()\n        assert patch._edge_default is True\n"], "sample_975": ["def test_solve_linear_system_LU():\n    from sympy import Matrix, symbols\n    x, y, z = symbols('x y z')\n    system = Matrix([\n        [1, 2, 0, 1],\n        [3, 2, 2, 1],\n        [2, 0, 0, 1]\n    ])\n    assert solve_linear_system_LU(system, x, y, z) == {x: 1/2, y: 1/4, z: -1/2}\n\n    system = Matrix([\n        [1, 0, 0, 1],\n        [0, 1, 0, 1],\n        [0, 0, 1, 1]\n    ])\n    assert solve_linear_system_LU(system, x, y, z) == {x: 1, y: 1, z: 1}\n\n    system = Matrix([\n        [1, 2, 0, 1],\n        [0, 1, 2, 1],\n        [2, 0, 0, 1]\n    ])\n    assert solve_linear_system_LU(system, x, y, z) == {}\n\n    system = Matrix([\n        [1, 2, 0, 1],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ])\n    assert solve_linear_system_LU(system, x, y, z) == {}\n\n    system = Matrix([\n        [1, 2, 0, 1],\n        [0, 0, 2, 1],\n        [2, 0, 0, 1]\n    ])\n    assert solve_linear_system_LU(system, x, y, z) == {}\n", "def test_solvers_solve_linear_system_LU():\n    # test that there is no solutions\n    x, y = symbols('x y')\n    assert solve_linear_system_LU(Matrix([[0, 0, 0], [0, 0, 0]]), [x, y]) == {}\n\n    # test that there is one solution\n    x, y = symbols('x y')\n    eqs = Matrix([[1, 2, 1], [3, 2, 1], [2, 0, 1]])\n    assert solve_linear_system_LU(eqs, [x, y]) == {x: 1/2, y: 1/4}\n\n    # test that there are many solutions\n    x, y = symbols('x y')\n    eqs = Matrix([[1, 2, 0, 1], [3, 2, 2, 1], [2, 0, 0, 1]])\n    assert solve_linear_system_LU(eqs, [x, y]) == {x: 1/2, y: -1/2}\n", "def test_nsolve_with_multiple_functions():\n    from sympy import sin, cos, cosh, sinh, exp, lambdify, Matrix\n    from sympy.abc import x\n    from sympy.solvers import nsolve\n\n    f = sin(x)*cosh(x) - 1\n    df = cos(x)*cosh(x) + sin(x)*sinh(x)\n\n    # Test that nsolve works with a custom derivative\n    x0 = 3.14*100\n    f_lambdify = lambdify(x, f, 'mpmath')\n    df_lambdify = lambdify(x, df, 'mpmath')\n\n    sol = nsolve(f, x, x0, f=f_lambdify, df=df_lambdify)\n    assert mnorm(sol - 312.588469032184) < 1e-9\n\n    # Test that nsolve works with the default derivative\n    sol = nsolve(f, x, x0)\n    assert mnorm(sol - 312.588469032184) < 1e-9\n", "def test_nsolve():\n    x = Symbol('x')\n    assert nsolve(Eq(x**2 - 2, 0), 0) == mpf(1.4142135623730951)\n    assert nsolve(x**2 - 2, 0) == mpf(1.4142135623730951)\n    assert nsolve(Eq(sin(x), 0), 0) == mpf(3.141592653589793)\n    assert nsolve(sin(x), 0) == mpf(3.141592653589793)\n    assert nsolve(sin(x) - 0.5, 0) == mpf(0.8695652173913043)\n    assert nsolve(Eq(x**2 + y**2 - 25, 0), 0) == mpf(4.898979485566356)\n    assert nsolve(x**2 + y**2 - 25, 0) == mpf(4.898979485566356)\n    assert nsolve(x**2 + y**2 - 25, 0, prec=20) == mpf(4.898979485566356)\n    assert nsolve(x**2 + y**2 - 25, 0, solver='brentq') == mpf(4.898979485566356)\n    assert nsolve(x**2 + y**2 - 25, 0, solver='brentq', verify=False) == mpf(4.898979485566356)\n    # Test with a function that has a discontinuity\n    f = lambda x: mpf(2)/x\n    assert nsolve(f(x), 1, verify=False) == mpf(1.0)\n    # Test with a function that has a complex root\n    f = lambda x: mpf(2)*x**2 - mpf(1)\n    assert nsolve(f(x), 0, solver='bisect', verify=False) == mpf(0.7071067811865475 + 0.7071067811865475j)\n    # Test with a function that has a root close to the initial guess\n    f = lambda x: mpf(1)/x - mpf(0.0001)\n    assert nsolve(f(x), 1) == mpf(1.0)\n    # Test with a function that has a root with", "def test_solve_real_roots():\n    x = symbols('x')\n    eq = x**5 - 4*x**4 + 8*x**3 - 8*x**2 + 7\n    sols = nsolve(eq, x, (0, 1))\n    for sol in sols:\n        assert mnorm(eq.subs(x, sol)) < 1e-9\n    # test: https://github.com/sympy/sympy/issues/14072#issuecomment-464144861\n    # assert mnorm(mpf(0.19287309935246) - sols[0]) < 1e-9\n    # assert mnorm(mpf(1.27844411169911) - sols[1]) < 1e-9\n    # test: https://github.com/sympy/sympy/issues/14072#issuecomment-464144861\n    # assert mnorm(mpf(0.73908513321516) - sols[2]) < 1e-9\n"], "sample_194": ["    def test_BaseConstraint_deconstruct(self):\n        constraint = BaseConstraint('test')\n        deconstructed = constraint.deconstruct()\n        self.assertEqual(deconstructed[0], 'django.db.models.constraints.BaseConstraint')\n        self.assertEqual(deconstructed[1], ())\n        self.assertEqual(deconstructed[2], {'name': 'test'})\n", "    def test_check_constraint_creation(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                constraints = [\n                    CheckConstraint(check=Q(field1='test'), name='my_check_constraint')\n                ]\n\n        MyModel.objects.create(field1='test', field2='test')\n\n        self.assertEqual(get_constraints('myapp_mymodel'), [\n            ('my_check_constraint', 'CHECK (field1 = %s) DEFERRABLE INITIALLY DEFERRED', 'test',),\n        ])\n", "    def test_create_check_constraint_sql(self):\n        model = models.Model()\n        constraint = CheckConstraint(check=models.Q(a__gt=1), name='test_check')\n        schema_editor = mock.Mock(spec=models.schema.DatabaseWrapper)\n        self.assertIn(\n            'CHECK (a > 1)',\n            constraint.create_sql(model, schema_editor)\n        )\n", "    def test_check_constraint_creation(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    CheckConstraint(\n                        check=Q(field1='hello', field2='world'),\n                        name='check_constraint_example'\n                    )\n                ]\n\n        # Test that the constraint is created correctly\n        with self.assertLogs(level='INFO') as cm:\n            with atomic():\n                MyModel.objects.create(field1='hello', field2='world')\n            self.assertIn('INFO:check_constraint_example: true', cm.output)\n        self.assertEqual(MyModel._meta.constraints[0].name, 'check_constraint_example')\n        self.assertEqual(MyModel._meta.constraints[0].check, Q(field1='hello', field2='world'))\n\n        # Test that the constraint is enforced\n        with self.assertRaises(IntegrityError):\n            MyModel.objects.create(field1='hello', field2='other')\n", "    def test_check_constraint_creation(self):\n        # Test that a CheckConstraint can be created and its SQL is generated correctly\n        class MyModel(models.Model):\n            field1 = models.IntegerField()\n            field2 = models.IntegerField()\n\n        constraint = CheckConstraint(check=Q(field1__gt=0) | Q(field2__lt=0), name='test_constraint')\n        self.assertEqual(constraint.constraint_sql(MyModel, connection), 'CHECK (\"field1\" > 0 OR \"field2\" < 0)')\n\n        # Test that the constraint's SQL is properly quoted\n        constraint = CheckConstraint(check=Q(field1__gt='1'), name='test_constraint')\n        self.assertEqual(constraint.constraint_sql(MyModel, connection), 'CHECK (\"field1\" > 1)')\n\n        # Test that the constraint's SQL is properly generated for a boolean expression\n        constraint = CheckConstraint(check=(Q(field1__gt=0) & Q(field2__lt=0)), name='test_constraint')\n        self.assertEqual(constraint.constraint_sql(MyModel, connection), 'CHECK (\"field1\" > 0 AND \"field2\" < 0)')\n"], "sample_236": ["    def test_fast_delete_with_one_to_one_relation(self):\n        \"\"\"Test fast delete with a one-to-one relation.\"\"\"\n        # Create a parent model instance\n        parent = Parent.objects.create(name='parent')\n\n        # Create a child model instance with a one-to-one relation\n        child = Child.objects.create(parent=parent)\n\n        # Create a collector\n        collector = Collector(using='default')\n\n        # Add the child instance to the collector\n        collector.add([child])\n\n        # The child instance should be fast-deleted\n        self.assertTrue(collector.can_fast_delete(child))\n\n        # The parent instance should not be fast-deleted\n        self.assertFalse(collector.can_fast_delete(parent))\n\n        # Delete the child instance\n        collector.delete()\n\n        # The child instance should be deleted\n        self.assertIsNone(Child.objects.get(id=child.id))\n\n        # The parent instance should still exist\n        self.assertIsNotNone(Parent.objects.get(id=parent.id))\n", "    def test_protected_error(self):\n        # Test that ProtectedError is raised when trying to delete an object with a protected foreign key\n        b = B()\n        b.save()\n        b2 = B2(b=b)\n        b2.save()\n        with self.assertRaises(ProtectedError):\n            b.delete()\n", "    def test_protection_with_multiple_related_objects(self):\n        \"\"\"\n        Test that the collector raises a ProtectedError when multiple objects\n        are protected and cannot be deleted together.\n        \"\"\"\n        obj1, obj2, obj3 = create_a(), create_a(), create_a()\n        obj1.referrer = obj2\n        obj2.referrer = obj3\n        obj3.referrer = obj1\n        with self.assertRaises(ProtectedError):\n            Collector(using='default').collect([obj1])\n", "    def test_protected_error_raised_when_protected_object_referenced(self):\n        # Create protected objects\n        r = Referrer.objects.create(referrer_id=self.DEFAULT)\n        r2 = Referrer.objects.create(referrer_id=self.DEFAULT)\n        p = Parent.objects.create(referrer=r)\n\n        # Create objects that reference the protected objects\n        c = Child.objects.create(referrer=r)\n        c2 = Child.objects.create(referrer=r2)\n\n        # Try to delete a protected object\n        with self.assertRaises(ProtectedError):\n            Collector().collect([p])\n", "    def test_related_objects(self):\n        # Test that related_objects correctly handles relations with null values.\n        model = self.DEFAULT\n        collector = Collector(using='default')\n        # Create a parent object with a null value in the related field.\n        parent = model.objects.create(name='Parent with null value')\n        collector.collect([parent])\n        related_objects = collector.related_objects(model, model.related_model.all_field, [parent])\n        self.assertEqual(related_objects.count(), 0)\n"], "sample_443": ["    def setUp(self):\n        self.cache = FileBasedCache(os.path.join(tempfile.gettempdir(), \"cache\"), {})\n", "    def setUp(self):\n        self.cache = FileBasedCache(dir=tempfile.gettempdir(), params={})\n", "    def setUp(self):\n        self.cache = FileBasedCache(tempfile.mkdtemp(), {})\n", "    def test_touch_with_expired_key(self):\n        cache = FileBasedCache(settings.CACHES[\"default\"][\"LOCATION\"], settings.CACHES[\"default\"])\n        key = \"test_key\"\n        value = \"test_value\"\n        cache.set(key, value, 60)  # Set cache with short timeout\n        self.assertFalse(cache.touch(key, 30))  # Touch should return False for expired key\n        cache.touch(key, 60)  # Touch should renew cache\n", "    def test_cull_frequency_0(self):\n        # Test that culling the entire cache when cull_frequency = 0\n        cache = caches[\"default\"]\n        cache.set(\"key1\", \"value1\", 60)\n        cache.set(\"key2\", \"value2\", 60)\n        cache.set(\"key3\", \"value3\", 60)\n\n        # Ensure we're above max_entries\n        cache._max_entries = 2\n\n        # Cull the cache\n        cache._cull()\n\n        # Check that only key1 was deleted\n        self.assertTrue(cache.has_key(\"key1\"))\n        self.assertFalse(cache.has_key(\"key2\"))\n        self.assertFalse(cache.has_key(\"key3\"))\n"], "sample_212": ["    def setUp(self):\n        self.request = HttpRequest()\n        self.request.COOKIES = {}\n        self.request.session = {}\n        self.response = HttpResponse()\n        self.middleware = SessionMiddleware()\n", "    def test_session_save_every_request_setting(self):\n        # Test that the session is saved every request when SESSION_SAVE_EVERY_REQUEST is True\n        # and the session has been modified\n        settings.SESSION_SAVE_EVERY_REQUEST = True\n        request = HttpRequest()\n        request.session = {}\n        request.session['key'] = 'value'\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        middleware.process_response(request, HttpResponse())\n        self.assertTrue(request.session.modified)\n", "def test_session_middleware_empty_session_deletion(self):\n    \"\"\"\n    Test that a session cookie is deleted when the session is empty.\n    \"\"\"\n    # Create a test request and response\n    request = HttpRequest()\n    response = HttpResponse()\n\n    # Set up the session middleware\n    middleware = SessionMiddleware(get_response=lambda x: None)\n    request.session = None  # Simulate an empty session\n\n    # Set the session cookie on the request\n    request.COOKIES[settings.SESSION_COOKIE_NAME] = 'session_key'\n\n    # Process the request\n    middleware.process_request(request)\n\n    # Check that the session is empty\n    self.assertTrue(request.session is None)\n\n    # Process the response\n    response = middleware.process_response(request, response)\n\n    # Check that the session cookie has been deleted\n    self.assertNotIn(settings.SESSION_COOKIE_NAME, response.COOKIES)\n\n    # Check that the Vary header has been set correctly\n    self.assertIn('Cookie', response['Vary'])\n", "    def test_process_request_and_save_session(self):\n        # Test that session is properly saved and updated in response\n        from django.conf import settings\n        from django.contrib.sessions.backends.db import DatabaseBackend\n        settings.SESSION_ENGINE = 'django.contrib.sessions.backends.db'\n\n        class MockRequest(HttpRequest):\n            COOKIES = {}\n            session = None\n\n        class MockSessionMiddleware(SessionMiddleware):\n                self.get_response = get_response\n\n                super().process_request(request)\n                request.session['test_key'] = 'test_value'\n\n                return super().process_response(request, response)\n\n        request = MockRequest()\n        middleware = MockSessionMiddleware(get_response=lambda x: HttpResponse())\n        middleware.process_request(request)\n        middleware.process_response(request, HttpResponse())\n\n        self.assertEqual(request.session['test_key'], 'test_value')\n\n        response = middleware.process_response(request, HttpResponse())\n        self.assertEqual(response.cookies, {\n            settings.SESSION_COOKIE_NAME: request.session.session_key,\n        })\n", "    def test_session_save_every_request(self):\n        \"\"\"\n        Verify that session save is called for each request when\n        SESSION_SAVE_EVERY_REQUEST is True.\n        \"\"\"\n        from django.test.client import Client\n        client = Client()\n        client.get('/')\n\n        # Check that session is saved after the first request\n        with self.assertWarns(RemovedInDjango40Warning, msg='Passing None for the middleware get_response argument is deprecated.'):\n            SessionMiddleware()(None, client.response)\n\n        # Check that session is saved again after the second request\n        client.get('/')\n        with self.assertWarns(RemovedInDjango40Warning, msg='Passing None for the middleware get_response argument is deprecated.'):\n            SessionMiddleware()(None, client.response)\n\n        # Check that session is not saved when request status code is 500\n        client.response.status_code = 500\n        with self.assertWarns(RemovedInDjango40Warning, msg='Passing None for the middleware get_response argument is deprecated.'):\n            SessionMiddleware()(None, client.response)\n        self.assertFalse(client.response.cookies)\n"], "sample_297": ["    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='a1', num=1001)\n        cls.a2 = Author.objects.create(name='a2', num=2002)\n        cls.a3 = Author.objects.create(name='a3', num=3003)\n        cls.t1 = Tag.objects.create(name='t1')\n        cls.t2 = Tag.objects.create(name='t2')\n        cls.i1 = Item.objects.create(name='one', creator=cls.a1)\n        cls.i2 = Item.objects.create(name='two', creator=cls.a2)\n        cls.i3 = Item.objects.create(name='three', creator=cls.a3)\n        cls.r1 = Report.objects.create(name='r1', creator=cls.a1)\n        cls.r2 = Report.objects.create(name='r2', creator=cls.a2)\n", "    def test_no_annotation(self):\n        query = Annotation.objects.all()\n        result = list(query)\n        self.assertEqual(len(result), 2)\n        self.assertIn('a1', [ann.name for ann in result])\n        self.assertIn('a2', [ann.name for ann in result])\n", "    def test_split_exclude(self):\n        # Ensure that split_exclude() works when the exclude condition does not\n        # include a M2M relation.\n        q = Article.objects.filter(author__id=1)\n        q = q.exclude(category__id=1)\n        self.assertEqual(q.query._lookup_joins, [])\n", "    def test_get_children_from_q(self):\n        q = WhereNode()\n        q.children = [Node(), Node()]\n        self.assertEqual(list(get_children_from_q(q)), [Node(), Node()])\n", "    def test_alias_refcount(self):\n        \"\"\"\n        Tests that aliases are ref-counted correctly.\n\n        See ticket #17634.\n        \"\"\"\n        q = Query(model=Author)\n        alias = q.get_initial_alias()\n        self.assertEqual(q.alias_refcount[alias], 1)\n        q.ref_alias(alias)\n        self.assertEqual(q.alias_refcount[alias], 2)\n        q.unref_alias(alias, 1)\n        self.assertEqual(q.alias_refcount[alias], 1)\n        q.unref_alias(alias, 1)\n        self.assertEqual(q.alias_refcount[alias], 0)\n"], "sample_156": ["    def test_collect_fields_from_current_class(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        self.assertEqual(TestForm.declared_fields, {'field1': TestForm._meta.get_field('field1'), 'field2': TestForm._meta.get_field('field2')})\n", "    def test_clean_with_callable_initial_value(self):\n        class MyForm(Form):\n            field = CharField(initial=lambda: 'value')\n\n        form = MyForm(data={'field': 'value'})\n        self.assertEqual(form.clean(), {'field': 'value'})\n", "    def test_full_clean(self):\n        form = BaseForm(data={'first_name': 'John', 'last_name': 'Doe'})\n        form.full_clean()\n        self.assertIn('first_name', form.cleaned_data)\n        self.assertIn('last_name', form.cleaned_data)\n        self.assertNotIn('birthday', form.cleaned_data)\n", "    def test_declarative_fields_metaclass(self):\n        class TestForm(Form):\n            first_name = CharField()\n            last_name = CharField()\n            non_existent_field = CharField()\n\n        self.assertIn('first_name', TestForm.declared_fields)\n        self.assertIn('last_name', TestForm.declared_fields)\n        self.assertNotIn('non_existent_field', TestForm.declared_fields)\n        self.assertIn('non_existent_field', TestForm.base_fields)\n\n        class TestFormShadowed(Form):\n            first_name = CharField()\n            last_name = None\n\n        self.assertIn('first_name', TestFormShadowed.declared_fields)\n        self.assertNotIn('last_name', TestFormShadowed.declared_fields)\n        self.assertIn('last_name', TestFormShadowed.base_fields)\n", "    def test_form_instance_gets_correct_renderer(self):\n        # Arrange\n        renderer = 'custom_renderer'\n        form = Person()\n        original_default_renderer = BaseForm.default_renderer\n\n        # Act\n        BaseForm.default_renderer = renderer\n        form_with_renderer = Person()\n\n        # Assert\n        self.assertEqual(form_with_renderer.renderer, renderer)\n        BaseForm.default_renderer = original_default_renderer\n"], "sample_452": ["    def test_create_model_forward(self):\n        migration = Migration(\n            \"migrations\",\n            [\n                CreateModel(\n                    name=\"MyModel\",\n                    fields=[(\"name\", models.CharField(max_length=255))],\n                    options={\"verbose_name\": \"My Model\"},\n                    bases=[models.Model],\n                ),\n            ],\n            dependencies=[],\n        )\n        with CaptureQueriesContext() as capture_queries:\n            with transaction.atomic():\n                migration.apply(connection, test_db)\n        self.assertEqual(connection.introspection.get_table_list(), [\"auth_group\", \"auth_user\", \"migrations\", \"myapp_mymodel\"])\n        self.assertEqual(capture_queries.captured_queries, ['CREATE TABLE \"migrations_myapp_mymodel\" (\"id\" serial PRIMARY KEY, \"name\" varchar(255) NOT NULL);'])\n", "    def test_alter_index_together(self):\n        UnicodeModel.objects.create(value='h')\n        self.apply_and_assert_migration(\n            [\n                migrations.CreateModel(\n                    name='unicode',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID',),\n                         ('value', models.CharField(max_length=255,)),\n                    ],\n                    options={'managed': True},\n                ),\n                migrations.AlterIndexTogether(\n                    name='unicode',\n                    index_together=set([['value']]),\n                ),\n            ],\n            expected_initial_state=[],\n            expected_applied_state=[\n                {'fields': [('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID',),\n                            ('value', models.CharField(max_length=255,)),\n                            ), ('_unicode__value', models.CharField(max_length=255,)),\n                            ],\n                 'index_together': {('value',)},\n                 'options': {'managed': True},\n                 'managers': [('objects', models.Manager())]\n                 },\n            ],\n            expected_dry_run_initial_state=[],\n            expected_dry_run_applied_state=[\n                {'fields': [('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID',),\n                            ('value', models.CharField(max_length=255,)),\n                            ), ('_unicode__value', models.CharField(max_length=255,)),\n                            ],\n                 'index_together': {('value',)},\n                 'options': {'managed': True},\n                 'managers': [('objects', models.Manager())]\n                 },\n            ],\n            expected_sql=\"ALTER TABLE unicode ADD CONSTRAINT unicode_value_index_name UNIQUE (value)\"\n        )\n", "    def test_alter_model_options_with_unicode_model(self):\n        with self.assertNoFixtures, transaction.atomic():\n            # Create the model\n            UnicodeModel.objects.create()\n            # The initial state should be empty\n            self.assertEqual(\n                ModelState.from_state(self.project_state, self.app_label, 'unicode'),\n                ModelState(\n                    app_label=self.app_label,\n                    name='unicode',\n                    fields=[],\n                    options={'managed': True},\n                    bases=[models.Model],\n                    managers=[('objects', models.Manager())]\n                )\n            )\n\n        with self.assertNoFixtures, transaction.atomic():\n            # Alter the model\n            with override_settings(USE_I18N=False):\n                migration = Migration(\n                    '0001_alter_unicode_model',\n                    ['0001_initial'],\n                    state_to_apply=ModelState(\n                        app_label=self.app_label,\n                        name='unicode',\n                        fields=[],\n                        options={'managed': False},\n                        bases=[models.Model],\n                        managers=[('objects', models.Manager())]\n                    )\n                )\n                migration.add_operation(AlterModelOptions(\n                    name='unicode',\n                    options={'managed': False}\n                ))\n                migration.apply()\n\n            # The final state should reflect the change\n            self.assertEqual(\n                ModelState.from_state(\n                    self.project_state,\n                    self.app_label,\n                    'unicode'\n                ),\n                ModelState(\n                    app_label=self.app_label,\n                    name='unicode',\n                    fields=[],\n                    options={'managed': False},\n                    bases=[models.Model],\n                    managers=[('objects', models.Manager())]\n                )\n            )\n\n        with self.assertNoFixtures, transaction.atomic():\n            # Apply the reverse migration\n            migration = Migration(\n                '0002_reverse_alter_unicode_model',\n                ['0001_alter_unicode_model'],\n                state_to_apply=ModelState(\n                    app_label=self.app_label,\n                    name='unicode',\n                    fields=[],\n                    options={'managed': True},\n                    bases=[models.Model],\n                    managers=[('objects', models.Manager())]\n                )\n            )\n            migration.add_operation(AlterModelOptions(\n                name='unicode',\n                options={'managed': True}\n            ))\n            migration.apply()\n\n            # The final state should reflect the change\n            self.assertEqual(\n                ModelState.from_state(\n                    self.project_state,\n                    self.app_label,\n                    'unicode'\n                ),\n                ModelState(\n                    app_label=self.app_label,\n                    name='unicode',\n                    fields=[],\n                    options={'managed': True},\n                    bases=[models.Model],\n", "    def setUp(self):\n        self.model = UnicodeModel\n        self.old_name = \"UnicodeModel\"\n        self.new_name = \"NewUnicodeModel\"\n        self.old_migration = Migration(\n            [\"0002_auto_20210101_0000\"],\n            [\n                operations.CreateModel(\n                    self.old_name, ([\"name\"], {\"db_table\": \"unicode_model\"},)\n                )\n            ],\n        )\n        self.old_state = ModelState(\n            \"unicode\",\n            self.old_name,\n            ([\"name\"],),\n            {\"db_table\": \"unicode_model\"},\n            (models.Model,),\n            [],\n        )\n        self.new_migration = Migration(\n            [\"0002_auto_20210101_0000\"],\n            [\n                operations.RenameModel(self.old_name, self.new_name),\n                operations.CreateModel(\n                    self.new_name, ([\"name\"], {\"db_table\": \"new_unicode_model\"},)\n                ),\n            ],\n        )\n        self.new_state = ModelState(\n            \"unicode\",\n            self.new_name,\n            ([\"name\"],),\n            {\"db_table\": \"new_unicode_model\"},\n            (models.Model,),\n            [],\n        )\n", "    def test_alter_model_options_allow_null(self):\n        with self.assertRaises(IntegrityError):\n            with transaction.atomic():\n                unicode_model = UnicodeModel.objects.create()\n                unicode_model.save()\n                unicode_model.ordering = ['age']\n                unicode_model.save()\n\n        with self.assertRaises(IntegrityError):\n            with transaction.atomic():\n                unicode_model = UnicodeModel.objects.create(ordering=['age'])\n                unicode_model.save()\n\n        with self.assertRaises(IntegrityError):\n            with transaction.atomic():\n                unicode_model = UnicodeModel.objects.create(ordering=None)\n                unicode_model.save()\n"], "sample_1120": ["def test_zero_matrix_inverse():\n    \"\"\"Test that ZeroMatrix inverse raises NonInvertibleMatrixError\"\"\"\n    # Test that ZeroMatrix inverse raises NonInvertibleMatrixError\n    assert raises(NonInvertibleMatrixError, ZeroMatrix, 3, 3).inverse()\n\n    # Test that GenericZeroMatrix inverse raises NonInvertibleMatrixError\n    assert raises(NonInvertibleMatrixError, GenericZeroMatrix).inverse()\n\n    # Test that MatrixSymbol inverse returns itself\n    A = MatrixSymbol('A', 3, 3)\n    assert A.inverse() == A\n", "def test_MatrixElement(self):\n    A = MatrixSymbol('A', 2, 2)\n    a11, a12, a21, a22 = symbols('a11 a12 a21 a22')\n    A = A.subs({A[0, 0]: a11, A[0, 1]: a12, A[1, 0]: a21, A[1, 1]: a22})\n    M = A*A\n    elem = M[1, 1]\n    self.assertEqual(elem.doit(), a11*a12*a21 + a12*a22**2)\n    elem = M[1, 1].diff(a12)\n    self.assertEqual(elem, 2*a22)\n", "def test_matrix_derivative_identity():\n    from sympy import diff\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    E = MatrixSymbol('E', 2, 2)\n    result1 = diff(A * B, A)\n    result2 = diff(A * B, B)\n    result3 = diff(A * B * E, B)\n    assert result1.is_MatAdd and result1.is_MatMul\n    assert result2.is_MatAdd and result2.is_MatMul\n    assert result3.is_MatAdd and result3.is_MatMul\n", "    def test_matrix_element_equality(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        C = MatrixSymbol('C', 2, 2)\n        E1 = MatrixElement(A, 1, 1)\n        E2 = MatrixElement(B, 1, 1)\n        E3 = MatrixElement(C, 1, 1)\n        E4 = MatrixElement(A, 0, 0)\n        E5 = MatrixElement(B, 0, 0)\n        assert E1.equals(E2) == False\n        assert E1.equals(E3) == False\n        assert E1.equals(E4) == False\n        assert E1.equals(E5) == False\n        assert E1.equals(E1) == True\n        assert E2.equals(E2) == True\n        assert E3.equals(E3) == True\n        assert E4.equals(E4) == True\n        assert E5.equals(E5) == True\n", "def test_matrix_power_power_of_non_square_matrix():\n    from sympy import MatrixSymbol, NonSquareMatrixError\n    A = MatrixSymbol('A', 3, 5)\n    assert raises(NonSquareMatrixError, lambda: A**2)\n    assert raises(NonSquareMatrixError, lambda: A**-1)\n"], "sample_34": ["def test_composite_unit_reduce():\n    unit = u.Unit(\"m\")\n    assert unit.decompose() is unit\n    assert u.dimensionless_unscaled.decompose() is u.dimensionless_unscaled\n", "def test__UnitMetaClass_parse_strict():\n    # Test parse_strict == 'raise'\n    with catch_warnings(UnitsWarning):\n        with raises(ValueError):\n            Unit('not a unit', parse_strict='raise')\n\n    # Test parse_strict == 'warn'\n    with catch_warnings(UnitsWarning) as w:\n        Unit('not a unit', parse_strict='warn')\n        assert len(w) == 1\n        assert w[0].message.args[0].startswith(\"'{0}' did not parse as unit\")\n\n    # Test parse_strict == 'silent'\n    unit = Unit('not a unit', parse_strict='silent')\n    assert isinstance(unit, UnrecognizedUnit)\n    assert unit.name == 'not a unit'\n\n    # Test invalid input\n    with raises(TypeError):\n        Unit(None)\n", "def test_find_equivalent_units_with_prefix_units():\n    with catch_warnings() as w:\n        with u.set_enabled_units([u.km]):\n            assert isinstance(u.find_equivalent_units(\n                units={'m': u.m, 'km': u.km, 'cm': u.cm, 'mm': u.mm, 'um': u.um, 'nm': u.nm, 'pm': u.pm}), u.EquivalentUnitsList)\n", "def test_hash_of_repeated_creation():\n    unit = u.m.to(u.m)\n    assert hash(unit) == hash(u.m)\n    assert unit == u.m\n", "def test_format():\n    assert_allclose(unit_format.Generic.to_string(u.m), \"meter\")\n    assert_allclose(unit_format.Generic.to_string(u.m.to(\"SI\")), \"meter\")\n    assert_allclose(unit_format.Generic.to_string(u.m.to(\"CGS\")), \"cm\")\n    assert_allclose(unit_format.Generic.to_string(u.m.to(\"Imperial\")), \"foot\")\n    assert_allclose(unit_format.Generic.to_string(u.m.to(\"Galilean\")), \"stick\")\n    assert_allclose(unit_format.Generic.to_string(u.m.to(\"astronomical\")),\n                    \"astronomical unit\")\n\n    with catch_warnings() as w:\n        unit_format.Generic.to_string(\"invalid_unit\")\n        assert len(w) == 1 and isinstance(w[0].message, units.UnitsWarning)\n\n    assert unit_format.Generic.to_string(u.m, format=\"latex\") == r\"m\"\n\n    assert \"Generic\" in repr(unit_format.Generic)\n    assert \"latex\" in repr(unit_format.Latex)\n    assert \"unicode\" in repr(unit_format.Ununicode)\n\n    # Check the __format__ method\n    assert u.m.__format__(\"latex\") == r\"m\"\n    assert u.m.__format__(\"Generic\") == \"meter\"\n\n    # Check the __str__ and __bytes__ methods\n    assert u.m.__str__() == \"meter\"\n    assert u.m.__bytes__() == b\"meter\"\n\n    # Test the to_string method with a different format\n    assert unit_format.Generic.to_string(u.m, \"unicode\") == \"meter\"\n    assert unit_format.Generic.to_string(u.m, \"latex\") == r\"m\"\n\n    # Test the to_string method with an invalid format\n    with catch_warnings() as w:\n        unit_format.Generic.to_string(u.m, \"invalid_format\")\n        assert len(w) == 1 and isinstance(w[0].message, units.UnitsWarning)\n"], "sample_368": ["    def test_detect_soft_applied_empty_project_state(self):\n        executor = MigrationExecutor(connection=self.connection)\n        migration = migrations.CreateModel(\n            name='testmodel',\n            fields=[],\n            app_label='migrations'\n        )\n        project_state = ProjectState()\n        found_applied, project_state = executor.detect_soft_applied(project_state, migration)\n        self.assertFalse(found_applied)\n", "    def test_detect_soft_applied(self):\n        # Create a fake migration\n        class FakeMigration(migrations.Migration):\n            initial = True\n            dependencies = []\n            operations = [\n                migrations.CreateModel(\n                    name='FakeModel',\n                    fields=[\n                        migrations.Field('name', primary_key=True),\n                    ],\n                ),\n            ]\n\n        # Test that a soft-applied migration is detected as applied\n        self.assertTrue(\n            MigrationExecutor(connection=self.connection).detect_soft_applied(None, FakeMigration)[0]\n        )\n\n        # Test that a non-soft-applied migration is not detected as applied\n        class NonSoftAppliedMigration(migrations.Migration):\n            initial = False\n            dependencies = []\n            operations = [\n                migrations.CreateModel(\n                    name='NonSoftAppliedModel',\n                    fields=[\n                        migrations.Field('name', primary_key=True),\n                    ],\n                ),\n            ]\n\n        self.assertFalse(\n            MigrationExecutor(connection=self.connection).detect_soft_applied(None, NonSoftAppliedMigration)[0]\n        )\n\n        # Test that a migration that is the first in its app is not detected as applied\n        class FirstInAppMigration(migrations.Migration):\n            initial = True\n            dependencies = [('migrations', '0001_initial')]\n            operations = [\n                migrations.CreateModel(\n                    name='FakeModel',\n                    fields=[\n                        migrations.Field('name', primary_key=True),\n                    ],\n                ),\n            ]\n\n        self.assertFalse(\n            MigrationExecutor(connection=self.connection).detect_soft_applied(None, FirstInAppMigration)[0]\n        )\n", "    def test_detect_soft_applied_model(self):\n        from migrations import test_migrations\n        executor = MigrationExecutor(connection=connection)\n        project_state = executor._create_project_state()\n        migration = test_migrations.initial\n        applied, state = executor.detect_soft_applied(project_state, migration)\n        self.assertFalse(applied)\n        self.assertEqual(project_state.apps.get_model(migration.app_label, migration.name)._meta.db_table, None)\n", "    def test_detection_of_implicitly_applied_migration(self):\n        with self.settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"}):\n            app = global_apps.get_app_config('migrations')\n            migration = migrations.Migration(\n                app=app,\n                name='0001_initial',\n                dependencies=[],\n                initial=True,\n                operations=[\n                    migrations.RunPython('fake_operation'),\n                ],\n            )\n            executor = MigrationExecutor(connection=connection)\n            with self.assertRaises(DatabaseError):\n                executor.detect_soft_applied(None, migration)\n\n            state = executor._create_project_state()\n            executor.detect_soft_applied(state, migration)\n            self.assertTrue(migration.initial)\n", "    def test_detect_soft_applied(self):\n        # Create a test app and model\n        models = {\n            'migrations': {\n                'model': '0001_initial.py': [\n                    ('CreateModel', models.Model,\n                     {'name': 'MyModel', 'fields': {'id': 'AutoField', 'name': 'CharField', 'verbose_name': 'name'}},\n                     {'app_label': 'migrations', 'module': 'migrations'}),\n                ],\n            },\n            'migrations2': {\n                'model': '0001_initial.py': [\n                    ('CreateModel', models.Model,\n                     {'name': 'MyModel2', 'fields': {'id': 'AutoField', 'name': 'CharField', 'verbose_name': 'name'}},\n                     {'app_label': 'migrations2', 'module': 'migrations2'}),\n                ],\n            },\n        }\n\n        # Create the initial migration for 'migrations'\n        with open('migrations/0001_initial.py', 'w') as f:\n            f.write('from django.db import migrations, models\\n\\noperations = [\\n\\tmigrations.CreateModel(\\n\\t\\tname=\\'MyModel\\',\\n\\t\\tfields={\\'id\\': models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\\'id\\', db_column=\\'id\\')}\\n\\t)\\n]\\n')\n\n        # Create the initial migration for 'migrations2'\n        with open('migrations2/0001_initial.py', 'w') as f:\n            f.write('from django.db import migrations, models\\n\\noperations = [\\n\\tmigrations.CreateModel(\\n\\t\\tname=\\'MyModel2\\',\\n\\t\\tfields={\\'id\\': models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\\'id\\', db_column=\\'id\\')}\\n\\t)\\n]')\n\n        # Create the MigrationLoader and MigrationRecorder\n        from django.db.migrations import MigrationLoader\n        from .test_migrations import MigrationRecorder\n        migration_loader = MigrationLoader(connection)\n        migration_recorder = MigrationRecorder(connection)\n\n        # Run the migrations\n        executor = MigrationExecutor(connection)\n        executor.loader.project_state(('migrations', '0001_initial'), at_end=True)\n        executor.loader.project_state(('migrations2', '0001_initial'), at_end=True)\n\n        # Test that the migration is detected as applied when running against an"], "sample_994": ["def test_same_and_same_prec():\n    assert same_and_same_prec(Float(1.0), Float(1.0))\n    assert same_and_same_prec(Float(1.0), Float('1'))\n    assert same_and_same_prec(Float(1), Float(1))\n    assert same_and_same_prec(Float(1.0), Float(1.0, 10))\n", "def test_igcd():\n    assert igcd(36, 48) == 12\n    assert igcd(48, 36) == 12\n    assert igcd(48, 36, 24) == 12\n    assert igcd(36, 48, 24) == 12\n    assert igcd(24, 36, 48) == 12\n    assert igcd(48, 24, 36) == 12\n    assert igcd(48, 36, 24, 18) == 12\n    assert igcd(36, 48, 24, 18) == 12\n\n    assert igcd(-36, 48) == 12\n    assert igcd(48, -36) == 12\n    assert igcd(-48, 36) == 12\n    assert igcd(36, -48) == 12\n\n    assert igcd(1, 2) == 1\n    assert igcd(2, 1) == 1\n    assert igcd(1, 2, 3) == 1\n    assert igcd(2, 1, 3) == 1\n    assert igcd(2, 3, 1) == 1\n\n    assert igcd(2**31, 2**31 - 1) == 1\n    assert igcd(2**31 - 1, 2**31) == 1\n    assert igcd(2**31 - 1, 2**31 - 2) == 1\n    assert igcd(2**31 - 2, 2**31 - 1) == 1\n\n    with XFAIL(\"algorithm may not always produce the smallest integer\"):\n        assert igcd(2**32, 2**32) == 2**32\n        assert igcd(2**32 - 1, 2**32) == 2**32 - 1\n        assert igcd(2**32 - 2, 2**32 - 1) == 2**32 - 2\n\n    assert igcd(0, 48) == 48\n    assert igcd(48, 0) == 48\n    assert igcd(0, 48, 24) == 24\n    assert", "def test_floats():\n    assert same_and_same_prec(Rational(0.3, 1), Rational(4915, 16082))\n    assert same_and_same_prec(Rational(0.2, 1), Rational(3602879701896397, 18014398509481984))\n    assert same_and_same_prec(Rational(1.23, 1), Rational(123, 100))\n    assert same_and_same_prec(Rational('1e-2', 1), Rational(1, 100))\n    assert same_and_same_prec(Rational(3, 1), Rational(3, 1))\n    assert same_and_same_prec(Rational(3, 2), Rational(3, 2))\n    assert same_and_same_prec(Rational('60e2', 1), Rational(6000, 1))\n    assert same_and_same_prec(Rational('600e-2', 1), Rational(6, 1))\n\n    assert same_and_same_prec(Integer(3.5), Float(3.5))\n    assert same_and_same_prec(Integer(1), Float(1))\n    assert same_and_same_prec(Integer(10**20), Float(10**20))\n    assert same_and_same_prec(Rational(1.23456789123456789, 1), Float(1.23456789123456789))\n    assert same_and_same_prec(Integer(3.3), Float(3.3))\n\n    assert same_and_same_prec(Float(3.3, 10), Float(3.3))\n    assert same_and_same_prec(Float(1, 10), Float(1))\n\n    assert same_and_same_prec(Rational('1e-3', 3), Float('0.00100'))\n    assert same_and_same_prec(Rational(100, 4), Float('100.0'))\n    assert same_and_same_prec(Rational('123 456 789 . 123 456', ''), Float('123456789.123456'))\n    assert same_and_same_prec(Rational('12e-3', ''), Float('0.012'))\n\n    assert same_and_same_prec(oo + oo, nan)\n    assert same_and_same_prec(oo - oo, nan)\n    assert same_and_same_prec(oo * oo, oo)\n    assert same_and_same_prec(oo / oo, nan)\n    assert same_and_same_prec(oo / 2, oo)\n    assert same_and", "def test_mod():\n    x = Symbol('x')\n    assert srepr(Mod(x, 7)) == \"Mod(x, 7)\"\n    assert srepr(Mod(5, 7)) == \"Mod(5, 7)\"\n", "def test_foo():\n    from sympy.core.numbers import RationalConstant\n    from sympy.abc import foo\n    assert RationalConstant._new(1)._eval_power(2) == 1\n    assert RationalConstant._new(1)._eval_power(S.NegativeOne) == 1\n    assert RationalConstant._new(1)._eval_power(S.Half) == Rational(2, 1)\n    assert RationalConstant._new(1)._eval_power(S.Half)._eval_power(2) == Rational(2, 1)\n    assert RationalConstant._new(1)._eval_power(oo) == oo\n    assert RationalConstant._new(1)._eval_power(-oo) == S.Zero\n    assert RationalConstant._new(1)._eval_power(nan) == nan\n    assert RationalConstant._new(1)._eval_power(zoo) == zoo\n    assert RationalConstant._new(1)._eval_power(2)._eval_power(3) == 8\n    assert RationalConstant._new(1)._eval_power(2)._eval_power(oo) == oo\n    assert RationalConstant._new(1)._eval_power(-oo)._eval_power(2) == S.Zero\n    assert RationalConstant._new(1)._eval_power(nan)._eval_power(2) == nan\n    assert RationalConstant._new(1)._eval_power(zoo)._eval_power(2) == zoo\n    assert RationalConstant._new(1)._eval_power(2)._eval_power(3)._eval_power(4) == 16\n    assert RationalConstant._new(1)._eval_power(2)._eval_power(oo)._eval_power(3) == oo\n    assert RationalConstant._new(1)._eval_power(-oo)._eval_power(2)._eval_power(3) == S.Zero\n    assert RationalConstant._new(1)._eval_power(nan)._eval_power(2)._eval_power(3) == nan\n    assert RationalConstant._new(1)._eval_power(zoo)._eval_power(2)._eval_power(3) == zoo\n\n    assert RationalConstant._new(2)._eval_power(2)._eval_power(3) == Rational(8, 1)\n    assert RationalConstant._new(2)._eval_power(2)._eval_power"], "sample_339": ["    def test_baseformset_initial_form_count(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        formset = BaseModelFormSet(queryset=Book.objects.all())\n        self.assertEqual(formset.initial_form_count(), 1)\n", "    def test_model_form_fields(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=255)\n            author = models.CharField(max_length=255)\n        form = modelform_factory(Book, fields=['title', 'author'])\n        self.assertEqual(form.base_fields.keys(), ['title', 'author'])\n        self.assertEqual(form.base_fields['title'].widget.__class__.__name__, 'CharField')\n", "    def test_fields_for_model_non_editable_fields(self):\n        class NonEditableBook(models.Model):\n            name = models.CharField(max_length=100, editable=False)\n\n        form = fields_for_model(NonEditableBook)\n        self.assertEqual(form, {'name': None})\n", "    def test_model_form_class_attributes(self):\n        class BookForm(ModelForm):\n            class Meta:\n                model = Book\n                fields = ('title', 'author')\n\n        self.assertIn('title', BookForm.base_fields)\n        self.assertIn('author', BookForm.base_fields)\n        self.assertNotIn('id', BookForm.base_fields)\n", "    def test_preparation_of_value(self):\n        author = Author.objects.create(name='John Doe')\n        field = ModelChoiceField(Author.objects.all())\n        self.assertEqual(field.prepare_value(author), author.pk)\n        self.assertEqual(field.prepare_value(author.pk), author.pk)\n        self.assertEqual(field.prepare_value(author.name), author.pk)\n"], "sample_598": ["def test_format_timestamp():\n    timestamp = pd.Timestamp(\"2022-01-01\")\n    formatted = formatting.format_timestamp(timestamp)\n    assert formatted == \"2022-01-01\"\n\n    # Test OutOfBoundsDatetime\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(pd.Timestamp(\"1900-01-01\"))\n\n    # Test NaT\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test time with seconds\n    timestamp = pd.Timestamp(\"2022-01-01 12:30:30\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:30:30\"\n\n    # Test timezone\n    timestamp = pd.Timestamp(\"2022-01-01 12:30:30\", tz=\"US/Eastern\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:30:30-05:00\"\n\n    # Test Python datetime object\n    from datetime import datetime\n    timestamp = datetime(2022, 1, 1)\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01 00:00:00\"\n", "def test_diff_dataset_repr():\n    # Create two datasets with different dimensions and variables\n    ds1 = xr.Dataset({\n        \"x\": (\"x\", np.arange(10)),\n        \"y\": (\"x\", np.arange(10))\n    })\n    ds2 = xr.Dataset({\n        \"x\": (\"x\", np.arange(10)),\n        \"z\": (\"x\", np.arange(10))\n    })\n\n    # Test diff_dataset_repr with default compatibility\n    expected = (\n        \"Left and right Dataset objects are not equal\\n\"\n        \"Differing dimensions:\\n    (x) != (x)\\n\"\n        \"Differing Data variables:\\n\"\n        \"    x\\n\"\n        \"      Left: <xarray.DataArray 'x' (x: 10)\\n\"\n        \"      Right: <xarray.DataArray 'x' (x: 10)\\n\"\n        \"      Differing values:\\n\"\n        \"        L\\n\"\n        \"          [0 1 2 3 4 5 6 7 8 9]\\n\"\n        \"        R\\n\"\n        \"          [0 1 2 3 4 5 6 7 8 9]\\n\"\n        \"Data variables only on the left object:\\n\"\n        \"    y\\n\"\n        \"      <xarray.DataArray 'y' (x: 10)\\n\"\n    )\n    assert diff_dataset_repr(ds1, ds2) == expected\n\n    # Test diff_dataset_repr with 'identical' compatibility\n    expected = (\n        \"Left and right Dataset objects are not identical\\n\"\n        \"Differing dimensions:\\n    (x) != (x)\\n\"\n        \"Differing Data variables:\\n\"\n        \"    x\\n\"\n        \"      Left: <xarray.DataArray 'x' (x: 10)\\n\"\n        \"      Right: <xarray.DataArray 'x' (x: 10)\\n\"\n        \"      Differing values:\\n\"\n        \"        L\\n\"\n        \"          [0 1 2 3 4 5 6 7 8 9]\\n\"\n        \"        R\\n\"\n        \"          [0 1 2 3 4 5 6 7 8 9]\\n\"\n        \"    y\\n\"\n        \"      Left: <xarray.DataArray 'y", "def test_inline_sparse_repr():\n    # Test inline_sparse_repr with a COO object\n    import sparse\n    array = sparse.COO(np.array([1, 2, 3, 4, 5], dtype=np.int64), (5, 1))\n    expected_repr = \"<COO: nnz=5, fill_value=0>\"\n    assert formatting.inline_sparse_repr(array) == expected_repr\n\n    # Test inline_sparse_repr with a COO object and non-zero fill_value\n    array = sparse.COO(np.array([1, 2, 3, 4, 5], dtype=np.int64), (5, 1), fill_value=10)\n    expected_repr = \"<COO: nnz=5, fill_value=10>\"\n    assert formatting.inline_sparse_repr(array) == expected_repr\n\n    # Test inline_sparse_repr with a Dask Array object\n    import dask.array as da\n    array = da.from_array(np.array([1, 2, 3, 4, 5], dtype=np.int64))\n    expected_repr = \"dask.array<chunksize=(5,)>\"\n    assert formatting.inline_sparse_repr(array) == expected_repr\n\n    # Test inline_sparse_repr with a non-sparse object\n    array = np.array([1, 2, 3, 4, 5], dtype=np.int64)\n    with pytest.raises(AssertionError):\n        formatting.inline_sparse_repr(array)\n", "def test_diff_dataset_repr():\n    a = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    b = xr.Dataset({\"var1\": (\"x\", np.arange(10)), \"var2\": (\"y\", np.arange(10))})\n    assert diff_dataset_repr(a, b, \"identical\") == dedent(\n        \"\"\"\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x) != (x, y)\n        Differing Coordinates:\n        Coordinates:\n        Dimensions without coordinates: y\n        Differing data variables:\n        Data variables:\n        var1(float64): () \n        ...\n        ...\n        ... \n        var1(float64): () \n        ...\n        ...\n        ... \n        Differing attributes:\n    \"\"\"\n    )\n\n    a = xr.Dataset({\"var1\": (\"x\", np.arange(10)), \"var2\": (\"y\", np.arange(10))})\n    b = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    assert diff_dataset_repr(a, b, \"identical\") == dedent(\n        \"\"\"\n        Left and right Dataset objects are not identical\n        Differing dimensions:\n            (x, y) != (x)\n        Differing Coordinates:\n        Coordinates:\n        Dimensions without coordinates: y\n        Differing data variables:\n        Data variables:\n        var1(float64): () \n        ...\n        ...\n        ... \n        var1(float64): () \n        ...\n        ...\n        ... \n    \"\"\"\n    )\n\n    a = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    b = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    assert diff_dataset_repr(a, b, \"equal\") == \"\"\n\n    a = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    b = xr.Dataset({\"var1\": (\"x\", np.arange(11))})\n    assert diff_dataset_repr(a, b, \"equal\") == dedent(\n        \"\"\"\n        Left and right Dataset objects are not equal\n        Differing dimensions:\n            (x) != (x)\n        \"\"\"\n    )\n\n    a = xr.Dataset({\"var1\": (\"x\", np.arange(10))})\n    b = xr.Dataset({\"var2\": (\"y\", np.arange(10))})\n    assert diff_dataset_repr(a, b, \"allclose\") == dedent(\n        \"\"\"\n        Left", "def test_format_timestamp_out_of_bounds_datetime():\n    # Test OutOfBoundsDatetime exception\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(pd.Timestamp(\"9999-01-01 00:00:00\"))\n"], "sample_396": ["    def test_add_annotation(self):\n        # Test adding annotations to a query\n        q = Query(model=ModelA, alias_cols=True)\n        q.add_annotation(Count(\"id\"), alias=\"count\")\n        self.assertEqual(q.annotation_select[\"count\"].expression, Count(\"id\"))\n", "    def test_join_info(self):\n        model = ModelA\n        alias = \"T1\"\n        opts = model._meta\n        join_info = JoinInfo(\n            None,\n            (\"id\",),\n            opts,\n            (\"T1\",),\n            (),\n            (),\n            lambda field, alias: field.get_col(alias),\n        )\n        self.assertEqual(join_info.final_field, model._meta.pk)\n        self.assertEqual(join_info.targets, (model._meta.pk,))\n        self.assertEqual(join_info.opts, opts)\n        self.assertEqual(join_info.joins, (\"T1\",))\n        self.assertEqual(join_info.path, ())\n        self.assertEqual(\n            join_info.transform_function(model._meta.pk, \"T1\"), model._meta.pk.get_col(\"T1\")\n        )\n", "    def test_get_group_by_cols(self):\n        query = Query(ModelD)\n        query.select = (ModelD.field1,)\n        query.add_ordering(ModelD.field2)\n        query.add_select_related(\"field3\")\n        group_by_cols = query.get_group_by_cols()\n        self.assertEqual(len(group_by_cols), 3)\n        self.assertIn(ModelD.field1, group_by_cols)\n        self.assertIn(ModelD.field2, group_by_cols)\n        self.assertIn(ModelD.field3, group_by_cols)\n", "    def test_add_annotation_with_alias(self):\n        query = Article.objects.all()\n        query.add_annotation(Max(\"id\"), alias=\"max_id\")\n        self.assertEqual(query.annotation_select, {\"max_id\": Max(\"id\")})\n        self.assertEqual(query.annotation_select_mask, {\"max_id\"})\n        self.assertEqual(query.annotation_select_mask.cache_clear(), None)\n        query.annotation_select = None\n        query.annotation_select_mask = None\n        self.assertEqual(query.annotation_select_mask.cache_clear(), None)\n", "    def test_select_related_empty_fields(self):\n        with self.assertQuerysetEqual(\n            Article.objects.select_related().all(),\n            [\"<Article: Article object (1)>\"],\n            \"Article.objects.all()\"\n        )\n        with self.assertQuerysetEqual(\n            Article.objects.select_related().values(\"pk\", \"id\"),\n            [\"(1, 1)\"],\n            \"Article.objects.all().values('pk', 'id')\"\n        )\n        with self.assertQuerysetEqual(\n            Article.objects.select_related().values_list(\"pk\", \"id\"),\n            [[1, 1]],\n            \"Article.objects.all().values_list('pk', 'id')\"\n        )\n"], "sample_998": ["def test_latex_quaternion():\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == r'\\left( 1 + 2 \\mathrm{i} + 3 \\mathrm{j} + 4 \\mathrm{k} \\right)'\n", "def test_latex_mod():\n    assert latex(Mod(sin(1), 2)) == r'\\left(\\sin\\left(1\\right)\\right) \\bmod 2'\n", "    def test_LatexPrinter_printing_SingularityFunction(self):\n        assert latex(SingularityFunction(x, x, x)) == r'{\\langle %s \\rangle}^{%s}' % (latex(x - x), latex(1))\n", "def test_latex_poly():\n    # Test latex representation of Poly object\n    x = Symbol('x')\n    p = Poly(x**2 + 2*x + 1, x)\n    expected_latex = r'\\operatorname{Poly}\\left({x \\to {x^{2} + 2x + 1}, x\\right)'\n    assert latex(p) == expected_latex\n\n    # Test latex representation of PolyElement object\n    p_elem = p.list()[0]\n    expected_latex = r'1x^2'\n    assert latex(p_elem) == expected_latex\n\n    # Test latex representation of FracElement object\n    f_elem = p_elem.as_expr()\n    expected_latex = r'x^2'\n    assert latex(f_elem) == expected_latex\n", "    def test_latex_translating(self):\n        # Test that symbols are translated\n        assert latex(translate('x')) == r\"x\"\n        assert latex(translate('X')) == r\"\\Xi\"\n        assert latex(translate('X')) == r\"X\"\n        assert latex(translate('alpha')) == r\"A\"\n        assert latex(translate('xi')) == r\"\\Xi\"\n        assert latex(translate('Xi')) == r\"\\Xi\"\n        assert latex(translate('alpha')) == r\"A\"\n        assert latex(translate('alpha')) == r\"A\"\n        # Test that accented symbols are translated\n        assert latex(translate('hat{x}')) == r\"\\hat{x}\"\n        assert latex(translate('xhat')) == r\"\\hat{x}\"\n        # Test that accents work with the translation\n        assert latex(translate('hat{alpha}')) == r\"\\hat{A}\"\n        # Test that other symbols are not translated\n        assert latex(translate('Y')) == \"Y\"\n        # Test that symbols with underscores are not translated\n        assert latex(translate('x_1')) == \"x_1\"\n        # Test that symbols with dots are not translated\n        assert latex(translate('x.1')) == \"x.1\"\n        # Test that symbols with tildes are not translated\n        assert latex(translate('x~1')) == \"x~1\"\n        # Test that symbols with hats are not translated\n        assert latex(translate('x^1')) == \"x^1\"\n        # Test that symbols with accents are not translated\n        assert latex(translate('x{1}')) == \"x{1}\"\n        # Test that symbols with primes are not translated\n        assert latex(translate('x\\'1')) == \"x\\'1\"\n        # Test that symbols with primes are not translated\n        assert latex(translate('x\\'1')) == \"x\\'1\"\n        # Test that symbols with primes are not translated\n        assert latex(translate('x\\prime1')) == \"x\\prime1\"\n        # Test that numbers are not translated\n        assert latex(translate('1')) =="], "sample_1195": ["def test_simplify_gpgp():\n    from sympy.tensor.tensor import tensor_heads\n    LorentzIndex.metric = eye(4)\n    p = TensorHead('p', [LorentzIndex])\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    ps = p(i0)*G(-i0)*p(i1)*G(-i1)\n    qs = p(i0)*G(-i0)*p(i2)*G(-i2)\n    qs2 = p(i2)*G(-i2)*p(i0)*G(-i0)\n    assert _is_tensor_eq(simplify_gpgp(ps), qs)\n    assert _is_tensor_eq(simplify_gpgp(qs2), ps)\n", "def test_kahane_simplify_multiple_consecutive_contracted_gamma_matrices():\n    \"\"\"\n    Test kahane_simplify on multiple consecutive contracted gamma matrices.\n\n    \"\"\"\n    from sympy.physics.hep.gamma_matrices import LorentzIndex, kahane_simplify\n    from sympy.tensor.tensor import tensor_indices\n\n    i = tensor_indices('i', LorentzIndex)\n    t = G(i)*G(-i)*G(i)*G(-i)\n    t2 = kahane_simplify(t)\n    assert _is_tensor_eq(t2, 4*eye(4))\n\n", "def test_kahane_simplify_contracted_indices():\n    from sympy.tensor.tensor import tensor_indices\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n    # Test simple contraction (gamma^mu gamma_mu)\n    gamma = G(i0)*G(-i0)\n    res = kahane_simplify(gamma)\n    assert res == Matrix([[4, 0, 0, 0], [0, 4, 0, 0], [0, 0, 4, 0], [0, 0, 0, 4]])\n    # Test contraction with multiple indices\n    gamma = G(i0)*G(-i1)*G(i1)*G(-i0)\n    res = kahane_simplify(gamma)\n    assert res == Matrix([[-4, 0, 0, 0], [0, -4, 0, 0], [0, 0, -4, 0], [0, 0, 0, -4]])\n    # Test no contraction\n    gamma = G(i0)*G(-i1)*G(i1)*G(i2)*G(-i0)\n    res = kahane_simplify(gamma)\n    assert res == gamma\n    # Test more complex expression\n    gamma = G(i0)*G(-i1)*G(i2)*G(-i2)*G(i1)*G(-i0)\n    res = kahane_simplify(gamma)\n    assert res == Matrix([[8, 0, 0, 0], [0, -8, 0, 0], [0, 0, -8, 0], [0, 0, 0, 8]])\n", "def test_gamma_trace_contracted():\n    from sympy.tensor.tensor import tensor_indices, tensor_heads\n    from sympy.physics.hep.gamma_matrices import GammaMatrix as G, LorentzIndex\n\n    LorentzIndex.metric = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, -1]])\n\n    i, j = tensor_heads('i, j', [LorentzIndex])\n    t = G(i)*G(j)*G(-i)*G(-j)\n\n    expected_result = -2*G(j)*G(j)\n    assert _is_tensor_eq(gamma_trace(t), expected_result)\n", "def test_gamma_trace():\n    \"\"\"\n    Test gamma_trace function\n\n    \"\"\"\n    from sympy.tensor.tensor import tensor_indices, TensorHead\n    from sympy.physics.hep.gamma_matrices import LorentzIndex\n\n    # Check that the metric is a Lorentz Index type\n    assert isinstance(LorentzIndex.metric, LorentzIndex)\n    # Check that the metric is the only index in the gamma_trace\n    assert len(LorentzIndex.metric.indices) == 2\n\n    i0, i1, i2, i3, i4, i5 = tensor_indices('i0:6', LorentzIndex)\n\n    # Test with two gamma matrices\n    p = TensorHead('p', [LorentzIndex])\n    ps = p(i0)*G(-i0)\n    qs = p(i1)*G(-i1)\n    assert _is_tensor_eq(gamma_trace(G(i0)*G(i1)), 4*LorentzIndex.metric(i0, i1))\n\n    # Test with four gamma matrices\n    assert _is_tensor_eq(gamma_trace(G(i0)*G(i1)*G(i2)*G(i3)), 4*LorentzIndex.metric(i0, i1)*LorentzIndex.metric(i2, i3))\n\n    # Test with no gamma matrices\n    assert _is_tensor_eq(gamma_trace(p(i0)*G(-i0)*p(i1)*G(-i1)), 4*p(i0)*p(-i0))\n\n    # Test with one gamma matrix\n    assert _is_tensor_eq(gamma_trace(G(i0)*p(i1)*G(-i1)*p(-i0)), 4*p(i1)*p(-i1))\n\n    # Test with no metric index\n    assert _is_tensor_eq(gamma_trace(p(i0)*p(-i0)), 0)\n\n    # Test with a TensAdd expression\n    assert _is_tensor_eq(gamma_trace(ps*qs + ps*ps), 4*(p(i0)*p(-i0) + p(i1)*p(-i1)))\n\n    # Test with a large number of gamma matrices\n    i6 = tensor_indices('i6', LorentzIndex)\n    gamma = G(i0)*G(i1)*G(i2)*G(i3)*G(i4)*G(i5)*G(i6)\n    assert _is_tensor_eq(gamma_trace(gamma), 4*Lorentz"], "sample_49": ["    def test_format_value(self):\n        widget = SelectDateWidget()\n        value = datetime.date(2022, 2, 28)\n        self.assertEqual(widget.format_value(value), '2022-02-28')\n        \n        value = datetime.date(2022, 2, 0)\n        self.assertEqual(widget.format_value(value), '2022-02-00')\n        \n        value = datetime.date(2022, 13, 28)\n        self.assertEqual(widget.format_value(value), '2022-13-28')\n        \n        value = None\n        self.assertEqual(widget.format_value(value), None)\n", "    def test_media_property(self):\n        class TestWidget(Widget):\n            media = Media(css={'css_file': 'css_file.css'})\n\n        widget = TestWidget()\n        self.assertEqual(widget.media.css, {'css_file': 'css_file.css'})\n", "    def test_date_time_widget_media(self):\n        widget = DateTimeInput(format='YYYY-MM-DD HH:MM')\n        media = widget.media\n        self.assertEqual(media.css, {'all': ['django.forms.widgets.datetime.css']})\n", "    def test_media(self):\n        widget = MultiWidget(widgets=[TextInput(), TextInput()])\n        self.assertIn('http://media.example.com/static/css', str(widget.media))\n        self.assertIn('http://media.example.com/static/js', str(widget.media))\n\n        widget = MultiWidget(widgets=[TextInput(), TextInput(), TextInput()])\n        self.assertIn('http://media.example.com/static/css', str(widget.media))\n        self.assertIn('http://media.example.com/static/js', str(widget.media))\n\n        widget = MultiWidget(widgets=[TextInput(), TextInput(), TextInput()])\n        self.assertEqual(len(widget.media._css), 2)\n        self.assertEqual(len(widget.media._js), 1)\n", "    def test_widgets_media(self):\n        # Test Media property of widgets\n        text_input_media = Media(css={'css': {'cssfile.css': 'body { background-color: #f2f2f2; }'}},\n                              js=['jsfile.js'])\n        class MyWidget(TextInput):\n            media = text_input_media\n\n        self.assertEqual(MyWidget().media, text_input_media)\n\n        class MyWidget2(TextInput):\n            media = {'css': {'cssfile.css': 'body { background-color: #f2f2f2; }'}}\n\n        self.assertEqual(MyWidget2().media.css, {'cssfile.css': 'body { background-color: #f2f2f2; }'})\n\n        # Test Media class\n        media = Media()\n        self.assertEqual(media._css, {})\n        self.assertEqual(media._js, [])\n        self.assertEqual(media.render(), '')\n\n        media = Media(css={'css': {'cssfile.css': 'body { background-color: #f2f2f2; }'}},\n                     js=['jsfile.js'])\n        self.assertEqual(media._css, {'cssfile.css': 'body { background-color: #f2f2f2; }'})\n        self.assertEqual(media._js, ['jsfile.js'])\n        self.assertEqual(media.render(), '<link href=\"http://media.example.com/static/cssfile.css\" type=\"text/css\" media=\"all\" rel=\"stylesheet\"><script type=\"text/javascript\" src=\"http://media.example.com/static/jsfile.js\"></script>')\n\n        # Test Media property of MediaDefiningClass\n        class MyForm(Form):\n            field = CharField()\n\n        self.assertIsInstance(MyForm().media, Media)\n        self.assertEqual(MyForm().media._css, {})\n        self.assertEqual(MyForm().media._js, [])\n\n        # Test MultiWidget\n        widget1 = TextInput()\n        widget2 = TextInput()\n        multiwidget = MultiWidget(widgets=[widget1, widget2])\n        self.assertIsInstance(multiwidget.media, Media)\n        self.assertEqual(multiwidget.media._css, {})\n        self.assertEqual(multiwidget.media._js, [])\n\n        # Test form media\n        class MyForm(MediaDefiningClass):\n            media = Media(css={'css': {'cssfile.css': 'body { background-color: #f2f2f2; }'}},\n                         js=['jsfile.js'])\n            field = Char"], "sample_987": ["def test_evalf_abs():\n    from sympy.core.evalf import evalf\n    from sympy import Abs, S\n    assert evalf(Abs(S(1 + 2*I), evaluate=False), 50, {}) == (1, None, 50, None)\n", "def test_evalf_add():\n    from sympy import Float, Rational\n    from sympy.core.evalf import evalf\n    from sympy.abc import x\n\n    assert evalf(Add(1, 1), 10) == (2, None, 10, None)\n    assert evalf(Add(x, 1), 10) == (x._mpf_, None, 10, None)\n\n    # Issue 6088\n    assert evalf(Add(1, Rational(1, 2)), 10) == (3/2, None, 10, None)\n\n    # Test that Floats are converted correctly\n    assert evalf(Add(Float(0.1), Float(0.2)), 10) == (0.3, None, 10, None)\n\n    # Test that Floats and Rationals are converted correctly\n    assert evalf(Add(Float(0.5), Rational(1, 2)), 10) == (1, None, 10, None)\n", "def test_as_mpmath():\n    assert as_mpmath(0, 15, {}) == mpf(0)\n    assert as_mpmath(1, 15, {}) == mpf(1)\n    assert as_mpmath(-1, 15, {}) == mpf('-1')\n    assert as_mpmath(n, 15, {}) == mpf('inf')\n    assert as_mpmath(-n, 15, {}) == mpf('-inf')\n    assert as_mpmath(1+I, 15, {}) == mpc(1, 1)\n    assert as_mpmath(S.Zero, 15, {}) == mpf(0)\n    assert as_mpmath(S.NaN, 15, {}) == mpf('nan')\n    assert as_mpmath(S.Exp1, 15, {}) == mpf('e')\n    assert as_mpmath(S.Half, 15, {}) == mpf('0.5')\n    assert as_mpmath(S.Pi, 15, {}) == mpf('3.141592653589793116')\n    assert as_mpmath(S.ImaginaryUnit, 15, {}) == mpc(0, 1)\n    assert as_mpmath(S.NegativeOne, 15, {}) == mpf('-1')\n    assert as_mpmath(S.One, 15, {}) == mpf(1)\n", "def test_evalf_add_to_infinity():\n    from sympy import oo, S, Add\n    x = Symbol('x')\n    assert NS(Add(1, oo)) == \"inf\"\n    assert NS(oo + 1) == \"inf\"\n    assert NS(Add(1, -oo)) == \"-inf\"\n    assert NS(-oo + 1) == \"-inf\"\n", "def test_evalf_pow():\n    from sympy.core.numbers import Pi, E, exp\n    from sympy.core.evalf import exp as sympy_exp\n    assert NS(exp(x**3)) == '3.7875020353070717'\n    assert NS(exp(exp(x))) == '4.946970900228864'\n    assert NS(exp(x**x)) == '5.929123964418179'\n    assert NS(x**x) == '1.4446678610917848'\n    assert NS((exp(x))**(x/2)) == '0.9050568424913564'\n    assert NS(Pow(exp(x), x/2)) == '0.9050568424913564'\n    assert NS((x**3)**x) == '5.565877455849434'\n    assert NS((exp(x))**exp(x)) == '2.586002697819441'\n    assert NS((exp(x/2))**x) == '1.904827126654842'\n    assert NS(sympy_exp(x)**x) == '1.4446678610917848'\n    assert NS(x**S.Half) == '1.000000000000000'\n    assert NS(x**S.NegativeOne) == '0.707106781186548'\n    assert NS((x + I)**(x + I)) == '1.216688509800644+1.316926592740839j'\n    assert NS((x + I)**(x + I), chop=True) == '1.217+1.317j'\n    assert raises(PrecisionExhausted, NS, (x + I)**(x + I), prec=1, chop=True)\n"], "sample_542": ["def test_get_fontfamily(family, expected):\n    # Create a text with the specified font family\n    text = Text('Hello World', fontfamily=family)\n    assert text.get_fontfamily() == expected\n", "    def test_rotation(self, rotation):\n        t = Text(0.5, 0.5, \"Hello\", rotation=rotation)\n        assert t.get_rotation() == rotation\n", "def test_font_stretching():\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n\n    fontprop = FontProperties(stretch='ultra-condensed')\n    text = Text(0.5, 0.5, 'normal', fontproperties=fontprop, transform=ax.transAxes)\n    ax.add_artist(text)\n\n    fig.canvas.draw()\n\n    # Use assert_equal to compare the actual and expected images\n    assert_equal(text.get_fontstretch(), 'ultra-condensed')\n\n    fontprop = FontProperties(stretch='normal')\n    text.set_fontproperties(fontprop)\n    text.update_bbox_position_size(None)\n    text.draw(None)\n\n    # Use assert_almost_equal to compare the actual and expected font stretches\n    assert_almost_equal(text.get_fontstretch(), 'normal')\n\n    # Test font stretching with 'condensed' and 'expanded' values\n    fontprop = FontProperties(stretch='condensed')\n    text.set_fontproperties(fontprop)\n    text.update_bbox_position_size(None)\n    text.draw(None)\n    assert_almost_equal(text.get_fontstretch(), 'condensed')\n\n    fontprop = FontProperties(stretch='expanded')\n    text.set_fontproperties(fontprop)\n    text.update_bbox_position_size(None)\n    text.draw(None)\n    assert_almost_equal(text.get_fontstretch(), 'expanded')\n", "def test_text_set_wrap():\n    # Create a figure\n    fig, ax = plt.subplots()\n\n    # Create a text object with wrapping enabled\n    text = Text(0.5, 0.5, \"Hello world\", wrap=True)\n    ax.add_artist(text)\n\n    # Set the figure size to a small size to make the text wrap\n    fig.set_size_inches(2, 2)\n\n    # Check that the text is wrapped correctly\n    assert text.get_text() == \"Hello\\nworld\"\n\n    # Set the figure size to a large size to disable wrapping\n    fig.set_size_inches(10, 10)\n\n    # Check that the text is not wrapped\n    assert text.get_text() == \"Hello world\"\n", "def test_update_positions():\n    # Set up a figure and axis\n    fig, ax = plt.subplots()\n\n    # Create a text object\n    text = Text('Hello, world!', 0.5, 0.5, ha='center')\n\n    # Set the transform of the axis to a custom transform\n    trans = mtransforms.Affine2D().translate(10, 20)\n    ax.set_transform(trans)\n\n    # Update the position of the text\n    text.set_transform(trans)\n\n    # Check that the position of the text is correct\n    assert_almost_equal(text.get_position(), (10, 20))\n\n    # Update the position of the text to a new value\n    new_pos = (30, 40)\n    text.set_position(new_pos)\n\n    # Check that the position of the text is correct\n    assert_almost_equal(text.get_position(), new_pos)\n\n    # Check that the position of the text is correct when the axis transform is changed\n    new_trans = mtransforms.Affine2D().scale(2, 2)\n    ax.set_transform(new_trans)\n    assert_almost_equal(text.get_position(), new_pos)\n\n    # Test that the update_positions method is called correctly\n        # Check that the renderer is not None\n        assert renderer is not None\n        # Check that the update_positions method is called with the correct renderer\n        assert isinstance(renderer, mpl.backend_bases.RendererBase)\n\n    # Create a mock renderer\n    mock_renderer = mock_update_positions\n    text.update_positions(mock_renderer)\n\n    # Check that the text is updated correctly\n    assert text.get_window_extent() == Bbox.from_bounds(30, 40, 0, 0)\n"], "sample_334": ["    def test_as_table(self):\n        form = Person()\n        expected_output = \"\"\"", "    def test_bound_field_declarative(self):\n        form = Person()\n        self.assertIsInstance(form['first_name'], forms.BoundField)\n        self.assertIsInstance(form['last_name'], forms.BoundField)\n        self.assertIsInstance(form['birthday'], forms.BoundField)\n", "    def test_html_output(self):\n        # Test rendering as HTML table\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'})\n        html = form.as_table()\n        self.assertIsInstance(html, str)\n        self.assertIn('John', html)\n        self.assertIn('Doe', html)\n        self.assertIn('2000-01-01', html)\n\n        # Test rendering as HTML unordered list\n        form = Person(data={'first_name': 'Jane', 'last_name': 'Doe', 'birthday': '2001-01-01'})\n        html = form.as_ul()\n        self.assertIsInstance(html, str)\n        self.assertIn('Jane', html)\n        self.assertIn('Doe', html)\n        self.assertIn('2001-01-01', html)\n\n        # Test rendering as HTML paragraph\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'})\n        html = form.as_p()\n        self.assertIsInstance(html, str)\n        self.assertIn('John', html)\n        self.assertIn('Doe', html)\n        self.assertIn('2000-01-01', html)\n", "    def test_render_form(self):\n        form = Person()\n        html = form.as_table()\n        self.assertIsInstance(html, str)\n        self.assertIn('first_name', html)\n        self.assertIn('last_name', html)\n        self.assertIn('birthday', html)\n", "    def test_as_table(self):\n        form = Person()\n        html = form.as_table()\n        self.assertIsInstance(html, str)\n        self.assertIn('<tr', html)\n        self.assertIn('<td>', html)\n        self.assertIn('</td>', html)\n"], "sample_835": ["def test_adaboost_classifier_different_base_estimators():\n    \"\"\"Test AdaBoostClassifier with different base estimators.\"\"\"\n    # Test with DecisionTreeClassifier\n    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),\n                            random_state=rng)\n    clf.fit(X, y_class)\n    assert_array_almost_equal(clf.predict(X), ['foo', 'foo', 'foo', 1, 1, 1])\n\n    # Test with SVC\n    clf = AdaBoostClassifier(base_estimator=SVC(probability=True),\n                            random_state=rng)\n    clf.fit(X, y_class)\n    assert_array_almost_equal(clf.predict(X), ['foo', 'foo', 'foo', 1, 1, 1])\n\n    # Test with SVR\n    regr = AdaBoostRegressor(base_estimator=SVR(),\n                             random_state=rng)\n    regr.fit(X, y_regr)\n    assert_array_almost_equal(regr.predict(X), [-1, -1, -1, 1, 1, 1])\n", "def test_adaboostregressor_loss(loss):\n    X, y = make_regression(n_samples=100, n_features=1, noise=0, random_state=0)\n    regr = AdaBoostRegressor(loss=loss, n_estimators=10, random_state=0)\n    regr.fit(X, y)\n    assert regr.loss == loss\n", "def test_AdaBoostRegressor_score():\n    \"\"\"Check that scoring works on a regressor.\"\"\"\n    X, y = boston.data, boston.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                        random_state=rng)\n\n    regr = AdaBoostRegressor(random_state=rng)\n    regr.fit(X_train, y_train)\n    score = regr.score(X_test, y_test)\n    assert_array_almost_equal(score, regr.predict(X_test))\n", "def test_adaboost_regressor_median_predictions():\n    \"\"\"Regression test for the median predictions of the AdaBoost regressor.\"\"\"\n    # Test that the median prediction is stable over multiple runs\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([2, 4])\n    regr = AdaBoostRegressor(random_state=42)\n    regr.fit(X, y)\n\n    # Compute median predictions\n    predictions = regr._get_median_predict(X, len(regr.estimators_))\n\n    # Check that the median prediction is stable\n    regr.random_state = 23\n    new_predictions = regr._get_median_predict(X, len(regr.estimators_))\n    assert_array_almost_equal(predictions, new_predictions)\n\n    # Test that the median prediction is correct\n    regr = AdaBoostRegressor(random_state=42)\n    regr.fit(X, y)\n    assert_array_equal(regr.predict(X), regr._get_median_predict(X, len(regr.estimators_)))\n\n    # Test that the median prediction is correct for multiple input samples\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([2, 4, 6])\n    regr = AdaBoostRegressor(random_state=42)\n    regr.fit(X, y)\n    assert_array_equal(regr.predict(X), regr._get_median_predict(X, len(regr.estimators_)))\n\n    # Test that the median prediction is correct for multiple estimators\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([2, 4])\n    regr = AdaBoostRegressor(random_state=42)\n    regr.estimators_ = [DecisionTreeRegressor() for _ in range(10)]\n    regr.estimator_weights_ = np.array([1.0] * 10)\n    regr.fit(X, y)\n    assert_array_equal(regr.predict(X), regr._get_median_predict(X, len(regr.estimators_)))\n", "def test_AdaBoostRegressor_get_median_predict():\n    # Generate a random regression dataset\n    X = rng.rand(10, 2)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + rng.randn(10)\n    regr = AdaBoostRegressor(n_estimators=10, random_state=rng)\n    regr.fit(X, y)\n\n    # Test that the median prediction is close to the actual regression values\n    y_pred = regr.predict(X)\n    assert_array_almost_equal(y_pred, np.median(regr._get_median_predict(X, len(regr.estimators_)), decimal=2))\n"], "sample_305": ["    def test_FieldLookup___init__(self):\n        field = IntegerField()\n        lookup = Lookup(field, 1)\n        self.assertEqual(lookup.lhs, field)\n        self.assertEqual(lookup.rhs, 1)\n        self.assertEqual(lookup.rhs_is_direct_value(), True)\n", "    def test_exact_lookup(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n            objects = models.Manager()\n\n        Model.objects.create(field='test')\n        self.assertTrue(Model.objects.filter(field='test').exists())\n        self.assertFalse(Model.objects.filter(field='not test').exists())\n", "    def test_lookup_hash(self):\n        lookup1 = Lookup()\n        lookup2 = Lookup()\n        lookup3 = Lookup()\n\n        lookup1.lhs = 1\n        lookup2.lhs = 1\n        lookup3.lhs = 2\n\n        self.assertEqual(hash(lookup1.identity), hash(lookup2.identity))\n        self.assertNotEqual(hash(lookup1.identity), hash(lookup3.identity))\n", "    def test_exact_lookup_on_date_field(self):\n        from django.db.models.functions import ExtractYear\n        from django.db.models import Q\n        from datetime import date\n        self.assertEqual(\n            Author.objects.filter(age__exact=34).count(),\n            1\n        )\n        self.assertEqual(\n            Author.objects.filter(age__exact=34).values_list('name', flat=True),\n            ['Adrian Holovaty']\n        )\n        self.assertEqual(\n            Author.objects.filter(\n                Q(age__exact=34) | Q(age__exact=29)\n            ).count(),\n            3\n        )\n        with self.assertRaises(FieldError):\n            Author.objects.filter(age__exact=date(2020, 1, 1))\n", "    def test_year_aggregation(self):\n        # Test that year aggregation works correctly on DateField\n        Year = fields.IntegerField()\n        self.assertEqual(\n            Book.objects.filter(pubdate__year=2007).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007]\n        )\n        self.assertEqual(\n            Book.objects.filter(pubdate__year=2007).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007]\n        )\n\n        # Test that year aggregation works correctly on DateTimeField\n        self.assertEqual(\n            Book.objects.filter(pubdate__year=2007).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007]\n        )\n\n        # Test that year aggregation works correctly with the 'gte' lookup\n        self.assertEqual(\n            Book.objects.filter(pubdate__year__gte=2007).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007, 2008, 2009, 1995, 1991]\n        )\n\n        # Test that year aggregation works correctly with the 'lte' lookup\n        self.assertEqual(\n            Book.objects.filter(pubdate__year__lte=2008).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007, 2008]\n        )\n\n        # Test that year aggregation works correctly with the 'gt' lookup\n        self.assertEqual(\n            Book.objects.filter(pubdate__year__gt=2008).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [1995, 1991]\n        )\n\n        # Test that year aggregation works correctly with the 'lt' lookup\n        self.assertEqual(\n            Book.objects.filter(pubdate__year__lt=2008).values('pubdate').annotate(year=Year()).values_list('year', flat=True),\n            [2007, 2006, 2005, 2004, 2003, 2002, 2001, 2000, 1999, 1998, 1997, 1996, 1995, 1994, 1993, 1992, 1991]\n        )\n\n        # Test that year aggregation works correctly on a ForeignKey with a DateField"], "sample_964": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module'}\n    env.config.python_use_unqualified_type_names = True\n    env.config.add_module_names = True\n\n        annotation = 'int'\n        result = _parse_annotation(annotation, env)\n        assert result == [nodes.Text('int')]\n\n        annotation = 'None'\n        result = _parse_annotation(annotation, env)\n        assert result == [type_to_xref('None', env)]\n\n        annotation = '[1, 2, 3]'\n        result = _parse_annotation(annotation, env)\n        assert result == [\n            addnodes.desc_sig_punctuation('', '['),\n            nodes.Text('1'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            nodes.Text('2'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            nodes.Text('3'),\n            addnodes.desc_sig_punctuation('', ']')\n        ]\n\n        annotation = '(1, 2, 3)'\n        result = _parse_annotation(annotation, env)\n        assert result == [\n            addnodes.desc_sig_punctuation('', '('),\n            nodes.Text('1'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            nodes.Text('2'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            nodes.Text('3'),\n            addnodes.desc_sig_punctuation('', ')')\n        ]\n\n        annotation = '[1, [2, 3]]'\n        result = _parse_annotation(annotation, env)\n        assert result == [\n            addnodes.desc_sig_punctuation('', '['),\n            nodes.Text('1'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            addnodes.desc_sig_punctuation('', '['),\n            addnodes.desc_sig_punctuation('', '['),\n            nodes.Text('2'),\n            addnodes.desc_sig_punctuation('', ','),\n            addnodes.desc_sig_space(),\n            nodes.Text('3'),\n            addnodes.desc_sig_punctuation('', ']'),\n            addnodes.desc_sig_punctuation('', ']'),\n            addnodes.desc_sig_punctuation('', ']')\n        ]\n\n        annotation = '[1, typing.Literal[2, 3]]'\n        result = _", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n    env.config.python_use_unqualified_type_names = True\n    env.config.add_module_names = True\n\n    assert _parse_annotation('int') == ['int']\n    assert _parse_annotation('List[int]') == [\n        'List',\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(')'),\n        nodes.Text('int')\n    ]\n    assert _parse_annotation('List[int][]') == [\n        'List',\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(']'),\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(']'),\n        nodes.Text('int')\n    ]\n    assert _parse_annotation('typing.List[int]') == [\n        'typing.List',\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(')'),\n        nodes.Text('int')\n    ]\n    assert _parse_annotation('typing.List[int][]') == [\n        'typing.List',\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(']'),\n        addnodes.desc_sig_punctuation('['),\n        addnodes.desc_sig_punctuation(']'),\n        nodes.Text('int')\n    ]\n\n    # Test the case when env is None\n    with pytest.raises(RemovedInSphinx50Warning):\n        _parse_annotation('int')\n", "def test_parse_annotation_type():\n    env = Mock()\n    ast = ast.parse(\"List[int]\")\n    annotation = _parse_annotation(ast, env)\n    assert annotation[0].astext() == \"List[int]\"\n    assert annotation[1].astext() == \"[\"\n", "def test_parse_annotation():\n    env = Mock()\n    assert _parse_annotation('str') == [nodes.Text('str')]\n    assert _parse_annotation('str()') == [nodes.Text('str')]\n    assert _parse_annotation('str() -> str') == [\n        nodes.Text('str'),\n        nodes.Text(' -> str')\n    ]\n    assert _parse_annotation('str | int') == [\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_punctuation('|'),\n        addnodes.desc_sig_space()\n    ]\n    assert _parse_annotation('str(...') == [\n        addnodes.desc_sig_punctuation('...'),\n        addnodes.desc_sig_punctuation(')'),\n    ]\n    assert _parse_annotation('str(...)') == [\n        addnodes.desc_sig_punctuation('('),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation(')'),\n    ]\n    assert _parse_annotation('str() -> ...') == [\n        nodes.Text('str'),\n        nodes.Text(' -> ...'),\n    ]\n    assert _parse_annotation('str | ...') == [\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_punctuation('|'),\n        addnodes.desc_sig_space()\n    ]\n    assert _parse_annotation('str | ...()') == [\n        addnodes.desc_sig_space(),\n        addnodes.desc_sig_punctuation('|'),\n        addnodes.desc_sig_space(),\n        nodes.Text('...'),\n        addnodes.desc_sig_punctuation('('),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation(')'),\n    ]\n    assert _parse_annotation('str(...) -> int') == [\n        addnodes.desc_sig_punctuation('('),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation(')'),\n        nodes.Text(' -> int')\n    ]\n    assert _parse_annotation('int(...) -> int') == [\n        addnodes.desc_sig_punctuation('('),\n        nodes.Text('int'),\n        addnodes.desc_sig_punctuation(')'),\n        nodes.Text(' -> int')\n    ]\n    assert _parse_annotation('Union[int, str]') == [\n        addnodes.desc_sig_punctuation('('),\n        addnodes.desc_sig_keyword('Union'),\n        addnodes.desc_sig_punctuation('['),\n        nodes.Text('int'),\n        addnodes.desc_sig_punctuation(','),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation(']'),\n        addnodes.desc_sig_punctuation(')'),\n    ]\n    assert _parse_annotation('Union[int, str] -> int') == [\n        addnodes.desc", "def test_handle_signature_empty_classname():\n    env = Mock()\n    env.ref_context['py:class'] = 'SomeClass'\n    env.ref_context['py:module'] = 'some_module'\n    env.config.add_module_names = True\n\n    sig = 'function foo()'\n    signode = desc_signature(sig)\n    result = parse(sig)\n    expected = r'function foo()\\n\\n    '\n    assert result == expected\n    signode['module'] = 'some_module'\n    signode['class'] = 'SomeClass'\n    signode['fullname'] = 'some_module.SomeClass.foo'\n\n    PyFunction.handle_signature(sig, signode)\n    assert signode.astext() == expected\n"], "sample_774": ["def test_OneHotEncoder_handle_unknown_return_mask():\n    encoder = OneHotEncoder(handle_unknown='return_mask')\n    X = np.array([[1, 2], [2, 1], [3, 3]])\n    with pytest.raises(ValueError):\n        encoder.fit_transform(X)\n    encoder.fit(X)\n    X_transformed, mask = encoder.transform(X)\n    assert np.all(mask == np.array([[True, False], [False, True], [True, True]])\n                  .reshape(-1))\n    assert np.all(X_transformed == toarray(sparse.csr_matrix(np.zeros((3, 3)))))\n", "def test_ordinal_encoder_inverse_transform_with_unknown_categories():\n    \"\"\"Test the inverse transform method of OrdinalEncoder when categories are unknown.\"\"\"\n    encoder = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2], ['Unknown', 3]]\n    encoder.fit(X)\n    X_transformed = encoder.transform(X)\n    # Set one of the categories to unknown in the transformed data\n    X_transformed[1, 0] = 3  # unknown category\n    with assert_raises_regex(ValueError, \"Unknown category.*\"):\n        encoder.inverse_transform(X_transformed)\n", "def test_fit_transform_with_handle_unknown_error():\n    \"\"\"Test fit_transform with handle_unknown='error'\"\"\"\n    # Create a OneHotEncoder with handle_unknown='error'\n    encoder = OneHotEncoder(handle_unknown='error')\n\n    # Create a dataset with a feature that contains an unknown value\n    X = [['Male', 1], ['Female', 3], ['Unknown', 2]]\n\n    # Fit the encoder to the dataset\n    with pytest.raises(ValueError):\n        encoder.fit_transform(X)\n\n    # Check that the error message contains the unknown value\n    error_msg = str(encoder)\n    assert 'Found unknown categories [\\'Unknown\\'] in column 0 during transform' in error_msg\n\n    # Check that the error message contains the column index\n    assert 'column 0' in error_msg\n", "def test_onehot_encoder_categories_list():\n    encoder = OneHotEncoder(categories=[['a', 'b'], [1, 2, 3]])\n    X = [['a', 1], ['b', 2]]\n    encoder.fit(X)\n    X_encoded = encoder.transform(X).toarray()\n    expected = np.array([[1.0, 0.0, 1.0, 0.0]])\n    assert_array_equal(X_encoded, expected)\n", "def test_ordinal_encoder_non_unique_categories():\n    encoder = OrdinalEncoder(categories=[['a', 'b', 'b']])\n    X = [['a'], ['b'], ['b']]\n    with assert_raises(ValueError):\n        encoder.fit_transform(X)\n"], "sample_946": ["def test_parse_arglist():\n    env = Mock(spec=BuildEnvironment)\n    env.ref_context['py:module'] = 'module'\n    env.ref_context['py:class'] = 'class'\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_operator('', '*')),\n        desc_parameter('', '', desc_sig_name('', 'x')),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x=y', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_name('', 'x')),\n        desc_parameter('', '', desc_sig_operator('', '=')),\n        desc_parameter('', '', nodes.inline('', 'y', classes=['default_value'], support_smartquotes=False)),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x, y', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_name('', 'x')),\n        desc_parameter('', '', desc_sig_operator('', ', ')),\n        desc_parameter('', '', desc_sig_name('', 'y')),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x, y = z', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_name('', 'x')),\n        desc_parameter('', '', desc_sig_operator('', ', ')),\n        desc_parameter('', '', desc_sig_name('', 'y')),\n        desc_parameter('', '', desc_sig_operator('', '=')),\n        desc_parameter('', '', nodes.inline('', 'z', classes=['default_value'], support_smartquotes=False)),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x:y', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_name('', 'x')),\n        desc_parameter('', '', desc_sig_operator('', ':')),\n        desc_parameter('', '', desc_sig_name('', 'y')),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x:y, z', env)\n    assert_node(node, [\n        desc_parameter('', '', desc_sig_name('', 'x')),\n        desc_parameter('', '', desc_sig_operator('', ':')),\n        desc_parameter('', '', desc_sig_name('', 'y')),\n        desc_parameter('', '', desc_sig_operator('', ', ')),\n        desc_parameter('', '', desc_sig_name('', 'z')),\n    ])\n\n    node = addnodes.desc_signature('', '')\n    _parse_arglist('x = y: z', env)\n    assert_node(node, [\n        desc", "    def test_handle_signature(self, app, builder):\n        func = PyFunction()\n        func.state.document = app.builder.env.get_current_doc()\n        signode = desc_signature('func(name)')\n        assert func.handle_signature('func(name)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name)')\n        assert func.handle_signature('func(name, name)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, *, name)')\n        assert func.handle_signature('func(name, *, name)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name=1)')\n        assert func.handle_signature('func(name, name=1)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name=1, /)')\n        assert func.handle_signature('func(name, name=1, /)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, *, name=1)')\n        assert func.handle_signature('func(name, *, name=1)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name=1, name2=2)')\n        assert func.handle_signature('func(name, name=1, name2=2)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name=1, name2=2, /)')\n        assert func.handle_signature('func(name, name=1, name2=2, /)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, *, name=1, name2=2)')\n        assert func.handle_signature('func(name, *, name=1, name2=2)', signode) == ('func', '')\n\n        signode = desc_signature('func(name, name=1, *, name2=2)')\n        assert func.handle_signature('func(name, name=1, *, name2=2)', signode) == ('func', '')\n", "def test_python_domain_add_module():\n    app = Sphinx()\n    domain = PythonDomain()\n    env = BuildEnvironment(app, mock=True)\n\n    # Test that a new module is added to the domain\n    domain.note_module('test_module', 'test_module', 'Test Module', 'all', False)\n    assert domain.modules == {'test_module': ModuleEntry('test_module', 'test_module', 'Test Module', 'all', False)}\n\n    # Test that a new module is added with no index\n    domain.note_module('no_index_module', 'no_index_module', None, None, False)\n    assert domain.modules == {'test_module': ModuleEntry('test_module', 'test_module', 'Test Module', 'all', False),\n                             'no_index_module': ModuleEntry('no_index_module', 'no_index_module', None, None, False)}\n\n    # Test that a module is added with a synopsis\n    domain.note_module('synopsis_module', 'synopsis_module', 'This is a synopsis', 'all', False)\n    assert domain.modules == {'test_module': ModuleEntry('test_module', 'test_module', 'Test Module', 'all', False),\n                             'no_index_module': ModuleEntry('no_index_module', 'no_index_module', None, None, False),\n                             'synopsis_module': ModuleEntry('synopsis_module', 'synopsis_module', 'This is a synopsis', 'all', False)}\n\n    # Test that a module is added with a platform\n    domain.note_module('platform_module', 'platform_module', None, 'linux', False)\n    assert domain.modules == {'test_module': ModuleEntry('test_module', 'test_module', 'Test Module', 'all', False),\n                             'no_index_module': ModuleEntry('no_index_module', 'no_index_module', None, None, False),\n                             'synopsis_module': ModuleEntry('synopsis_module', 'synopsis_module', 'This is a synopsis', 'all', False),\n                             'platform_module': ModuleEntry('platform_module', 'platform_module', None, 'linux', False)}\n\n    # Test that a module is added with deprecation\n    domain.note_module('deprecated_module', 'deprecated_module', None, None, True)\n    assert domain.modules == {'test_module': ModuleEntry('test_module', 'test_module', 'Test Module', 'all', False),\n                             'no_index_module': ModuleEntry('no_index_module', 'no_index_module',", "def test_PyModule_directive():\n    # Test a module directive\n    env = Mock()\n    env.config.add_module_names = True\n    app = Mock()\n    app.builder = Mock()\n    domain = PythonDomain()\n    directive = PyModule(env, app)\n    result = directive.run()\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.target)\n    assert result[0].get('ids') == ['module-my-module']\n    assert result[0].get('modname') == 'my-module'\n    assert result[0].get('reftitle') == 'my-module'\n    assert result[0].get('refdoc') == domain.env.docname\n    assert result[0].get('refuri') == 'index.html#module-my-module'\n\n    # Test a module directive with synopsis\n    env = Mock()\n    env.config.add_module_names = True\n    app = Mock()\n    app.builder = Mock()\n    domain = PythonDomain()\n    directive = PyModule(env, app, options={'synopsis': 'My module'})\n    result = directive.run()\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.target)\n    assert result[0].get('ids') == ['module-my-module']\n    assert result[0].get('modname') == 'my-module'\n    assert result[0].get('reftitle') == 'My module (my-module)'\n    assert result[0].get('refdoc') == domain.env.docname\n    assert result[0].get('refuri') == 'index.html#module-my-module'\n\n    # Test a module directive with deprecated\n    env = Mock()\n    env.config.add_module_names = True\n    app = Mock()\n    app.builder = Mock()\n    domain = PythonDomain()\n    directive = PyModule(env, app, options={'deprecated': True})\n    result = directive.run()\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.target)\n    assert result[0].get('ids') == ['module-my-module']\n    assert result[0].get('modname') == 'my-module'\n    assert result[0].get('reftitle') == 'my-module (deprecated)'\n    assert result[0].get('refdoc') == domain.env.docname\n    assert result[0].get('refuri') == 'index.html#module-my-module'\n\n    # Test", "def test_parse_arglist():\n    env = Mock()\n    env.ref_context['py:module'] = 'module'\n    env.ref_context['py:class'] = 'class'\n    env.config.add_module_names = True\n\n    # Test normal case\n    arglist = 'arg1, arg2, arg3, arg4, arg5'\n    signode = addnodes.desc_signature('function(arg1, arg2, arg3, arg4, arg5)', '')\n    _parse_arglist(arglist, env)\n    expected = 'arg1, *arg2, arg3, arg4, arg5'\n    assert signode.astext() == expected\n\n    # Test optional arguments\n    arglist = 'arg1 [, arg2], arg3 [, arg4], arg5'\n    signode = addnodes.desc_signature('function(arg1 [, arg2], arg3 [, arg4], arg5)', '')\n    _parse_arglist(arglist, env)\n    expected = 'arg1, arg2, arg3, arg4, arg5'\n    assert signode.astext() == expected\n\n    # Test KW only arguments\n    arglist = 'arg1, *, arg2, *, arg3'\n    signode = addnodes.desc_signature('function(arg1, *, arg2, *, arg3)', '')\n    _parse_arglist(arglist, env)\n    expected = 'arg1, *arg2, *arg3'\n    assert signode.astext() == expected\n\n    # Test positional only arguments\n    arglist = 'arg1, /, arg2'\n    signode = addnodes.desc_signature('function(arg1, /, arg2)', '')\n    _parse_arglist(arglist, env)\n    expected = 'arg1, /arg2'\n    assert signode.astext() == expected\n\n    # Test empty arglist\n    arglist = ''\n    signode = addnodes.desc_signature('function()', '')\n    _parse_arglist(arglist, env)\n    expected = ''\n    assert signode.astext() == expected\n\n    # Test empty parameter list\n    arglist = 'arg1, *, arg2'\n    signode = addnodes.desc_signature('function(arg1, *, arg2)', '')\n    _parse_arglist('', env)\n    expected = ''\n    assert signode.astext() == expected\n\n    # Test invalid syntax\n    arglist = 'arg1, /, *'\n    signode = addnodes"], "sample_962": ["def test_undecorate_mock_object_with_args():\n    with mock(['sphinx.ext.autodoc.mock']):\n        mock_obj = _make_subclass('mock_obj', 'sphinx.ext.autodoc.mock')\n        mock_obj.__sphinx_decorator_args__ = ('arg1', 'arg2')\n        assert undecorate(mock_obj) == 'arg1'\n", "def test_ismock_object_as_class_attribute():\n    # Given\n    class TestClass:\n        mock_obj = _MockObject()\n\n    # When\n    result = ismock(TestClass.mock_obj)\n\n    # Then\n    assert result is True\n\n", "def test_mock_object():\n    \"\"\"Test MockObject behavior.\"\"\"\n    # Test __display_name__ attribute\n    mock_obj = _MockObject()\n    assert mock_obj.__display_name__ == '_MockObject'\n\n    # Test __name__ attribute\n    assert mock_obj.__name__ == ''\n\n    # Test __sphinx_mock__ attribute\n    assert mock_obj.__sphinx_mock__ is True\n\n    # Test __sphinx_decorator_args__ attribute\n    mock_obj2 = _MockObject('test', ('test', _MockObject))\n    assert mock_obj2.__sphinx_decorator_args__ == (_MockObject,)\n\n    # Test __new__ method\n    mock_obj3 = _MockObject('test', (_MockObject, _MockObject))\n    assert isinstance(mock_obj3, _MockObject)\n\n    # Test __init__ method\n    mock_obj4 = _MockObject()\n    assert mock_obj4.__qualname__ == '_MockObject'\n\n    # Test __len__ method\n    assert len(mock_obj4) == 0\n\n    # Test __contains__ method\n    assert '_MockObject' not in mock_obj4\n\n    # Test __iter__ method\n    assert list(iter(mock_obj4)) == []\n\n    # Test __mro_entries__ method\n    assert mock_obj4.__mro_entries__(()) == (type(mock_obj4),)\n\n    # Test __getitem__ method\n    mock_obj5 = mock_obj4['test']\n    assert isinstance(mock_obj5, _MockObject)\n\n    # Test __getattr__ method\n    mock_obj6 = mock_obj4.attr\n    assert isinstance(mock_obj6, _MockObject)\n\n    # Test __call__ method\n    mock_obj7 = mock_obj4()\n    assert isinstance(mock_obj7, _MockObject)\n\n    # Test __repr__ method\n    assert repr(mock_obj4) == '_MockObject'\n", "def test_ismock_mocked_class():\n    with mock(['sphinx.ext.autodoc.mock']):\n        assert ismock(MyClass1)\n        assert ismock(MyClass2)\n        assert not ismock(Integral)\n        assert not ismock(Struct)\n        assert not ismock(TracebackType)\n", "def test_ismock_object():\n    with mock(['_MockObject']):\n        mock_obj = _MockObject()\n        assert ismock(mock_obj)\n\n    mock_obj = MyClass1()\n    assert not ismock(mock_obj)\n\n    mock_obj = _MockModule('_MockObject')\n    assert ismock(mock_obj)\n\n    mock_obj = 'not a mock object'\n    assert not ismock(mock_obj)\n\n    mock_obj = (1, 2, 3)\n    assert not ismock(mock_obj)\n"], "sample_1013": ["def test_imp_namespace_list():\n    from sympy.utilities.lambdify import _imp_namespace\n    from sympy.abc import x\n    expr = [x**2, x**3, x**4]\n    namespace = _imp_namespace(expr)\n    assert len(namespace) == 3\n    assert namespace[x] == x**2\n    assert namespace[x**2] == x**3\n    assert namespace[x**3] == x**4\n", "def test_lambdastr_dummify_with_expression():\n    from sympy.abc import x, y, z\n    expr = x + y + z\n    f = lambdify(x, expr, dummify=True)\n    assert f(1, 2, 3) == 6\n", "def test_lambdastr_multiple_args():\n    from sympy import lambdify, sin, cos\n    from sympy.abc import x, y\n    f = lambdify((x, y), sin(x) + cos(y), modules='math')\n    # test that lambdastr preserves the arguments\n    assert f.__code__.co_argcount == 2\n", "def test_lambdify_with_Tensorflow_and_max():\n    from sympy import Max\n    f = Max(x, sin(x))\n    func = lambdify(x, f, 'tensorflow')\n    result = func(tf.constant(1.0))\n    assert result.op.name == 'Maximum:0'\n    assert result.dtype == tf.float32\n    assert result.shape == ()\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    assert sess.run(func(var)) == 1.0\n    tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n    assert sess.run(func(tensor)).shape == (2, 2)\n", "def test_lambdify_maximum_depth():\n    from sympy import Max\n    max_depth = 10\n    max_depth_expr = Max(*[Max(*[Max(*[Max(x, x) for _ in range(max_depth - i)]\n                                   for _ in range(max_depth - i)])\n                           for i in range(max_depth)])\n\n    max_depth_expr = Max(Max(x, Max(x, x)), Max(x, Max(x, x)))\n\n    max_depth_func = lambdify(x, max_depth_expr, modules='tensorflow')\n\n    assert max_depth_func(1.0) == 1.0\n    assert max_depth_func(2.0) == 2.0\n    assert max_depth_func(3.0) == 3.0\n"], "sample_459": ["    def test_uuid_exact_lookup_native_field(self):\n        field = models.UUIDField(unique=True)\n        self.assertTrue(field.registered_lookup_class(\"exact\").can_use_none_as_rhs)\n        self.assertTrue(field.registered_lookup_class(\"exact\").rhs_is_direct_value(True))\n", "    def test_uuid_iexact(self):\n        value = models.UUIDField().get_db_prep_value(\"123e4567-e89b-12d3-a456-426655440000\")\n        self.assertEqual(value, \"123e4567e89b12d3a456426655440000\")\n", "    def test_gt_lookup(self):\n        lookup = self.model._meta.get_field(\"value\").get_lookup(\"gt\")\n        self.assertEqual(lookup.lookup_name, \"gt\")\n", "    def test_overflows_with_gt(self):\n        field = self.model._meta.get_field(\"value\")\n        internal_type = field.get_internal_type()\n        underflow, overflow = connection.ops.integer_field_range(internal_type)\n        with self.assertRaises(FullResultSet):\n            PositiveIntegerModel.objects.create(value=overflow + 1)\n", "    def test_year_exact(self):\n        self.model.objects.create(value=2022)\n        queryset = self.model.objects.filter(value__year_exact=2022)\n        self.assertEqual(queryset.count(), 1)\n"], "sample_527": ["def test_figure_subplot_polar():\n    fig = Figure(figsize=(6, 6))\n    ax = fig.add_subplot(111, projection='polar')\n    assert isinstance(ax, projections.PolarAxes)\n", "    def test_figure_init_size_negative(self, figsize):\n        with pytest.raises(ValueError):\n            Figure(figsize=figsize)\n", "def test_subplots_adjust_empty_figure():\n    fig = plt.figure()\n    assert fig.get_layout_engine() is None\n    fig.subplots_adjust(left=0.1, right=0.9)\n    assert fig.get_layout_engine() is None\n    assert fig.get_layout_engine().adjust_compatible\n    assert fig.get_layout_engine().colorbar_gridspec\n    fig.clear()\n    assert fig.get_layout_engine() is None\n", "def test_figure__add_axes__plot_inside_rectangle():\n    fig, ax = plt.subplots()\n\n    # Create a rectangle\n    x = [0, 0.1, 0.2, 0.2]\n    y = [0, 0, 0.1, 0.1]\n    rect = path.Path(np.column_stack([x, y]))\n\n    # Add the rectangle to the axes\n    rect_patch = patches.PathPatch(rect, facecolor='red', edgecolor='black')\n    ax.add_patch(rect_patch)\n\n    # Create a plot inside the rectangle\n    x = [0.05, 0.15]\n    y = [0.05, 0.05]\n    ax.plot(x, y, 'bo')\n\n    # Create a new axes inside the figure\n    fig.add_axes([0.1, 0.1, 0.2, 0.2], axisbelow=True)\n\n    # Test if the plot inside the rectangle is also visible in the new axes\n    new_ax = fig.axes[1]\n    assert new_ax.collections[0].get_facecolor() == 'blue'\n    assert new_ax.lines[0].get_markerfacecolor() == 'blue'\n", "    def test_savefig(self, tmp_path, verbose, figure_size):\n        fig, ax = plt.subplots(1, 1)\n        ax.plot([1, 2, 3])\n        # Save figure without passing filename or format\n        fig.savefig(tmp_path / 'test-savefig')\n        # Save figure with filename and format\n        fig.savefig(tmp_path / 'test-savefig-format', format='png')\n\n        if verbose:\n            fig.show()\n\n        # Check that a valid image file was created\n        assert tmp_path / 'test-savefig.png' is not None\n"], "sample_786": ["def test_inverse_transform_with_onehot_encode():\n    X = [[-2, 1.5, -4, -1],\n         [-1, 2.5, -3, -0.5],\n         [0, 3.5, -2, 0.5],\n         [1, 4.5, -1, 2]]\n    est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    expected_Xinv = [[-1.5, 1.5, -3.5, -0.5],\n                     [-0.5, 2.5, -2.5, -0.5],\n                     [0.5, 3.5, -1.5, 0.5],\n                     [0.5, 3.5, -1.5, 1.5]]\n    assert_array_equal(Xinv, expected_Xinv)\n", "def test_KBinsDiscretizer_encode(test_strategy):\n    X = [[-2, 1.5, -4, -1],\n         [-1, 2.5, -3, -0.5],\n         [0, 3.5, -2, 0.5],\n         [1, 4.5, -1, 2]]\n\n    estimator = KBinsDiscretizer(n_bins=3, strategy=test_strategy, encode=encode)\n    estimator.fit(X)\n\n    Xt = estimator.transform(X)\n    assert_array_equal(Xt, expected)\n", "def test_inverse_transform():\n    est = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    X_inv = est.inverse_transform(Xt)\n    assert_array_equal(X_inv, X)\n", "def test_KBinsDiscretizer_encode_onehot_dense():\n    # Test onehot-dense encoding with n_bins=5\n    discretizer = KBinsDiscretizer(n_bins=5, encode='onehot-dense')\n    X = [[-2, 1, -4, -1],\n         [-1, 2, -3, -0.5],\n         [0, 3, -2, 0.5],\n         [1, 4, -1, 2]]\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (4, 20))\n    assert Xt.dtype == np.float64\n\n    # Test onehot-dense encoding with n_bins=2\n    discretizer = KBinsDiscretizer(n_bins=2, encode='onehot-dense')\n    X = [[-2, 1, -4, -1],\n         [-1, 2, -3, -0.5],\n         [0, 3, -2, 0.5],\n         [1, 4, -1, 2]]\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (4, 8))\n\n    # Test onehot-dense encoding with n_bins=1\n    discretizer = KBinsDiscretizer(n_bins=1, encode='onehot-dense')\n    X = [[-2, 1, -4, -1],\n         [-1, 2, -3, -0.5],\n         [0, 3, -2, 0.5],\n         [1, 4, -1, 2]]\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (4, 4))\n\n    # Test encoding on an empty array\n    X = []\n    discretizer = KBinsDiscretizer(n_bins=5, encode='onehot-dense')\n    discretizer.fit(X)\n    Xt = discretizer.transform(X)\n    assert_array_equal(Xt.shape, (0, 0))\n", "def test_kbinsdiscretizer_inverse_transform_with_ordinal_encoding():\n    \"\"\"Test inverse_transform when encode='ordinal'\"\"\"\n    X = [[-2, 1.5, -4, -1],\n         [-1, 2.5, -3, -0.5],\n         [0, 3.5, -2, 0.5],\n         [1, 4.5, -1, 2]]\n\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n    est.fit(X)\n    Xt = est.transform(X)\n    Xinv = est.inverse_transform(Xt)\n    expected = np.array([[-1.5, 1.5, -3.5, -0.5],\n                        [-0.5, 2.5, -2.5, -0.5],\n                        [0.5, 3.5, -1.5, 0.5],\n                        [0.5, 3.5, -1.5, 1.5]])\n    assert_array_equal(Xinv, expected)\n"], "sample_387": ["    def test_formfield_for_dbfield_datetime(self):\n        class MyModel(admin.ModelAdmin):\n            formfield_overrides = {\n                datetime.datetime: {\"form_class\": forms.SplitDateTimeField},\n            }\n\n        ff = self.assertFormfield(\n            datetime.datetime, \"datetime\", formfield_for_dbfield_datetime, **MyModel.__dict__\n        )\n        self.assertIsInstance(ff.field, datetime.datetime)\n        self.assertIsInstance(ff.form_class, forms.SplitDateTimeField)\n", "    def test_has_add_permission(self):\n        ma = admin.ModelAdmin(Company, admin.site)\n        self.assertTrue(ma.has_add_permission(self.request))\n        self.assertFalse(ma.has_add_permission(self.request, obj=Company.objects.get(id=1)))\n", "    def test_date_field(self):\n        # Test for correct behavior of date_field\n        ff = self.assertFormfield(Individual, \"dob\", admin.widgets.AdminDateWidget)\n", "    def test_inline_formset_factory_with_invalid_fks(self):\n        # Test that inline_formset_factory raises an error when given an invalid fk_name\n        with self.assertRaises(ValueError):\n            admin.inlineformset_factory(User, Profile, fk_name='invalid')\n", "    def test_formfield_for_dbfield_inline_model_admin(self):\n        # We need to check that formfield_for_dbfield is called for each field\n        # in the parent model, even when it's an inline.\n        class MyInline(admin.StackedInline):\n            model = Car\n            fk_name = 'car'\n            exclude = ['make']\n\n        car_admin = admin.ModelAdmin(Car, admin.site)\n        inline_admin = MyInline(car_admin.model, admin.site)\n\n        inline_model_admin = admin.inlines[0]\n\n        # Create a test model with two fields\n        class TestModel(admin.ModelAdmin):\n            model = Car\n            formfield_overrides = {\n                Car._meta.get_field('make'): {'widget': admin.widgets.AdminInputWidget}\n            }\n\n        # Test that formfield_for_dbfield is called for all fields in the parent\n        # model, including the ones that are excluded in the inline model admin.\n        ff = inline_model_admin.formfield_for_dbfield(TestModel.model._meta.get_field('make'), request=None)\n        self.assertIsInstance(ff.widget, admin.widgets.AdminInputWidget)\n\n        # Test that formfield_for_dbfield is called for all fields in the parent\n        # model, including the ones that are not in the exclude list of the\n        # inline model admin.\n        ff = inline_model_admin.formfield_for_dbfield(TestModel.model._meta.get_field('model'), request=None)\n        self.assertIsInstance(ff.widget, admin.widgets.AdminInputWidget)\n\n        # Test that formfield_for_dbfield is called for all fields in the parent\n        # model, including the ones that are in the exclude list of the inline\n        # model admin.\n        ff = inline_model_admin.formfield_for_dbfield(TestModel.model._meta.get_field('owner'), request=None)\n        self.assertIsInstance(ff.widget, admin.widgets.AdminUserListRawIdWidget)\n"], "sample_669": ["def test_capture_manager_suspend_resume_global_capture():\n    \"\"\"Check that global capturing can be suspended and resumed.\"\"\"\n    capman = CaptureManager(method=\"fd\")\n    capman.start_global_capturing()\n    assert capman.is_globally_capturing() is True\n    capman.suspend_global_capture()\n    assert capman.is_globally_capturing() is False\n    capman.resume_global_capture()\n    assert capman.is_globally_capturing() is True\n    capman.stop_global_capturing()\n    assert capman.is_globally_capturing() is False\n", "    def test_resume_global_capture(self):\n        # Test that resume_global_capture works when global capturing is not enabled\n        capman = CaptureManager(\"no\")\n        capman.resume_global_capture()\n        assert capman._global_capturing is None\n\n        # Test that resume_global_capture works when global capturing is enabled\n        capman._global_capturing = _get_multicapture(\"fd\")\n        capman.resume_global_capture()\n        assert capman._global_capturing is not None\n\n        # Test that resume_global_capture does not throw an exception when _global_capturing is None\n        capman._global_capturing = None\n        capman.resume_global_capture()\n", "    def test_read_global_capture(self, method):\n        # Arrange\n        capman = CaptureManager(method)\n        capman.start_global_capturing()\n\n        # Act\n        with capman.item_capture(\"call\", None):\n            sys.stdout.write(\"stdout content\\n\")\n            sys.stderr.write(\"stderr content\\n\")\n\n        # Assert\n        out, err = capman.read_global_capture()\n        assert out == \"stdout content\\n\"\n        assert err == \"stderr content\\n\"\n", "    def test_global_capture(self, capsys, method, capture_method):\n        capman = capture.CaptureManager(capture_method)\n        capman.start_global_capturing()\n        out = capsys.stdout\n        err = capsys.stderr\n        try:\n            out.write(\"global capture\")\n            err.write(\"global capture\")\n            capman.suspend_global_capture()\n            with pytest.raises(TypeError):\n                out.write(\"should not be able to write\")\n                err.write(\"should not be able to write\")\n        finally:\n            capman.resume_global_capture()\n            out.write(\"after suspend/resume\")\n            err.write(\"after suspend/resume\")\n", "    def test_global_capturing(self, capsys, method):\n        capman = CaptureManager(method)\n        capman.start_global_capturing()\n        try:\n            with capsys.disabled():\n                print(\"global capturing disabled\")\n                assert sys.stdout.isatty()\n                assert sys.stderr.isatty()\n            assert not sys.stdout.isatty()\n            assert not sys.stderr.isatty()\n        finally:\n            capman.suspend_global_capture()\n        out, err = capman.read_global_capture()\n        assert out == \"\"\n        assert err == \"\"\n"], "sample_27": ["    def test_header_diff_duplicate_keywords(self):\n        # Test that HeaderDiff correctly handles keywords with different numbers\n        # of duplicates\n        hdul = HDUList([PrimaryHDU(Header(\"SIMPLE = T\"))])\n        hdul.append(PrimaryHDU(Header(\"SIMPLE = T\\nSIMPLE = T\\nHISTORY = This is a test\")))\n        hdul2 = HDUList([PrimaryHDU(Header(\"SIMPLE = T\\nSIMPLE = T\\nHISTORY = This is a test\\nHISTORY = Another history\"))])\n        diff = HeaderDiff.fromdiff(FITSDiff(hdul, hdul2), hdul[0].header, hdul2[0].header)\n        assert not diff.identical\n        assert diff.diff_duplicate_keywords == {'SIMPLE': (2, 2)}\n        assert diff.diff_keyword_values == {'HISTORY': [('This is a test', 'This is a test'), ('This is a test', 'Another history')]}\n", "    def test_header_diff_duplicates(self):\n        hdu = fits.PrimaryHDU()\n        header = hdu.header\n\n        # Add a duplicate HISTORY card\n        header.append(('HISTORY', 'Test history'))\n\n        # Create a copy of the header without the duplicate HISTORY card\n        header_copy = Header(header.tostring())\n\n        diff = HeaderDiff(header, header_copy)\n        self.assertFalse(diff.identical)\n\n        diff_report = diff.report()\n\n        self.assertIn('Inconsistent duplicates of keyword HISTORY', diff_report)\n        self.assertIn('Occurs 2 time(s) in a, 1 times in (b)', diff_report)\n\n        # Add another duplicate HISTORY card\n        header.append(('HISTORY', 'Test history 2'))\n\n        # Create a copy of the header without the duplicate HISTORY cards\n        header_copy = Header(header.tostring())\n\n        diff = HeaderDiff(header, header_copy)\n        self.assertFalse(diff.identical)\n\n        diff_report = diff.report()\n\n        self.assertIn('Inconsistent duplicates of keyword HISTORY', diff_report)\n        self.assertIn('Occurs 3 time(s) in a, 1 times in (b)', diff_report)\n", "    def test_diff_ImageDataDiff(self):\n        hdua = ImageHDU(np.ones((3, 3)))\n        hdub = ImageHDU(np.ones((3, 3)))\n        hdub.data += 1\n        diff = ImageDataDiff(hdua, hdub)\n        self.assertFalse(diff.identical)\n        self.assertEqual(diff.diff_dimensions, ((3, 3), (3, 3)))\n        self.assertEqual(diff.diff_pixels, [((0, 0), (1.0, 2.0))])\n", "def test_FITS_diff_on_string_headers():\n    hdu1 = fits.PrimaryHDU(header=Header.fromstring(\"SIMPLE  =                    T / file does follow FITS standards\"))\n    hdu2 = fits.PrimaryHDU(header=Header.fromstring(\"SIMPLE  =                    T / file does follow FITS standards\\nSIMPLE  =                    T / file does follow FITS standards\"))\n    fd = FITSDiff(hdu1, hdu2)\n    assert not fd.identical\n    assert fd.diff_hdu_count == (1, 2)\n    assert fd.diff_hdus == [(0, HDUDiff.fromdiff(fd, hdu1, hdu2), \"\", hdu1.header['SIMPLE'])]\n", "def test_header_diff_common_keywords():\n    \"\"\"\n    Verify that the common_keywords attribute is populated correctly.\n\n    This test was added to check the coverage of the code that determines the\n    common keywords between two headers.  Prior to the addition of this test,\n    there was no coverage of this code.\n    \"\"\"\n    hdu = fits.PrimaryHDU()\n    header1 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    header2 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    diff = HeaderDiff.fromdiff(hdu.header, header1, header2)\n    assert diff.common_keywords == ['SIMPLE']\n\n    header1 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    header2 = Header.fromstring(\"SIMPLE  =T / written by PyFITS\")\n    diff = HeaderDiff.fromdiff(hdu.header, header1, header2)\n    assert diff.common_keywords == ['SIMPLE']\n\n    header1 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    header2 = Header.fromstring(\"SIMPLE  =T / written by IDL; other key = value\")\n    diff = HeaderDiff.fromdiff(hdu.header, header1, header2)\n    assert diff.common_keywords == ['SIMPLE']\n\n    header1 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    header2 = Header.fromstring(\"SIMPLE  =F / written by IDL\")\n    diff = HeaderDiff.fromdiff(hdu.header, header1, header2)\n    assert diff.common_keywords == []\n\n    header1 = Header.fromstring(\"SIMPLE  =T / written by IDL\")\n    header2 = Header.fromstring(\"SIMPLE  =T / written by IDL; BLANKCARD\")\n    diff = HeaderDiff.fromdiff(hdu.header, header1, header2)\n    assert diff.common_keywords == ['SIMPLE']\n"], "sample_673": ["    def test_repr_failure_missing_docstring(self):\n        # Arrange\n        import doctest\n        class TestClass:\n            pass\n        class DoctestItemWithMissingDocstring(DoctestItem):\n                return self.fspath, self.dtest.lineno, \"[doctest] %s\" % self.name\n\n        # Act\n        doctest_item = DoctestItemWithMissingDocstring(\n            name=\"test_name\", parent=None, runner=None, dtest=None\n        )\n        excinfo = ExceptionInfo(doctest.DocTestFailure(test=None, example=None, got=None))\n\n        # Assert\n        result = doctest_item.repr_failure(excinfo)\n        assert isinstance(result, ReprFailDoctest)\n        assert result.reprlocation_lines == []\n", "    def test_repr_failure_DocTestFailure(self):\n        # Given\n        item = DoctestItem(\"test_example\", None, runner=None, dtest=None)\n        failures = [\n            doctest.DocTestFailure(\n                test=doctest.DocTest(\"test\", \"module\"),\n                example=doctest.Example(\"example\", \"source\"),\n                got=\"got\",\n            )\n        ]\n        excinfo = ExceptionInfo(\n            None,\n            None,\n            None,\n            None,\n            MultipleDoctestFailures(failures),\n        )\n        # When\n        repr_failure = item.repr_failure(excinfo)\n        # Then\n        assert isinstance(repr_failure, ReprFailDoctest)\n", "    def test_repr_failure_multiple_failures(self, capsys):\n        # Prepare a docstring with multiple failures\n        module = pytest.Module(\"test_module\")\n        doctest_textfile = DoctestTextfile.from_parent(parent=module, fspath=\"test.txt\")\n        doctest_item = DoctestItem.from_parent(parent=doctest_textfile, name=\"test_function\")\n        doctest_item.dtest = doctest.DocTest(\n            \"test_function\", \"module\", \"test_function\", \"test.txt\", 0\n        )\n        doctest_item.dtest.examples = [\n            doctest.Example(\n                \">>> 1 + 1\", \">>> 2\", \">>> expected 3 but got 2\", doctest.DocTestFailure()\n            ),\n            doctest.Example(\n                \">>> 2 + 2\", \">>> 4\", \">>> expected 5 but got 4\", doctest.DocTestFailure()\n            ),\n        ]\n        with _patch_unwrap_mock_aware():\n            doctest_item.fixture_request = FixtureRequest(doctest_item)\n            doctest_item.fixture_request._fillfixtures()\n\n        # Run the test\n        doctest_item.runtest()\n\n        # Check the output\n        output = capsys.readouterr().out\n        assert \"MultipleDoctestFailures\" in output\n        assert \"test_function\" in output\n        assert \"expected 3 but got 2\" in output\n        assert \"expected 5 but got 4\" in output\n", "def test_repr_failure_multiple_doctest_failures():\n    # Given\n    class MockDoctest:\n            self.examples = [\n                {\"lineno\": 1, \"source\": \"assert True\"},\n                {\"lineno\": 2, \"source\": \"assert False\"},\n                {\"lineno\": 3, \"source\": \"assert True\"},\n            ]\n\n    # When\n    item = DoctestItem.from_parent(parent=None, name=\"MockDoctest\", runner=None, dtest=MockDoctest())\n    with _patch_unwrap_mock_aware():\n        item.setup()\n    item.dtest.lineno = 1\n    item.dtest.filename = \"mock.py\"\n    item.runner.run(item.dtest, out=[])\n    item.runner.continue_on_failure = False\n    failures = MultipleDoctestFailures([\n        doctest.DocTestFailure(item.dtest, item.dtest.examples[0], \"assert False\"),\n        doctest.DocTestFailure(item.dtest, item.dtest.examples[1], \"assert False\"),\n    ])\n    with pytest.raises(MultipleDoctestFailures):\n        item.repr_failure(ExceptionInfo(failures))\n\n    # Then\n    # Check that the repr output is as expected\n    assert \"mock.py:1\" in str(item.repr_failure(ExceptionInfo(failures)))\n    assert \"mock.py:2\" in str(item.repr_failure(ExceptionInfo(failures)))\n    assert \"[doctest] MockDoctest\" in str(item.repr_failure(ExceptionInfo(failures)))\n", "    def test_repr_failure(self):\n        import doctest\n        import pytest\n\n        class TestModule:\n                pass\n\n                \"\"\"\n                >>> self.docstring()\n\n                Example output:\n                'hello world'\n                \"\"\"\n\n        class TestDoctextFile:\n                pass\n\n                \"\"\"\n                >>> self.docstring()\n\n                Example output:\n                'hello world'\n                \"\"\"\n\n        test_module = TestModule()\n        test_file = TestDoctextFile()\n\n        runner = _get_runner()\n        module = DoctestModule.from_parent(name=\"test_module\", fspath=py.path.local(\"test_module.py\"))\n        item = DoctestItem.from_parent(parent=module, name=\"docstring\", runner=runner, dtest=doctest.DocTest(test_module, \"test_module.py\", 1, 0, \"docstring\", [\">>> self.docstring()\",\"Example output:\",\"'hello world'\"]))\n\n        item.setup()\n        try:\n            item.runtest()\n            pytest.fail(\"MultipleDoctestFailures not raised\")\n        except MultipleDoctestFailures:\n            pass\n\n        file = DoctestTextfile.from_parent(name=\"test_file\", fspath=py.path.local(\"test_file.txt\"))\n        item = DoctestItem.from_parent(parent=file, name=\"docstring\", runner=runner, dtest=doctest.DocTest(test_file, \"test_file.txt\", 1, 0, \"docstring\", [\">>> self.docstring()\",\"Example output:\",\"'hello world'\"]))\n\n        item.setup()\n        try:\n            item.runtest()\n            pytest.fail(\"MultipleDoctestFailures not raised\")\n        except MultipleDoctestFailures:\n            pass\n\n        # Test repr_failure for None failures\n        item = DoctestItem.from_parent(parent=file, name=\"docstring\", runner=runner, dtest=doctest.DocTest(test_file, \"test_file.txt\", 1, 0, \"docstring\", [\">>> self.docstring()\",\"Example output:\",\"'hello world'\"]))\n        item.setup()\n        assert item.repr_failure(None) is None\n\n        # Test repr_failure for DocTestFailure\n        failure = doctest.DocTestFailure(test_file.docstring, test_file.docstring, \"hello world\")\n        item.dtest = failure\n        item.setup()\n        result = item.repr_failure(None"], "sample_710": ["def test_is_skipped_no_skipped_object():\n    class MyClass:\n        pass\n    assert _is_skipped(MyClass) is False\n", "def test_make_xunit_fixture_scope(tmpdir, monkeypatch: MonkeyPatch):\n    \"\"\"Test that the scope of the fixture is set correctly.\"\"\"\n    # Create a test module with a test method\n    tmpdir.join(\"test_module.py\").write(\n        \"\"\"\n        import unittest\n        from _pytest.pytester import pytester\n\n        class TestClass(unittest.TestCase):\n            @pytest.fixture(scope=\"function\", autouse=True)\n                pass\n\n                pass\n        \"\"\"\n    )\n\n    # Run pytest with the test module\n    pytester = Pytester()\n    pytester.makepyfile(content=tmpdir.join(\"test_module.py\").read())\n    result = pytester.runpytest_inprocess()\n\n    # Check that the scope of the fixture is set correctly\n    assert result.ret == ExitCode.OK\n    assert pytester.subprocess.arglist[0].args[2] == \"-k\"\n    assert \"test_method\" in pytester.result.outlines\n", "def test_pytest_runtest_makereport(unittest, item, call):\n    # Given\n    item.originalname = \"test_example\"\n    item._excinfo = [None]\n    call.excinfo = None\n\n    # When\n    pytest_runtest_makereport(item, call)\n\n    # Then\n    assert call.excinfo is not None\n    assert isinstance(call.excinfo.value, pytest.skip.Exception)\n", "def test_check_testcase_implements_trial_reporter(tmpdir):\n    # Test check_testcase_implements_trial_reporter function\n    class TestTestCase:\n        pass\n\n    # Create a temporary directory\n    tester = Pytester.frompyfunc(tmpdir)\n\n    # Test that the function doesn't crash with a new class\n    # that doesn't implement the IReporter interface\n    check_testcase_implements_trial_reporter(done=[1])\n\n    # Test that the function doesn't crash with a class that\n    # implements the IReporter interface\n    class TestReporter:\n            pass\n\n            pass\n\n            pass\n\n    classImplements(TestReporter, IReporter)\n    check_testcase_implements_trial_reporter(done=[1])\n\n    # Test that the function crashes if the IReporter interface\n    # is not implemented\n    class TestNonReporter:\n            pass\n\n    check_testcase_implements_trial_reporter(done=[0])\n", "    def test_fully_collecting_unittest_test_cases(self, pytester: Pytester) -> None:\n        \"\"\"Make sure all unittest test cases are properly collected, including ones with `__test__ = False`.\n\n        The test will run in an environment where `unittest` is not imported yet, to simulate real-world usage.\n        \"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class MyTestCase(unittest.TestCase):\n                    assert False\n            class MyTestCase2(unittest.TestCase):\n                __test__ = False\n                    assert False\n            class MyTestCase3(unittest.TestCase):\n                    assert False\n                __test__ = False\n            \"\"\"\n        )\n        result = pytester.runpytest_inprocess(\"-p pytest_unittest\")\n        result.assert_outcomes(0)\n"], "sample_834": ["def test_NCA_init_auto():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    nca = NeighborhoodComponentsAnalysis(init='auto', random_state=0)\n    nca.fit(X, y)\n    assert nca.init == 'lda'\n", "def test_loss_and_grad_random_init():\n    X = rng.rand(10, 10)\n    y = np.random.randint(0, 2, 10)\n    nca = NeighborhoodComponentsAnalysis(n_components=5, init='random')\n    nca.fit(X, y)\n    transformation = np.random.rand(5, X.shape[1])\n    loss, gradient = nca._loss_grad_lbfgs(transformation, X, (y[:, np.newaxis] == y[np.newaxis, :]))\n    assert loss > 0\n    assert gradient.shape == transformation.flatten().shape\n", "def test_nca_callback():\n    nca = NeighborhoodComponentsAnalysis(callback=lambda x, i: print(f'Callback called with x={x}, i={i}'))\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=3, n_repeated=0, n_classes=2)\n    nca.fit(X, y)\n    assert 'Callback called with x=' in nca._callback.call_args_list[0][0][0]\n", "def test_nca_callback():\n    # Initialize a NeighborhoodComponentsAnalysis object with a custom callback\n    nca = NeighborhoodComponentsAnalysis(callback=lambda x, i: print(f'Callback called at iteration {i}'))\n\n    # Create a random dataset\n    X, y = make_blobs(n_samples=100, centers=5, n_features=10, random_state=0)\n    y = y.astype(int)\n\n    # Fit the NCA object with the dataset\n    nca.fit(X, y)\n\n    # Check that the callback was called\n    assert nca.n_iter_ > 0\n", "def test_NCA_inverse_transformation():\n    \"\"\"Test that the inverse transformation is valid.\n\n    If the original transformation is valid, the inverse transformation should\n    map back to the original data. The inverse transformation should be computed\n    as the transpose of the original transformation.\n\n    \"\"\"\n    X, y = make_blobs(n_samples=100, n_features=5, centers=5, cluster_std=0.8,\n                       random_state=0)\n    nca = NeighborhoodComponentsAnalysis(n_components=2)\n    nca.fit(X, y)\n    X_embedded = nca.transform(X)\n    inverse_X = nca.inverse_transform(X_embedded)\n    assert_array_almost_equal(X, inverse_X)\n"], "sample_678": ["    def test_get_extended_length_path_str(self):\n        \"\"\"Test get_extended_length_path_str().\"\"\"\n        path = \"C:\\\\path\\\\to\\\\file.txt\"\n        expected = \"\\\\\\\\?\\\\C:\\\\path\\\\to\\\\file.txt\"\n        assert get_extended_length_path_str(path) == expected\n\n        path = \"C:\\\\path\\\\to\\\\file.txt\"\n        expected = \"\\\\\\\\?\\\\C:\\\\path\\\\to\\\\file.txt\"\n        assert get_extended_length_path_str(path) == expected\n\n        path = \"C:\\\\path\\\\to\\\\file.txt\"\n        expected = \"\\\\\\\\?\\\\UNC\\\\C\\\\path\\\\to\\\\file.txt\"\n        assert get_extended_length_path_str(path) == expected\n\n        path = \"\\\\\\\\unc\\\\path\\\\to\\\\file.txt\"\n        expected = \"\\\\\\\\?\\\\UNC\\\\UNC\\\\path\\\\to\\\\file.txt\"\n        assert get_extended_length_path_str(path) == expected\n", "    def test_get_extended_length_path_str(self):\n        \"\"\"Test get_extended_length_path_str function.\"\"\"\n        # Test UNC path\n        unc_path = r\"\\\\server\\share\\dir\"\n        expected = r\"\\\\?\\UNC\\server\\share\\dir\"\n        assert get_extended_length_path_str(unc_path) == expected\n\n        # Test long path\n        long_path = r\"C:\\long\\path\\to\\file.txt\"\n        expected = r\"\\\\?\\C:\\long\\path\\to\\file.txt\"\n        assert get_extended_length_path_str(long_path) == expected\n\n        # Test path without prefix\n        path = r\"C:\\dir\\file.txt\"\n        expected = path\n        assert get_extended_length_path_str(path) == expected\n", "    def tmp_dir(self, request, tmp_path):\n        path = tmp_path / \"test_dir\"\n        path.mkdir()\n        return path\n", "    def test_make_numbered_dir(self, path, prefix, keep, expected_number, expected_path):\n        root = Path(\"/tmp\")\n        with unittest.mock.patch(\"py._pytest.pathlib.Path.mkdir\") as mock_mkdir:\n            p = make_numbered_dir(root, prefix)\n            mock_mkdir.assert_called_once_with()\n            assert p == Path(expected_path)\n", "    def test_parse_num_edge_cases(self, match):\n        # Test that parse_num handles edge cases correctly\n        assert parse_num(\"123\") == 123\n        assert parse_num(\"-123\") == -123\n        assert parse_num(\"abc\") == -1\n        assert parse_num(\"123abc\") == -1\n        assert parse_num(\"\") == -1\n        assert parse_num(None) == -1\n"], "sample_635": ["    def test_docstring_type_sphinx(self):\n        \"\"\"Test Sphinx docstring type\"\"\"\n        self._run_test(\"\"\"\n                \"\"\"\n                :param x: a parameter\n                :type x: int\n                :param y: another parameter\n                :type y: str\n                \"\"\"\n                pass\n        \"\"\", msgs=[MessageTest(msg_id='docstring-type-mismatch', line=3, col=2)])\n", "    def test_sphinx_docstring(self):\n        self._test_docstring(\"sphinx\", \"sphinx\", \"param1 :type: int\")\n", "    def test_default_docstring_type(self):\n        \"\"\"Test default docstring type with no configuration\"\"\"\n        node = astroid.extract_node('''\n            class A:\n                    \"\"\"Some default docstring.\"\"\"\n        ''')\n        self._check_message(node, 'docstring-type', msg_id='default')\n", "    def test_numpy_docstring_type(self):\n        \"\"\"Test that docstring type is correctly detected as numpy\"\"\"\n        self._check_message(\n            \"\"\"\n                \"\"\"\"\n                    Args:\n                        a (int): int\n                        b (int): int\n                    \"\"\", \n            \"docstring-type\", nodes.FunctionDef, param_name=\"foo\", line=1, col=1\n        )\n", "    def test_negative_docstring_with_param_section(self):\n        \"\"\"Test a docstring with a param section and no description\"\"\"\n        with set_config(self, {\n            \"accept_no_param_doc\": False,\n        }):\n            self.checker.register_global_options()\n\n            node = astroid.extract_node(\"\"\"\n                \"\"\"\n                :param param1: first parameter\n                :param param2:\n                \"\"\"\n                pass\n            \"\"\")\n            self.assertMessageEqual(node, \"docstring-param-doc\", \"param2\")\n"], "sample_1156": ["def test_asinh():\n    x = symbols('x')\n    assert asinh(-1).equals(-log(sqrt(2) - 1))\n    assert asinh(2).equals(log(sqrt(2) + 1))\n    assert asinh(0).equals(0)\n    assert asinh(oo).equals(oo)\n    assert asinh(-oo).equals(-oo)\n    assert asinh(nan).equals(nan)\n\n    assert asinh(1).evalf(2).equals(asinh(1).evalf(2))\n    assert asinh(-1).evalf(2).equals(asinh(-1).evalf(2))\n    assert asinh(2).evalf(2).equals(asinh(2).evalf(2))\n    assert asinh(0).evalf(2).equals(asinh(0).evalf(2))\n    assert asinh(oo).evalf(2).equals(asinh(oo).evalf(2))\n    assert asinh(-oo).evalf(2).equals(asinh(-oo).evalf(2))\n    assert asinh(nan).evalf(2).equals(asinh(nan).evalf(2))\n\n    assert asinh(1).evalf(3).equals(asinh(1).evalf(3))\n    assert asinh(-1).evalf(3).equals(asinh(-1).evalf(3))\n    assert asinh(2).evalf(3).equals(asinh(2).evalf(3))\n    assert asinh(0).evalf(3).equals(asinh(0).evalf(3))\n    assert asinh(oo).evalf(3).equals(asinh(oo).evalf(3))\n    assert asinh(-oo).evalf(3).equals(asinh(-oo).evalf(3))\n    assert asinh(nan).evalf(3).equals(asinh(nan).evalf(3))\n\n    assert asinh(1).evalf(4).equals(asinh(1).evalf(4))\n    assert asinh(-1).evalf(4).equals(asinh(-1).evalf(4))\n    assert asinh(2).evalf(4).equals(asinh(2).evalf(4))\n    assert asinh(0).evalf(4).equals(asinh(0).evalf(4))\n    assert asinh", "def test_asinh():\n    x = Symbol('x')\n    assert unchanged(asinh(x) + asinh(x)).simplify() == 2*asinh(x)\n    assert unchanged(asinh(x) + asinh(2*x)).simplify() == 2*asinh(x + 2*x)\n    assert unchanged(asinh(x) + asinh(-x)).simplify() == 2*asinh(-x)\n    assert unchanged(asinh(x) + asinh(x + 2)).simplify() == 2*asinh(x + x + 2)\n    assert unchanged(asinh(2*x) + asinh(x)).simplify() == 2*asinh(x + 2*x)\n    assert unchanged(asinh(-2*x) + asinh(x)).simplify() == 2*asinh(-x + 2*x)\n    assert unchanged(asinh(x) + asinh(2*x + 2)).simplify() == 2*asinh(x + 2*x + 2)\n    assert unchanged(asinh(2*x + 2) + asinh(x)).simplify() == 2*asinh(x + 2*x + 2)\n    assert unchanged(asinh(-x + 2) + asinh(x)).simplify() == 2*asinh(-x + x + 2)\n    assert unchanged(asinh(x) - asinh(x)).simplify() == 0\n    assert unchanged(asinh(x) - asinh(2*x)).simplify() == -2*asinh(x + 2*x)\n    assert unchanged(asinh(x) - asinh(-x)).simplify() == 0\n    assert unchanged(asinh(x) - asinh(x + 2)).simplify() == -2*asinh(x + x + 2)\n    assert unchanged(asinh(2*x) - asinh(x)).simplify() == -2*asinh(x + 2*x)\n    assert unchanged(asinh(-2*x) - asinh(x)).simplify() == -2*asinh(-x + 2*x)\n    assert unchanged(asinh(x) - asinh(2*x + 2)).simplify() == -2*asinh(x + 2*x + 2)\n    assert unchanged(asinh(2*x + 2) - asinh(x)).simplify() == -2*asinh(x + 2*x + 2", "def test_atanh_nonnegative_value():\n    x = Symbol('x', real=True)\n    assert atanh(x).is_nonnegative is False\n    assert atanh(x).is_positive is False\n    assert atanh(x).is_nonpositive is True\n    assert atanh(x).is_negative is False\n", "def test_hyperbolic_function_rewrite_as_csch():\n    assert sinh(pi * I / 2).rewrite(csch) == S.Pi / (2 * csch(pi / 2))\n    assert cosh(pi / 2).rewrite(csch) == S.ImaginaryUnit * (cosh(pi / 2 + I * pi / 2) + I * sinh(pi / 2))\n    assert tanh(pi * I / 4).rewrite(csch) == (sinh(pi / 4) * cosh(pi / 4) + cosh(pi / 4) * sinh(pi / 4)) / (cosh(pi / 2) * sinh(pi / 2))\n    assert coth(pi / 2).rewrite(csch) == 1 / csch(pi / 2)\n    assert sech(pi / 2).rewrite(csch) == S.ImaginaryUnit / csch(pi / 2 + I * pi / 2)\n    assert asinh(pi / 2).rewrite(csch) == log(csch(pi / 2) + sqrt(1 + 1 / csch(pi / 2)**2))\n    assert acosh(pi / 2).rewrite(csch) == sqrt(1 + 1 / csch(pi / 2)**2)\n    assert atanh(pi / 2).rewrite(csch) == 1 / (csch(pi / 2) + sqrt(1 + 1 / csch(pi / 2)**2))\n    assert acoth(pi / 2).rewrite(csch) == 1 / (1 / csch(pi / 2) - sqrt(1 + 1 / csch(pi / 2)**2))\n    assert asech(pi / 2).rewrite(csch) == log(2 / csch(pi / 2) + sqrt(1 / csch(pi / 2)**2 - 1))\n    assert acsch(pi / 2).rewrite(csch) == log(1 / csch(pi / 2) + sqrt(1 / csch(pi / 2)**2 + 1))\n", "    def test_csch(self):\n        from sympy import cosh, sinh, csch, S, I, log, sqrt\n        from sympy.testing.pytest import raises\n\n        # csch definition\n        assert csch(0).is_real\n        assert csch(S.Pi*I).is_real\n\n        # csch properties\n        assert csch(S.Pi*I).is_positive is False\n        assert csch(2*S.Pi*I).is_positive is True\n\n        # csch behavior with oo\n        assert csch(oo).is_finite is False\n\n        # csch behavior with nan\n        assert csch(nan).is_finite is False\n\n        # csch behavior with zoo\n        assert csch(zoo).is_finite is False\n\n        # csch properties with zero\n        assert csch(0).is_zero\n\n        # csch behavior with negative numbers\n        assert csch(-S.Pi*I).is_positive is False\n\n        # csch behavior with complex numbers\n        assert csch(S.Pi*I + S.Pi).is_positive is False\n        assert csch(2*S.Pi*I).is_positive is True\n        assert csch(-S.Pi*I + S.Pi).is_positive is True\n\n        # csch behavior with real numbers\n        assert csch(S.Pi).is_finite is True\n        assert csch(S.Pi + S.Pi).is_finite is True\n        assert csch(S.Pi - S.Pi).is_finite is True\n\n        # csch behavior with zero arguments\n        assert csch(0).is_zero\n\n        # csch evaluation\n        assert csch(S.Pi*I).simplify() == S.ImaginaryUnit / (cosh(S.Pi))\n\n        # csch derivatives\n        from sympy import diff\n        assert diff(csch, sinh).simplify() == -coth(cosh)\n\n        # csch Taylor series\n        from sympy import series\n        assert series(csch(x), x, 0, 3).simplify() == (4*x**3 - 2*x)/3 + S.One/x\n\n        # csch as_real_imag\n        re_part, im_part = csch(S.Pi*I).as_real_imag()\n        assert re_part.is_zero\n        assert im_part == 1/Sinh(S"], "sample_741": ["def test_GridSearchCV_refit_error():\n    # Tests that refit raises an error when the estimator has a score method\n    # but no scoring function is given.\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    grid_search = GridSearchCV(LinearSVC(), {'C': [1, 2]})\n    grid_search.fit(X, y)\n    assert_raise_message(ValueError, \"For multi-metric scoring, the parameter refit must be set to a scorer key to refit an estimator with the best parameter setting on the whole data and make the best_* attributes available for that metric.\", grid_search.refit)\n", "def test_GridSearchCV_refit_with_multiple_metrics():\n    \"\"\"Test GridSearchCV refit with multiple metrics\"\"\"\n    # create a scorer for each metric\n    precision = make_scorer(f1_score, average='macro')\n    recall = make_scorer(recall_score, average='macro')\n    accuracy = make_scorer(accuracy_score)\n\n    # define a Pipeline with multiple scorers\n    pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('estimator', DecisionTreeClassifier(random_state=42)),\n        ('scorer', make_scorer(f1_score, average='macro')),\n        ('scorer', precision),\n        ('scorer', recall),\n        ('scorer', accuracy)\n    ])\n\n    # define a GridSearchCV instance with multiple scorers\n    param_grid = {'estimator__max_depth': [3, 5, 10]}\n    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring={'f1_macro': precision, 'precision_macro': precision, 'recall_macro': recall, 'accuracy': accuracy})\n\n    # fit the GridSearchCV instance\n    grid_search.fit(X, y)\n\n    # check that the best_index_ attribute is set correctly\n    assert_true(grid_search.best_index_ >= 0)\n\n    # check that the best_score_ attribute is set correctly\n    assert_true(grid_search.best_score_ >= 0)\n\n    # check that the best_params_ attribute is set correctly\n    assert_equal(grid_search.best_params_, {'estimator__max_depth': 5})\n\n    # check that the scorer_ attribute is set correctly\n    assert_equal(grid_search.scorer_, precision)\n\n    # check that the refit attribute is set correctly\n    assert_true(grid_search.refit)\n", "def test_gridsearchcv_best_index():\n    # Regression test for GH #1553\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=5,\n                              n_redundant=5, random_state=1)\n    param_grid = {'C': [0.1, 1, 10]}\n    est = LinearSVC()\n    search = GridSearchCV(estimator=est, param_grid=param_grid)\n    search.fit(X, y)\n    # Ensure that best_index_ matches the index of the best scoring estimator\n    # with a single split.\n    assert_equal(search.best_index_, np.argmax(search.cv_results_['mean_test_score']))\n", "    def test_init_with_refit_false(self):\n        \"\"\"Test that BaseSearchCV with refit=False does not refit the estimator\"\"\"\n        X, y = make_classification(n_samples=100, n_features=5, n_informative=3)\n        search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid={'max_depth': [1, 2]})\n        search.fit(X, y)\n        assert_true(not search.best_estimator_.tree_.node_count)\n", "def test_GridSearchCV_with_invalid_scoring_param():\n    clf = DecisionTreeClassifier()\n    param_grid = {'max_depth': [1, 2, 3]}\n    cv = StratifiedKFold(n_splits=2)\n    scoring = 'accuracy'\n\n    grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=cv,\n                       scoring=scoring)\n    assert_raise_message(ValueError, \"Unknown scorer: accuracy\", grid.fit, X, y)\n"], "sample_434": ["    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.view = TemplateView()\n", "    def test_view_is_async(self):\n        class TestView(View):\n                pass\n\n        self.assertFalse(TestView.view_is_async)\n\n        class AsyncTestView(View):\n            async def get(self, request):\n                pass\n\n        self.assertTrue(AsyncTestView.view_is_async)\n\n        class MixedTestView(View):\n                pass\n            async def head(self, request):\n                pass\n\n        with self.assertRaises(ImproperlyConfigured):\n            MixedTestView.view_is_async\n", "    def test_render_to_response_with_context(self):\n        view = TemplateView()\n        context = {\"title\": \"Test Title\"}\n        response = view.render_to_response(context)\n        self.assertIsInstance(response, TemplateResponse)\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.view = View()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n"], "sample_529": ["def test_get_window_extent_different_bboxes():\n    # Test that get_window_extent returns different bboxes for the legend and the\n    # frame.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.set_title('title')\n\n    with rc_context(rc={'legend.loc': 'best', 'legend.borderpad': 1.0}):\n        legend = ax.legend(title='legend title')\n\n    assert not legend.get_window_extent().equals(legend.get_frame().get_window_extent())\n", "    def test_draggable_legend_bbox_update(self):\n        fig, ax = plt.subplots()\n        line = ax.plot([1, 2, 3])[0]\n        ax.legend(handles=[line])\n\n        # Test that set_bbox_to_anchor works as expected\n        legend = ax.get_legend()\n        assert legend.get_bbox_to_anchor() is None\n\n        legend.set_bbox_to_anchor((0.5, 0.5))\n        assert legend.get_bbox_to_anchor().x0 == 0.5\n        assert legend.get_bbox_to_anchor().y0 == 0.5\n\n        # Test that set_bbox_to_anchor with a tuple works as expected\n        legend.set_bbox_to_anchor((0.5, 0.5, 0.5, 0.5))\n        assert legend.get_bbox_to_anchor().x0 == 0.5\n        assert legend.get_bbox_to_anchor().y0 == 0.5\n        assert legend.get_bbox_to_anchor().width == 0.5\n        assert legend.get_bbox_to_anchor().height == 0.5\n\n        # Test that set_bbox_to_anchor with a BboxBase works as expected\n        bbox = mtransforms.Bbox.from_bounds(0.2, 0.2, 0.5, 0.5)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().x0 == 0.2\n        assert legend.get_bbox_to_anchor().y0 == 0.2\n        assert legend.get_bbox_to_anchor().width == 0.5\n        assert legend.get_bbox_to_anchor().height == 0.5\n\n        # Test that the legend is updated when bbox_to_anchor is updated\n        legend.set_bbox_to_anchor((0.6, 0.6))\n        assert legend.get_bbox_to_anchor().x0 == 0.6\n        assert legend.get_bbox_to_anchor().y0 == 0.6\n\n        # Test that the legend is updated when bbox_to_anchor is updated with\n        # a BboxBase\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().x0 == 0.2\n        assert legend.get_bbox_to_anchor().y0 == 0.2\n        assert legend.get_bbox_to_anchor().width == 0.5\n        assert legend.get_bbox_to_anchor().height == 0.5\n", "def test_legend_handler_map_update(tmpdir, legend_handler_map):\n    # Create a figure with a collection of paths\n    fig, ax = plt.subplots()\n    patches = [mpatches.Rectangle((0.1, 0.1), 0.2, 0.2) for _ in range(5)]\n    collection = mcollections.PatchCollection(patches)\n    ax.add_patch(collection)\n\n    # Get the legend handler map from the Legend instance\n    legend_handler_map_before = mlegend.Legend(ax, [collection], [\"label\"]).get_legend_handler_map()\n\n    # Update the legend handler map\n    mlegend.Legend.set_default_handler_map(legend_handler_map)\n\n    # Get the legend handler map from the Legend instance after updating\n    legend_handler_map_after = mlegend.Legend(ax, [collection], [\"label\"]).get_legend_handler_map()\n\n    # Check that the updated legend handler map was propagated to the Legend instance\n    assert legend_handler_map_after == legend_handler_map\n\n    # Check that the Legend instance still has the correct default handler map\n    legend_handler_map_default = mlegend.Legend.get_default_handler_map()\n    assert legend_handler_map_default == {PathCollection: HandlerTuple()}\n\n    # Check that the Legend instance still has the custom handler map\n    assert legend_handler_map_before == {PathCollection: HandlerTuple()}\n\n    # Clean up\n    del legend_handler_map_before\n    del legend_handler_map_after\n    del legend_handler_map_default\n", "def testLegend_title_font_properties():\n    fig, axs = plt.subplots(2)\n    axs[0].plot([1, 2, 3])\n    axs[1].plot([4, 5, 6])\n    handles, labels, _, _ = mlegend._parse_legend_args(axs)\n    legend = mlegend.Legend(axs[1], handles, labels, loc='upper right')\n    with rc_context({'legend.title_fontsize': 10}):\n        legend.set_title('Test Legend')\n        prop = legend.get_title().get_fontproperties()\n        assert prop.get_size_in_points() == 10\n    legend.set_title('Test Legend', title_fontproperties={'size': 12})\n    prop = legend.get_title().get_fontproperties()\n    assert prop.get_size_in_points() == 12\n", "def test_custom_handler_map():\n    # Test that a custom handler map is used for a legend with\n    # multiple artists with different handler types.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], label='Line')\n    patch = mpatches.Rectangle((0.1, 0.1), 0.2, 0.2)\n    patch.set_label('Patch')\n    ax.add_patch(patch)\n    handles, labels = _get_legend_handles_labels([ax])\n\n    # Define a custom handler map that maps Line2D to a custom handler\n        return mlines.Line2D([0, 0], [0, 0], linestyle='--', color='red')\n    handler_map = {'Line2D': custom_handler}\n\n    with rc_context({'legend.fancybox': True}):\n        leg = mlegend.Legend(ax, handles, labels, handler_map=handler_map)\n        leg.draw(ax.figure.canvas.get_renderer())\n        leg_bbox = leg.get_window_extent()\n        assert leg_bbox.width > 10\n        assert leg_bbox.height > 10\n"], "sample_1145": ["def test_refine_simplify_pow():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    \n    assert refine(Pow(x, 3), Q.real(x)) == x**3\n    assert refine(Pow(x, Rational(1,2)), Q.real(x)) == x**(Rational(1,2))\n    assert refine(Pow(x, -1), Q.real(x)) == 1/x\n    assert refine(Pow(x, -2), Q.real(x)) == 1/x**2\n    \n    assert refine(Pow(x, 3), Q.positive(x)) == x**3\n    assert refine(Pow(x, Rational(1,2)), Q.positive(x)) == x**(Rational(1,2))\n    assert refine(Pow(x, -1), Q.positive(x)) == 1/x\n    assert refine(Pow(x, -2), Q.positive(x)) == 1/x**2\n    \n    assert refine(Pow(x, 3), Q.negative(x)) == -x**3\n    assert refine(Pow(x, Rational(1,2)), Q.negative(x)) == -x**(Rational(1,2))\n    assert refine(Pow(x, -1), Q.negative(x)) == -1/x\n    assert refine(Pow(x, -2), Q.negative(x)) == -1/x**2\n    \n    assert refine(Pow(x, 3), Q.zero(x)) == 0\n    assert refine(Pow(x, Rational(1,2)), Q.zero(x)) == 0\n    assert refine(Pow(x, -1), Q.zero(x)) == 0\n    assert refine(Pow(x, -2), Q.zero(x)) == 0\n    \n    assert refine(Pow(x, 3), Q.even(x)) == x**3\n    assert refine(Pow(x, Rational(1,2)), Q.even(x)) == abs(x)\n    assert refine(Pow(x, -1), Q.even(x)) == 1/x\n    assert refine(Pow(x, -2), Q.even(x)) == 1/x**2\n    \n    assert refine(Pow(x, 3), Q.odd(x)) == -x**3\n    assert refine(Pow(x, Rational(1,2)), Q.odd(x)) == -abs(x", "def test_refine_Pow_floating_point_exponent():\n    from sympy.assumptions.refine import refine_Pow\n    from sympy import Q, S, pi, exp, I\n    from sympy.abc import x\n\n    result = refine_Pow(S.Pi ** (2 + 3*I), Q.real(x))\n    assert result == exp(2 * pi * I)\n\n    result = refine_Pow(exp(2 * pi * I), Q.real(x))\n    assert result == exp(2 * pi * I)\n\n    result = refine_Pow(exp(2 * pi * I), Q.imaginary(x))\n    assert result == exp(2 * pi * I)\n\n    result = refine_Pow(S.Pi, Q.real(x))\n    assert result == S.Pi\n\n    result = refine_Pow(S.Pi, Q.imaginary(x))\n    assert result == S.Pi\n", "def test_refine_Pow_with_complex_numbers():\n    from sympy import I, Q, refine\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    result = refine((-1)**(x + I*y + 2*z), Q.real(x) & Q.real(y) & Q.real(z))\n    assert result == (-1)**(x + I*y + 2*z)\n\n    result = refine((-1)**(x + I*y + 2*z), Q.real(x) & Q.real(y) & Q.real(z) & Q.odd(z))\n    assert result == (-1)**(x + I*y)\n\n    result = refine((-1)**(x + I*y + 2*z), Q.real(x) & Q.real(y) & Q.real(z) & Q.even(z))\n    assert result == (-1)**(x + I*y + 2*z)\n\n    result = refine((-1)**(x + I*y + 2*z), Q.real(x) & Q.real(y) & Q.real(z) & Q.even(z) & Q.even(y))\n    assert result == (-1)**(x + 2*z)\n\n    result = refine((-1)**(x + I*y + 2*z), Q.real(x) & Q.real(y) & Q.real(z) & Q.even(z) & Q.odd(y))\n    assert result == (-1)**(x + 2*z + 1)\n", "def test_refine_sqrt():\n    from sympy import Q\n    w = Symbol('w', real=True)\n    expr = sqrt(w**2)\n    assert refine(expr, Q.positive(w)) == w\n    assert refine(expr, Q.real(w)) == Abs(w)\n", "def test_refine_Pow_exponent_is_Symbol():\n    from sympy import Q\n    from sympy.abc import x, y\n    from sympy.assumptions.refine import refine_Pow\n\n    # Test that refine_Pow does not change the expression when the exponent is not a number\n    expr = (-1)**x\n    expected = expr\n    assert refine_Pow(expr, Q.real(x)) == expected\n\n    # Test that refine_Pow does not change the expression when the exponent is a Symbol and the base is -1\n    expr = (-1)**(x + y)\n    expected = expr\n    assert refine_Pow(expr, Q.real(x)) == expected\n\n    # Test that refine_Pow simplifies the expression when the exponent is a Symbol and the base is -1 and the exponent is even\n    expr = (-1)**(2*x)\n    expected = (-1)**(2*x)\n    assert refine_Pow(expr, Q.real(x)) == expected\n\n    # Test that refine_Pow simplifies the expression when the exponent is a Symbol and the base is -1 and the exponent is odd\n    expr = (-1)**(2*x + 1)\n    expected = -1\n    assert refine_Pow(expr, Q.real(x)) == expected\n"], "sample_602": ["def test_open_dataset_engine_selection():\n    path = 'tests/data/*.nc'\n    path = str(_get_default_engine(path))\n\n    ds = xr.open_dataset(path)\n    assert ds is not None\n\n    ds = xr.open_dataset(path, engine='netcdf4')\n    assert ds is not None\n    assert ds.engine == 'netcdf4'\n\n    ds = xr.open_dataset(path, engine='scipy')\n    assert ds is not None\n    assert ds.engine == 'scipy'\n\n    ds = xr.open_dataset(path, engine='pydap')\n    assert ds is not None\n    assert ds.engine == 'pydap'\n\n    ds = xr.open_dataset(path, engine='h5netcdf')\n    assert ds is not None\n    assert ds.engine == 'h5netcdf'\n\n    ds = xr.open_dataset(path, engine='pynio')\n    assert ds is not None\n    assert ds.engine == 'pynio'\n\n    ds = xr.open_dataset(path, engine='cfgrib')\n    assert ds is not None\n    assert ds.engine == 'cfgrib'\n\n    ds = xr.open_dataset(path, engine='pseudonetcdf')\n    assert ds is not None\n    assert ds.engine == 'pseudonetcdf'\n\n    ds = xr.open_dataset(path, engine='zarr')\n    assert ds is not None\n    assert ds.engine == 'zarr'\n\n    try:\n        ds = xr.open_dataset(path, engine='invalid')\n    except ValueError as e:\n        assert str(e) == 'unrecognized engine for to_netcdf: invalid'\n", "def test_open_mfdataset_parallel():\n    # Generate a list of 10 datasets with 10 variables each\n    datasets = []\n    for i in range(10):\n        da = xr.DataArray(np.random.rand(10, 10), dims=['x', 'y'])\n        ds = xr.Dataset({'var%i' % i: da})\n        datasets.append(ds)\n\n    # Use open_mfdataset with parallel=True and concat_dim='y'\n    combined = xr.open_mfdataset(datasets, chunks=10, concat_dim='y', parallel=True)\n\n    # Check that the resulting Dataset has the correct variables\n    assert len(combined.variables) == 10\n\n    # Check that the variables have the correct shape\n    for var in combined.variables.values():\n        assert var.shape == (10, 10)\n\n    # Check that the resulting Dataset has the correct attributes\n    assert 'y' in combined.dims\n    assert 'x' in combined.dims\n", "def test_open_dataarray_from_netcdf4_gz_file():\n    ds = xr.Dataset(\n        {\n            \"var\": (\"x\", np.random.rand(10)),\n        },\n        coords={\"x\": np.arange(10)},\n    )\n    ds.to_netcdf(\"test.nc\")\n    ds.to_netcdf(\"test.nc.gz\", format=\"NETCDF3_64BIT\")\n    ds_gz = xr.open_dataarray(\"test.nc.gz\")\n    assert_identical(ds_gz, ds)\n", "def test_open_dataarray_with_mask_and_scale():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        data = np.ones((10, 10), dtype=np.float64)\n        da = xr.DataArray(data, dims=['x', 'y'])\n        da.attrs['scale_factor'] = 2.0\n        da.attrs['add_offset'] = 1.0\n        da.attrs['_FillValue'] = 0.0\n        da.to_netcdf(f'{tmpdir}/data.nc')\n\n        da_read = xr.open_dataarray(f'{tmpdir}/data.nc', mask_and_scale=True)\n        assert_identical(da_read.data, 2.0 * data + 1.0)\n\n        da_read = xr.open_dataarray(f'{tmpdir}/data.nc', mask_and_scale=False)\n        assert_identical(da_read.data, data)\n", "def test_open_dataset_remote_url():\n    from . import assert_identical\n\n    with assert_identical.temp_insecure_cache_dir():\n        ds = xr.open_dataset(\"https://cdo.uni-koeln.de/~cdo/test_data/ERA5_Accumulated_sfc_precipitation_1979-2019.nc\")\n        ds_2 = xr.open_dataset(\"https://cdo.uni-koeln.de/~cdo/test_data/ERA5_Accumulated_sfc_precipitation_1979-2019.nc\")\n        assert ds.equals(ds_2)\n"], "sample_1161": ["def test_Matrix_print():\n    M = MatrixSymbol('M', 2, 2)\n    Mslice = MatrixSlice(M, 1, 1, 0, 0)\n    Mbase = MatrixBase(M)\n\n    assert sstr(M) == 'MatrixSymbol(\"M\", 2, 2)'\n    assert sstr(Mslice) == 'MatrixSlice(%s, 1, 1, 0, 0)' % sstr(M)\n    assert sstr(Mbase) == 'MatrixBase(%s)' % sstr(M)\n    assert sstr(MatrixElement(M, 1, 2)) == '%s[1, 2]' % sstr(M)\n\n    with raises(TypeError):\n        sstr(1)\n", "    def test_print_tuple(self):\n        # Test _print_tuple and _print_Tuple are the same\n        assert StrPrinter().doprint(Tuple([S(1), 2, 3])) == \" (1, 2, 3)\"\n        assert StrPrinter().doprint(Tuple([S(1), 2, 3])) == StrPrinter().doprint((S(1), 2, 3))\n", "    def test_str_printer_integer_settings(self):\n        s = sstr(Integer(123), sympy_integers=True)\n        assert s == \"S(123)\"\n", "def test_StrPrinter():\n    p = StrPrinter()\n    assert p._print(FiniteSet(1, 2, 3)) == '{1, 2, 3}'\n    assert p._print(Basic) == 'Basic'\n    assert p._print(ConditionalExpression(x > 1, x, 0)) == 'ConditionalExpression(x > 1, x, 0)'\n    assert p._print(Floor(x)) == 'Floor(x)'\n    assert p._print(ceiling(x)) == 'ceiling(x)'\n    assert p._print(Interval(0, 1, left_open=True, right_open=True)) == 'Interval{.Lopen}({0}, {1})'\n    assert p._print(Interval(0, 1, left_open=False, right_open=True)) == 'Interval{.Ropen}({0}, {1})'\n    assert p._print(Interval(0, 1, left_open=True, right_open=False)) == 'Interval{.Lopen}({0}, {1})'\n    assert p._print(Interval(0, 1, left_open=False, right_open=False)) == 'Interval{m}({0}, {1})'\n    assert p._print(Rational(1, 2)) == '1/2'\n    assert p._print(S.pi) == 'pi'\n    assert p._print(sin(x)) == 'sin(x)'\n    assert p._print(sqrt(x)) == 'sqrt(x)'\n    assert p._print(Pow(x, Rational(1, 2))) == 'sqrt(x)'\n    assert p._print(Pow(x, Rational(1, 2), evaluate=False)) == 'x**(1/2)'\n    assert p._print(Exp(x)) == 'Exp(x)'\n    assert p._print(Exp(x), rational=True) == 'Exp(x)'\n    assert p._print(Exp(x), rational=False) == 'E**x'\n", "def test_print_Derivative():\n    from sympy.core import sympify\n    from sympy.core.function import diff\n    from sympy.testing.pytest import raises\n\n    expr = diff(x**2, x)\n    assert sstr(expr) == 'Derivative(2*x, x)'\n\n    expr = diff(x**2 + 3*x - 4, x)\n    assert sstr(expr) == 'Derivative(x**2 + 3*x - 4, x)'\n\n    expr = diff(x**2, x, 2)\n    assert sstr(expr) == 'Derivative(2*x, x, x)'\n\n    expr = diff(x**2, x, 3)\n    assert sstr(expr) == 'Derivative(2*x, x, x, x)'\n\n    # Test an expression with multiple variables\n    expr = diff(x*y, (x, y))\n    assert sstr(expr) == 'Derivative(y*x, (x, y))'\n\n    # Test an expression with multiple variables and order\n    expr = diff(x*y, (x, y), 2)\n    assert sstr(expr) == 'Derivative(y*x, (x, y, x, y))'\n\n    # Test an expression with multiple variables and order and w.r.t a specific variable\n    expr = diff(x*y, (x, y), order=2, variable=x)\n    assert sstr(expr) == 'Derivative(y*x, (x, x, x))'\n\n    # Test an expression with multiple variables and order and w.r.t a specific variable\n    # in the reverse order\n    expr = diff(x*y, (y, x), order=2, variable=y)\n    assert sstr(expr) == 'Derivative(x*y, (y, y, y))'\n\n    # Test an expression with multiple variables and order and w.r.t a specific variable\n    # with the point argument\n    expr = diff(x*y, (x, y), order=2, variable=x, point=0)\n    assert sstr(expr) == 'Derivative(y*x, (x, x, x), point=0)'\n\n    # Test an expression with multiple variables and order and w.r.t a specific variable\n    # with the point argument and reverse=True\n    expr = diff(x*y, (y, x), order=2, variable=x, point=0, reverse=True)\n   "], "sample_70": ["    def test_can_fast_delete_with_signals(self):\n        # Create some objects and add them to the collector\n        collector = Collector(using='default')\n        collector.collect([A(), A(), A()])\n\n        # Create a model with a pre_delete signal\n        class TestModel(models.Model):\n            pass\n\n        signals.pre_delete.connect(lambda sender, instance, **kwargs: None, sender=TestModel)\n\n        # Check that the objects can still be fast deleted\n        self.assertTrue(collector.can_fast_delete(A.objects.all()))\n\n        # Clean up\n        signals.pre_delete.disconnect(lambda sender, instance, **kwargs: None, sender=TestModel)\n", "    def test_related_objects(self):\n        \"\"\"Test that related objects are collected correctly.\"\"\"\n        parent = Parent.objects.create(name='parent')\n        child = Child.objects.create(parent=parent)\n        collector = Collector(using='default')\n        collector.collect([child], source=Parent, nullable=False)\n        self.assertIn(child, collector.data[Child])\n        self.assertIn(parent, collector.data[Parent])\n", "    def test_fast_delete_with_reverse_dependency(self):\n        # Create some objects\n        a = create_a()\n        r = self.DEFAULT\n        r.a = a\n        r.save()\n\n        # Create a Collector\n        collector = Collector(using=self.databases['default'])\n\n        # Add objects to the collector\n        collector.add([r])\n\n        # Check that the Collector can fast delete the objects\n        self.assertTrue(collector.can_fast_delete([r]))\n\n        # Check that the Collector can fast delete the objects even if they are related\n        # to other objects\n        collector.collect([r], source=None)\n        self.assertTrue(collector.can_fast_delete([r]))\n", "    def test_related_objects(self):\n        child = create_a()\n        parent = Parent.objects.create(name='parent', children=[child])\n        collector = Collector(using='default')\n        collector.collect([child], source=parent, nullable=False)\n        self.assertIn(parent, collector.data[Parent])\n        self.assertNotIn(child, collector.data[Parent])\n        self.assertEqual(len(collector.field_updates), 0)\n        collector.sort()\n        self.assertEqual(len(collector.data), 1)\n        self.assertIn(child, collector.data[Parent])\n        self.assertNotIn(parent, collector.data[Parent])\n", "    def test_can_fast_delete_false_for_non_nullable_relationships(self):\n        r = get_default_r()\n        c = r.c\n        with self.assertRaises(ValueError):\n            c.delete()\n        self.assertFalse(Collector(None).can_fast_delete([c]))\n"], "sample_811": ["def test_check_pairwise_arrays_accept_sparse():\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    Y = csr_matrix(np.array([[5, 6], [7, 8]]))\n    X, Y = check_pairwise_arrays(X, Y)\n    assert isinstance(X, csr_matrix)\n    assert isinstance(Y, csr_matrix)\n", "def test_check_pairwise_arrays():\n    # Test for check_pairwise_arrays with X and Y being equal\n    X = np.array([[1, 2], [3, 4]])\n    Y = X\n    X, Y = check_pairwise_arrays(X, Y)\n    assert_array_equal(X, Y)\n\n    # Test for check_pairwise_arrays with X and Y being unequal\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    X, Y = check_pairwise_arrays(X, Y)\n    assert_array_equal(X, X)\n    assert_array_equal(Y, Y)\n\n    # Test for check_pairwise_arrays with Y being None\n    X = np.array([[1, 2], [3, 4]])\n    X, Y = check_pairwise_arrays(X, None)\n    assert_array_equal(X, Y)\n\n    # Test for check_pairwise_arrays with X and Y having different shapes\n    X = np.array([[1, 2])\n    Y = np.array([[3, 4], [5, 6]])\n    with pytest.raises(ValueError):\n        check_pairwise_arrays(X, Y)\n\n    # Test for check_pairwise_arrays with X and Y having different numbers of features\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7]])\n    with pytest.raises(ValueError):\n        check_pairwise_arrays(X, Y)\n\n    # Test for check_pairwise_arrays with X and Y having different data types\n    X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n    Y = np.array([[5, 6], [7, 8]], dtype=np.float64)\n    with pytest.raises(TypeError):\n        check_pairwise_arrays(X, Y)\n\n    # Test for check_pairwise_arrays with sparse X and dense Y\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    Y = np.array([[5, 6], [7, 8]])\n    X, Y = check_pairwise_arrays(X, Y)\n    assert_array_equal(X.toarray(), Y)\n\n    # Test for check_pairwise_arrays with dense X and sparse Y\n    X = np.array([[1, 2], [3, 4]])\n    Y = csr_matrix(np.array([[5", "def test_pairwise_distances_argmin_min():\n    X = np.array([[0, 1], [1, 1]])\n    Y = np.array([[0, 1], [2, 1]])\n    distances, indices = pairwise_distances_argmin_min(X, Y)\n    assert_equal(distances[0], 0)\n    assert_equal(distances[1], 1)\n    assert_equal(indices[0], 0)\n    assert_equal(indices[1], 1)\n    assert_equal(pairwise_distances_argmin(X, Y), indices)\n", "def test_pairwise_kernels_allclose(metric):\n    X = np.random.rand(100, 10)\n    Y = np.random.rand(200, 10)\n    with config_context(working_memory=10):\n        K = pairwise_kernels(X, Y, metric=metric)\n        K_true = pairwise_kernels(X, Y, metric=metric, n_jobs=-1)\n    assert_allclose(K, K_true)\n", "def test_euclidean_distances():\n    X = np.array([[1., 2.], [3., 4.], [5., 6.]])\n    Y = np.array([[7., 8.], [9., 10.], [11., 12.]])\n    distances = euclidean_distances(X, Y)\n    expected_distances = np.array([[0., 5.196152422706632, 9.488934185740996],\n                                  [5.196152422706632, 0., 5.196152422706632],\n                                  [9.488934185740996, 5.196152422706632, 0.]])\n    assert_array_almost_equal(distances, expected_distances)\n"], "sample_483": ["    def test_check_admin_app(self):\n        # Test check_admin_app function\n        errors = check_admin_app([])\n        self.assertEqual(errors, [])\n", "    def test_check_fields(self):\n        # Test that fields only contains existing fields, doesn't contain duplicates\n        admin = ValidFields()\n        self.assertEqual(admin.check(), [])\n\n        # Test that fields is a list or tuple\n        class InvalidFields(admin.ModelAdmin):\n            fields = 'not a list'\n\n        self.assertEqual(\n            InvalidFields().check(),\n            [\n                checks.Error(\n                    \"The value of 'fields' must be a list or tuple.\",\n                    obj=InvalidFields.__class__,\n                    id=\"admin.E004\",\n                )\n            ],\n        )\n\n        # Test that fields contains duplicate field(s)\n        class DuplicateFields(admin.ModelAdmin):\n            fields = [\"title\", \"title\"]\n\n        self.assertEqual(\n            DuplicateFields().check(),\n            [\n                checks.Error(\n                    \"The value of 'fields' contains duplicate field(s).\",\n                    obj=DuplicateFields.__class__,\n                    id=\"admin.E006\",\n                )\n            ],\n        )\n\n        # Test that fields only contains field names that are listed on the model\n        class InvalidField(admin.ModelAdmin):\n            fields = [\"title\", \"invalid_field\"]\n\n        self.assertEqual(\n            InvalidField().check(),\n            [\n                refer_to_missing_field(\n                    field=\"invalid_field\",\n                    option=\"fields\",\n                    obj=InvalidField.__class__,\n                    id=\"admin.E006\",\n                )\n            ],\n        )\n", "    def test_check_admin_app(self):\n        with self.assertRaises(SystemExit):\n            check_admin_app(app_configs=[])\n", "    def test__check_fields_without_fields_set(self):\n        class MyAdmin(admin.ModelAdmin):\n            fields = None\n\n        check = BaseModelAdminChecks()\n        self.assertEqual(check.check(MyAdmin()), [])\n", "    def test_check_admin_app(self):\n        errors = check_admin_app(app_configs=[])\n        self.assertEqual(errors, [])\n"], "sample_10": ["    def test_has_masked_values(self, table_types):\n        t = self.t\n\n        t.masked = False\n        assert not t.has_masked_values\n\n        t.masked = True\n        t.add_column(t.a.mask)\n        assert t.has_masked_values\n\n        t.add_column(t.a)\n        assert not t.has_masked_values\n\n        t.masked = False\n        t.add_column(t.a)\n        t.add_column(t.a.mask)\n        assert not t.has_masked_values\n        t.masked = True\n        assert t.has_masked_values\n\n        t = self.t.copy()\n        t.add_column(t.a)\n        t.add_column(t.a.mask)\n        t.masked = False\n        assert not t.has_masked_values\n        t.masked = True\n        assert t.has_masked_values\n\n        t = self.t.copy()\n        t.add_column(t.a.mask)\n        t.add_column(t.a)\n        t.masked = False\n        assert not t.has_masked_values\n        t.masked = True\n        assert t.has_masked_values\n\n        t = self.t.copy()\n        t.add_column(t.a)\n        t.add_column(t.a)\n        t.masked = False\n        assert not t.has_masked_values\n        t.masked = True\n        assert t.has_masked_values\n", "    def test_set_table_column(self, table_types):\n        self._setup(table_types)\n        a = self.a\n        b = self.b\n        t = table_types()\n\n        # Test: Add column with name but no data\n        with pytest.warns(TableReplaceWarning):\n            t['d'] = self.d\n        assert t['d'].shape == (3,)\n        assert t['d'][0] == 7\n        assert t['d'][1] == 8\n        assert t['d'][2] == 7\n\n        # Test: Add column with name and data\n        c = self.c\n        t['e'] = c\n        assert t['e'].shape == (3,)\n        assert t['e'][0] == 7\n        assert t['e'][1] == 8\n        assert t['e'][2] == 9\n\n        # Test: Add column with name and data, but column already exists\n        with pytest.warns(TableReplaceWarning):\n            t['e'] = c\n        assert t['e'].shape == (3,)\n        assert t['e'][0] == 7\n        assert t['e'][1] == 8\n        assert t['e'][2] == 9\n\n        # Test: Add column with default name\n        t['f'] = self.c\n        assert t['f'].shape == (3,)\n        assert t['f'][0] == 7\n        assert t['f'][1] == 8\n        assert t['f'][2] == 9\n\n        # Test: Add column with default name, but column already exists\n        with pytest.warns(TableReplaceWarning):\n            t['f'] = c\n        assert t['f'].shape == (3,)\n        assert t['f'][0] == 7\n        assert t['f'][1] == 8\n        assert t['f'][2] == 9\n\n        # Test: Add column with index and name\n        t['g'] = self.d\n        assert t['g'].shape == (3,)\n        assert t['g'][0] == 7\n        assert t['g'][1] == 8\n        assert t['g'][2] == 7\n\n       ", "    def _setup(self, table_types):\n        super()._setup(table_types)\n", "    def test_settable_column_1(self, column_type):\n        self._setup(table_types=(Table, column_type))\n\n        # Test that setting a column name that doesn't exist raises a KeyError\n        t = self.t\n        with pytest.raises(KeyError):\n            t['e'] = self.a\n\n        # Test that setting a column with a different length raises an exception\n        with pytest.raises(ValueError):\n            t['a'] = self.b[:2]\n\n        # Test that setting a column with the same name replaces the existing one\n        t['a'] = self.d\n        assert (t['a'] == [7, 8, 7]).all()\n\n        # Test that setting a column with a different name doesn't replace the existing one\n        t['e'] = self.d\n        assert (t['e'] == [7, 8, 7]).all()\n\n        # Test that setting a column with a different name doesn't replace the existing one, even if the data has the same length\n        t['a'] = self.b\n        assert (t['a'] == [4, 5, 6]).all()\n        assert (t['e'] == [7, 8, 7]).all()\n\n        # Test that setting a column with a different name and length doesn't replace the existing one\n        t['a'] = self.b[:3]\n        assert (t['a'] == [4, 5, 6]).all()\n        assert (t['e'] == [7, 8, 7]).all()\n\n        # Test that setting a masked column with a different name replaces the existing one\n        t['a'] = MaskedColumn(self.d, name='a', mask=[True, False, True])\n        assert ((t['a'] == [np.ma.masked, 8, np.ma.masked]).all() and\n                (t['a'].mask == [True, False, True]).all())\n\n        # Test that setting a column with a different name and data type doesn't replace the existing one\n        t['a'] = self.obj\n        assert (t['a'] == [1, 'string', 3]).all()\n", "    def test_set_table_column(self, table_types, name, value):\n        self._setup(table_types)\n        self._table_type.__init__({'a': [1, 2, 3], 'b': [4, 5, 6]},\n                                  masked=self._table_type.masked,\n                                  names=['a', 'b'],\n                                  meta={'meta_key': 'meta_value'},\n                                  copy=self._table_type.copy,\n                                  rows=[[7, 8, 9], [10, 11, 12], [13, 14, 15]])\n        setattr(self._table_type, name, value)\n        assert getattr(self._table_type, name) == value\n"], "sample_717": ["def test_fetch_lfw_people_with_empty_folder():\n    \"\"\"Test that fetch_lfw_people raises an error when the data folder is empty\"\"\"\n    lfw_home, data_folder_path = check_fetch_lfw(data_home=SCIKIT_LEARN_EMPTY_DATA, download_if_missing=False)\n    with assert_raises(IOError):\n        fetch_lfw_people(data_home=SCIKIT_LEARN_EMPTY_DATA)\n\n", "def test_fetch_lfw_pairs_subset_not_in_label_filenames():\n    \"\"\"Test that an error is raised when subset is not in label_filenames\"\"\"\n    lfw_home, data_folder_path = check_fetch_lfw(data_home=SCIKIT_LEARN_EMPTY_DATA)\n    with assert_raises(ValueError):\n        fetch_lfw_pairs(subset='invalid_subset', data_home=SCIKIT_LEARN_EMPTY_DATA)\n", "def test_fetch_lfw_people_invalid_funneled():\n    \"\"\"Check that the data loader correctly handles invalid funneled parameter\"\"\"\n    # Set funneled to False and check that an IOError is raised\n    with assert_raises(IOError):\n        fetch_lfw_people(funneled=False)\n", "def test_fetch_lfw_people_scale_face():\n    \"\"\"Test the scale_face function\"\"\"\n    from sklearn.datasets import fetch_lfw_people\n    from numpy.testing import assert_array_almost_equal\n\n    # Setup\n    data_home = tempfile.mkdtemp(prefix=\"scikit_learn_lfw_test_\")\n    lfw_home, data_folder_path = fetch_lfw_people.check_fetch_lfw(\n        data_home=data_home, download_if_missing=False)\n\n    # Test\n    face = np.random.rand(250, 250)\n    scaled_face = fetch_lfw_people.scale_face(face)\n    expected_scaled_face = face - face.min()\n    expected_scaled_face /= expected_scaled_face.max()\n\n    # Assert\n    assert_array_almost_equal(scaled_face, expected_scaled_face)\n", "def test_fetch_lfw_people_target_names():\n    \"\"\"Check that the target names are correctly extracted from the folder paths\"\"\"\n    # Create a temporary directory\n    data_home = tempfile.mkdtemp(prefix=\"scikit_learn_lfw_test_\")\n    lfw_home = os.path.join(data_home, 'lfw_home')\n    os.makedirs(lfw_home)\n    # Create a person with name and multiple faces\n    person_name = 'John_Lee'\n    folder_path = os.path.join(lfw_home, \"lfw_funneled\", person_name)\n    os.makedirs(folder_path)\n    for i in range(5):\n        file_path = os.path.join(folder_path, person_name + '_%04d.jpg' % i)\n        with open(file_path, 'wb') as f:\n            f.write(b'Fake image data')\n    # Check that the target names are correctly extracted\n    people, target, target_names = fetch_lfw_people(data_home=data_home, funneled=True)\n    target_names_set = set(target_names)\n    expected_target_names = {person_name.replace('_', ' ')}\n    assert_equal(target_names_set, expected_target_names)\n\n"], "sample_140": ["    def test_decorator_called_with_variable_names(self):\n        @sensitive_variables('username', 'password')\n            return username, password\n\n        decorated_function = my_function\n        self.assertEqual(decorated_function.sensitive_variables, ('username', 'password'))\n", "    def test_sensitive_variables_decorator(self):\n        @sensitive_variables('user', 'password')\n            password = user.pass_word\n            self.assertEqual(password, 'my_password')\n\n        my_user = User()\n        my_user.pass_word = 'my_password'\n        my_function(my_user)\n\n        capturedOutput = StringIO()\n        sys.stdout = capturedOutput\n        try:\n            my_function(my_user)\n        except Exception as e:\n            self.assertIn('password', capturedOutput.getvalue())\n\n        capturedOutput = StringIO()\n        sys.stdout = capturedOutput\n        try:\n            my_function(my_user)\n        except Exception as e:\n            self.assertNotIn('my_password', capturedOutput.getvalue())\n", "    def test_sensitive_variables_without_any_variables(self):\n        @sensitive_variables()\n            pass\n\n        wrapper = my_function\n        self.assertEqual(wrapper.sensitive_variables, '__ALL__')\n", "    def test_sensitivity_checking_decorator_decorator_form(self):\n        @sensitive_variables('user')\n            return user\n\n        with self.assertWarns(UserWarning):\n            my_function(User())\n", "    def test_sensitive_variables_decorator_with_variable_names(self):\n        @sensitive_variables('user', 'password')\n            return f'Hello, {user} and {password}'\n\n        result = my_function(User())\n        self.assertIn('user', dir(my_function))\n        self.assertIn('password', dir(my_function))\n        self.assertNotIn('user', sensitive_variables_wrapper.sensitive_variables)\n        self.assertIn('password', sensitive_variables_wrapper.sensitive_variables)\n\n        with self.assertRaises(TypeError):\n            my_function = sensitive_variables('user', 'password')(my_function)\n"], "sample_971": ["def test_DisableWarningIsErrorFilter(tmpdir):\n    \"\"\"Test DisableWarningIsErrorFilter.\"\"\"\n    app = ParallelTasks(tmpdir)\n    logger = logging.getLogger('sphinx')\n    handler = logging.StreamHandler()\n\n    # Initialize logger\n    logger.setLevel(logging.WARNING)\n    logger.addHandler(handler)\n\n    # Test normal behavior\n    with skip_warningiserror(skip=False):\n        logger.warning('Normal warning')\n    handler.flush()\n    assert strip_escseq(handler.stream.getvalue()) == 'WARNING: Normal warning\\n'\n\n    # Test DisableWarningIsErrorFilter\n    disabler = DisableWarningIsErrorFilter()\n    handler.addFilter(disabler)\n    try:\n        with skip_warningiserror(skip=False):\n            logger.warning('Warning that should raise error')\n    except SphinxWarning as e:\n        assert e.message == 'Warning that should raise error'\n    else:\n        assert False, \"Expected exception not raised\"\n\n    # Test reset\n    handler.removeFilter(disabler)\n    with skip_warningiserror(skip=False):\n        logger.warning('Warning that should raise error')\n    handler.flush()\n    assert strip_escseq(handler.stream.getvalue()) == 'WARNING: Warning that should raise error\\n'\n", "def test_pending_warnings():\n    # Given\n    logger = logging.getLogger('test_pending_warnings')\n    logger.setLevel(logging.INFO)\n\n    # When\n    with pending_warnings() as memhandler:\n        logger.warning('Warning message!')\n\n    # Then\n    assert len(memhandler.buffer) == 1\n", "def test_convert_serializable():\n    record = logging.LogRecord(\n        name=\"test\",\n        level=logging.INFO,\n        pathname=\"path/to/file.py\",\n        lineno=10,\n        msg=\"This is a test message.\",\n        args=(),\n        exc_info=(None, None, None),\n        func=None,\n        created=1643723400.123456\n    )\n    record.location = \"path/to/file.py:10\"\n    convert_serializable([record])\n    assert record.msg == \"path/to/file.py:10: This is a test message.\"\n    assert record.args == ()\n", "def test_DisableWarningIsErrorFilter():\n    # Given\n    app = ParallelTasks()\n    logger = logging.getLogger('sphinx')\n    original_warningiserror = app.warningiserror\n    app.warningiserror = True\n    disabler = DisableWarningIsErrorFilter()\n\n    # When\n    with skip_warningiserror(skip=False) as _:\n        with prefixed_warnings(\"prefix:\"):\n            with logging.disable_warningiserror():\n                logger.warning('Warning message!')\n\n    # Then\n    assert not app.warningiserror\n    assert original_warningiserror  # original value preserved\n\n", "compilation error"], "sample_382": ["    def test_watch_for_template_changes_multiple_directories(self, mock_reset_loaders):\n        # Arrange\n        backend1 = mock.Mock()\n        backend2 = mock.Mock()\n        engines.all.return_value = [backend1, backend2]\n        backend1.engine.dirs = ['dir1']\n        backend2.engine.dirs = ['dir2']\n        backend1.engine.template_loaders = [mock.Mock()]\n        backend2.engine.template_loaders = [mock.Mock()]\n        backend1.engine.template_loaders[0].get_dirs.return_value = ['dir3']\n        backend2.engine.template_loaders[0].get_dirs.return_value = ['dir4']\n\n        # Act\n        watch_for_template_changes(None, None)\n\n        # Assert\n        mock_reset_loaders.assert_called_once()\n        backend1.watch_dir.assert_called_once_with(Path('dir1'), '**/*')\n        backend2.watch_dir.assert_called_once_with(Path('dir2'), '**/*')\n", "    def test_watch_for_template_changes(self, mock_receiver, mock_reset_loaders):\n        # Mock the autoreload_started event\n        autoreload_started.send = mock.Mock()\n        mock_receiver.return_value.__name__ = 'template_loaders_watch_changes'\n\n        # Get the list of template directories\n        template_dirs = get_template_directories()\n\n        # Mock the watch_dir method\n        autoreload.autoreload_config = mock.Mock()\n        autoreload.autoreload_config.watch_dir = mock.Mock()\n\n        # Call the watch_for_template_changes function\n        watch_for_template_changes(autoreload.autoreload_config)\n\n        # Check that the watch_dir method was called for each template directory\n        self.assertEqual(autoreload.autoreload_config.watch_dir.call_count, len(template_dirs))\n        for dir in template_dirs:\n            autoreload.autoreload_config.watch_dir.assert_any_call(dir, '**/*')\n", "    def test_reset_loaders_called_once_when_template_changed(self, reset_loaders_mock):\n        # Arrange\n        file_path = EXTRA_TEMPLATES_DIR / 'example.txt'\n        file_path.write_text('new content')\n\n        # Act\n        template_changed(sender=None, file_path=file_path)\n\n        # Assert\n        reset_loaders_mock.assert_called_once()\n", "    def test_watch_for_template_changes_disconnects_listener(self, disconnect, reset_loaders):\n        # Test that the template changes listener is disconnected when a new app is installed\n        # and the existing app is uninstalled.\n        self.assertEqual(disconnect.call_count, 0)\n        with override_settings(INSTALLED_APPS=['template_tests', 'new_app']):\n            watch_for_template_changes(sender=autoreload, **kwargs={})\n        self.assertEqual(disconnect.call_count, 1)\n        self.assertEqual(reset_loaders.call_count, 0)\n\n        # Test that the template changes listener is re-connected when the existing app is re-installed.\n        watch_for_template_changes(sender=autoreload, **kwargs={})\n        self.assertEqual(disconnect.call_count, 1)\n        with override_settings(INSTALLED_APPS=['template_tests']):\n            watch_for_template_changes(sender=autoreload, **kwargs={})\n        self.assertEqual(disconnect.call_count, 2)\n        self.assertEqual(reset_loaders.call_count, 1)\n", "def test_watch_for_template_changes(self, mock_watch_for_template_changes):\n    \"\"\"Test that watch_for_template_changes correctly watches for all template directories\"\"\"\n    # Add some existing directories to the test environment\n    existing_directories = [str(EXTRA_TEMPLATES_DIR), str(EXTRA_TEMPLATES_DIR / 'subdir')]\n    for directory in existing_directories:\n        Path(directory).mkdir(parents=True, exist_ok=True)\n\n    # Verify that watch_for_template_changes is called with the existing directories\n    get_template_directories_mock = mock.patch('template_tests.get_template_directories')\n    get_template_directories_mock.return_value = set(existing_directories)\n\n    # Call the watch_for_template_changes function\n    watch_for_template_changes(autoreload_started())\n\n    # Assert that watch_for_template_changes was called with the correct directories\n    get_template_directories_mock.assert_called_once()\n    mock_watch_for_template_changes.assert_called_once_with(autoreload_started(), **{})\n"], "sample_642": ["def test_preprocess_options_argument_preprocessing_error_when_invalid_option_and_value_provided(\n    capsys: CaptureFixture,", "def test_preprocess_options(\n    capsys: CaptureFixture,\n    run: Run,\n    option: str,\n    value: str | None,\n    expected_error: type[BaseException] | None,\n    expected_error_message: str | None,", "def test_preprocess_options(args, expected_output):\n    with fake_home():\n        run = Run()\n        run._rcfile = None\n        run._plugins = []\n        run._output = None\n        processed_args = _preprocess_options(run, args)\n        assert processed_args == expected_output\n", "def test_preprocess_options_error_message(tmp_path: Path) -> None:\n    \"\"\"Test that an error message is properly raised when an option expects a value and none is given.\"\"\"\n    with tempdir():\n        run = Run()\n        with pytest.raises(ArgumentPreprocessingError) as e:\n            _preprocess_options(run, [\"--init-hook\"])\n        assert str(e.value) == \"Option init-hook expects a value\"\n", "def test_parse_rich_type_value(tmp_path: Path) -> Iterator[None]:\n    \"\"\"Test _parse_rich_type_value.\"\"\"\n    file_path = tmp_path / \"test.toml\"\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"[type]\\nint = int\\nstr = str\\nlist = list\\n\")\n        f.write(\"[types]\\ntuple = tuple\\nset = set\\n\")\n\n    with fake_home():\n        with testutils.pylint_run() as run:\n            config.parse_config_from_file(file_path)\n            with run.linter.disable_checks(\"import-self\"):\n                run.checker.check(None)\n                for arg in run.checker.get_args():\n                    if arg.arg_name == \"type\":\n                        assert _parse_rich_type_value(arg.arg_value) == \"int,str,list\"\n                    elif arg.arg_name == \"types\":\n                        assert _parse_rich_type_value(arg.arg_value) == \"tuple,set\"\n\n    file_path = tmp_path / \"test2.toml\"\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"[type]\\nint = int\\nstr = str\\nlist = list\\n\")\n        f.write(\"[types]\\ntuple = tuple\\nset = set\\nfoo = bar\")\n\n    with fake_home():\n        with testutils.pylint_run() as run:\n            config.parse_config_from_file(file_path)\n            with run.linter.disable_checks(\"import-self\"):\n                run.checker.check(None)\n                for arg in run.checker.get_args():\n                    if arg.arg_name == \"type\":\n                        assert _parse_rich_type_value(arg.arg_value) == \"int,str,list\"\n                    elif arg.arg_name == \"types\":\n                        assert _parse_rich_type_value(arg.arg_value) == \"tuple,set,foo:bar\"\n\n    file_path = tmp_path / \"test3.toml\"\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"[type]\\nint = int\\nstr = str\\nlist = list\\n\")\n        f.write(\"[types]\\ntuple = tuple\\nset = set\\n\")\n        f.write(\"[sub]\\ntuple = tuple\\nset = set\\n\")\n\n    with fake_home():\n        with testutils.pylint_run() as run:\n            config.parse_config_from_file(file_path)\n            with run.linter.disable_checks(\"import-self\"):\n                run"], "sample_420": ["    def test_meta_options_fields_for_model(self):\n        form = modelform_factory(model=Author, form=ModelForm)\n        self.assertEqual(\n            form._meta.fields,\n            {\n                \"name\": forms.CharField(),\n                \"born\": forms.DateField(),\n                \"deceased\": forms.DateField(),\n            },\n        )\n        form = modelform_factory(model=Author, form=ModelForm, fields=[\"name\"])\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField()})\n        form = modelform_factory(\n            model=Author,\n            form=ModelForm,\n            fields=[\"name\"],\n            exclude=[\"born\"],\n        )\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField()})\n        form = modelform_factory(\n            model=Author,\n            form=ModelForm,\n            exclude=[\"name\"],\n        )\n        self.assertEqual(form._meta.fields, {\"born\": forms.DateField(), \"deceased\": forms.DateField()})\n        form = modelform_factory(\n            model=Author,\n            form=ModelForm,\n            fields=[\"name\", \"deceased\"],\n            exclude=[\"born\"],\n        )\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField(), \"deceased\": forms.DateField()})\n        form = modelform_factory(\n            model=Author,\n            form=ModelForm,\n            fields=[\"born\", \"deceased\"],\n            exclude=[\"name\"],\n        )\n        self.assertEqual(form._meta.fields, {\"born\": forms.DateField(), \"deceased\": forms.DateField()})\n        form = modelform_factory(model=Author, form=ModelForm, fields=[\"name\", \"born\"])\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField(), \"born\": forms.DateField()})\n        form = modelform_factory(model=Author, form=ModelForm, fields=[\"name\", \"deceased\"])\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField(), \"deceased\": forms.DateField()})\n        form = modelform_factory(model=Author, form=ModelForm, fields=[\"name\"])\n        self.assertEqual(form._meta.fields, {\"name\": forms.CharField()})\n        form = modelform_factory(model=Author, form=ModelForm)\n        self.assertEqual(\n            form._meta.fields,\n            {\n                \"name\": forms.CharField(),\n                \"born\": forms.DateField(),\n                \"deceased\": forms.DateField(),\n            },\n        )\n", "    def test_fields_for_model_fails_with_invalid_field_name(self):\n        class ModelForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = (\"title\", \"author\")\n\n        with self.assertRaises(FieldError):\n            fields_for_model(Book, fields=(\"title\", \"not_a_field\"))\n", "    def test_exclude(self):\n        form = ModelFormWithExclude(\n            model=TextFile,\n            fields=[\"file\"],\n            exclude=[\"name\"],\n        )\n        self.assertEqual(\n            form.fields,\n            {\"file\": forms.FileField(widget=forms.FileInput(attrs={\"id\": \"id_file\"}))},\n        )\n", "    def test_invalid_choice_with_required_field(self):\n        class TestModel(models.Model):\n            test_field = models.CharField()\n\n        field = forms.ModelChoiceField(TestModel.objects.all())\n        field.required = True\n\n        form = forms.Form({\"test_field\": \"invalid_choice\"})\n        with self.assertRaises(ValidationError):\n            field.clean(form.data[\"test_field\"])\n", "    def test_fields_for_model_applying_limit_choices_to(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n            value = models.IntegerField(choices=[(1, 'Option 1'), (2, 'Option 2')])\n\n            class Meta:\n                managed = False\n                default_related_name = 'myapp'\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = MyModel\n                formfield_callback = lambda f, **kwargs: forms.CharField()\n\n        model_field = MyModel._meta.get_field('value')\n        form_field = MyForm._meta.fields['value']\n\n        self.assertEqual(form_field.limit_choices_to, None)\n        self.assertNotEqual(model_field.limit_choices_to, None)\n\n        fields_for_model = fields_for_model(MyModel, formfield_callback=lambda f, **kwargs: forms.CharField())\n        self.assertEqual(fields_for_model['value'].limit_choices_to, None)\n\n        fields_for_model = fields_for_model(MyModel, apply_limit_choices_to=False)\n        self.assertEqual(fields_for_model['value'].limit_choices_to, model_field.limit_choices_to)\n"], "sample_31": ["def test_write_latex_cosmology_in_meta_false_on_no_meta_table_error(self, cosmo):\n    cosmo.__parameters__ = []\n    with pytest.raises(ValueError) as excinfo:\n        write_latex(cosmo, \"test_latex_table.tex\", cls=QTable, cosmology_in_meta=True)\n    assert str(excinfo.value) == \"cosmology_in_meta must be True when passing in a table with metadata\"\n", "    def cosmology(self):\n        from astropy.cosmology import Planck15\n        return Planck15\n", "def test_latex_table_class(self, cosmology, tmp_path):\n    \"\"\"Test that we can specify a custom table class.\"\"\"\n    table_class = self.table_class\n    write_latex(cosmology, tmp_path / \"output.tex\", cls=table_class)\n    table = table_class.read(tmp_path / \"output.tex\")\n    assert table.dtype.names == cosmology.__parameters__\n", "def test_latex_names_false_without_unit_conversion(self):\n    \"\"\"\n    Test that the parameter names are not converted to LaTeX names when `latex_names=False`\n    and there is no unit conversion.\n    \"\"\"\n    class MyCosmology(Cosmology):\n        H0 = Parameter(\"H0\", value=67.4, unit=u.km / (u.Mpc * u.s))\n        Om0 = Parameter(\"Om0\", value=0.315)\n\n    table = to_table(MyCosmology(), cls=QTable, cosmology_in_meta=False)\n    write_latex(MyCosmology(), file=None, latex_names=False)\n\n    assert table.colnames == ['H0', 'Om0']\n", "def test_write_latex_latex_names_false(self):\n    \"\"\"Test that write_latex does not convert parameter names when latex_names is False.\"\"\"\n    cosmo = Cosmology(name='Test', H0=67.4, Om0=0.315, Ode0=0.685)\n    with captured_stdout() as stdout:\n        write_latex(cosmo, \"test.tex\", latex_names=False)\n    table = read_latex(\"test.tex\")\n    assert stdout.getvalue() == \"\"\n    assert table.colnames == cosmo.__parameters__\n    assert \"H0\" in table.colnames\n    assert \"Om0\" in table.colnames\n    assert \"Ode0\" in table.colnames\n"], "sample_64": ["    def test_httpresponse_content_type(self):\n        response = HttpResponse(content_type='text/plain')\n        self.assertEqual(response['Content-Type'], 'text/plain')\n", "    def test_set_cookie_with_unicode_key(self):\n        response = HttpResponse()\n        response.set_cookie('\u00e4bcd\u00e9f', 'hello')\n        self.assertEqual(response.cookies['\u00e4bcd\u00e9f']['value'], 'hello')\n        self.assertEqual(response.cookies['\u00e4bcd\u00e9f']['expires'], '')\n", "    def test_HTTPResponse_init(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        self.assertEqual(response.charset, settings.DEFAULT_CHARSET)\n        self.assertEqual(response.content, b'')\n", "    def test_http_response_repr(self):\n        response = HttpResponse('Content')\n        self.assertIn('status_code=200', str(response))\n        self.assertIn('content_type=\"text/html; charset=%s\"' % settings.DEFAULT_CHARSET, str(response))\n", "    def test_set_cookie_with_invalid_max_age(self):\n        response = HttpResponse()\n        with self.assertRaises(ValueError):\n            response.set_cookie('test', max_age=-1)\n"], "sample_694": ["def test_check_ispytest():\n    with warnings.catch_warnings(record=True):\n        check_ispytest(True)\n        assert not any(warning in sys.warnlist for warning in [PRIVATE])\n        with pytest.warns(PytestDeprecationWarning):\n            check_ispytest(False)\n        assert any(warning in sys.warnlist for warning in [PRIVATE])\n", "def test_check_ispytest():\n    \"\"\"Test check_ispytest function.\"\"\"\n    \n    # Test case when _ispytest is set to True\n    with warnings.catch_warnings(record=True) as w:\n        check_ispytest(True)\n        assert len(w) == 0\n    \n    # Test case when _ispytest is set to False\n    with warnings.catch_warnings(record=True) as w:\n        check_ispytest(False)\n        assert len(w) == 1\n        assert issubclass(w[0].category, PytestDeprecationWarning)\n        assert str(w[0].message).startswith(\"A private pytest class or function was used.\")\n", "def test_check_ispytest():\n    \"\"\"Test that check_ispytest issues a warning when pytest is not True.\"\"\"\n    # Arrange\n    with warnings.catch_warnings(record=True) as caught_warnings:\n        # Act\n        check_ispytest(False)\n\n    # Assert\n    assert len(caught_warnings.listwarnings) == 1\n    assert isinstance(caught_warnings.listwarnings[0].message.category, PytestDeprecationWarning)\n    assert \"A private pytest class or function was used.\" in str(caught_warnings.listwarnings[0].message)\n\n", "def test_check_ispytest():\n    with pytest.warns(PytestDeprecationWarning) as record:\n        check_ispytest(False)\n        assert len(record.list) == 1\n        assert record.list[0].category is PytestDeprecationWarning\n", "def test_check_ispytest():\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", PytestDeprecationWarning)\n        check_ispytest(False)\n        assert len(warnings.warnlist) == 1\n        warning = warnings.warnlist[0]\n        assert isinstance(warning.message, PytestDeprecationWarning)\n"], "sample_159": ["    def test_check_user_model_username_field_is_not_none(self):\n        with self.assertRaises(checks.Error):\n            check_user_model()\n", "    def test_check_user_model_non_unique_username(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n", "    def test_check_user_model(self):\n        # Test that check_user_model does not return any errors for a valid user model.\n        errors = check_user_model(self.apps.get_model('auth_tests', 'CustomUserUniqueUsername'))\n        self.assertEqual(errors, [])\n", "    def test_check_models_permissions(self):\n        # Test case for check_models_permissions\n        # Test builtin permission name length\n        CustomUserNonUniqueUsername._meta.permissions = [\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername'),\n        ]\n        errors = check_models_permissions(apps=get_app_configs())\n        self.assertEqual(len(errors), 0)\n\n        # Test builtin permission codename length\n        CustomUserNonUniqueUsername._meta.permissions = [\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername12345678901234567890'),\n        ]\n        errors = check_models_permissions(apps=get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'auth.E007')\n\n        # Test custom permission name length\n        CustomUserNonUniqueUsername._meta.permissions = [\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername12345678901234567890'),\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername'),\n        ]\n        errors = check_models_permissions(apps=get_app_configs())\n        self.assertEqual(len(errors), 2)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'auth.E008')\n\n        # Test custom permission codename length\n        CustomUserNonUniqueUsername._meta.permissions = [\n            ('view_customusernonuniqueusername123', 'View CustomUserNonUniqueUsername'),\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername'),\n        ]\n        errors = check_models_permissions(apps=get_app_configs())\n        self.assertEqual(len(errors), 2)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, 'auth.E012')\n\n        # Test custom permissions codename clashing\n        CustomUserNonUniqueUsername._meta.permissions = [\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername'),\n            ('view_customusernonuniqueusername', 'View CustomUserNonUniqueUsername'),\n        ]\n        errors = check_models_permissions(apps=get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Error)\n        self.assertEqual(errors[0].id, '", "    def test_is_anonymous_and_is_authenticated_properties(self):\n        # Ensure that is_anonymous and is_authenticated are properties\n        from auth_tests.models import CustomUserNonUniqueUsername\n        user = CustomUserNonUniqueUsername.objects.create()\n        self.assertTrue(isinstance(user.is_anonymous, property))\n        self.assertTrue(isinstance(user.is_authenticated, property))\n"], "sample_1082": ["def test_reciprocal_hyperbolic_functions():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    # Test csch\n    assert csch(0).is_positive\n    assert not csch(0).is_negative\n    assert csch(0).is_finite\n    assert csch(0).is_real\n    assert csch(0)._eval_conjugate().is_real\n\n    # Test sech\n    assert sech(0).is_positive\n    assert not sech(0).is_negative\n    assert sech(0).is_finite\n    assert sech(0).is_real\n    assert sech(0)._eval_conjugate().is_real\n\n    # Test asinh\n    assert asinh(0).is_positive\n    assert not asinh(0).is_negative\n    assert asinh(0).is_finite\n    assert not asinh(0).is_real\n    assert asinh(0)._eval_conjugate().is_real\n\n    # Test acosh\n    assert acosh(1).is_positive\n    assert not acosh(1).is_negative\n    assert acosh(1).is_finite\n    assert acosh(1).is_real\n    assert acosh(1)._eval_conjugate().is_real\n\n    # Test atanh\n    assert atanh(0).is_positive\n    assert not atanh(0).is_negative\n    assert atanh(0).is_finite\n    assert not atanh(0).is_real\n    assert atanh(0)._eval_conjugate().is_real\n\n    # Test acoth\n    assert acoth(1).is_positive\n    assert not acoth(1).is_negative\n    assert acoth(1).is_finite\n    assert acoth(1).is_real\n    assert acoth(1)._eval_conjugate().is_real\n\n    # Test asech\n    assert asech(1).is_positive\n    assert not asech(1).is_negative\n    assert asech(1).is_finite\n    assert asech(1).is_real\n    assert asech(1)._eval_conjugate().is_real\n\n    # Test acsch\n    assert acsch(1).is_positive\n    assert not acsch(1).is_negative\n    assert ac", "def test_asinh_eval():\n    from sympy import symbols, sinh\n    x = symbols('x')\n\n    # Test case 1: Evaluating asinh(0)\n    assert asinh(0) == 0\n\n    # Test case 2: Evaluating asinh(1)\n    assert asinh(1) == log(sqrt(2) + 1)\n\n    # Test case 3: Evaluating asinh(-1)\n    assert asinh(-1) == log(sqrt(2) - 1)\n\n    # Test case 4: Evaluating asinh(x) where x is a symbolic variable\n    assert asinh(x) == x\n\n    # Test case 5: Evaluating asinh(sinh(x)) where x is a symbolic variable\n    assert asinh(sinh(x)) == x\n\n    # Test case 6: Evaluating asinh(-sinh(x)) where x is a symbolic variable\n    assert asinh(-sinh(x)) == -x\n\n    # Test case 7: Evaluating asinh(sinh(x) + sinh(y)) where x and y are symbolic variables\n    assert asinh(sinh(x) + sinh(y)) == x + y\n", "def test_atanh_is_extended_real():\n    atanh_val = atanh(1 + I)\n    assert atanh_val.is_extended_real\n    atanh_val = atanh(0)\n    assert not atanh_val.is_extended_real\n    atanh_val = atanh(sqrt(2) + sqrt(2)*I)\n    assert atanh_val.is_extended_real\n", "def test_acsch_eval():\n    # Check if acsch returns the correct result when the argument is a complex number\n    x = Symbol('x')\n    assert acsch(x).args == (x,), \"acsch should return an instance of InverseHyperbolicFunction with one argument\"\n    assert acsch(sinh(x)).args[0] == x, \"acsch(sinh(x)) should return x\"\n    # Check if acsch returns the correct result when the argument is a real number\n    assert acsch(2).is_real, \"acsch(2) should be a real number\"\n    assert acsch(-2).is_real, \"acsch(-2) should be a real number\"\n    # Check if acsch returns the correct result when the argument is an expression with a complex number\n    x = Symbol('x')\n    assert acsch(x + I * pi).args[0] == x + I * pi, \"acsch(x + I*pi) should return x + I*pi\"\n    # Check if acsch returns the correct result when the argument is an expression with a non-integer power\n    assert acsch(x**Rational(1, 2)).args[0] == x**Rational(1, 2), \"acsch(x**Rational(1, 2)) should return x**Rational(1, 2)\"\n    # Check if acsch returns the correct result when the argument is an expression with a negative number\n    assert acsch(-x).args[0] == -x, \"acsch(-x) should return -x\"\n    # Check if acsch returns the correct result when the argument is an expression with a zero\n    assert acsch(0).args[0] == 0, \"acsch(0) should return 0\"\n    # Check if acsch returns the correct result when the argument is an expression with a positive infinity\n    assert acsch(oo).args[0] == oo, \"acsch(oo) should return oo\"\n    # Check if acsch returns the correct result when the argument is an expression with a negative infinity\n    assert acsch(-oo).args[0] == -oo, \"acsch(-oo) should return -oo\"\n", "def test_hyperbolic_tanh_expansion():\n    x = symbols('x')\n    tanh_expansion = tanh(x).as_expr().expand()\n    tanh_expansion_diff = diff(tanh_expansion, x)\n    tanh_expansion_diff_simplified = tanh_expansion_diff.simplify()\n    tanh_expansion_diff_poly = Poly(tanh_expansion_diff_simplified, x)\n    assert tanh_expansion_diff_poly.as_expr() == tanh_expansion_diff_simplified\n    assert tanh_expansion_diff_poly.is_polynomial()\n    assert tanh_expansion_diff_poly.as_expr().is_real\n"], "sample_848": ["def test_MultiOutputRegressor_fit():\n    # Test MultiOutputRegressor with different base estimators\n    X, y = datasets.make_regression(n_samples=100, n_features=10, n_targets=3, random_state=0)\n    base_estimators = [LinearRegression(), Ridge(), SGDRegressor()]\n\n    for base_estimator in base_estimators:\n        multioutput_regressor = MultiOutputRegressor(base_estimator, n_jobs=-1)\n        multioutput_regressor.fit(X, y)\n        for i in range(y.shape[1]):\n            assert_array_almost_equal(multioutput_regressor.estimators_[i].coef_, base_estimator.coef_)\n            assert_array_almost_equal(multioutput_regressor.estimators_[i].intercept_, base_estimator.intercept_)\n", "def test_MultiOutputRegressor_partial_fit_sample_weight():\n    X, y = datasets.make_regression(n_samples=100, n_features=5, n_targets=3, random_state=0)\n    regr = MultiOutputRegressor(LinearRegression())\n    regr.partial_fit(X[:50], y[:50])\n    regr.partial_fit(X[50:], y[50:], sample_weight=[0.5, 0.3, 0.2])\n    y_pred = regr.predict(X)\n    assert isinstance(y_pred, np.ndarray)\n    assert y_pred.shape == (100, 3)\n    assert all(s > 0 for s in regr.estimators_[0].coef_)\n    assert all(s > 0 for s in regr.estimators_[1].coef_)\n    assert all(s > 0 for s in regr.estimators_[2].coef_)\n", "def test_MultiOutputRegressor_partial_fit():\n    \"\"\"Test partial_fit on MultiOutputRegressor.\"\"\"\n    X, y = datasets.make_regression(n_samples=10, n_features=5, n_targets=3, random_state=0)\n    estimator = Lasso(alpha=0.1)\n    for i in range(y.shape[1]):\n        estimator.fit(X, y[:, i])\n    multi_output_regressor = MultiOutputRegressor(estimator)\n    multi_output_regressor.fit(X, y)\n    assert_array_almost_equal(multi_output_regressor.predict(X), multi_output_regressor.estimators_[0].predict(X))\n\n    multi_output_regressor.partial_fit(X, y)\n    assert_array_almost_equal(multi_output_regressor.predict(X), multi_output_regressor.estimators_[0].predict(X))\n\n    y_new = np.zeros((X.shape[0], y.shape[1]))\n    y_new[:, 2] = np.random.randn(X.shape[0])\n    multi_output_regressor.partial_fit(X, y_new)\n    assert_array_almost_equal(multi_output_regressor.predict(X), multi_output_regressor.estimators_[2].predict(X))\n", "def test_MultiOutputRegressor_n_jobs():\n    # Test MultiOutputRegressor with n_jobs\n    X, Y = datasets.make_regression(n_samples=100, n_features=10, n_targets=5)\n    estimator = Lasso()\n    m = MultiOutputRegressor(estimator, n_jobs=1)\n    m.fit(X, Y)\n    m.fit(X, Y)\n    m.fit(X, Y)\n    m.partial_fit(X, Y)\n    m.partial_fit(X, Y)\n    m.partial_fit(X, Y)\n", "def test_base_class_instantiation():\n    # Test that MultiOutputRegressor can be instantiated with an\n    # unsuitable estimator\n    with assert_raises(ValueError):\n        MultiOutputRegressor(estimator=\"unsuitable_estimator\")\n\n    # Test that MultiOutputRegressor can be instantiated with an estimator\n    # that has no fit method\n    with assert_raises(ValueError):\n        class UnsuitableEstimator:\n                pass\n\n        MultiOutputRegressor(estimator=UnsuitableEstimator())\n\n    # Test that MultiOutputRegressor can be instantiated with an estimator\n    # that has a fit method but no predict method\n    class UnsuitableEstimator:\n            pass\n\n    with assert_raises(ValueError):\n        MultiOutputRegressor(estimator=UnsuitableEstimator())\n\n    # Test that MultiOutputRegressor can be instantiated with an estimator\n    # that has a fit method and a predict method\n    class SuitableEstimator:\n            pass\n\n            pass\n\n    MultiOutputRegressor(estimator=SuitableEstimator())\n\n    # Test that MultiOutputRegressor can be instantiated with an estimator\n    # that has a fit method and a predict method and is a ClassifierMixin\n    class SuitableClassifierEstimator(ClassifierMixin):\n            pass\n\n            pass\n\n    with assert_raises(ValueError):\n        MultiOutputRegressor(estimator=SuitableClassifierEstimator())\n\n    # Test that MultiOutputRegressor can be instantiated with an estimator\n    # that has a fit method and a predict method and is a RegressorMixin\n    class SuitableRegressorEstimator(RegressorMixin):\n            pass\n\n            pass\n\n    MultiOutputRegressor(estimator=SuitableRegressorEstimator())\n"], "sample_473": ["    def setUp(self):\n        super().setUp()\n        request_started.disconnect(close_old_connections)\n", "    def test_create_request(self):\n        scope = {\"method\": \"GET\", \"path\": \"/test/\", \"headers\": [], \"query_string\": \"\"}\n        body_file = tempfile.TemporaryFile()\n        handler = ASGIHandler()\n        request, error_response = handler.create_request(scope, body_file)\n        self.assertIsInstance(request, ASGIRequest)\n        self.assertIsNone(error_response)\n", "    def test_asgi_handler_init(self):\n        handler = ASGIHandler()\n        self.assertIsInstance(handler.request_class, ASGIRequest)\n", "    def setUp(self):\n        request_started.disconnect(close_old_connections)\n", "    def test_get_scheme(self):\n        request = self.request_factory.get('/test', HTTP_HOST='example.com')\n        handler = self.handler_class()\n        self.assertEqual(handler.get_scheme(request), 'http')\n\n        request.META['HTTP_X_FORWARDED_PROTO'] = 'https'\n        self.assertEqual(handler.get_scheme(request), 'https')\n"], "sample_745": ["def test_check_inverse_transform_with_check_inverse_set_to_false():\n    \"\"\"Check that check_inverse_transform does not raise a warning when check_inverse is False.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    func = lambda x: x ** 2\n    inverse_func = lambda x: np.sqrt(x)\n\n    transformer = FunctionTransformer(func, inverse_func, check_inverse=False)\n    with assert_no_warnings():\n        transformer._check_inverse_transform(X)\n", "def test_pass_y():\n    # Test that the pass_y parameter is properly handled\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    args_store = []\n    kwargs_store = {}\n    func = _make_func(args_store, kwargs_store)\n\n    transformer = FunctionTransformer(pass_y=True, func=func)\n    transformer.fit(X, y)\n\n    transformer.transform(X, y)\n\n    assert len(args_store) == 2\n    assert kwargs_store == {'y': np.array([1, 2])}\n\n    # Test that the pass_y parameter is ignored when deprecated\n    args_store = []\n    kwargs_store = {}\n    func = _make_func(args_store, kwargs_store)\n\n    transformer = FunctionTransformer(pass_y=True, func=func)\n    transformer.transform(X, y=y)\n\n    assert len(args_store) == 1\n    assert kwargs_store == {}\n\n    # Test that the pass_y parameter is deprecated\n    with assert_warns_message(DeprecationWarning, \"The parameter y on transform() is \"\n                                            \"deprecated since 0.19 and will be \"\n                                            \"removed in 0.21\"):\n        transformer = FunctionTransformer(pass_y=True, func=func)\n        transformer.transform(X, y=y)\n", "def test_check_inverse_transform_with_custom_functions():\n    # Test that check_inverse_transform correctly raises a warning when the\n    # provided functions are not inverse of each other.\n    X = np.random.rand(100, 10)\n    func = _make_func([], [], lambda X, y: np.log(X))\n    inverse_func = _make_func([], [], lambda X, y: np.exp(X))\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func)\n    with pytest.warns(UserWarning) as record:\n        transformer._check_inverse_transform(X)\n    assert len(record) == 1\n    assert \"The provided functions are not strictly inverse of each other\" in str(record[0].message)\n\n    # Test that check_inverse_transform does not raise an assertion error when\n    # check_inverse is False.\n    transformer.check_inverse = False\n    transformer._check_inverse_transform(X)\n\n    # Test that check_inverse_transform does not raise an assertion error when\n    # func and inverse_func are the same.\n    transformer.func = transformer.inverse_func = lambda X, y: np.log(X)\n    transformer._check_inverse_transform(X)\n\n    # Test that check_inverse_transform correctly checks that func and inverse_func\n    # are inverse of each other when check_inverse is True.\n    transformer.func = lambda X, y: np.log(X)\n    transformer.inverse_func = lambda X, y: np.exp(X)\n    transformer.check_inverse = True\n    transformer._check_inverse_transform(X)\n", "def test_function_transformer_fit_check_inverse_transform_deprecated_pass_y():\n    \"\"\"Check that check_inverse_transform() raises a warning when pass_y is True\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n\n    func = _make_func([], [], func=lambda X, *args: X + args[0])\n    inverse_func = _make_func([], [], func=lambda X, *args: X - args[0])\n\n    transformer = FunctionTransformer(func=func, inverse_func=inverse_func, pass_y=True)\n    with assert_warns_message(UserWarning, transformer._check_inverse_transform, X):\n        transformer.fit(X, y)\n    assert_equal(transformer._check_input(X), X)\n\n", "def test_func_default_identity():\n    \"\"\"Ensure the default identity function behaves as expected.\"\"\"\n    args_store = []\n    kwargs_store = {}\n    transformer = FunctionTransformer()\n    X = np.array([1, 2, 3])\n\n    result = transformer.fit_transform(X)\n    assert_array_equal(result, X)\n\n    transformer = FunctionTransformer(func=None)\n    result = transformer.fit_transform(X)\n    assert_array_equal(result, X)\n"], "sample_1184": ["def test_conjugate_gauss_beams_wavelength():\n    from sympy.physics.optics import conjugate_gauss_beams\n    l, w_i, w_o, f = symbols('l w_i w_o f')\n    wavelen = 532e-9\n    kwargs = {'f': f}\n    expected_s_in = f * (1 - sqrt(1/(w_o/w_i)**2 - wavelen**2*l**2/(pi**2*f**2)))\n    expected_s_out = f*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 - wavelen**2*l**2/(pi**2*f**2)))/w_i**2\n    expected_f = f\n    result = conjugate_gauss_beams(wavelen, w_i, w_o, **kwargs)\n    assert streq(expected_s_in, result[0])\n    assert streq(expected_s_out, result[1])\n    assert streq(expected_f, result[2])\n", "def test_rayleigh2waist_with_complex_input():\n    from sympy.physics.optics import rayleigh2waist\n\n    l, wavelen = symbols('l wavelen')\n    z_r = pi*wavelen + 2*l\n    assert streq(rayleigh2waist(z_r, wavelen), sqrt(wavelen*z_r)/sqrt(pi))\n", "def test_gaussian_conj():\n    wavelen = 532e-9\n    z_r_in = 5\n    s_in = 1\n    f = 2\n\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, z_r_in/(1 - (s_in/f)**2 + (z_r_in/f)**2))\n    assert streq(s_out, 1 / ( -1/(s_in + z_r_in**2/(s_in - f)) + 1/f ))\n\n    s_in = -1\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, z_r_in/(1 - (s_in/f)**2 + (z_r_in/f)**2))\n    assert streq(s_out, 1 / ( -1/(s_in + z_r_in**2/(s_in - f)) + 1/f ))\n\n    # Test when f equals to s_in\n    s_in = 2\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, z_r_in)\n    assert streq(s_out, s_in)\n\n    # Test when f equals to 0\n    s_in = 1\n    f = 0\n    try:\n        gaussian_conj(s_in, z_r_in, f)\n        assert False\n    except ZeroDivisionError:\n        pass\n\n    # Test when s_in equals to 0\n    s_in = 0\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, z_r_in)\n    assert streq(s_out, 0)\n\n    # Test when z_r_in equals to 0\n    z_r_in = 0\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, 0)\n    assert streq(s_out, 0)\n\n    # Test when z_r_in equals to infinity\n    z_r_in = oo\n    s_out, z_r_out, m = gaussian_conj(s_in, z_r_in, f)\n    assert streq(z_r_out, oo)\n    assert streq(s_out,", "def test_geometric_conj_af():\n    # Test that geometric_conj_af returns the correct result for an infinite distance\n    from sympy import symbols\n    a, f = symbols('a f')\n    assert streq(geometric_conj_af(a, f), f)\n    \n    # Test that geometric_conj_af returns the correct result for a finite distance\n    a, f = symbols('a f')\n    result = geometric_conj_af(a, f)\n    expected = -a*f/(a - f)\n    assert streq(result, expected)\n", "def test_rayleigh2waist():\n    w, wavelen = symbols('w wavelen')\n    wavelen_val = 500e-9\n    w_val = 10e-6\n    z_r = rayleigh2waist(w, wavelen)\n    assert streq(z_r, sqrt(wavelen*w**2/pi))\n\n    # test with numerical values\n    z_r_val = rayleigh2waist(w_val, wavelen_val)\n    assert streq(z_r_val, 1.59e-5)\n\n    # test with complex values\n    w_complex = 10e-6 + 10e-6*I\n    z_r_complex = rayleigh2waist(w_complex, wavelen_val)\n    assert streq(z_r_complex, 1.59e-5 + 1.59e-5*I)\n\n    # test with symbolic expressions\n    w_expr = symbols('w')*10e-6\n    z_r_expr = rayleigh2waist(w_expr, wavelen_val)\n    assert streq(z_r_expr, sqrt(wavelen_val*w**2/pi))\n\n    # test with negative values\n    w_neg = -10e-6\n    z_r_neg = rayleigh2waist(w_neg, wavelen_val)\n    assert streq(z_r_neg, sqrt(wavelen_val*(-w)**2/pi))\n"], "sample_360": ["    def test_loc_mem_cache_fallback_to_default_cache_timeout(self):\n        # Test that the default cache timeout is used when page_timeout is not provided\n        middleware = UpdateCacheMiddleware(None)\n        request = HttpRequest()\n        response = HttpResponse()\n        response['Cache-Control'] = 'max-age=0'\n        middleware.process_response(request, response)\n        self.assertEqual(middleware.cache_timeout, settings.CACHE_MIDDLEWARE_SECONDS)\n", "    def test_update_cache_middleware_with_default_settings(self):\n        response = HttpResponse('Hello, world!')\n        middleware = UpdateCacheMiddleware(lambda x: x)\n        response = middleware.process_response(None, response)\n        self.assertEqual(response.cache_control.max_age, 60)\n", "    def test_update_cache_middleware_sets_cache_timeout(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        middleware = UpdateCacheMiddleware(lambda x: None)\n        patched_response = middleware.process_response(request, response)\n        self.assertEqual(patched_response['Cache-Control'], 'max-age=180')\n", "    def setUp(self):\n        super().setUp()\n        self.rf = RequestFactory()\n        self.response = HttpResponse(content_type='text/html', status=200)\n", "    def setUp(self):\n        # Ensure we use a fresh cache for each test case\n        self.cache_key_prefix = 'test_cache_key_prefix'\n        self.cache_alias = 'test_cache_alias'\n        self.cache_timeout = 3600  # 1 hour\n\n        # Set up the cache settings\n        settings.CACHE_MIDDLEWARE_SECONDS = self.cache_timeout\n        settings.CACHE_MIDDLEWARE_KEY_PREFIX = self.cache_key_prefix\n        settings.CACHE_MIDDLEWARE_ALIAS = self.cache_alias\n"], "sample_1143": ["def test_mod_inverse():\n    from sympy import S, Rational, pi, oo, zoo, sqrt\n    from sympy.core.numbers import mod_inverse\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(-3, 11) == 7\n    assert mod_inverse(3, 1) == 1\n    assert mod_inverse(3, 0) == S.NaN\n    assert mod_inverse(0, 11) == S.NaN\n    assert mod_inverse(1, 11) == 1\n    assert mod_inverse(-1, 11) == 10\n    assert mod_inverse(4, 11) == 3\n    assert mod_inverse(-4, 11) == 8\n    assert mod_inverse(9, 11) == 4\n    assert mod_inverse(-9, 11) == 7\n    assert mod_inverse(7, 11) == 8\n    assert mod_inverse(-7, 11) == 4\n    assert mod_inverse(10, 11) == 10\n    assert mod_inverse(-10, 11) == 1\n    assert mod_inverse(1, 11) == 1\n    assert mod_inverse(2, 3) == 2\n    assert mod_inverse(1, 2) == S.NaN\n    assert mod_inverse(2, 2) == S.NaN\n    assert mod_inverse(-1, 2) == S.NaN\n    assert mod_inverse(-2, 2) == S.NaN\n    assert mod_inverse(-2, 3) == 2\n    assert mod_inverse(-1, 3) == 2\n    assert mod_inverse(2, -1) == S.NaN\n    assert mod_inverse(-2, -1) == S.NaN\n    assert mod_inverse(1, -1) == S.NaN\n    assert mod_inverse(-1, -1) == S.NaN\n    assert mod_inverse(2, -3) == S.NaN\n    assert mod_inverse(-2, -3) == S.NaN\n    assert mod_inverse(1, -3) == S.NaN\n    assert mod_inverse(-1, -3) == S.NaN\n    assert mod_inverse(oo, 3) == S.NaN\n    assert mod_inverse(-oo, 3) == S.NaN\n    assert mod_inverse(oo, -3) == S.NaN\n", "def test_division_by_zero():\n    from sympy.core.numbers import seterr, comp, oo\n    seterr(divide=True)\n    comp(oo/2, oo/3)\n", "def test_Float_creation_from_mpf_tuple():\n    # all four args, including bits\n    assert same_and_same_prec(Float((1, 5, 0, 3)), Float(5))\n    # all four args, including bits\n    assert same_and_same_prec(Float((1, 5, 10, 3)), Float(5))\n    # three args\n    assert same_and_same_prec(Float((1, 5, 0)), Float(5))\n    # three args\n    assert same_and_same_prec(Float((1, 5, 10)), Float(5))\n    # two args, including bits\n    assert same_and_same_prec(Float((1, 5, 0, 2, 3)), Float(5))\n    # two args, including bits\n    assert same_and_same_prec(Float((1, 5, 10, 2, 3)), Float(5))\n    # two args\n    assert same_and_same_prec(Float((1, 5, 0, 2)), Float(5))\n    # two args\n    assert same_and_same_prec(Float((1, 5, 10, 2)), Float(5))\n    # no args, create Float(0)\n    assert same_and_same_prec(Float(), Float(0))\n    # single arg\n    assert same_and_same_prec(Float((1,)), Float(1))\n    # tuple with 1 arg\n    assert same_and_same_prec(Float((1, 0, 2)), Float(1))\n    # tuple with 2 args\n    assert same_and_same_prec(Float((1, 5, 2)), Float(5))\n    # tuple with 3 args\n    assert same_and_same_prec(Float((1, 5, 0, 2)), Float(5))\n    # tuple with 4 args\n    assert same_and_same_prec(Float((1, 5, 0, 3, 2)), Float(5))\n", "compilation error", "def test_floordiv_integer():\n    x, y = Integer(12), Integer(4)\n    assert x // y == Integer(3)\n    assert y // x == Integer(0)\n    with XFAIL('Can fail on 32-bit Python'):\n        assert x // y.is_Integer\n    assert (x // S.Half) == Integer(24)\n    assert (x // S.One) == x\n    with XFAIL('Can fail on 32-bit Python'):\n        assert (x // S.Half).is_Integer\n    with raises(SympifyError):\n        x // S(1/3)\n    with raises(SympifyError):\n        x // \"3\"\n    with raises(SympifyError):\n        x // Symbol('x')\n    with XFAIL('Can fail on 32-bit Python'):\n        x // oo\n    with XFAIL('Can fail on 32-bit Python'):\n        y // oo\n    with raises(SympifyError):\n        x // nan\n    with XFAIL('Can fail on 32-bit Python'):\n        x // -oo\n    with XFAIL('Can fail on 32-bit Python'):\n        y // -oo\n"], "sample_1009": ["def test_normalize():\n    a = ReferenceFrame('A')\n    b = ReferenceFrame('B')\n    v = 5 * a.x + 3 * a.y + 2 * a.z\n    assert v.normalize().args[0] == [1, 3/5, 2/5]\n", "def test_separate():\n    \"\"\"Test the separate method of the Vector class.\"\"\"\n    from sympy import symbols\n    from sympy.physics.vector import ReferenceFrame, Vector\n    q = symbols('q')\n    N = ReferenceFrame('N')\n    R1 = ReferenceFrame('R1')\n    R2 = ReferenceFrame('R2')\n    v = R1.x + R2.y\n    assert v.separate() == {R1: R1.x, R2: R2.y}\n", "def test_diff_zero():\n    N = ReferenceFrame('N')\n    t = symbols('t')\n    q1 = dynamicsymbols('q1')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n    v = q1 * A.x + 1 * N.y\n    assert v.diff(t, N) == Vector([(q1.diff(t), A)])\n    assert v.diff(t, A) == Vector([(-q1.diff(t), N)])\n    assert v.diff(q1, N) == Vector([(q1, N)])\n    assert v.diff(q1, A) == Vector([(-1, N)])\n    assert raises(VectorTypeError, v.diff, 'not a dynamicsymbols', N)\n", "def test_time_derivative():\n    # Test time_derivative method\n    q1 = symbols('q1')\n    t = symbols('t')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n\n    # Test case 1: simple time derivative\n    v = q1 * A.x\n    assert v.dt(N) == q1 * A.x\n\n    # Test case 2: time derivative with constants\n    v = 2 * A.x\n    assert v.dt(N) == 0 * A.x\n\n    # Test case 3: time derivative with complex expression\n    v = q1**2 * A.x\n    assert v.dt(N) == 2 * q1 * A.x\n\n    # Test case 4: time derivative with dynamicsymbols\n    v = dynamicsymbols('v') * A.x\n    assert v.dt(N) == dynamicsymbols('v') * A.x\n\n    # Test case 5: time derivative with Expression\n    v = sin(q1) * A.x\n    assert v.dt(N) == cos(q1) * q1 * A.x\n\n    # Test case 6: time derivative with sympy function\n    v = cos(q1) * A.x\n    assert v.dt(N) == -sin(q1) * q1 * A.x\n\n    # Test case 7: time derivative with vector\n    v = q1 * A.x + q1 * A.y\n    assert v.dt(N) == q1 * A.x\n\n    # Test case 8: time derivative with vector and dynamicsymbols\n    v = dynamicsymbols('v') * A.x + q1 * A.y\n    assert v.dt(N) == dynamicsymbols('v') * A.x\n\n    # Test case 9: time derivative with invalid input\n    with raises(VectorTypeError, match=\"Expected an instance of <class 'sympy.physics.vector.Vector'>\"):\n        Vector(1).dt(N)\n\n    # Test case 10: time derivative with invalid input type\n    with raises(TypeError, match=\"A time derivative must be taken in a ReferenceFrame\"):\n        v.dt(1)\n\n", "def test_separate():\n    from sympy.physics.vector.frame import _check_frame\n\n    # Test with a zero vector\n    v1 = Vector(0)\n    assert v1.separate() == {}\n\n    # Test with a vector in a single frame\n    v2 = A.x + A.y + A.z\n    assert v2.separate() == {A: A.x + A.y + A.z}\n\n    # Test with a vector in multiple frames\n    v3 = A.x + A.y + 2 * A.z\n    assert v3.separate() == {A: A.x + A.y + 2 * A.z}\n    v4 = A.x + A.y + A.z\n    assert (v3 + v4).separate() == {A: A.x + 2 * A.y + 3 * A.z}\n\n    # Test with a vector that has a different frame in each component\n    v5 = A.x + B.x + A.y + B.y\n    assert v5.separate() == {A: A.x + A.y, B: B.x + B.y}\n\n    # Test with a zero vector and a vector with different frames\n    v6 = A.x + B.x + A.y + B.y\n    assert (Vector(0) + v6).separate() == {A: A.x + A.y, B: B.x + B.y}\n\n    # Test with a vector that has a zero component in a frame\n    v7 = A.x + A.y + 0 * B.x + B.y\n    assert v7.separate() == {A: A.x + A.y, B: B.y}\n"], "sample_250": ["    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.tz = get_default_timezone()\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.now = datetime.now()\n        self.time_format = TimeFormat(self.now)\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.time_format = TimeFormat(datetime.now())\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n"], "sample_3": ["def test_e_csv_header_from_yaml_with_delimiter():\n    \"\"\"Test reading a header from a YAML file with a custom delimiter.\"\"\"\n    lines = ['# %ECSV 1.0',\n             '# ---',\n             '# datatype:',\n             '# - {name: a, datatype: int64, delimiter: ,}',\n             '# - {name: b, datatype: float64}',\n             'a,b c',\n             '1,1.0 c',\n             '2,2.0 d',\n             '3,3.0 e']\n    header = EcsvHeader(table_meta={'delimiter': ','})\n    for line in lines:\n        if line.startswith('#'):\n            header.process_lines([line])\n        else:\n            header.write([line])\n    assert header.splitter.delimiter == ','\n    assert header.cols[0].info.delimiter == ','\n    assert header.cols[1].info.delimiter == ','\n    assert header.cols[2].info.delimiter == ','\n\n    # Test reading a header from a YAML file with a custom delimiter that is not\n    # in DELIMITERS\n    lines = ['# %ECSV 1.0',\n             '# ---',\n             '# datatype:',\n             '# - {name: a, datatype: int64, delimiter: |}',\n             '# - {name: b, datatype: float64}',\n             'a|b c',\n             '1|1.0 c',\n             '2|2.0 d',\n             '3|3.0 e']\n    header = EcsvHeader(table_meta={'delimiter': '|'})\n    for line in lines:\n        if line.startswith('#'):\n            header.process_lines([line])\n        else:\n            header.write([line])\n    assert header.splitter.delimiter == '|'\n    assert header.cols[0].info.delimiter == '|'\n    assert header.cols[1].info.delimiter == '|'\n    assert header.cols[2].info.delimiter == '|'\n\n    # Test reading a header from a YAML file with a custom delimiter that is not\n    # in DELIMITERS and does not raise a ValueError\n    lines = ['# %ECSV 1.0',\n             '# ---',\n             '# datatype:',\n             '# - {name: a, datatype: int64, delimiter: $}',\n             '# - {name: b, datatype: float64}',\n             'a$b c',\n             '1$1.0 c',\n             '2$2.0 d',\n             '3$3.", "def test_ecsv_write_comments():\n    lines = []\n    header = EcsvHeader()\n    header.write(lines)\n    expected_lines = [\n        '# %ECSV 1.0',\n        '---',\n        'cols: []',\n        'schema: astropy-2.0',\n        'delimiter: '\n    ]\n    assert lines == expected_lines\n", "def test_ecsv_outputter():\n    # Test the EcsvOutputter class to ensure it converts Column objects to Table objects\n    # without raising an exception.\n    cols = [T_DTYPES['int64'], T_DTYPES['float64'], T_DTYPES['str']]\n    meta = {'meta': 'test_meta'}\n    outputter = ascii.EcsvOutputter()\n    table = outputter(cols, meta)\n    assert isinstance(table, Table)\n\n    # Test that the EcsvOutputter class preserves mixin columns\n    mixin_cols = ascii.mixins_from_cols([T_DTYPES['int64'], T_DTYPES['float64'], T_DTYPES['str']])\n    mixin_table = ascii._construct_mixins_from_columns(ascii.Table(cols))\n    outputter = ascii.EcsvOutputter()\n    output = outputter(mixin_cols, mixin_table.meta)\n    assert output is mixin_table\n\n    # Test the EcsvOutputter class to ensure it handles MaskedColumn objects\n    # correctly when converting them to Table objects\n    masked_cols = [MaskedColumn(T_DTYPES['int64'].data, T_DTYPES['int64'].mask, T_DTYPES['int64'].name)]\n    meta = {'meta': 'test_meta'}\n    outputter = ascii.EcsvOutputter()\n    table = outputter(masked_cols, meta)\n    assert isinstance(table, Table)\n\n    # Test that the EcsvOutputter class handles empty tables correctly\n    cols = []\n    meta = {'meta': 'test_meta'}\n    outputter = ascii.EcsvOutputter()\n    table = outputter(cols, meta)\n    assert isinstance(table, Table)\n\n    # Test that the EcsvOutputter class handles tables with no meta correctly\n    cols = [T_DTYPES['int64'], T_DTYPES['float64'], T_DTYPES['str']]\n    meta = {}\n    outputter = ascii.EcsvOutputter()\n    table = outputter(cols, meta)\n    assert isinstance(table, Table)\n\n    # Test that the EcsvOutputter class handles tables with mixin columns and no meta correctly\n    mixin_cols = ascii.mixins_from_cols([T_DTYPES['int64'], T_DTYPES['float64'], T_DTYPES['str']])\n    meta = {}\n    outputter = ascii.EcsvOutputter()\n    table = outputter(mixin_cols, meta)\n    assert isinstance(table, Table)\n", "def test_ecsv_outputter__float_dtype_written_as_str():\n    \"\"\"Test that float data type is written as string instead of float\"\"\"\n    ecsv_content = '''# %ECSV 0.9", "def test_eCSV_round_trip():\n    \"\"\"Check that data is read and written back correctly.\"\"\"\n    table = simple_table(T_DTYPES, format='ascii.ecsv', delimiter=',')\n    with nullcontext(table):\n        ecsv_string = table.read()\n    with nullcontext(ecsv_string):\n        out = ascii.read(ecsv_string, format='ascii.ecsv', delimiter=',')\n    assert isinstance(out, Table)\n    assert out.colnames == ['a', 'b', 'c']\n    assert out.dtype['a'].name == 'int64'\n    assert out.dtype['b'].name == 'float64'\n    assert out.dtype['c'].name == 'str'\n\n    for col, orig_col in zip(out.columns, table.columns):\n        if col.name == 'c':  # string column with empty strings in table\n            assert np.all(col == ['ab 0', 'ab, 1', 'ab2'])\n        else:\n            assert np.array_equal(col, orig_col)\n"], "sample_570": ["    def test_define_support_grid(self, x):\n        kde = KDE()\n        grid = kde.define_support(x, cache=False)\n        assert isinstance(grid, np.ndarray)\n", "    def test_default_binning_univariate(self, x):\n        hist = Histogram(stat=\"density\", bins=\"auto\")\n        result = hist(x)\n        assert_array_equal(result, 1)\n", "    def test_default_bins(self, x):\n        hist = Histogram()\n        hist.define_bin_params(x)\n        assert_array_equal(hist.bins, np.histogram_bin_edges(x, \"auto\"))\n", "    def test_bivariate_cumulative(self, x, y, weights):\n        hist = Histogram(stat=\"cumulative\", bins=10)\n        hist_x, bin_edges_x = hist(x, weights=weights)\n        hist_y, bin_edges_y = hist(y, weights=weights)\n        hist_xy, _ = hist(x, y, weights=weights)\n\n        assert hist_xy.shape == (10, 10)\n\n        assert np.isclose(hist_xy.sum(), hist_x.max() + hist_y.max())\n", "    def test_kde_clip(self, x, y, weights):\n        kde = KDE(bw_method=\"scott\", bw_adjust=0.5, clip=(-2, 2), cumulative=True)\n        density, support = kde(x, y, weights)\n        assert_array_equal(kde.clip, (-2, 2))\n"], "sample_797": ["def test_quantile_transform_robust_scale_equal():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    qt = QuantileTransformer(n_quantiles=10, random_state=0)\n    qt.fit(X)\n    robust_scaler = RobustScaler()\n    robust_scaler.fit(X)\n    assert_array_almost_equal(quantile_transform(X, n_quantiles=10,\n                                                random_state=0),\n                             robust_scaler.transform(X),\n                             decimal=5)\n", "def test_power_transform_warning():\n    \"\"\"Test warning for power_transform method with invalid parameter\"\"\"\n    with warnings.catch_warnings(record=True) as w:\n        power_transform(X=[[1, 2], [3, 2], [4, 5]], method='invalid',\n                       standardize=True)\n        assert_warns_message(UserWarning,\n                            \"The default value of 'method' will change from \"\n                            \"'box-cox' to 'yeo-johnson' in version 0.23.\",\n                            power_transform, X=[[1, 2], [3, 2], [4, 5]],\n                            method='invalid', standardize=True)\n", "    def test_fit_transform_empty_array(self):\n        scaler = MaxAbsScaler()\n        X = np.array([])\n        assert_equal(scaler.fit_transform(X).shape, (0, 0))\n", "def test_PowerTransformer_box_cox_bounds():\n    # First, we test that the Box-Cox transform correctly raises a ValueError\n    # when the input data contains zero or negative values\n    with pytest.raises(ValueError):\n        PowerTransformer(method='box-cox').fit([[1, -1], [0, 2]])\n\n    # We also test that the Box-Cox transform correctly transforms positive data\n    # and raises a ValueError when it encounters a zero or negative value\n    power_transformer = PowerTransformer(method='box-cox')\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n    with pytest.raises(ValueError):\n        power_transformer.fit_transform([[1, -1], [0, 2]])\n    transformed_X = power_transformer.fit_transform(X)\n    assert_equal(len(transformed_X), 3)\n    assert_equal(transformed_X.shape[1], 2)\n    assert_greater_equal(np.min(transformed_X), -BOUNDS_THRESHOLD)\n    assert_less_equal(np.max(transformed_X), BOUNDS_THRESHOLD)\n\n    # Now, we test the Yeo-Johnson transform, which can handle both positive\n    # and negative values\n    power_transformer = PowerTransformer(method='yeo-johnson')\n    X = np.array([[1, -1], [0, 2], [4, 5]])\n    transformed_X = power_transformer.fit_transform(X)\n    assert_equal(len(transformed_X), 3)\n    assert_equal(transformed_X.shape[1], 2)\n    assert_greater_equal(np.min(transformed_X), -BOUNDS_THRESHOLD)\n    assert_less_equal(np.max(transformed_X), BOUNDS_THRESHOLD)\n\n    # We also test that the inverse transform correctly inverts the Yeo-Johnson\n    # transform\n    power_transformer = PowerTransformer(method='yeo-johnson')\n    X = np.array([[1, -1], [0, 2], [4, 5]])\n    transformed_X = power_transformer.fit_transform(X)\n    inverse_X = power_transformer.inverse_transform(transformed_X)\n    assert_array_almost_equal(X, inverse_X)\n", "def test_power_transform_sparse():\n    # Test power_transform with sparse input data\n    # This is a regression test for issue #11019\n    X = sparse_random(10, 10, density=0.1, random_state=0)\n    # The power transform should fail because X contains NaNs\n    with assert_raises_regex(ValueError, 'Cannot compute power transformation'):\n        power_transform(X)\n\n    # Test that the warning for changing default method is emitted\n    with assert_warns_message(FutureWarning,\n                             'The default value of ''method'' will change '\n                             'from ''box-cox'' to ''yeo-johnson'' in '\n                             'version 0.23'):\n        power_transform(X)\n\n    # Test power_transform with a valid sparse input data\n    X = sparse_random(10, 10, density=0.1, random_state=0)\n    # Remove NaNs and zeros\n    X.data[X.data == 0] = np.nan\n    X.eliminate_zeros()\n    X = X.tocsr()\n    # Check that power_transform returns the correct result\n    X_trans = power_transform(X)\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_less(X_trans, 10)\n    assert_array_less(0, X_trans)\n\n    # Test that power_transform returns the correct result for yeo-johnson method\n    X_trans = power_transform(X, method='yeo-johnson')\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_less(X_trans, 10)\n    assert_array_less(0, X_trans)\n\n    # Test that power_transform returns the correct result for box-cox method\n    # (this should pass)\n    X_trans = power_transform(X, method='box-cox')\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_less(X_trans, 10)\n    assert_array_less(0, X_trans)\n\n    # Test power_transform with a valid dense input data\n    X = np.random.rand(10, 10)\n    X_trans = power_transform(X)\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_less(X_trans, 10)\n    assert_array_less(0, X_trans)\n\n    # Test that power_transform returns the correct result for yeo-johnson method\n    X_trans = power_transform(X, method='yeo-johnson')\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_less"], "sample_530": ["def test_PackerBase_init():\n    # Create a PackerBase instance\n    packer = PackerBase(pad=10, sep=5, width=100, height=50, align='center', mode='expand', children=None)\n    assert packer.pad == 10\n    assert packer.sep == 5\n    assert packer.width == 100\n    assert packer.height == 50\n    assert packer.align == 'center'\n    assert packer.mode == 'expand'\n    assert packer._children is None\n", "def test_get_packed_offsets():\n    # Test the get_packed_offsets function\n        widths = [1, 2, 3]\n        total, offsets = _get_packed_offsets(widths, None, 0.5, mode=\"fixed\")\n        assert total == 6.0\n        assert_allclose(offsets, [0, 1.5, 3])\n\n        widths = [1, 2, 3]\n        total, offsets = _get_packed_offsets(widths, 10, 0.5, mode=\"expand\")\n        assert total == 10\n        assert_allclose(offsets, [0, 2, 6])\n\n        widths = [1, 2, 3]\n        total, offsets = _get_packed_offsets(widths, None, 0.5, mode=\"equal\")\n        assert total == 8\n        assert_allclose(offsets, [1, 3, 5])\n\n        widths = [1, 2, 3]\n        total, offsets = _get_packed_offsets(widths, 8, 0.5, mode=\"equal\")\n        assert total == 8\n        assert_allclose(offsets, [1, 3, 5])\n\n    test_fixed_mode()\n    test_expand_mode()\n    test_equal_mode()\n", "def test_offsetbox_stale():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    box = OffsetBox()\n    assert box.stale is False\n    box.set_offset((5, 5))\n    assert box.stale is True\n    box.set_offset((5, 5))\n    assert box.stale is True\n    box.set_transform(mtransforms.IdentityTransform())\n    assert box.stale is True\n", "def test_offsetbox_set_offset():\n    # Create a figure and axis\n    fig, ax = plt.subplots()\n\n    # Create an offset box\n    box = OffsetBox()\n    box.set_figure(fig)\n\n    # Set the offset\n    box.set_offset((1, 1))\n\n    # Create a text\n    text = mtext.Text(0, 0, 'Hello')\n    text.set_figure(fig)\n    text.set_transform(box.get_transform())\n    box._children.append(text)\n\n    # Set the offset again\n    box.set_offset((2, 2))\n\n    # Check the offset\n    assert box._offset == (2, 2)\n\n    # Check the offset of the child\n    assert text.get_transform().get_matrix().ravel() == np.array([2, 2, 1, 1])\n", "def test_packed_boxes_in_frame():\n    fig, ax = plt.subplots()\n\n    vpb = VPacker(pad=0.1, sep=0.1, align='bottom')\n    vpb.add_artist(mpatches.Rectangle((0.1, 0.1), 0.2, 0.2, facecolor='blue'))\n    vpb.add_artist(mpatches.Rectangle((0.3, 0.3), 0.2, 0.2, facecolor='red'))\n    vpb.add_artist(mpatches.Rectangle((0.5, 0.5), 0.2, 0.2, facecolor='green'))\n\n    hpb = HPacker(pad=0.1, sep=0.1, align='bottom')\n    hpb.add_artist(mpatches.Rectangle((0.1, 0.6), 0.2, 0.2, facecolor='blue'))\n    hpb.add_artist(mpatches.Rectangle((0.3, 0.6), 0.2, 0.2, facecolor='red'))\n    hpb.add_artist(mpatches.Rectangle((0.5, 0.6), 0.2, 0.2, facecolor='green'))\n\n    da = DrawingArea(1.0, 1.0)\n    da.add_artist(vpb)\n    da.add_artist(hpb)\n\n    aabb = AnnotationBbox(OffsetBox((0.0, 0.0), width=1.0, height=1.0),\n                          (0.5, 0.5), pad=0.2, frameon=False)\n\n    ax.add_patch(da)\n    ax.add_artist(aabb)\n    ax.set_xlim(-0.1, 1.1)\n    ax.set_ylim(-0.1, 1.1)\n\n        for i, (x, y, w, h) in enumerate(ax.patches):\n            assert_allclose(w, 0.2, atol=0.01)\n            assert_allclose(h, 0.2, atol=0.01)\n            assert_allclose(x, 0.3 + i*0.3, atol=0.01)\n            assert_allclose(y, 0.1, atol=0.01)\n\n        for i, (x, y, w, h)"], "sample_996": ["def test_product_product():\n    # Test that a product of two products gets evaluated correctly\n    P1 = product(x, (x, 1, n))\n    P2 = product(x**2, (x, 1, n))\n    P = product(P1, P2)\n    expected = Product(x**3, (x, 1, n))\n    assert P.is_equal(expected)\n\n    # Test that a product of a product with a non-commutative term gets evaluated correctly\n    P1 = product(x, (x, 1, n))\n    P = product(x**2 * f(x), (x, 1, n))\n    expected = Product(x**3 * f(x), (x, 1, n))\n    assert P.is_equal(expected)\n\n    # Test that a product of two products with different symbols gets evaluated correctly\n    P1 = product(x, (x, 1, n))\n    P2 = product(y, (y, 1, m))\n    P = product(P1, P2)\n    expected = Product(x, (x, 1, n), Product(y, (y, 1, m)))\n    assert P.is_equal(expected)\n", "def test_product_reverse_order():\n    from sympy.concrete.summations import Sum\n    n, m, x, a, b = symbols('n m x a b', integer=True)\n    # Check that the function works correctly for a product of one limit\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(0)\n    assert Pr == Product(1/x, (x, b + 1, a - 1))\n    assert Pr.doit() == 1/RisingFactorial(b + 1, a - b - 1)\n\n    # Check that the function works correctly for a product with multiple limits\n    S = Sum(x*y, (x, a, b), (y, 1, 5))\n    S0 = S.reverse_order(0)\n    assert S0 == Sum(-x*y, (x, b + 1, a - 1), (y, 1, 5))\n    S1 = S0.reverse_order(1)\n    assert S1 == Sum(x*y, (x, b + 1, a - 1), (y, 5, 0))\n\n    # Check that the function works correctly when some limits are specified by index\n    S = Sum(x*y, (x, a, b), (y, 1, 5))\n    S0 = S.reverse_order(0)\n    assert S0 == Sum(-x*y, (x, b + 1, a - 1), (y, 1, 5))\n    S1 = S0.reverse_order(1)\n    assert S1 == Sum(x*y, (x, b + 1, a - 1), (y, 5, 0))\n\n    # Check that the function works correctly when mixed notations are used\n    S = Sum(x*y, (x, a, b), (y, 1, 5))\n    S0 = S.reverse_order(x, 1)\n    assert S0 == Sum(x*y, (x, b + 1, a - 1), (y, 5, 0))\n    S1 = S0.reverse_order(y)\n    assert S1 == Sum(x*y, (x, b + 1, a - 1), (y, 5, 0))\n\n    # Check that the function raises an error when the index is out of range\n   ", "def test_product_reverse_order():\n    from sympy import Product, simplify\n    from sympy.abc import a, b\n\n    P = Product(x, (x, a, b))\n    Pr = P.reverse_order(x)\n    assert Pr == Product(1/x, (x, b + 1, a - 1))\n    Pr = Pr.doit()\n    assert simplify(Pr) == 1/RisingFactorial(b + 1, -b - a)\n    assert simplify(P.doit()) == RisingFactorial(a, b + 1 - a)\n", "def test_eval_product_commutative():\n    from sympy import Product, symbols\n    k, a, n = symbols('k a n', integer=True)\n\n    P = Product(k, (k, 1, n))\n    P1 = Product(k, (k, a, n))\n\n    P2 = Product(k, (k, a, n))\n    assert P2._eval_product(k, (k, a, n)) is None\n\n    result = P1._eval_product(P, (k, a, n))\n    assert result == Product(P, (k, 1, n))\n", "def test_product_with_complex_function():\n    from sympy import Im, Re\n    z = symbols('z')\n    z1, z2 = symbols('z1 z2')\n    assert product(z, (z, 1, 2)).doit() == 2*z - 1\n    assert product(z**2, (z, 1, 2)).doit() == (2*z - 1)**2\n    assert product(z1*z, (z1, 1, 2), (z2, 1, 2)).doit() == 4*(z1*z)**2\n    assert product(Im(z), (z, 1, 2)).doit() == Im(z1)*Im(z2)\n    assert product(Re(z), (z, 1, 2)).doit() == Re(z1)*Re(z2)\n"], "sample_901": ["def test_kmeans_predict():\n    # Create a synthetic dataset\n    centers = np.array([[0.0, 5.0, 0.0, 0.0, 0.0],\n                        [1.0, 1.0, 4.0, 0.0, 0.0],\n                        [1.0, 0.0, 0.0, 5.0, 1.0]])\n    X, _ = make_blobs(n_samples=100, centers=centers,\n                      cluster_std=1., random_state=42)\n    kmeans = KMeans(n_clusters=centers.shape[0], random_state=42)\n    kmeans.fit(X)\n    labels = kmeans.predict(X)\n\n    # Check that the labels are correct\n    assert_array_equal(labels, np.array([1, 1, 2, 0, 0, 2, 2, 1, 1, 0, 2]))\n\n    # Check that the cluster centers are correct\n    assert_array_almost_equal(kmeans.cluster_centers_, centers)\n\n    # Check that the inertia is correct\n    assert_allclose(kmeans.inertia_, 150.0)\n", "def test_k_means_tuning(representation, algo, dtype, tol, max_iter):\n    if representation == 'dense':\n        X = np.random.rand(100, 5).astype(dtype)\n    else:\n        X = sp.rand(100, 5, format='csr', dtype=dtype)\n    kmeans = KMeans(init='k-means++', n_init=1, tol=tol, max_iter=max_iter,\n                    algorithm=algo, random_state=42)\n    kmeans.fit(X)\n    assert_allclose(kmeans.cluster_centers_.shape, (2, 5))\n    assert_array_equal(kmeans.cluster_centers_.dtype, dtype)\n", "def test_kmeans_max_iter_influence_dense_representation(representation, dtype):\n    n_samples, n_clusters, n_features = 100, 5, 5\n    X, true_labels = make_blobs(n_samples=n_samples, n_clusters=n_clusters,\n                                cluster_std=1., random_state=42)\n\n    if representation == 'dense':\n        X = np.array(X, dtype=dtype)\n    elif representation == 'sparse':\n        X = sp.csr_matrix(X, dtype=dtype)\n    else:\n        raise ValueError(\"representation must be 'dense' or 'sparse'\")\n\n    kmeans = KMeans(init='k-means++', n_clusters=n_clusters, max_iter=10,\n                    n_init=1, algorithm=algo)\n    kmeans.fit(X)\n    labels = kmeans.labels_\n    score = kmeans.score(X)\n\n    kmeans_max_iter = KMeans(init='k-means++', n_clusters=n_clusters,\n                             max_iter=1000, n_init=1, algorithm=algo)\n    kmeans_max_iter.fit(X)\n    labels_max_iter = kmeans_max_iter.labels_\n    score_max_iter = kmeans_max_iter.score(X)\n\n    assert not np.array_equal(labels, labels_max_iter)\n    assert score_max_iter < score\n", "def test_kmeans_init_centers(density, algo, dtype):\n    n_samples, n_features = 100, 10\n    X = np.random.rand(n_samples, n_features).astype(dtype)\n    if density == 'sparse':\n        X = sp.csr_matrix(X)\n    X_centers = np.random.rand(n_samples, n_features).astype(dtype)\n    X_centers = X_centers.astype(X.dtype)\n    if density == 'sparse':\n        X_centers = sp.csr_matrix(X_centers)\n\n    kmeans = KMeans(n_clusters=2, init='kmeans++', algorithm=algo,\n                    random_state=42, n_init=1)\n    kmeans.fit(X)\n\n    assert_array_almost_equal(kmeans.cluster_centers_, kmeans.init,\n                             decimal=5)\n\n    kmeans = KMeans(n_clusters=2, init=X_centers, algorithm=algo,\n                    random_state=42, n_init=1)\n    kmeans.fit(X)\n\n    assert_array_almost_equal(kmeans.cluster_centers_, X_centers,\n                             decimal=5)\n\n    kmeans = KMeans(n_clusters=2, init='random', algorithm=algo,\n                    random_state=42, n_init=1)\n    kmeans.fit(X)\n\n    assert_array_almost_equal(kmeans.cluster_centers_, kmeans.init,\n                             decimal=5)\n\n    kmeans = KMeans(n_clusters=2, init=lambda X, k, random_state: X[:k, :],\n                    algorithm=algo, random_state=42, n_init=1)\n    kmeans.fit(X)\n\n    assert_array_almost_equal(kmeans.cluster_centers_, kmeans.init,\n                             decimal=5)\n", "def test_kmeans_unbalanced_data():\n    \"\"\"Test kmeans on unbalanced data.\"\"\"\n    X, y = make_blobs(n_samples=[100, 1, 1, 1, 1, 1], centers=centers,\n                       cluster_std=1., random_state=42)\n    kmeans = KMeans(init='k-means++', n_clusters=3, n_init=1)\n    kmeans.fit(X)\n    assert kmeans.cluster_centers_.shape == (3, 5)\n    assert kmeans.labels_.shape == (600,)\n"], "sample_1137": ["def test_quantity_simplify_with_multiple_prefixes():\n    from sympy import Rational\n    from sympy.physics.units.prefixes import kilo, mega\n    from sympy.physics.units import kilogram, meter, second, meter_per_second\n    from sympy.physics.units import quantity_simplify\n    # test that multiple prefixes are correctly replaced with numerical values\n    expr = kilo * kilo * meter * meter / (meter_per_second ** 2)\n    expected = Rational(1000) ** 2 * meter * meter / meter_per_second ** 2\n    assert quantity_simplify(expr) == expected\n    # test that prefixes are correctly replaced in expressions with multiple terms\n    expr = kilo * meter + mega * meter\n    expected = Rational(1000) * meter + Rational(1_000_000) * meter\n    assert quantity_simplify(expr) == expected\n    # test that prefixes are correctly replaced when the input is a Quantity\n    expr = Quantity(kilo * meter, meter)\n    expected = Rational(1000) * meter\n    assert quantity_simplify(expr) == expected\n", "def test_quantity_simplify_empty_quantity():\n    from sympy.physics.units.quantities import Quantity\n    from sympy.physics.units.prefixes import Prefix\n    from sympy.physics.units.util import quantity_simplify\n    from sympy.physics.units import UnitSystem\n\n    unit_system = UnitSystem.get_unit_system(\"SI\")\n\n    q = Quantity(\"m\")\n    assert quantity_simplify(q) == q\n", "def test_convert_to_multiple_quantities():\n    from sympy.physics.units import UnitSystem, speed_of_light, meter, gram\n    from sympy.physics.units import convert_to\n    unit_system = UnitSystem.get_unit_system(\"SI\")\n\n    # Test with multiple target quantities\n    result = convert_to(speed_of_light, [meter, gram])\n    assert result == speed_of_light\n\n    # Test with different units\n    result = convert_to(speed_of_light, [kilometer, gram])\n    expected = speed_of_light * (1000/gram)\n    assert result == expected\n\n    result = convert_to(speed_of_light, [meter, gram])\n    expected = speed_of_light\n    assert result == expected\n\n    # Test with invalid target units\n    result = convert_to(speed_of_light, [meter, second, kilogram])\n    assert result == speed_of_light\n\n    # Test with a single target unit\n    result = convert_to(speed_of_light, meter)\n    expected = speed_of_light\n    assert result == expected\n\n    result = convert_to(speed_of_light, kilometer)\n    expected = speed_of_light * (1000/gram)\n    assert result == expected\n\n    result = convert_to(speed_of_light, gram)\n    expected = speed_of_light * (1000**2)\n    assert result == expected\n\n    # Test with a list of target units with a single element\n    result = convert_to(speed_of_light, [meter])\n    expected = speed_of_light\n    assert result == expected\n", "def test_quantity_simplify_empty_quantity():\n    from sympy.physics.units.prefixes import kilo\n    from sympy.physics.units.definitions import kilogram\n    expr = kilo * kilogram\n    simplified = quantity_simplify(expr)\n    assert simplified.equals(expr), \"Quantity simplify failed: \" + str(expr)\n", "def test_convert_to_complex_expression():\n    from sympy.physics.units import convert_to\n    from sympy.physics.units import second, kilogram, meter, speed_of_light\n\n    expr = speed_of_light ** 2 * kilogram * meter\n    target_units = second ** 2\n    result = convert_to(expr, target_units)\n    assert result == 89875517873681764.64 * second ** 2\n"], "sample_285": ["    def test_base_finder_init(self):\n        finder = BaseFinder()\n        self.assertIsInstance(finder, BaseFinder)\n        self.assertIsNone(finder.locations)\n        self.assertIsNone(finder.storages)\n", "    def test_default_file_path_with_prefix(self):\n        finder = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        path = 'static/file.txt'\n        self.assertEqual(finder.find(path, all=False), os.path.join(TEST_ROOT, 'static', 'file.txt'))\n", "    def test_check_valid_staticfiles_dirs(self):\n        errors = check_finders(None)\n        self.assertEqual(len(errors), 0)\n", "    def test_base_finder_check_method(self):\n        with self.assertRaises(NotImplementedError):\n            BaseFinder().check()\n", "    def test_finder_order(self):\n        finder1 = get_finder('django.contrib.staticfiles.finders.FileSystemFinder')\n        finder2 = get_finder('django.contrib.staticfiles.finders.AppDirectoriesFinder')\n        self.assertGreater(finder1.locations, finder2.locations)\n"], "sample_1150": ["def test_Range():\n    # Test Range with integer step\n    r = Range(1, 5, 2)\n    assert r == Range(1, 5, 2)\n    assert list(r) == [1, 3]\n    assert r.inf == 1\n    assert r.sup == 3\n    assert r.measure == 2\n\n    # Test Range with non-integer step\n    r = Range(1, 5, 2.5)\n    assert r == Range(1, 5, 2.5)\n    assert list(r) == [1, 2.5, 4]\n    assert r.inf == 1\n    assert r.sup == 4\n    assert r.measure == 3\n\n    # Test Range with infinite start\n    r = Range(-oo, 5, 2)\n    assert r == Range(-oo, 5, 2)\n    assert list(r) == [-4, 2, 4]\n    assert r.inf == -4\n    assert r.sup == 4\n    assert r.measure == oo\n\n    # Test Range with infinite stop\n    r = Range(1, oo, 2)\n    assert r == Range(1, oo, 2)\n    assert list(r) == [1, 3, 5]\n    assert r.inf == 1\n    assert r.sup == oo\n    assert r.measure == oo\n\n    # Test Range with negative step\n    r = Range(5, 1, -2)\n    assert r == Range(5, 1, -2)\n    assert list(r) == [5, 3, 1]\n    assert r.inf == 5\n    assert r.sup == 1\n    assert r.measure == 3\n\n    # Test Range with invalid step\n    r = Range(1, 5, 0)\n    with raises(ValueError):\n        list(r)\n\n    # Test Range with invalid start\n    r = Range(oo, 5, 2)\n    with raises(ValueError):\n        list(r)\n\n    # Test Range with invalid stop\n    r = Range(1, -oo, 2)\n    with raises(ValueError):\n        list(r)\n\n    # Test Range with symbolic start\n    x = Symbol('x')\n    r = Range(x, 5, 2)\n    assert r == Range(x, 5, 2)\n    assert list(r", "    def test_polar_complex_region_polar_form(self):\n        r = Interval(0, 1)\n        theta = Interval(0, 2*pi)\n        c = ComplexRegion(r*theta, polar=True)\n        self.assertEqual(c.polar, True)\n        self.assertEqual(c.variables, symbols('r, theta', cls=Dummy))\n        self.assertEqual(c.psets, (ProductSet(r, theta),))\n        self.assertEqual(c.a_interval, r)\n        self.assertEqual(c.b_interval, theta)\n", "def test_ImageSet_lambda_return_type():\n    from sympy import S\n    i = ImageSet(Lambda(x, x**2), S.Integers)\n    assert isinstance(i, ImageSet)\n", "def test_ImageSet_complex_domain():\n    from sympy.sets.fancysets import ImageSet\n    x = symbols('x')\n    z = ImageSet(Lambda(x, x**2), S.Integers)\n    assert z in S.Reals\n    assert not z in S.Naturals\n    assert z == S.Union(S.Naturals, S.Negatives)\n    assert (z & S.Naturals) == S.Naturals\n    assert (z & S.Naturals0) == S.Naturals\n    assert (z & S.Integers) == S.Integers\n    assert (z & S.Naturals) == S.Union(S.Naturals, S.Negatives)\n    assert (z & S.Naturals0) == S.Union(S.Naturals, S.Negatives)\n    assert (z & S.Reals) == S.Reals\n", "def test_ImageSet_properties():\n    from sympy.sets.fancysets import ImageSet\n    f = Lambda(x, x**2)\n    s = S.Naturals\n    imset = ImageSet(f, s)\n    assert imset.is_iterable\n    assert imset.base_set == s\n    assert imset.base_pset == ProductSet(s)\n    assert len(imset.base_sets) == 1\n    assert imset.base_sets[0] == s\n"], "sample_492": ["    def test_serialize_enum(self):\n        enum_class = TextEnum\n        serialized_enum = OperationWriter.serialize(enum_class.A).split(\"\\n\")\n        self.assertEqual(serialized_enum, [\"TextEnum.A\"])\n", "    def test_deconstructible_class(self):\n        writer = OperationWriter(DeconstructibleInstances())\n        self.assertEqual(\n            writer.render(),\n            \"migrations.DeconstructibleInstances(\\n    ),\\n)\",\n        )\n", "    def test_simple_operation(self):\n        operation = migrations.RunPython(functools.partial(print, 'Hello, world!'))\n        writer = OperationWriter(operation)\n        self.assertEqual(writer.render(), \"migrations.RunPython(print, 'Hello, world!',),\")\n        self.assertEqual(writer.imports, set())\n", "    def test_serialize_enum(self):\n        # Test serializing enums\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(TextEnum.A)).render(),\n            '\"TextEnum.A\"'\n        )\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(TextEnum.B)).render(),\n            '\"TextEnum.B\"'\n        )\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(TextTranslatedEnum.A)).render(),\n            '\"_(\"a-value\")\"'\n        )\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(TextTranslatedEnum.B)).render(),\n            '\"_(\"value-b\")\"'\n        )\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(BinaryEnum.A)).render(),\n            \"'b'a-value'\"\n        )\n        self.assertEqual(\n            OperationWriter(MigrationWriter.serialize(BinaryEnum.B)).render(),\n            \"'b'value-b'\"\n        )\n", "    def test_serialize_operation_with_enum_arguments(self):\n        class CustomEnum(enum.Enum):\n            A = 1\n            B = 2\n\n            return migrations.CreateModel(\n                model_name=\"test_model\",\n                fields=[\n                    (\"enum_field\", models.CharField(max_length=255)),\n                ],\n                options={\"verbose_name\": \"Test model\"},\n                bases=(models.Model,),\n                app_label=\"tests\",\n                app_config=None,\n            )\n\n        operation = make_operation(CustomEnum)\n        writer = OperationWriter(operation)\n        self.assertIn('enum_field=CustomEnum.A', writer.render())\n        self.assertIn('verbose_name=\"Test model\"', writer.render())\n"], "sample_940": ["def test_getargspec():\n        pass\n\n    argspec = inspect.getargspec(func)\n    assert argspec.args == ['a', 'b']\n    assert argspec.varargs is None\n    assert argspec.varkw is None\n    assert argspec.defaults == (None, None)\n    assert argspec.kwonlyargs == ['c']\n    assert argspec.kwdefaults is None\n    assert argspec.annotations == {}\n", "def test_unpartial():\n        pass\n\n    partial_func = functools.partial(func)\n    assert unpartial(partial_func) is func\n", "def test_signature_from_str_signature():\n    sig = inspect.signature_from_str(signature=\"def func(a: int, *, b: str) -> None\")\n    assert sig.parameters == {\n        'a': inspect.Parameter('a', inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=int),\n        'b': inspect.Parameter('b', inspect.Parameter.KEYWORD_ONLY, annotation=str),\n    }\n    assert sig.return_annotation is inspect.Signature.empty\n", "def test_unwrap_all_mock_object():\n    \"\"\"Test unwrap_all for mock object.\"\"\"\n    from unittest.mock import Mock\n    class A:\n        pass\n    mock_obj = Mock(spec=A)\n    assert unwrap_all(mock_obj) is mock_obj\n", "def test_is_singledispatch_method():\n    class my_single_dispatch_method:\n            self.name = name\n\n        @singledispatchmethod\n            return self.dispatch(arg)\n\n        @dispatch(int)\n            return arg\n\n        @dispatch(str)\n            return arg\n\n    obj = my_single_dispatch_method('test')\n    assert inspect.is_singledispatch_method(obj.dispatch)\n"], "sample_1176": ["def test_Float():\n    assert same_and_same_prec(Float(1, 0), Float(1, 0))\n    assert Float(0, 5) == 0\n    assert Float(0, 5).is_Zero == True\n    assert Float(0, 5).is_Number == True\n    assert Float(0, 5).is_Float == True\n    assert Float(0, 5).is_real == True\n    assert Float(0, 5).is_positive == False\n    assert Float(0, 5).is_negative == False\n    assert Float(0, 5).is_extended_real == True\n    assert Float(0, 5).is_extended_positive == False\n    assert Float(0, 5).is_extended_negative == False\n    assert Float(0, 5).is_infinite == False\n    assert Float(0, 5).is_finite == True\n    assert Float(0, 5).is_zero == True\n    assert Float(0, 5).is_nonzero == False\n    assert Float(0, 5).is_positive == False\n    assert Float(0, 5).is_negative == True\n    assert Float(0, 5).is_real == True\n    assert Float(0, 5).is_finite == True\n    assert Float(0, 5).is_number == True\n    assert Float(0, 5).is_finite == True\n    assert Float(0, 5).is_finite == True\n    assert Float(0, 5).is_integer == False\n    assert Float(0, 5).is_rational == False\n    assert Float(0, 5).is_irrational == True\n    assert Float(0, 5).is_transcendental == False\n    assert Float(0, 5).is_algebraic == False\n    assert Float(0, 5).is_finite == True\n    assert Float(0, 5).is_negative == True\n    assert Float(0, 5).is_odd == False\n    assert Float(0, 5).is_even == True\n    assert Float(0, 5).is_positive == False\n    assert Float(0, 5).is_extended_real == True\n    assert Float(0, 5).is_nonnegative ==", "    def test_float_comparison_with_nan(self):\n        assert comp(nan, 1) is False\n        assert comp(nan, -1) is False\n        assert comp(nan, nan) is False\n        assert comp(nan, oo) is False\n        assert comp(nan, -oo) is False\n        assert comp(nan, oo + oo) is False\n        assert comp(nan, -oo + oo) is False\n", "def test_GoldenRatio():\n    from sympy import Rational\n    assert GoldenRatio.is_irrational\n    assert GoldenRatio.is_real\n    assert GoldenRatio.is_positive\n    assert not GoldenRatio.is_negative\n    assert not GoldenRatio.is_number.is_algebraic\n    assert not GoldenRatio.is_number.is_transcendental\n    assert GoldenRatio.is_number.is_number\n    assert not GoldenRatio.is_number.is_finite\n    assert not GoldenRatio.is_number.is_integer\n    assert GoldenRatio.is_number.is_real\n    assert GoldenRatio.is_number.is_extended_real\n    assert GoldenRatio.is_number.is_positive\n    assert not GoldenRatio.is_number.is_negative\n    assert GoldenRatio.is_number.is_extended_positive\n    assert GoldenRatio.is_number.is_extended_negative is False\n    assert GoldenRatio.is_number.is_infinite is False\n    assert not GoldenRatio.is_number.is_extended_real.is_finite\n    assert not GoldenRatio.is_number.is_extended_real.is_integer\n    assert not GoldenRatio.is_number.is_extended_real.is_irrational\n    assert GoldenRatio.is_number.is_extended_real.is_real\n    assert GoldenRatio.is_number.is_extended_real.is_extended_real\n    assert GoldenRatio.is_number.is_extended_real.is_extended_positive\n    assert GoldenRatio.is_number.is_extended_real.is_extended_negative is False\n    assert GoldenRatio.is_number.is_extended_real.is_infinite is False\n    assert GoldenRatio.is_number.is_prime is False\n    assert GoldenRatio.is_number.is_finite is False\n    assert GoldenRatio.is_number.is_integer is False\n    assert GoldenRatio.is_number.is_irrational\n    assert GoldenRatio.is_number.is_number\n    assert GoldenRatio.is_number.is_real\n    assert GoldenRatio.is_number.is_transcendental is False\n    assert GoldenRatio._eval_rewrite_as_sqrt() == GoldenRatio._eval_expand_func()\n    assert GoldenRatio.approximation_interval(Integer) == (1, 2)\n    assert GoldenRatio.approximation_interval(Rational) is None\n    assert GoldenRatio._eval_evalf(10)._prec == 14\n    assert GoldenRatio._as_mpf_val(10)._mpf_ == mpf(mpmath.mp.math.mp.mpf(GoldenRatio._eval_expand_func().evalf(10)))\n    assert GoldenRatio._eval_power(S.One) == GoldenRatio\n    assert GoldenRatio._eval_power(S.NegativeOne) == S.Infinity\n    assert GoldenRatio._eval_power(S", "def test_float_negate():\n    assert Float('1.2').negate() == Float('-1.2')\n    assert -Float('1.2') == Float('-1.2')\n    assert Float('-1.2').negate() == Float('1.2')\n    assert -Float('-1.2') == Float('1.2')\n", "def test_Float_floor():\n    assert Float(1.5).floor() == Integer(1)\n    assert Float(-1.5).floor() == Integer(-2)\n"], "sample_254": ["    def test_inline_admin_checks(self):\n        # Verify InlineModelAdmin's model attribute is not None\n        self.assertIsNotNone(InnerInline.model)\n        self.assertIsNotNone(Inner4Stacked.model)\n        self.assertIsNotNone(Inner4Tabular.model)\n        # Verify InlineModelAdmin's fk_name attribute is not None\n        self.assertIsNotNone(InnerInline.fk_name)\n        self.assertIsNotNone(Inner4Stacked.fk_name)\n        self.assertIsNotNone(Inner4Tabular.fk_name)\n        # Verify InlineModelAdmin's extra attribute is not None\n        self.assertIsNotNone(InnerInline.extra)\n        self.assertIsNotNone(Inner4Stacked.extra)\n        self.assertIsNotNone(Inner4Tabular.extra)\n        # Verify InlineModelAdmin's min_num attribute is not None\n        self.assertIsNotNone(InnerInline.min_num)\n        self.assertIsNotNone(Inner4Stacked.min_num)\n        self.assertIsNotNone(Inner4Tabular.min_num)\n        # Verify InlineModelAdmin's max_num attribute is not None\n        self.assertIsNotNone(InnerInline.max_num)\n        self.assertIsNotNone(Inner4Stacked.max_num)\n        self.assertIsNotNone(Inner4Tabular.max_num)\n        # Verify InlineModelAdmin's template attribute is not None\n        self.assertIsNotNone(InnerInline.template)\n        self.assertIsNotNone(Inner4Stacked.template)\n        self.assertIsNotNone(Inner4Tabular.template)\n        # Verify InlineModelAdmin's verbose_name attribute is not None\n        self.assertIsNotNone(InnerInline.verbose_name)\n        self.assertIsNotNone(Inner4Stacked.verbose_name)\n        self.assertIsNotNone(Inner4Tabular.verbose_name)\n        # Verify InlineModelAdmin's verbose_name_plural attribute is not None\n        self.assertIsNotNone(InnerInline.verbose_name_plural)\n        self.assertIsNotNone(Inner4Stacked.verbose_name_plural)\n        self.assertIsNotNone(Inner4Tabular.verbose_name_plural)\n        # Verify InlineModelAdmin's can_delete attribute is not None\n        self.assertIsNotNone(InnerInline.can_delete)\n        self.assertIsNotNone(Inner4Stacked.can_delete)\n        self.assertIsNotNone(Inner4Tabular.can_delete)\n        # Verify InlineModelAdmin's show_change_link attribute is not None\n        self.assertIsNotNone(InnerInline.show_change_link)\n        self.assertIsNotNone(Inner4Stacked.show_change_link)\n        self.assertIsNotNone(Inner4Tabular.show_change_link)\n        # Verify InlineModelAdmin's checks_class attribute is", "    def test_inline_admin_media(self):\n        inline = InnerInline()\n        self.assertIsInstance(inline.media, forms.Media)\n        self.assertIn('inlines.js', inline.media._js)\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.holder = Holder.objects.create(dummy=13)\n        Inner.objects.create(dummy=42, holder=cls.holder)\n", "    def test_inline_view(self):\n        self.client.force_login(self.superuser)\n        url = reverse('admin:HolderInline', kwargs={'parent_lookup': 'pk', 'parent_id': self.holder.pk})\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n", "    def test_has_change_permission(self):\n        \"\"\"Inline model admin has_change_permission correctly considers the parent model's permissions.\"\"\"\n        admin = InnerInline(Inner, admin_site)\n        request = self.factory.get(reverse('admin:admin_inlines_holder_changelist'))\n        request.user = self.superuser\n        obj = Holder.objects.create(dummy=13)\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = self.superuser\n        request.method = 'GET'\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = self.superuser\n        request.method = 'POST'\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        request.method = 'GET'\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        request.method = 'POST'\n        self.assertTrue(admin.has_change_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        self.assertFalse(admin.has_add_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        request.method = 'GET'\n        self.assertFalse(admin.has_add_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        request.method = 'POST'\n        self.assertFalse(admin.has_add_permission(request, obj))\n\n        request.user = self.superuser\n        self.assertFalse(admin.has_delete_permission(request, obj))\n\n        request.user = self.superuser\n        request.method = 'GET'\n        self.assertFalse(admin.has_delete_permission(request, obj))\n\n        request.user = self.superuser\n        request.method = 'POST'\n        self.assertFalse(admin.has_delete_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        self.assertFalse(admin.has_delete_permission(request, obj))\n\n        request.user = User.objects.create_user(username='normal', email='normal@example.com', password='secret')\n        request.method = 'GET'\n        self.assertFalse(admin.has_delete_permission"], "sample_665": ["    def test_collect_file(self):\n        collector = PyCollector()\n        path = py.path.local(\"test_file.py\")\n        collector.fspath = path\n        collected = list(collector.collect())\n        assert collected == []\n", "    def test_parametrize(self):\n        # arrange\n        metafunc = Metafunc(\n            definition=FunctionDefinition(\"test_function\"),\n            fixtureinfo=fixtures.FixtureInfo([\"fixture1\", \"fixture2\"]),\n            config=pytest.config,\n            cls=None,\n            module=None,\n        )\n\n        # act\n        metafunc.parametrize(\"arg1\", [1, 2], indirect=True)\n        parametrize_ids = metafunc._resolve_arg_ids([\"arg1\"], None, [metafunc.getparam(\"arg1\")], item=metafunc.definition)\n\n        # assert\n        assert parametrize_ids == [\"fixture1-1\", \"fixture1-2\", \"fixture2-1\", \"fixture2-2\"]\n", "    def test_collect_function(self, tmp_path):\n        tmp_path.mkdir()\n        test_module = tmp_path.join(\"test_module.py\")\n        with test_module.open(\"w\") as f:\n            f.write(\n                textwrap.dedent(\n                    \"\"\"\n                        pass\n                    \"\"\"\n                )\n            )\n        tmp_path.join(\"__init__.py\").touch()\n        session = Session()\n        session._fixturemanager.add_functional_fixture(\"x\", \"int\")\n        session._fixturemanager.add_functional_fixture(\"y\", \"int\")\n        fixtureinfo = session._fixturemanager.getfixtureinfo(\n            FunctionDefinition(\"test_function\", None, config=session.config),\n            None,\n            None,\n            funcargs=True,\n        )\n        collector = Function(\n            \"test_function\", parent=session, fixtureinfo=fixtureinfo\n        )\n        fixtures.fillfixtures(collector)\n        collector.setup()\n        assert collector.fixturenames == [\"x\", \"y\"]\n", "    def test_collect_module(self, tmpdir):\n        module = tmpdir.join('test_module.py')\n        module.write('''\n            import pytest\n\n            @pytest.fixture\n                return 1\n\n                pass\n        ''')\n\n        # Create a dummy config object\n        config = pytest.config\n        session = Session(config)\n\n        # Create a dummy collector\n        collector = Module(module.strpath, None, config, session)\n\n        # Run the collect method\n        items = collector.collect()\n\n        # Check that the correct items were collected\n        assert len(items) == 2\n        for item in items:\n            if isinstance(item, Function):\n                assert item.function.__name__ == 'test_function'\n                assert item.fixturenames == {'fixture1'}\n            elif isinstance(item, FixtureInfo):\n                assert item.fixturedef.argname == 'fixture1'\n                assert item.fixturedef.scope == 'function'\n", "def test_pytest_addoption(tmpdir):\n    from _pytest.main import _in_venv\n    from _pytest.main import pytest_addoption\n    from _pytest.main import pytest_configure\n    from _pytest.main import pytest_parse_args\n\n    config = pytest.config\n    optionmanager = pytest.config.optionmanager\n    assert optionmanager.addoption(\"non_existent_option\") == \"non_existent_option\"\n\n    assert pytest_addoption(config) == 0\n    assert optionmanager.addoption(\"non_existent_option\", \"desc\") == \"non_existent_option\"\n\n    # check that we don't get an error if the group doesn't exist\n    group = config.getini(\"general\")\n    assert isinstance(group, pytest._config.OptionGroup)\n    pytest_addoption(config)\n\n    # check that we don't crash if we add two options with the same name\n    pytest_addoption(config)\n\n    # check that we don't crash when trying to add an option to the parser\n    # that is already in the parser\n    pytest_addoption(config)\n\n    # check that we can configure the parser\n    pytest_configure(config)\n\n    # check that the main function doesn't crash if it is called with an invalid\n    # option\n    pytest_parse_args([\"-v\", \"--non-existent-option\"])\n\n    # check that the main function doesn't crash when running a test\n    # with the -v option\n    pytest_parse_args([\"-v\", \"--fixtures\"])\n\n    # check that the main function doesn't crash when running a test with\n    # the --fixtures-per-test option\n    pytest_parse_args([\"--fixtures-per-test\"])\n\n    # check that the main function doesn't crash when running a test with the\n    # --show-fixtures option\n    pytest_parse_args([\"--fixtures\"])\n\n    # check that the main function doesn't crash when running a test with\n    # the --show-fixtures-per-test option\n    pytest_parse_args([\"--fixtures-per-test\"])\n\n    # check that the main function doesn't crash when running a test with the\n    # --import-mode option\n    pytest_parse_args([\"--import-mode\", \"prepend\"])\n\n    # check that the main function doesn't crash when running a test with\n    # the --import-mode option\n    pytest_parse_args([\"--import-mode\", \"append\"])\n\n    # check that the main function doesn't crash when running a test with the\n    # --import-mode option\n    pytest_parse_args([\"--import"], "sample_57": ["    def test_password_field_is_read_only(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertTrue(form.fields['password'].widget.read_only)\n", "    def test_read_only_password_hash_field(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertTrue(form.fields['password'].read_only)\n        self.assertFalse(form.fields['password'].has_changed())\n", "    def testReadOnlyPasswordHashWidget(self):\n        widget = ReadOnlyPasswordHashWidget()\n        self.assertEqual(widget.read_only, True)\n", "    def test_read_only_password_hash_field(self):\n        form = UserChangeForm(data={'username': 'testclient', 'password': 'password', 'email': 'testclient@example.com'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('password', form.errors)\n", "    def test_no_password(self):\n        form = UserChangeForm({'password': ''})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['password'], '')\n"], "sample_569": ["    def test_ci_lowess(self):\n        plotter = lm._RegressionPlotter(\n            x=self.rs.uniform(size=20),\n            y=self.rs.uniform(size=20),\n            x_bins=4,\n            ci=50,\n            lowess=True,\n            seed=self.rs,\n        )\n        with pytest.raises(ValueError):\n            plotter.fit_regression()\n", "    def test_x_ci(self, x_ci):\n        plotter = lm._RegressionPlotter(None, None, x_ci=x_ci)\n        with pytest.raises(ValueError):\n            plotter.x_ci\n", "    def test_x_ci_regression_plot(self, x_ci):\n        df = self.df.copy()\n        g = lm.regplot(x=\"x\", y=\"y\", x_ci=x_ci, ci=None, ax=plt.gca())\n        assert g.lines[0].get_xdata().max() == df.x.max()\n", "    def test_lmplot_with_complex_data(self):\n        # Test that we can pass arbitrary data to lmplot\n        df = pd.DataFrame({\n            'A': np.random.rand(100),\n            'B': np.random.rand(100),\n            'C': np.random.randint(0, 3, 100),\n            'D': np.random.randn(100)\n        })\n        g = lm.lmplot(x='A', y='B', hue='C', data=df)\n        assert len(g.axes) == 3\n\n        # Test that we can pass categorical data to lmplot\n        df = pd.DataFrame({\n            'A': np.random.rand(100),\n            'B': np.random.rand(100),\n            'C': np.random.choice(['a', 'b', 'c'], 100)\n        })\n        g = lm.lmplot(x='A', y='B', hue='C', data=df)\n        assert len(g.axes) == 3\n\n        # Test that we can pass ordered categorical data to lmplot\n        df = pd.DataFrame({\n            'A': np.random.rand(100),\n            'B': np.random.rand(100),\n            'C': pd.Categorical(['a', 'b', 'a', 'c', 'b', 'a', 'c', 'a', 'b', 'a'], ordered=True)\n        })\n        g = lm.lmplot(x='A', y='B', hue='C', data=df)\n        assert len(g.axes) == 3\n\n        # Test that we can pass numeric data with different units to lmplot\n        df = pd.DataFrame({\n            'A': np.random.rand(100),\n            'B': np.random.rand(100),\n            'C': np.random.randn(100)\n        })\n        g = lm.lmplot(x='A', y='B', hue='C', data=df, units='C')\n        assert len(g.axes) == 3\n", "def test_regressionplot_regression_order():\n    \"\"\"Regression plot should correctly compute polynomial fit.\"\"\"\n    data = pd.DataFrame({'x': np.linspace(0, 10, 100), 'y': np.sin(np.linspace(0, 10, 100))})\n    x, y = 'x', 'y'\n    plotter = lm._RegressionPlotter(x, y, data, x_estimator=None, x_bins=None, x_ci=None,\n                                    scatter=True, fit_reg=True, ci=95, n_boot=100, units=None,\n                                    seed=42, order=2, logistic=False, lowess=False, robust=False,\n                                    logx=False, x_partial=None, y_partial=None,\n                                    truncate=True, x_jitter=None, y_jitter=None, color=None, label=None)\n\n    plotter.fit_regression(grid=None)\n\n    x, yhat, err_bands = plotter.fit_regression(grid=None)\n    npt.assert_equal(len(x), 100)\n    assert np.allclose(yhat, np.polyval(np.polyfit(plotter.x, plotter.y, 2), x))\n    if err_bands is not None:\n        assert np.allclose(err_bands[:, 0], np.polyval(np.polyfit(plotter.x, plotter.y, 2) - 1.96 * np.std(plotter.y - np.polyval(np.polyfit(plotter.x, plotter.y, 2), plotter.x)) / np.sqrt(len(plotter.x)), x))\n        assert np.allclose(err_bands[:, 1], np.polyval(np.polyfit(plotter.x, plotter.y, 2) + 1.96 * np.std(plotter.y - np.polyval(np.polyfit(plotter.x, plotter.y, 2), plotter.x)) / np.sqrt(len(plotter.x)), x))\n"], "sample_482": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_truncator_empty(self):\n        # Test that an empty string is not changed when truncated\n        self.assertEqual(self.render_template(\"truncator_empty\"), \"\")\n"], "sample_852": ["def test_make_friedman3_invalid_n_samples():\n    \"\"\"Test that make_friedman3 raises an error when n_samples is less than 5.\"\"\"\n    with pytest.raises(ValueError):\n        make_friedman3(n_samples=4)\n", "def test_make_gaussian_quantiles():\n    # Test with default values\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=3)\n    assert y.shape == (100,)\n    assert np.unique(y).size == 3\n\n    # Test with non-default values\n    X, y = make_gaussian_quantiles(n_samples=50, n_features=3, n_classes=2)\n    assert y.shape == (50,)\n    assert np.unique(y).size == 2\n\n    # Test with mean not being zero\n    X, y = make_gaussian_quantiles(mean=[1, 1], n_samples=100, n_features=2, n_classes=3)\n    assert y.shape == (100,)\n    assert np.unique(y).size == 3\n\n    # Test with different covariance matrix\n    X, y = make_gaussian_quantiles(cov=5, n_samples=100, n_features=2, n_classes=3)\n    assert y.shape == (100,)\n    assert np.unique(y).size == 3\n\n    # Test with non-integer number of samples\n    X, y = make_gaussian_quantiles(n_samples=50.5, n_features=2, n_classes=3)\n    assert y.shape == (50,)\n    assert np.unique(y).size == 3\n\n    # Test with non-integer number of features\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2.5, n_classes=3)\n    assert y.shape == (100,)\n    assert np.unique(y).size == 3\n\n    # Test with non-integer number of classes\n    X, y = make_gaussian_quantiles(n_samples=100, n_features=2, n_classes=2.5)\n    assert y.shape == (100,)\n    assert np.unique(y).size == 2\n\n    # Test with negative number of samples\n    with pytest.raises(ValueError):\n        make_gaussian_quantiles(n_samples=-100, n_features=2, n_classes=3)\n\n    # Test with negative number of features\n    with pytest.raises(ValueError):\n        make_gaussian_quantiles(n_samples=100, n_features=-2, n_classes=3)\n\n    # Test with negative number of classes\n    with pytest.raises(ValueError):\n        make_gaussian_quantiles(n_samples=100, n_features=2, n", "def test_make_sparse_spd_matrix_norm_diag():\n    # Test make_sparse_spd_matrix with norm_diag = True\n    matrix = make_sparse_spd_matrix(dim=5, norm_diag=True)\n    assert_all_finite(matrix)\n\n    # Test that the matrix is symmetric\n    assert_array_equal(matrix, matrix.T)\n\n    # Test that the matrix is positive definite\n    assert (np.all(linalg.eigvals(matrix) > 0))\n\n    # Test that the matrix has 1 on the diagonal\n    assert_array_equal(np.diag(matrix), np.ones(5))\n\n    # Test that the matrix has a low sparsity level\n    assert (np.count_nonzero(matrix) > 1)\n", "def test_make_low_rank_matrix():\n    # Test that the matrix is indeed low rank\n    n_samples = 100\n    n_features = 100\n    effective_rank = 10\n    random_state = 42\n    X = make_low_rank_matrix(n_samples, n_features, effective_rank, random_state=random_state)\n    U, S, V = linalg.svd(X)\n    assert_array_almost_equal(S, S[:effective_rank], decimal=5)\n\n    # Test that the matrix is SPD\n    X = make_spd_matrix(n_samples, n_features)\n    assert linalg.isspd(X)\n\n    # Test that the matrix is sparse SPD\n    X = make_sparse_spd_matrix(n_samples, n_features)\n    assert linalg.isspd(X)\n    assert isinstance(X, sp.csr_matrix)\n\n    # Test that the matrix is not sparse SPD\n    X = make_low_rank_matrix(n_samples, n_features, effective_rank, random_state=random_state)\n    assert not isinstance(X, sp.csr_matrix)\n\n    # Test that the matrix has the correct singular values profile\n    n_samples = 100\n    n_features = 100\n    effective_rank = 10\n    tail_strength = 0.5\n    X = make_low_rank_matrix(n_samples, n_features, effective_rank, tail_strength, random_state=42)\n    U, S, V = linalg.svd(X)\n    S = np.sort(S)\n    assert_array_almost_equal(S, S[:effective_rank] + tail_strength * np.exp(-0.1 * np.arange(effective_rank, n_features) / effective_rank), decimal=5)\n", "def test_make_gaussian_quantiles_mean():\n    # Test that the mean is correctly applied when generating the normal distribution\n    n_samples = 100\n    n_features = 2\n    mean = [1, 2]\n    cov = 1.\n    n_classes = 3\n    result = make_gaussian_quantiles(mean=mean, n_samples=n_samples,\n                                    n_features=n_features, n_classes=n_classes)\n    X, y = result\n    assert_array_equal(X.mean(axis=0), np.array(mean))\n"], "sample_436": ["    def test_runserver_with_invalid_port_number(self):\n        with self.assertRaises(CommandError) as cm:\n            call_command(\"runserver\", \"abc\")\n        self.assertIn(\"is not a valid port number.\", str(cm.exception))\n", "    def test_runserver_command_ipv6_invalid_address(self):\n        # Test that an IPv6 address is correctly identified as invalid when it's not enclosed in square brackets\n        with captured_stderr() as stderr:\n            with self.assertRaises(CommandError):\n                call_command('runserver', '2001:0db8:85a3:0000:0000:8a2e:0370:7334', interactive=False)\n        self.assertIn('2001:0db8:85a3:0000:0000:8a2e:0370:7334 is not a valid IPv6 address.', stderr.getvalue())\n", "    def setUp(self):\n        super().setUp()\n        self.write_settings(\n            \"settings.py\",\n            apps=[\"django.contrib.auth\", \"django.contrib.contenttypes\", \"admin_scripts\"],\n            sdict={\"DATABASES\": {\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\"}}},\n        )\n        self.write_settings(\"tests.py\", extra=\"from django.test import TestCase\")\n", "    def test_empty_addrport_arg(self):\n        \"\"\"\n        An empty addrport arg should default to default_addr and default_port.\n        \"\"\"\n        with captured_stdout() as stdout:\n            with captured_stderr() as stderr:\n                call_command(\"runserver\", \"\")\n        self.assertEqual(stderr.getvalue(), \"\")\n", "    def test_ipv6_support(self):\n        with captured_stdout() as captured:\n            execute_from_command_line([\"django-admin\", \"runserver\", \"--ipv6\"])\n            output = captured.getvalue().strip()\n            self.assertIn(\"IPv6 address\", output)\n        with captured_stdout() as captured:\n            execute_from_command_line([\"django-admin\", \"runserver\", \"192.168.1.1\"])\n            output = captured.getvalue().strip()\n            self.assertIn(\"IPv4 address\", output)\n"], "sample_15": ["def test_round_to_quantity():\n    q = u.Quantity([1, 2, 3, 4, 5], u.m)\n    expected = u.Quantity([1, 2, 3, 4, 5], u.m)\n    assert_array_equal(q.round().value, expected.value)\n\n    # Test with different units\n    q = u.Quantity([1, 2, 3, 4, 5], u.m)\n    expected = u.Quantity([1, 2, 3, 4, 5], u.cm)\n    assert_array_equal(q.round(unit=u.cm).value, expected.value)\n\n    # Test with decimals argument\n    q = u.Quantity([1.1234, 2.1234, 3.1234], u.m)\n    expected = u.Quantity([1.1, 2.1, 3.1], u.m)\n    assert_array_equal(q.round(decimals=1).value, expected.value)\n\n    # Test with out argument\n    q = u.Quantity([1, 2, 3, 4, 5], u.m)\n    out = np.empty_like(q.value)\n    expected = u.Quantity([1, 2, 3, 4, 5], u.m)\n    assert_array_equal(q.round(out=out).value, expected.value)\n", "def test_add_with_quantity_and_non_quantity():\n    q1 = 2 * u.m\n    q2 = 3 * u.m\n    res = np.add(q1, q2)\n    assert res == 5 * u.m\n", "def test_nansum():\n    q = u.Quantity([1, 2, np.nan, 4])\n    result = q.nansum()\n    assert isinstance(result, u.Quantity)\n    assert np.isclose(result.value, 7)\n    assert result.unit == q.unit\n\n    # Check that the nansum method does not raise a UnitsError\n    # even when there are NaNs in the input array\n    q = u.Quantity([1, 2, u.m, 4])\n    result = q.nansum()\n    assert isinstance(result, u.Quantity)\n    assert np.isclose(result.value, 7)\n    assert result.unit == q.unit\n\n    # Check that the nansum method raises a UnitsError when the units\n    # are incompatible\n    q = u.Quantity([1, 2, u.m, 4])\n    with pytest.raises(UnitsError):\n        q = u.Quantity([1, 2, 3, 4], unit=u.s)\n        q.nansum()\n", "def test_neg_sin():\n    # Test that np.sin works with quantities and the appropriate unit is\n    # preserved.\n    test_cases = [\n        testcase(\n            f=np.sin,\n            q_in=u.Quantity([1, 2], u.deg),\n            q_out=u.Quantity([-0.8414709848078965, -0.9092974268256817], u.rad),\n        ),\n        testcase(\n            f=np.sin,\n            q_in=u.Quantity([1, 2], u.deg),\n            q_out=u.Quantity([-0.8414709848078965, -0.9092974268256817], u.rad),\n        ),\n        # Test that an error is raised if the input is not a quantity.\n        testexc(\n            f=np.sin,\n            q_in=[1, 2],\n            exc=ValueError,\n            msg=\"The input could not be fully interpreted as a scalar or array\",\n        ),\n        # Test that an error is raised if the unit is not supported by the\n        # function.\n        testexc(\n            f=np.sin,\n            q_in=u.Quantity([1, 2], u.m),\n            exc=UnitsError,\n            msg=\"UnitsError\",\n        ),\n    ]\n    for case in test_cases:\n        if isinstance(case, testcase):\n            expected = case.q_out\n            expected_unit = expected.unit\n            result = case.f(case.q_in)\n            assert_array_equal(result, expected.value)\n            assert result.unit == expected_unit\n        else:\n            with pytest.raises(case.exc):\n                case.f(case.q_in)\n                assert case.msg in str(e)\n", "def test_tan():\n    # test tan\n    q_in = u.Quantity([0.5, 0.75], u.rad)\n    q_out = u.Quantity([0.5463066979, 0.6954778797], u.dimensionless_unscaled)\n\n    testcase = testcase(f=np.tan, q_in=q_in, q_out=q_out)\n\n    with pytest.raises(UnitsError):\n        testcase = testexc(\n            f=np.tan,\n            q_in=q_in,\n            exc=UnitsError,\n            msg=\"Input array and Unit to not have the same units\",\n        )\n\n        erfa_ufunc(np.tan, u.Quantity([0.5, 0.75], u.deg), q_in.unit)\n        testcase = testexc(\n            f=np.tan,\n            q_in=u.Quantity([0.5, 0.75], u.deg),\n            exc=erfa_ufunc(np.tan, u.Quantity([0.5, 0.75], u.deg)),\n            msg=\"erfa.tan failed\",\n        )\n\n        with warnings.catch_warnings(record=True):\n            q_out = testcase.f(\n                u.Quantity([0.5, 0.75], u.deg), q_in, q_in.unit / u.rad\n            )\n            testcase = testwarn(\n                f=np.tan,\n                q_in=u.Quantity([0.5, 0.75], u.deg),\n                wfilter=warnings.warn,\n            )\n\n            with pytest.raises(UserWarning):\n                testcase = testwarn(\n                    f=np.tan,\n                    q_in=u.Quantity([0.5, 0.75], u.deg),\n                    wfilter=UserWarning,\n                )\n"], "sample_534": ["def test_contour_labels_inline_spacing():\n    fig, ax = plt.subplots()\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2 + Y**2\n    contour = ax.contour(X, Y, Z, 5, inline=True, inline_spacing=10)\n    labels = contour.clabels()\n    assert len(labels) == 5\n    for label in labels:\n        ax.add_patch(label)\n", "def test_contour_labeling_horizontal_line():\n    # Create a simple contour plot with a horizontal line\n    x = np.linspace(0, 1, 100)\n    y = np.linspace(0, 1, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.ones((100, 100))\n\n    fig, ax = plt.subplots()\n    cs = ax.contour(X, Y, Z, 10)\n\n    # Label the contour at a specific level\n    cs.clabel(levels=[0.5])\n\n    # Check if the label is placed correctly\n    assert len(cs.labelTexts) == 1\n    label = cs.labelTexts[0]\n    assert np.allclose(label.get_position(), (0.5, 0.5))\n    assert np.isclose(label.get_rotation(), 90)\n\n    # Check if the label is placed correctly for a horizontal line\n    cs.clabel(levels=[0.5], transform=False)\n    assert len(cs.labelTexts) == 1\n    label = cs.labelTexts[0]\n    assert np.allclose(label.get_position(), (0.5, 0.5))\n    assert np.isclose(label.get_rotation(), 90)\n\n    # Test with a custom rotation\n    cs.clabel(levels=[0.5], rotation=45)\n    assert len(cs.labelTexts) == 1\n    label = cs.labelTexts[0]\n    assert np.allclose(label.get_position(), (0.5, 0.5))\n    assert np.isclose(label.get_rotation(), 45)\n", "def test_contour_labeler_no_labels():\n    \"\"\"Test that ContourLabeler does not add any labels when no levels are given.\"\"\"\n    fig, ax = plt.subplots()\n    levels = [10, 20, 30, 40]\n    x, y = np.meshgrid(np.linspace(0, 10, 10), np.linspace(0, 10, 10))\n    z = np.sin(x) * np.cos(y)\n    c = ax.contour(x, y, z, levels=levels)\n    cl = c.labels(inline=True, inline_spacing=5)\n    assert len(cl) == 0\n    assert ax.collections[0].collections[0].get_paths() == ax.collections[0].collections[0].get_paths()\n", "def test_contour_labeler_zorder():\n    \"\"\"\n    Test the zorder of contour labels when calling clabel.\n    \"\"\"\n    # Create a contour plot\n    x = np.linspace(-10, 10, 100)\n    y = np.linspace(-10, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    fig, ax = plt.subplots()\n    cset = ax.contour(X, Y, Z, [0, 1])\n\n    # Test that labels are drawn at correct zorder\n    cset.clabel(zorder=5)\n\n    # Check the zorder of the labels\n    labels = cset.labelTexts\n    assert labels[0].get_zorder() == 5\n\n    # Test that the zorder is correctly set when calling clabel with no zorder argument\n    cset.clabel()\n    labels = cset.labelTexts\n    assert labels[0].get_zorder() == 3\n\n    # Test that the zorder is correctly set when calling clabel with a specific zorder\n    cset.clabel(zorder=10)\n    labels = cset.labelTexts\n    assert labels[0].get_zorder() == 10\n", "def test_add_label_near_xy():\n    with rc_context(rc={'keymap.button': 'custom'}):\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        z = np.sin(2*x)\n        fig, ax = plt.subplots()\n        contour = ax.contour(x, y, z, 10, colors='r')\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-2, 2)\n        contour.clabel()\n\n        ax.add_artist(mpl.patches.Circle((5, 0), 1, color='b'))\n        contour.add_label_near(5, 0, transform=False)\n\n        # Check that the label is added\n        assert len(contour.labelTexts) == 1\n\n        # Check that the label is drawn correctly\n        assert contour.labelTexts[0].get_x() == 5\n        assert contour.labelTexts[0].get_y() == 0\n        assert contour.labelTexts[0].get_rotation() == 90\n\n        # Check that the contour is updated correctly\n        assert contour.labelXYs == [(5, 0)]\n\n        # Check that the contour is not updated when inlining\n        contour.labelManual = True\n        contour.add_label_near(5, 0, inline=True, transform=False)\n        assert contour.labelXYs == [(5, 0)]\n\n        # Check that the label is not drawn when inlining\n        assert len(contour.labelTexts) == 1\n"], "sample_271": ["    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with self.assertRaises(NotImplementedError):\n            next(reloader.tick())\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        t = reloader.tick()\n        self.assertTrue(t is not None)\n        self.assertTrue(hasattr(t, '__next__'))\n", "    def test_tick(self):\n        import_module('django.utils.autoreload')\n        reloader = autoreload.StatReloader()\n        with mock.patch.object(time, 'sleep', side_effect=[None]):\n            with mock.patch.object(reloader, 'should_stop', return_value=False):\n                with contextlib.ExitStack() as stack:\n                    thread = stack.enter_context(threading.Thread(target=reloader.tick))\n                    thread.start()\n                    thread.join(timeout=0.1)\n                    self.assertTrue(thread.is_alive())\n                    reloader.stop()\n                    thread.join()\n", "    def test_tick_runs_forever(self):\n        reloader = autoreload.StatReloader()\n        thread = threading.Thread(target=reloader.tick)\n        thread.start()\n        time.sleep(1)  # Wait for the thread to start checking for changes\n        thread.join(1)  # Give the thread some time to run\n        self.assertTrue(thread.is_alive())  # Check if the thread is still alive\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n        self.script_path = self.temporary_file('script.py')\n\n        self.write_script_code(self.script_path)\n\n        self.module_name = 'test_script_module'\n        self.import_and_cleanup(self.module_name)\n\n        self.run_command(['python', '-m', 'autoreload', self.temp_dir])\n"], "sample_427": ["    def test_management_form_init(self):\n        form = ManagementForm(\n            auto_id=\"test_auto_id\",\n            prefix=\"test_prefix\",\n            initial={\n                TOTAL_FORM_COUNT: 5,\n                INITIAL_FORM_COUNT: 2,\n                MIN_NUM_FORM_COUNT: 1,\n                MAX_NUM_FORM_COUNT: 10,\n            },\n            renderer=None,\n        )\n        self.assertEqual(form.auto_id, \"test_auto_id\")\n        self.assertEqual(form.prefix, \"test_prefix\")\n        self.assertEqual(form.initial, {\n            TOTAL_FORM_COUNT: 5,\n            INITIAL_FORM_COUNT: 2,\n            MIN_NUM_FORM_COUNT: 1,\n            MAX_NUM_FORM_COUNT: 10,\n        })\n", "    def test_base_formset_management_form_valid(self):\n        formset = ChoiceFormSet(data={\n            \"TOTAL_FORMS\": \"1\",\n            \"INITIAL_FORMS\": \"0\",\n            \"MAX_NUM_FORMS\": \"1\",\n            \"MIN_NUM_FORMS\": \"0\",\n            \"0-choice\": \"choice1\",\n            \"0-votes\": \"1\",\n        })\n        self.assertTrue(formset.management_form.is_valid())\n", "    def test_formset_factory_min_num_min_max(self):\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, min_num=10, max_num=5)\n", "    def test_delete_forms(self):\n        formset_data = [(1, 2), (3, 4), (5, 6)]\n        delete_data = {\"0-DELETE\": \"on\", \"1-DELETE\": \"\"}\n        formset = ChoiceFormSet(\n            data=formset_data, data2=delete_data, prefix=\"choices\", auto_id=False\n        )\n        self.assertFalse(formset.is_valid())\n        with self.assertRaises(ValidationError):\n            formset.full_clean()\n", "    def test_formset_factory_absolute_max(self):\n        \"\"\"Test that formset_factory raises an error when max_num is higher than absolute_max.\"\"\"\n        with self.assertRaises(ValueError):\n            formset_factory(FavoriteDrinkForm, max_num=1001, absolute_max=1000)\n"], "sample_672": ["def test_safeformat():\n    \"\"\"test that safeformat() doesn't raise an exception when an object has a failing __repr__ method\"\"\"\n    class Obj:\n            raise Exception(\"Failing __repr__\")\n\n    assert safeformat(Obj()) == \"<class '__main__.Obj'>\"\n", "def test_saferepr_maxsize_too_small():\n    class ExampleClass:\n            return \"a\" * 1000\n    with pytest.raises(ValueError):\n        saferepr(ExampleClass(), maxsize=5)\n", "def test_safeformat():\n    class CustomClass:\n            raise Exception(\"Test exception\")\n\n    obj = CustomClass()\n    assert \"Exception('Test exception') CustomClass at 0x...\" in safeformat(obj)\n", "def test_safeformat_handles_lambda():\n    \"\"\"Test safeformat() with a lambda function.\"\"\"\n        return \"test\"\n    assert safeformat(test_lambda) == \"test\"\n", "def test_safeformat():\n    class TestClass:\n            raise Exception(\"Test Exception\")\n\n    obj = TestClass()\n    assert safeformat(obj).startswith(\"<Exception (\")  # Check that repr() exception is correctly reported\n    assert safeformat(obj).endswith(\"object at 0x\")  # Check that id is present in repr()\n"], "sample_1066": ["def test_mathml():\n    from sympy import exp\n    from sympy.printing.mathml import mathml\n    from sympy.printing.mathml import MathMLContentPrinter, MathMLPresentationPrinter\n\n    # Test MathMLContentPrinter\n    expr = exp(x)\n    mcp = MathMLContentPrinter()\n    mathml_expr = mcp.doprint(expr)\n    assert mathml_expr.startswith('<apply')\n    assert mathml_expr.endswith('</apply>')\n\n    # Test MathMLPresentationPrinter\n    expr = exp(x)\n    mpp = MathMLPresentationPrinter()\n    mathml_expr = mpp.doprint(expr)\n    assert mathml_expr.startswith('<mrow')\n    assert mathml_expr.endswith('</mrow>')\n\n    # Test mathml function with presentation printer\n    expr = exp(x)\n    mathml_expr = mathml(expr, printer='presentation')\n    assert mathml_expr.startswith('<mrow')\n    assert mathml_expr.endswith('</mrow>')\n\n    # Test mathml function with content printer\n    expr = exp(x)\n    mathml_expr = mathml(expr, printer='content')\n    assert mathml_expr.startswith('<apply')\n    assert mathml_expr.endswith('</apply>')\n\n    # Test mathml function with invalid printer\n    expr = exp(x)\n    with raises(TypeError, match='Invalid printer'):\n        mathml(expr, printer='invalid')\n\n    # Test mathml function with missing expr\n    with raises(TypeError, match='expr must be a sympy expression'):\n        mathml(None)\n\n", "def test_printing_FiniteSet():\n    finite_set = FiniteSet(1, 2, 3, 4, 5)\n    expected_mathml = '<mfenced close=\"}\" open=\"{\"><mrow><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mn>4</mn><mo>,</mo><mn>5</mn></mrow></mfenced>'\n    assert mathml(finite_set, printer='content') == expected_mathml\n\n", "def test_MathMLContentPrinter_MatrixSymbol():\n    from sympy import MatrixSymbol\n\n    matrix = MatrixSymbol('M', 2, 2)\n    mp = MathMLContentPrinter()\n    expected_output = '<mi><mtable columnalign=\"right left\" columnspacing=\"0em\" displaystyle=\"true\" rowspacing=\"3pt\"><mtr><mtd><mn>2</mn></mtd><mtd><mn>2</mn></mtd></mtr><mtr><mtd><mn>2</mn></mtd><mtd><mn>2</mn></mtd></mtr></mtable></mi>'\n    assert mp.doprint(matrix) == expected_output\n", "def test_MathMLContentPrinter_Derivative():\n    x = Symbol('x')\n    expr = Derivative(x**3, x)\n    expected = '<apply><diff/><bvar><ci>x</ci></bvar><lowlimit><cn>1</cn></lowlimit><uplimit><cn>oo</cn></uplimit><apply><power/><ci>x</ci><cn>3</cn></apply></apply>'\n    assert mathml(expr, printer='content') == expected\n", "def test_mathml_printer_infinite_sum():\n    expr = Sum(sin(x), (x, 0, oo))\n    assert mathml(expr) == '<apply><apply><int/> <ci>x</ci><ci>&#x2218;</ci><ci>0</ci><ci>&#x221E;</ci></apply><apply><sin/><ci>x</ci></apply></apply>'\n"], "sample_1042": ["def test_AssumptionsPropagation():\n    n, m, k = symbols('n m k', integer=True)\n    i, j = Idx('i', n), Idx('j', m)\n    A = IndexedBase('A', shape=(n, n), real=True)\n    B = A[i, j]\n    assert B.is_real\n    assert B.is_finite\n    assert B.assumptions0 == {'finite': True, 'real': True}\n\n    C = A[i, j, k]\n    assert C.is_real\n    assert C.is_finite\n    assert C.assumptions0 == {'finite': True, 'real': True}\n\n    D = IndexedBase('D')\n    assert not D.is_real\n    assert not D.is_finite\n    assert D.assumptions0 == {}\n\n    E = D[i, j, k]\n    assert not E.is_real\n    assert not E.is_finite\n    assert E.assumptions0 == {}\n\n    F = IndexedBase('F', commutative=False)\n    assert F.is_commutative == False\n    assert F.assumptions0 == {'commutative': False}\n\n    G = F[i, j, k]\n    assert G.is_commutative == False\n    assert G.assumptions0 == {'commutative': False}\n", "    def test_equality(self):\n        A = IndexedBase('A')\n        B = IndexedBase('B')\n        assert A == A\n        assert A != B\n        assert A != S(1)\n        assert A == IndexedBase('A')\n", "def test_assumptions():\n    x = IndexedBase('x', real=True)\n    i = Idx('i', 5)\n    A = x[i]\n    assert A.is_real\n    assert not A.is_imaginary\n    assert A.is_real is True\n    assert not A.is_imaginary is True\n\n    x = IndexedBase('x', integer=True)\n    i = Idx('i', 5)\n    A = x[i]\n    assert A.is_integer\n    assert not A.is_real\n\n    x = IndexedBase('x', positive=True)\n    i = Idx('i', 5)\n    A = x[i]\n    assert A.is_positive\n    assert not A.is_negative\n    assert A.is_positive is True\n    assert not A.is_negative is True\n\n    x = IndexedBase('x', negative=True)\n    i = Idx('i', 5)\n    A = x[i]\n    assert A.is_negative\n    assert not A.is_positive\n    assert A.is_negative is True\n    assert not A.is_positive is True\n", "def test_IndexedBase_assignment():\n    from sympy import IndexedBase, Idx, Indexed\n    from sympy.abc import x, y, z\n    A = IndexedBase('A', shape=(x, y))\n    i, j, k = map(Idx, 'ijk')\n    assert A[i, j, k].rank == 3\n    A[i, j, k] = 1\n    assert A[i, j, k] == 1\n    assert A[i, j].rank == 2\n    A[i, j] = 2\n    assert A[i, j] == 2\n    assert A.rank == 2\n    A.rank = 3\n    assert A.rank == 3\n    assert A[i, j].rank == 3\n    A[i, j] = 3\n    assert A[i, j] == 3\n    A[i, j, k] = 4\n    assert A[i, j, k] == 4\n    A.rank = 2\n    assert A.rank == 2\n    A[i, j] = 5\n    assert A[i, j] == 5\n    assert A[i, j, k] == 4\n", "def test_idx():\n    i, j, k = symbols('i j k', integer=True)\n    A = IndexedBase('A')\n    assert Idx('j').is_integer\n    assert Idx('j').is_finite\n    assert Idx('j').is_real\n    assert Idx('j').is_symbol\n    assert Idx('j').is_Atom\n    assert Idx('j')._diff_wrt\n\n    A = IndexedBase('A', shape=(4, 4))\n    A[i, j].shape\n    A[i, j].ranges\n    A[i, j].rank\n    A[i, j].base\n    A[i, j].indices\n    A[i, j].name\n    A[i, j].free_symbols\n    A[i, j].expr_free_symbols\n    A[i, j].assumptions0\n    A[i, j].free_symbols\n    assert A[i, j].lower == 0\n    assert A[i, j].upper == 3\n\n    A = IndexedBase('A', shape=(4, 4))\n    assert A[i, j].lower == 0\n    assert A[i, j].upper is None\n    assert A[i, j, 0].lower == 0\n    assert A[i, j, 0].upper is None\n\n    i = Idx('i', 4)\n    j = Idx('j', 3)\n    assert i.lower == 0\n    assert i.upper == 3\n    assert j.lower == 0\n    assert j.upper == 2\n    assert i <= 3\n    assert i >= 0\n    assert i < 4\n    assert i > 0\n    assert 0 <= i\n    assert 4 > i\n    assert 4 >= i\n    assert 0 < i\n\n    A = IndexedBase('A', shape=(4, 4))\n    assert A[i, j, 0] < A[i, j, 1]\n    assert A[i, j, 3] > A[i, j, 2]\n\n    i = Idx('i', oo)\n    assert i.lower == 0\n    assert i.upper == oo\n    assert i <= 5\n    assert i >= -5\n    assert i < oo\n    assert i > -5\n    assert -"], "sample_1073": ["def test_sqrt_denest_rec():\n    # Test with sqrt(-72*sqrt(2) + 158*sqrt(5) + 498)\n    w = -6*sqrt(55)-6*sqrt(35)-2*sqrt(22)-2*sqrt(14)+2*sqrt(77)+6*sqrt(10)+65\n    z = _sqrtdenest_rec(sqrt(w))\n    assert z.equals(-sqrt(11) - sqrt(7) + sqrt(2) + 3*sqrt(5))\n", "def test_is_sqrt():\n    assert not is_sqrt(1)\n    assert is_sqrt(sqrt(4))\n    assert is_sqrt(sqrt(sqrt(16)))\n    assert is_sqrt(sqrt(r2))\n    assert is_sqrt(r2**2)\n    assert not is_sqrt(1 + sqrt(2))\n    assert not is_sqrt(1 + sqrt(2) + sqrt(3))\n", "def test_sqrtdenest_more_cases():\n    x = Symbol('x')\n    w = sqrt(x + 3)\n    # Test cases from [2] page 4\n    z = w + 1 + sqrt(2)\n    assert sqrtdenest((z**2).expand()).doit() == w + 1 + sqrt(1 + w + 2)\n    z = w + 1 + sqrt(2)*sqrt(3)\n    assert sqrtdenest((z**2).expand()).doit() == (w + 1 + sqrt(2)*sqrt(3)).simplify()\n    z = w + 1 + sqrt(2)\n    assert sqrtdenest((z**2).expand()).doit() == w + 1 + sqrt(1 + w + 2)\n    z = w + 1 + sqrt(2)*sqrt(6)\n    assert sqrtdenest((z**2).expand()).doit() == (w + 1 + sqrt(2)*sqrt(6)).simplify()\n    z = w + 1 + sqrt(2)*sqrt(3) + sqrt(2)\n    assert sqrtdenest((z**2).expand()).doit() == (w + 1 + sqrt(2)*sqrt(3) + sqrt(2)).simplify()\n    # Test case from [1] page 2\n    z = -sqrt(5) + 1 + sqrt(2)\n    assert sqrtdenest((z**2).expand()).doit() == -sqrt(2) + 1 + sqrt(3)\n    # Test case from [1] page 2\n    z = 2*sqrt(r5) + 3*sqrt(r2)\n    assert sqrtdenest(z).simplify() == 2*sqrt(5) + 3*sqrt(2)\n    # Test case from [1] page 2\n    z = 2*sqrt(r5) + 3*sqrt(r3)\n    assert sqrtdenest(z).simplify() == 3*sqrt(r5) + 2*sqrt(3)\n    # Test case from [1] page 2\n    z = 2*sqrt(r5) + 3*sqrt(r2) + 1\n    assert sqrtdenest(z).simplify() == 2*sqrt", "def test_sqrt_biquadratic_denest_nested():\n    from sympy import sqrt, Symbol, cos, S\n    x = Symbol('x', real=True)\n    r = sqrt(6)\n    z = sqrt(sqrt(5) + 3*r + 2*r**2)\n    assert sqrtdenest(z) == -sqrt(2) + 2*sqrt(2)*r + 3*r + sqrt(5)\n", "def test_denesting_with_fourth_root():\n    z = sqrt(-sqrt(r2)*sqrt(r3) + sqrt(r3)*sqrt(r5) + r6)\n    assert sqrtdenest(z) == -r2 + r3 + sqrt(r5) + sqrt(r15)\n"], "sample_1027": ["def test_poly_attrs():\n    from sympy import Poly\n    from sympy.abc import x, y\n\n        for attr in attrs:\n            assert hasattr(f, attr)\n            assert callable(getattr(f, attr))\n            assert isinstance(getattr(f, attr)(), Poly)\n\n    f = Poly(x**2 + 2*x + 1)\n    attrs = [\n        'degree',\n        'degree_list',\n        'total_degree',\n        'homogeneous_order',\n        'LC',\n        'TC',\n        'EC',\n        'coeff_monomial',\n        'nth',\n        'coeff',\n        'LM',\n        'EM',\n        'LT',\n        'ET',\n        'max_norm',\n        'l1_norm',\n        'clear_denoms',\n        'eval',\n        'half_gcdex',\n        'gcdex',\n        'invert',\n        'revert',\n        'subresultants',\n        'resultant',\n        'discriminant',\n        'dispersion',\n        'dispersionset',\n        'cofactors',\n        'gcd',\n        'lcm',\n        'trunc',\n        'monic',\n        'content',\n        'primitive',\n        'compose',\n        'decompose',\n        'shift',\n        'transform',\n        'sturm',\n        'gff_list',\n        'norm',\n        'sqf_norm',\n        'sqf_part',\n        'sqf_list',\n        'factor_list',\n        'factor',\n        'cancel',\n        'ground_roots',\n        'root',\n        'real_roots',\n        'all_roots',\n        'nroots',\n        'refine_root',\n        'count_roots',\n        'is_zero',\n        'is_one',\n        'is_sqf',\n        'is_monic',\n        'is_primitive',\n        'is_ground',\n        'is_linear',\n        'is_quadratic',\n        'is_monomial',\n        'is_homogeneous',\n        'is_irreducible',\n        'is_univariate',\n        'is_multivariate',\n        'is_cyclotomic',\n    ]\n    _check_attrs(f, attrs)\n\n    f = Poly(x*y**2 + x*y + 1)\n    attrs = [\n        'degree',\n        'degree_list',\n        'total_degree',\n        'homogeneous_order',\n        'LC',\n        'TC',\n        'EC',\n        'coeff_monomial',\n", "def test_LC():\n    # Test basic usage\n    p = Poly(x**2 + 2*x*y**2 + x*y + 3*y, x, y)\n    assert LC(p) == 1\n\n    # Test with multiple generators\n    p = Poly(x*y**2 + x*y + 1, x, y)\n    assert LC(p) == 1\n\n    # Test with a non-trivial leading coefficient\n    p = Poly(2*x**2 + 2*x*y**2 + x*y + 3*y, x, y)\n    assert LC(p) == 2\n\n    # Test with a non-univariate polynomial\n    p = Poly(x**2*y**2 + x*y + 1, x, y)\n    assert LC(p) == 1\n\n    # Test with a non-polynomial input\n    p = Poly(2*x + 3, x)\n    assert LC(p) == 2\n\n    # Test with a polynomial with a complex coefficient\n    p = Poly(2*x + 3*I, x)\n    assert LC(p) == 2\n\n    # Test with a polynomial with a rational coefficient\n    p = Poly(2*x + 3/2, x)\n    assert LC(p) == 2\n\n    # Test with a polynomial with an irrational coefficient\n    p = Poly(2*x + sqrt(2), x)\n    assert LC(p) == 2\n\n    # Test with a polynomial with a negative leading coefficient\n    p = Poly(-2*x + 3, x)\n    assert LC(p) == -2\n\n    # Test with a zero polynomial\n    p = Poly(0, x)\n    assert LC(p) == 0\n\n    # Test with a one polynomial\n    p = Poly(1, x)\n    assert LC(p) == 1\n\n    # Test with a zero polynomial with multiple generators\n    p = Poly(0, x, y)\n    assert LC(p) == 0\n\n    # Test with a polynomial that has a factor with a zero coefficient\n    p = Poly(x**2*y**2 + x*y + 0, x, y)\n    assert LC(p) == 1\n\n    # Test with a polynomial that has a factor with a negative coefficient\n    p = Poly(x**2*y**2 + x*y - 3, x, y)\n    assert LC(p) == ", "    def test_LM():\n        # Test LM() with a polynomial\n        p = Poly(x**2 + 2*x*y**2 + x*y + 3*y, x, y)\n        assert LM(p) == x**2\n        assert LM(p, x) == x**2\n        assert LM(p, y) == y**2\n        assert LM(p, (x, y)) == x**2\n        # Test LM() with a monomial\n        p = Poly(4*x**2 + 2*x*y**2 + x*y + 3*y, x, y)\n        assert LM(p, x) == x**2\n        assert LM(p, y) == y**2\n        # Test LM() with a univariate polynomial\n        p = Poly(x**2 + 2*x*y**2 + x*y + 3*y, x)\n        assert LM(p, x) == x**2\n        assert LM(p, y) == y**2\n", "def test_terms_gcd():\n    x, y = symbols('x y')\n\n    # Test that terms_gcd does not modify the expression if it's not a polynomial\n    assert terms_gcd(x + 1) == x + 1\n\n    # Test that terms_gcd works with a single polynomial\n    p = Poly(x**2 + 2*x*y**2 + x*y + 3*y)\n    assert terms_gcd(p).as_expr() == x**2 + y*(x + 3)\n\n    # Test that terms_gcd works with multiple polynomials\n    p1 = Poly(x**2 + 2*x*y**2 + x*y + 3*y)\n    p2 = Poly(x + y)\n    assert terms_gcd(p1 + p2).as_expr() == (x + y)*(x + y + 3)\n\n    # Test that terms_gcd works with deep=True\n    assert terms_gcd((x + 1)*(y + 1)).as_expr() == x*y + x + y + 1\n\n    # Test that terms_gcd works with clear=False\n    assert terms_gcd(x/3 + y/2, clear=False).as_expr() == y*(x/3 + 2/3) + x/3\n\n    # Test that terms_gcd works with expand=False\n    assert terms_gcd((x + 1)*(y + 1), expand=False).as_expr() == (x + 1)*(y + 1)\n\n    # Test that terms_gcd works with deep=True and expand=False\n    assert terms_gcd((x + 1)*(y + 1), expand=False, deep=True).as_expr() == x*y + x + y + 1\n\n    # Test that terms_gcd works with a polynomial and a non-polynomial\n    assert terms_gcd(x + 1, p).as_expr() == x + 1\n\n    # Test that terms_gcd raises an error when the expression is not a polynomial\n    with raises(PolynomialError):\n        terms_gcd(1 + x)\n\n    # Test that terms_gcd raises an error when the expression is a constant\n    with raises(PolynomialError):\n        terms_gcd(1)\n\n    # Test that terms_gcd raises an error when the expression is a number\n    with raises(PolynomialError):\n        terms_gcd(5)\n", "def test_refine_root():\n    from sympy import sqrt, Symbol, oo, I, sqrt, Poly, Eq\n    from sympy.abc import x\n\n    # real root\n    f = Poly(x**2 - 3)\n    f1 = Poly(x**2 - 3, x)\n    assert refine_root(f, 1, 2) == (1.732050807568878, 1.732050807568878)\n    assert refine_root(f1, 1, 2, eps=1e-6) == (1.732050807568878, 1.732050807568878)\n\n    # complex root\n    f = Poly(x**2 + 1)\n    f1 = Poly(x**2 + 1, x)\n    assert refine_root(f, 1, 2) == (oo, oo)\n    assert refine_root(f1, 1, 2, eps=1e-6) == (oo, oo)\n\n    # real root with extension\n    f = Poly((x - sqrt(3))*(x + sqrt(3)), x, domain=ZZ)\n    f1 = Poly((x - sqrt(3))*(x + sqrt(3)), x, domain=QQ)\n    assert refine_root(f, 1, 2) == (1, 2)\n    assert refine_root(f1, 1, 2, eps=1e-6) == (1.00000000000000, 2.00000000000000)\n\n    # real root with square root of rational\n    f = Poly(x**2 - 3)\n    f1 = Poly(x**2 - 3, x)\n    assert refine_root(f, 1, 2) == (sqrt(3), sqrt(3))\n    assert refine_root(f1, 1, 2, eps=1e-6) == (1.732050807568878, 1.732050807568878)\n\n    # square-free polynomial\n    f = Poly(x**2 - 3)\n    f1 = Poly(x**2 - 3, x)\n    assert refine_root(f, 1, 2) == (1.732050807568878, 1.732050807568878)\n    assert refine_root(f1, 1, 2, eps=1e-6) =="], "sample_394": ["    def setUp(self):\n        super().setUp()\n        self.client.force_login(self.superuser)\n        self.g1 = Gallery.objects.create(name=\"Test gallery\")\n        self.g2 = Gallery.objects.create(name=\"Test gallery 2\")\n        self.ch = Chapter.objects.create(title=\"Test chapter\", book=None)\n        self.ch2 = Chapter.objects.create(title=\"Test chapter 2\", book=None)\n        self.p1 = Picture.objects.create(gallery=self.g1, chapter=self.ch)\n        self.p2 = Picture.objects.create(gallery=self.g2, chapter=self.ch2)\n", "    def test_change_view_with_fields(self):\n        response = self.client.get(\"/admin/books/book/1/change/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"change_form\", response.context)\n", "    def test_fieldset_to_str(self):\n        response = self.client.get(reverse('admin:admin_views_article_changelist'))\n        self.assertEqual(len(response.context['cl'].fieldsets), 1)\n        self.assertEqual(response.context['cl'].fieldsets[0][0], \"Content\")\n", "    def test_inline_related_model_formfield_override(self):\n        response = self.client.get(reverse('admin:adminorderedmodelmethod_changelist'))\n        form = response.context['adminform'].form\n        field = form.base_fields['method']\n        self.assertEqual(field.widget.attrs['class'], 'django.forms.widgets.Textarea')\n", "    def test_inline_formset_submit(self):\n        response = self.client.get(\"/admin/section/section/\")\n\n        # Extract the formset prefix and the formset fields.\n        formset_prefix = \"article_set\"\n        fields = self.get_admin_form_fields(response)\n\n        # Extract the initial data for the formset.\n        initial_data = []\n        for field in fields:\n            if field.field[\"name\"].startswith(formset_prefix):\n                initial_data.append(field.field[\"name\"])\n\n        # Submit the formset with initial data and check the response status code.\n        data = {field: field.initial for field in initial_data}\n        data.update(self.inline_post_data)\n        response = self.client.post(\n            \"/admin/section/section/\",\n            data,\n            follow=True,\n        )\n        self.assertEqual(response.status_code, 200)\n\n        # Extract the AdminForm instance from the response context.\n        admin_form = response.context[\"adminform\"]\n\n        # Check if the AdminForm instance has any errors.\n        self.assertFalse(admin_form.is_valid(), \"AdminForm is not valid\")\n\n        # Check the readonly fields of the AdminForm instance.\n        self.assertEqual(\n            self.get_admin_readonly_fields(response),\n            [\"title\", \"content\", \"date\", \"section\", \"published\", \"slug\"],\n        )\n\n        # Check the readonly fields of the inline formset.\n        inline_fields = []\n        for fieldset in response.context[\"inline_admin_formsets\"][0]:\n            for field_line in fieldset:\n                inline_fields.extend(field_line)\n        self.assertEqual(\n            [f.field[\"name\"] for f in inline_fields],\n            [\"article_set-TOTAL_FORMS\", \"article_set-INITIAL_FORMS\", \"article_set-MAX_NUM_FORMS\"],\n            [\"article_set-0-id\", \"article_set-0-title\", \"article_set-0-content\", \"article_set-0-date_0\", \"article_set-0-date_1\", \"article_set-0-section\", \"article_set-0-hidden\"],\n            [\"article_set-1-id\", \"article_set-1-title\", \"article_set-1-content\", \"article_set-1-date_0\", \"article_set-1-date_1\", \"article_set-1-section\", \"article_set-1-hidden\"],\n            [\"article_set-2-id\", \"article_set-2-title\", \"article_set-2-content\", \"article_set-2-date_0\", \"article_set"], "sample_84": ["    def test_urlsafe_base64_encode_empty_string(self):\n        self.assertEqual(urlsafe_base64_encode(b\"\"), \"\")\n", "    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('not a date')\n", "    def test_urlsafe_base64_decode_empty_string(self):\n        self.assertEqual(urlsafe_base64_decode(''), b'')\n", "    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('not a valid date')\n", "    def test_urlunquote(self):\n        self.assertEqual(urlunquote('Hello World!'), 'Hello World!')\n        self.assertEqual(urlunquote('Hello%20World!'), 'Hello World!')\n        self.assertEqual(urlunquote('Hello%20%20World!'), 'Hello  World!')\n"], "sample_192": ["    def test_base_formset_init(self):\n        formset = BaseFormSet()\n        self.assertTrue(formset.is_bound)\n        self.assertIsNone(formset.prefix)\n        self.assertEqual(formset.auto_id, 'id_%s')\n        self.assertEqual(formset.data, {})\n        self.assertEqual(formset.files, {})\n        self.assertIsNone(formset.initial)\n        self.assertEqual(formset.form_kwargs, {})\n        self.assertEqual(formset.error_class, ErrorList)\n        self.assertIsNone(formset._errors)\n        self.assertIsNone(formset._non_form_errors)\n", "    def test_management_form_is_valid(self):\n        formset = ChoiceFormSet(prefix='choices', data={\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1000',\n            'choices-0-choice': 'choice1',\n            'choices-0-votes': '10',\n        })\n        self.assertTrue(formset.management_form.is_valid())\n", "    def test_formset_factory_default_min_num(self):\n        ChoiceFormSet = formset_factory(Choice)\n        self.assertEqual(ChoiceFormSet().min_num, DEFAULT_MIN_NUM)\n", "    def test_formset_factory_default_formset(self):\n        FormSet = formset_factory(form=Choice)\n        self.assertIsInstance(FormSet, BaseFormSet)\n", "    def test_management_form_validation(self):\n        formset = ChoiceFormSet()\n        form = formset.management_form\n        with self.assertRaises(ValidationError):\n            form.is_valid()\n        self.assertFalse(formset.is_bound)\n\n        formset.data = {\n            'TOTAL_FORMS': '1',\n            'INITIAL_FORMS': '0',\n            'MAX_NUM_FORMS': '1',\n        }\n        form = formset.management_form\n        form.is_valid()\n        self.assertTrue(formset.is_bound)\n"], "sample_643": ["def test_colorize_ansi_color_deprecation(tmp_path):\n    # Arrange\n    with warnings.catch_warnings(record=True) as warning_list:\n        warnings.simplefilter(\"always\")\n\n        # Act\n        msg_style = \"red\"\n        colorized_msg = colorize_ansi(\"test message\", msg_style)\n\n        # Assert\n        assert colorized_msg == f\"\\033[31mtest message\\033[0m\"\n        assert len(warning_list) == 1\n        assert str(warning_list[0].message) == (\n            \"In pylint 3.0, the colorize_ansi function of Text reporters will only accept a MessageStyle parameter\"\n        )\n", "def test_colorize_ansi_no_style():\n    reporter = TextReporter()\n    msg = Message(\"test message\", msg_id=\"I\", category=\"INFO\")\n    assert colorize_ansi(msg.msg) == \"test message\"\n\n", "def test_colorize_ansi(tmp_path):\n    with open(\"temp.txt\", \"w\") as f:\n        sys.stdout = f\n\n        msg_style = MessageStyle(\"green\")\n        assert colorize_ansi(\"Hello, World!\", msg_style) == f\"\\033[32mHello, World!\\033[0m\"\n\n        msg_style = MessageStyle(\"red\")\n        assert colorize_ansi(\"Hello, World!\", msg_style) == f\"\\033[31mHello, World!\\033[0m\"\n\n        msg_style = MessageStyle(\"red\", (\"bold\",))\n        assert colorize_ansi(\"Hello, World!\", msg_style) == f\"\\033[1;31mHello, World!\\033[0m\"\n\n        msg_style = MessageStyle(\"red\", (\"bold\", \"italic\"))\n        assert colorize_ansi(\"Hello, World!\", msg_style) == f\"\\033[3;1;31mHello, World!\\033[0m\"\n\n        # Check for invalid color and style\n        msg_style = MessageStyle(\"invalid\", (\"invalid\",))\n        assert colorize_ansi(\"Hello, World!\", msg_style) == \"Hello, World!\"\n\n        # Check for no style and no color\n        msg_style = MessageStyle(None, ())\n        assert colorize_ansi(\"Hello, World!\", msg_style) == \"Hello, World!\"\n\n    sys.stdout = sys.__stdout__\n\n", "def linter_with_reporters():\n    \"\"\"Create a linter with reporters.\"\"\"\n    linter = PyLinter()\n    TextReporter.register(linter)\n    ParseableTextReporter.register(linter)\n    VSTextReporter.register(linter)\n    ColorizedTextReporter.register(linter)\n    return linter\n\n", "def reporter_with_color_mapping():\n    return ColorizedTextReporter()\n\n"], "sample_1040": ["def test_mathml_derivative():\n    from sympy.printing.mathml import mathml, MathMLContentPrinter\n    from sympy import diff, sin, x\n    mp = MathMLContentPrinter()\n    expr = diff(sin(x), x)\n    mathml_expr = mathml(expr, printer='content')\n    expected = \"\"\"<apply><diff/><bvar><ci>x</ci></bvar><lowlimit/><uplimit/><int><apply><sin/><ci>x</ci></apply></int></apply>\"\"\"\n    assert mathml_expr == expected\n", "def test_mathml_printing_random_symbol():\n    from sympy.printing.mathml import mathml, MathMLContentPrinter\n    from sympy.stats.rv import RandomSymbol\n    from sympy import pi\n    u = RandomSymbol('u', 'uniform', (0, 2*pi))\n    content_mathml = mathml(u)\n    presentation_mathml = mathml(u, printer='presentation')\n    assert content_mathml.startswith('<apply><ci>')\n    assert presentation_mathml.startswith('<mrow><mi>')\n    assert 'uniform' in content_mathml\n    assert 'uniform' in presentation_mathml\n", "def test_mathml_printing():\n    from sympy.printing.mathml import MathMLContentPrinter\n    from sympy.abc import x, y\n\n    # Test mathml function with content printer\n    mathml_content = mathml(x + 1, printer='content')\n    assert isinstance(mathml_content, str)\n    assert '<apply>' in mathml_content\n    assert '<plus/>' in mathml_content\n    assert '<ci>x</ci>' in mathml_content\n    assert '<cn>1</cn>' in mathml_content\n\n    # Test mathml function with presentation printer\n    mathml_presentation = mathml(x + 1, printer='presentation')\n    assert isinstance(mathml_presentation, str)\n    assert '<mrow>' in mathml_presentation\n    assert '<mi>x</mi>' in mathml_presentation\n    assert '<mo>+</mo>' in mathml_presentation\n    assert '<mn>1</mn>' in mathml_presentation\n\n    # Test mathml function with invalid printer\n    with raises(TypeError):\n        mathml(x + 1, printer='invalid')\n\n    # Test print_mathml function with content printer\n    print_mathml_content = print_mathml(x + 1, printer='content')\n    assert isinstance(print_mathml_content, NoneType)  # print statement should not return anything\n\n    # Test print_mathml function with presentation printer\n    print_mathml_presentation = print_mathml(x + 1, printer='presentation')\n    assert isinstance(print_mathml_presentation, NoneType)  # print statement should not return anything\n\n    # Test print_mathml function with invalid printer\n    with raises(TypeError):\n        print_mathml(x + 1, printer='invalid')\n\n    # Test MathMLContentPrinter with a more complex expression\n    mathml_content_complex = mathml(x**2 + 2*x + 1, printer='content')\n    assert isinstance(mathml_content_complex, str)\n    assert '<apply>' in mathml_content_complex\n    assert '<plus/>' in mathml_content_complex\n    assert '<times/>' in mathml_content_complex\n    assert '<ci>x</ci>' in mathml_content_complex\n    assert '<cn>2</cn>' in mathml_content_complex\n\n    # Test MathMLPresentationPrinter with a more complex expression\n    mathml_presentation_complex = mathml(x**2 + 2*x + 1, printer='presentation", "def test_mathml_matrix():\n    M = Matrix([[1, 2], [3, 4]])\n    assert mpp.doprint(M) == (\n        '<mtable columnalign=\"left left\" columnspacing=\"1em\" displaystyle=\"true\" '\n        'rowalign=\"axis\" rowlines=\"none\" showcolumnroles=\"false\" '\n        '<mtd columnalign=\"left\" columnspacing=\"1em\" displaystyle=\"false\" '\n        'rowsep=\"3pt\"><mtable columnalign=\"left left\" columnspacing=\"1em\" '\n        'displaystyle=\"false\" rowalign=\"axis\" rowlines=\"none\" showcolumnroles=\"false\">'\n        '<mtr><mtd columnalign=\"left\" columnspacing=\"1em\" displaystyle=\"false\" '\n        'rowsep=\"3pt\"><mn>1</mn></mtd><mtd columnalign=\"left\" columnspacing=\"1em\" '\n        'displaystyle=\"false\" rowspacing=\"3pt\"><mn>2</mn></mtd></mtr></mtable></mtd>'\n        '<mtd columnalign=\"left\" columnspacing=\"1em\" displaystyle=\"false\" rowspacing=\"3pt\">'\n        '<mn>3</mn></mtd><mtd columnalign=\"left\" columnspacing=\"1em\" displaystyle=\"false\" '\n        'rowsep=\"3pt\"><mn>4</mn></mtd></mtr></mtable></math>'\n    )\n    assert mp.doprint(M) == (\n        '<apply><matrix><matrixrow><ci>a11</ci><ci>a12</ci></matrixrow>'\n        '<matrixrow><ci>a21</ci><ci>a22</ci></matrixrow></matrix></apply>'\n    )\n", "def test_MathMLContentPrinter__print_Pow_root_notation():\n    from sympy import Symbol, Pow, Rational\n    from sympy.printing.mathml import MathMLContentPrinter\n    mp = MathMLContentPrinter()\n    x = Symbol('x')\n    p = Pow(x, Rational(1, 2))\n    assert mp._print_Pow(p).toxml() == '<apply><root><ci>power</ci><mrow><mi>x</mi></mrow><cn>2</cn></root></apply>'\n"], "sample_581": ["def test_blueprint_make_setup_state():\n    app = flask.Flask(__name__)\n    blueprint = Blueprint(\"test_blueprint\", __name__)\n    state = blueprint.make_setup_state(app, {})\n    assert state.app == app\n    assert state.blueprint == blueprint\n    assert state.options == {}\n    assert state.first_registration == True\n    assert state.subdomain is None\n    assert state.url_prefix is None\n    assert state.name == \"test_blueprint\"\n    assert state.url_defaults == {}\n", "def test_blueprint_register_blueprint():\n    app = flask.Flask(__name__)\n    bp = Blueprint(\"test_bp\", __name__)\n\n    # Record a function that raises an error when the blueprint is registered\n        raise ValueError(\"Test error\")\n\n    bp.record(record_error)\n\n    # Register the blueprint\n    with pytest.raises(ValueError) as e:\n        bp.register(app, {})\n\n    # Check that the error was raised in the record method\n    assert \"Test error\" in str(e.value)\n\n    # Register the blueprint again\n    with pytest.raises(ValueError) as e:\n        bp.register(app, {})\n\n    # Check that the same error is raised again\n    assert \"Test error\" in str(e.value)\n", "def test_blueprint_app_errorhandler_blueprint_errorhandler():\n    app = flask.Flask(__name__)\n    blueprint = Blueprint(\"test_blueprint\", __name__)\n\n    @blueprint.app_errorhandler(404)\n        return \"Not found\", 404\n\n    @blueprint.errorhandler(404)\n        return \"Not found\", 404\n\n    with pytest.raises(ValueError):\n        blueprint.register(app, {})\n\n    blueprint.register(app, {\"name\": \"test_blueprint\"})\n\n    response = app.test_client().get(\"/non-existent\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    with pytest.raises(TemplateNotFound):\n        app.test_client().get(\"/template\")\n\n    response = app.test_client().get(\"/non-existent\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    app.register_blueprint(blueprint, url_prefix=\"/blueprint\")\n\n    response = app.test_client().get(\"/blueprint/non-existent\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    response = app.test_client().get(\"/blueprint/template\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    with pytest.raises(TemplateNotFound):\n        app.test_client().get(\"/blueprint/template\")\n\n    response = app.test_client().get(\"/non-existent\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    blueprint.register(app, {\"name\": \"test_blueprint\", \"url_prefix\": \"/blueprint2\"})\n\n    response = app.test_client().get(\"/blueprint2/non-existent\")\n    assert response.status_code == 404\n    assert response.data.decode() == \"Not found\"\n\n    with pytest.raises(TemplateNotFound):\n        app.test_client().get(\"/blueprint2/template\")\n", "def test_blueprint_add_url_rule_with_empty_prefix():\n    app = flask.Flask(__name__)\n    bp = Blueprint(\"test_bp\", __name__, url_prefix=\"\")\n    bp.add_url_rule(\"/test\", view_func=lambda: \"test\")\n    assert \"/test\" in app.url_map\n    assert app.url_map[\"/test\"].rule == \"/\"\n    assert app.url_map[\"/test\"].endpoint == \"test_bp.test\"\n", "def test_add_url_rule_register_multiple_rules_with_same_endpoint():\n    app = flask.Flask(__name__)\n    class TestBlueprint(Blueprint):\n            super().__init__(\"test_blueprint\", __name__, url_prefix=\"/api\")\n\n        @setupmethod\n            self.record(lambda s: s.add_url_rule(rule, endpoint, view_func, **options))\n\n    blueprint = TestBlueprint()\n    blueprint.add_url_rule(\"/rule1\", \"endpoint1\", None)\n    blueprint.add_url_rule(\"/rule2\", \"endpoint1\", None)\n\n    state = blueprint.make_setup_state(app, {})\n    with pytest.raises(AssertionError) as excinfo:\n        blueprint.register(app, {})\n    assert \"The endpoint 'endpoint1' is already registered for this blueprint\" in str(excinfo.value)\n"], "sample_993": ["def test_zero_mul_simp():\n    # test zero_mul_simp function to combine two reduced words\n    from sympy.combinatorics.free_groups import zero_mul_simp\n    from sympy.combinatorics.free_groups import FreeGroup\n    F = FreeGroup(\"a, b\")\n    a = F.dtype(((F.symbols[0], 3),))\n    b = F.dtype(((F.symbols[1], 4),))\n    ab = a*b\n    zero_mul_simp(ab.array_form, 0)\n    assert ab.array_form == ((F.symbols[0], 3), (F.symbols[1], 4))\n    ab = b*a\n    zero_mul_simp(ab.array_form, 0)\n    assert ab.array_form == ((F.symbols[0], 3), (F.symbols[1], 4))\n    ab = a*a\n    zero_mul_simp(ab.array_form, 0)\n    assert ab.array_form == ((F.symbols[0], 6),)\n    ab = b*b\n    zero_mul_simp(ab.array_form, 0)\n    assert ab.array_form == ((F.symbols[1], 8),)\n    ab = a*b*b\n    zero_mul_simp(ab.array_form, 1)\n    assert ab.array_form == ((F.symbols[0], 3), (F.symbols[1], 5))\n    ab = a*a*a\n    zero_mul_simp(ab.array_form, 0)\n    assert ab.array_form == ((F.symbols[0], 9),)\n", "def test_free_group_order():\n    F, x, y, z = free_group(\"x, y, z\")\n    assert F.order() == oo\n    assert F[0].order() == 1\n    assert (x*y).order() == oo\n    assert (x*y**-1).order() == oo\n    assert (x*y*z).order() == oo\n    assert (x*y*z**-1).order() == oo\n", "def test_FreGroupElement_inverse():\n    F = FreeGroup(\"x, y\")\n    assert F.identity.inverse() == F.identity\n    assert (F.identity * F.x).inverse() == F.x**-1\n    assert (F.x * F.y).inverse() == F.y**-1 * F.x**-1\n    assert (F.x**2 * F.y).inverse() == F.y**-1 * F.x**-2\n    assert (F.x * F.y * F.x).inverse() == F.x**-1 * F.y**-1 * F.x**-1\n    assert (F.x**-1 * F.y).inverse() == F.y**-1 * F.x\n    assert (F.x**-1 * F.y * F.x).inverse() == F.x * F.y * F.x\n    assert (F.x**-1 * F.y**-1 * F.x).inverse() == F.x**-1 * F.y\n", "def test_zero_mul_simp():\n    from sympy.combinatorics.free_groups import zero_mul_simp\n    assert zero_mul_simp([(x, 1), (x, 2)], 0) == [(x, 3)]\n    assert zero_mul_simp([(x, 2), (x, 1), (x, 1)], 0) == [(x, 4)]\n    assert zero_mul_simp([(x, 1), (y, 2), (x, 1)], 0) == [(x, 2), (y, 2)]\n    assert zero_mul_simp([(x, 0), (x, 1)], 0) == [(x, 1)]\n    assert zero_mul_simp([(x, 0), (x, 0)], 0) == []\n    assert zero_mul_simp([], 0) == []\n", "def test_subword_index():\n    F, a, b = free_group(\"a, b\")\n    w = a**5*b*a**2*b**-4*a\n    assert w.subword_index(a**2*b) == 4\n    assert w.subword_index(a*b) == 1\n    assert w.subword_index(a*b*a**-1) == 2\n    assert w.subword_index(a*b*a) == 0  # returns index of first occurrence\n\n    w = a**5*b*a**2\n    assert w.subword_index(a*b*a) == 0  # returns index of first occurrence\n    assert w.subword_index(a*b**2*a) == 1\n    assert w.subword_index(a*b**-2*a) == 0  # returns index of first occurrence\n\n    with raises(ValueError):\n        w.subword_index(a*b**2*a, 0)  # invalid start index\n\n    w = a**5\n    assert w.subword_index(a) == 0  # returns index of first occurrence\n    with raises(ValueError):\n        w.subword_index(a, -1)  # invalid start index\n    with raises(ValueError):\n        w.subword_index(a, 5)  # invalid end index\n"], "sample_187": ["    def test_capfirst(self):\n        # Test capfirst function\n        self.assertEqual(text.capfirst(\"hello\"), \"Hello\")\n        self.assertEqual(text.capfirst(\"\"), \"\")\n        self.assertEqual(text.capfirst(\"hello world\"), \"Hello world\")\n        self.assertEqual(text.capfirst(\"hello world!\"), \"Hello world!\")\n", "def test_truncator_chars(self):\n    truncator = text.Truncator('This is a very long string')\n    self.assertEqual(truncator.chars(15), 'This is a very lo\u2026')\n    self.assertEqual(truncator.chars(5), 'This\u2026')\n    self.assertEqual(truncator.chars(0), '')\n    self.assertEqual(truncator.chars(15, '...'), 'This is a very lo...')\n    self.assertEqual(truncator.chars(15, ''), 'This is a very lo')\n", "    def test_smart_split(self):\n        self.assertEqual(list(smart_split(\"This is 'a person's' test.\")), ['This', \"'a person\\\\'s'\", 'test.'])\n        self.assertEqual(list(smart_split('Another \"person\\'s\" test.')), ['Another', '\"person\\\\\\'s\"', 'test.'])\n        self.assertEqual(list(smart_split('A \"funky\" style test.')), ['A', '\"funky\"', 'style test.'])\n", "    def test_get_valid_filename(self):\n        self.assertEqual(get_valid_filename(\"john's portrait in 2004.jpg\"), \"johns_portrait_in_2004.jpg\")\n        self.assertEqual(get_valid_filename(\"This is a valid filename!\"), \"This_is_a_valid_filename!\")\n        self.assertEqual(get_valid_filename(\"   leading spaces   \"), \"leading_spaces\")\n        self.assertEqual(get_valid_filename(\"   trailing spaces   \"), \"trailing_spaces\")\n        self.assertEqual(get_valid_filename(\"no spaces here\"), \"nospaceshere\")\n        self.assertEqual(get_valid_filename(\"\"), \" \")\n        self.assertEqual(get_valid_filename(\"  \"), \" \")\n        self.assertEqual(get_valid_filename(\"hello world\"), \"hello_world\")\n        self.assertEqual(get_valid_filename(\"hello-world\"), \"hello-world\")\n        self.assertEqual(get_valid_filename(\"hello_world\"), \"hello_world\")\n", "    def test_smart_split(self):\n        # Test with single quoted string\n        self.assertEqual(list(text.smart_split(r\"'Hello world'\")), [\"'Hello world'\"])\n        self.assertEqual(list(text.smart_split(r\"'Hello ')), [\"'Hello \"])\n        self.assertEqual(list(text.smart_split(r\"'world'\")), [\"'world'\"])\n\n        # Test with double quoted string\n        self.assertEqual(list(text.smart_split(r'\"Hello world\"')), [\"\\\"Hello world\\\"\"])\n        self.assertEqual(list(text.smart_split(r'\"Hello \")), [\"\\\"Hello \"])\n        self.assertEqual(list(text.smart_split(r'\"world\"')), [\"\\\"world\\\"\"])\n\n        # Test with escaped quotes\n        self.assertEqual(list(text.smart_split(r\"Hello 'world'\",)), [\"Hello 'world'\"])\n        self.assertEqual(list(text.smart_split(r\"Hello \\\"world\\\"\")), [\"Hello \\\"world\\\"\"])\n\n        # Test with mixed quotes\n        self.assertEqual(list(text.smart_split(r\"'Hello ' \\\"world\\\" ' Foo'\")), [\"'Hello ' \\\"world\\\" ' Foo'\"])\n\n        # Test with empty string\n        self.assertEqual(list(text.smart_split('')), [])\n\n        # Test with single quote in the middle of a string\n        self.assertEqual(list(text.smart_split(r\"Hello 'world' test\")), [\"Hello 'world' test\"])\n\n        # Test with double quote in the middle of a string\n        self.assertEqual(list(text.smart_split(r\"Hello \\\"world\\\" test\")), [\"Hello \\\"world\\\" test\"])\n\n        # Test with double quotes at the start and end of a string\n        self.assertEqual(list(text.smart_split(r'\"test\"')), [\"\\\"test\\\"\"])\n"], "sample_103": ["    def test_Aggregate_set_source_expressions(self):\n        # Test that set_source_expressions handles a non-filter expression.\n        aggregate = Aggregate(expression=1)\n        self.assertEqual(aggregate.get_source_expressions(), [1])\n        aggregate.set_source_expressions([F('name')])\n        self.assertEqual(aggregate.get_source_expressions(), [F('name')])\n\n        # Test that set_source_expressions handles a filter expression.\n        aggregate = Aggregate(expression=1, filter=F('name'))\n        self.assertEqual(aggregate.get_source_expressions(), [1, F('name')])\n        aggregate.set_source_expressions([F('age')])\n        self.assertEqual(aggregate.get_source_expressions(), [F('age'), F('name')])\n\n        # Test that set_source_expressions raises an error when both a filter and a non-filter expression are provided.\n        aggregate = Aggregate(expression=1, filter=F('name'))\n        with self.assertRaises(ValueError):\n            aggregate.set_source_expressions([F('age')])\n", "    def test_Variance_with_sample_and_filter(self):\n        # Test Variance with sample and filter\n        result = Author.objects.annotate(\n            sample_variance=Variance('age', sample=True, filter=Q(age__gt=35))\n        ).aggregate(sample_variance=Variance('age', sample=True, filter=Q(age__gt=35)))\n        self.assertEqual(result['sample_variance'], Approximate(18.75))\n", "    def test_Aggregate_resolve_expression(self):\n        query = Author.objects.annotate(\n            avg_age=Avg('age'),\n            max_age=Max('age'),\n            min_age=Min('age'),\n            std_dev_age=StdDev('age'),\n            sum_age=Sum('age'),\n            variance_age=Variance('age'),\n        ).values('avg_age', 'max_age', 'min_age', 'std_dev_age', 'sum_age', 'variance_age')\n        self.assertEqual(query.count(), 9)\n", "    def test_Aggregate_allow_distinct(self):\n        # Test that the allow_distinct attribute is respected\n        with self.assertRaises(TypeError):\n            Aggregate(expression=F('id'), distinct=True)\n", "def test_Aggregate_resolve_expression(self):\n    # Test that aggregates are correctly resolved\n    # Test case 1: Aggregate with single expression\n    author = self.a1\n    query = author.books.aggregate(avg_rating=Avg('rating'))\n    self.assertEqual(query['avg_rating'], Decimal('4.25'))\n\n    # Test case 2: Aggregate with distinct expression\n    query = author.friends.aggregate(avg_age=Avg('age', distinct=True))\n    self.assertEqual(query['avg_age'], 34.0)\n\n    # Test case 3: Aggregate with filter expression\n    query = author.books.filter(isbn='159059725').aggregate(avg_rating=Avg('rating'))\n    self.assertEqual(query['avg_rating'], 4.5)\n\n    # Test case 4: Aggregate with filter expression and multiple conditions\n    query = author.books.filter(isbn='159059725', rating__gt=4.0).aggregate(avg_rating=Avg('rating'))\n    self.assertEqual(query['avg_rating'], 4.5)\n\n    # Test case 5: Aggregate with filter expression and invalid filter expression\n    with self.assertRaises(FieldError):\n        author.books.filter(isbn='159059725', rating__gt=F('price')).aggregate(avg_rating=Avg('rating'))\n\n    # Test case 6: Aggregate with filter expression and expression inside filter expression\n    query = author.books.filter(isbn__startswith=F('isbn')).aggregate(avg_rating=Avg('rating'))\n    self.assertEqual(query['avg_rating'], 4.166666666666667)\n\n    # Test case 7: Aggregate with filter expression and non-existent field\n    with self.assertRaises(FieldError):\n        author.books.filter(isbn__startswith=F('non_existent_field')).aggregate(avg_rating=Avg('rating'))\n\n    # Test case 8: Aggregate with subquery\n    query = author.books.filter(id__in=Book.objects.filter(contact=self.a1).values('id')).aggregate(avg_rating=Avg('rating'))\n    self.assertEqual(query['avg_rating'], 4.25)\n"], "sample_983": ["def test_sparsematrix_scalar_rmul():\n    m = SparseMatrix(2, 2, {(0, 0): 2})\n    M = m.scalar_rmul(2)\n    assert M._smat == {(0, 0): 4}\n", "def test_sparse_matrix_col_join():\n    # Test column joining with a null matrix\n    A = SparseMatrix.eye(3)\n    B = SparseMatrix.zeros(0, 4)\n    C = A.col_join(B)\n    assert C.rows == 3\n    assert C.cols == 4\n    assert C.shape == (3, 4)\n\n    # Test column joining with a matrix\n    A = SparseMatrix.eye(3)\n    B = SparseMatrix.eye(4)\n    C = A.col_join(B)\n    assert C.rows == 3\n    assert C.cols == 7\n    assert C.shape == (3, 7)\n\n    # Test column joining with a matrix of different number of rows\n    A = SparseMatrix.eye(3)\n    B = SparseMatrix.eye(5)\n    raises(ValueError, lambda: A.col_join(B))\n\n    # Test column joining with a matrix of different number of columns\n    A = SparseMatrix.eye(3)\n    B = SparseMatrix.eye(3)\n    C = A.col_join(B)\n    assert C.rows == 3\n    assert C.cols == 6\n    assert C.shape == (3, 6)\n", "def test_sparsematrix_zeros():\n    # Test that zeros creates a SparseMatrix\n    m = SparseMatrix.zeros(2, 2)\n    assert isinstance(m, SparseMatrix)\n", "def test_sparse_matrix_col_op():\n    from sympy import Abs, S, Matrix\n    from sympy.matrices import SparseMatrix\n    M = SparseMatrix(3, 3, {(1, 0): 1, (1, 2): 1, (2, 0): 2})\n    M.col_op(0, lambda v, i: v + M[i, 2])\n    expected = Matrix([[3, 0, 1], [3, 0, 1], [4, 0, 2]])\n    assert M == expected\n", "def test_SparseMatrix_LDLdecomposition():\n    from sympy.matrices import SparseMatrix\n    A = SparseMatrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n    L, D = A.LDLdecomposition()\n    assert L.rows == 3\n    assert L.cols == 3\n    assert D.rows == 3\n    assert D.cols == 3\n    assert L.is_upper_triangular()\n    assert D.is_diagonal()\n    assert L._mat == [[1, 0, 0], [3/5, 1, 0], [-1/5, 1/3, 1]]\n    assert D._mat == [[25, 0, 0], [0, 9, 0], [0, 0, 9]]\n    assert L*T == A\n    assert (L*D)*L.T == A\n    assert L*D*L.T == A\n"], "sample_60": ["    def test_get_extra(self):\n        \"\"\"Test get_extra method.\"\"\"\n        inline_admin = MediaInline(admin_site)\n        request = RequestFactory().request\n        self.assertEqual(inline_admin.get_extra(request, Episode.objects.get(pk=self.episode_pk)), 0)\n", "    def test_get_queryset(self):\n        # Test that MediaInlineAdmin returns all media with Episode content type.\n        media_inline = MediaInline(admin_site)\n        request = RequestFactory().post('/')\n        request.user = self.superuser\n        request.session = {}\n        media_inline.get_queryset(request)\n        self.assertEqual(media_inline.get_queryset(request).count(), 2)\n        self.assertEqual(media_inline.get_queryset(request).values_list('content_type_id', flat=True).get(), ContentType.objects.get_for_model(Episode).pk)\n", "    def test_inline_formset_context(self):\n        admin_url = reverse('admin:media_media_changelist')\n        response = self.client.get(admin_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertInHTML('<input type=\"checkbox\" id=\"action-toggle\"', response.rendered_content)\n        self.assertInHTML('<th class=\"admin-detail\">Category</th>', response.rendered_content)\n        self.assertInHTML('<th class=\"admin-detail\">Episode</th>', response.rendered_content)\n        self.assertInHTML('<input type=\"checkbox\" name=\"action\" value=\"change\"', response.rendered_content)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n        self.inline_media_url = reverse('admin:media_media_changelist')\n", "    def test_inline_admin_changelist_view(self):\n        # Create a media instance and assign it to an episode\n        m = Media(content_object=Episode.objects.get(pk=self.episode_pk), url='http://example.com/inline_media.mp3')\n        m.save()\n\n        # Visit the admin changelist for the Episode model\n        url = reverse('admin:%s_%s_changelist' % ('media', 'media'))\n        response = self.client.get(url)\n\n        # Check that the inline admin form is displayed\n        self.assertContains(response, 'Inline admin form')\n\n        # Create a request to POST to the changelist view with a selected media instance\n        selected_media_pk = m.pk\n        data = {'%s-%s-MEDIA' % (Media._meta.app_label, Media._meta.model_name): selected_media_pk}\n        response = self.client.post(url, data)\n\n        # Check that the form is displayed with the selected media instance\n        self.assertContains(response, 'Media content_object')\n\n        # Create a request to POST to the changelist view with a deleted media instance\n        data = {'%s-%s-MEDIA' % (Media._meta.app_label, Media._meta.model_name): selected_media_pk, 'DELETE': ''}\n        response = self.client.post(url, data)\n\n        # Check that the form is displayed with a deleted media instance\n        self.assertContains(response, 'Media content_object', count=0)\n"], "sample_1204": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_432": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", email=\"a@b.com\", password=\"xxx\"\n        )\n        cls.parent = Parent.objects.create(name=\"parent\")\n        cls.child = Child.objects.create(name=\"child\", parent=cls.parent)\n", "    def setUp(self):\n        self.parent = Parent.objects.create(name='Parent')\n        self.child = Child.objects.create(name='Child')\n        self.parent.children.add(self.child)\n", "    def setUp(self):\n        self.user = self._create_superuser(\"super\")\n", "    def _get_inline_formset(self, request, model_admin, parent_object):\n        with transaction.atomic(using=router.db_for_write(model_admin.model)):\n            formsets, inline_instances = model_admin._create_formsets(\n                request, parent_object, change=True\n            )\n        return formsets[0]\n", "    def test_get_formfield_for_dbfield(self):\n        from .models import Parent\n\n        class TestAdmin(admin.ModelAdmin):\n            model = Parent\n            formfield_overrides = {\n                models.CharField: {\"widget\": widgets.AdminTextInputWidget}\n            }\n\n        admin = TestAdmin()\n        field = models.CharField()\n        self.assertEqual(admin.formfield_for_dbfield(field, self.factory.get(\"/admin/\")).widget.__class__.__name__, 'AdminTextInputWidget')\n"], "sample_762": ["def test_clone_empty_estimator():\n    estimator = MyEstimator(empty=None)\n    cloned_estimator = clone(estimator)\n    assert_dict_equal(estimator.get_params(deep=False), cloned_estimator.get_params(deep=False))\n", "    def test_clone_list(self):\n        \"\"\"Test cloning of lists of objects.\"\"\"\n        estimator_list = [K(c=1, d=1), K(c=2, d=2), K(c=3, d=3)]\n        cloned = clone(estimator_list)\n        self.assertEqual(len(cloned), 3)\n        for i, estimator in enumerate(cloned):\n            self.assertEqual(estimator.c, i + 1)\n            self.assertEqual(estimator.d, i + 1)\n", "def test_clone():\n    # test simple\n    clone1 = clone(MyEstimator(l1=3, empty=1))\n    assert_equal(clone1.l1, 3)\n    assert_equal(clone1.empty, 1)\n\n    # test clone with a deep list\n    clone2 = clone(MyEstimator(l1=[1, 2, 3], empty=[1, 2, 3]))\n    assert_equal(clone2.l1, [1, 2, 3])\n    assert_equal(clone2.empty, [1, 2, 3])\n\n    # test clone with a deep set\n    clone3 = clone(MyEstimator(l1={1, 2, 3}, empty={1, 2, 3}))\n    assert_equal(clone3.l1, {1, 2, 3})\n    assert_equal(clone3.empty, {1, 2, 3})\n\n    # test clone with a deep dict\n    clone4 = clone(MyEstimator(l1={'a': 1, 'b': 2}, empty={'a': 1, 'b': 2}))\n    assert_equal(clone4.l1, {'a': 1, 'b': 2})\n    assert_equal(clone4.empty, {'a': 1, 'b': 2})\n\n    # test clone of a transformer\n    clone5 = clone(TransformerMixin())\n    assert isinstance(clone5, TransformerMixin)\n\n    # test clone of a list of estimators\n    clone6 = clone([MyEstimator(l1=1), MyEstimator(l1=2)])\n    assert isinstance(clone6, list)\n    assert isinstance(clone6[0], MyEstimator)\n    assert isinstance(clone6[1], MyEstimator)\n\n    # test clone of a dict of estimators\n    clone7 = clone({MyEstimator(l1=1): 1, MyEstimator(l1=2): 2})\n    assert isinstance(clone7, dict)\n    assert isinstance(clone7[MyEstimator(l1=1)], MyEstimator)\n    assert isinstance(clone7[MyEstimator(l1=2)], MyEstimator)\n\n    # test clone of an estimator with a custom __init__ method\n    clone8 = clone(ModifyInitParams())\n    assert_equal(clone8.a, np.array([0]))\n\n    # test clone of a buggy estimator\n    clone9 = clone(Buggy())\n    assert", "def test_clone():\n    est = MyEstimator(l1=1, empty=[1, 2, 3])\n    cloned = clone(est)\n    assert_equal(cloned.l1, 1)\n    assert_equal(cloned.empty, [1, 2, 3])\n    assert_equal(cloned.__class__.__name__, 'MyEstimator')\n\n    est = K(c=1, d=[1, 2, 3])\n    cloned = clone(est)\n    assert_equal(cloned.c, 1)\n    assert_equal(cloned.d, [1, 2, 3])\n    assert_equal(cloned.__class__.__name__, 'K')\n\n    est = T(a=np.array([1, 2, 3]), b=4)\n    cloned = clone(est)\n    assert_equal(cloned.a, np.array([1, 2, 3]))\n    assert_equal(cloned.b, 4)\n    assert_equal(cloned.__class__.__name__, 'T')\n\n    est = K(c=1, d=[1, 2, 3])\n    cloned = clone(est, safe=False)\n    assert_equal(cloned.c, 1)\n    assert_equal(cloned.d, [1, 2, 3])\n    assert_equal(cloned.__class__.__name__, 'K')\n\n    # Test that clone raises an exception for non-estimators\n    with assert_raises(TypeError):\n        clone(1)\n    with assert_raises(TypeError):\n        clone([1, 2, 3])\n    with assert_raises(TypeError):\n        clone('1')\n\n    # Test that clone raises an exception for non-list, tuple or set of\n    # non-estimators\n    with assert_raises(TypeError):\n        clone(1, safe=False)\n    with assert_raises(TypeError):\n        clone([1, 2, 3], safe=False)\n    with assert_raises(TypeError):\n        clone('1', safe=False)\n\n    # Test that clone raises a TypeError for objects that don't have a get_params\n    with assert_raises(TypeError):\n        clone(Buggy(a=1))\n    with assert_raises(TypeError):\n        clone(VargEstimator(1, 2))\n\n    # Test that clone raises an exception for objects that have a get_params\n    # but don't set their parameters correctly\n    with assert_raises(RuntimeError):\n        est = Buggy(a=1)\n        clone(est)\n    with assert_raises(RuntimeError):\n        est =", "    def test_base_estimator_clone(self):\n        # Test cloning a non-estimator object\n        no_estimator = NoEstimator()\n        with assert_raises(TypeError):\n            clone(no_estimator)\n\n        # Test cloning a class with no get_params method\n        class MockObject:\n                self.x = x\n        mock_obj = MockObject(1)\n        with assert_raises(TypeError):\n            clone(mock_obj)\n\n        # Test cloning an estimator with non-estimator parameters\n        k = K(c='hello', d='world')\n        k_clone = clone(k)\n        assert_equal(k_clone.c, 'hello')\n        assert_equal(k_clone.d, 'world')\n\n        # Test cloning an estimator with estimator parameters\n        t = T(a='a', b='b')\n        t_clone = clone(t)\n        assert_equal(t_clone.a, 'a')\n        assert_equal(t_clone.b, 'b')\n\n        # Test cloning an estimator with empty parameters\n        t = T()\n        t_clone = clone(t)\n        assert_equal(t_clone.a, None)\n        assert_equal(t_clone.b, None)\n\n        # Test cloning a nested estimator\n        pipe = Pipeline([('step1', t), ('step2', k)])\n        pipe_clone = clone(pipe)\n        assert_equal(pipe_clone.steps[0][1].a, 'a')\n        assert_equal(pipe_clone.steps[1][1].c, 'hello')\n\n        # Test cloning a pipeline with a nested estimator that is not an\n        # estimator\n        t = T()\n        pipe = Pipeline([('step1', t)])\n        with assert_raises(TypeError):\n            clone(pipe)\n\n        # Test cloning a model with a deprecate __init__ that doesn't fulfill a\n        # is a\n        m = ModifyInitParams()\n        m_clone = clone(m)\n        assert_equal(m_clone.a.tolist(), [0])\n\n        # Test cloning an estimator with different types of parameters\n        m = ModifyInitParams(a=np.array([0]))\n        m_clone = clone(m)\n        assert_equal(m_clone.a.tolist(), [0])\n\n        # Test cloning a buggy estimator that doesn't set its parameters right\n        b = Buggy(a=None)\n        b_clone = clone(b)\n        assert_equal(b_clone.a, 1)\n"], "sample_536": ["def test_slider_step():\n    ax = widgets.Slider(ax=plt.gca(), label=\"Test slider\", valmin=0, valmax=10,\n                        valinit=5, valfmt=None, dragging=True, valstep=1)\n    assert ax.val == 5\n    ax.on_changed(lambda x: x)\n    assert ax.val == 5\n    ax.set_val(10)\n    assert ax.val == 10\n    ax.on_changed(lambda x: x)\n    assert ax.val == 10\n    ax.set_val(3)\n    assert ax.val == 3\n    ax.on_changed(lambda x: x)\n    assert ax.val == 3\n    ax.set_val(5)\n    assert ax.val == 5\n    ax.on_changed(lambda x: x)\n    assert ax.val == 5\n", "def test_Cursor():\n    fig, ax = plt.subplots()\n    cursor = widgets.Cursor(ax)\n    cursor.ax.set_xlim(-1, 1)\n    cursor.ax.set_ylim(-1, 1)\n    event = mock_event('motion_notify_event', x=0, y=0, inaxes=ax)\n    cursor.onmove(event)\n    cursor.ax.set_xlim(-2, 2)\n    cursor.ax.set_ylim(-2, 2)\n    event = mock_event('motion_notify_event', x=1, y=1, inaxes=ax)\n    cursor.onmove(event)\n", "def test_Button():\n    fig, ax = plt.subplots()\n    button = widgets.Button(ax, \"test\")\n    button.ax.set_xlim(0, 1)\n    button.ax.set_ylim(0, 1)\n    click_and_drag(button.ax, (0.5, 0.5), (0.5, 0.5), button.ax)\n    assert button._motion(mock_event(MouseEvent(ax, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)))\n    assert button.ax.get_facecolor() == [0.85]\n    click_and_drag(button.ax, (0.5, 0.5), (0.5, 0.5), button.ax)\n    assert button.ax.get_facecolor() == [0.95]\n", "def test_span_selector_onmove_callback():\n    ax = get_ax()\n    fig = ax.figure\n    span = widgets.SpanSelector(ax, lambda *args: None, direction='horizontal',\n                               onmove_callback=lambda *args: None)\n    event = mock_event(MouseEvent, x=0.5, y=0.5, inaxes=ax)\n    span.onmove(event)\n    assert span.onmove_callback is not None\n    assert span.onmove_callback(0.5, 0.5)\n", "def test_hover_line():\n    # Test that the hover line updates correctly when moving over the axes\n    ax = get_ax()\n    canvas = ax.figure.canvas\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    cursor = widgets.Cursor(ax, useblit=False)\n\n        if event.button != 1:\n            return\n        if event.xdata is not None and event.ydata is not None:\n            line = ax.lines[-1]\n            assert_allclose(line.get_xdata(), (event.xdata, event.xdata))\n            assert_allclose(line.get_ydata(), (event.ydata, event.ydata))\n\n    # Hover over the axes\n    event = mock_event('motion_notify_event', x=5, y=5, xdata=5, ydata=5)\n    check_hover_line(event)\n    assert cursor.canvas.widgetlock.available(cursor)  # Check if the lock is available\n\n    # Click and drag the cursor to move it\n    event = click_and_drag(event, ax, x=10, y=10)\n    check_hover_line(event)\n\n    # Check that the cursor is visible when the mouse is over the axes\n    ax.figure.canvas.draw_idle()\n    assert len(ax.lines) == 2  # The cursor lines are drawn\n\n    # Check that the cursor is not visible when the mouse is outside the axes\n    event = mock_event('motion_notify_event', x=15, y=15, xdata=None, ydata=None)\n    check_hover_line(event)\n    assert len(ax.lines) == 1  # Only the axes lines are drawn\n\n    # Clean up\n    ax.clear()\n    cursor.disconnect_events()\n"], "sample_619": ["def test_encode_cf_datetime_non_standard_calendar(num_dates, units, calendar):\n    da = DataArray(num_dates, dims=[\"time\"])\n    expected = (num_dates, units, calendar)\n    actual = encode_cf_datetime(da, calendar=calendar)\n    assert expected == actual\n", "def test_decode_cf_datetime_with_non_standard_calendar_non_cftime(\n    num_dates, units, calendar, has_cftime_1_4_1", "def test_decode_cf_datetime_cftime_calendar(num_dates, units, calendar):\n    # The reference implementation does not support non-standard calendars\n    # so we need to test this function directly\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", \"Ambiguous reference date string\")\n        dates = decode_cf_datetime(num_dates, units, calendar=calendar)\n        assert_equal(\n            dates,\n            cftime_to_nptime(dates, raise_on_invalid=True)\n        )\n", "def test_decode_cf_datetime_datetime64_list():\n    dates = np.array([pd.Timestamp(\"2000-01-01\"), pd.Timestamp(\"2000-01-02\")], dtype=\"datetime64[ns]\")\n    num_dates, units, calendar = encode_cf_datetime(dates)\n    assert_equal(decode_cf_datetime(num_dates, units, calendar), dates)\n", "def test_use_cftime_honoured_nonstandard_calendar():\n    with warnings.catch_warnings(record=True):\n        warnings.simplefilter(\"always\")\n\n        # This should use cftime\n        ds = Dataset(\n            {\n                \"time\": (\"t\", cftime_range(\"2000-01-01\", periods=10, freq=\"D\", calendar=\"noleap\")),\n            },\n            coords=True,\n        )\n\n        ds.to_netcdf(\"output.nc\", use_cftime=True)\n        with Dataset(\"output.nc\") as loaded_ds:\n            assert isinstance(loaded_ds[\"time\"].values, np.ndarray)\n\n        # This should use pandas\n        ds = Dataset(\n            {\n                \"time\": (\"t\", cftime_range(\"2000-01-01\", periods=10, freq=\"D\", calendar=\"noleap\")),\n            },\n            coords=True,\n        )\n\n        ds.to_netcdf(\"output.nc\", use_cftime=False)\n        with Dataset(\"output.nc\") as loaded_ds:\n            assert isinstance(loaded_ds[\"time\"].values, np.ndarray)\n\n        # This should raise an error when use_cftime is False\n        ds = Dataset(\n            {\n                \"time\": (\"t\", cftime_range(\"2000-01-01\", periods=10, freq=\"D\", calendar=\"noleap\")),\n            },\n            coords=True,\n        )\n\n        with pytest.raises(ValueError):\n            ds.to_netcdf(\"output.nc\", use_cftime=False)\n"], "sample_819": ["def test_voting_classifier_single_estimator():\n    X, y = datasets.load_iris().data[:, 1:3], datasets.load_iris().target\n    clf = VotingClassifier(estimators=[('lr', LogisticRegression())])\n    assert_raise_message(ValueError, \"'estimators' should be a list of \"\n                                 \"(string, estimator) tuples\",\n                       clf.fit, X, y)\n", "def test_voting_regressor_fails_without_estimators():\n    # Test that the VotingRegressor raises an error when no estimators are provided\n    with assert_raise_message(ValueError, 'Invalid `estimators` attribute, `estimators`'\n                             ' should be a list of (string, estimator) tuples'):\n        VotingRegressor().fit(X_r, y_r)\n", "def test_voting_regressor_sample_weight():\n    # Test sample weights with a single regressor\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    regressor = VotingRegressor([('lr', LinearRegression())], weights=[1])\n    regressor.fit(X, y)\n    y_pred = regressor.predict(X)\n    assert_almost_equal(y_pred, np.array([2.5, 7.5, 14., 21.5, 30., 42.5]))\n\n    # Test sample weights with multiple regressors\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=[0.5, 0.5])\n    regressor.fit(X, y)\n    y_pred = regressor.predict(X)\n    assert_almost_equal(y_pred, np.array([3.75, 8.75, 15.75, 23.75, 32.5, 44.25]))\n\n    # Test sample weights with non-uniform weights\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=[1, 2])\n    regressor.fit(X, y)\n    y_pred = regressor.predict(X)\n    assert_almost_equal(y_pred, np.array([3.25, 8.25, 15.25, 23.25, 32.25, 43.5]))\n", "def test_VotingClassifier_transform_soft_flatten_transform_False():\n    \"\"\"Check VotingClassifier.transform() with voting='soft', flatten_transform=False\"\"\"\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.ensemble import RandomForestClassifier\n\n    X, y = datasets.load_iris().data[:, 1:3], datasets.load_iris().target\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', flatten_transform=False)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    eclf.fit(X_train, y_train)\n    y_pred_proba = eclf.transform(X_test)\n    assert y_pred_proba.shape == (len(X_test), 3, 3)  # (n_samples, n_classifiers, n_classes)\n    assert_array_equal(np.argmax(y_pred_proba, axis=2), eclf.predict(X_test))\n", "def test_votingclassifier_sample_weight_none_n_jobs_multiple():\n    X, y = datasets.make_classification(n_samples=10, n_features=5, n_informative=3)\n    clf1 = LogisticRegression(solver='lbfgs', max_iter=1000)\n    clf2 = RandomForestClassifier(n_estimators=10)\n    clf3 = SVC(probability=True)\n    voting = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='hard')\n    voting.n_jobs = 2\n    with pytest.raises(ValueError):\n        voting.fit(X, y, sample_weight=None)\n"], "sample_446": ["    def test_floatformat_autoescape_off(self):\n        with translation.override(\"en-us\"):\n            self.assertEqual(self.engine.from_string(\n                \"{% autoescape off %}{{ a|floatformat }} {{ b|floatformat }}\"\n                \"{% endautoescape %}\",\n                {\"a\": 34.23234, \"b\": 34.00000}\n            ).render(), \"34.2 34\")\n", "    def test_floatformat_with_negative_zero(self):\n        self.render_template(\"floatformat01\", {\"a\": 0, \"b\": 0})\n        rendered = self.rendered_template\n        self.assertIn(\"0\", rendered)\n", "    def test_floatformat_positive_number(self):\n        # Test positive number with no decimal places.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 1|floatformat }}{% endautoescape %}\",\n            {\"a\": 1},\n            {\"django.template.context_processors.autoescape\": None},\n        ).render(), \"1\")\n\n        # Test positive number with positive decimal places.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 1.5|floatformat:2 }}{% endautoescape %}\",\n            {\"a\": 1.5},\n            {\"django.template.context_processors.autoescape\": None},\n        ).render(), \"1.50\")\n\n        # Test positive number with negative decimal places.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 1.5|floatformat:-1 }}{% endautoescape %}\",\n            {\"a\": 1.5},\n            {\"django.template.context_processors.autoescape\": None},\n        ).render(), \"1.5\")\n\n        # Test positive number with 'g' suffix.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 6666.6666|floatformat:2g }}{% endautoescape %}\",\n            {\"a\": 6666.6666},\n            {\"django.template.context_processors.autoescape\": None},\n        ).render(), \"6,667\")\n\n        # Test positive number with 'u' suffix.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 6666.6666|floatformat:2u }}{% endautoescape %}\",\n            {\"a\": 6666.6666},\n            {\"django.template.context_processors.autoescape\": None},\n        ).render(), \"6666.67\")\n\n        # Test positive number with 'gu' suffix.\n        self.assertEqual(self.engine.from_string(\n            \"{% autoescape off %}{{ 6666.6666|floatformat:2gu }}{% endautoescape %}\",\n            {\"a\": 6666.6666},\n            {\"django.translation.USE_L10N\": True},\n            {\"django.template.context_processors.autoescape\": None", "    def test_floatformat_with_zero_decimal_places(self):\n        with self.assertRaises(InvalidOperation):\n            floatformat(\"34.23234\")\n", "    def test_floatformat_empty_string(self):\n        # Test that empty string is not converted to 0 when no argument is provided\n        with self.settings(USE_L10N=True):\n            with translation.override('en'):\n                template = self.env.from_string(\"{% autoescape off %}{{ ''|floatformat }}{% endautoescape %}\")\n                result = template.render({\"a\": None})\n                self.assertEqual(result, '')\n                template = self.env.from_string(\"{% autoescape off %}{{ ''|floatformat:0 }}{% endautoescape %}\")\n                result = template.render({\"a\": None})\n                self.assertEqual(result, '')\n"], "sample_350": ["    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_iterator(self):\n        # Test that iterator() returns an iterable that yields Model instances.\n        queryset = Celebrity.objects.all()\n        iterator = queryset.iterator()\n        self.assertIsInstance(next(iterator), Celebrity)\n        with CaptureQueriesContext() as queries:\n            list(iterator)\n            self.assertEqual(len(queries), 1)\n", "    def test_values_select_and_values_list(self):\n        qs = Celebrity.objects.values('id', 'name').filter(name='John')\n        self.assertEqual(qs.model._fields, ['id', 'name'])\n\n        qs = Celebrity.objects.values_list('id', 'name', flat=True).filter(name='John')\n        self.assertEqual(qs.model._fields, ['id', 'name'])\n        self.assertIsInstance(qs._iterable_class, FlatValuesListIterable)\n\n        qs = Celebrity.objects.values_list('id', 'name', named=True).filter(name='John')\n        self.assertEqual(qs.model._fields, ['id', 'name'])\n        self.assertIsInstance(qs._iterable_class, NamedValuesListIterable)\n\n        qs = Celebrity.objects.values_list('id', 'name', flat=False).filter(name='John')\n        self.assertEqual(qs.model._fields, ['id', 'name'])\n        self.assertIsInstance(qs._iterable_class, ValuesListIterable)\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_aggregate_with_distinct_fields(self):\n        # Test that aggregate() raises a NotImplementedError when used with distinct()\n        qs = Celebrity.objects.all()\n        with self.assertRaises(NotImplementedError):\n            qs.aggregate(Sum('age'), distinct=('name',))\n"], "sample_845": ["def test_hashing_vectorizer_binary_output():\n    vectorizer = HashingVectorizer(binary=True, n_features=100)\n    X = vectorizer.fit_transform([\"hello world\", \"hello\"])\n    expected_output = sparse.csr_matrix([[1, 1, 1], [1, 1, 0]])\n    assert_array_equal(X.toarray(), expected_output.toarray())\n", "def test_hashing_vectorizer_transform_shape():\n    hv = HashingVectorizer(n_features=10)\n    X = hv.transform(ALL_FOOD_DOCS)\n    assert X.shape == (12, 10)\n\n", "def test_HashingVectorizer():\n    vectorizer = HashingVectorizer(n_features=10)\n    vectorizer.fit_transform([\"hello world\"])\n    assert vectorizer.n_features_ == 10\n", "def test_vectorizer_inverse_transform():\n    X = CountVectorizer(analyzer='word', stop_words='english').fit_transform(\n        [\"the quick brown fox\", \"the fox jumps over the lazy dog\"])\n    expected_result = [[\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\",\n                       \"the\", \"lazy\", \"dog\"],\n                      [\"the\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n    inverse_result = X.inverse_transform(X)\n    assert_array_equal(inverse_result, expected_result)\n\n", "def test_custom_analyzer_TfidfVectorizer():\n    X = TfidfVectorizer(analyzer=lazy_analyze).fit_transform(\n        [\"the ultimate feature\", \"the ultimate feature\", \"the ultimate feature\"])\n    assert_array_equal(X.toarray(), [[0.0, 1.0]])\n\n    # Test that TfidfVectorizer throws a warning when a custom analyzer is used with\n    # input='filename' or 'file'\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"error\")\n        TfidfVectorizer(input='filename', analyzer=lazy_analyze)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, ChangedBehaviorWarning)\n        assert \"Since v0.21, vectorizers pass the data to the custom analyzer\" in str(w[-1].message)\n\n    # Test that TfidfVectorizer throws a warning when a custom analyzer is used with\n    # input='file'\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"error\")\n        TfidfVectorizer(input='file', analyzer=lazy_analyze)\n        assert len(w) == 1\n        assert issubclass(w[-1].category, ChangedBehaviorWarning)\n        assert \"Since v0.21, vectorizers pass the data to the custom analyzer\" in str(w[-1].message)\n\n    # Test that TfidfVectorizer does not throw a warning when a custom analyzer is used\n    # with input='content'\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"error\")\n        TfidfVectorizer(input='content', analyzer=lazy_analyze)\n        assert len(w) == 0\n"], "sample_484": ["    def setUpTestData(cls):\n        cls.author = Author.objects.create(name=\"John Smith\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.author2 = Author.objects.create(name=\"Rhonda Smith\", alias=\"smithr\")\n        cls.author3 = Author.objects.create(name=\"Rhonda\", alias=\"smithr\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        Author.objects.create(name=\"Rhonda\")\n"], "sample_81": ["    def test_locale_prefix_pattern_default_language(self):\n        urlconf = URLResolver(RegexPattern(r'^/'))\n        locale_prefix_pattern = LocalePrefixPattern(prefix_default_language=True)\n        self.assertEqual(locale_prefix_pattern.match('/en/path/'), ('path/', (), {}))\n        self.assertEqual(locale_prefix_pattern.match('/path/'), ('path/', (), {}))\n", "    def test_route_pattern_default_converter(self):\n        resolver = get_resolver()\n        with override_settings(ROOT_URLCONF=__name__):\n            pattern = RoutePattern('route/<int:pk>/', 'my_view')\n            resolver.urlconf_module = pattern\n            resolver.urlconf_name = __name__\n            pattern.regex = pattern._compile('route/(?P<pk>[0-9]+)')\n            self.assertEqual(pattern.converters, {'pk': 'int'})\n", "    def test_locale_regex_descriptor_no_locale(self):\n        descriptor = LocaleRegexDescriptor('_regex')\n        self.assertEqual(descriptor.regex.pattern, '')\n", "    def test_reverse_dict_with_language_prefix(self):\n        urlconf = 'django.urls.include(\"django.contrib.auth.urls\")'\n        resolver = get_resolver(urlconf)\n        reverse_dict = resolver.reverse_dict\n        self.assertIn('en', reverse_dict)\n        self.assertIn('fr', reverse_dict)\n\n        self.assertIn('login', reverse_dict['en'])\n        self.assertIn('logout', reverse_dict['en'])\n        self.assertIn('password_change', reverse_dict['en'])\n\n        self.assertIn('login', reverse_dict['fr'])\n        self.assertIn('logout', reverse_dict['fr'])\n        self.assertIn('password_change', reverse_dict['fr'])\n", "    def test_resolve_error_handler(self):\n        # Test that resolve_error_handler returns the correct handler for a given view type\n        resolver = get_resolver()\n        handler = resolver.resolve_error_handler('404')\n        self.assertEqual(handler, getattr(resolver.urlconf_module, 'handler404', None))\n\n        # Test that resolve_error_handler returns the lazy-imported handler\n        from django.conf import urls\n        resolver = get_resolver()\n        handler = resolver.resolve_error_handler('404')\n        self.assertEqual(handler, urls.handler404)\n"], "sample_418": ["    def test_floatformat(self):\n        self.assertEqual(self.render_template(\"floatformat01\"), \"34.2\")\n        self.assertEqual(self.render_template(\"floatformat02\"), \"34\")\n        self.assertEqual(self.render_template(\"floatformat03\"), \"", "    def test_floatformat_positive(self):\n        self.assertEqual(self.engine.from_string(self.floatformat01).render({\"num1\": 34.23234}), \"34.2\")\n        self.assertEqual(self.engine.from_string(self.floatformat02).render({\"num2\": 34.0}), \"34\")\n        self.assertEqual(self.engine.from_string(self.floatformat03).render({\"num3\": 34.26000}), \"34.3\")\n", "    def test_slugify(self):\n        self.assertEqual(self.context[\"slugify01\"], \"hello-world\")\n", "    def test_filesizeformat(self):\n        self.assertEqual(self.template.resolve({\"some_list\": [102]}), \"102 bytes\")\n        self.assertEqual(self.template.resolve({\"some_list\": [102]}), \"102 bytes\")\n        self.assertEqual(self.template.resolve({\"some_list\": [-102]}), \"-102 bytes\")\n        self.assertEqual(self.template.resolve({\"some_list\": 10245.567}), \"12.1 KB\")\n        self.assertEqual(self.template.resolve({\"some_list\": -10245.567}), \"-12.1 KB\")\n        self.assertEqual(self.template.resolve({\"some_list\": \"102\"}), \"102 bytes\")\n        self.assertEqual(self.template.resolve({\"some_list\": \"102\"}), \"102 bytes\")\n        self.assertEqual(self.template.resolve({\"some_list\": \"10245.567\"}), \"12.1 KB\")\n        self.assertEqual(self.template.resolve({\"some_list\": \"10245.567\"}), \"12.1 KB\")\n", "    def test_default_yesno_with_mapping(self):\n        self.assertEqual(self.render_template(\"default_yesno01\"), gettext(\"yes\"))\n        self.assertEqual(self.render_template(\"default_yesno02\"), gettext(\"no\"))\n        self.assertEqual(self.render_template(\"default_yesno03\"), gettext(\"maybe\"))\n"], "sample_748": ["def test_randomized_search_cv():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n    estimator = DecisionTreeClassifier(random_state=42)\n    param_distributions = {'max_depth': [2, 3, 4], 'min_samples_split': [2, 3]}\n    rs = RandomizedSearchCV(estimator, param_distributions, n_iter=3)\n    with ignore_warnings():\n        rs.fit(X, y)\n    assert rs.best_score_ is not None\n    assert rs.best_params_ is not None\n    assert rs.best_estimator_.max_depth == rs.best_params_['max_depth']\n    assert rs.best_estimator_.min_samples_split == rs.best_params_['min_samples_split']\n", "def test_GridSearchCV_grid_scores_deprecation_warning():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=15,\n                               n_redundant=5, random_state=1)\n    param_grid = {'kernel': ['linear', 'rbf'], 'C': [1, 10]}\n    scoring = 'accuracy'\n\n    # Test when GridSearchCV is used as a context manager\n    with pytest.warns(DeprecationWarning) as record:\n        GridSearchCV(DecisionTreeClassifier(), param_grid, scoring=scoring).fit(X, y)\n    assert_warns_message(DeprecationWarning, record[0].message_args[0],\n                        'The grid_scores_ attribute was deprecated in version 0.18'\n                        ' in favor of the more elaborate cv_results_ attribute.'\n                        ' The grid_scores_ attribute will not be available from 0.20')\n\n    # Test when GridSearchCV is used as a function\n    grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, scoring=scoring)\n    grid_search.fit(X, y)\n    assert_warns_message(DeprecationWarning, grid_search.grid_scores_.warning,\n                        'The grid_scores_ attribute was deprecated in version 0.18'\n                        ' in favor of the more elaborate cv_results_ attribute.'\n                        ' The grid_scores_ attribute will not be available from 0.20')\n", "def test_GridSearchCV_fit_calling_refit_with_parameter_grid_with_empty_dict():\n    \"\"\"Test that GridSearchCV raises a warning when refit=True and the parameter grid is an empty dict.\"\"\"\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_classification(n_samples=10, n_features=10, n_informative=5)\n    param_grid = {}\n    cv = KFold(5, shuffle=True, random_state=0)\n    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=cv, refit=True)\n    with assert_warns_message(FitFailedWarning, \"Parameter grid is an empty dict\"):\n        grid_search.fit(X, y)\n", "def test_param_grid_warning_on_parameter_grid_with_empty_string():\n    # Test that we raise a warning when the parameter grid contains an empty string\n    param_grid = {'foo': ['a', 'b', '']}\n    with pytest.warns(UserWarning):\n        ParameterGrid(param_grid)\n\n", "def test_GridSearchCV_return_train_score():\n    # Test that return_train_score=\"warn\" behaves as True in addition to\n    # raising a warning\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    estimator = DecisionTreeClassifier(random_state=42)\n    param_grid = {\"max_depth\": [3, 5, 10]}\n    grid_search = GridSearchCV(estimator, param_grid, cv=KFold(2),\n                              return_train_score=\"warn\")\n    grid_search.fit(X, y)\n    assert_warns_message(\n        DeprecationWarning,\n        \"You are accessing a training score (mean_train_score), \"\n        \"which will not be available by default any more in 0.21.\",\n        lambda: grid_search.cv_results_)\n    assert_equal(grid_search.cv_results_['mean_train_score'], grid_search.cv_results_['mean_train_score'])\n\n    # Test that return_train_score=False raises no warning\n    grid_search = GridSearchCV(estimator, param_grid, cv=KFold(2),\n                              return_train_score=False)\n    grid_search.fit(X, y)\n    assert_no_warnings(grid_search.cv_results_)\n    assert_not_equal(grid_search.cv_results_, {})\n\n    # Test that return_train_score=True does not raise any warnings or errors\n    grid_search = GridSearchCV(estimator, param_grid, cv=KFold(2),\n                              return_train_score=True)\n    grid_search.fit(X, y)\n    assert_no_warnings(grid_search.cv_results_)\n    assert_equal(grid_search.cv_results_['mean_train_score'], grid_search.cv_results_['mean_train_score'])\n\n    # Test that GridSearchCV with default return_train_score=\"warn\" raises a warning\n    # when trying to access a training score that is not computed\n    grid_search = GridSearchCV(estimator, param_grid, cv=KFold(2),\n                              return_train_score=\"warn\")\n    assert_warns_message(\n        FutureWarning,\n        \"You are accessing a training score (mean_train_score), \"\n        \"which will not be available by default any more in 0.21.\",\n        lambda: grid_search.cv_results_['mean_train_score'])\n\n    # Test that GridSearchCV with default return_train_score=\"warn\" raises a\n    # warning when trying to access a training score that is not computed\n    # with a different value of the key\n    grid_search = Grid"], "sample_753": ["def test_logistic_regression_path_univariate():\n    # Check that we get the correct output when there is only one feature\n    X = np.array([[1], [2], [3]])\n    y = np.array([0, 0, 1])\n    Cs = 10\n\n    with ignore_warnings():\n        coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs,\n                                                     solver='lbfgs',\n                                                     multi_class='ovr',\n                                                     penalty='l2',\n                                                     fit_intercept=True)\n    assert_equal(len(coefs), Cs)\n    assert_equal(len(n_iter), Cs)\n", "def test_logistic_regression_multi_class_multinomial():\n    # Test with multi-class problem and multinomial loss\n    X, y = make_classification(n_classes=3, n_samples=1000, n_informative=2,\n                               random_state=42)\n    y = y + 1  # Ensure labels are not zero\n    lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n    lr.fit(X, y)\n    assert_array_almost_equal(lr.predict_proba(X).sum(axis=1), np.ones(X.shape[0]))\n", "def test_multiclass_logistic_regression_penalty_l2():\n    \"\"\"Test the multiclass logistic regression penalty='l2'\"\"\"\n    # Generate some data\n    X, y = make_classification(n_samples=100, n_features=5, n_informative=5,\n                               n_redundant=0, n_repeated=0, n_classes=3,\n                               n_clusters_per_class=1, random_state=1)\n\n    # Train the model\n    clf = LogisticRegression(multi_class='multinomial', penalty='l2', solver='newton-cg',\n                            max_iter=1000)\n    clf.fit(X, y)\n\n    # Check the predictions\n    check_predictions(clf, X, y)\n\n    # Check the coefficients\n    assert_array_almost_equal(clf.coef_.shape, (3, X.shape[1]))\n    assert_array_almost_equal(clf.intercept_.shape, (3,))\n\n    # Check the intercept\n    for i in range(3):\n        assert_array_almost_equal(clf.coef_[i], clf.intercept_[i])\n\n    # Check the number of iterations\n    assert_greater(clf.n_iter_, 0)\n    assert_greater(clf.n_iter_, 0)\n\n    # Check the type of the coefficients\n    assert isinstance(clf.coef_, np.ndarray)\n    assert isinstance(clf.intercept_, np.ndarray)\n\n    # Check the class labels\n    assert_array_equal(clf.classes_, np.unique(y))\n", "def test_logistic_regression_solver_newton_cg_multinomial():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, weights=[0.8, 0.2],\n                               random_state=42)\n\n    X_sp = sp.csr_matrix(X)\n    clf = LogisticRegression(solver='newton-cg', penalty='l2',\n                            multi_class='multinomial', max_iter=100,\n                            tol=1e-4, random_state=42, class_weight='balanced')\n    check_predictions(clf, X, y)\n\n    # Test that the `predict_proba` method works correctly for multinomial\n    # regression\n    probs = clf.predict_proba(X)\n    assert_array_almost_equal(np.sum(probs, axis=1), np.ones(len(y)))\n    assert_array_equal(probs.argmax(axis=1), y)\n\n    # Test that the `predict_log_proba` method works correctly for multinomial\n    # regression\n    log_probs = clf.predict_log_proba(X)\n    assert_array_almost_equal(np.sum(np.exp(log_probs), axis=1), np.ones(len(y)))\n    assert_array_equal(np.argmax(log_probs, axis=1), y)\n\n    # Test that the `coef_` attribute is correct for multinomial regression\n    assert_array_almost_equal(clf.coef_.shape, (2, X.shape[1]))\n\n    # Test that the `intercept_` attribute is correct for multinomial regression\n    assert_array_almost_equal(clf.intercept_.shape, (2,))\n\n    # Test that the `classes_` attribute is correct for multinomial regression\n    assert_array_equal(clf.classes_, np.array([0, 1]))\n\n    # Test that the `n_iter_` attribute is correct for multinomial regression\n    assert_array_almost_equal(clf.n_iter_, 100)\n\n    # Test that the `score` method works correctly for multinomial regression\n    score = clf.score(X, y)\n    assert_greater(score, 0.7)\n    assert_greater(score, 0.3)\n\n    # Test that the `predict` method works correctly for multinomial regression\n    predicted = clf.predict(X)\n    assert_array_equal(predicted, y)\n\n    # Test that the `predict_proba` method returns the correct", "def test_logistic_regression_path():\n    X = np.array([[-1, -1], [-2, -1], [-3, -1], [1, 1], [2, 1], [3, 1]])\n    y = np.array([0, 0, 0, 1, 1, 1])\n    C = 1.0\n    assert_raise_message(ValueError, r\"Logistic Regression supports only liblinear, \"\n                     \"newton-cg, lbfgs, sag and saga solvers, got 'foo'\",\n                     lambda: logistic_regression_path(X, y, solver='foo', C=C))\n    assert_raise_message(ValueError, r\"multi_class should be either multinomial or \"\n                     \"ovr, got 'foo'\",\n                     lambda: logistic_regression_path(X, y, multi_class='foo', C=C))\n    assert_raise_message(ValueError, r\"multi_class should be either multinomial or \"\n                     \"ovr, got 'multinomial'\",\n                     lambda: logistic_regression_path(X, y, multi_class='multinomial', C=C, solver='liblinear'))\n    assert_raise_message(ValueError, r\"Penalty term must be positive; got (C=None)\",\n                     lambda: logistic_regression_path(X, y, C=None))\n    assert_raise_message(ValueError, r\"Maximum number of iteration must be positive;\"\n                     \" got (max_iter=-1)\",\n                     lambda: logistic_regression_path(X, y, max_iter=-1))\n    assert_raise_message(ValueError, r\"Tolerance for stopping criteria must be \"\n                     \"positive; got (tol=-1.0)\",\n                     lambda: logistic_regression_path(X, y, tol=-1.0))\n\n    C = 1e-1\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=[C], C=C)\n    assert Cs.shape == (1,)\n    assert coefs.shape == (1, 2)\n\n    C = [1e-1, 1e0, 1e1]\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=C, C=C)\n    assert Cs.shape == (3,)\n    assert coefs.shape == (3, 2)\n\n    # check that n_iter has correct shape\n    n_iter, Cs = n_iter[0], Cs\n    assert n_iter.shape == (3,)\n    assert_allclose(n_iter, [0, 5, 5])\n\n"], "sample_1207": ["def test_repeated_decimals():\n    assert parse_expr('.3[1]', transformations=['repeated_decimals']) == Rational(1, 10)\n    assert parse_expr('.3[11]', transformations=['repeated_decimals']) == Rational(11, 90)\n", "def test_split_symbols_custom():\n    from sympy.parsing.sympy_parser import split_symbols, parse_expr\n    expression = \"abcde\"\n    result = parse_expr(expression, transformations=(split_symbols,))\n    assert isinstance(result, Symbol), \"Expected result to be a Symbol\"\n    assert result == Symbol('a*b*c*d*e'), \"Unexpected result\"\n", "def test_auto_symbol():\n    x = Symbol('x')\n    local_dict = {'x': x}\n    global_dict = {}\n    result = lambda_notation(auto_symbol([(*parse_expr('x', local_dict, global_dict, standard_transformations),)], local_dict, global_dict), local_dict, global_dict)\n    assert result == [(*parse_expr('Symbol(x)', local_dict, global_dict, standard_transformations),)]\n    \n    # Test that auto_symbol doesn't convert named tuples\n    local_dict = {'named_tuple': 'named_tuple'}\n    global_dict = {}\n    result = lambda_notation(auto_symbol([(*parse_expr('named_tuple', local_dict, global_dict, standard_transformations),)], local_dict, global_dict), local_dict, global_dict)\n    assert result == [(*parse_expr('named_tuple', local_dict, global_dict, standard_transformations),)]\n    \n    # Test that auto_symbol doesn't convert keywords\n    local_dict = {}\n    global_dict = {'True': True}\n    result = lambda_notation(auto_symbol([(*parse_expr('True', local_dict, global_dict, standard_transformations),)], local_dict, global_dict), local_dict, global_dict)\n    assert result == [(*parse_expr('True', local_dict, global_dict, standard_transformations),)]\n    \n    # Test that auto_symbol correctly converts undefined variables\n    local_dict = {}\n    global_dict = {}\n    result = lambda_notation(auto_symbol([(*parse_expr('x', local_dict, global_dict, standard_transformations),)], local_dict, global_dict), local_dict, global_dict)\n    assert result == [(*parse_expr('Function(x)', local_dict, global_dict, standard_transformations),)]\n    \n    # Test that auto_symbol correctly converts undefined variables with a leading dot\n    local_dict = {}\n    global_dict = {}\n    result = lambda_notation(auto_symbol([(*parse_expr('.x', local_dict, global_dict, standard_transformations),)], local_dict, global_dict), local_dict, global_dict)\n    assert result == [(*parse_expr('Function(.x)', local_dict, global_dict, standard_transformations),)]\n", "def test_parse_expr_factorial_notation():\n    result = parse_expr(\"2! + 3! + 4! + 5!\")\n    assert result == 2*3*4*5 + 2*3 + 2 + 1\n    assert isinstance(result, Mul)\n    assert str(result) == \"120 + 6 + 2 + 1\"\n", "def test_parse_expr_with_multiple_equals_signs():\n    result = parse_expr(\"(1 + 2 = 3)*(4 + 5 = 6)\", evaluate=False)\n    assert result == Eq(3, Eq(6, 5)) * Eq(7, 4)\n"], "sample_761": ["def test_iterative_imputer_init():\n    imputer = IterativeImputer(strategy='mean')\n    assert imputer.initial_strategy == 'mean'\n\n    imputer = IterativeImputer(strategy='median')\n    assert imputer.initial_strategy == 'mean'\n\n    imputer = IterativeImputer(strategy='most_frequent')\n    assert imputer.initial_strategy == 'mean'\n\n    imputer = IterativeImputer(strategy='constant')\n    assert imputer.initial_strategy == 'mean'\n\n    imputer = IterativeImputer(strategy='mean', initial_strategy='median')\n    assert imputer.initial_strategy == 'median'\n", "def test_iterative_imputer_multivariate():\n    np.random.seed(0)\n    X = np.array([[1, 2, np.nan, np.nan, 6],\n                  [4, np.nan, 6, 2, 5],\n                  [10, 5, np.nan, 9, 0]])\n    Xt = np.array([[1, 2, 4.0, 2.0, 6],\n                   [4, 3.0, 6, 2, 5],\n                   [10, 5, 9.0, 9, 0]])\n\n    imputer = IterativeImputer(strategy=\"median\")\n    imputer.fit(X)\n    Xt_imputed = imputer.transform(X)\n\n    assert_array_almost_equal(Xt, Xt_imputed)\n\n    imputer = IterativeImputer(strategy=\"mean\")\n    imputer.fit(X)\n    Xt_imputed = imputer.transform(X)\n\n    assert_array_almost_equal(Xt, Xt_imputed)\n\n    imputer = IterativeImputer(strategy=\"most_frequent\")\n    imputer.fit(X)\n    Xt_imputed = imputer.transform(X)\n\n    assert_array_almost_equal(Xt, Xt_imputed)\n\n    imputer = IterativeImputer(strategy=\"constant\", fill_value=0)\n    imputer.fit(X)\n    Xt_imputed = imputer.transform(X)\n\n    assert_array_almost_equal(Xt, Xt_imputed)\n", "def test_iterative_imputer_imputation_order():\n    from sklearn.impute import IterativeImputer\n\n    X = np.array([[np.nan, 1, 3],\n                  [4, np.nan, 6],\n                  [10, 5, 9]])\n\n    imputer = IterativeImputer(strategy='mean')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt = np.array([[ 7., 1., 3.],\n                           [4., 4., 6.],\n                           [10., 5., 9.]])\n\n    assert_array_almost_equal(Xt, expected_Xt)\n\n    # Test for imputation order\n    imputer = IterativeImputer(strategy='mean', imputation_order='roman')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt_roman = np.array([[7., 1., 3.],\n                                  4., 4., 6.],\n                                 [10., 5., 9.]])\n\n    assert_array_almost_equal(Xt, expected_Xt_roman)\n\n    imputer = IterativeImputer(strategy='mean', imputation_order='arabic')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt_arabic = np.array([[7., 1., 3.],\n                                   [10., 5., 9.],\n                                   4., 4., 6.])\n\n    assert_array_almost_equal(Xt, expected_Xt_arabic)\n\n    imputer = IterativeImputer(strategy='mean', imputation_order='ascending')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt_ascending = np.array([[7., 1., 3.],\n                                      [10., 5., 9.],\n                                      4., 4., 6.])\n\n    assert_array_almost_equal(Xt, expected_Xt_ascending)\n\n    imputer = IterativeImputer(strategy='mean', imputation_order='descending')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt_descending = np.array([[7., 1., 3.],\n                                       [4., 4., 6.],\n                                       [10., 5., 9.]])\n\n    assert_array_almost_equal(Xt, expected_Xt_descending)\n\n    imputer = IterativeImputer(strategy='mean', imputation_order='random')\n    Xt = imputer.fit_transform(X)\n\n    expected_Xt_random = np.array([[7., 1., 3", "def test_iterative_imputer_with_all_nan_features():\n    from sklearn.linear_model import LinearRegression\n    imputer = IterativeImputer(max_iter=1, initial_strategy='constant',\n                               strategy='mean', missing_values=np.nan)\n    X = np.array([[np.nan, np.nan, np.nan],\n                  [np.nan, np.nan, np.nan],\n                  [np.nan, np.nan, np.nan]])\n    Xt = imputer.fit_transform(X)\n    assert_array_equal(Xt, np.zeros_like(X))\n    assert_array_equal(imputer.n_features_with_missing_, 0)\n", "def test_iterative_imputer_categorical():\n    \"\"\"Test IterativeImputer for categorical data.\n\n    We have a dataset with a categorical variable and a numerical variable.\n    The categorical variable is used to impute the numerical variable. The\n    numerical variable is imputed using a Bayesian Ridge model. We verify\n    that the imputed values are correct.\n\n    \"\"\"\n    X = np.array([[1, 2, np.nan],\n                  [np.nan, np.nan, 3],\n                  [4, 5, 6]])\n\n    y = np.array([1, 2, 3])\n\n    imputer = IterativeImputer(strategy='constant', missing_values=np.nan,\n                               estimator=BayesianRidge())\n    X_imputed = imputer.fit_transform(X)\n\n    # Verify that the numerical variable is correctly imputed\n    X_expected = np.array([[1, 2, 2],\n                          [3, 3, 3],\n                          [4, 5, 6]])\n\n    assert_array_almost_equal(X_imputed[:, 1], X_expected[:, 1])\n\n    # Verify that the categorical variable is not imputed\n    assert_array_almost_equal(X_imputed[:, 0], X[:, 0])\n\n    # Test the case when the categorical variable is used to impute the\n    # numerical variable\n    X = np.array([[1, np.nan, 3],\n                  [np.nan, np.nan, 4],\n                  [2, 5, 6]])\n\n    imputer = IterativeImputer(strategy='constant', missing_values=np.nan,\n                               estimator=BayesianRidge())\n    X_imputed = imputer.fit_transform(X)\n\n    # Verify that the numerical variable is correctly imputed\n    X_expected = np.array([[1, 1, 3],\n                          [2, 2, 4],\n                          [2, 5, 6]])\n\n    assert_array_almost_equal(X_imputed[:, 1], X_expected[:, 1])\n\n    # Verify that the categorical variable is not imputed\n    assert_array_almost_equal(X_imputed[:, 0], X[:, 0])\n"], "sample_675": ["def test_caplog_record_tuples(requests_fixture):\n    log_handler = LogCaptureHandler()\n    logger = logging.getLogger(\"test_log\")\n    logger.setLevel(logging.INFO)\n    log_handler.setLevel(logging.INFO)\n    log_handler.emit(logging.LogRecord(\n        name=\"test_log\",\n        level=logging.INFO,\n        pathname=\"test_file.py\",\n        lineno=10,\n        msg=\"Test message\",\n        args=None,\n        exc_info=None\n    ))\n\n    caplog = LogCaptureFixture(request.node)\n    caplog.set_level(logging.INFO)\n    caplog.handler = log_handler\n\n    assert caplog.record_tuples == [(\"test_log\", 20, \"Test message\")]\n", "def test_logging_level_set_per_test(testdir):\n    \"\"\"Verify log level set per test.\"\"\"\n    testdir.makefile(\n        \".py\",\n        \"test1.py=import pytest\\n\"\n        \"import logging\\n\"\n        \"log = logging.getLogger()\\n\"\n        \"log.setLevel(logging.INFO)\\n\"\n        \"def test_log_info():\\n\"\n        \"    log.info('test1 log info')\\n\"\n        \"def test_log_warning():\\n\"\n        \"    log.warning('test1 log warning')\\n\",\n    )\n    testdir.makefile(\n        \".py\",\n        \"test2.py=import pytest\\n\"\n        \"import logging\\n\"\n        \"log = logging.getLogger()\\n\"\n        \"log.setLevel(logging.WARNING)\\n\"\n        \"def test_log_warning():\\n\"\n        \"    log.warning('test2 log warning')\\n\"\n        \"def test_log_error():\\n\"\n        \"    log.error('test2 log error')\\n\",\n    )\n    testdir.makeconftest(\n        \"\"\"\n        pytest_plugins = [\"logging_plugin\"]\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_level = WARNING\n        log_cli_level = INFO\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--junit-xml=report.xml\")\n    assert \"test1 log info\" not in result.stdout.str()\n    assert \"test1 log warning\" in result.stdout.str()\n    assert \"test2 log warning\" in result.stdout.str()\n    assert \"test2 log error\" in result.stdout.str()\n", "def test_colored_formatter(caplog):\n    # test that the colored formatter is created\n    caplog._finalize()  # Reset fixture\n    # create a logger\n    logger = logging.getLogger(\"test_logger\")\n    # get the terminal writer\n    terminal_writer = caplog._config.option.terminal_writer\n    # create a colored formatter\n    formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\", \"%H:%M:%S\")\n    # test that the formatter is created correctly\n    assert formatter._level_to_fmt_mapping is not None\n    assert formatter._original_fmt == \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\"\n    # test that the formatted level name is correct\n    assert formatter.LEVELNAME_FMT_REGEX.search(\"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\") is not None\n    # test that the level names are correct\n    for level, color_opts in formatter.LOGLEVEL_COLOROPTS.items():\n        formatted_levelname = formatter.LEVELNAME_FMT_REGEX.sub(\n            terminal_writer.markup(logging.getLevelName(level), **{name: True for name in color_opts}), \n            formatter._fmt\n        )\n        assert formatted_levelname != formatter._original_fmt\n", "def test_catching_logs_enter_exit():\n    \"\"\"Test that the context manager `catching_logs` works as expected\"\"\"\n    # Arrange\n    caplog = pytest.fixture(\"caplog\")\n    caplog.handler.setLevel(logging.INFO)\n    original_handler = logging.getLogger().handlers[0]\n    assert original_handler.level == logging.NOTSET\n\n    # Act\n    with catching_logs(caplog.handler, level=logging.INFO):\n        logging.getLogger().handlers[0].setLevel(logging.INFO)\n        logging.info(\"Test info message\")\n\n    # Assert\n    assert original_handler.level == logging.INFO\n    assert len(caplog.records) == 1\n    assert caplog.records[0].levelname == \"INFO\"\n\n    # Act\n    caplog.handler.setLevel(logging.ERROR)\n    with catching_logs(caplog.handler, level=logging.ERROR):\n        logging.getLogger().handlers[0].setLevel(logging.INFO)\n        logging.error(\"Test error message\")\n\n    # Assert\n    assert original_handler.level == logging.INFO\n    assert len(caplog.records) == 2\n    assert caplog.records[1].levelname == \"ERROR\"\n\n    # Act\n    with catching_logs(caplog.handler, level=logging.WARNING):\n        logging.getLogger().handlers[0].setLevel(logging.INFO)\n        logging.warning(\"Test warning message\")\n\n    # Assert\n    assert original_handler.level == logging.INFO\n    assert len(caplog.records) == 3\n    assert caplog.records[2].levelname == \"WARNING\"\n", "def test_formatter_colors():\n    \"\"\"Test that the ColoredLevelFormatter correctly colors log messages.\"\"\"\n    log_level = logging.INFO\n    color = \"green\"\n    formatter = ColoredLevelFormatter(None, \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\")\n    assert color in formatter.LOGLEVEL_COLOROPTS[log_level]\n    assert formatter.LEVELNAME_FMT_REGEX.search(formatter._fmt)\n    colored_levelname = formatter.LEVELNAME_FMT_REGEX.sub(\n        formatter._style._fmt[\"levelname\"],\n        formatter._fmt,\n    )\n    assert colored_levelname.startswith(\"\\x1b[\")\n\n    terminal_writer = create_terminal_writer(None)\n    formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s\")\n    assert formatter._level_to_fmt_mapping == {logging.INFO: colored_levelname}\n    assert formatter._style._fmt == colored_levelname\n    assert re.search(f\"\\x1b[{color}m\", formatter.format(logging.LogRecord(\"test\", log_level, 1, \"Test message\", None, None, None)))\n"], "sample_701": ["def test_check_ispytest(monkeypatch):\n    # Test that check_ispytest warns when not called with _ispytest=True\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False)\n\n    # Test that check_ispytest does not warn when called with _ispytest=True\n    check_ispytest(True)\n\n    # Test that check_ispytest warns when called without the _ispytest argument\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest()\n\n    # Test that check_ispytest warns when not called with a boolean value\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(\"not a boolean\")\n", "def test_check_ispytest():\n    # Test that check_ispytest warns when called with ispytest=False\n    with pytest.warns(PytestDeprecationWarning) as record:\n        deprecated.check_ispytest(False)\n    assert len(record) == 1\n\n    # Test that check_ispytest does not warn when called with ispytest=True\n    with pytest.warns(None):\n        deprecated.check_ispytest(True)\n\n    # Test that check_ispytest is a function\n    assert isinstance(deprecated.check_ispytest, function)\n\n    # Test that check_ispytest has the correct signature\n    signature = inspect.signature(deprecated.check_ispytest)\n    assert signature.parameters == {'ispytest': inspect.Parameter('ispytest', inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=bool, default=False)}\n", "def test_deprecated_warning_messages():\n    # Test deprecated warning messages are correct\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(FILLFUNCARGS.format(name=\"pytest_func\"))\n        assert str(record[0].message) == \"pytest_func is deprecated, use function._request._fillfixtures() instead if you cannot avoid reaching into internals.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(PYTEST_COLLECT_MODULE.format(name=\"pytest_collect_module\"))\n        assert str(record[0].message) == \"pytest.collect.pytest_collect_module was moved to pytest.pytest_collect_module\\nPlease update to the new name.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(YIELD_FIXTURE)\n        assert str(record[0].message) == \"@pytest.yield_fixture is deprecated.\\nUse @pytest.fixture instead; they are the same.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(MINUS_K_DASH)\n        assert str(record[0].message) == \"The `-k '-expr'` syntax to -k is deprecated.\\nUse `-k 'not expr'` instead.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(MINUS_K_COLON)\n        assert str(record[0].message) == \"The `-k 'expr:'` syntax to -k is deprecated.\\nPlease open an issue if you use this and want a replacement.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(WARNING_CAPTURED_HOOK)\n        assert str(record[0].message) == \"The pytest_warning_captured is deprecated and will be removed in a future release.\\nPlease use pytest_warning_recorded instead.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(WARNING_CMDLINE_PREPARSE_HOOK)\n        assert str(record[0].message) == \"The pytest_cmdline_preparse hook is deprecated and will be removed in a future release. \\nPlease use pytest_load_initial_conftests hook instead.\"\n\n    with pytest.warns(PytestDeprecationWarning) as record:\n        warn(FSCOLLECTOR_GETHOOKPROXY_ISINITPATH)\n        assert str(record[0].message) == \"The gethookproxy() and isinitpath() methods of FSCollector and Package are deprecated;", "def test_FSCOLLECTOR_GETHOOKPROXY_ISINITPATH(caplog):\n    # Test that FSCollector and Package methods are deprecated\n    class TestCollector(FSCollector):\n            super().__init__(session)\n\n    collector = TestCollector(None)\n    with pytest.deprecated_call():\n        collector.gethookproxy()\n    assert \"gethookproxy() is deprecated\" in caplog.text\n    with pytest.deprecated_call():\n        collector.isinitpath()\n    assert \"isinitpath() is deprecated\" in caplog.text\n\n    # Test that session.gethookproxy() and session.isinitpath() work as expected\n    class TestSession:\n            return None\n\n            return False\n\n    session = TestSession()\n    assert collector.gethookproxy() is None\n    assert collector.isinitpath() is False\n", "def test_check_ispytest():\n    # Test the default behavior with _ispytest=False\n    with warnings.catch_warnings(record=True) as w:\n        with pytest.warns(PytestDeprecationWarning):\n            deprecated.check_ispytest(False)\n            assert len(w) == 1\n            assert w[0].message.args[0].startswith(\"A private pytest class or function was used.\")\n\n    # Test the case where _ispytest=True\n    with warnings.catch_warnings(record=True) as w:\n        deprecated.check_ispytest(True)\n            assert len(w) == 0\n\n    # Test the case where the stack level is not provided\n    with warnings.catch_warnings(record=True) as w:\n        deprecated.check_ispytest(False)\n        assert len(w) == 1\n        assert w[0].message.args[0].startswith(\"A private pytest class or function was used.\")\n\n    # Test the case where the stack level is greater than 3\n    with warnings.catch_warnings(record=True) as w:\n        deprecated.check_ispytest(False, stacklevel=5)\n        assert len(w) == 1\n        assert w[0].message.args[0].startswith(\"A private pytest class or function was used.\")\n"], "sample_1061": ["def test_isqrt():\n    assert isqrt(16) == 4\n    assert isqrt(25) == 5\n    assert isqrt(36) == 6\n    assert isqrt(49) == 7\n    assert isqrt(64) == 8\n    assert isqrt(81) == 9\n    assert isqrt(100) == 10\n\n    assert isqrt(-1) == -1\n    assert isqrt(-4) == -2\n    assert isqrt(-9) == -3\n\n    assert isqrt(0) == 0\n\n    with raises(ValueError):\n        isqrt(-10)\n    with raises(ValueError):\n        isqrt(10.5)\n", "def test_integer_nthroot():\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(2, 2) == (1, False)\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(25, 2) == (5, True)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(27, 2) == (5, False)\n    assert integer_nthroot(32, 2) == (5, True)\n    assert integer_nthroot(49, 2) == (7, True)\n    assert integer_nthroot(64, 2) == (8, True)\n    assert integer_nthroot(81, 2) == (9, True)\n    assert integer_nthroot(100, 2) == (10, True)\n\n    assert integer_nthroot(-1, 2) == (1, False)\n    assert integer_nthroot(-2, 2) == (1, False)\n    assert integer_nthroot(-4, 2) == (2, False)\n    assert integer_nthroot(-9, 2) == (3, False)\n    assert integer_nthroot(-16, 2) == (4, False)\n    assert integer_nthroot(-25, 2) == (5, False)\n\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(64, 3) == (4, True)\n    assert integer_nthroot(125, 3) == (5, True)\n\n    assert integer_nthroot(-1, 3) == (1, False)\n    assert integer_nthroot(-8, 3) == (2, False)\n    assert integer_nthroot(-27, 3) == (3, False)\n    assert integer_nthroot(-64, 3) == (4, False)\n    assert integer_nthroot(-125, 3) == (5, False)\n", "def test_isqrt():\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(3) == 1\n    assert isqrt(4) == 2\n    assert isqrt(5) == 2\n    assert isqrt(6) == 2\n    assert isqrt(7) == 2\n    assert isqrt(8) == 2\n    assert isqrt(9) == 3\n    assert isqrt(10) == 3\n    assert isqrt(11) == 3\n    assert isqrt(12) == 3\n    assert isqrt(13) == 3\n    assert isqrt(14) == 3\n    assert isqrt(15) == 3\n    assert isqrt(16) == 4\n    assert isqrt(17) == 4\n    assert isqrt(18) == 4\n    assert isqrt(19) == 4\n    assert isqrt(20) == 4\n    assert isqrt(21) == 4\n    assert isqrt(22) == 4\n    assert isqrt(23) == 4\n    assert isqrt(24) == 4\n    assert isqrt(25) == 5\n\n    assert isqrt(256) == 16\n    assert isqrt(255) == 15\n    assert isqrt(256) == 16\n    assert isqrt(257) == 16\n    assert isqrt(258) == 16\n\n    with raises(ValueError):\n        isqrt(-1)\n    with raises(ValueError):\n        isqrt(-2)\n\n    with raises(ValueError):\n        isqrt(0)\n    with raises(ValueError):\n        isqrt(-1)\n\n    with raises(ValueError):\n        isqrt(1.5)\n    with raises(ValueError):\n        isqrt(1.2)\n", "def test_isqrt():\n    # Test positive integers\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(4) == 2\n    assert isqrt(8) == 2\n    assert isqrt(16) == 4\n\n    # Test non-positive integers\n    with raises(ValueError):\n        isqrt(-1)\n\n    # Test non-integer inputs\n    assert isqrt(3.5) == 1\n    assert isqrt(1.5) == 1\n\n    # Test very large integers\n    assert isqrt(2**52 + 2**27) == 2**26\n\n    # Test inputs that overflow\n    with raises(OverflowError):\n        isqrt(2**53)\n", "def test_integer_nthroot():\n    assert integer_nthroot(121, 2) == (11, True)\n    assert integer_nthroot(121, 2) == (11, True)\n    assert integer_nthroot(121, 3) == (4, False)\n    assert integer_nthroot(0, 2) == (0, True)\n    assert integer_nthroot(-16, 2) == (-4, True)\n    assert integer_nthroot(-16, 3) == (-2, False)\n    assert integer_nthroot(100, 2) == (10, True)\n    assert integer_nthroot(100, 3) == (4, False)\n    assert integer_nthroot(1000, 3) == (10, True)\n    assert integer_nthroot(1000, 4) == (5, False)\n    assert integer_nthroot(-1, 2) == (-1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(-4, 3) == (-1, False)\n    assert integer_nthroot(8, 3) == (2, True)\n    assert integer_nthroot(27, 3) == (3, True)\n    assert integer_nthroot(-8, 3) == (-2, True)\n    assert integer_nthroot(-27, 3) == (-3, True)\n    assert integer_nthroot(64, 3) == (4, True)\n    assert integer_nthroot(125, 3) == (5, True)\n    assert integer_nthroot(256, 3) == (8, True)\n    assert integer_nthroot(-64, 3) == (-4, True)\n    assert integer_nthroot(-125, 3) == (-5, True)\n    assert integer_nthroot(-256, 3) == (-8, True)\n"], "sample_1133": ["def test_fresnel_coefficients_complex_angle():\n    n1, n2 = symbols('n1 n2')\n    angle_of_incidence = pi/6\n    coeff = fresnel_coefficients(angle_of_incidence, n1, n2)\n    assert isinstance(coeff, list)\n    assert len(coeff) == 4\n    for i in coeff:\n        assert not i.is_real\n", "def test_refraction_angle():\n    n1, n2 = symbols('n1 n2')\n    n1, n2 = 1.2, 1.5\n    angle_of_incidence = pi/6\n    angle_of_refraction = refraction_angle(angle_of_incidence, n1, n2)\n    assert round(ae(angle_of_refraction, n1/n2, 6), 6) == 0.41152\n    assert round(ae(angle_of_refraction, n1/n2, 6), 6) == 0.41152\n", "def test_fresnel_coefficients_complex():\n    r, theta, n1, n2 = symbols('r theta n1 n2')\n    angle_of_incidence = acos(r/n2)\n    n1 = 1.33\n    n2 = 1.5\n    theta = 0.3\n    angle_of_incidence = theta\n    result = fresnel_coefficients(angle_of_incidence, n1, n2)\n    expected = [0.235625382192159 + 0.971843958291041*I,\n                -0.816477005968898 - 0.577377951366403*I]\n    assert ae(result[0], expected[0], 7), \"Error in fresnel_coefficients: R_p\"\n    assert ae(result[1], expected[1], 7), \"Error in fresnel_coefficients: R_s\"\n    expected = [0.658921776708929 + 0.651354770181179*I]\n    assert ae(result[2], expected[0], 7), \"Error in fresnel_coefficients: T_p\"\n    assert ae(result[3], expected[0], 7), \"Error in fresnel_coefficients: T_s\"\n", "def test_refraction_angle_plane_with_equal_mediums():\n    n1, n2 = symbols('n1, n2')\n    n1 = Medium(refractive_index=n1)\n    n2 = Medium(refractive_index=n2)\n    angle_of_incidence = symbols('angle_of_incidence')\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n\n    result1 = refraction_angle(n1, n2, n2, plane=P)\n    result2 = refraction_angle(r1, n1, n2, plane=P)\n    result3 = refraction_angle(angle_of_incidence, n1, n2)\n\n    assert result1 == Ray3D(Point3D(0, 0, 0), Matrix([0, 0, 1]))\n    assert result2 == Ray3D(Point3D(0, 0, 0), Matrix([0, 0, 1]))\n    assert result3 == asin(n1*sin(angle_of_incidence)/n2)\n", "def test_refraction_angle_float_and_plane():\n    from sympy.physics.optics.utils import refraction_angle\n    from sympy.geometry import Point3D, Ray3D, Plane\n    from sympy.matrices import Matrix\n    from sympy import symbols, pi\n    n = Matrix([0, 0, 1])\n    P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n    r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n    r2 = Ray3D(Point3D(-1, -1, -1), Point3D(0, 0, 0))\n    r1_reflection = refraction_angle(r1, 1, 1, normal=n, plane=P)\n    r2_reflection = refraction_angle(r2, 1, 1, normal=n, plane=P)\n    assert isinstance(r1_reflection, Ray3D)\n    assert isinstance(r2_reflection, Ray3D)\n    assert r1_reflection.origin == r2_reflection.origin\n    assert r1_reflection.direction_ratio == -r2_reflection.direction_ratio\n"], "sample_252": ["    def test_has_any_keys_lookup(self):\n        model = JSONModel()\n        model.json_field = {'key1': 'value1', 'key2': 'value2'}\n        self.assertTrue(model.json_field.has_any_keys(['key1', 'key2']))\n", "    def test_formfield(self):\n        # Test that the formfield correctly sets the encoder and decoder\n        field = JSONField(encoder=JSONEncoder, decoder=DjangoJSONEncoder)\n        formfield = field.formfield()\n        self.assertEqual(formfield.encoder, JSONEncoder)\n        self.assertEqual(formfield.decoder, DjangoJSONEncoder)\n", "def test_json_field_deconstruct(self):\n    field = JSONField()\n    name, path, args, kwargs = field.deconstruct()\n    self.assertEqual(name, 'json_field')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'encoder': None, 'decoder': None})\n\n    field = JSONField(encoder=lambda x: x, decoder=lambda x: x)\n    name, path, args, kwargs = field.deconstruct()\n    self.assertEqual(name, 'json_field')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'encoder': lambda x: x, 'decoder': lambda x: x})\n", "    def test_key_transform_postgresql(self):\n        model = JSONModel.objects.create(value={'a': 1})\n        json_field = KeyTransform('a', model.value)\n        self.assertEqual(str(json_field), 'JSON_EXTRACT(\"value\", \\'$.a\\')')\n        self.assertEqual(json_field.get_prep_lookup(), {'a'})\n", "def test_json_field_from_db_value_default(self):\n    with CaptureQueriesContext() as queries:\n        instance = JSONModel.objects.create(data={'key': 'value'})\n        field = JSONModel._meta.get_field('data')\n        value = field.from_db_value(instance.data, None, None)\n        self.assertEqual(value, {'key': 'value'})\n        self.assertEqual(len(queries.captured), 1)\n"], "sample_357": ["    def test_simple_change(self):\n        before = self.make_project_state([self.author_unmanaged])\n        after = self.make_project_state([self.author_with_managers])\n        questioner = mock.Mock()\n        questioner.ask_initial.return_value = True\n        questioner.ask_managers.return_value = [('a', 'A'), ('b', 'B')]\n        changes = self.get_changes(before.models, after.models, questioner)\n        self.assertEqual(len(changes), 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelManagers'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, managers=[('a', 'A'), ('b', 'B')])\n", "    def test_alter_field_null_and_default(self):\n        author_null_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, null=True, default=models.NOT_PROVIDED)),\n        ])\n        from_state = ProjectState()\n        from_state.add_model(author_null_default.clone())\n        to_state = ProjectState()\n        to_state.add_model(author_null_default.clone())\n        to_state.models[\"testapp\", \"Author\"].fields[\"name\"].null = False\n        to_state.models[\"testapp\", \"Author\"].fields[\"name\"].default = 'some default'\n        autodetector = MigrationAutodetector(from_state, to_state)\n        changes = autodetector._detect_changes()\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, **{\n            'preserve_default': False,\n            'field': models.CharField(max_length=200, null=False, default='some default'),\n        })\n", "def test_add_field_with_callable_default_changes_to_dependencies(self):\n    # Arrange\n    before_states = self.make_project_state([\n        self.author_empty,\n        self.other_pony,\n    ])\n    after_states = self.make_project_state([\n        self.author_empty,\n        self.author_name,\n        self.other_pony_food,\n    ])\n    questioner = mock.Mock()\n    questioner.ask_not_null_addition.return_value = 'callable_default'\n\n    # Act\n    changes = self.get_changes(before_states, after_states, questioner)\n    migrations = changes['testapp']\n\n    # Assert\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='0001_initial')\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, dependencies=[('testapp', 'Author', None, True)])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, name='0002_add_field_name')\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, dependencies=[\n        ('testapp', 'Author', None, True),\n        ('otherapp', 'Pony', None, True),\n    ])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='0001_initial')\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, dependencies=[('testapp', 'Author', None, True)])\n    self.assertOperationFieldAttributes(changes, 'testapp', 1, 0, name='name', default='callable_default')\n    self.assertOperationFieldAttributes(changes, 'testapp', 1, 0, field__blank=False, field__null=False, field__auto_created=False)\n    self.assertOperationFieldAttributes(changes, 'testapp', 1, 0, field__verbose_name='name')\n", "def test_alter_model_options_with_swappable(self):\n    from django.db import migrations\n    from django.db.models import SwappableField\n\n    initial = self.make_project_state([\n        self.author_empty,\n        self.publisher,\n        self.book,\n    ])\n    new = self.make_project_state([\n        self.author_empty,\n        self.publisher,\n        self.book,\n        self.author_dates_of_birth_auto_now,\n        self.publisher_with_author,\n        self.book_with_book,\n    ])\n    changes = self.get_changes(initial.models, new.models)\n\n    self.assertNumberMigrations(changes, \"testapp\", 4)\n\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationTypes(changes, \"testapp\", 1, [\"CreateModel\"])\n    self.assertOperationTypes(changes, \"testapp\", 2, [\"AddField\"])\n    self.assertOperationTypes(changes, \"testapp\", 3, [\"AlterModelOptions\"])\n\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"0001_initial\")\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, dependencies=[(\"testapp\", \"__first__\")])\n    self.assertOperationAttributes(changes, \"testapp\", 1, 0, name=\"0002_add_date_of_birth\")\n    self.assertOperationAttributes(changes, \"testapp\", 1, 0, dependencies=[\"testapp.0001_initial\"])\n    self.assertOperationAttributes(changes, \"testapp\", 2, 0, name=\"0003_add_publisher\")\n    self.assertOperationAttributes(changes, \"testapp\", 2, 0, dependencies=[\"testapp.0001_initial\"])\n    self.assertOperationAttributes(changes, \"testapp\", 3, 0, name=\"0004_alter_author\")\n    self.assertOperationAttributes(changes, \"testapp\", 3, 0, dependencies=[\"testapp.0002_add_date_of_birth\", \"testapp.0003_add_publisher\"])\n\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=None)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 1, 0, default=None)\n    self.assertOperationFieldAttributes(changes, \"testapp\", 2, 0, preserve_default=True, field__remote_field__model=\"testapp.Author\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 3, 0, options={\"permissions\": [('", "    def test_generate_removed_fields_when_field_is_removed_from_multiple_models(self):\n        # Make sure that when a field is removed from multiple models,\n        # it's included in each of the removal operations.\n        # From: testapp.Author: (\"name\", models.CharField(max_length=200))\n        # To:   testapp.Author: (\"name\", models.CharField(max_length=200, null=True))\n        #       testapp.Publisher: (\"name\", models.CharField(max_length=200))\n        before_state = self.make_project_state([\n            self.author_name,\n            self.publisher,\n        ])\n        after_state = self.make_project_state([\n            self.author_name_null,\n            self.publisher,\n        ])\n        changes = self.get_changes(before_state.models, after_state.models)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, name=\"name\", field_name=\"name\")\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 1, 0, name=\"name\", field_name=\"name\")\n        self.assertNumberMigrations(changes, \"testapp\", 3)\n        self.assertOperationTypes(changes, \"testapp\", 2, [\"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 2, 0, name=\"name\", field_name=\"name\")\n        self.assertNumberMigrations(changes, \"testapp\", 4)\n        self.assertOperationTypes(changes, \"testapp\", 3, [\"RemoveField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 3, 0, name=\"name\", field_name=\"name\")\n\n        # Make sure that when a field is removed from multiple models,\n        # it's not included in the dependencies of the removal operations.\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 2, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 3, [])\n        self.assertMigrationDependencies(changes, \"testapp\", 4, [])\n"], "sample_266": ["    def test_build_graph(self):\n        \"\"\"\n        Test that the graph is built correctly.\n        \"\"\"\n        # Set up a test app with migrations\n        app_config = apps.get_app_config('test_app')\n        app_package_name = app_config.name\n        migrations_dir = os.path.join(app_package_name, MIGRATIONS_MODULE_NAME)\n        migration_paths = [\n            os.path.join(migrations_dir, '0001_initial.py'),\n            os.path.join(migrations_dir, '0002_data.py'),\n            os.path.join(migrations_dir, '0003_more_data.py'),\n        ]\n        for path in migration_paths:\n            with open(path, 'w') as f:\n                f.write(f'from django.db import migrations\\n')\n                f.write(f'class Migration(migrations.Migration):\\n')\n                f.write(f'    operations = []\\n')\n\n        # Create a MigrationLoader instance\n        loader = MigrationLoader(connection=connection)\n        loader.build_graph()\n\n        # Check the graph has been built correctly\n        self.assertEqual(loader.graph.nodes, {\n            ('test_app', '0001_initial'): None,\n            ('test_app', '0002_data'): None,\n            ('test_app', '0003_more_data'): None,\n        })\n", "    def test_migrations_module(self):\n        with override_settings(\n            MIGRATION_MODULES={\n                'myapp': 'path.to.migrations',\n            }\n        ):\n            loader = MigrationLoader(connection=connections['default'])\n            module_name, explicit = loader.migrations_module('myapp')\n            self.assertEqual(module_name, 'path.to.migrations')\n            self.assertTrue(explicit)\n", "    def test_build_graph_with_replacements(self):\n        # Create some migrations\n        with self.settings(\n            MIGRATION_MODULES={\n                'test_app': 'test_project.test_app.migrations',\n            },\n        ):\n            migration_1 = self.call_command('makemigrations', '--empty', 'test_app', '--empty')\n            migration_1 = self.call_command('makemigrations', '--empty', 'test_app', '--empty')\n            migration_2 = self.call_command('makemigrations', '--empty', 'test_app', '--empty')\n\n        # Create a replacement migration\n        with self.settings(\n            MIGRATION_MODULES={\n                'test_app': 'test_project.test_app.migrations',\n            },\n        ):\n            migration_3 = self.call_command('makemigrations', '--empty', 'test_app', '--empty')\n            self.call_command('migrate', 'test_app', '0001', '--replace', '0002')\n            self.call_command('migrate', 'test_app', '0003')\n\n        # Load the migrations\n        loader = MigrationLoader(connection=connections['default'])\n        loader.build_graph()\n\n        # Check that the graph is correct\n        self.assertIn(('test_app', '0002'), loader.graph.nodes)\n        self.assertIn(('test_app', '0003'), loader.graph.nodes)\n        self.assertIn(('test_app', '0001'), loader.graph.nodes)\n        self.assertIn(('test_app', '__first__'), loader.graph.nodes)\n\n        # Check that the replacements are correct\n        self.assertEqual(loader.replacements, {('test_app', '0002'): loader.disk_migrations[('test_app', '0003')]})\n", "    def test_build_graph_with_replacing_migration(self):\n        # Create a test app with a migration that replaces another migration\n        with override_settings(INSTALLED_APPS=['myapp']):\n            with self.settings(MIGRATION_MODULES={'myapp': 'tests.test_apps.myapp.migrations'}):\n                self._create_test_migration(myapp='tests.test_apps.myapp')\n                myapp_migration = self._create_migration('tests.test_apps.myapp', '0015_replace_migration')\n                myapp_migration.replaces = ['0014_old_migration']\n                self._apply_migration('tests.test_apps.myapp', '0015_replace_migration')\n\n        self.assertTrue(MigrationLoader('default').graph.has_node(('myapp', '0015_replace_migration')))\n", "    def test_migrations_module_setting(self):\n        \"\"\"\n        Test that the migrations module can be overridden by the\n        MIGRATION_MODULES setting.\n        \"\"\"\n        # First, test when MIGRATION_MODULES is not set\n        loader = MigrationLoader(connection=connection)\n        self.assertEqual(loader.migrations_module('app1'), ('tests.migrations', False))\n        self.assertEqual(loader.migrations_module('app2'), ('tests.migrations', False))\n        # Then, test when MIGRATION_MODULES is set\n        self.assertEqual(loader.migrations_module('app1'), ('tests.migrations', True))\n        self.assertEqual(loader.migrations_module('app2'), ('tests.migrations', True))\n        # Finally, test that the MIGRATION_MODULES setting is not overridden by the\n        # MigrationLoader's constructor.\n        self.assertEqual(loader.migrations_module('app1'), ('tests.migrations', True))\n        self.assertEqual(loader.migrations_module('app2'), ('tests.migrations', True))\n"], "sample_687": ["def test_log_cli_level_set(caplog):\n    logger.setLevel(logging.INFO)\n    with caplog.at_level(logging.INFO):\n        logger.info(\"test\")\n    assert \"INFO\" in caplog.text\n    assert \"test\" in caplog.text\n\n    logger.setLevel(logging.DEBUG)\n    with caplog.at_level(logging.DEBUG):\n        logger.debug(\"test\")\n    assert \"DEBUG\" in caplog.text\n    assert \"test\" in caplog.text\n", "def test_get_log_level_for_setting():\n    config = Config()\n    assert get_log_level_for_setting(config) is None\n    config.getoption(\"log_level\") = \"WARNING\"\n    assert get_log_level_for_setting(config) == 30\n    config.getoption(\"log_level\") = \"WARNING\"\n    assert get_log_level_for_setting(config, \"log_level\") == 30\n    config.getoption(\"log_level\") = \"NOT_A_LOG_LEVEL\"\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(config, \"log_level\")\n    config.getoption(\"log_level\") = \"WARNING\"\n    config.getini(\"log_level\") = None\n    assert get_log_level_for_setting(config) is None\n    config.getini(\"log_level\") = \"WARNING\"\n    assert get_log_level_for_setting(config) == 30\n    config.getini(\"log_level\") = \"NOT_A_LOG_LEVEL\"\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(config)\n    config.getoption(\"log_level\") = \"WARNING\"\n    config.getini(\"log_level\") = \"WARNING\"\n    assert get_log_level_for_setting(config) == 30\n", "def test_log_level_not_set(caplog):\n    log_level = get_log_level_for_setting(caplog._item.config, \"log_level\")\n    assert log_level is None\n    logger.warning(\"This message should not be caught\")\n", "def test_log_capture_levels():\n    \"\"\"Test that the log capture fixture allows to set logging levels.\n\n    Also, test that loggers are properly restored after a test.\n    \"\"\"\n        logger.setLevel(logging.INFO)\n        sublogger.setLevel(logging.ERROR)\n        logging.error(\"error message\")\n        logging.warning(\"warning message\")\n        logging.info(\"info message\")\n\n    caplog = caplog fixtures=True\n    test_function()\n    assert caplog.records\n    assert len(caplog.records) == 3\n    assert caplog.record_tuples == [(\"root\", 30, \"error message\"), (\"root\", 30, \"warning message\"), (\"root\", 20, \"info message\")]\n\n    # Test that loggers are properly restored\n    logger.setLevel(logging.CRITICAL)\n    sublogger.setLevel(logging.DEBUG)\n    caplog.clear()\n    test_function()\n    assert caplog.records\n    assert len(caplog.records) == 1\n    assert caplog.record_tuples == [(\"root\", 50, \"error message\")]\n", "def test_log_capture_initialization():\n    \"\"\"Test the initialization of LogCaptureHandler.\"\"\"\n    pytester = Testdir()\n    pytester.copy_example(\"test_example.py\")\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n\n        logger = logging.getLogger(__name__)\n        logger.info(\"test message\")\n        \"\"\"\n    )\n\n    pytester.ensurepyproject(\"log_cli_level=INFO\")\n    result = pytester.runpytest()\n    assert result.ret == 0\n    assert \"test message\" not in result.stderr\n    assert \"test message\" in result.out\n"], "sample_274": ["    def test_invalid_choice(self):\n        form = InlineForeignKeyField(parent_instance=None)\n        with self.assertRaises(ValidationError):\n            form.clean(\"invalid_choice\")\n", "    def test_choice_model_form_valid(self):\n        form = ChoiceModelForm(data={'choice': 'choice1'})\n        self.assertTrue(form.is_valid())\n", "    def test_model_choice_field_choice_error_messages(self):\n        field = ModelChoiceField(queryset=ChoiceModel.objects.all())\n        with self.assertRaises(ValidationError) as cm:\n            field.clean('invalid_choice')\n        self.assertEqual(cm.exception.messages, field.error_messages['invalid_choice'])\n", "    def test_modelchoicefield_invalid_choice(self):\n        form = ChoiceModelForm({'choice': 'invalid'})\n        expected = {'choice': ['Select a valid choice. That choice is not one of the available choices.']}\n        self.assertFormErrors(expected, form)\n", "    def test_inline_foreign_key_field(self):\n        # Test that an inline foreign key field has the correct label and value\n        model = ChoiceModel.objects.create()\n        form = InlineForeignKeyField(model, label='Parent', to_field='id')\n        self.assertEqual(form.label, 'Parent')\n        self.assertEqual(form.initial, model.id)\n"], "sample_616": ["def test_covariance():\n    from numpy.testing import assert_allclose\n\n    # Create a DataArray with random values\n    da_a = xr.DataArray(np.random.randn(5, 5), dims=[\"x\", \"y\"])\n    da_b = xr.DataArray(np.random.randn(5, 5), dims=[\"x\", \"y\"])\n\n    # Calculate the covariance\n    cov = xr.cov(da_a, da_b)\n    assert cov.ndim == 1\n    assert cov.shape == (5,)\n\n    # Check that the covariance is symmetric\n    assert_allclose(cov, cov.T)\n\n    # Check that the covariance is zero when the variables are independent\n    da_c = xr.DataArray(np.random.randn(5, 5), dims=[\"x\", \"y\"])\n    assert_allclose(xr.cov(da_a, da_c), 0)\n\n    # Check that the covariance is not defined when the variables are not aligned\n    da_d = xr.DataArray(np.random.randn(6, 6), dims=[\"x\", \"y\"])\n    with pytest.raises(ValueError):\n        xr.cov(da_a, da_d)\n\n    # Check that the covariance is not defined when the variables have different shapes\n    da_e = xr.DataArray(np.random.randn(5, 6), dims=[\"x\", \"y\"])\n    with pytest.raises(ValueError):\n        xr.cov(da_a, da_e)\n\n    # Check that the covariance is not defined when the variables have different dimensions\n    da_f = xr.DataArray(np.random.randn(5, 5), dims=[\"x\", \"z\"])\n    with pytest.raises(ValueError):\n        xr.cov(da_a, da_f)\n\n    # Check that the covariance is not defined when the variables have missing values\n    da_g = xr.DataArray(np.random.randn(5, 5), dims=[\"x\", \"y\"])\n    da_g[0, 0] = np.nan\n    with pytest.raises(ValueError):\n        xr.cov(da_a, da_g)\n", "def test_unify_chunks_array():\n    \"\"\"Test unify_chunks on DataArray.\"\"\"\n    a = xr.DataArray(np.arange(12).reshape(2, 6), dims=[\"time\", \"lat\"])\n    b = xr.DataArray(np.arange(12).reshape(2, 6), dims=[\"time\", \"lat\"])\n    b.chunk({\"time\": 2, \"lat\": 3})\n\n    c = unify_chunks(a, b)\n    assert c[0].chunk[\"time\"] == 2\n    assert c[0].chunk[\"lat\"] == 6\n    assert c[1].chunk[\"time\"] == 2\n    assert c[1].chunk[\"lat\"] == 6\n\n", "def test_broadcast_compat_data():\n    \"\"\"Test broadcast_compat_data by checking all dimensions are correctly propagated\"\"\"\n    da1 = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = xr.DataArray([[4, 5], [6, 7]], dims=[\"x\", \"y\"])\n    da3 = xr.DataArray([8, 9], dims=[\"y\"])\n\n    da2 = da2.expand_dims({\"z\": [0]})\n    da3 = da3.expand_dims({\"z\": [1]})\n\n    broadcast_dims = (\"x\",)\n    core_dims = (\"y\",)\n    result = broadcast_compat_data(da2, broadcast_dims, core_dims)\n\n    expected_shape = (3, 1)\n    assert result.shape == expected_shape\n\n    da2 = da2.expand_dims({\"z\": [0, 1]})\n    da3 = da3.expand_dims({\"z\": [0, 1]})\n\n    broadcast_dims = (\"x\", \"z\")\n    core_dims = (\"y\",)\n    result = broadcast_compat_data(da2, broadcast_dims, core_dims)\n\n    expected_shape = (3, 2)\n    assert result.shape == expected_shape\n\n    da1 = xr.DataArray([1, 2, 3], dims=[\"y\"])\n    da2 = xr.DataArray([[4, 5], [6, 7]], dims=[\"x\", \"y\"])\n    da3 = xr.DataArray([8, 9], dims=[\"x\"])\n\n    broadcast_dims = (\"x\",)\n    core_dims = (\"y\",)\n    result = broadcast_compat_data(da3, broadcast_dims, core_dims)\n\n    expected_shape = (1, 2)\n    assert result.shape == expected_shape\n\n    da1 = xr.DataArray([1, 2, 3], dims=[\"y\"])\n    da2 = xr.DataArray([[4, 5], [6, 7]], dims=[\"x\", \"y\"])\n    da3 = xr.DataArray([8, 9], dims=[\"x\"])\n\n    broadcast_dims = ()\n    core_dims = (\"x\", \"y\")\n    result = broadcast_compat_data(da3, broadcast_dims, core_dims)\n\n    expected_shape = (2, 2)\n    assert result.shape == expected_shape\n\n    da1 = xr.DataArray([1, 2, 3], dims=[\"y\"])\n    da2 = xr.DataArray([[4, 5], [6, 7]],", "def test_unify_chunks():\n    from . import Dataset\n\n    # Create two datasets with different chunk sizes\n    ds1 = Dataset(\n        data_vars={\n            \"var1\": (\"x\", np.arange(10)),\n            \"var2\": (\"y\", np.arange(12)),\n        },\n        coords={\"x\": np.arange(10), \"y\": np.arange(12)},\n        chunks={\"x\": 5, \"y\": 6},\n    )\n    ds2 = Dataset(\n        data_vars={\n            \"var1\": (\"x\", np.arange(10)),\n            \"var2\": (\"y\", np.arange(12)),\n        },\n        coords={\"x\": np.arange(10), \"y\": np.arange(12)},\n        chunks={\"x\": 4, \"y\": 6},\n    )\n\n    # Unify the chunks\n    unified = unify_chunks(ds1, ds2)\n\n    # Check that the chunk sizes are unified\n    assert unified[0].chunks == {\"x\": 5, \"y\": 6}\n    assert unified[1].chunks == {\"x\": 5, \"y\": 6}\n\n    # Check that the variables have the correct values\n    assert_allclose(unified[0].data_vars[\"var1\"].values, ds1.data_vars[\"var1\"].values)\n    assert_allclose(unified[1].data_vars[\"var1\"].values, ds2.data_vars[\"var1\"].values)\n\n    # Check that the variables have the correct coords\n    assert unified[0].coords[\"x\"].values == ds1.coords[\"x\"].values\n    assert unified[1].coords[\"x\"].values == ds2.coords[\"x\"].values\n\n    # Check that the unified datasets have the same attrs\n    assert unified[0].attrs == ds1.attrs\n    assert unified[1].attrs == ds2.attrs\n\n    # Check that the unified datasets have the same encoding\n    assert unified[0].encoding == ds1.encoding\n    assert unified[1].encoding == ds2.encoding\n\n    # Check that the unified datasets can be saved to netCDF4 format\n    unified.to_netcdf(\"unified.nc\")\n    ds1.to_netcdf(\"ds1.nc\")\n    ds2.to_netcdf(\"ds2.nc\")\n    assert_identical(unified[0].to_netcdf(), ds1.to_netcdf())\n    assert_identical(unified[1].", "def test_unify_chunks_dask_array():\n    if not has_dask():\n        pytest.skip(\"dask tests are only available when dask is installed\")\n\n    ds1 = xr.Dataset(\n        {\"a\": ((\"x\",), np.arange(10))},\n        coords={\"x\": np.arange(10)},\n        attrs={\"foo\": \"bar\"},\n    )\n    ds1[\"a\"] = ds1[\"a\"].chunk(chunks={\"x\": 5})\n\n    ds2 = xr.Dataset(\n        {\"a\": ((\"x\",), np.arange(10))},\n        coords={\"x\": np.arange(10)},\n        attrs={\"foo\": \"baz\"},\n    )\n    ds2[\"a\"] = ds2[\"a\"].chunk(chunks={\"x\": 7})\n\n    ds3 = xr.Dataset(\n        {\"a\": ((\"x\",), np.arange(10))},\n        coords={\"x\": np.arange(10)},\n        attrs={\"foo\": \"qux\"},\n    )\n    ds3[\"a\"] = ds3[\"a\"].chunk(chunks={\"x\": 3})\n\n    unified = unify_chunks(ds1, ds2, ds3)\n    assert len(unified) == 3\n    assert all(isinstance(ds, xr.Dataset) for ds in unified)\n    assert unified[0].attrs == {\"foo\": \"bar\"}\n    assert unified[1].attrs == {\"foo\": \"baz\"}\n    assert unified[2].attrs == {\"foo\": \"qux\"}\n    assert unified[0][\"a\"].chunksize == {\"x\": 7}\n    assert unified[1][\"a\"].chunksize == {\"x\": 7}\n    assert unified[2][\"a\"].chunksize == {\"x\": 7}\n    assert all(ds._indexes == unified[0]._indexes for ds in unified)\n\n    # Test that unify_chunks raises a ValueError when inputs have different chunk sizes\n    # for the same dimension\n    with pytest.raises(ValueError):\n        xr.Dataset({\"a\": ((\"x\",), np.arange(10))}, coords={\"x\": np.arange(10)}).chunk(\n            chunks={\"x\": 5}\n        )\n        xr.Dataset({\"a\": ((\"x\",), np.arange(10))}, coords={\"x\": np.arange(10)}).chunk(\n            chunks={\"x\": 7}\n        )\n\n    # Test that unify_chunks works for DataArrays\n    da1 = xr.DataArray(np.arange(10), dims=[\"x\"], coords={\"x"], "sample_758": ["def test_check_non_negative():\n    # Check that check_non_negative raises a ValueError when X contains negative values\n    with pytest.raises(ValueError):\n        check_non_negative(np.array([-1, 0, 1]), 'estimator')\n\n    # Check that check_non_negative raises a ValueError when X is a sparse matrix with negative values\n    with pytest.raises(ValueError):\n        check_non_negative(sp.csr_matrix([-1, 0, 1]), 'estimator')\n\n    # Check that check_non_negative returns None when X does not contain negative values\n    result = check_non_negative(np.array([1, 0, 1]), 'estimator')\n    assert result is None\n\n    # Check that check_non_negative returns None when X is a sparse matrix without negative values\n    result = check_non_negative(sp.csr_matrix([1, 0, 1]), 'estimator')\n    assert result is None\n", "def test_has_fit_parameter():\n    class Estimator:\n            pass\n\n            pass\n\n    class EstimatorWithParameters:\n            pass\n\n            pass\n\n    assert has_fit_parameter(Estimator(), 'fit') is False\n    assert has_fit_parameter(EstimatorWithParameters(), 'fit') is True\n    assert has_fit_parameter(EstimatorWithParameters(), 'sample_weight') is True\n\n    with pytest.raises(ValueError):\n        has_fit_parameter(Estimator(), 'sample_weight')\n    with pytest.raises(ValueError):\n        has_fit_parameter(Estimator(), 'fit', 'some_estimator')\n    with pytest.raises(ValueError):\n        has_fit_parameter('some_estimator', 'fit')\n", "def test_check_is_fitted_with_custom_message():\n    class CustomEstimator:\n            pass\n\n            pass\n\n            raise NotFittedError(\"Custom error message\")\n\n    estimator = CustomEstimator()\n    check_is_fitted(estimator, [\"fit\"], msg=\"Custom error message\")\n", "def test_check_array_with_dtype_conversion():\n    # Test that an array is converted to the correct dtype when check_array is\n    # called with a specific dtype\n    X = np.array([1, 2, 3], dtype=np.float32)\n    assert_equal(check_array(X, dtype=np.float64).dtype, np.float64)\n\n    # Test that an array is not converted when the dtype is already correct\n    X = np.array([1, 2, 3], dtype=np.float64)\n    assert_equal(check_array(X, dtype=np.float64).dtype, np.float64)\n\n    # Test that an array is converted to the correct dtype when check_array is\n    # called with a list of dtypes\n    X = np.array([1, 2, 3], dtype=np.float32)\n    assert_equal(check_array(X, dtype=[np.float32, np.float64]).dtype, np.float64)\n\n    # Test that an array is converted to the correct dtype when check_array is\n    # called with a list of dtypes and the array's dtype is in the list\n    X = np.array([1, 2, 3], dtype=np.float32)\n    assert_equal(check_array(X, dtype=[np.float32, np.float64]).dtype, np.float32)\n\n    # Test that an error is raised when the array's dtype is not in the list of\n    # allowed dtypes\n    X = np.array([1, 2, 3], dtype=np.float32)\n    with pytest.raises(ValueError):\n        check_array(X, dtype=[np.float64])\n\n    # Test that an error is raised when the dtype is not a list or a string\n    with pytest.raises(ValueError):\n        check_array(np.array([1, 2, 3]), dtype='float64')\n", "def test_check_array_multidimensional_array():\n    X = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    with pytest.raises(ValueError):\n        check_array(X, ensure_2d=True, allow_nd=False)\n    X = np.array([[[1, 2], [3, 4]], [5, 6], [7, 8]])\n    with pytest.raises(ValueError):\n        check_array(X, ensure_2d=True, allow_nd=False)\n    X = np.array([[1, 2], [3, 4]])\n    X = X[np.newaxis, :, :]\n    check_array(X, ensure_2d=True, allow_nd=False)\n    X = np.array([[1, 2], [3, 4]])\n    X = X[:, np.newaxis, :]\n    check_array(X, ensure_2d=True, allow_nd=False)\n"], "sample_122": ["    def test_patch_vary_headers_no_existing_vary(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['Accept', 'Accept-Language'])\n        self.assertEqual(response['Vary'], 'Accept, Accept-Language')\n", "    def test_patch_vary_headers(self):\n        from django.middleware.cache import UpdateCacheMiddleware\n        request = RequestFactory().get('/')\n        response = HttpResponse()\n        UpdateCacheMiddleware().process_response(request, response)\n        self.assertEqual(response['Vary'], '')\n\n        patch_vary_headers(response, ['Accept', 'Accept-Language'])\n        self.assertEqual(response['Vary'], 'Accept, Accept-Language')\n\n        patch_vary_headers(response, ['Accept-Language'])\n        self.assertEqual(response['Vary'], 'Accept-Language')\n\n        patch_vary_headers(response, ['Accept', '*'])\n        self.assertEqual(response['Vary'], '*')\n\n        patch_vary_headers(response, ['Accept', 'Accept-Language', '*'])\n        self.assertEqual(response['Vary'], '*')\n", "    def test_patch_cache_control_with_true_values(self):\n        response = HttpResponse()\n        patch_cache_control(response, no_store=True, must_revalidate=True)\n        self.assertEqual(response['Cache-Control'], 'no-store, must-revalidate')\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.middleware = CacheMiddleware()\n", "    def test_get_cache_key(self):\n        request = RequestFactory().get('/')\n        cache_key = get_cache_key(request)\n        self.assertEqual(cache_key, 'views.decorators.cache.cache_header.test_key_prefix.0')\n"], "sample_1012": ["def test_print_piecewise():\n    from sympy.printing.pycode import pycode\n    from sympy import symbols, Piecewise, sin, cos, tan\n\n    x, y = symbols('x y')\n    expr = Piecewise((sin(x), x < 0), (cos(x), True))\n    result = pycode(expr)\n    expected = \"numpy.select([x < 0, True], [numpy.sin(x), numpy.cos(x)], default=numpy.nan)\"\n    assert result == expected\n", "def test_PyCodePrinter__print_Rational():\n    from sympy.codegen import Assignment\n    from sympy.core.numbers import Rational\n    from sympy.printing.pycode import PythonCodePrinter\n\n    printer = PythonCodePrinter()\n    expr = Rational(1, 2)\n    result = printer._print_Rational(expr)\n    assert result == '0/1'\n", "def test_print_FunctionDefinition():\n    # Test that the printer handles nested FunctionDefinition\n    f = Function('f')\n    g = Function('g')\n    f1 = Function('f1')\n    f1 = f1.subs(f, g)\n    fd = FunctionDefinition('f1', [f], f)\n    fd = fd.subs(f, g)\n    code = pycode(fd, fully_qualified_modules=False)\n    assert code == \"def f1(g):\\n    g\"\n", "def test_SciPyPrinter(self):\n    from sympy import sin, cos, exp\n    sp = SciPyPrinter()\n    assert sp.doprint(sin(x)) == 'scipy.special.exp(1j*sin(x))'\n    assert sp.doprint(cos(x)) == 'scipy.special.exp(-1j*sin(x))'\n    assert sp.doprint(exp(x)) == 'exp(x)'\n", "def test_PythonCodePrinter_inline_not_working_correctly():\n    x, y = symbols('x y')\n    expr = atan2(x, y)\n    printer = PythonCodePrinter(settings={'inline': False})\n    assert printer.doprint(expr) == 'math.atan2(x, y)'\n    assert printer._settings['inline'] is False\n"], "sample_696": ["def test_check_ispytest():\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False)\n\n    # Test that check_ispytest does not raise a warning when called with ispytest=True\n    with pytest.warns(None):\n        check_ispytest(True)\n\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False and stacklevel=2\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False, stacklevel=2)\n\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False and stacklevel=1\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False, stacklevel=1)\n\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False and stacklevel=0\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False, stacklevel=0)\n\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False and stacklevel>3\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False, stacklevel=4)\n\n    # Test that check_ispytest raises a PytestDeprecationWarning when called with ispytest=False and stacklevel=-1\n    with pytest.warns(PytestDeprecationWarning):\n        check_ispytest(False, stacklevel=-1)\n", "def test_check_ispytest():\n    # Test check_ispytest with pytest=True\n    with pytest.warns(None):\n        deprecated.check_ispytest(True)\n\n    # Test check_ispytest with pytest=False\n    with pytest.warns(deprecated.PRIVATE):\n        deprecated.check_ispytest(False)\n\n    # Test check_ispytest with invalid types\n    with pytest.raises(TypeError):\n        deprecated.check_ispytest(\"not a bool\")\n\n    # Test check_ispytest with invalid default value\n        deprecated.check_ispytest(_ispytest)\n    with pytest.warns(deprecated.PRIVATE):\n        func()\n\n    # Test check_ispytest with function call\n    class TestClass:\n            deprecated.check_ispytest(_ispytest)\n            assert _ispytest is False\n    with pytest.warns(deprecated.PRIVATE):\n        TestClass().my_private_function()\n", "def test_check_ispytest():\n    with warnings.catch_warnings(record=True) as w:\n        deprecated.check_ispytest(False)\n        assert len(w) == 1\n        assert isinstance(w[0].message.category, PytestDeprecationWarning)\n        assert str(w[0].message) == \"A private pytest class or function was used.\"\n", "def test_fillfuncargs():\n    \"\"\"Test deprecated warning FILLFUNCARGS\"\"\"\n    with warnings.catch_warnings(record=True) as w:\n        # Test function without the _request._fillfixtures() call\n        @pytest.fixture\n            pass\n\n        test_func()\n\n        # Should not raise an exception\n        assert len(w records) == 0\n\n        # Test function with the _request._fillfixtures() call\n        with pytest.warns(FILLFUNCARGS):\n            @pytest.fixture\n                pytest._request._fillfixtures()\n\n            test_func()\n\n        # Should have raised an exception\n        assert len(w records) == 1\n", "def test_check_ispytest():\n    \"\"\"Test the check_ispytest function.\"\"\"\n    # Test default behavior when _ispytest is not set\n    with pytest.warns(PytestDeprecationWarning) as record:\n        check_ispytest(False)\n    assert len(record) == 1\n\n    # Test that the warning is not raised when _ispytest is set to True\n    check_ispytest(True)\n    assert len(record) == 1  # The previous warning is still in the record\n\n    # Test that the warning is raised when the function is called outside of pytest\n    with pytest.warns(PytestDeprecationWarning):\n        sys.path.insert(0, str(mock.Mock(spec=\"pytest_module\"))\n        import pytest_module  # noqa: F401\n        pytest_module.check_ispytest(False)\n"], "sample_689": ["def test_deprecation_messages():\n    \"\"\"Test that the deprecation messages are instances of PytestDeprecationWarning.\"\"\"\n    for message in [\n        FILLFUNCARGS,\n        PYTEST_COLLECT_MODULE,\n        MINUS_K_DASH,\n        MINUS_K_COLON,\n        WARNING_CAPTURED_HOOK,\n        FSCOLLECTOR_GETHOOKPROXY_ISINITPATH,\n        STRICT_OPTION,\n    ]:\n        with warnings.catch_warnings(record=True) as w:\n            warnings.warn(str(message.message), message.category)\n            assert w[0].category == message.category\n", "def test_deprecation_warnings():\n    with warnings.catch_warnings(record=True) as w:\n        # Test deprecation warning for FILLFUNCARGS\n        with pytest.warns(FILLFUNCARGS, match=\"fillfuncargs\"):\n            pytest.collect.fillfuncargs()\n\n        # Test deprecation warning for PYTEST_COLLECT_MODULE\n        with pytest.warns(PYTEST_COLLECT_MODULE, match=\"pytest.collect_module\"):\n            pytest.collect.Module()\n\n        # Test deprecation warning for MINUS_K_DASH\n        with pytest.warns(MINUS_K_DASH, match=\"The `-k '-expr'` syntax\"):\n            pytest.main([\"-k\", \"'-expr'\"])\n\n        # Test deprecation warning for MINUS_K_COLON\n        with pytest.warns(MINUS_K_COLON, match=\"The `-k 'expr:'` syntax\"):\n            pytest.main([\"-k\", \"'expr:'\"])\n\n        # Test deprecation warning for WARNING_CAPTURED_HOOK\n        with pytest.warns(WARNING_CAPTURED_HOOK, match=\"pytest_warning_captured\"):\n            pytest.warns(pytest_warning_captured)\n\n        # Test deprecation warning for FSCOLLECTOR_GETHOOKPROXY_ISINITPATH\n        with pytest.warns(FSCOLLECTOR_GETHOOKPROXY_ISINITPATH, match=\"FSCollector\"):\n            pytest.FSCollector()\n\n        # Test deprecation warning for STRICT_OPTION\n        with pytest.warns(STRICT_OPTION, match=\"The --strict option\"):\n            pytest.main([\"--strict\"])\n\n        # Test DEPRECATED_EXTERNAL_PLUGINS\n        with pytest.raises(ValueError):\n            pytest.register_plugin(\"pytest_catchlog\")\n", "def test_deprecated_warnings():\n    with pytest.warns(PytestDeprecationWarning, match=\"The `pytest.collect.{name} was moved to pytest.{name}\\n\"\n            \"Please update to the new name.\"):\n        pytest.collect.collected()  # Should raise PytestDeprecationWarning\n", "def test_DEPRECATED_EXTERNAL_PLUGINS():\n    with warnings.catch_warnings(record=True) as w:\n        pytest.main(['-k', 'foo:bar'])  # type: ignore\n        assert len(w) == 1\n        assert issubclass(w[-1].category, PytestDeprecationWarning)\n", "def test_DEPRECATED_EXTERNAL_PLUGINS():\n    # Test that all external plugins in DEPRECATED_EXTERNAL_PLUGINS are properly ignored\n    for plugin in DEPRECATED_EXTERNAL_PLUGINS:\n        with warnings.catch_warnings(record=True):\n            pytest.config.hookimpl(lambda: pytest_collect_session(session=None, startdir=None))\n            assert pytest.config.pluginmanager._external_plugins == {plugin: True}\n"], "sample_311": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.c1 = Category.objects.create(name='Category 1')\n        cls.c2 = Category.objects.create(name='Category 2')\n        cls.a1 = Answer.objects.create(\n            content='<p>Answer 1</p>',\n            date=datetime.datetime(2008, 3, 18, 11, 54, 58),\n            section=cls.s1,\n            title='Question 1',\n        )\n        cls.a2 = Answer.objects.create(\n            content='<p>Answer 2</p>',\n            date=datetime.datetime(2000, 3, 18, 11, 54, 58),\n            section=cls.s1,\n            title='Question 2',\n        )\n        cls.a3 = Answer.objects.create(\n            content='<p>Answer 3</p>', date=datetime.datetime(2009, 3, 18, 11, 54, 58), section=cls.s1\n        )\n        cls.p1 = Post.objects.create(title='A Long Title', published=True, slug='a-long-title')\n        cls.color1 = Color.objects.create(value='Red', warm=True)\n        cls.color2 = Color.objects.create(value='Orange', warm=True)\n        cls.color3 = Color.objects.create(value='Blue', warm=False)\n        cls.color4 = Color.objects.create(value='Green', warm=False)\n        cls.fab1 = Fabric.objects.create(surface='x')\n        cls.fab2 = Fabric.objects.create(surface='y')\n        cls.fab3 = Fabric.objects.create(surface='plain')\n        cls.b1 = Book.objects.create(name='Book 1')\n        cls.b2 = Book.objects.create(name='Book 2')\n        cls.pro1 = Promo.objects.create(name='Promo 1', book=cls.b1)\n        cls.pro2 = Promo.objects.create(name='Promo 2', book=cls.b2)\n        cls.chap1 = Chapter.objects.create(title='Chapter 1', content='[ insert contents here ]', book=cls.b1)\n        cls.chap2 = Chapter.objects.create(title='Chapter 2', content='[ insert contents here ]', book=cls.b1)\n        cls.chap3 = Chapter.objects.create(title='Chapter ", "    def test_custom_admin_site(self):\n        response = self.client.get(reverse('admin:index', current_app='customadmin'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Custom admin site')\n", "    def test_index_page_contains_header(self):\n        response = self.client.get('/admin/')\n        self.assertContains(response, 'Django administration')\n", "    def test_unregistered_model_admin(self):\n        with self.assertRaises(NotRegistered):\n            site.registered_models = {}\n            self.assertFalse(site.is_registered(Book))\n            site.unregister(Book)\n            self.assertFalse(site.is_registered(Book))\n            self.assertTrue(site.is_registered(AdminOrderedModelMethod))\n", "    def test_admin_site_urls(self):\n        response = self.client.get('/admin/')\n        self.assertEqual(response.status_code, 200)\n\n        url = '/admin/%s/%s/' % (Article._meta.app_label, Article._meta.model_name)\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        # Test URLs for a non-existent model\n        try:\n            reverse('admin:non_existent_model_change', args=['foo', 1])\n            self.fail(\"Expected a NoReverseMatch exception\")\n        except NoReverseMatch:\n            pass\n\n        # Test that we can make a reverse URL for an existing model\n        reverse('admin:%s_%s_change' % (Article._meta.app_label, Article._meta.model_name), args=[1])\n\n        # Test that the admin site has the correct title\n        response = self.client.get('/admin/')\n        self.assertContains(response, 'Site administration', status_code=200)\n\n        # Test that the admin site has the correct page title\n        response = self.client.get('/admin/%s/%s/' % (Article._meta.app_label, Article._meta.model_name))\n        self.assertContains(response, 'Article administration', status_code=200)\n\n        # Test that the admin site has the correct subtitle\n        response = self.client.get('/admin/')\n        self.assertEqual(response.context['subtitle'], None, status_code=200)\n\n        # Test that the admin site has the correct available apps\n        response = self.client.get('/admin/')\n        self.assertEqual(len(response.context['available_apps']), 2, status_code=200)\n\n        # Test that the admin site has the correct is_popup value\n        response = self.client.get('/admin/')\n        self.assertEqual(response.context['is_popup'], False, status_code=200)\n\n        # Test that the admin site has the correct is_nav_sidebar_enabled value\n        response = self.client.get('/admin/')\n        self.assertEqual(response.context['is_nav_sidebar_enabled'], True, status_code=200)\n\n        # Test the admin view for a model with no 'admin_view' defined\n        class NoAdminViewModel(ModelAdmin):\n            model = Article\n            modelAdmin = ModelAdmin\n\n        site.register(NoAdminViewModel)\n\n        try:\n            site.get_urls()\n            self.fail('Expected a AttributeError')\n        except AttributeError:\n            pass\n\n        # Test the admin view for a model with 'admin_view' defined\n        class MyAdminView"], "sample_730": ["    def test_default_init(self):\n        multi_task_elastic_net = MultiTaskElasticNet()\n        assert_equal(multi_task_elastic_net.fit_intercept, True)\n        assert_equal(multi_task_elastic_net.normalize, False)\n        assert_equal(multi_task_elastic_net.max_iter, 1000)\n        assert_equal(multi_task_elastic_net.tol, 0.0001)\n        assert_equal(multi_task_elastic_net.warm_start, False)\n        assert_equal(multi_task_elastic_net.random_state, None)\n        assert_equal(multi_task_elastic_net.selection, 'cyclic')\n", "def test_multitask_lasso_init():\n    l1 = MultiTaskLasso(alpha=0.1)\n    assert_equal(l1.alpha, 0.1)\n    assert_equal(l1.l1_ratio, 1.0)\n    assert_equal(l1.fit_intercept, True)\n    assert_equal(l1.normalize, False)\n    assert_equal(l1.max_iter, 1000)\n    assert_equal(l1.tol, 1e-4)\n    assert_equal(l1.warm_start, False)\n    assert_equal(l1.random_state, None)\n    assert_equal(l1.selection, 'cyclic')\n\n    l1 = MultiTaskLasso(alpha=0.1, l1_ratio=0.5)\n    assert_equal(l1.alpha, 0.1)\n    assert_equal(l1.l1_ratio, 0.5)\n    assert_equal(l1.fit_intercept, True)\n    assert_equal(l1.normalize, False)\n    assert_equal(l1.max_iter, 1000)\n    assert_equal(l1.tol, 1e-4)\n    assert_equal(l1.warm_start, False)\n    assert_equal(l1.random_state, None)\n    assert_equal(l1.selection, 'cyclic')\n", "def test_LassoCV__alpha_grid():\n    # Test that the alpha grid generated is correct\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    l1_ratio = 0.5\n    eps = 1e-3\n    n_alphas = 10\n    alphas = _alpha_grid(X, y, Xy=None, l1_ratio=l1_ratio, eps=eps,\n                         n_alphas=n_alphas, normalize=False)\n    assert_equal(len(alphas), n_alphas)\n", "def test_input_array_types():\n    # Test with integer array\n    X = np.array([[1, 2], [3, 4]], dtype=np.int32)\n    y = np.array([1, 2], dtype=np.int32)\n    model = ElasticNetCV()\n    model.fit(X, y)\n\n    # Test with float array\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n    y = np.array([1.0, 2.0], dtype=np.float32)\n    model = ElasticNetCV()\n    model.fit(X, y)\n\n    # Test with complex array\n    X = np.array([[1 + 2j, 2 + 4j], [3 + 4j, 4 + 8j]], dtype=np.complex128)\n    y = np.array([1 + 2j, 2 + 4j], dtype=np.complex128)\n    model = ElasticNetCV()\n    model.fit(X, y)\n\n    # Test with empty array\n    X = np.array([], dtype=np.int32)\n    y = np.array([], dtype=np.int32)\n    model = ElasticNetCV()\n    with pytest.raises(ValueError):\n        model.fit(X, y)\n\n    # Test with non-array input\n    X = object()\n    y = object()\n    model = ElasticNetCV()\n    with pytest.raises(TypeError):\n        model.fit(X, y)\n\n    # Test with invalid dtype\n    X = np.array([[1, 2], [3, 4]], dtype=np.string_)\n    y = np.array([1, 2], dtype=np.string_)\n    model = ElasticNetCV()\n    with pytest.raises(TypeError):\n        model.fit(X, y)\n", "def test_elasticnetcv_alpha_grid():\n    # The alpha_grid function should be tested with different inputs\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]])\n    y = np.array([1, 2, 3.1])\n    alphas = _alpha_grid(X, y, l1_ratio=0.5)\n    assert_array_almost_equal(alphas, [3.31231168e-01, 1.44013093e-01,\n                                      6.48321633e-02, 2.95506013e-02,\n                                      1.37752806e-02, 6.18864203e-03,\n                                      2.82842712e-03, 1.26491106e-03,\n                                      5.65764543e-04, 2.53914414e-04,\n                                      1.14457206e-04, 5.11985603e-05,\n                                      2.30992801e-05, 1.03649100e-05,\n                                      4.64511800e-06, 2.08855900e-06,\n                                      9.35262900e-07, 4.17631450e-07,\n                                      1.87665200e-07, 8.44413900e-08,\n                                      3.80154950e-08, 1.71362450e-08,\n                                      7.72209400e-09, 3.48589700e-09,\n                                      1.57681850e-09, 7.12400900e-10,\n                                      3.22910100e-10, 1.46373450e-10,\n                                      6.64320500e-11, 2.99698450e-11,\n                                      1.34949100e-11, 6.07344800e-12,\n                                      2.74258550e-12, 1.23346000e-12,\n                                      5.55249500e-13, 2.50922000e-13,\n                                      1.13464600e-13, 5.13204500e-14,\n                                      2.31395750e-"], "sample_568": ["def test_text_to_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Test default text color\n    text = art3d.Text3D(0, 0, 0, 'Hello')\n    ax.add_artist(text)\n    plt.savefig('test_default_text_color.png')\n\n    # Test text color\n    text = art3d.Text3D(0, 0, 0, 'Hello', color='r')\n    ax.add_artist(text)\n    plt.savefig('test_text_color.png')\n\n    # Test text color with alpha\n    text = art3d.Text3D(0, 0, 0, 'Hello', color='r', alpha=0.5)\n    ax.add_artist(text)\n    plt.savefig('test_text_color_alpha.png')\n\n    # Test text font size\n    text = art3d.Text3D(0, 0, 0, 'Hello', fontsize=20)\n    ax.add_artist(text)\n    plt.savefig('test_font_size.png')\n\n    # Test text font family\n    text = art3d.Text3D(0, 0, 0, 'Hello', fontname='sans-serif')\n    ax.add_artist(text)\n    plt.savefig('test_font_family.png')\n\n    # Test text horizontal alignment\n    text = art3d.Text3D(0, 0, 0, 'Hello', ha='center')\n    ax.add_artist(text)\n    plt.savefig('test_horizontal_alignment.png')\n\n    # Test text vertical alignment\n    text = art3d.Text3D(0, 0, 0, 'Hello', va='center')\n    ax.add_artist(text)\n    plt.savefig('test_vertical_alignment.png')\n\n    # Test text rotation\n    text = art3d.Text3D(0, 0, 0, 'Hello', rotation=45)\n    ax.add_artist(text)\n    plt.savefig('test_rotation.png')\n\n    # Test text rotation in degrees\n    text = art3d.Text3D(0, 0, 0, 'Hello', rotation_deg=45)\n    ax.add_artist(text)\n    plt.savefig('test_rotation_deg.png')\n\n    # Test 2D text converted to 3D\n    text = Text(0, 0, 'Hello')\n    art3d.text_2d_to_3d(text, z=", "def test_plot3D_collection_zsort():\n    fig, ax = plt.subplots()\n    poly = PolyCollection([[[1, 0, 0], [0, 1, 0], [0, 0, 1]],\n                          [[2, 0, 0], [0, 2, 0], [0, 0, 2]]],\n                         colors=['red', 'green'], alpha=0.5)\n    poly.set_zsort('min')\n    art3d.poly_collection_2d_to_3d(poly, zdir='x')\n    poly.set_sort_zpos(0)\n    poly.do_3d_projection()\n    ax.plot3D([1, 2], [0, 0], [1, 2], 'bo-')\n    plot_cuboid(ax, (1, 1, 1))\n    ax.set_xlim3d(-1, 3)\n    ax.set_ylim3d(-1, 3)\n    ax.set_zlim3d(-1, 3)\n\n    @check_figures_equal(extensions=[\"png\"])\n        return mpl3d_image_comparison(fig)\n", "def test_pathpatch3d_zorder():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    path = Path([[0, 0], [1, 0], [1, 1], [0, 1]])\n    patch = mpatch.PathPatch(path, facecolor='b', edgecolor='r')\n    ax.add_patch(patch)\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n    art3d.pathpatch_2d_to_3d(patch, z=1, zdir='z')\n    plt.show()\n\n    # Now, add another path patch and set its zorder to 10\n    path2 = Path([[0, 0], [1, 1], [1, 0]])\n    patch2 = mpatch.PathPatch(path2, facecolor='g', edgecolor='b', zorder=10)\n    art3d.pathpatch_2d_to_3d(patch2, z=2, zdir='z')\n    plt.show()\n\n    # Check that the second path patch has a higher z-order\n    assert ax.get_zorder(patch2) > ax.get_zorder(patch)\n", "def test_collection3d_depthshade():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2])\n    line3d = art3d.Line3D([0, 1, 2], [0, 1, 2], [0, 1, 2])\n    art3d.line_2d_to_3d(line3d, 0, 'z')\n    ax.add_line(line3d)\n    ax.set_xlim(-1, 3)\n    ax.set_ylim(-1, 3)\n    ax.set_zlim(-1, 3)\n    ax.set_facecolor('w')\n    ax.set_aspect('equal')\n    ax.set_alpha(0.7)\n\n    collection3d = art3d.Line3DCollection([[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]],\n                                           [[0.5, 0.5, 1], [1, 0.5, 1], [1, 1, 1], [0.5, 1, 1]]],\n                                          depths=[0, 1], color='r')\n    art3d.line_collection_2d_to_3d(collection3d, 0, 'z', depthshade=True)\n    ax.add_collection(collection3d)\n\n    fig.tight_layout()\n    plt.show()\n\n    assert collection3d.get_depthshade()\n    collection3d.set_depthshade(False)\n    assert not collection3d.get_depthshade()\n", "def test_poly3d_collection_verts_shape(vert_shape):\n    \"\"\"Ensure that Poly3DCollection accepts verts of arbitrary shape.\"\"\"\n    poly = art3d.Poly3DCollection([np.ones(vert_shape)])\n    assert poly.get_verts()[0].shape == vert_shape\n\n"], "sample_398": ["    def test_password_reset_view_with_valid_form(self):\n        url = reverse(\"password_reset\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        form = response.context[\"form\"]\n        self.assertEqual(form.__class__, PasswordResetForm)\n", "    def test_password_change_view_get_method(self):\n        \"\"\"Check that password change view responds to GET requests.\"\"\"\n        response = self.client.get(reverse(\"password_change\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_change_form.html\")\n", "    def test_password_reset_view(self):\n        # Test that a 200 response is returned when the view is accessed.\n        response = self.client.get(\"/password_reset/\")\n        self.assertEqual(response.status_code, 200)\n", "    def test_login_view_redirects_to_next_page(self):\n        self.client.force_login(self.u1)\n        response = self.client.get(\"/login/?next=/password_reset/\")\n        self.assertRedirects(response, \"/password_reset/\")\n", "    def test_password_reset_form_html(self):\n        response = self.client.get(\"/password_reset/\")\n        self.assertTemplateUsed(response, \"registration/password_reset_form.html\")\n"], "sample_439": ["    def test_fields_from_base_class(self):\n        class ParentForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        class ChildForm(ParentForm):\n            field3 = CharField()\n\n        self.assertEqual(ChildForm.declared_fields, {\"field1\": CharField(), \"field3\": CharField()})\n", "    def test_has_error_with_code(self):\n        form = FrameworkForm(data={\"name\": \"John\", \"language\": \"P\"})\n        form.add_error(\"language\", ValidationError(\"Invalid language\", code=\"invalid_language\"))\n        self.assertTrue(form.has_error(\"language\", \"invalid_language\"))\n        self.assertFalse(form.has_error(\"name\"))\n        self.assertFalse(form.has_error(\"language\", \"invalid_field\"))\n", "    def test_form_validation_error_message(self):\n        form = Person(data={\"first_name\": \"John\", \"last_name\": \"Doe\"})\n        form.add_error(\"last_name\", \"This field is required.\")\n        self.assertTrue(form.has_error(\"last_name\"))\n        self.assertEqual(form.errors[\"last_name\"], [\"This field is required.\"])\n", "    def test_render_as_p(self):\n        form = Person()\n        html = form.as_p()\n        # Check for the presence of a div tag\n        self.assertInHTML(\"<div\", html)\n        self.assertInHTML(\"</div>\", html)\n", "    def test_form_initial_data(self):\n        form = Person(initial={\"first_name\": \"John\"})\n        self.assertEqual(form.initial, {\"first_name\": \"John\", \"last_name\": \"\", \"birthday\": None})\n"], "sample_690": ["    def test_evaluate_condition_string(self, pytester: Pytester):\n        item = pytester.makeitem(\"test_evaluate_condition_string\")\n        mark = Mark(\"skipif\", {\"condition\": \"1 + 1 == 2\"})\n        result, reason = evaluate_condition(item, mark, mark.kwargs[\"condition\"])\n        assert result is False\n        assert reason == \"condition: 1 + 1 == 2\"\n", "def test_evaluate_condition_str_condition():\n    item = Item()\n    mark = Mark(name=\"skipif\", args=(), kwargs={\"reason\": \"foo\"})\n    condition = \"os.name == 'nt'\"\n    expected_result, expected_reason = (True, \"condition: os.name == 'nt'\")\n    assert evaluate_condition(item, mark, condition) == expected_result, expected_reason\n\n", "def test_evaluate_condition_string_condition():\n    # Given a string condition\n    item = Item(\"test example\")\n    mark = Mark(\"xfail\")\n    condition = \"os.name == 'nt'\"\n    result, reason = evaluate_condition(item, mark, condition)\n    assert result is True\n    assert reason == \"condition: os.name == 'nt'\"\n", "    def test_evaluate_condition_bool(self, pytester: Pytester):\n        item = item = pytester.makeitem(\n            \"test_func\",\n            name=\"test_func\",\n            nodeid=\"test_func\",\n            config=pytester.config,\n        )\n        item.config.option.runxfail = False\n        mark = Mark(\"xfail\", reason=None)\n        mark.kwargs = {\"condition\": True}\n        item.iter_markers = lambda name: [mark]\n        result, reason = evaluate_condition(item, mark, True)\n        assert result is True\n        assert reason == \"condition: True\"\n", "    def test_evaluate_condition_string_with_filename(self, pytester: Pytester) -> None:\n        pytester.makefile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(\"1 == 2\")\n                pass\n            \"\"\"\n        )\n        result = evaluate_condition(\n            item=pytester.pathogen.item(\"test_skip\"),\n            mark=pytester.pathogen.mark(\"skipif\"),\n            condition=\"1 == 2\",\n        )\n        assert result == (True, \"condition: 1 == 2\")\n"], "sample_96": ["    def test_list_display_links_none(self):\n        class TestModelAdmin(ModelAdmin):\n            list_display = ['field1', 'field2']\n            list_display_links = None\n\n        self.assertIsInvalid(TestModelAdmin, TestModel, \"The value of 'list_display_links' must be a list, a tuple, or None.\")\n", "    def test_search_fields_no_fields(self):\n        class ModelAdmin(ModelAdmin):\n            search_fields = None\n        self.assertIsInvalid(ModelAdmin, ValidationTestModel)\n", "    def test_list_filter_item(self):\n        class InvalidFilter(SimpleListFilter):\n            title = 'Invalid filter'\n            parameter_name = 'invalid_filter'\n\n        class MyAdmin(ModelAdmin):\n            list_filter = ('invalid_filter', InvalidFilter)\n\n        self.assertIsInvalid(MyAdmin, ValidationTestModel, \"'invalid_filter' must not inherit from 'FieldListFilter'.\")\n", "    def test_inline_model_admin_save_on_top_is_not_a_boolean(self):\n        # Test that save_on_top is not a boolean\n        admin_obj = ModelAdmin(model=ValidationTestModel, admin_site=AdminSite())\n        with self.assertRaises(ValueError):\n            InlineModelAdmin(model=ValidationTestInlineModel, parent_model=admin_obj.model, admin_site=admin_obj.admin_site, save_on_top=123)\n", "    def test_fieldsets_with_duplicate_fields(self):\n        class ValidationTestModelAdmin(ModelAdmin):\n            fieldsets = [\n                ('First', {'fields': ['title', 'artist', 'title']}),\n            ]\n\n        self.assertIsInvalid(\n            ValidationTestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'fieldsets[0][1]' must contain the key 'fields'.\",\n            id='admin.E011'\n        )\n"], "sample_304": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_1052": ["def test_C89CodeGen():\n    x, y, z = symbols('x y z')\n    routine = make_routine('test', Eq(x + y*z, 1), argument_sequence=[x, y])\n    c89_codegen = C89CodeGen()\n    code = get_string(c89_codegen.dump_c, [routine], 'file', header=False, empty=False)\n    assert code == \"\"\"double test(double x, double y)", "def test_codegen_named_arguments():\n    from sympy.abc import x, y\n    from sympy.utilities.codegen import codegen\n\n    source = get_string(CCodeGen.dump_c, [\n        make_routine('test', [x+y], argument_sequence=['x', 'y'], global_vars=None)])\n    expected = \"\"\"double test(double x, double y) {\n       double test_result;\n       test_result = x + y;\n       return test_result;\n    }\n    \"\"\"\n    assert source == expected\n", "def test_print_rust_function_with_args():\n    # Test printing a function with arguments in Rust\n    x, y, z = symbols('x y z')\n    code = make_routine('f', x + y * z, ('x', 'y'), language='RUST')\n    expected = \"\"\"/*\\n * %s Autogenerated by sympy\\n */\\n\\", "def test_make_routine_multivar():\n    x, y, z = symbols('x y z')\n    r = make_routine('test', [x*y, Eq(z, x + y)])\n    assert len(r.arguments) == 3\n    assert len(r.arguments[0].expr.free_symbols) == 3\n    assert len(r.results) == 1\n    assert r.arguments[2].name == 'z'\n    assert r.arguments[2].expr == x + y\n", "def test_codegen_Clang_fails_on_unsupported_function():\n    x, y = symbols('x y')\n    r = make_routine('test', [x/y], language=\"C\")\n    with raises(CodeGenError):\n        get_string(CCodeGen(dump_h=True, printer=None), [r])\n"], "sample_197": ["def test_timesince_depth_zero(self):\n    with self.assertRaises(ValueError):\n        timesince(self.t, depth=0)\n", "    def test_timesince_with_depth_zero(self):\n        self.assertRaises(ValueError, timesince, self.t, depth=0)\n", "    def test_timesince_with_depth_zero(self):\n        self.assertRaises(ValueError, timesince, self.t, depth=0)\n", "    def test_timesince_with_depth_zero(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=0)\n", "    def test_leap_year(self):\n        self.t = datetime.datetime(2000, 8, 14, 13, 46, 0)  # Leap year\n        self.assertEqual(timesince(self.t, time_strings=TIME_STRINGS), '1 year, 10 months')\n"], "sample_365": ["    def test_lazy_object_copy(self):\n            return [1, 2, 3]\n\n        lazy_obj = SimpleLazyObject(setupfunc)\n        self.assertIsNone(lazy_obj._wrapped)\n        self.assertEqual(lazy_obj.copy(), lazy_obj)\n", "    def test_lazy_object_attribute_access(self):\n        class LazyClass(LazyObject):\n                self._wrapped = 'wrapped object'\n\n        lazy_object = LazyClass()\n        self.assertIsInstance(lazy_object._wrapped, type(None))\n        self.assertEqual(lazy_object._wrapped, None)\n\n        # Test attribute access\n        lazy_object._setup()\n        self.assertEqual(lazy_object._wrapped, 'wrapped object')\n\n        # Test attribute setting\n        lazy_object._wrapped = 'new value'\n        self.assertEqual(lazy_object._wrapped, 'new value')\n\n        # Test attribute deletion\n        del lazy_object._wrapped\n        with self.assertRaises(TypeError):\n            delattr(lazy_object, '_wrapped')\n", "    def test_lazy_object_copying(self):\n            return object()\n\n        lazy_object = SimpleLazyObject(get_object)\n        lazy_object_copy = copy.copy(lazy_object)\n        self.assertIs(lazy_object, lazy_object_copy)\n", "    def test_lazy_property(self):\n        class Example:\n            @lazy\n                return 'example_value'\n\n            @cached_property\n                return self.get_value()\n\n        obj = Example()\n        self.assertEqual(obj.cached_value, 'example_value')\n        self.assertEqual(obj.get_value, obj.cached_value.fget)\n", "    def test_lazy_proxy_methods(self):\n        # Test that the proxy methods are correctly forwarded to the wrapped object\n        wrapped_obj = mock.Mock()\n        lazy_obj = lazy(lambda: wrapped_obj)(object)\n        self.assertEqual(lazy_obj.__str__(), '<__main__.object object at 0x...>')\n        wrapped_obj.name = 'Mock object'\n        self.assertEqual(lazy_obj.__str__(), 'Mock object')\n        self.assertEqual(lazy_obj.__eq__(wrapped_obj), True)\n        self.assertEqual(lazy_obj.__lt__(wrapped_obj), False)\n        self.assertEqual(lazy_obj.__hash__(), hash(wrapped_obj))\n        self.assertEqual(lazy_obj.__class__, object)\n        self.assertEqual(lazy_obj.__dir__(), dir(wrapped_obj))\n        self.assertEqual(lazy_obj.__len__(), 0)\n        self.assertEqual(lazy_obj.__contains__('test'), False)\n        self.assertEqual(lazy_obj.__getitem__('test'), None)\n        self.assertEqual(lazy_obj.__setitem__('test', 'value'), None)\n        self.assertEqual(lazy_obj.__delitem__('test'), None)\n        self.assertEqual(lazy_obj.__iter__(), iter(wrapped_obj))\n        self.assertEqual(lazy_obj.__getattribute__('name'), 'Mock object')\n"], "sample_183": ["    def setUpTestData(cls):\n        cls.model = Client.objects.create(id=1, name='John', age=30)\n        cls.model2 = Client.objects.create(id=2, name='Jane', age=25)\n        cls.model3 = Client.objects.create(id=3, name='Bob', age=40)\n", "    def test_add(self):\n        from .models import Client\n        client = Client.objects.create(\n            name='John Doe', email='john@example.com', integer=1\n        )\n        query = Client.objects.filter(name=F('name') + ' Doe')\n        self.assertEqual(query.count(), 1)\n        self.assertEqual(query.get().name, 'John Doe')\n", "    def test_funcs_with_empty_when_clauses(self):\n        with self.assertRaises(TypeError):\n            Case(When(), then='abc')\n        with self.assertRaises(TypeError):\n            Case(When(True, False), when='abc')\n        with self.assertRaises(TypeError):\n            Case(When(Q('foo')), when='abc')\n", "    def test_window_expression(self):\n        from django.db.models.functions import Window, RowRange\n        from django.db.models import F\n        from django.db.models.expressions import Case, When\n\n        CaseTestModel.objects.create(integer=1, integer2=1, string='1')\n        CaseTestModel.objects.create(integer=2, integer2=2, string='2')\n        CaseTestModel.objects.create(integer=3, integer2=3, string='3')\n\n        qs = CaseTestModel.objects.annotate(\n            w=Window(\n                F('integer'),\n                partition_by=[F('integer2')],\n                order_by=['-F(\"integer\")']\n            )\n        )\n        self.assertEqual(qs.query.group_by, ['integer2'])\n        self.assertEqual(qs.query.annotations['w'].source_expressions, [F('integer'), F('integer2')])\n\n        self.assertEqual(qs.annotate(\n            w=Window(\n                F('integer'),\n                partition_by=[F('integer2')],\n                order_by=['-F(\"integer\")']\n            )\n        ).values('w').query.annotations['w'].source_expressions, [F('integer'), F('integer2')])\n        self.assertEqual(qs.annotate(\n            w=Window(\n                F('integer'),\n                partition_by=[F('integer2')],\n                order_by=['-F(\"integer\")']\n            )\n        ).values('w').query.group_by, ['integer2'])\n", "    def test_resolve_expression(self):\n        from django.db.models.expressions import When, When, Q, Case\n        when = When(\n            Q(integer=1), then=F('integer2'),\n            Q(integer=2), then=F('integer'),\n            default=F('string')\n        )\n        self.assertEqual(when.output_field, fields.CharField())\n        expression = when.resolve_expression(queryset=CaseTestModel.objects.all())\n        self.assertEqual(expression.cases[0].condition, Q(integer=1))\n        self.assertEqual(expression.cases[1].condition, Q(integer=2))\n        self.assertEqual(expression.cases[0].result.output_field, fields.CharField())\n        self.assertEqual(expression.cases[1].result.output_field, fields.CharField())\n        self.assertEqual(expression.default.output_field, fields.CharField())\n"], "sample_857": ["def test_fit_early_stopping():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    clf = DecisionTreeClassifier(min_impurity_split=1.0)\n    with pytest.raises(NotFittedError):\n        clf.predict(X)\n\n    clf.fit(X, y)\n    clf.predict(X)\n", "def test_decision_tree_multiple_outputs():\n    # Test decision tree on a multiple outputs problem\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([[1, 2], [3, 4], [5, 6]])\n\n    # Fit a decision tree regressor\n    dt = DecisionTreeRegressor()\n    dt.fit(X, y)\n\n    # Test that the decision tree regressor can handle multiple outputs\n    # correctly\n    assert dt.n_outputs_ == 2\n    assert dt.predict(X).shape == (3, 2)\n\n    # Test that the decision tree regressor can handle multiple outputs\n    # correctly when the outputs are not all numeric\n    y = np.array([[1, 2.5], [3.5, 4], [5, 6]])\n    dt.fit(X, y)\n    assert dt.n_outputs_ == 2\n    assert dt.predict(X).shape == (3, 2)\n\n    # Test that the decision tree regressor can handle multiple outputs\n    # correctly when the outputs are of different types\n    y = np.array([[1, \"a\"], [3.5, 4], [\"b\", 6]])\n    dt.fit(X, y)\n    assert dt.n_outputs_ == 2\n    assert dt.predict(X).shape == (3, 2)\n\n    # Test that the decision tree regressor can handle multiple outputs\n    # correctly when the outputs are of different shapes\n    y = np.array([[1, np.array([2, 3])], [3.5, 4], [\"b\", 6]])\n    dt.fit(X, y)\n    assert dt.n_outputs_ == 2\n    assert dt.predict(X).shape == (3, 2)\n\n    # Test that the decision tree regressor can handle multiple outputs\n    # correctly when the outputs are of different sizes\n    y = np.array([[1, [2, 3, 4]], [3.5, 4], [\"b\", 6]])\n    dt.fit(X, y)\n    assert dt.n_outputs_ == 2\n    assert dt.predict(X).shape == (3, 2)\n", "    def test_min_impurity_decrease(self):\n        X, y = datasets.load_digits(return_X_y=True)\n        for tree_type in ALL_TREES:\n            tree_ = ALL_TREES[tree_type](random_state=42)\n            tree_.set_params(min_impurity_decrease=0.0)\n            tree_.set_params(min_impurity_split=1e-7)\n            tree.fit(X, y)\n            tree_ = ALL_TREES[tree_type](random_state=42)\n            tree_.set_params(min_impurity_split=0.0)\n            tree_.set_params(ccp_alpha=1e-7)\n            tree.fit(X, y)\n            assert_almost_equal(tree_.tree_.cost_complexity_pruning_path(X, y)[1][0],\n                               tree.tree_.impurity.sum())\n", "def test_tree_fit_on_empty_data():\n    with TempMemmap() as tmp_file:\n        X = np.empty((0, 5))\n        y = np.empty(0)\n        with pytest.raises(ValueError):\n            DecisionTreeClassifier().fit(X, y)\n\n        with pytest.raises(ValueError):\n            DecisionTreeRegressor().fit(X, y)\n\n", "def test_DTree_confidence_interval():\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([-1, -1, -1, 1, 1, 1])\n\n    for tree_type in [\"DecisionTreeClassifier\", \"DecisionTreeRegressor\"]:\n        tree = ALL_TREES[tree_type](criterion=\"entropy\")\n        with pytest.raises(NotFittedError):\n            tree.predict(X)\n\n        tree.fit(X, y)\n        X_pred = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n        pred = tree.predict(X_pred)\n        assert isinstance(pred, np.ndarray)\n\n    for tree_type in [\"ExtraTreeClassifier\", \"ExtraTreeRegressor\"]:\n        tree = ALL_TREES[tree_type](criterion=\"entropy\")\n        with pytest.raises(NotFittedError):\n            tree.predict(X)\n\n        tree.fit(X, y)\n        X_pred = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5]])\n        pred = tree.predict(X_pred)\n        assert isinstance(pred, np.ndarray)\n\n    for sparse_tree in SPARSE_TREES:\n        for sparse in [True, False]:\n            X_sparse = X if sparse else csc_matrix(X)\n            X_pred = X if sparse else csc_matrix(X)\n\n            for tree_type in [sparse_tree]:\n                tree = ALL_TREES[tree_type](criterion=\"entropy\")\n                with pytest.raises(NotFittedError):\n                    tree.predict(X_sparse)\n\n                tree.fit(X_sparse, y)\n                pred = tree.predict(X_pred)\n                assert isinstance(pred, np.ndarray)\n\n                # Check that the tree is still in memory\n                tree_copy = copy.deepcopy(tree)\n                assert tree_copy.tree_ is not None\n"], "sample_1201": ["def test_cgs_gauss_system():\n    # Test if cgs_gauss system is correctly defined\n    assert cgs_gauss.name == \"cgs_gauss\"\n    assert len(cgs_gauss.base_units) == 3\n    assert cgs_gauss.base_units == [centimeter, gram, second]\n    assert len(cgs_gauss.units) == 0\n\n    # Test if dimension system is correctly extended\n    assert cgs_gauss.dimension_system == dimsys_length_weight_time\n    assert cgs_gauss.dimension_system._new_dims['impedance'] == (1, -1)\n    assert cgs_gauss.dimension_system._new_dims['conductance'] == (-1, 1)\n    assert cgs_gauss.dimension_system._new_dims['capacitance'] == (1,)\n    assert cgs_gauss.dimension_system._new_dims['inductance'] == (2, -1)\n    assert cgs_gauss.dimension_system._new_dims['charge'] == (0.5, 1.5, -1)\n    assert cgs_gauss.dimension_system._new_dims['current'] == (0.5, 1.5, -2)\n    assert cgs_gauss.dimension_system._new_dims['voltage'] == (-0.5, 0.5, -1)\n    assert cgs_gauss.dimension_system._new_dims['magnetic_density'] == (-0.5, 0.5, -1)\n    assert cgs_gauss.dimension_system._new_dims['magnetic_flux'] == (1.5, 0.5, -1)\n\n    # Test if quantity dimensions and scale factors are correctly set\n    assert cgs_gauss.get_quantity_dimension(statcoulomb) == charge\n    assert cgs_gauss.get_quantity_dimension(coulomb) == charge\n    assert cgs_gauss.get_quantity_dimension(statampere) == current\n    assert cgs_gauss.get_quantity_dimension(statvolt) == voltage\n    assert cgs_gauss.get_quantity_dimension(volt) == voltage\n    assert cgs_gauss.get_quantity_dimension(gauss) == magnetic_density\n    assert cgs_gauss.get_quantity_dimension(tesla) == magnetic_density\n    assert cgs_gauss.get_quantity_dimension(maxwell) == magnetic_flux\n    assert cgs_gauss.get_quantity_dimension(coulomb_constant) == 1\n\n    # Test if quantity scale factors are correctly set\n   ", "def test_cgs_gauss_dimensionality():\n    # Test dimensionality of cgs_gauss unit system\n    assert cgs_gauss.base_units == [centimeter, gram, second]\n    assert cgs_gauss.units == []\n    assert cgs_gauss.name == \"cgs_gauss\"\n    assert cgs_gauss.dimension_system == dimsys_cgs\n\n    # Test quantity scale factors\n    assert cgs_gauss.quantity_scale_factors == {\n        coulomb_constant: 1,\n        coulomb: centimeter**(S(3)/2)*gram**(S.Half)/second,\n        statcoulomb: centimeter**(S(3)/2)*gram**(S.Half)/second,\n        statampere: statcoulomb/second,\n        statvolt: erg/statcoulomb,\n        volt: volt,\n        gauss: sqrt(gram/centimeter)/second,\n        tesla: gauss,\n        maxwell: sqrt(centimeter**3*gram)/second,\n        debye: One/10**18*statcoulomb*centimeter,\n        oersted: sqrt(gram/centimeter)/second,\n        ohm: 10**5/speed_of_light**2*second/centimeter,\n        farad: One/10**5*speed_of_light**2*centimeter,\n        henry: 10**5/speed_of_light**2/centimeter*second**2,\n        coulombs_constant: 1\n    }\n\n    # Test quantity dimensions\n    assert cgs_gauss.quantity_dimensions == {\n        coulomb: charge,\n        coulomb_constant: 1,\n        statcoulomb: charge,\n        statampere: current,\n        statvolt: voltage,\n        volt: voltage,\n        gauss: magnetic_density,\n        tesla: magnetic_density,\n        maxwell: magnetic_flux,\n        debye: charge,\n        oersted: current,\n        ohm: impedance,\n        farad: capacitance,\n        henry: inductance,\n    }\n\n    # Test conversion to SI units\n    assert convert_to(coulomb, SI).expr == coulomb * (speed_of_light * statcoulomb / coulomb)\n    assert convert_to(ampere, SI).expr == ampere * (speed_of_light * statcoulomb / coulomb) / second\n    assert convert_to(volt, SI).expr == volt *", "def test_set_quantity_scale_factor_for_length():\n    # Test that the set_quantity_scale_factor method works correctly for length\n    cgs_gauss = cgs_gauss_gauss\n    cgs_gauss.set_quantity_scale_factor(length, 10*centimeter)\n    assert cgs_gauss.scale_factors[length] == 10*centimeter\n    assert cgs_gauss.scale_factors[centimeter] == 10\n", "def test_cgs_gauss_conversion():\n    # Test conversion of unit coulomb\n    assert convert_to(coulomb, statcoulomb, cgs_gauss).simplify() == 1  # 1 statC in cgs_gauss units\n    \n    # Test conversion of unit ampere\n    assert convert_to(ampere, statampere, cgs_gauss).simplify() == 10*speed_of_light  # 10*statA in SI units\n    \n    # Test conversion of unit volt\n    assert convert_to(volt, statvolt, cgs_gauss).simplify() == 10**6/speed_of_light  # 10^6/statV in SI units\n    \n    # Test conversion of unit weber\n    assert convert_to(weber, maxwell, cgs_gauss).simplify() == 10**8  # 10^8 maxwell in SI units\n    \n    # Test conversion of unit tesla\n    assert convert_to(tesla, gauss, cgs_gauss).simplify() == 10**4  # 10^4 gauss in SI units\n    \n    # Test conversion of unit debye\n    assert convert_to(debye, statcoulomb*centimeter, cgs_gauss).simplify() == One/10**18  # 1/(10^18 statC*cm) in SI units\n    \n    # Test conversion of unit oersted\n    assert convert_to(oersted, (gram/centimeter)/second, cgs_gauss).simplify() == 1  # 1/(G/cm/s) in SI units\n    \n    # Test conversion of unit ohm\n    assert convert_to(ohm, (second/centimeter)/(speed_of_light**2), cgs_gauss).simplify() == 10**5  # 10^5/(cm/s^2*speed_of_light^2) in SI units\n    \n    # Test conversion of unit farad\n    assert convert_to(farad, (speed_of_light**2*centimeter)/One, cgs_gauss).simplify() == 10**5  # 10^5*speed_of_light^2/cm in SI units\n    \n    # Test conversion of unit henry\n    assert convert_to(henry, (speed_of_light**2*centimeter*second**2)/One, cgs_gauss).simplify() == 10", "def test_convert_to_cgs_gauss():\n    # Test conversion from SI to cgs-gauss system\n    value = coulomb_constant\n    converted_value = convert_to(value, cgs_gauss)\n    assert converted_value == 1\n\n    # Test conversion from SI to cgs-gauss system\n    value = elementary_charge\n    converted_value = convert_to(value, cgs_gauss)\n    assert converted_value == 4.803e-10 * statcoulomb\n\n    # Test conversion from cgs-gauss to SI system\n    value = charge\n    converted_value = convert_to(value, SI)\n    assert converted_value == 10 * speed_of_light * coulomb\n\n    # Test conversion from cgs-gauss to cgs-gauss system (should be identity)\n    value = statcoulomb\n    converted_value = convert_to(value, cgs_gauss)\n    assert converted_value == statcoulomb\n\n    # Test conversion of an expression with multiple units to cgs-gauss system\n    value = joule / (meter ** 2)\n    converted_value = convert_to(value, cgs_gauss)\n    assert converted_value == erg / centimeter ** 2\n\n    # Test conversion of an expression with multiple units to SI system\n    value = newton * meter\n    converted_value = convert_to(value, SI)\n    assert converted_value == joule / meter\n"], "sample_614": ["def test_inline_variable_array_repr():\n    da = xr.DataArray(np.arange(10), dims=[\"x\"])\n    max_width = 20\n    result = formatting.inline_variable_array_repr(da, max_width)\n    expected = \"0 4 8\"  # truncated representation of DataArray\n    assert result == expected\n\n    max_width = 10\n    result = formatting.inline_variable_array_repr(da, max_width)\n    expected = \"...\"  # ellipsis representation due to exceeding max_width\n    assert result == expected\n\n    da = xr.DataArray(np.arange(10), dims=[\"x\"], attrs={\"units\": \"m\"})\n    max_width = 20\n    result = formatting.inline_variable_array_repr(da, max_width)\n    expected = \"0 4 8 (units=m)\"  # representation of DataArray with attrs\n    assert result == expected\n", "def test_limit_lines():\n    # Test that limit_lines handles string splitting and ellipsis correctly\n    s = dedent(\n        \"\"\"\\\n        line 1\n        line 2\n        line 3\n        line 4\n        line 5\n        line 6\n        line 7\n        line 8\n        line 9\n        line 10\n        \"\"\"\n    )\n    assert limit_lines(s, limit=5) == dedent(\n        \"\"\"\\\n        line 1\n        line 2\n        ...\n        line 7\n        line 8\n        line 9\n        line 10\n        \"\"\"\n    )\n\n    # Test that limit_lines handles multi-line strings correctly\n    s = dedent(\n        \"\"\"\\\n        line 1\n        line 2\n        line 3\n\n        line 4\n        line 5\n        line 6\n\n        line 7\n        line 8\n        line 9\n        \"\"\"\n    )\n    assert limit_lines(s, limit=5) == dedent(\n        \"\"\"\\\n        line 1\n        line 2\n        ...\n        line 3\n\n        line 7\n        line 8\n        ...\n        line 9\n        \"\"\"\n    )\n\n    # Test that limit_lines leaves single-line strings unchanged\n    s = dedent(\n        \"\"\"\\\n        line 1 line 2 line 3\n        \"\"\"\n    )\n    assert limit_lines(s, limit=5) == s\n\n    # Test that limit_lines leaves short multi-line strings unchanged\n    s = dedent(\n        \"\"\"\\\n        line 1\n        line 2\n        \"\"\"\n    )\n    assert limit_lines(s, limit=10) == s\n\n    # Test that limit_lines raises a TypeError for non-string input\n    with pytest.raises(TypeError):\n        limit_lines(42, limit=5)\n", "def test_format_timestamp():\n    # Test for format_timestamp function\n    # Test with valid timestamp\n    t = pd.Timestamp('2022-01-01 12:00:00')\n    assert formatting.format_timestamp(t) == \"2022-01-01T12:00:00\"\n    \n    # Test with invalid timestamp\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp('2023-02-29 12:00:00')\n    \n    # Test with NaT\n    assert formatting.format_timestamp(pd.NaT) == 'NaT'\n", "    def test_inline_variable_array_repr(self):\n        arr = np.arange(10)\n        var = xr.DataArray(arr)\n        with pytest.raises(TypeError):\n            formatting.inline_variable_array_repr(arr, 5)\n        result = formatting.inline_variable_array_repr(var, 50)\n        assert result == \"0  1  2  3  4  5  6  7  8  9\"\n\n        max_width = 20\n        result = formatting.inline_variable_array_repr(var, max_width)\n        assert len(result.split()) == max_width // 2 + 1\n        result = formatting.inline_variable_array_repr(var, 15)\n        assert len(result.split()) == 3\n", "def test_summarize_variable_with_index(self):\n    # Test summarizing a variable with an index\n    da = xr.DataArray([1, 2, 3], dims=[\"time\"])\n    ds = xr.Dataset({\"var\": da})\n    ds.coords[\"time\"] = da.time\n\n    result = formatting.summarize_variable(\"var\", ds[\"var\"], 20, is_index=True)\n    expected = \"  * var time* (time) np.int64  [1 2 3]\"\n    assert result == expected\n\n"], "sample_630": ["def test_dotwriter_write_packages(setup):\n    \"\"\"test packages diagram writer\"\"\"\n    expected_dot_file = \"packages_No_Name.dot\"\n    expected_file_path = os.path.join(os.path.dirname(__file__), \"data\", expected_dot_file)\n    with open(expected_file_path, \"r\") as expected_stream:\n        expected_lines = _file_lines(expected_stream.name)\n    generated_lines = _file_lines(writer.file_name)\n    diff = unified_diff(\n        expected_lines, generated_lines, fromfile=expected_dot_file, tofile=writer.file_name\n    )\n    assert not diff\n\n", "def test_diagram_writer_close_graph_called():\n    \"\"\"test if close_graph is called for each diagram\"\"\"\n\n    class TestDiagramWriter(DiagramWriter):\n            super().__init__(config, [{\"title\": \"diagram1\"}, {\"title\": \"diagram2\"}])\n\n            for diagram in diadefs:\n                self.close_graph()\n\n    config = Config()\n    writer = TestDiagramWriter(config)\n    writer.write([{\"title\": \"diagram1\"}, {\"title\": \"diagram2\"}])\n\n    assert len(writer.close_graph.mock_calls) == 2\n", "def test_dot_writer_close_graph():\n    \"\"\"test close_graph method\"\"\"\n    writer = DotWriter(CONFIG)\n    graph_file = tempfile.NamedTemporaryFile(suffix=\".dot\", mode=\"w+\")\n    writer.file_name = graph_file.name\n    writer.close_graph()\n    with open(graph_file.name, \"r\") as f:\n        dot_graph = f.readlines()\n    assert \"digraph\" in dot_graph[0]\n    assert \"}\" in dot_graph[-1]\n", "def test_dot_writer_write_classes_file(tmp_path):\n    \"\"\"test write_classes method with file creation\"\"\"\n    config = Config()\n    config.output_format = \"dot\"\n    config.output_directory = tmp_path\n    writer = DotWriter(config)\n    writer.write([astroid.Module(\"module\", astroidMOD, \"module\")])\n    generated_file = tmp_path / \"classes_No_Name.dot\"\n    assert generated_file.exists()\n    with open(generated_file, \"r\") as f:\n        content = f.read()\n    expected_content = _file_lines(os.path.join(os.path.dirname(__file__), \"expected_classes.dot\"))\n    diff = unified_diff(expected_content, content.splitlines(), fromfile=\"expected_classes.dot\", tofile=generated_file.name)\n    assert not diff\n", "def test_get_values_ClassWithAttributesAndMethods(self):\n    \"\"\"check that get_values returns the correct label for a class with attributes and methods\"\"\"\n    classes = [\n        astroid.Class(\n            name=\"Class1\",\n            is_abstract=False,\n            is_interface=False,\n            attributes=[astroid.Attribute(name=\"attr1\", parent=\"Class1\")],\n            methods=[\n                astroid.FunctionDef(\n                    name=\"method1\", args=astroid.Arguments(args=[astroid.arg(arg=\"arg1\")])\n                )\n            ],\n        ),\n        astroid.Class(\n            name=\"Class2\",\n            is_abstract=False,\n            is_interface=False,\n            attributes=[astroid.Attribute(name=\"attr2\", parent=\"Class2\")],\n            methods=[\n                astroid.FunctionDef(\n                    name=\"method2\", args=astroid.Arguments(args=[astroid.arg(arg=\"arg2\")])\n                )\n            ],\n        ),\n    ]\n    writer = DotWriter(Config())\n    obj = classes[0]\n    result = writer.get_values(obj)\n    expected_label = (\n        \"Class1\"\n        + \"|\"\n        + \"attr1\"\n        + \"|\"\n        + \"method1(arg1):\"\n    )\n    assert result[\"label\"] == expected_label\n"], "sample_190": ["    def setUpTestData(cls):\n        # Create a few Authors.\n        cls.au1 = Author.objects.create(name='Author 1', alias='a1')\n        cls.au2 = Author.objects.create(name='Author 2', alias='a2')\n        # Create a few Articles.\n        cls.a1 = Article.objects.create(\n            headline='Article 1',\n            pub_date=datetime(2005, 7, 26),\n            author=cls.au1,\n            slug='a1',\n        )\n        cls.a2 = Article.objects.create(\n            headline='Article 2',\n            pub_date=datetime(2005, 7, 27),\n            author=cls.au1,\n            slug='a2',\n        )\n        cls.a3 = Article.objects.create(\n            headline='Article 3',\n            pub_date=datetime(2005, 7, 27),\n            author=cls.au1,\n            slug='a3',\n        )\n        cls.a4 = Article.objects.create(\n            headline='Article 4',\n            pub_date=datetime(2005, 7, 28),\n            author=cls.au1,\n            slug='a4',\n        )\n        cls.a5 = Article.objects.create(\n            headline='Article 5',\n            pub_date=datetime(2005, 8, 1, 9, 0),\n            author=cls.au2,\n            slug='a5',\n        )\n        cls.a6 = Article.objects.create(\n            headline='Article 6',\n            pub_date=datetime(2005, 8, 1, 8, 0),\n            author=cls.au2,\n            slug='a6',\n        )\n        cls.a7 = Article.objects.create(\n            headline='Article 7',\n            pub_date=datetime(2005, 7, 27),\n            author=cls.au2,\n            slug='a7',\n        )\n        # Create a few Tags.\n        cls.t1 = Tag.objects.create(name='Tag 1')\n        cls.t1.articles.add(cls.a1, cls.a2, cls.a3)\n        cls.t2 = Tag.objects.create(name='Tag 2')\n        cls.t2.articles.add(cls.a3, cls.a4, cls.a5)\n        cls.t3 = Tag.objects.create(name='Tag 3')\n        cls.t3.articles.add(cls.a5, cls.a6, cls.a7)\n", "    def test_in_lookup_with_none_values(self):\n        author = Author.objects.create(name='Author 3', alias='a3')\n        Article.objects.create(headline='Article 8', pub_date=datetime(2005, 8, 1, 9, 0), author=author, slug='a8')\n\n        in_lookup = In(OuterRef('id'), [self.au1.id, self.au2.id, None, self.au3.id])\n        qs = Article.objects.annotate(exists=Exists(in_lookup)).filter(exists=True)\n        self.assertEqual(list(qs.values_list('id')), [self.a1.id, self.a2.id, self.a3.id])\n", "    def test_exact_lookup_with_query_rhs(self):\n        \"\"\"\n        Regression test for #28118.\n        \"\"\"\n        q = Article.objects.filter(author__exact=Author.objects.filter(pk__gt=0))\n        self.assertEqual(list(q), [self.a1, self.a2, self.a3, self.a4])\n", "    def test_exists_lookup(self):\n        class ExistsLookupTest(models.Model):\n            exists = models.Exists(models.Q(author__name='Author 1'))\n\n        queryset = ExistsLookupTest.objects.all()\n        self.assertEqual(queryset.count(), 0)\n", "    def test_invalid_lhs_type(self):\n        with self.assertRaises(TypeError):\n            # Test the following line raises a TypeError\n            Exact(lhs=5, rhs='hello')\n"], "sample_538": ["def test_Bbox_splity():\n    # Create a box\n    box = Bbox([[0, 0], [1, 1]])\n\n    # Split the box into two horizontal boxes\n    boxes = box.splity(0.5)\n    assert_array_equal(boxes, [Bbox([[0, 0], [1, 0.5]), Bbox([[0, 0.5], [1, 1])])\n\n    # Split the box into two horizontal boxes\n    boxes = box.splity([0, 0.5, 1])\n    assert_array_equal(boxes, [Bbox([[0, 0], [1, 0]),\n                                Bbox([[0, 0.5], [1, 0.5]),\n                                Bbox([[0, 1], [1, 1])])\n\n    # Test that splity with no arguments raises an error\n    with pytest.raises(ValueError):\n        box.splity()\n\n    # Test that splity with an argument outside the valid range raises an error\n    with pytest.raises(ValueError):\n        box.splity(-1)\n\n    # Test that splity with an argument larger than 1 raises an error\n    with pytest.raises(ValueError):\n        box.splity(2)\n", "    def test_blended_transform(self):\n        # Create a blended transform with two Affine2D instances\n        x_transform = mtransforms.Affine2D().scale(2, 1)\n        y_transform = mtransforms.Affine2D().scale(1, 2)\n        blended_transform = mtransforms.blended_transform_factory(x_transform, y_transform)\n\n        # Test that the blended transform is indeed a blended transform\n        assert isinstance(blended_transform, mtransforms.BlendedAffine2D)\n\n        # Test that the blended transform has the correct depth\n        assert blended_transform.depth == 2\n\n        # Test that the blended transform has the correct affine property\n        assert blended_transform.is_affine\n\n        # Test that the blended transform has the correct separable property\n        assert blended_transform.is_separable\n\n        # Test that the blended transform has the correct inverse property\n        assert blended_transform.has_inverse\n\n        # Test that the blended transform is correctly serialized and deserialized\n        serialized_transform = mtransforms.to_values(blended_transform)\n        deserialized_transform = mtransforms.from_values(*serialized_transform)\n        assert deserialized_transform == blended_transform\n\n        # Test that the blended transform can be frozen and thawed\n        frozen_transform = blended_transform.frozen()\n        thawed_transform = frozen_transform.thaw()\n        assert thawed_transform == blended_transform\n", "def test_offset_copy_units(unit, x, y):\n    fig, ax = plt.subplots()\n    ax.add_patch(mpatches.Rectangle((0, 0), 1, 1, facecolor='r'))\n    trans = mtransforms.offset_copy(ax.transData, fig, x, y, unit)\n    x0, y0 = 0.3, 0.3\n    x1, y1 = 0.8, 0.8\n    assert interval_contains(trans.transform((x0, y0)), (x0 + x, y0 + y))\n    assert interval_contains(trans.transform((x1, y1)), (x1 + x, y1 + y))\n    assert not interval_contains(trans.transform((x0, y0)), (x0 - x, y0 - y))\n    assert not interval_contains(trans.transform((x1, y1)), (x1 - x, y1 - y))\n    assert interval_contains_open(trans.transform((x0, y0)), (x0 + x, y0 + y))\n    assert interval_contains_open(trans.transform((x1, y1)), (x1 + x, y1 + y))\n    assert not interval_contains_open(trans.transform((x0, y0)), (x0 - x, y0 - y))\n    assert not interval_contains_open(trans.transform((x1, y1)), (x1 - x, y1 - y))\n", "    def test_scale(self):\n        # Create an Affine2D instance and scale it\n        aff = Affine2D().scale(2, 3)\n        assert aff.transform([1, 2]) == [2, 6]\n\n        # Create an Affine2D instance and rotate it\n        aff = Affine2D().rotate(math.pi / 2)\n        assert aff.transform([1, 2]) == [-2, 1]\n\n        # Create an Affine2D instance and translate it\n        aff = Affine2D().translate(1, 2)\n        assert aff.transform([1, 2]) == [2, 4]\n\n        # Create an Affine2D instance and skew it\n        aff = Affine2D().skew(math.pi / 4, math.pi / 4)\n        assert aff.transform([1, 2]) == [1.4142135623730951, 2.8284271247461903]\n", "    def test_affine_transform(self):\n        # Test the Affine2D class\n        # Create a 2D affine transformation that scales and translates\n        # a figure\n        scale_trans = mtransforms.Affine2D().scale(1.5, 2.0)\n        trans = mtransforms.Affine2D().translate(1.0, 2.0)\n        a = Affine2DBase(scale_trans)\n        b = Affine2DBase(trans)\n        \n        # Create a new composite transform\n        c = a + b\n        \n        # Check that the composite transform is correctly set up\n        assert c.input_dims == 2\n        assert c.output_dims == 2\n        assert c.is_separable\n        assert c.is_affine\n        \n        # Check that the composite transform can be applied to a point\n        point = np.array([0.0, 0.0])\n        transformed_point = c.transform(point)\n        assert np.allclose(transformed_point, np.array([2.0, 4.0]))\n        \n        # Check that the composite transform can be inverted\n        inverse_transformed_point = c.inverted().transform(transformed_point)\n        assert np.allclose(inverse_transformed_point, point)\n"], "sample_683": ["    def test_is_capturing(self, capman: CaptureManager, method: str) -> None:\n        capman._method = method\n        assert capman.is_capturing() == \"global\"\n", "    def test_suspend_resume(self, capsys, capman, method, in_):\n        # Arrange\n        capman._method = method\n        capman._global_capturing = None\n        capman._capture_fixture = None\n        capsys._capture = None\n        capsys._captured_out = capsys._captureclass.EMPTY_BUFFER\n        capsys._captured_err = capsys._captureclass.EMPTY_BUFFER\n\n        # Act\n        capman.suspend_global_capture(in_)\n        assert capman.is_capturing() == \"global\"\n        assert capman.is_globally_capturing()\n        assert not capman._global_capturing._in_suspended\n        capman.resume_global_capture()\n        assert not capman.is_globally_capturing()\n        assert not capman._global_capturing._in_suspended\n", "    def test_captures_output_in_threads(self, method: str, capman: CaptureManager):\n        # check the \"tee-sys\" and \"fd\" cases are implemented correctly\n        capman._method = method\n        capman.start_global_capturing()\n\n            print(\"Hello from thread!\")\n\n        with pytest.raises(capture.UnsupportedOperation):\n            open(\"/dev/null\").read()\n\n        with pytest.raises(capture.UnsupportedOperation):\n            sys.stdin.buffer.read()\n\n        import threading\n        t = threading.Thread(target=worker)\n        t.start()\n        t.join()\n\n        out, err = capman.read_global_capture()\n        assert \"Hello from thread!\\n\" in out\n        assert \"Hello from thread!\\n\" not in err\n\n        capman.suspend_global_capture(True)\n        with pytest.raises(capture.UnsupportedOperation):\n            open(\"/dev/null\").read()\n        with pytest.raises(capture.UnsupportedOperation):\n            sys.stdin.buffer.read()\n        capman.resume_global_capture()\n\n        capman.stop_global_capturing()\n", "    def test_catch_keyboard_interrupt(self, method):\n        capman = CaptureManager(method)\n        with pytest.raises(SystemExit):\n            capman.start_global_capturing()\n            with capman.global_and_fixture_disabled():\n                pytest.main([\"-s\", \"--capture=onexit\", \"-x\", \"-k\", \"test_something\"])\n        out, err = capman.read_global_capture()\n        assert \"KeyboardInterrupt\" in out or \"KeyboardInterrupt\" in err\n", "    def test_capturing_method(self, capman: CaptureManager, method: str) -> None:\n        # Test that the correct capture method is used when a test is run with\n        # a specific capturing method, even if a fixture is also used.\n        capman._method = method\n        with capsys(capture.SysCapture, capman.request):\n            captured = sys.stdout.write(\"Hello, world!\")\n        assert capman.is_capturing() == \"global\"\n        assert captured == b\"Hello, world!\\n\"\n"], "sample_128": ["    def test_clone(self):\n        index = Index(fields=['field1', '-field2'], name='my_index', db_tablespace='my_tablespace')\n        cloned_index = index.clone()\n        self.assertEqual(index, cloned_index)\n        self.assertIsNot(index, cloned_index)\n        self.assertEqual(cloned_index.fields, index.fields)\n        self.assertEqual(cloned_index.name, index.name)\n        self.assertEqual(cloned_index.db_tablespace, index.db_tablespace)\n", "    def test_create_sql_with_opclasses(self):\n        article = Article.objects.create(title='test', slug='test')\n        index = Index(fields=['title'], opclasses=['varchar_pattern_ops'])\n        self.assertIn('varchar_pattern_ops', index.create_sql(Article, connection))\n", "    def test_index_deconstruction_and_reconstruction(self):\n        index = Index(\n            fields=['field1', 'field2'],\n            name='my_index',\n            db_tablespace='my_tablespace',\n            opclasses=['my_opclass'],\n            condition=Q(field1__gt=1),\n            include=['field3'],\n        )\n        path, args, kwargs = index.deconstruct()\n        new_index = Index(**kwargs)\n        self.assertEqual(index, new_index)\n", "    def test_index_opclasses(self):\n        from myapp.models import MyModel\n        model = MyModel()\n        index = Index(fields=('field1', '-field2'), opclasses=('int4', 'varchar'))\n        self.assertEqual(index.opclasses, ('int4', 'varchar'))\n        with self.assertRaises(ValueError):\n            Index(fields=('field1', '-field2'), opclasses=('int4',))\n", "    def test_create_sql_with_condition(self):\n        article = Article.objects.create(title='test title')\n        index = Index(fields=['title'], condition=Q(title='test title'))\n        self.assertIn('WHERE', index.create_sql(Article, connection))\n"], "sample_787": ["def test_multilabel_confusion_matrix():\n    # Test sample-wise multilabel_confusion_matrix\n    y_true = np.array([[1, 0, 1],\n                       [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0],\n                       [0, 1, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_MCM = np.array([[1., 0., 0., 0.],\n                             [0., 1., 0., 0.],\n                             [0., 0., 1., 0.],\n                             [0., 0., 0., 1.]])\n    assert_array_almost_equal(MCM, expected_MCM)\n\n    # Test sample-wise multilabel_confusion_matrix for a binary problem\n    y_true = np.array([[1, 0],\n                       [0, 1]])\n    y_pred = np.array([[1, 0],\n                       [0, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_MCM = np.array([[1., 0., 0., 0.],\n                             [0., 1., 0., 0.],\n                             [0., 0., 1., 0.],\n                             [0., 0., 0., 1.]])\n    assert_array_almost_equal(MCM, expected_MCM)\n\n    # Test multilabel_confusion_matrix with labels\n    y_true = np.array([[1, 0, 1],\n                       [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0],\n                       [0, 1, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1, 2],\n                                     samplewise=True)\n    expected_MCM = np.array([[1., 0., 0., 0.],\n                             [0., 1., 0., 0.],\n                             [0., 0., 1., 0.],\n                             [0., 0., 0., 1.]])\n    assert_array_almost_equal(MCM, expected_MCM)\n\n    # Test multilabel_confusion_matrix with sample weights\n    y_true = np.array([[1, ", "def test_fbeta_score_binary(pos_label):\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 0, 1])\n    average = 'binary'\n    assert_almost_equal(fbeta_score(y_true, y_pred, 0.5, pos_label=pos_label,\n                                   average=average, sample_weight=None), 0.5)\n    assert_almost_equal(fbeta_score(y_true, y_pred, 1.0, pos_label=pos_label,\n                                   average=average, sample_weight=None), 1.0)\n    assert_almost_equal(fbeta_score(y_true, y_pred, 2.0, pos_label=pos_label,\n                                   average=average, sample_weight=None), 0.0)\n\n    with pytest.raises(ValueError):\n        fbeta_score(y_true, y_pred, 0.5, pos_label=pos_label,\n                    average='micro', sample_weight=None)\n\n    with pytest.raises(ValueError):\n        fbeta_score(y_true, y_pred, 0.5, pos_label=pos_label,\n                    average='macro', sample_weight=None)\n\n    with pytest.raises(ValueError):\n        fbeta_score(y_true, y_pred, 0.5, pos_label=pos_label,\n                    average='weighted', sample_weight=None)\n\n    with pytest.raises(ValueError):\n        fbeta_score(y_true, y_pred, 0.5, pos_label=pos_label,\n                    average='samples', sample_weight=None)\n", "def test_multilabel_confusion_matrix():\n    \"\"\"Test multilabel_confusion_matrix with samplewise=True\"\"\"\n    y_true = np.array([[0, 0, 1], [1, 1, 0]])\n    y_pred = np.array([[0, 0, 0], [0, 1, 1]])\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        mcm = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_mcm = np.array([\n        [[2, 1], [1, 2]],\n        [[2, 0], [0, 2]],\n        [[0, 2], [2, 1]]\n    ])\n    assert_array_equal(mcm, expected_mcm)\n", "def test_matthews_corrcoef_zeros():\n    # Test that Matthews Correlation Coefficient (MCC) returns 0 when both\n    # true positives and false positives are 0.\n\n    y_true = np.array([1, 0, 0, 1, 0])\n    y_pred = np.array([0, 0, 0, 0, 0])\n\n    expected_result = 0.0\n\n    result = matthews_corrcoef(y_true, y_pred)\n\n    assert_equal(result, expected_result)\n", "def test_log_loss_binary():\n    y_true = np.array([0, 1, 0, 1, 1, 0])\n    y_pred = np.array([0.2, 0.7, 0.3, 0.9, 0.6, 0.4])\n    eps = 1e-15\n    assert_almost_equal(log_loss(y_true, y_pred, eps=eps), 0.12017831546086171)\n"], "sample_932": ["compilation error", "compilation error", "def test_unsupported_multi_char_char_literal():\n    ast = parse('char', \"\"\"\n        char a(b, c);\n    \"\"\")\n    assert isinstance(ast.declaration, ASTTrailingTypeSpecName)\n    assert isinstance(ast.declaration.name, ASTNestedName)\n    assert isinstance(ast.declaration.name.names[0], ASTNestedNameElement)\n    assert isinstance(ast.declaration.name.names[0].identOrOp, ASTIdentifier)\n    assert ast.declaration.name.names[0].identOrOp.identifier == \"a\"\n    assert ast.declaration.name.names[0].identOrOp.is_anon()\n    assert ast.declaration.name.names[0].templateArgs is None\n", "def test_class_with_bases():\n    input = \"\"\"", "def test_notype_template_param_with_init():\n    idDict = {\n        1: None,\n        2: '2DcE',\n        3: '3DcE',\n        4: '4DcE'\n    }\n    check('templateParam', '::template<T> int T::var;', idDict,\n          '::template<T> int T::var;')\n"], "sample_1108": ["compilation error", "def test_rotate_left():\n    a = [0, 1, 2]\n    assert rotate_left(a, 1) == [1, 2, 0]\n", "def test_is_palindromic():\n    assert is_palindromic('abcba')\n    assert is_palindromic('abcba', 1)\n    assert is_palindromic('abcba', 2)\n    assert is_palindromic('abccba', 2)\n    assert not is_palindromic('abc')\n    assert not is_palindromic('abc', 1)\n    assert not is_palindromic('abc', 2)\n    assert is_palindromic([1, 2, 1])\n    assert not is_palindromic([1, 2, 3])\n    assert is_palindromic('abba', 2)\n    assert not is_palindromic('abba', 1)\n", "def test_generate_involutions():\n    # test the fixed points and permutations\n    assert list(generate_involutions(3)) == [(0, 1, 2), (0, 2, 1), (1, 0, 2), (2, 1, 0)]\n    assert list(generate_involutions(4)) == [(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 3, 1), (0, 3, 1, 2), (1, 0, 3, 2), (1, 2, 3, 0), (1, 3, 0, 2), (1, 3, 2, 0), (2, 0, 3, 1), (2, 1, 3, 0), (2, 3, 0, 1), (3, 0, 1, 2), (3, 0, 2, 1), (3, 1, 0, 2), (3, 2, 0, 1)]\n    # test that permutations of the same elements are not returned\n    assert len(list(generate_involutions(3))) == 4\n    # test that involutions are returned\n    assert len(list(generate_involutions(4))) == 12\n", "def test_unique_derivations():\n    # from sympy import symbols, Poly, sin, cos, exp, log, Poly\n    # from sympy.testing.pytest import raises, skipIfPython2, slow\n    # from sympy.abc import x\n    # x = symbols('x')\n    # assert sorted(list(unique_derivations(sin(x))) == [\n    #             log(sin(x) + cos(x), cos(x)*sin(x)/(sin(x) + cos(x))])\n    # ]\n\n    # # sin(x) is like sin(x+pi)\n    # assert len(unique_derivations(sin(x + symbols('pi')))) == 1\n    # assert unique_derivations(sin(x + symbols('pi')))[0] == log(sin(x) + cos(x))\n    # # cos(x) is like cos(x+pi)\n    # assert len(unique_derivations(cos(x + symbols('pi')))) == 1\n    # assert unique_derivations(cos(x + symbols('pi')))[0] == cos(x)*sin(x)/(sin(x) + cos(x))\n\n    # with raises(ValueError):\n    #     unique_derivations(sin(x) + cos(x))\n\n    # with raises(ValueError):\n    #     unique_derivations(sin(x) + cos(x + symbols('pi')))\n\n    # with raises(ValueError):\n    #     unique_derivations(x)\n\n    # # sin(x+2*pi) is like sin(x)\n    # assert len(unique_derivations(sin(x + 2*x))) == 1\n    # assert unique_derivations(sin(x + 2*x))[0] == log(sin(x) + cos(x))\n\n    # with raises(ValueError):\n    #     unique_derivations(sin(x) + cos(2*x))\n\n    # assert len(unique_derivations(log(x))) == 1\n    # assert unique_derivations(log(x))[0] == 1/x\n\n    # assert len(unique_derivations(exp(x))) == 1\n    # assert unique_derivations(exp(x))[0] == exp(x)\n\n    # with raises(ValueError):\n    #     unique_derivations(x + log(x))\n\n    # assert len(unique_derivations(exp(x) + log(x))) == 1\n    # assert unique_derivations(exp(x) + log(x))[0] == exp(x)\n\n    # with raises(ValueError):\n    #     unique_derivations(exp(x) + log(x) + sin(x))\n\n    # assert len(unique_derivations(sin(x + x"], "sample_1134": ["def test_LatexPrinter():\n    assert latex(1 + 1) == r\"1 + 1\"\n    assert latex(1 + 1 + 1) == r\"1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1\"\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1\"\n\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1\"\n\n    assert latex(1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1 + 1) == r\"1 + 1 + 1 + 1 + 1 +", "def test_latexPrinter_print_MeijerG():\n    meijerg = MeijerG((1, 2), (3, 4), x, y)\n    printer = LatexPrinter()\n    assert printer.doprint(meijerg) == r\"G_{2, 2}^{1, 1}\\left(\\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix} \\middle| x, y \\right)\"\n", "def test_latex_printer_hermitian_matrix():\n    # Test a Hermitian matrix\n    from sympy import Matrix\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M) == r'\\left[\\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix}\\right]'\n\n    # Test a Hermitian matrix with a specific delimiter\n    assert latex(M, mat_delim='(') == r'\\left(\\begin{matrix} 1 & 2 \\\\ 3 & 4 \\end{matrix}\\right)'\n", "def test_latex_indexedbase():\n    s = Symbol('s')\n    indexed = IndexedBase(s)\n    assert latex(indexed) == s._latex_form\n    indexed.name = 'S'\n    assert latex(indexed) == 'S'\n", "def test_LatexPrinter_subfactorial():\n    assert latex(subfactorial(4)) == r\"4!'\\left( 4 \\right)\"\n"], "sample_519": ["def test_subplots_adjust():\n    # Create a figure with a single subplot\n    fig, ax = plt.subplots()\n\n    # Set the figure size\n    fig.set_size_inches(6, 6)\n\n    # Add some text to the subplot\n    ax.text(0.5, 0.5, 'Test', ha='center', va='center')\n\n    # Adjust the subplot layout\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n\n    # Check that the subplot layout has been adjusted correctly\n    assert ax.get_position().left == 0.1\n    assert ax.get_position().right == 0.9\n    assert ax.get_position().top == 0.9\n    assert ax.get_position().bottom == 0.1\n\n    # Check that the figure size has not changed\n    assert fig.get_size_inches()[0] == 6\n    assert fig.get_size_inches()[1] == 6\n\n    # Close the figure\n    plt.close(fig)\n", "def test_figure_set_size_inches(dpi, size_inches):\n    fig = Figure(dpi=dpi)\n    fig.set_size_inches(*size_inches)\n    assert np.allclose(fig.get_size_inches(), size_inches)\n\n", "def test_figure_pickle():\n    \"\"\"Test that pickling a figure works correctly.\"\"\"\n    f = Figure(figsize=(10, 10))\n    ax1 = f.add_subplot(111)\n    ax1.plot(np.linspace(0, 10, 100))\n\n    # Make sure the figure has a manager\n    plt.show(block=False)\n\n    # Save the figure\n    with io.BytesIO() as stream:\n        f.savefig(stream, format='png')\n\n    # Clear the figure\n    f.clear()\n\n    # Pickle the figure\n    pickled_f = pickle.dumps(f)\n\n    # Unpickle the figure\n    unpickled_f = pickle.loads(pickled_f)\n\n    # Test that the figure is still good\n    assert isinstance(unpickled_f, Figure)\n    assert isinstance(unpickled_f.axes[0], Axes)\n    assert len(unpickled_f.axes) == 1\n    assert np.allclose(unpickled_f.axes[0].lines[0].get_xdata(),\n                      np.linspace(0, 10, 100))\n\n    # Clear the unpickled figure\n    unpickled_f.clear()\n\n    # Test that the unpickled figure has the same properties as the original\n    assert isinstance(f.canvas, unpickled_f.canvas.__class__)\n    assert isinstance(f.canvas.figure, unpickled_f.canvas.__class__)\n\n    # Make sure that the unpickled figure has the same axes as the original\n    assert len(unpickled_f.axes) == len(f.axes)\n    for ax, unpickled_ax in zip(f.axes, unpickled_f.axes):\n        assert ax is not unpickled_ax\n        assert ax._id == unpickled_ax._id\n        assert ax.get_xaxis().is_allticks_set() == unpickled_ax.get_xaxis().is_allticks_set()\n        assert ax.get_yaxis().is_allticks_set() == unpickled_ax.get_yaxis().is_allticks_set()\n", "def test_figure_subplot_adjust(tight_layout):\n    # This test is meant to be used with the subplot_adjust test\n    # in tests/test_plot.py. It is used to test the following\n    # matplotlib issue:\n    # https://github.com/matplotlib/matplotlib/issues/14383\n    fig, ax = plt.subplots()\n\n    ax.text(0.1, 0.1, \"left\")\n    ax.text(0.1, 0.9, \"bottom\")\n    ax.text(0.9, 0.1, \"right\")\n    ax.text(0.9, 0.9, \"top\")\n\n    if tight_layout:\n        ax.set_tight_layout()\n\n    fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.1, hspace=0.1)\n\n    # Check that the subplots_adjust call has been overridden by tight_layout\n    assert not hasattr(fig, '_tight_layout_called')\n\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\n    assert ax.get_xlabel() == \"x\"\n    assert ax.get_ylabel() == \"y\"\n\n    # Check that the tight_layout call did not interfere with the subplots_adjust call\n    fig.subplots_adjust(left=0.2, right=0.8, bottom=0.2, top=0.8, wspace=0.2, hspace=0.2)\n\n    assert not hasattr(fig, '_tight_layout_called')\n\n    # Check that the subplots_adjust call has been overridden by tight_layout\n    assert hasattr(fig, '_tight_layout_called')\n\n    # Check that the tight_layout call did not interfere with the subplots_adjust call\n    ax.set_xlabel(\"y\")\n    ax.set_ylabel(\"x\")\n\n    assert ax.get_xlabel() == \"y\"\n    assert ax.get_ylabel() == \"x\"\n", "def test_subplots_adjust():\n    # Regression for gh-21732\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.0, right=0.0)\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(right=0.0, left=0.0)\n\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(bottom=0.0, top=0.0)\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(top=0.0, bottom=0.0)\n\n    # Regression for gh-21629\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(wspace=0.0, hspace=0.0)\n    with pytest.raises(ValueError):\n        fig.subplots_adjust(wspace=0.0, hspace=0.0)\n"], "sample_42": ["def test_brightness_temperature_Jy_to_K():\n    frequency = u.GHz(500)\n    K = u.brightness_temperature(frequency)(1*u.mJy/u.sr).to(u.K)\n    Jy = K.to(u.mJy/u.sr, equivalencies=u.brightness_temperature(frequency))\n    assert_quantity_allclose(Jy, 1*u.mJy/u.sr)\n", "def test_brightness_temperature():\n    # Test brightness_temperature with frequency in Hz\n    freq = u.Quantity(1e9, u.Hz)\n    equiv = u.brightness_temperature(freq)\n    assert_quantity_allclose(u.Jy/u.sr.to(u.K, equiv), 6.626e-8)\n\n    # Test brightness_temperature with frequency in GHz\n    freq = u.Quantity(1, u.GHz)\n    equiv = u.brightness_temperature(freq)\n    assert_quantity_allclose(u.Jy/u.sr.to(u.K, equiv), 2.404e-5)\n\n    # Test brightness_temperature with frequency in AA\n    freq = u.Quantity(1, u.AA)\n    equiv = u.brightness_temperature(freq)\n    with warnings.catch_warnings(record=True) as w:\n        u.Jy/u.sr.to(u.K, u.brightness_temperature(freq))\n        assert len(w) == 1\n        assert \"The inputs to `brightness_temperature` have changed.\" in str(w[0].message)\n\n    # Test brightness_temperature with frequency in eV\n    freq = u.Quantity(1, u.eV)\n    equiv = u.brightness_temperature(freq)\n    assert_quantity_allclose(u.Jy/u.sr.to(u.K, equiv), 3.827e-6)\n\n    # Test brightness_temperature with beam_area in steradians\n    beam_area = u.Quantity(1, u.sr)\n    equiv = u.brightness_temperature(freq, beam_area)\n    assert_quantity_allclose(u.Jy/u.sr.to(u.K, equiv), 2.404e-5)\n\n    # Test brightness_temperature with beam_area in beam\n    beam_area = u.Quantity(1, u.beam)\n    equiv = u.brightness_temperature(freq, beam_area)\n    assert_quantity_allclose(u.Jy/u.beam.to(u.K, equiv), 2.404e-5)\n", "def test_doppler_optical():\n    # Test that the optical convention for velocity produces the expected results\n    # for different input types\n    # Wavelength\n    CO_restfreq = 115.27120*u.GHz  # rest frequency of 12 CO 1-0 in GHz\n    optical_CO_equiv = u.doppler_optical(CO_restfreq)\n    measured_freq = 115.2832*u.GHz\n    optical_velocity = measured_freq.to(u.km/u.s, equivalencies=optical_CO_equiv)\n    assert_quantity_allclose(optical_velocity, -31.20584348799674*u.km/u.s)\n\n    # Frequency\n    optical_freq_equiv = u.doppler_optical(CO_restfreq)\n    measured_velocity = -31.20584348799674*u.km/u.s\n    optical_freq = measured_velocity.to(u.GHz, equivalencies=optical_freq_equiv)\n    assert_quantity_allclose(optical_freq, 115.2832*u.GHz)\n\n    # Energy\n    optical_energy_equiv = u.doppler_optical(CO_restfreq)\n    measured_energy = -31.20584348799674*u.km/u.s * u.k_B * CO_restfreq\n    optical_energy = measured_energy.to(u.eV, equivalencies=optical_energy_equiv)\n    assert_quantity_allclose(optical_energy, 4.823849714999995e-06*u.eV)\n\n    # Test that the optical convention for velocity raises an error when\n    # given a unit that is not a spectral equivalent\n    with pytest.raises(u.UnitsError):\n        u.doppler_optical(u.m)\n", "def test_dimensionless_angles():\n    from astropy import units as u\n    from astropy import constants as c\n    from .. import dimensionless_angles\n\n    radian = u.Unit('radian')\n    m_per_m = u.Unit('m/m')\n\n    equiv = dimensionless_angles()\n    assert len(equiv) == 1\n    assert len(equiv[0]) == 2\n    assert equiv[0][0] == radian\n    assert equiv[0][1] is None\n    assert c.rad.to_value(radian, equivalencies=equiv) == 1\n\n    # Test that dimensionless_angles() doesn't affect the case where the\n    # dimensionless quantity is raised to a power\n    radian_per_m = u.Unit('radian/m')\n    assert radian_per_m.to_value(radian_per_m, equivalencies=equiv) == 1\n\n    # Test that dimensionless_angles() works even when the dimensionless quantity\n    # is part of a more complicated unit\n    arcsec_per_arcmin = u.Unit('arcsec/arcmin')\n    assert arcsec_per_arcmin.to_value(arcsec_per_arcmin, equivalencies=equiv) == 1\n\n    # Test that dimensionless_angles() works with quantities\n    radian_quantity = u.Quantity(1, radian)\n    m_per_m_quantity = u.Quantity(1, m_per_m)\n    assert radian_quantity.to_value(radian, equivalencies=equiv) == 1\n    assert m_per_m_quantity.to_value(m_per_m, equivalencies=equiv) == 1\n", "def test_brightness_temperature():\n    # Test that brightness temperature conversion works for a frequency\n    frequency = u.GHz(5)\n    # Test the temperature to flux and back\n    temperature = 1 * u.K\n    flux = temperature.to(u.Jy/u.sr, u.brightness_temperature(frequency))\n    assert_quantity_allclose(flux, 2.256 * u.Jy/u.sr)\n\n    # Test the flux to temperature and back\n    flux = 1 * u.Jy/u.sr\n    temperature = flux.to(u.K, u.brightness_temperature(frequency))\n    assert_quantity_allclose(temperature, 2.256 * u.K)\n\n    # Test that units are preserved when using the brightness temperature\n    # conversion with a different frequency\n    frequency_2 = u.GHz(10)\n    temperature_2 = 1 * u.K\n    flux_2 = temperature_2.to(u.Jy/u.sr, u.brightness_temperature(frequency_2))\n    assert_quantity_allclose(flux_2, 22.56 * u.Jy/u.sr)\n\n    # Test the case with no beam area\n    with pytest.raises(ValueError):\n        frequency.to(u.K, u.brightness_temperature(frequency))\n\n    # Test the case with an invalid beam area\n    beam_area = u.sr(1)\n    with pytest.raises(ValueError):\n        frequency.to(u.K, u.brightness_temperature(frequency, beam_area=beam_area))\n\n    # Test that the brightness temperature conversion is equivalent to the old\n    # temperature conversion\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        flux_old = frequency.to(u.K, u.temperature())\n    assert_quantity_allclose(flux_old, 14.4 * u.K)\n"], "sample_253": ["    def test_client_query(self):\n        with mock.patch.object(pywatchman.client, 'client') as mock_client:\n            reloader = autoreload.WatchmanReloader()\n            reloader.client.query('version')\n            mock_client.query.assert_called_once_with('version')\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n        self.module_name = 'test_module'\n        self.module_path = self.temporary_file(self.module_name + '.py')\n        self.module_path.write_text('import os\\nprint(\"Hello World\")')\n        self.module_path.parent.mkdir()\n        sys.path_importer_cache.clear()\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        # Test that tick() returns a generator.\n        with self.assertRaises(StopIteration):\n            next(reloader.tick())\n", "    def test_tick(self):\n        with self.mock.patch('django.utils.autoreload.logger') as mock_logger:\n            reloader = autoreload.StatReloader()\n            reloader.tick()\n            mock_logger.debug.assert_called_once_with('File %s first seen with mtime %s', '<File object at 0x...>', 1)\n            mock_logger.debug.assert_called_count(2)\n            mock_logger.debug.assert_called_with('File %s previous mtime: %s, current mtime: %s', '<File object at 0x...>', 1, 2)\n", "    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n"], "sample_641": ["def test_get_pdata_path_invalid_chars(path, expected):\n    \"\"\"Test invalid chars in path are stripped.\"\"\"\n    assert _get_pdata_path(Path(path), 1) == expected\n", "def test_load_results_invalid_pylint_home_type(tmp_path):\n    \"\"\"Test loading results from a different type of pylint_home.\"\"\"\n    # Create a new pylint_home directory\n    new_pylint_home = tmp_path / \"new_pylint_home\"\n    new_pylint_home.mkdir()\n\n    # Create a data file in the new pylint_home\n    base_name = Path(\"test_module\")\n    data_file = _get_pdata_path(base_name, 1, new_pylint_home)\n    with open(data_file, \"wb\") as stream:\n        pickle.dump(LinterStats(), stream)\n\n    # Test that load_results raises a TypeError when pylint_home is a string\n    assert load_results(base_name, str(new_pylint_home)) is None\n\n    # Test that load_results raises a TypeError when pylint_home is an int\n    assert load_results(base_name, 123) is None\n", "def test_save_results_with_different_input(\n    path: str,\n    results: LinterStats,\n    pylint_home: Path,\n    expected: Path | None,", "def test_get_pdata_path(path, recurs, pylint_home, expected):\n    result = _get_pdata_path(path, recurs, pylint_home)\n    assert result == expected\n", "def test_load_results_none_when_pylint_home_is_empty_directory():\n    \"\"\"Test that load_results returns None when the specified pylint_home is an empty directory.\"\"\"\n    # Arrange\n    empty_dir = Path(\"test_dir\")\n    empty_dir.mkdir()\n    base = Path(\"base\")\n    # Act\n    result = load_results(base, empty_dir)\n    # Assert\n    assert result is None\n    # Clean up\n    empty_dir.rmdir()\n"], "sample_514": ["def _colorbar_extension_fraction(spacing):\n    \"\"\"\n    Produce 9 colorbars with variable extend fraction for either\n    uniform or proportional spacing.\n\n    Helper function for test_colorbar_extension_fraction.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate((None, 0.1, 0.2, 0.5, 0.8)):\n            # Create a subplot.\n            cax = fig.add_subplot(9, 1, i*5 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing=spacing)\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n\n", "def test_colorbar_extension_length_all_types():\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, spacing in enumerate(('uniform', 'proportional')):\n            for k, extendfrac in enumerate((None, 'auto', 0.1, 0.5)):\n                for m, extendrect in enumerate((True, False)):\n                    # Create a subplot.\n                    cax = fig.add_subplot(12*3, 1, (i*9 + j*3 + k*3 + m + 1))\n                    # Generate the colorbar.\n                    Colorbar(cax, cmap=cmap, norm=norm,\n                             boundaries=boundaries, values=values,\n                             extend=extension_type, extendfrac=extendfrac,\n                             orientation='horizontal', spacing=spacing,\n                             extendrect=extendrect)\n                    # Turn off text and ticks.\n                    cax.tick_params(left=False, labelleft=False,\n                                    bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_update_normal():\n    # Create a figure and adjust whitespace for subplots.\n    fig, axs = plt.subplots(2, 1, figsize=(6, 6))\n    fig.subplots_adjust(hspace=.6)\n\n    # Generate the first image.\n    im = axs[0].imshow(np.random.rand(10, 10), cmap='viridis')\n\n    # Generate the second image with a different norm.\n    norm = Normalize(vmin=0, vmax=5)\n    im2 = axs[0].imshow(np.random.rand(10, 10), cmap='viridis', norm=norm)\n\n    # Create a colorbar for the first image.\n    cbar = Colorbar(axs[0], im)\n\n    # Update the norm and check that the colorbar was updated.\n    cbar.update_normal(im2)\n\n    # Check that the colorbar has the correct norm.\n    assert cbar.norm == norm\n\n    # Generate the second image.\n    im3 = axs[1].imshow(np.random.rand(10, 10), cmap='viridis')\n\n    # Update the norm and check that the colorbar was updated.\n    cbar.update_normal(im3)\n\n    # Check that the colorbar has the correct norm.\n    assert cbar.norm == im3.norm\n\n    # Close the figure.\n    plt.close(fig)\n", "def test_colorbar_extension_length_lognorm():\n    \"\"\"\n    Test that a lognorm colorbar with rectangular extensions has the\n    correct extend lengths when using uniform or proportional spacing.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap = cm.get_cmap(\"RdBu\", lut=5)\n    norm = LogNorm(1e-5, 1e5)\n    clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n    boundaries = values = norm.boundaries\n    values = values[:-1]\n\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Create a subplot.\n        cax = fig.add_subplot(12, 1, i*3 + 1)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendrect=True,\n                 orientation='horizontal', spacing='uniform')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n        # Add a title to the subplot.\n        cax.set_title(f\"Uniform spacing, {extension_type} extension\")\n\n        # Create a subplot.\n        cax = fig.add_subplot(12, 1, i*3 + 2)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendrect=True,\n                 orientation='horizontal', spacing='proportional')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n        # Add a title to the subplot.\n        cax.set_title(f\"Proportional spacing, {extension_type} extension\")\n\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_extension_length_padding():\n    \"\"\"\n    Test padding of the extensions in the colorbar.\n\n    See #10016.\n    \"\"\"\n    # Create a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate((None, 'auto', 0.1)):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*3 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing='proportional',\n                     pad=0.05)\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n"], "sample_1175": ["def test_Piecewise():\n    p = Piecewise((x,x<1),(x**2,True))\n    assert pretty(p) == 'Piecewise((x, x < 1), (x**2, True))'\n    assert pretty(p, order='none') == 'Piecewise((x, x < 1), (x**2, True))'\n    assert pretty(p, order='lex') == 'Piecewise((x, x < 1), (x**2, True))'\n    assert upretty(p) == 'Piecewise((x,\\\\n    x < 1), (x**2,\\\\n    True))'\n    assert upretty(p, order='none') == 'Piecewise((x,\\\\n    x < 1), (x**2,\\\\n    True))'\n    assert upretty(p, order='lex') == 'Piecewise((x,\\\\n    x < 1), (x**2,\\\\n    True))'\n", "def test_PrettyPrinter():\n    pretty_printer = PrettyPrinter()\n    array1 = ImmutableDenseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    array2 = ImmutableSparseNDimArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    assert str(pretty_printer.doprint(array1)) == \"Matrix([[1, 2], [3, 4]])\\nMatrix([[5, 6], [7, 8]])\"\n    assert str(pretty_printer.doprint(array2)) == \"Matrix([[1, 2], [3, 4]])\\nMatrix([[5, 6], [7, 8]])\"\n", "def test_Mul():\n    assert pretty(2**3 * 2**3) == pretty(8)\n    assert pretty(2**3 * 2**3, order='none') == pretty(8)\n    assert pretty(2**3 * 2**3, order='lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='none') == pretty(8)\n    assert pretty(2**3 * 2**3, order='lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='grlex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-lex') == pretty(8)\n    assert pretty(2**3 * 2**3, order='rev-grlex') == pretty(8)\n", "def test_print_MIMOSeries():\n    from sympy.physics.control.lti import TransferFunction, Series, MIMOParallel\n    from sympy import symbols, exp\n    t = symbols('t')\n    expr = Series(TransferFunction(1, 1, t), Series(TransferFunction(2, 2, t)))\n    expr = MIMOParallel(expr)\n    assert upretty(expr) == r'''\n    \\begin{bmatrix}\n        1 \\\\\n        2\n    \\end{bmatrix}    \\n\n    \\N{RIGHTWARDS ARROW FROM BAR}  \\n\n    \\begin{bmatrix}\n        1 \\\\\n        2\n    \\end{bmatrix}    \\n\n    + \\begin{bmatrix}\n        1 \\\\\n        2\n    \\end{bmatrix}    \\n\n    \\N{RIGHTWARDS ARROW FROM BAR}  \\n\n    \\begin{bmatrix}\n        1 \\\\\n        2\n    \\end{bmatrix}    \\n", "def test_tensor_element():\n    x, y, z = symbols('x, y, z')\n    T = TensorHead('T', [x, y, z])\n    ti = tensor_indices(3, 'i')\n    e1 = TensorElement(T, (1, 2, 0))\n    assert e1.as_base_expr() == x**2*y*z\n    e2 = TensorElement(T, (2, 0, 2))\n    assert e2.as_base_expr() == x*y**2*z**2\n    e3 = TensorElement(T, (0, 0, 0))\n    assert e3.as_base_expr() == 1\n    e4 = TensorElement(T, (3, 3, 3))\n    assert e4.as_base_expr() == x*y*z\n\n    e5 = ImmutableDenseNDimArray([1, 2, 3])\n    assert e5.as_base_expr() == 1\n\n    e6 = ImmutableSparseNDimArray([0, 2, 0])\n    assert e6.as_base_expr() == 2\n\n    assert tensorproduct(e1, e2).as_base_expr() == x**3*y**3*z**3\n    assert tensorproduct(e3, e4).as_base_expr() == 1\n    assert tensorproduct(e1, e4).as_base_expr() == x**2*y**2*z**2\n    assert tensorproduct(e2, e5).as_base_expr() == x*y**2*z**2\n    assert tensorproduct(e3, e5).as_base_expr() == x*y*z\n    assert tensorproduct(e4, e5).as_base_expr() == x*y*z\n    assert tensorproduct(e1, e6).as_base_expr() == x**2*y*z\n    assert tensorproduct(e2, e6).as_base_expr() == x*y**2*z\n    assert tensorproduct(e3, e6).as_base_expr() == x*y\n    assert tensorproduct(e4, e6).as_base_expr() == x*y\n"], "sample_617": ["def test_cov_compute():\n    # create a 2D array with some values\n    da_a = xr.DataArray(np.array([[1, 2], [3, 4]]), dims=(\"x\", \"y\"))\n\n    # create another 2D array with the same shape and some values\n    da_b = xr.DataArray(np.array([[5, 6], [7, 8]]), dims=(\"x\", \"y\"))\n\n    # compute the covariance\n    cov = apply_ufunc(np.cov, da_a, da_b)\n\n    # check that the result is a DataArray\n    assert isinstance(cov, xr.DataArray)\n\n    # check that the result has the correct shape\n    assert cov.shape == ()\n\n    # check that the result has the correct value\n    assert_allclose(cov, -1.4999999999999998)\n\n    # check that the result has the correct dtype\n    assert cov.dtype == np.float64\n", "def test__ensure_numeric():\n    da = xr.DataArray(\n        np.arange(10),\n        dims=[\"x\"],\n        coords={\"x\": np.arange(10)},\n        name=\"test\",\n        attrs={\"test\": \"attr\"},\n    )\n    da_numeric = _ensure_numeric(da)\n    assert da_numeric.dtype == np.float64\n    assert da_numeric.name == \"test\"\n    assert da_numeric.attrs == {\"test\": \"attr\"}\n    assert da_numeric == da\n\n    ds = xr.Dataset(\n        {\"test\": da},\n        coords={\"x\": np.arange(10)},\n    )\n    ds_numeric = _ensure_numeric(ds)\n    assert ds_numeric[\"test\"].dtype == np.float64\n    assert ds_numeric[\"test\"].name == \"test\"\n    assert ds_numeric[\"test\"].attrs == {\"test\": \"attr\"}\n    assert ds_numeric[\"test\"] == da\n", "def test_unify_chunks_unifies_chunks():\n    # First, create some datasets with different chunk sizes\n    ds1 = xr.Dataset(\n        {\"x\": ((\"x\", \"y\"), np.random.rand(2, 4))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(4)},\n        chunks={\"x\": 1},\n    )\n\n    ds2 = xr.Dataset(\n        {\"x\": ((\"x\", \"y\"), np.random.rand(2, 4))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(4)},\n        chunks={\"x\": 2},\n    )\n\n    # Then, test that unify_chunks unifies the chunks\n    ds_unified = xr.Dataset(\n        {\"x\": ((\"x\", \"y\"), np.random.rand(2, 4))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(4)},\n        chunks={\"x\": 2, \"y\": 4},\n    )\n\n    unified = unify_chunks(ds1, ds2)\n    assert_identical(unified[0], ds_unified)\n\n    # Also test that unify_chunks unifies the chunks for multiple datasets\n    ds3 = xr.Dataset(\n        {\"x\": ((\"x\", \"y\"), np.random.rand(2, 4))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(4)},\n        chunks={\"x\": 1, \"y\": 1},\n    )\n\n    unified = unify_chunks(ds1, ds2, ds3)\n    assert_identical(unified[0], ds_unified)\n\n    # Test that unify_chunks raises an error if the datasets have different variable names\n    with pytest.raises(ValueError):\n        unify_chunks(ds1, ds2.rename({\"x\": \"z\"}))\n\n    # Test that unify_chunks works when one of the datasets has no chunks\n    ds4 = xr.Dataset(\n        {\"x\": ((\"x\", \"y\"), np.random.rand(2, 4))},\n        coords={\"x\": np.arange(2), \"y\": np.arange(4)},\n    )\n\n    unified = unify_chunks(ds1, ds4)\n    assert_identical(unified[0], ds_unified)\n", "def test_unify_chunks():\n    from xarray.core.utils import _Rechunker\n    from xarray.core.pycompat import dask_version\n\n    if not dask_version >= Version(\"2022.5.0\"):\n        pytest.skip(\"dask>=2022.5.0 required\")\n\n    da = xr.DataArray(\n        np.arange(12),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(4), \"y\": np.arange(3)},\n        chunks={\"x\": 2},\n    )\n    da2 = xr.DataArray(\n        np.arange(12),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(4), \"y\": np.arange(3)},\n        chunks={\"y\": 2},\n    )\n\n    da = da.unify_chunks()\n    da2 = da2.unify_chunks()\n\n    assert_identical(\n        da._to_temp_dataset().variables[\"x\"].chunksize,\n        da2._to_temp_dataset().variables[\"x\"].chunksize,\n    )\n    assert_identical(\n        da._to_temp_dataset().variables[\"y\"].chunksize,\n        da2._to_temp_dataset().variables[\"y\"].chunksize,\n    )\n\n    # Test unify_chunks on multiple datasets\n    ds = xr.Dataset({\"a\": da, \"b\": da2})\n    ds = ds.unify_chunks()\n\n    assert_identical(\n        ds._to_temp_dataset().variables[\"a\"].chunksize,\n        ds._to_temp_dataset().variables[\"b\"].chunksize,\n    )\n\n    # Test unify_chunks with different chunk sizes\n    da = xr.DataArray(\n        np.arange(12),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(4), \"y\": np.arange(3)},\n        chunks={\"x\": 2, \"y\": 3},\n    )\n    da2 = xr.DataArray(\n        np.arange(12),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(4), \"y\": np.arange(3)},\n        chunks={\"x\": 3, \"y\": 2},\n    )\n\n    da = da.unify_chunks()\n    da2 = da2.unify_chunks()\n\n    assert_identical(\n        da._to_temp_dataset().variables[\"x\"].chunksize,\n        da2._to_temp_dataset().variables[\"x\"].chunksize,\n    )\n    assert_identical(\n        da._to_temp_dataset().variables[\"y\"].", "def test_broadcast_compat_data():\n    \"\"\"Tests that `broadcast_compat_data` works with Variables and ndarrays\"\"\"\n    from . import dtypes\n    from .variable import Variable\n\n    x = Variable((\"a\", [1, 2]), np.array([1, 2]))\n    y = Variable((\"a\", [1, 2]), np.array([3, 4]))\n    z = np.array([5, 6])\n\n    expected = np.array([[1, 3], [2, 4]])\n\n    # Test broadcast_dims=[[]]\n    assert broadcast_compat_data(x, [], [()] == expected)\n\n    # Test broadcast_dims=['a']\n    assert broadcast_compat_data(x, ['a'], [()] == expected)\n\n    # Test broadcast_dims=['a', 'b']\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, ['a', 'b'], [()])\n\n    # Test with a non-array Variable\n    with pytest.raises(ValueError):\n        broadcast_compat_data(z, ['a'], [()])\n\n    # Test with an nd array\n    assert broadcast_compat_data(z, [], [()]) == z\n\n    # Test with a Variable that has a broadcastable dimension\n    z = Variable((\"a\", [1, 2]), np.array([5, 6]))\n    assert broadcast_compat_data(z, ['a'], [()]) == z.data\n\n    # Test with a Variable that has a non-broadcastable dimension\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, ['a'], [('b', 2)])\n"], "sample_867": ["def test_ParameterSampler_indexable():\n    param_distributions = {'a': [1, 2], 'b': [True, False]}\n    sampler = ParameterSampler(param_distributions, 5, random_state=42)\n    for params in sampler:\n        assert isinstance(params, dict)\n        for key, value in params.items():\n            assert isinstance(key, str)\n            assert isinstance(value, (int, bool))\n", "def test_parameter_sampler_non_iterable():\n    \"\"\"Test that ParameterSampler raises a TypeError if a distribution\n    is not an iterable or has no rvs method\"\"\"\n    sampler = ParameterSampler({'a': expon()}, n_iter=1)\n    assert_raises(TypeError, iter, sampler)\n", "def test_ParameterSampler_fails_with_inconsistent_distribution_types():\n    # Test that ParameterSampler fails with an inconsistent distribution types.\n    sampler = ParameterSampler({'a': [1, 2], 'b': expon()}, n_iter=1)\n    with assert_raises(ValueError, match='Parameter value is not iterable '\n                                        'or distribution \\(key=\\'a\\', value=[1, '\n                                        '2]\\)'):\n        next(sampler)\n", "def test_ParameterSampler_iter_all_distributions_as_lists():\n    param_distributions = {'a': [1, 2], 'b': bernoulli()}\n    sampler = ParameterSampler(param_distributions, n_iter=1)\n    assert list(sampler) == [{'a': 1, 'b': 0.7184202942632586}]\n", "compilation error"], "sample_370": ["    def test_many_to_many_add(self):\n        reader1 = Reader.objects.get(id=1)\n        book5 = Book.objects.create(title='Pride and Prejudice')\n        self.assertEqual(reader1.books_read.count(), 2)\n        reader1.books_read.add(book5)\n        self.assertEqual(reader1.books_read.count(), 3)\n", "    def test_get_queryset(self):\n        parent = Author.objects.create()\n        self.assertEqual(\n            set(parent.children.values_list('name', flat=True)),\n            set([author.name for author in [self.author1, self.author2, self.author3]])\n        )\n", "    def test_related_objects_manager(self):\n        manager = Author.objects.get(name='Charlotte').books.all()\n        self.assertIsInstance(manager, QuerySet)\n        self.assertIn('book1', [str(book) for book in manager])\n\n        self.assertEqual(Book.objects.filter(authors=Author.objects.get(name='Charlotte')).count(), 3)\n        self.assertEqual(Book.objects.filter(authors__name='Charlotte').count(), 3)\n        self.assertEqual(Book.objects.filter(authors=Author.objects.filter(name='Charlotte')).count(), 3)\n\n        self.assertEqual(Author.objects.get(name='Charlotte').books.all().count(), 3)\n        self.assertEqual(Author.objects.get(name='Charlotte').books.all().filter(title='Jane Eyre').count(), 1)\n        self.assertEqual(Author.objects.get(name='Charlotte').books.all().filter(title__startswith='Jane').count(), 1)\n", "    def test_through_manager_creation(self):\n        author = Author.objects.get(name='Charlotte')\n        through_model = Bookmark.through\n        bookmarks = through_model.objects.filter(author=author)\n        self.assertEqual(bookmarks.count(), 1)\n        bookmark = bookmarks.get()\n        self.assertEqual(bookmark.reader, author)\n", "    def test_through_model_instance_getting_added_to_both_sides(self):\n        \"\"\"\n        Regression test for #29434. When creating a new instance of a through\n        model, it should be added to the set of related instances on both\n        sides of a many-to-many relation.\n        \"\"\"\n        author = Author.objects.create(name='Test Author')\n        book = Book.objects.create(title='Test Book')\n        relationship = ManyToManyDescriptor(book.authors.rel, reverse=False)\n        through_instance = book.authors.through()\n        through_instance.book_id = book.id\n        through_instance.author_id = author.id\n        self.assertTrue(through_instance in book.authors.all())\n        self.assertTrue(through_instance in author.books.all())\n"], "sample_80": ["    def test_combine_queries_with_different_models(self):\n        q1 = Query(model=Author)\n        q1.add_filter(Q(author_name='John'))\n        q2 = Query(model=Item)\n        q2.add_filter(Q(item_name='Item1'))\n        q1.combine(q2, AND)\n        self.assertEqual(q1.where.children[1].children[0].lookup_name, 'AND')\n        self.assertEqual(q1.where.children[1].children[0].children[0].lookup_name, 'AND')\n        self.assertEqual(q1.where.children[1].children[0].children[1].lookup_name, 'AND')\n", "    def test_set_group_by(self):\n        query = Query(Author)\n        query.add_fields(['id', 'name'])\n        query.add_ordering('name')\n        query.add_distinct_fields('name')\n        query.set_group_by()\n        expected = ('id', 'name')\n        self.assertEqual(query.group_by, expected)\n", "    def test_build_lookup_invalid_lookup(self):\n        # Test that an invalid lookup raises a FieldError when using a filter\n        query = Query(Author)\n        with self.assertRaises(FieldError):\n            query.solve_lookup_type('foo__bar__baz')\n", "    def test_get_field_names_from_opts(self):\n        field = CharField(max_length=10)\n        self.assertEqual(get_field_names_from_opts(field._meta), ['id', 'max_length'])\n        self.assertEqual(get_field_names_from_opts(field._meta, concrete_fields=[field]), ['max_length'])\n", "    def test_setup_joins_paths_to_parent(self):\n        \"\"\"\n        Test that the setup_joins() method correctly handles paths to parent models\n        in concrete inheritance.\n        \"\"\"\n        author = Author.objects.create(name='John')\n        item = author.item_set.create(name='item')\n        object_c = item.object_c_set.create(name='object_c')\n        ranking = item.ranking_set.create(score=1)\n\n        query = Query(Author)\n        query.add_filter(('name__iexact', 'John'))\n        query.add_fields(('id', 'item__id', 'item__name', 'item__object_c__id'))\n        query.setup_joins(('item__name', 'object_c__id'), query.get_meta(), query.get_initial_alias())\n\n        self.assertIn('item', query.alias_map)\n        self.assertIn('object_c', query.alias_map)\n        self.assertIn('item__object_c', query.alias_map)\n        self.assertIn('item__object_c__id', query.alias_map)\n        self.assertIn('item__name', query.alias_map)\n        self.assertEqual(query.alias_map['item__object_c__id'].join_type, INNER)\n        self.assertEqual(query.alias_map['item__name'].join_type, INNER)\n"], "sample_100": ["    def test_watchman_reloader_available(self):\n        reloader = autoreload.get_reloader()\n        self.assertIsInstance(reloader, autoreload.WatchmanReloader)\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        self.assertTrue(reloader.check_availability())\n        self.assertTrue(reloader.tick().__next__())\n", "    def setUp(self):\n        self.dirname = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.dirname)\n", "    def test_reloader_setup(self):\n        reloader = autoreload.StatReloader()\n        self.assertFalse(reloader.should_stop)\n", "    def test_file_missing(self):\n        tmpdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tmpdir)\n        filename = Path(tmpdir) / 'non_existent_module.py'\n        self.import_and_cleanup('non_existent_module')\n        self.assertFileNotFound(filename)\n"], "sample_386": ["    def test_safe_data_repr(self):\n        safe_data = SafeData()\n        self.assertEqual(repr(safe_data), \"SafeData()\")\n", "    def test_mark_safe_with_lazy_string(self):\n        # Test mark_safe with a lazy string\n        lazy_str = lazy(lambda: \"Hello, World!\")\n        self.assertIsInstance(mark_safe(lazy_str), SafeString)\n", "    def test_mark_safe_callable_with_lazy(self):\n        # Test mark_safe with a lazy string\n        lazy_str = lazy(lambda: \"Hello, World!\")\n        marked_safe = mark_safe(lazy_str)\n        self.assertIsInstance(marked_safe, SafeString)\n        self.assertEqual(marked_safe, \"Hello, World!\")\n", "    def test_safe_data(self):\n        # Test that SafeData correctly implements the __html__ method\n        data = SafeData()\n        self.assertEqual(data.__html__(), data)\n", "    def test_mark_safe_callable(self):\n        # Test that mark_safe can mark a callable as safe.\n        @mark_safe\n            return f\"Hello, {name}!\"\n\n        self.assertIsInstance(hello(\"world\"), SafeString)\n        self.assertEqual(hello(\"world\"), \"Hello, world!\")\n"], "sample_915": ["def test_is_singledispatch_method():\n    # Test that is_singledispatch_method returns False for non-singledispatch\n    # methods\n    class MyMethod:\n        pass\n    assert not inspect.is_singledispatch_method(MyMethod())\n\n    # Test that is_singledispatch_method returns False for non-methods\n    class MyObject:\n        pass\n    assert not inspect.is_singledispatch_method(MyObject())\n\n    # Test that is_singledispatch_method returns True for singledispatch\n    # methods\n    from functools import singledispatch\n    @singledispatch\n        pass\n    assert inspect.is_singledispatch_method(mysDispatch)\n\n    # Test that is_singledispatch_method returns True for singledispatch\n    # methods with docstrings\n    @singledispatch\n        \"\"\"Docstring\"\"\"\n        pass\n    assert inspect.is_singledispatch_method(mysDispatch)\n", "def test_stringify_signature():\n    class MyFunction:\n            \"\"\"Docstring for MyFunction\"\"\"\n            pass\n\n    sig = inspect.signature(MyFunction)\n    expected = '(a: int, b: str)'\n    assert stringify_signature(sig) == expected\n\n    sig = inspect.signature(MyFunction.__init__)\n    expected = '(self, /, a: int, b: str)'\n    assert stringify_signature(sig) == expected\n\n    class MyPartialFunction:\n            pass\n\n    partial_func = functools.partial(MyPartialFunction, a=1, b='hello')\n    sig = inspect.signature(partial_func)\n    expected = '(a: int = 1, b: str = \\'hello\\')'\n    assert stringify_signature(sig) == expected\n\n    class MyPartialMethod:\n            \"\"\"Docstring for MyPartialMethod\"\"\"\n            pass\n\n    obj = MyPartialMethod()\n    partial_method = functools.partialmethod(MyPartialMethod.__init__, a=1, b='hello')\n    sig = inspect.signature(partial_method)\n    expected = '(self, /, a: int = 1, b: str = \\'hello\\')'\n    assert stringify_signature(sig) == expected\n\n    class MyFunctionWithReturn:\n            \"\"\"Docstring for MyFunctionWithReturn\"\"\"\n            pass\n\n    sig = inspect.signature(MyFunctionWithReturn)\n    expected = '(a: int, b: str) -> int'\n    assert stringify_signature(sig) == expected\n\n    sig = inspect.signature(MyFunctionWithReturn.__init__)\n    expected = '(self, /, a: int, b: str) -> int'\n    assert stringify_signature(sig) == expected\n\n    class MyFunctionWithVarargs:\n            \"\"\"Docstring for MyFunctionWithVarargs\"\"\"\n            pass\n\n    sig = inspect.signature(MyFunctionWithVarargs)\n    expected = '(a: int, *args: str)'\n    assert stringify_signature(sig) == expected\n\n    sig = inspect.signature(MyFunctionWithVarargs.__init__)\n    expected = '(self, /, a: int", "def test_is_singledispatch_method():\n    \"\"\"Test is_singledispatch_method() function.\"\"\"\n    from functools import singledispatchmethod\n\n    class TestClass:\n        @singledispatchmethod\n            return str(x)\n\n    bound_method = TestClass().test\n    assert is_singledispatch_method(bound_method)\n\n    assert not is_singledispatch_method(singledispatchmethod(TestClass.test))\n\n    # Test case for py35-37\n    try:\n        from sphinx.pycode.ast import unparse as ast_unparse  # type: ignore\n    except ImportError:\n        pass\n    else:\n        class TestClass:\n            @singledispatchmethod\n                return str(x)\n\n        class TestClass2:\n            @singledispatchmethod\n                return str(x)\n\n        try:\n            sig = inspect.signature(TestClass.test)\n            assert sig.return_annotation is not sig.empty\n            assert sig.parameters['x'].annotation is not sig.parameters['x'].empty\n        except NameError:\n            pass\n        else:\n            assert sig.return_annotation == int\n            assert sig.parameters['x'].annotation == int\n\n        sig = inspect.signature(TestClass2.test)\n        assert sig.return_annotation is not sig.empty\n        assert sig.parameters['x'].annotation is not sig.parameters['x'].empty\n        assert sig.return_annotation == int\n        assert sig.parameters['x'].annotation == int\n\n", "def test_is_singledispatch_function():\n    class Dispatcher(type):\n        @singledispatch\n            pass\n\n    @Dispatcher.dispatch\n        pass\n\n    func2 = Dispatcher().dispatch\n    assert inspect.is_singledispatch_function(func1)\n    assert inspect.is_singledispatch_function(func2)\n    assert not inspect.is_singledispatch_function(Dispatcher())\n", "def test_isattributedescriptor():\n    class TestClass:\n            self.a = a\n            self.b = b\n\n        @property\n            return self.a + self.b\n\n    t = TestClass(1, 2)\n\n    # check data descriptor\n    assert inspect.isattributedescriptor(TestClass.c)\n    # check non-data descriptor\n    assert not inspect.isattributedescriptor(t.__dict__)\n    # check function\n    assert not inspect.isattributedescriptor(TestClass.__init__)\n    # check cython function\n    assert not inspect.isattributedescriptor(lambda x: x + x)\n"], "sample_21": ["def test__interpret_err_lines_empty_err_specs():\n    colnames = ['a', 'b', 'c']\n    ncols = len(colnames)\n    err_specs = {}\n    assert _interpret_err_lines(err_specs, ncols, colnames) == colnames\n\n", "def test_read_table_qdp_err_specs_unsorted():\n    # Create a table with error specifications\n    table = Table(\n        {\n            \"a\": [1, 2, 3],\n            \"a_perr\": [0.1, 0.2, 0.3],\n            \"a_nerr\": [0.4, 0.5, 0.6],\n            \"b\": [4, 5, 6],\n            \"b_err\": [0.7, 0.8, 0.9],\n        },\n        masked=True,\n    )\n\n    # Create a QDP file with the same table and error specifications\n    with io.StringIO() as f:\n        ascii.write_table_qdp(table, f, err_specs={\"serr\": [4], \"terr\": [1, 2]})\n\n    # Read the table from the QDP file\n    qdp_file = f.getvalue()\n    tables = _read_table_qdp(qdp_file, err_specs={\"serr\": [4], \"terr\": [1, 2]})\n\n    # Check that the table is read correctly\n    assert len(tables) == 1\n    assert tables[0].colnames.tolist() == [\"a\", \"a_perr\", \"a_nerr\", \"b\", \"b_err\"]\n    assert np.allclose(tables[0][\"a\"].data, np.array([1, 2, 3]))\n    assert np.allclose(tables[0][\"a_perr\"].data, np.array([0.1, 0.2, 0.3]))\n    assert np.allclose(tables[0][\"a_nerr\"].data, np.array([0.4, 0.5, 0.6]))\n    assert np.allclose(tables[0][\"b\"].data, np.array([4, 5, 6]))\n    assert np.allclose(tables[0][\"b_err\"].data, np.array([0.7, 0.8, 0.9]))\n\n    # Check that the initial comments are stored correctly\n    assert tables[0].meta[\"initial_comments\"] == [\"!\", \"a b c d\"]\n    assert tables[0].meta[\"comments\"] == [\"!\"]\n\n    # Check that the error specifications are applied correctly\n    assert np.allclose(tables[0][\"a\"].mask, np.array([False, False, False]))\n    assert np.allclose", "def test_get_type_from_list_of_lines_delimiter():\n    lines = [\n        \"! Initial comment\",\n        \"123.45, 2.34, 5.67\",\n        \"READ SERR 1\",\n        \"! 2nd table comment\",\n        \"789.01, 3.23, 4.56, 7.89, 8.90\",\n        \"READ TERR 2\",\n        \"! 3rd table comment\",\n        \"NO, NO, NO, NO, NO\",\n    ]\n    delimiter = \",\"\n    types, ncol = _get_type_from_list_of_lines(lines, delimiter=delimiter)\n    assert types == [\"comment\", \"data,3\", \"command\", \"comment\", \"data,5\", \"command\", \"comment\"]\n    assert ncol == 5\n", "def test__get_tables_from_qdp_file_multiple_tables_with_different_columns():\n    lines = [\n        \"! Initial comment 1\",\n        \"! Initial comment 2\",\n        \"READ TERR 1\",\n        \"READ SERR 2\",\n        \"! Table 1 comment\",\n        \"!a a_err b\",\n        \"53000.5   0.25  1\",\n        \"54000.5   1.25  2\",\n        \"NO NO NO\",\n        \"! Table 2 comment\",\n        \"!a a_err b_err\",\n        \"53000.5   0.25  1  0.5\",\n        \"54000.5   1.25  2  1\",\n    ]\n    contents, ncol = _get_type_from_list_of_lines(lines)\n    table_list = _get_tables_from_qdp_file(lines)\n\n    assert len(table_list) == 2\n    assert table_list[0].shape == (2, 3)\n    assert table_list[1].shape == (2, 4)\n    assert table_list[0].colnames == ['col1', 'col2_err', 'col3']\n    assert table_list[1].colnames == ['col1', 'col2_err', 'col3_err', 'col4']\n", "def test_read_table_qdp_names():\n    table = Table([[1, 2], [3, 4]], names=['a', 'b'])\n    file_contents = \"\"\"! Initial comment"], "sample_345": ["    def setUp(self):\n        self.temp_dir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.temp_dir)\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with self.assertRaises(NotImplementedError):\n            reloader.tick()\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with self.assertRaises(NotImplementedError):\n            next(reloader.tick())\n", "    def test_iter_modules_and_files_sorting(self):\n        import_module('django.test')\n        self.import_and_cleanup('django.test')\n        \n        path = self.temporary_file('test.py')\n        with open(path, 'w') as f:\n            f.write('from django.test import TestCase')\n        \n        module_path = path.resolve().absolute()\n        self.addCleanup(lambda: sys.path_importer_cache.clear())\n        self.addCleanup(lambda: sys.modules.pop('test', None))\n        \n        # Test that modules are sorted based on their name, longest first\n        self.import_and_cleanup('test.test')\n        self.import_and_cleanup('test.test_module')\n        self.import_and_cleanup('test')\n        \n        self.clear_autoreload_caches()\n        self.assertEqual(list(autoreload.iter_all_python_module_files()), [module_path])\n        \n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n        ", "    def test_update_watches(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            # Create a test file\n            test_file = Path(tmpdir) / 'test.py'\n            test_file.write_text('print(\"hello\")')\n            # Create a non-existent directory\n            non_existent_dir = Path('/non/existent/dir')\n            # Create a glob pattern\n            glob_pattern = '**/test.py'\n            # Create a file in sys.path\n            sys_path_file = Path(__file__).parent / 'test_watchman_reloader.py'\n            # Create a file with a different name in sys.path\n            different_name_file = Path(__file__).parent / 'test_watchman_reloader_different_name.py'\n\n            reloader = autoreload.WatchmanReloader()\n            reloader.extra_files = {str(test_file)}\n            reloader.directory_globs = {non_existent_dir: {glob_pattern}}\n            reloader._update_watches()\n\n            # Check that the file is being watched\n            self.assertIn(str(test_file), reloader.watched_files())\n            # Check that the non-existent directory is not being watched\n            self.assertNotIn(non_existent_dir, reloader.watched_roots(reloader.watched_files()))\n            # Check that the glob pattern is being watched\n            self.assertIn(glob_pattern, reloader.directory_globs[non_existent_dir])\n            # Check that the file in sys.path is being watched\n            self.assertIn(str(sys_path_file), reloader.watched_files())\n            # Check that the file with a different name in sys.path is not being watched\n            self.assertNotIn(str(different_name_file), reloader.watched_files())\n"], "sample_182": ["    def test_difference_with_empty_queryset(self):\n        qs = Number.objects.difference(Number.objects.none())\n        self.assertEqual(list(qs), [])\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_values_with_none_and_expressions(self):\n        class MyModel(models.Model):\n            field1 = IntegerField()\n            field2 = IntegerField()\n\n        MyModel.objects.create(field1=1, field2=2)\n        MyModel.objects.create(field1=3, field2=4)\n\n        qs = MyModel.objects.values('field1', 'field2').values('field1')\n        self.assertNumbersEqual(qs, [(1,), (3,)])\n", "    def setUpTestData(cls):\n        cls.qs = Number.objects.all()\n", "    def test_union_queryset_select_related(self):\n        \"\"\"Test select_related() with union query.\"\"\"\n        qs1 = Number.objects.filter(num__lte=5).select_related('other_num')\n        qs2 = Number.objects.filter(num__gt=5).select_related('other_num')\n        qs3 = qs1.union(qs2)\n        # We need to check the select_related() usage on the union() result.\n        # However, the union() method ignores the select_related() usage, so we\n        # need to check if it was applied to the original querysets.\n        self.assertTrue(qs1.query.select_related)\n        self.assertTrue(qs2.query.select_related)\n        # The union() method doesn't apply select_related() to the result.\n        self.assertFalse(qs3.query.select_related)\n        self.assertEqual(list(qs3), [\n            Number(num=0, other_num=10),\n            Number(num=1, other_num=9),\n            Number(num=2, other_num=8),\n            Number(num=3, other_num=7),\n            Number(num=4, other_num=6),\n            Number(num=5, other_num=5),\n            Number(num=6, other_num=4),\n            Number(num=7, other_num=3),\n            Number(num=8, other_num=2),\n            Number(num=9, other_num=1),\n            Number(num=10, other_num=0),\n        ])\n"], "sample_892": ["    def test_decision_function_sparse_input_types(self, estimator, X, y, classes, expected):\n        clf = AdaBoostClassifier(estimator=estimator)\n        clf.fit(X, y)\n        assert_array_equal(clf.decision_function(X.toarray()), expected)\n", "def test_predict_log_proba():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    clf = AdaBoostClassifier()\n    clf.fit(X, np.array([0, 0, 1]))\n    assert_array_almost_equal(\n        clf.predict_log_proba(X),\n        np.array([[0, 0], [0, 0], [0, 1]]),\n    )\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    reg = AdaBoostRegressor()\n    reg.fit(X, np.array([-1, -1, 1]))\n    assert_array_almost_equal(\n        reg.predict_log_proba(X),\n        np.array([[-np.inf, -np.inf], [-np.inf, -np.inf], [-np.inf, 0]]),\n    )\n\n    # Test that predict_log_proba works with a base estimator\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n    clf.fit(X, np.array([0, 0, 1]))\n    assert_array_almost_equal(\n        clf.predict_log_proba(X),\n        np.array([[0, 0], [0, 0], [0, 1]]),\n    )\n\n    # Test that predict_log_proba works with a base estimator that has predict_proba\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor())\n    reg.fit(X, np.array([-1, -1, 1]))\n    assert_array_almost_equal(\n        reg.predict_log_proba(X),\n        np.array([[-np.inf, -np.inf], [-np.inf, -np.inf], [-np.inf, 0]]),\n    )\n", "    def test_adaboost_regressor_fits(self):\n        # Test that AdaBoostRegressor fits\n        X = [[1, 2], [3, 4]]\n        y = [2, 3]\n        regr = AdaBoostRegressor()\n        regr.fit(X, y)\n        self.assertTrue(hasattr(regr, 'estimators_'))\n        self.assertTrue(hasattr(regr, 'estimator_weights_'))\n", "def test_adaboost_regressor_median_prediction():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([2, 4, 5])\n\n    # Test with perfect fit\n    clf = AdaBoostRegressor()\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2, 4, 5])\n\n    # Test with non-perfect fit\n    y = np.array([2, 3, 5])\n    clf = AdaBoostRegressor()\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2.0, 3.0, 5.0])\n\n    # Test with a regression model as base estimator\n    base_estimator = LinearRegression()\n    base_estimator.fit(X, y)\n    clf = AdaBoostRegressor(base_estimator=base_estimator)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2.0, 3.0, 5.0])\n\n    # Test with a regression model as base estimator and a loss function different than 'linear'\n    base_estimator = LinearRegression()\n    base_estimator.fit(X, y)\n    clf = AdaBoostRegressor(base_estimator=base_estimator, loss='square')\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2.0, 3.0, 5.0])\n\n    # Test with a different number of estimators\n    clf = AdaBoostRegressor(n_estimators=10)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2.0, 3.0, 5.0])\n\n    # Test with a different learning rate\n    clf = AdaBoostRegressor(learning_rate=0.5)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), [2.0, 3.0, 5.0])\n", "def test_adaboost_regressor_with_all_ones_as_sample_weight():\n    \"\"\"Test that AdaBoostRegressor behaves correctly with all ones as sample weights\"\"\"\n    # Test setup\n    rng = np.random.RandomState(0)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    sample_weight = np.ones_like(y)\n\n    # Create an AdaBoost regressor with a DecisionTreeRegressor as base estimator\n    ada_regressor = AdaBoostRegressor(\n        base_estimator=DecisionTreeRegressor(), n_estimators=10\n    )\n\n    # Fit the AdaBoost regressor\n    ada_regressor.fit(X, y, sample_weight=sample_weight)\n\n    # Check that the estimator_weights_ and estimator_errors_ attributes are correctly set\n    assert ada_regressor.estimator_weights_.sum() == 1.0\n    assert np.all(ada_regressor.estimator_errors_ == 1.0)\n\n    # Check that the prediction is the same as the weighted median of the predictions\n    # of the base estimators\n    predictions = np.array([est.predict(X) for est in ada_regressor.estimators_])\n    median_predictions = np.median(predictions, axis=0)\n    assert_array_almost_equal(ada_regressor.predict(X), median_predictions)\n\n    # Check that the staged_predict method yields the same results as predict\n    for i, prediction in enumerate(ada_regressor.staged_predict(X)):\n        assert_array_almost_equal(prediction, median_predictions)\n        if i >= ada_regressor.n_estimators - 1:\n            break\n\n    # Check that the decision_function method returns the same results as predict\n    decision = ada_regressor.decision_function(X)\n    assert_array_almost_equal(decision, median_predictions)\n\n    # Check that the staged_decision_function method yields the same results as\n    # decision_function\n    for decision in ada_regressor.staged_decision_function(X):\n        assert_array_almost_equal(decision, median_predictions)\n        if i >= ada_regressor.n_estimators - 1:\n            break\n\n    # Check that the predict_log_proba method raises a ValueError\n    with pytest.raises(ValueError):\n        ada_regressor.predict_log_proba(X)\n\n    # Check that the predict_proba method returns the same results as predict_log_proba\n    predict_proba = ada"], "sample_75": ["    def test_m2m_descriptor_prefetch_related(self):\n        # Test that ManyToManyDescriptor uses prefetch_related when possible.\n        Book.objects.all().prefetch_related('authors').values_list('id', flat=True)\n\n        with CaptureQueriesContext() as queries:\n            result = Book.objects.prefetch_related('authors').values_list('id', flat=True)\n            self.assertEqual(len(queries), 2)\n\n        # Test that ManyToManyDescriptor doesn't use prefetch_related when it's not possible.\n        Book.objects.all().select_related('authors').values_list('id', flat=True)\n\n        with CaptureQueriesContext() as queries:\n            result = Book.objects.select_related('authors').values_list('id', flat=True)\n            self.assertEqual(len(queries), 1)\n", "    def test_foreign_key_relation_through_swapped_model(self):\n        class SwappableAuthor(Author):\n            class Meta:\n                swappable = True\n                swappable_settings = {'AUTHOR': 'myapp.Author'}\n\n        class Book(models.Model):\n            authors = models.ForeignKey(SwappableAuthor, on_delete=models.CASCADE, related_name='books')\n\n        Book.objects.create(title='My Book')\n        self.assertRaises(ValueError, lambda: Book._meta.get_field('authors').remote_field.to)\n", "    def test_foreignkey_check_related_name_is_valid(self):\n        with self.assertRaises(ObjectDoesNotExist):\n            Author.objects.create(name='Unknown')\n        \n        author = Author.objects.create(name='John')\n        Field.objects.create(name='foreign', related_name='invalid').full_clean()\n", "    def test_m2m_through_model(self):\n        # Test that the m2m through model is correctly generated.\n        self.assertTrue(BookReview.objects.filter(book=Book.objects.get(title='Poems')).exists())\n        self.assertTrue(BookReview.objects.filter(reader=Reader.objects.get(name='Amy')).exists())\n", "    def test_prefetch_related_on_self_reference(self):\n        # Test that self-reference in prefetch_related doesn't cause an infinite loop\n        # (issue #26388)\n        class Employee(models.Model):\n            name = models.CharField(max_length=255)\n            supervisor = models.ForeignKey('self', on_delete=models.CASCADE)\n\n        employees = Employee.objects.prefetch_related('supervisor').all()\n        with CaptureQueriesContext() as queries:\n            list(employees)\n        self.assertEqual(len(queries), 1)  # Only one query should be executed\n"], "sample_982": ["def test_prime_factors():\n    assert primefactors(1) == []\n    assert primefactors(0) == [0]\n    assert primefactors(-1) == [-1]\n    assert primefactors(-6) == [-2, -3]\n    assert primefactors(2) == [2]\n    assert primefactors(3) == [3]\n    assert primefactors(4) == [2]\n    assert primefactors(5) == [5]\n    assert primefactors(6) == [2, 3]\n    assert primefactors(7) == [7]\n    assert primefactors(8) == [2]\n    assert primefactors(9) == [3]\n    assert primefactors(10) == [2, 5]\n    assert primefactors(11) == [11]\n    assert primefactors(12) == [2, 3]\n    assert primefactors(13) == [13]\n    assert primefactors(14) == [2, 7]\n    assert primefactors(15) == [3, 5]\n    assert primefactors(16) == [2]\n    assert primefactors(17) == [17]\n    assert primefactors(18) == [2, 3]\n    assert primefactors(19) == [19]\n    assert primefactors(20) == [2, 5]\n    assert primefactors(21) == [3, 7]\n    assert primefactors(22) == [2, 11]\n    assert primefactors(23) == [23]\n    assert primefactors(24) == [2, 3]\n    assert primefactors(25) == [5]\n    assert primefactors(26) == [2, 13]\n    assert primefactors(27) == [3]\n    assert primefactors(28) == [2, 7]\n    assert primefactors(29) == [29]\n    assert primefactors(30) == [2, 3, 5]\n    assert primefactors(31) == [31]\n    assert primefactors(32) == [2]\n    assert primefactors(33) == [3, 11]\n    assert primefactors(34) == [2, 17]\n    assert primefactors", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(3) == 1\n    assert primeomega(4) == 2\n    assert primeomega(5) == 1\n    assert primeomega(6) == 2\n    assert primeomega(7) == 1\n    assert primeomega(8) == 3\n    assert primeomega(9) == 2\n    assert primeomega(10) == 2\n    assert primeomega(11) == 1\n    assert primeomega(12) == 3\n    assert primeomega(13) == 1\n    assert primeomega(14) == 2\n    assert primeomega(15) == 2\n    assert primeomega(16) == 4\n    assert primeomega(17) == 1\n    assert primeomega(18) == 2\n    assert primeomega(19) == 1\n    assert primeomega(20) == 3\n", "def test_primefactors():\n    # test edge cases\n    assert primefactors(0) == []\n    assert primefactors(-5) == [5]\n    assert primefactors(1) == []\n\n    # test simple cases\n    assert set(primefactors(10)) == {2, 5}\n    assert set(primefactors(16)) == {2}\n    assert set(primefactors(15)) == {3, 5}\n\n    # test large numbers\n    assert set(primefactors(1000003)) == {1000003}\n    assert set(primefactors(1000007)) == {1000007}\n    assert set(primefactors(1000009)) == {1000009}\n\n    # test with limit\n    assert set(primefactors(1000003, limit=1000)) == set()\n    assert set(primefactors(1000007, limit=1000)) == set()\n    assert set(primefactors(1000009, limit=1000)) == set()\n\n    # test with verbose\n    assert primefactors(1000003, verbose=True) == [1000003]\n    assert primefactors(1000007, verbose=True) == [1000007]\n    assert primefactors(1000009, verbose=True) == [1000009]\n\n    # test with multiple large prime factors\n    assert set(primefactors(1000003*1000007)) == {1000003, 1000007}\n    assert set(primefactors(1000003*1000009)) == {1000003, 1000009}\n    assert set(primefactors(1000007*1000009)) == {1000007, 1000009}\n\n    # test with multiple large prime factors and limit\n    assert set(primefactors(1000003*1000007, limit=1000)) == set()\n    assert set(primefactors(1000003*1000009, limit=1000)) == set()\n    assert set(primefactors(1000007*1000009, limit=1000)) == set()\n", "def test_prime_omega():\n    from sympy.ntheory.factor_ import primeomega\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(3) == 1\n    assert primeomega(5) == 1\n    assert primeomega(8) == 1\n    assert primeomega(9) == 2\n    assert primeomega(16) == 2\n    assert primeomega(25) == 2\n    assert primeomega(30) == 3\n    assert primeomega(32) == 3\n    assert primeomega(36) == 3\n    assert primeomega(37) == 1\n    assert primeomega(48) == 3\n    assert primeomega(49) == 3\n    assert primeomega(50) == 3\n    assert primeomega(64) == 3\n    assert primeomega(65) == 3\n    assert primeomega(68) == 3\n    assert primeomega(72) == 3\n    assert primeomega(75) == 3\n    assert primeomega(77) == 2\n    assert primeomega(81) == 4\n    assert primeomega(88) == 3\n    assert primeomega(90) == 3\n    assert primeomega(99) == 3\n    assert primeomega(100) == 4\n    assert primeomega(121) == 3\n    assert primeomega(125) == 3\n    assert primeomega(128) == 4\n    assert primeomega(144) == 4\n    assert primeomega(150) == 4\n    assert primeomega(169) == 3\n    assert primeomega(171) == 4\n    assert primeomega(176) == 4\n    assert primeomega(175) == 3\n    assert primeomega(176) == 4\n    assert primeomega(184) == 4\n    assert primeomega(196) == 4\n    assert primeomega(200) == 4\n    assert primeomega(225) == 4\n    assert primeomega(230) == 4\n    assert primeomega(243) == 4\n    assert primeomega(256) == 5\n    assert primeomega(270) == 4\n    assert primeomega(273)", "    def test_udivisor_count(self):\n        assert udivisor_count(12) == 6\n        assert udivisor_count(27) == 6\n        assert udivisor_count(32) == 4\n"], "sample_1018": ["def test_fcode_FortranReturn():\n    from sympy import Function, sin, cos, atan2\n    from sympy.codegen.ast import FortranReturn\n\n    expr = Function(\"f\")()\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return'\n    assert result == expected\n\n    expr = Function(\"f\")(sin, cos)\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return sin(cos)'\n    assert result == expected\n\n    expr = Function(\"f\")(sin, cos, atan2)\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return sin(cos,atan2)'\n    assert result == expected\n\n    expr = Function(\"f\")()\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return'\n    assert result == expected\n\n    expr = Function(\"f\")(sin, cos)\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return sin(cos)'\n    assert result == expected\n\n    expr = Function(\"f\")(sin, cos, atan2)\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return sin(cos,atan2)'\n    assert result == expected\n\n    expr = FortranReturn()\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return'\n    assert result == expected\n\n    expr = FortranReturn(sin, cos)\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return sin(cos)'\n    assert result == expected\n\n    expr = FortranReturn(sin, cos, atan2)\n    result = fcode(expr, **{\"standard\": 77})\n    expected = '      return sin(cos,atan2)'\n    assert result == expected\n\n    expr = FortranReturn()\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return'\n    assert result == expected\n\n    expr = FortranReturn(sin, cos)\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return sin(cos)'\n    assert result == expected\n\n    expr = FortranReturn(sin, cos, atan2)\n    result = fcode(expr, **{\"standard\": 95})\n    expected = 'return sin(cos,atan2)'\n    assert", "def test_fcode_matrix_assignment():\n    A = MatrixSymbol('A', 3, 3)\n    B = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    code = fcode(A, B)\n    expected = \"      A(1, 1) = 1\\n      A(1, 2) = 2\\n      A(1, 3) = 3\\n      A(2, 1) = 4\\n      A(2, 2) = 5\\n      A(2, 3) = 6\\n      A(3, 1) = 7\\n      A(3, 2) = 8\\n      A(3, 3) = 9\"\n    assert code.strip() == expected.strip()\n", "def test_type_mappings():\n    # Test type mappings\n    assert FCodePrinter().type_mappings[float] == 'real*8'\n    assert FCodePrinter().type_mappings[int] == 'integer*4'\n    assert FCodePrinter().type_mappings[complex] == 'complex*16'\n    assert FCodePrinter().type_mappings[bool] == 'logical'\n", "def test_print_fcode_indexed_with_contract():\n    x = IndexedBase('x', shape=(5,))\n    y = IndexedBase('y', shape=(5,))\n    t = IndexedBase('t', shape=(5,))\n    Dy = IndexedBase('Dy', shape=(5,))\n    i = Idx('i', 5)\n    e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n    fcode(e, assign_to=e.lhs, contract=True)\n", "def test_print_NumberSymbol():\n    from sympy import Number, Rational\n    from sympy.codegen import fcode, FCodePrinter\n    from sympy.printing.fcode import FCodePrinter\n\n    # Ensure that _print_NumberSymbol works correctly\n    x = Number(5.0)\n    assert fcode(x) == \"5.0d0\"\n    x = Rational(1, 2)\n    assert fcode(x) == \"0.5d0\"\n"], "sample_628": ["def test_ignore_comment_directive(self):\n    self.checker.ignore_comment_directive_list = [\"fmt: on\"]\n    # Test that comments with \"fmt: on\" are ignored\n    self._check_fixtures_for_docstring_ignore_directive(\n        \"\"\"\n                \"\"\"\"\n                    # fmt: on\n                    \"\"\"\n        \"\"\",\n        []\n    )\n\n    # Test that comments without \"fmt: on\" are not ignored\n    self._check_fixtures_for_docstring_ignore_directive(\n        \"\"\"\n                \"\"\"\"\n                    # fmt: off\n                    \"\"\"\n        \"\"\",\n        [Message(\"wrong-spelling-in-docstring\", line=1, args=(\"my_function\", \"my_function\", \"^my_function\", \"'my_functiion'\"))]\n    )\n\n    # Test that comments with multiple directives are not ignored\n    self._check_fixtures_for_docstring_ignore_directive(\n        \"\"\"\n                \"\"\"\"\n                    # fmt: on\n                    # fmt: off\n                    \"\"\"\n        \"\"\",\n        [Message(\"wrong-spelling-in-docstring\", line=1, args=(\"my_function\", \"my_function\", \"^my_function\", \"'my_functiion'\"))]\n    )\n", "    def test_max_spelling_suggestions(self):\n        \"\"\"Test that the max spelling suggestions are limited.\"\"\"\n        self.set_config(max_spelling_suggestions=2)\n        self._check_messages(\"wrong-spelling-in-comment\", \"C0401\", \"t\", line_no=1)\n        self._check_messages(\"wrong-spelling-in-comment\", \"C0401\", \"st\", line_no=2)\n        # Check that only 2 suggestions are given.\n        self.assertNumMessages(2)\n        self.assertMessageEqual(\n            \"wrong-spelling-in-comment\",\n            \"Used when a word in comment is not spelled correctly.\",\n            line_no=1,\n            col_offset=1,\n            args=(\"t\", \"test\", \" 'test' or 'tet'\",),\n        )\n        self.assertMessageEqual(\n            \"wrong-spelling-in-comment\",\n            \"Used when a word in comment is not spelled correctly.\",\n            line_no=2,\n            col_offset=1,\n            args=(\"st\", \"test\", \" 'set' or 'sit'\",),\n        )\n", "def test_ignore_words(self):\n    \"\"\"Test that words in ignore list are not flagged\"\"\"\n    source = \"\"\"", "def test_spelling_checker_invalid_characters_in_docstring(self):\n    \"\"\"check that invalid characters in a docstring is handled correctly\"\"\"\n    source = \"\"\"\n    \"\"\"\n    expected_msg = Message(\"invalid-characters-in-docstring\", args=(\"a \",))\n    self.checker._check_docstring(node=astroid.Module(source))\n    self.assertMessageExpected(self.messages, expected_msg)\n", "    def test_check_spelling_in_docstring_line(self):\n        # Test case where the docstring is a single line comment\n        # Test case for spelling checker against a docstring\n        #   with a single line.\n        source = \"\"\"\n            class TestDocstring:\n                    \"\"\"This is a single line docstring.\"\"\"\n        \"\"\"\n        exp = [Message(\n            'wrong-spelling-in-docstring',\n            line=1,\n            col=46,\n            args=(\"line\", \"This is a single line docstring.\\n\", \"^\\^\", \"'docstring' or 'document' or 'docuement' or 'documen' or 'documennt' or 'documenntt'\"),\n        )]\n        self.checker.open()\n        self._run(source, exp)\n        self.checker.close()\n"], "sample_1199": ["compilation error", "def test_tensor_product_simp_scalar():\n    A, B, C, D = symbols('A,B,C,D', commutative=False)\n    e = TensorProduct(A, B) * 2\n    e_simplified = tensor_product_simp(e)\n    assert e_simplified == 2 * TensorProduct(A, B), \"Tensor product simplification failed\"\n", "compilation error", "def test_tensor_product_simp_Add():\n    A = symbols('A', commutative=False)\n    B = symbols('B', commutative=False)\n    C = symbols('C', commutative=False)\n    D = symbols('D', commutative=False)\n    tp1 = TensorProduct(A, B)\n    tp2 = TensorProduct(C, D)\n    e = tp1 + tp2\n    expected = (A + C)*B*D\n    assert unchanged(tensor_product_simp(e), expected)\n", "def test_tensorproduct_simp_with_Bra_Ket():\n    a, b = symbols('a, b', commutative=False)\n    bra = TP(Bra(Qubit(0)), Dagger(Qubit(1)))\n    ket = TP(Qubit(0), Qubit(1))\n    tp = bra * ket\n    expected = TensorProduct(Bra(Qubit(0)), Dagger(Qubit(0)), Bra(Qubit(1)), Dagger(Qubit(1)))\n    assert tensor_product_simp(tp) == expected\n"], "sample_1044": ["def test_integer_nthroot():\n    assert integer_nthroot(16, 2) == (4, True)\n    assert integer_nthroot(16, 3) == (2, True)\n    assert integer_nthroot(16, 4) == (2, False)\n    assert integer_nthroot(26, 2) == (5, False)\n    assert integer_nthroot(26, 3) == (2, False)\n    assert integer_nthroot(26, 4) == (2, False)\n    assert integer_nthroot(0, 3) == (0, True)\n    assert integer_nthroot(2, 2) == (1, True)\n    assert integer_nthroot(-2, 2) == (1, False)\n    assert integer_nthroot(-16, 2) == (4, False)\n    assert integer_nthroot(-16, 3) == (2, False)\n    assert integer_nthroot(-16, 4) == (2, False)\n    assert integer_nthroot(-26, 2) == (5, False)\n    assert integer_nthroot(-26, 3) == (2, False)\n    assert integer_nthroot(-26, 4) == (2, False)\n    assert integer_nthroot(-2, 3) == (1, False)\n    assert integer_nthroot(-2, 4) == (1, False)\n    assert integer_nthroot(-2, 5) == (1, False)\n    assert integer_nthroot(-2, 6) == (1, False)\n    assert integer_nthroot(-2, 7) == (1, False)\n    assert integer_nthroot(-2, 8) == (1, False)\n    assert integer_nthroot(-2, 9) == (1, False)\n    assert integer_nthroot(1, 1) == (1, True)\n    assert integer_nthroot(1, 2) == (1, True)\n    assert integer_nthroot(1, 3) == (1, True)\n    assert integer_nthroot(1, 4) == (1, True)\n    assert integer_nthroot(1, 5) == (1, True)\n    assert integer_nthroot(1, 6) == (1, True)\n    assert integer_nthroot(1, 7) == (1, True)\n    assert integer_nthroot(1, 8", "def test_Pow_matches():\n    from sympy import sqrt, I, oo\n    # positive base, positive exponent\n    assert Pow(sqrt(4), 2).matches(Pow(2, 2)) is not None\n    assert Pow(sqrt(4), 2).matches(Pow(4, 2)) is not None\n    assert Pow(sqrt(4), 2).matches(sqrt(16)) is not None\n    assert Pow(2, 2).matches(sqrt(4)) is not None\n    # positive base, negative exponent\n    assert Pow(sqrt(4), -2).matches(Pow(2, -2)) is not None\n    assert Pow(sqrt(4), -2).matches(Pow(4, -2)) is not None\n    assert Pow(sqrt(4), -2).matches(sqrt(1/16)) is not None\n    assert Pow(2, -2).matches(sqrt(1/4)) is not None\n    # negative base, even exponent\n    assert Pow(-sqrt(4), 2).matches(Pow(-2, 2)) is not None\n    assert Pow(-sqrt(4), 2).matches(Pow(-4, 2)) is not None\n    assert Pow(-sqrt(4), 2).matches(sqrt(16)) is not None\n    assert Pow(-2, 2).matches(sqrt(4)) is not None\n    # negative base, odd exponent\n    assert Pow(-sqrt(4), -2).matches(Pow(-2, -2)) is not None\n    assert Pow(-sqrt(4), -2).matches(Pow(-4, -2)) is not None\n    assert Pow(-sqrt(4), -2).matches(sqrt(1/16)) is not None\n    assert Pow(-2, -2).matches(sqrt(1/4)) is not None\n    # zero base\n    assert Pow(0, 0).matches(1) is not None\n    assert Pow(0, 1).matches(0) is not None\n    assert Pow(0, 2).matches(0) is not None\n    assert Pow(0, 3).matches(0) is not None\n    assert Pow(0, -1).matches(oo) is not None\n    # infinity base\n    assert Pow(oo, 2).matches(oo)", "def test_is_power():\n    a, b = Symbol('a'), Symbol('b')\n    assert Pow(a, 2).is_power()\n    assert Pow(2, b).is_power()\n    assert Pow(Pow(a, 2), b).is_power()\n    assert (a**2).is_power()\n    assert (2**b).is_power()\n\n    with raises(InconsistentAssumptions):\n        Pow(S.Zero, 2).is_power()\n    with raises(InconsistentAssumptions):\n        Pow(S.One, -1).is_power()\n    with raises(InconsistentAssumptions):\n        Pow(S.Infinity, 2).is_power()\n", "def test_Pow_as_base_exp():\n    assert Pow(2, 3).as_base_exp() == (2, 3)\n    assert Pow(3, -4).as_base_exp() == (3, -4)\n    assert Pow(4, 1/2).as_base_exp() == (2, 1)\n    assert Pow(sqrt(4), 1/3).as_base_exp() == (2**(1/3), 1/3)\n    assert Pow(1, 1).as_base_exp() == (1, 1)\n    assert Pow(0, -1).as_base_exp() == (1, -1)\n    assert Pow(-2, 1).as_base_exp() == (-2, 1)\n    assert Pow(-2, -1).as_base_exp() == (-1/2, -1)\n    assert Pow(1/2, -2).as_base_exp() == (4, -2)\n    assert Pow(3 + 4*I, 1/2).as_base_exp() == ((3 + 4*I)**(1/2), 1/2)\n", "def test_Pow_nseries():\n    from sympy import symbols, sin, exp, cos, oo, I, O\n    x = symbols('x')\n    assert Pow(sin(x), 3)._eval_nseries(x, 5).removeO() == \\\n        x**3 + 3*x**5 + 15*x**7/4 + O(x**5)\n    assert Pow(sin(x), -3)._eval_nseries(x, 5).removeO() == \\\n        1 + 3*x**3/2 - 15*x**5/8 + O(x**5)\n    assert Pow(x, 3)._eval_nseries(x, 5) == x**3 + 3*x**4 + O(x**5)\n    assert Pow(x, -3)._eval_nseries(x, 5) == 1/x**3 + O(x**5)\n    assert Pow(oo, -2)._eval_nseries(x, 5) == \\\n        O(x**(-5), x)\n    assert Pow(x, 3)._eval_nseries(x, 5, logx=x).removeO() == \\\n        x**3 + 3*x**4 + 9*x**5 + O(x**5)\n    assert Pow(x, -3)._eval_nseries(x, 5, logx=x).removeO() == \\\n        1/x**3 + O(x**5)\n    assert Pow(1 + x, 5)._eval_nseries(x, 5).removeO() == \\\n        5 + 25*x + 125*x**2/2 + 625*x**3/6 + 3125*x**4/24 + O(x**5)\n    assert Pow(1 + x, -5)._eval_nseries(x, 5).removeO() == \\\n        1 - 5*x + 25*x**2/2 - 125*x**3/6 + 625*x**4/24 + O(x**5)\n    assert Pow(exp(x), 3)._eval_nseries(x, 5).removeO() == \\\n        exp(3*x) + 3*exp(3*x)*x + 9*exp(3*x)*x**2/2 + O(x**3)\n    assert Pow(exp(x), -3)._eval_nseries(x, 5).removeO() == \\\n        1/x**3 - "], "sample_1205": ["def test_gcdex():\n    R, x, y = ring('x,y', QQ)\n    f = x**2 + 2*x*y + y**2\n    g = x + y\n    h, s, t = f.gcdex(g)\n    assert h == -y + 1\n    assert s == -y + 1\n    assert t == x + y\n\n    R, x, y = ring('x,y', ZZ)\n    f = x**2 + 2*x*y + y**2\n    g = x + y\n    h, s, t = f.gcdex(g)\n    assert h == y + 1\n    assert s == y + 1\n    assert t == x + y\n\n    R, x, y = ring('x,y', FF)\n    f = x**2 + 2*x*y + y**2\n    g = x + y\n    h, s, t = f.gcdex(g)\n    assert h == 1\n    assert s == 1\n    assert t == x + y\n\n    R, x, y = ring('x,y', RR)\n    f = x**2 + 2*x*y + y**2\n    g = x + y\n    with raises(ExactQuotientFailed):\n        f.gcdex(g)\n", "    def test_ring_hash(self):\n        _, x, y = ring(\"x,y\", ZZ)\n        assert hash(xring(\"x,y\", ZZ)[0]) == hash(xring(\"x,y\", ZZ)[0])\n        assert hash(ring(\"x,y\", ZZ)[0]) == hash(ring(\"x,y\", ZZ)[0])\n", "def test_ring_new():\n    R, x, y, z = ring(\"x, y, z\", ZZ)\n    p = R.ground_new(4)\n    assert p == 4\n", "def test_PolyElement_lt():\n    # Test that the __lt__ method works correctly for two polynomials.\n    R, x, y = ring('x,y', ZZ)\n    f = x + y\n    g = x + y + 1\n    assert f < g\n    assert not g < f\n", "def test_ring():\n    R, x, y = ring('x,y', ZZ)\n    assert str(R) == 'Polynomial ring in x, y over ZZ with lex order'\n"], "sample_571": ["def test_lmplot_dropna():\n    df = pd.DataFrame({\n        'x': np.random.rand(10),\n        'y': np.random.rand(10),\n        'z': np.random.rand(10)\n    })\n    df.loc[[5], 'x'] = np.nan\n    df.loc[[7], 'y'] = np.nan\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=df, dropna=True)\n    assert len(g.ax_flat[0].collections) == 1\n\n    g = lm.lmplot(x=\"x\", y=\"y\", data=df, dropna=False)\n    assert len(g.ax_flat[0].collections) == 2\n", "    def test_lmlot_partial_regression(self):\n        data = pd.DataFrame(dict(x=self.rs.normal(size=60),\n                                 d=self.rs.randint(-2, 3, 60),\n                                 y=self.rs.gamma(4, size=60),\n                                 z=self.rs.gamma(4, size=60),\n                                 s=np.tile(list(\"abcdefghij\"), 6)))\n        fig = lm.lmplot(x=\"x\", y=\"y\", x_partial=\"z\", data=data)\n        assert fig.fig.get_size_inches() == (8, 6)\n        ax = fig.axes[0]\n        assert ax.get_title() == \"x\"\n        assert ax.get_xlabel() == \"x\"\n        assert ax.get_ylabel() == \"y\"\n", "    def test_x_ci_none(self):\n        \"\"\"Test that x_ci=None when plotting a linear regression model.\"\"\"\n        plotter = _RegressionPlotter(self.rs.normal(size=100), self.rs.normal(size=100),\n                                     ci=None, x_ci=None)\n        ax = plt.gca()\n        plotter.plot(ax, scatter_kws={}, line_kws={})\n        assert plotter.x_ci is None\n", "    def test_regress_out(self):\n        # Test that x is not affected by y when using regress_out\n        plotter = lm._RegressionPlotter(self.df['x'], self.df['y'])\n        plotter.regress_out(self.df['x'], self.df['y'])\n        npt.assert_equal(plotter.x, self.df['x'])\n\n        # Test that y is affected by x when using regress_out\n        plotter = lm._RegressionPlotter(self.df['x'], self.df['y'])\n        plotter.regress_out(self.df['y'], self.df['x'])\n        y_mean = self.df['y'].mean()\n        npt.assert_array_equal(plotter.y, self.df['y'] - y_mean)\n\n        # Test that x and y are affected by each other when using regress_out\n        plotter = lm._RegressionPlotter(self.df['x'], self.df['y'])\n        plotter.regress_out(self.df['x'], self.df['y'])\n        plotter.regress_out(self.df['y'], self.df['x'])\n        x_mean = self.df['x'].mean()\n        y_mean = self.df['y'].mean()\n        npt.assert_array_equal(plotter.x, self.df['x'] - x_mean)\n        npt.assert_array_equal(plotter.y, self.df['y'] - y_mean)\n", "    def test_lmplot_units(self):\n        \"\"\"Regression plots using nested data.\"\"\"\n        np.random.seed(7)\n        df = pd.DataFrame(dict(x=self.rs.normal(size=60),\n                               d=self.rs.randint(-2, 3, 60),\n                               y=self.rs.gamma(4, size=60),\n                               s=np.tile(list(\"abcdefghij\"), 6)))\n        df[\"z\"] = df.y + self.rs.randn(60)\n        df[\"y_na\"] = df.y.copy()\n        df.loc[[10, 20, 30], 'y_na'] = np.nan\n        df = df.melt(id_vars=[\"s\", \"z\"], var_name=\"color\", value_name=\"y\")\n        df[\"units\"] = \"s\"\n        df[\"group\"] = df[\"s\"].apply(lambda x: list(\"abcdefghij\").index(x))\n\n        fig = lm.lmplot(x=\"x\", y=\"y\", hue=\"color\", units=\"units\",\n                        data=df, ci=95, fit_reg=True)\n\n        assert fig.axes[0].collections[0].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[1].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[2].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[3].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[4].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[5].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[6].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes[0].collections[7].get_facecolor() == (0.25, 0.25, 0.25, 0.5)\n        assert fig.axes"], "sample_864": ["def test_mean_shift__estimate_bandwidth__quantile_invalid():\n    X = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n    with pytest.raises(ValueError):\n        estimate_bandwidth(X, quantile=-0.1)\n", "def test_estimate_bandwidth_n_jobs():\n    X = np.array([[1, 1], [2, 1], [1, 0], [4, 7], [3, 5], [3, 6]])\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        bandwidth = estimate_bandwidth(X, n_jobs=-1)\n        assert bandwidth > 0\n\n", "def test_estimate_bandwidth_with_sparse_input():\n    X = sparse.csr_matrix(np.random.rand(100, 10))\n    with pytest.warns(None):\n        estimate_bandwidth(X, n_jobs=1)\n", "def test_mean_shift_estimate_bandwidth_non_negative_quantile():\n    \"\"\"Test that the estimated bandwidth is non-negative for quantile > 0.\"\"\"\n    X = np.array([[1, 1], [2, 1], [1, 0]])\n    quantile = 0.5\n    bandwidth = estimate_bandwidth(X, quantile=quantile)\n    assert bandwidth >= 0\n", "def test_mean_shift_seeds_not_initialized_properly():\n    \"\"\"Test that seeds are correctly initialized when not specified.\n\n    This test verifies that when no seeds are specified, the algorithm\n    correctly initializes the seeds either using get_bin_seeds or using\n    all data points when bin_seeding is False.\n\n    \"\"\"\n    clustering = MeanShift().fit(X)\n    assert clustering.cluster_centers_.shape[0] > 0\n    if clustering.bin_seeding:\n        assert np.allclose(clustering.cluster_centers_, get_bin_seeds(X, clustering.bandwidth, clustering.min_bin_freq))\n\n"], "sample_887": ["def test_calibrated_classifiers_when_ensemble_false_and_cv_not_prefit(tmpdir):\n    X, y = make_classification(n_samples=N_SAMPLES, n_features=6, random_state=42)\n    estimator = clone(DummyClassifier(strategy=\"stratified\"))\n    calibrated = CalibratedClassifierCV(estimator, cv=LeaveOneOut(), ensemble=False)\n    calibrated.fit(X, y)\n    assert len(calibrated.calibrated_classifiers_) == 1\n    assert calibrated.calibrated_classifiers_[0].estimator is estimator\n", "def test_calibrated_classifiers_covariate_shift(method, ensemble, data):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Introduce covariate shift by scaling X_test\n    X_test *= 2\n\n    base_clf = LogisticRegression()\n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=\"prefit\", method=method)\n    calibrated_clf.fit(X_train, y_train)\n    predictions = calibrated_clf.predict_proba(X_test)\n\n    # Check that the calibrated classifier is not affected by the covariate shift\n    np.testing.assert_allclose(calibrated_clf.predict_proba(X_test), predictions, atol=1e-3)\n", "def test_calibrated_classifiers_with_prefit_estimator(data):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    clf = CalibratedClassifierCV(base_estimator=LinearSVC(), cv=\"prefit\")\n    clf.fit(X_train, y_train)\n    assert clf.calibrated_classifiers_ == [clf.estimator]\n\n", "def test_calibration_display_from_estimator_binary_accuracy_ensemble(data):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.1, random_state=42\n    )\n    clf = MultinomialNB()\n    clf.fit(X_train, y_train)\n    y_prob = clf.predict_proba(X_test)[:, 1]\n    display = CalibrationDisplay.from_estimator(\n        clf,\n        X_test,\n        y_test,\n        n_bins=10,\n        strategy=\"quantile\",\n        name=\"MultinomialNB\",\n        ref_line=True,\n    )\n    display.ax_.set_title(\"Calibration curve for MultinomialNB\")\n    assert display.ax_.get_title() == \"Calibration curve for MultinomialNB\"\n    assert display.line_.get_label() == \"MultinomialNB\"\n    assert display.figure_.get_size_inches() == (6.4, 4.8)\n    display.plot()\n    plt.close()\n    assert display.ax_.get_xlabel() == \"Mean predicted probability (Positive class: 1)\"\n    assert display.ax_.get_ylabel() == \"Fraction of positives (Positive class: 1)\"\n    assert display.figure_.get_size_inches() == (6.4, 4.8)\n    assert display.line_.get_label() == \"MultinomialNB\"\n    assert display.line_.get_linestyle() == \"s-\"\n", "def test_calibrated_classifier_cv_ensemble_predict_proba():\n    \"\"\"Check that the predict_proba of CalibratedClassifierCV\n    behaves correctly with ensemble=True.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=4, random_state=0)\n    clf = LogisticRegression(random_state=0)\n    clf.fit(X, y)\n    clf_pred = clf.predict_proba(X)\n    calibrated = CalibratedClassifierCV(clf, cv=None, ensemble=True)\n    calibrated.fit(X, y)\n    calibrated_pred = calibrated.predict_proba(X)\n\n    assert_array_almost_equal(calibrated_pred, clf_pred)\n"], "sample_400": ["def test_rename_model_with_proxy(self):\n    before_states = self.make_project_state([\n        self.author_proxy,\n        self.author,\n    ])\n    after_states = self.make_project_state([\n        self.author_proxy_options,\n        self.author,\n    ])\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"AuthorProxy\")\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, name=\"Author\", model_name=\"testapp.AuthorProxy\")\n", "def test_deleted_models_with_dependencies(self):\n    before_states = [\n        self.author_with_book,\n        self.author_with_book_order_wrt,\n        self.book,\n    ]\n    after_states = [\n        self.author_with_book,\n        self.author_with_book_order_wrt,\n        self.book_proxy_fk,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"testapp\", 0, [\"DeleteModel\", \"AlterOrderWithRespectTo\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"Author\", order_with_respect_to=\"book\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, name=\"Author\", order_with_respect_to=None\n    )\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"DeleteModel\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"Book\", dependencies=[(\"testapp\", \"Author\")]\n    )\n    self.assertOperationFieldAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\"\n    )\n", "    def test_nested_deconstructible_default_field_types(self):\n        questioner = mock.Mock(spec=MigrationQuestioner)\n        questioner.ask_not_null_alteration.return_value = \"a default value\"\n        questioner.ask_auto_now_add_addition.return_value = \"a default value\"\n        questioner.ask_unique_callable_default_addition.return_value = \"a default value\"\n        questioner.ask_not_null_addition.return_value = \"a default value\"\n        author_name_nested_deconstructible_1 = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"name\",\n                    models.CharField(\n                        max_length=200,\n                        default=DeconstructibleObject(\n                            DeconstructibleObject(1),\n                            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2\")),\n                            a=DeconstructibleObject(\"A\"),\n                            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n                        ),\n                    ),\n                ),\n            ],\n        )\n        author_name_nested_deconstructible_2 = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"name\",\n                    models.CharField(\n                        max_length=200,\n                        default=DeconstructibleObject(\n                            DeconstructibleObject(1),\n                            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2\")),\n                            a=DeconstructibleObject(\"A\"),\n                            b=DeconstructibleObject(B=DeconstructibleObject(\"c\")),\n                        ),\n                    ),\n                ),\n            ],\n        )\n        author_name_nested_deconstructible_3 = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\n                    \"name\",\n                    models.CharField(\n                        max_length=200,\n                        default=DeconstructibleObject(\n                            DeconstructibleObject(1),\n                            (DeconstructibleObject(\"t1\"), DeconstructibleObject(\"t2\")),\n                            a=DeconstructibleObject(\"A\"),\n                            b=DeconstructibleObject(B=DeconstructibleObject(\"c-changed\")),\n                        ),\n                    ),\n                ),\n            ],\n        )\n        changes = self.get_changes(\n            [\n                author_name_nested_deconstructible_1,\n                author_name_nested_deconstructible_2,\n                author_name_nested_deconstructible_3,\n            ],\n            [\n                author_name_nested_deconstructible_", "    def test_create_altered_indexes_with_renamed_index_together(self):\n        # Create a test model with an index_together that is renamed\n        author_name_renamed = self.author_name_renamed.clone()\n        author_name_renamed.options[\"indexes\"] = [\n            models.Index(fields=[\"name\"], name=\"author_name_idx\")\n        ]\n        book = self.book.clone()\n        book.options[\"indexes\"] = [\n            models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\")\n        ]\n        book.options[\"index_together\"] = {(\"author\", \"title\")}\n        book.options[\"unique_together\"] = {(\"author\", \"title\")}\n        book.relations = {\n            \"testapp.Author\": {\n                \"name\": self.author_name_renamed,\n                \"id\": self.author_name_renamed,\n            },\n        }\n\n        # Create a test project state\n        before_states = self.make_project_state(\n            [\n                self.author_name_renamed,\n                self.book,\n            ]\n        )\n        after_states = self.make_project_state(\n            [\n                self.author_name,\n                self.book,\n            ]\n        )\n\n        # Run the autodetector\n        changes = self.get_changes(before_states.models, after_states.models)\n\n        # Check the changes\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameIndex\"]\n        )\n        migration = changes[\"testapp\"][0]\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"author_name_idx\",\n            old_name=\"book_title_author_idx\",\n            new_name=\"book_title_author_idx\",\n            old_fields=[\"name\"],\n        )\n", "def test_m2m_through_renamed(self):\n    # Create a new project with a model that has a many-to-many relationship\n    # with another model, and both models have a field that is being renamed\n    before_models = [\n        self.author_renamed_with_book,\n        self.contract_renamed,\n        self.publisher,\n    ]\n    after_models = [\n        self.author_renamed_with_book,\n        self.contract,\n        self.publisher,\n    ]\n    changes = self.get_changes(before_models, after_models)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"Author\", old_name=\"Writer\", new_name=\"Author\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, old_name=\"Deal\", new_name=\"Contract\"\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 2, old_name=\"Deal\", new_name=\"Contract\"\n    )\n\n    # Create a new project with a model that has a many-to-many relationship\n    # with another model, and the model with the foreign key to the other model\n    # is being renamed\n    before_models = [\n        self.author_with_book_order_wrt,\n        self.contract,\n        self.publisher_with_book,\n    ]\n    after_models = [\n        self.author_with_book,\n        self.contract_renamed,\n        self.publisher,\n    ]\n    changes = self.get_changes(before_models, after_models)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n\n    # Create a new project with a model that has a many-to-many relationship\n    # with another model, and the model with the through model is being renamed\n    before_models = [\n        self.author_with_book_order_wrt,\n        self.contract_renamed,\n        self.publisher_with_book,\n    ]\n    after_models = [\n        self.author_with_book,\n        self.contract,\n        self.publisher_with_aardvark_author,\n    ]\n    changes = self.get_changes(before_models, after_models)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assert"], "sample_231": ["    def test_is_active(self):\n        # Test default behavior when DEBUG is True\n        self.assertFalse(SafeExceptionReporterFilter().is_active(self.request_factory()))\n        \n        # Test default behavior when DEBUG is False\n        with override_settings(DEBUG=False):\n            self.assertTrue(SafeExceptionReporterFilter().is_active(self.request_factory()))\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        expected = {'DEBUG': 'True', 'SECRET_KEY': '********************'}\n        self.assertEqual(filter.get_safe_settings(), expected)\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertEqual(len(settings_dict), 0)\n        settings_dict = {\n            'SECRET_KEY': 'secret_key',\n            'DATABASES': {\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                    'NAME': 'db.sqlite3'\n                }\n            }\n        }\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertEqual(len(settings_dict), 2)\n        self.assertEqual(settings_dict['SECRET_KEY'], '********************')\n        self.assertEqual(settings_dict['DATABASES'], {\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'db.sqlite3'\n            }\n        })\n", "    def test_cleansed_substitute(self):\n        \"\"\" Test cleansed_substitute \"\"\"\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleansed_substitute, '********************')\n", "    def test_cleansing_setting_with_callable_value(self):\n        filter = SafeExceptionReporterFilter()\n        setting = settings.ALLOWED_HOSTS\n        wrapper = CallableSettingWrapper(setting)\n        self.assertIsInstance(wrapper._wrapped, callable)\n        self.assertEqual(filter.cleanse_setting('ALLOWED_HOSTS', wrapper), wrapper)\n"], "sample_445": ["    def test_timesince_zero_time(self):\n        \"\"\"\n        Test that timesince returns the correct string for zero time.\n        \"\"\"\n        self.assertEqual(timesince(self.t), '0 minutes')\n", "def test_timesince_future_time(self):\n    future_time = datetime.datetime(2025, 8, 14, 13, 46, 0)\n    self.assertEqual(timesince(future_time), \"0 minutes\")\n", "def test_timesince_leap_year(self):\n    # Test timesince with a leap year\n    self.t = datetime.datetime(2020, 2, 29, 13, 46, 0)  # Leap year\n    self.assertEqual(timesince(self.t), '1 year')\n    self.assertEqual(timesince(self.t, depth=1), '1 year')\n    self.assertEqual(timesince(self.t, depth=2), '1 year')\n    self.assertEqual(timesince(self.t, time_strings={'year': 'y\u00f6ld\u00fcr', 'month': 'ay\u0131n', 'week': 'hafta', 'day': 'g\u00fcn', 'hour': 'saat', 'minute': 'dakika'}), 'y\u00f6ld\u00fcr')\n", "    def test_depth_zero(self):\n        self.assertRaises(ValueError, timesince, self.t, depth=0)\n", "def test_timesince_with_future_date(self):\n    # Test with a future date\n    self.t = datetime.datetime(2099, 8, 14, 13, 46, 0)\n    self.assertEqual(timesince(self.t), \"0 minutes\")\n    self.assertEqual(timeuntil(self.t), \"0 minutes\")\n"], "sample_412": ["def test_linebreaks_autoescape(self):\n    self.check_output(linebreaks, \"Hello\\n\\nWorld\", \"Hello\\n\\nWorld\")\n    self.check_output(linebreaks, \"Hello\\nWorld\\n\\n\", \"Hello\\n\\nWorld\\n\\n\")\n    self.check_output(\n        linebreaks, \"<p>Hello</p>\\n\\n<p>World</p>\", \"<p>Hello</p>\\n\\n<p>World</p>\"\n    )\n    self.check_output(\n        linebreaks,\n        \"Hello\\n\\n<p>World</p>\",\n        \"<p>Hello</p>\\n\\n<p>World</p>\",\n    )\n    self.check_output(linebreaks, \"Hello\\n\\n\", \"<p>Hello</p>\\n\\n\")\n    self.check_output(linebreaks, \"Hello\\n\\nWorld\", \"<p>Hello</p>\\n\\n<p>World</p>\")\n", "    def test_json_script(self):\n        value = {'key': 'value'}\n        expected_output = '<script type=\"application/json\">{\"key\": \"value\"}</script>'\n        self.check_output(json_script, value, expected_output)\n", "    def test_escapejs__empty_string(self):\n        self.check_output(escapejs, \"\")\n", "def test_conditional_escape_with_preescaped_string(self):\n    # Test conditional_escape() with a pre-escaped string.\n    value = conditional_escape(\"<p>Hello, World!</p>\")\n    self.assertEqual(value, \"<p>Hello, World!</p>\")\n", "    def test_html_safe_decorator(self):\n        @html_safe\n        class MyClass:\n                self.value = value\n\n                return mark_safe(\"<b>{}</b>\".format(self.value))\n\n        self.assertTrue(conditional_escape(MyClass(\"hello\").value) == \"<b>hello</b>\")\n        self.assertTrue(conditional_escape(str(MyClass(\"hello\").value)) == \"<b>hello</b>\")\n"], "sample_850": ["def test_Nystroem_kernel():\n    # test rbf kernel\n    nystroem_rbf = Nystroem(kernel=\"rbf\", gamma=0.2, n_components=300,\n                             random_state=1)\n    data_transformed = nystroem_rbf.fit_transform(X)\n    assert_array_almost_equal(np.dot(data_transformed.T, data_transformed),\n                             pairwise_kernels(X, metric=\"rbf\"))\n\n    # test laplacian kernel\n    nystroem_laplacian = Nystroem(kernel=\"laplacian\", gamma=0.2,\n                                   n_components=300, random_state=1)\n    data_transformed = nystroem_laplacian.fit_transform(X)\n    assert_array_almost_equal(np.dot(data_transformed.T, data_transformed),\n                             pairwise_kernels(X, metric=\"laplacian\"))\n\n    # test polynomial kernel\n    nystroem_polynomial = Nystroem(kernel=\"poly\", degree=3, gamma=0.2,\n                                   n_components=300, random_state=1)\n    data_transformed = nystroem_polynomial.fit_transform(X)\n    assert_array_almost_equal(np.dot(data_transformed.T, data_transformed),\n                             pairwise_kernels(X, metric=\"poly\", degree=3))\n\n    # test sigmoid kernel\n    nystroem_sigmoid = Nystroem(kernel=\"sigmoid\", gamma=0.2, coef0=1.,\n                                 n_components=300, random_state=1)\n    data_transformed = nystroem_sigmoid.fit_transform(X)\n    assert_array_almost_equal(np.dot(data_transformed.T, data_transformed),\n                             pairwise_kernels(X, metric=\"sigmoid\"))\n\n    # test chi2 kernel\n    nystroem_chi2 = Nystroem(kernel=\"chi2\", gamma=0.2, n_components=300,\n                              random_state=1)\n    data_transformed = nystroem_chi2.fit_transform(X)\n    assert_array_almost_equal(np.dot(data_transformed.T, data_transformed),\n                             pairwise_kernels(X, metric=\"chi2\"))\n\n    # test precomputed kernel\n    nystroem_precomputed = Nystroem(kernel=\"precomputed\",\n                                    n_components=300, random_state=1)\n    K = pairwise_kernels(X, X, metric=\"rbf\")\n    data_transformed = nystroem_precomputed.fit_transform(X)\n    assert_array_almost_equal(np.dot", "def test_Nystroem_transform():\n    \"\"\"Test Nystroem transform\"\"\"\n    # Test with simple kernel\n    sampler = Nystroem(kernel=\"rbf\", gamma=0.5, n_components=10)\n    sampler.fit(X)\n    X_new = sampler.transform(X)\n    assert X_new.shape == (X.shape[0], sampler.n_components)\n\n    # Test with precomputed kernel\n    X_new = pairwise_kernels(X, X, metric=\"rbf\", gamma=0.5)\n    sampler = Nystroem(kernel=\"rbf\", gamma=0.5, n_components=10)\n    sampler.fit(X)\n    X_new_approx = sampler.transform(X)\n    assert_array_almost_equal(X_new, X_new_approx)\n\n    # Test with custom kernel\n        return np.dot(X, Y.T)\n    sampler = Nystroem(kernel=my_kernel, n_components=10)\n    sampler.fit(X)\n    X_new_approx = sampler.transform(X)\n    assert X_new_approx.shape == (X.shape[0], sampler.n_components)\n\n    # Test with invalid kernel\n    with pytest.raises(ValueError):\n        sampler = Nystroem(kernel=\"invalid_kernel\", n_components=10)\n        sampler.fit(X)\n", "def test_Nystroem_transform_shapes():\n    # create test data\n    X = np.random.rand(10, 5)\n\n    # test with default parameters\n    nystroem = Nystroem(kernel=\"rbf\", n_components=1)\n    nystroem.fit(X)\n    X_transformed = nystroem.transform(X)\n    assert X_transformed.shape == (10, 1)\n\n    # test with custom kernel\n    nystroem = Nystroem(kernel=\"linear\", gamma=1, n_components=2)\n    nystroem.fit(X)\n    X_transformed = nystroem.transform(X)\n    assert X_transformed.shape == (10, 2)\n\n    # test with sparse data\n    X_sparse = csr_matrix(np.random.rand(10, 5))\n    nystroem = Nystroem(kernel=\"rbf\", n_components=1)\n    nystroem.fit(X_sparse.toarray())\n    X_transformed_sparse = nystroem.transform(X_sparse)\n    assert X_transformed_sparse.shape == (10, 1)\n\n    # test with gamma=None\n    nystroem = Nystroem(kernel=\"rbf\", n_components=1)\n    nystroem.fit(X)\n    X_transformed = nystroem.transform(X)\n    assert X_transformed.shape == (10, 1)\n\n    # test with invalid n_components\n    nystroem = Nystroem(kernel=\"rbf\", n_components=1000)\n    with assert_raises(ValueError):\n        nystroem.fit(X)\n\n    # test with invalid kernel\n    nystroem = Nystroem(kernel=\"invalid_kernel\")\n    with assert_raises(ValueError):\n        nystroem.fit(X)\n", "def test_SkewedChi2Sampler_values_equal_to_skewedness():\n    sampler = SkewedChi2Sampler(skewedness=1., n_components=10, random_state=0)\n    X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n    with pytest.raises(ValueError):\n        sampler.fit_transform(X)\n", "def test_SkewedChi2Sampler_different_skewedness():\n    # SkewedChi2Sampler has different behavior with different skewedness\n    sampler = SkewedChi2Sampler(skewedness=2., n_components=10, random_state=0)\n    X = [[0.1, 0.2], [0.7, 0.3]]\n    with assert_warns(UserWarning):\n        sampler.fit_transform(X)\n    with assert_warns(UserWarning):\n        sampler.transform(X)\n    sampler.skewedness = 0.1\n    with pytest.raises(ValueError):\n        sampler.transform(X)\n\n"], "sample_361": ["def test_urlize(self):\n    # Test empty string\n    self.assertEqual(urlize(''), '')\n    \n    # Test single word\n    self.assertEqual(urlize('hello'), 'hello')\n    \n    # Test single URL\n    self.assertEqual(urlize('http://example.com'), '<a href=\"http://example.com\">http://example.com</a>')\n    \n    # Test single URL with trailing punctuation\n    self.assertEqual(urlize('http://example.com.'), '<a href=\"http://example.com.\">http://example.com.</a>')\n    \n    # Test single URL with leading punctuation\n    self.assertEqual(urlize('(http://example.com)'), '<a href=\"http://example.com\">(http://example.com)</a>')\n    \n    # Test single URL with wrapping punctuation\n    self.assertEqual(urlize('[http://example.com]'), '<a href=\"http://example.com\">[http://example.com]</a>')\n    \n    # Test single URL with email\n    self.assertEqual(urlize('user@example.com'), '<a href=\"mailto:user@example.com\">user@example.com</a>')\n    \n    # Test multiple URLs\n    self.assertEqual(urlize('http://example1.com http://example2.com'), '<a href=\"http://example1.com\">http://example1.com</a> <a href=\"http://example2.com\">http://example2.com</a>')\n    \n    # Test multiple URLs with trailing punctuation\n    self.assertEqual(urlize('http://example1.com. http://example2.com.'), '<a href=\"http://example1.com.\">http://example1.com.</a> <a href=\"http://example2.com.\">http://example2.com.</a>')\n    \n    # Test multiple URLs with leading punctuation\n    self.assertEqual(urlize('(http://example1.com) (http://example2.com)'), '<a href=\"http://example1.com\">(http://example1.com)</a> <a href=\"http://example2.com\">(http://example2.com)</a>')\n    \n    # Test multiple URLs with wrapping punctuation\n    self.assertEqual(urlize('[http://example1.com] [http://example2.com]'), '<a href=\"http://example1.com\">[http://example1.com]</a> <a href=\"http://example2.com\">[http://example2.com]</a>')\n", "def test_html_safe_decorator(self):\n    class TestClass:\n            return 'Hello, world!'\n\n    @html_safe\n    class SafeTestClass:\n            return 'Hello, world!'\n\n    class WithHtmlSafeDecorator:\n        @html_safe\n            return 'Hello, world!'\n\n    self.check_output(lambda x: isinstance(x.__html__, callable), TestClass(), False)\n    self.check_output(lambda x: isinstance(x.__html__, callable), SafeTestClass(), True)\n    self.check_output(lambda x: isinstance(x.__html__, callable), WithHtmlSafeDecorator(), True)\n\n    class WithStrButNoHtmlSafeDecorator:\n            return 'Hello, world!'\n\n    class WithHtmlButNoStrMethod:\n        @html_safe\n            return 'Hello, world!'\n\n    with self.assertRaises(ValueError):\n        @html_safe\n        class WithHtmlButNoStrMethodWithoutHtmlSafeDecorator:\n                return 'Hello, world!'\n\n    with self.assertRaises(ValueError):\n        @html_safe\n        class WithStrButNoHtmlSafeDecorator:\n            pass\n", "def test_linebreaks(self):\n    self.check_output(linebreaks, value=\"Hello\\n\\nWorld\", output=\"<p>Hello</p><p>World</p>\")\n    self.check_output(linebreaks, value=\"Hello\\nWorld\", output=\"<p>Hello World</p>\")\n    self.check_output(linebreaks, value=\"Hello\\nWorld\\n\", output=\"<p>Hello World</p>\")\n    self.check_output(linebreaks, value=\"Hello World\", output=\"<p>Hello World</p>\")\n    self.check_output(linebreaks, value=\"\", output=\"\")\n    self.check_output(linebreaks, value=\"<p>Hello World</p>\", output=\"<p>Hello World</p>\")\n    self.check_output(linebreaks, value=\"<p>Hello</p><p>World</p>\", output=\"<p>Hello</p><p>World</p>\")\n", "def test_linebreaks_autoescape(self):\n    self.check_output(linebreaks, 'Hello\\nWorld', '<p>Hello</p><p>World</p>')\n    self.check_output(linebreaks, 'Hello\\n\\nWorld', '<p>Hello</p><p>World</p>')\n    self.check_output(linebreaks, 'Hello\\n\\n\\nWorld', '<p>Hello</p><p>World</p>')\n    self.check_output(linebreaks, 'Hello\\n\\n\\n\\nWorld', '<p>Hello</p><p>World</p>')\n", "    def check_output(self, function, value, output=None):\n        \"\"\"\n        function(value) equals output. If output is None, function(value)\n        equals value.\n        \"\"\"\n        if output is None:\n            output = value\n        self.assertEqual(function(value), output)\n"], "sample_851": ["def test_mean_squared_log_error_negative_values():\n    y_true = [-1, -2, -3, -4]\n    y_pred = [-1, -2, -3, -5]\n    with pytest.warns(UndefinedMetricWarning):\n        with pytest.raises(ValueError):\n            mean_squared_log_error(y_true, y_pred)\n", "def test_mean_poisson_deviance_negative_y_pred():\n    y_true = [2, 0, 1, 4]\n    y_pred = [-0.5, -0.5, 2., 2.]\n    with pytest.warns(UserWarning) as record:\n        mean_poisson_deviance(y_true, y_pred)\n    assert len(record) == 1\n    assert \"Poisson deviance error with strictly positive y_pred\" in str(record[0].message)\n", "def test_mean_tweedie_deviance_power_validation():\n    # Test power parameter validation\n    # Test power less than 0\n    with pytest.warns(UserWarning):\n        mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=-1)\n    # Test power greater than 3\n    with pytest.warns(UserWarning):\n        mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=4)\n    # Test power not an integer\n    with pytest.warns(UserWarning):\n        mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=2.5)\n    # Test power not within the range\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=0.5)\n    # Test power in allowed range\n    assert isinstance(mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=1), float)\n    assert isinstance(mean_tweedie_deviance(y_true=[2, 0, 1, 4], y_pred=[0.5, 0.5, 2., 2.], power=2), float)\n", "def test_max_error_unsupported_output_type():\n    y_true = [3, 2, 7, 1]\n    y_pred = [4, 2, 7, 1]\n    with pytest.raises(ValueError):\n        max_error(y_true, y_pred, multioutput=True)\n", "def test_mean_squared_error_squared():\n    \"\"\"Test mean squared error squared with a custom dtype\"\"\"\n    y_true = np.array([[1, 2, 3, 4]])\n    y_pred = np.array([[1, 2, 3, 4]])\n    sample_weight = np.array([1, 1, 1, 1])\n    with pytest.warns(UndefinedMetricWarning):\n        assert_allclose(mean_squared_error(y_true, y_pred, sample_weight, squared=True, dtype=np.float32), 0.0)\n    assert_allclose(mean_squared_error(y_true, y_pred, sample_weight, squared=False, dtype=np.float32), 0.0)\n"], "sample_123": ["    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('not a date')\n", "    def test_urlencode_none_values_in_doseq(self):\n        # Test urlencode() behavior when doseq is True\n        query = {'a': [None, 'foo', 'bar']}\n        with self.assertRaises(TypeError) as cm:\n            urlencode(query, doseq=True)\n        self.assertEqual(str(cm.exception), self.cannot_encode_none_msg)\n", "    def test_urlencode_empty_dict(self):\n        data = {}\n        expected = ''\n        self.assertEqual(urlencode(data), expected)\n", "    def test_empty_url(self):\n        self.assertFalse(url_has_allowed_host_and_scheme(''))\n", "    def test_parse_http_date_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('Invalid date')\n"], "sample_393": ["    def test_check_programs(self):\n        # Test that check_programs raises CommandError when a required program is missing\n        with self.assertRaises(CommandError):\n            with captured_stderr() as stderr:\n                check_programs(\"non-existent-program\")\n                self.assertIn(\"Can't find non-existent-program\", stderr.value)\n        # Test that check_programs raises CommandError when multiple required programs are missing\n        with self.assertRaises(CommandError):\n            with captured_stderr() as stderr:\n                check_programs(\"non-existent-program1\", \"non-existent-program2\")\n                self.assertIn(\"Can't find non-existent-program1\", stderr.value)\n                self.assertIn(\"Can't find non-existent-program2\", stderr.value)\n        # Test that check_programs doesn't raise anything when all required programs are present\n        with captured_stderr() as stderr:\n            check_programs(\"xgettext\", \"msgmerge\", \"msgattrib\")\n            self.assertEqual(stderr.value, \"\")\n", "    def testLocationCommentGeneratedWithFullPath(self):\n        out, po_contents = self._run_makemessages(\n            domain='djangojs', no_location=False, verbosity=2\n        )\n        self.assertLocationCommentPresent('djangojs.po', None, 'dirA', 'dirB', 'foo.js')\n", "    def test_build_potfiles(self):\n        output, _ = self._run_makemessages()\n        self.assertIn(\"examining files with the extensions: html,txt,py\", output)\n\n        with tempfile.NamedTemporaryDirectory() as tmpdir:\n            tmpdir_path = Path(tmpdir)\n            src_path = tmpdir_path / \"src\"\n            src_path.mkdir()\n            (tmpdir_path / \"locale\" / \"de\" / \"LC_MESSAGES\").mkdir(parents=True, exist_ok=True)\n            (tmpdir_path / \"locale\" / \"de\" / \"LC_MESSAGES\" / \"django.po\").touch()\n            self.remove_po_files()\n\n            with open(src_path / \"file1.py\", \"w\", encoding=\"utf-8\") as f:\n                f.write(\"msgid 'hello'\")\n            with open(src_path / \"file2.txt\", \"w\", encoding=\"utf-8\") as f:\n                f.write(\"msgid 'world'\")\n            with open(src_path / \"file3.html\", \"w\", encoding=\"utf-8\") as f:\n                f.write('msgid \"hello\"')\n\n            self._run_makemessages(verbosity=2)\n\n            potfile = tmpdir_path / \"locale\" / \"de\" / \"LC_MESSAGES\" / \"django.pot\"\n            self.assertTrue(potfile.exists())\n            self.assertIn(\"hello\", potfile.read_text(encoding=\"utf-8\"))\n            self.assertIn(\"world\", potfile.read_text(encoding=\"utf-8\"))\n", "    def test_djangojs_extractor(self):\n        # Test that the extractor for djangojs domain works correctly\n        self._run_makemessages(domain=\"djangojs\")\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n            self.assertMsgIdPlural(\"foo\", po_contents)\n            self.assertMsgStr(' translations for \"foo\" and \"bar\"', po_contents)\n            self.assertNotMsgId(\"foo\", po_contents)\n", "    def test_process_files_passes_through_when_no_translatable_files(self):\n        with self.assertDoesNotRaise(CommandError):\n            self._run_makemessages(all=True)\n            with open(self.PO_FILE) as fp:\n                po_contents = fp.read()\n            self.assertNotIn(\"msgid\", po_contents)\n            self.assertNotIn(\"msgstr\", po_contents)\n"], "sample_722": ["def test_mini_batch_kmeans_max_iter():\n    # Test that max_iter is correctly used in MiniBatchKMeans\n    X, y = make_blobs(n_samples=100, centers=2, n_features=2,\n                       cluster_std=1.0, random_state=0)\n    mbk = MiniBatchKMeans(n_clusters=2, max_iter=1, batch_size=10)\n    mbk.fit(X)\n    assert mbk.n_iter_ == 1\n\n    mbk = MiniBatchKMeans(n_clusters=2, max_iter=10, batch_size=10)\n    mbk.fit(X)\n    assert mbk.n_iter_ <= 10\n\n    mbk = MiniBatchKMeans(n_clusters=2, max_iter=100, batch_size=10)\n    mbk.fit(X)\n    assert mbk.n_iter_ <= 10\n", "    def test_incremental_init(self):\n        X, _ = make_blobs(n_samples=100, centers=[[1, 2], [4, 2], [4, 4]], cluster_std=0.1, random_state=0)\n        km = KMeans(n_clusters=3, init='k-means++', n_init=1)\n        km.fit(X)\n        km2 = KMeans(n_clusters=3, init='k-means++', n_init=1)\n        km2.partial_fit(X)\n        assert_array_equal(km.cluster_centers_, km2.cluster_centers_)\n", "def test_partial_fit_single_point():\n    n_clusters = 2\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    km = MiniBatchKMeans(n_clusters=n_clusters)\n    km.partial_fit(X[:1])\n    assert_equal(km.cluster_centers_.shape, (n_clusters, X.shape[1]))\n    assert_equal(km.counts_.shape, (n_clusters,))\n    assert_equal(km.labels_.shape, (1,))\n", "def test_kmeans_convergence_warning():\n    kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=100,\n                    tol=1e-7, verbose=0, n_init=10)\n    X = np.array([[1, 2], [1, 3], [1, 4]])\n    kmeans.fit(X)\n    # should have converged (should not raise a warning)\n    with assert_warns_message(\n            ConvergenceWarning,\n            r\"Number of distinct clusters \\(.*\\) found smaller than n_clusters \\(.*\\).*\",\n            kmeans.partial_fit, X):\n        # This partial_fit call will raise a warning\n        kmeans.partial_fit(X)\n", "    def test_partial_fit_empty_data(self):\n        # Test that partial_fit with empty data raises a ValueError\n        X = []\n        mbk = MiniBatchKMeans(n_clusters=5)\n        assert_raises(ValueError, mbk.partial_fit, X)\n"], "sample_846": ["def test_fit_transform_remainder_transformer():\n    \"\"\"Test the fit_transform with a remainder transformer.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    X_remainder = np.array([[0], [0]])\n    transformer = make_column_transformer(\n        ('remainder', TransNo2D(), slice(0, 2)),\n        ('remainder', TransNo2D(), slice(2, 4)))\n    transformer.fit(X_remainder)\n    result = transformer.fit_transform(X_remainder)\n    assert_array_equal(result, X_remainder)\n", "def test_ColumnTransformer_sparse_output():\n    # Check that sparse output is correctly set when the output of all\n    # transformers is sparse and the threshold is below the sparse output\n    # density\n    X = np.array([[1, 2], [3, 4]])\n    transformers = [('sparse', SparseMatrixTrans(), [0]), ('dense', TransNo2D(), [1])]\n    ct = ColumnTransformer(transformers, sparse_threshold=0.2)\n    X_sparse = sparse.csr_matrix(X)\n    result = ct.fit_transform(X_sparse)\n    assert result.dtype == np.float64\n\n    # Check that sparse output is correctly set when the output of all\n    # transformers is dense and the threshold is above the sparse output\n    # density\n    X = np.array([[1, 2], [3, 4]])\n    transformers = [('dense', TransNo2D(), [0]), ('dense', TransNo2D(), [1])]\n    ct = ColumnTransformer(transformers, sparse_threshold=0.8)\n    result = ct.fit_transform(X)\n    assert not sparse.issparse(result)\n\n    # Check that sparse output is correctly set when the output of some\n    # transformers is sparse and the threshold is below the sparse output\n    # density\n    X = np.array([[1, 2], [3, 4]])\n    transformers = [('sparse', SparseMatrixTrans(), [0]), ('dense', TransNo2D(), [1])]\n    ct = ColumnTransformer(transformers, sparse_threshold=0.2)\n    X_sparse = sparse.csr_matrix(X)\n    result = ct.fit_transform(X_sparse)\n    assert sparse.issparse(result)\n\n    # Check that sparse output is correctly set when the output of some\n    # transformers is dense and the threshold is above the sparse output\n    # density\n    X = np.array([[1, 2], [3, 4]])\n    transformers = [('dense', TransNo2D(), [0]), ('dense', TransNo2D(), [1])]\n    ct = ColumnTransformer(transformers, sparse_threshold=0.8)\n    result = ct.fit_transform(X)\n    assert not sparse.issparse(result)\n\n    # Check that sparse output is correctly set when the output of all\n    # transformers is sparse and the threshold is exactly equal to the sparse\n    # output density\n    X = np.array([[1, 2], [3, 4]])\n    transformers =", "def test_column_transformer_sparse_threshold():\n    # Test that column transformer returns dense output when there is a mix of sparse\n    # and dense data and sparse_threshold is set to 0.\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    trans = make_column_transformer(\n        (StandardScaler(), 'sparse_col'),\n        (OneHotEncoder(), 'dense_col'),\n    )\n    trans.sparse_threshold = 0.0\n    trans.fit_transform(X, X)\n\n    assert not sparse.isspmatrix(trans.transform(X))\n\n    # Test that column transformer returns sparse output when there is a mix of sparse\n    # and dense data and sparse_threshold is set to the default value.\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    trans = make_column_transformer(\n        (StandardScaler(), 'sparse_col'),\n        (OneHotEncoder(), 'dense_col'),\n    )\n    trans.sparse_threshold = 0.3\n    trans.fit_transform(X, X)\n\n    assert sparse.isspmatrix(trans.transform(X))\n\n    # Test that column transformer returns dense output when all data is sparse.\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    trans = make_column_transformer(\n        (StandardScaler(), 'sparse_col'),\n    )\n    trans.sparse_threshold = 0.0\n    trans.fit_transform(X, X)\n\n    assert not sparse.isspmatrix(trans.transform(X))\n\n    # Test that column transformer returns sparse output when all data is sparse.\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    trans = make_column_transformer(\n        (StandardScaler(), 'sparse_col'),\n    )\n    trans.sparse_threshold = 0.3\n    trans.fit_transform(X, X)\n\n    assert sparse.isspmatrix(trans.transform(X))\n\n    # Test that column transformer returns sparse output when all data is dense.\n    X = np.array([[1, 2], [3, 4]])\n    trans = make_column_transformer(\n        (StandardScaler(), 'dense_col'),\n    )\n    trans.sparse_threshold = 0.3\n    trans.fit_transform(X, X)\n\n    assert not sparse.isspmatrix(trans.transform(X))\n", "def test_sparse_threshold():\n    # check that when all transformers output sparse matrices,\n    # the output is sparse\n    X = np.arange(10).reshape(-1, 1)\n    transformer = make_column_transformer((SparseMatrixTrans(), [0]),\n                                         n_jobs=-1)\n    transformer.fit_transform(X)\n    assert sparse.isspmatrix(transformer.transform(X))\n\n    # check that when all transformers output dense matrices,\n    # the output is dense\n    X = np.arange(10).reshape(-1, 1)\n    transformer = make_column_transformer((DoubleTrans(), [0]),\n                                         n_jobs=-1)\n    assert not sparse.isspmatrix(transformer.transform(X))\n\n    # check that when the sparse threshold is reached, the output is sparse\n    X = np.arange(10).reshape(-1, 1)\n    transformer = make_column_transformer((SparseMatrixTrans(), [0]),\n                                         (DoubleTrans(), [0]),\n                                         n_jobs=-1, sparse_threshold=0.1)\n    assert sparse.isspmatrix(transformer.transform(X))\n    assert_almost_equal(transformer.transform(X).toarray().sum(),\n                       1 * 2 * 10)  # sum of all non-zero elements\n\n    # check that the output is dense when no sparse threshold is reached\n    X = np.arange(10).reshape(-1, 1)\n    transformer = make_column_transformer((SparseMatrixTrans(), [0]),\n                                         (DoubleTrans(), [0]),\n                                         n_jobs=-1, sparse_threshold=0.9)\n    assert not sparse.isspmatrix(transformer.transform(X))\n    assert_almost_equal(transformer.transform(X).sum(),\n                       1 * 2 * 10)  # sum of all elements\n\n    # check that the output is sparse when the transformed output consists\n    # of all sparse data\n    X = sparse.csr_matrix(np.arange(10).reshape(-1, 1))\n    transformer = make_column_transformer((SparseMatrixTrans(), [0]),\n                                         n_jobs=-1, sparse_threshold=0.1)\n    assert sparse.isspmatrix(transformer.transform(X))\n\n    # check that the output is dense when the transformed output consists\n    # of all dense data\n    X = np.arange(10).reshape(-1, 1)\n    transformer = make_column_transformer((DoubleTrans(), [0]),\n                                         n_jobs=-1, sparse_threshold=0.1)\n    assert not sparse", "def test_ColumnTransformer_fit_transform_non_sparse_non_pass_through():\n    # Test non-sparse input and non 'passthrough' output\n    X = np.array([[1., 2., 3., 4., 5.],\n                  [6., 7., 8., 9., 10.]])\n    y = np.array([1, 2])\n    ct = make_column_transformer((('standardscaler', StandardScaler()), [0, 1, 3]), remainder='drop')\n    ct.fit(X, y)\n    X_transformed = ct.transform(X)\n    assert_array_equal(X_transformed.shape, (2, 3))\n    assert_array_equal(X_transformed[:, 0], np.array([0.57735029, -0.81649658]))\n    assert_array_equal(X_transformed[:, 1], np.array([0., 0.]))\n    assert_array_equal(X_transformed[:, 2], np.array([0.81649658, -0.57735029]))\n"], "sample_12": ["def test_latitude_comparison_with_angle():\n    latitude1 = Latitude('10d')\n    latitude2 = Latitude('10d')\n    assert latitude1 == latitude2\n\n    latitude3 = Latitude('15d')\n    assert not latitude1 == latitude3\n\n    latitude4 = Angle('10d')\n    assert latitude1 != latitude4\n", "def test_Latitude_invalid_unit():\n    with pytest.warns(UserWarning):\n        Latitude('10.2345', unit='m')\n", "    def test_init_with_angle_and_unit(self):\n        angle = Angle(10 * u.deg, unit=u.deg)\n        assert angle.value == 10\n        assert angle.unit == u.deg\n", "def test_longitude_wrap_at_sets_valid_angles():\n    \"\"\"Test Longitude's wrap_at method with valid angles.\"\"\"\n    lon = Longitude([350 * u.deg, 180 * u.deg, -10 * u.deg])\n    lon.wrap_at(180 * u.deg)\n    assert_allclose(lon.value, np.array([10., 180., -10.]) * u.deg)\n", "def test_Latitude__default_wrap_angle():\n    lat = Latitude(180 * u.deg)\n    assert np.allclose(lat.wrap_angle.to_value(u.deg), 360.0)\n    assert np.allclose(lat.value, 180.0)\n    assert np.allclose(lat.wrap_angle.value, 360.0)\n"], "sample_179": ["    def test_index_together_field_list_clashes(self):\n        class InvalidModel(models.Model):\n            class Meta:\n                index_together = [('field1', 'field2'), 'field3']\n\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n            field3 = models.CharField(max_length=255)\n\n        with self.assertRaisesRegex(Error, 'All \"index_together\" elements must be lists or tuples.'):\n            InvalidModel.check()\n", "    def test_unique_together_check(self):\n        class Model(models.Model):\n            class Meta:\n                unique_together = [('field1', 'field2')]\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'models.E010')\n", "    def test_index_together_fails_on_duplicate_fields(self):\n        from .invalid_models import IndexTogetherModel\n        with self.assertRaises(Error) as e:\n            IndexTogetherTests().check(IndexTogetherModel)\n        self.assertEqual(len(e.exception.list), 1)\n        self.assertEqual(str(e.exception.list[0]), \"'index_together' refers to the nonexistent field, related field, or lookup 'field1, field2'.\")\n", "    def test_max_column_name_length(self):\n        with override_settings(\n            DATABASES={\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                },\n                'other': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                },\n            }\n        ):\n            with self.assertRaises(ValueError):\n                models.Model.objects.create()\n", "    def test_indexes_columns(self):\n        # Check that index columns can be added as tuples with field names\n        # (from the local fields of the model) and expressions.\n        model = models.Model(\n            title=models.CharField(max_length=200),\n            body=models.TextField(),\n            created_at=models.DateTimeField(auto_now_add=True)\n        )\n\n        # Index on a field\n        class IndexWithField(models.Index):\n            fields = [('title',)]\n\n        model.add_to_class('my_index', IndexWithField())\n\n        # Index on an expression\n        class IndexWithExpression(models.Index):\n            fields = [('LOWER(body)',)]\n\n        model.add_to_class('my_expression_index', IndexWithExpression())\n\n        # Check that we can create such indexes in the database\n        self.assertTrue(model._meta.has_index('my_index'))\n        self.assertTrue(model._meta.has_index('my_expression_index'))\n"], "sample_640": ["def test_is_attribute_typed_annotation():\n    module = astroid.Module(\"module\")\n    class_node = astroid.ClassDef(\"MyClass\", module, [])\n    class_node.locals[\"my_attr\"] = astroid.AssignName(\"my_attr\")\n    class_node.locals[\"my_attr\"].parent = astroid.AnnAssign(\"my_attr\", class_node, \"typing.Any\")\n    assert utils.is_attribute_typed_annotation(class_node, \"my_attr\")\n", "def test_is_builtin(name, expected):\n    assert utils.is_builtin(name) == expected\n", "def test_is_attr_private():\n    \"\"\"Test is_attr_private.\"\"\"\n    assert utils.is_attr_private(\"__private__\") == \"____private__\"\n    assert utils.is_attr_private(\"public\") is None\n    assert utils.is_attr_private(\"__public__\") is None\n    assert utils.is_attr_private(\"_private\") is None\n    assert utils.is_attr_private(\"____private__\") is None\n    assert utils.is_attr_private(\"_public\") is None\n    assert utils.is_attr_private(\"_private_public\") is None\n    assert utils.is_attr_private(\"__private_public__\") is None\n", "def test_is_builtin(name, expected):\n    assert utils.is_builtin(name) == expected\n", "compilation error"], "sample_1065": ["def test_binomial_complex_argument():\n    n, k = symbols('n k', real=True)\n    # Test case for complex argument\n    expr = binomial(n, k)\n    assert expr.subs(n, 5+2*I) != 0\n    assert expr.subs(n, 5-2*I) != 0\n    assert expr.subs(k, 3+I) != 0\n    assert expr.subs(k, 3-2*I) != 0\n    # Raise error when both arguments are not real\n    assert raises(ArgumentIndexError, lambda: binomial(n, k).subs(n, 5+2*I).subs(k, 3+I))\n", "def test_factorial_evaluation():\n    n = Symbol('n')\n    assert factorial(n).diff(n) == (gamma(n + 1)*polygamma(0, n + 1))\n", "def test_binomial_even_k():\n    from sympy.abc import n, k\n    binomial_expr = binomial(n, k)\n    expanded_binomial = expand_func(binomial_expr)\n\n    # Check if the expanded binomial expression has the correct order of terms\n    # when k is even\n    expected_terms = sorted(expanded_binomial.as_ordered_terms(),\n                           key=lambda term: term.as_numer_denom()[0])\n    actual_terms = sorted(expanded_binomial.args,\n                         key=lambda arg: arg.as_numer_denom()[0])\n    assert (expected_terms == actual_terms), f\"Expected {expected_terms} but got {actual_terms}\"\n\n    # Check if the expanded binomial expression has the correct coefficients\n    # when k is even\n    expected_coeffs = [term.as_numer_denom()[0] for term in expected_terms]\n    actual_coeffs = [term.as_numer_denom()[0] for term in actual_terms]\n    assert (expected_coeffs == actual_coeffs), f\"Expected {expected_coeffs} but got {actual_coeffs}\"\n\n    # Check if the expanded binomial expression has the correct powers\n    # when k is even\n    expected_powers = [term.as_numer_denom()[1].as_base_exp()[0] for term in expected_terms]\n    actual_powers = [term.as_numer_denom()[1].as_base_exp()[0] for term in actual_terms]\n    assert (expected_powers == actual_powers), f\"Expected {expected_powers} but got {actual_powers}\"\n\n    # Check if the expanded binomial expression has the correct bases\n    # when k is even\n    expected_bases = [term.as_numer_denom()[1].as_base_exp()[1] for term in expected_terms]\n    actual_bases = [term.as_numer_denom()[1].as_base_exp()[1] for term in actual_terms]\n    assert (expected_bases == actual_bases), f\"Expected {expected_bases} but got {actual_bases}\"\n", "def test_subfactorial_convergence():\n    from sympy import limit, oo, Symbol, subfactorial\n    x = Symbol('x')\n    assert limit(subfactorial(x), x, oo) == S.Infinity\n", "    def test_subfactorial_equality(self):\n        x = Symbol('x')\n        assert subfactorial(x) == x.subs(x, subfactorial(x))\n        "], "sample_348": ["    def test_base_model_form(self):\n        class TestModel(Model):\n            name = models.CharField(max_length=100)\n\n        form = BaseModelForm(data={'name': 'test'}, instance=TestModel.objects.create(name='test'))\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['name'], 'test')\n        self.assertEqual(form.instance.pk, 1)\n        self.assertEqual(form.instance.name, 'test')\n\n        with self.assertRaises(ValueError):\n            BaseModelForm(data={'name': 'test'}, instance=None)\n\n        class InvalidModelForm(BaseModelForm):\n            class Meta:\n                model = TestModel\n\n        with self.assertRaises(ValueError):\n            InvalidModelForm(data={'name': 'test'})\n\n        form = InvalidModelForm(data={'name': 'test'}, instance=TestModel.objects.create(name='test'))\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['name'], 'test')\n        self.assertEqual(form.instance.pk, 1)\n        self.assertEqual(form.instance.name, 'test')\n", "    def test_modelchoicefield_required(self):\n        class ValidationTestModelForm(forms.ModelForm):\n            class Meta:\n                model = ValidationTestModel\n                fields = ('name', 'choice_field')\n                required_fields = ('name',)\n\n        form = ValidationTestModelForm(data={'name': 'test', 'choice_field': 'Invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('choice_field', form.errors)\n", "    def test_no_model_in_meta(self):\n        class TestModelForm(forms.ModelForm):\n            class Meta:\n                pass\n\n        model_admin = admin.ModelAdmin\n        self.assertIsInvalid(model_admin, TestModelForm, \"ModelForm has no model class specified.\")\n", "    def test_inline_formset_form_is_valid(self):\n        formset = BaseModelFormSet(data={'fields': 'text'}, queryset=Song.objects.all())\n        self.assertTrue(formset.is_valid())\n", "    def test_fields_for_model_returns_empty_dict_for_invalid_fields(self):\n        model = Model()\n        with self.assertRaises(FieldError):\n            fields_for_model(model, ['invalid_field'])\n"], "sample_1103": ["def test_pow_as_numer_denom():\n    from sympy.abc import x, y\n    assert same_and_same_prec((x**y).as_numer_denom()[0], x**y.as_numer_denom()[0])\n    assert same_and_same_prec((x**y).as_numer_denom()[1], x**y.as_numer_denom()[1])\n    assert (x**y).as_numer_denom()[0].is_Pow\n    assert (x**y).as_numer_denom()[1].is_Pow\n\n    assert same_and_same_prec((x**y).as_numer_denom()[0].as_numer_denom()[0], x**y.as_numer_denom()[0].as_numer_denom()[0])\n    assert same_and_same_prec((x**y).as_numer_denom()[0].as_numer_denom()[1], x**y.as_numer_denom()[0].as_numer_denom()[1])\n    assert (x**y).as_numer_denom()[0].as_numer_denom()[0].is_Pow\n    assert (x**y).as_numer_denom()[0].as_numer_denom()[1].is_Pow\n\n    assert (x**y).as_numer_denom()[1].as_numer_denom()[0].is_Pow\n    assert (x**y).as_numer_denom()[1].as_numer_denom()[1].is_Pow\n\n    assert same_and_same_prec((x**y).as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[0], x**y.as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[0])\n    assert same_and_same_prec((x**y).as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[1], x**y.as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[1])\n    assert (x**y).as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[0].is_Pow\n    assert (x**y).as_numer_denom()[1].as_numer_denom()[0].as_numer_denom()[1].is_Pow\n\n    assert (x**y).as_numer_denom()[1].as_numer_denom()[1].as_numer_denom()[0].is_Pow\n", "    def test_pow_integer(self):\n        self.assertEqual(Pow(2, 3).args, (2, 3))\n        self.assertEqual(Pow(2, -3).args, (2, -3))\n        self.assertEqual(Pow(-2, 3).args, (-2, 3))\n        self.assertEqual(Pow(-2, -3).args, (-2, -3))\n", "def test_pow_mixed_expression():\n    x, y = symbols('x,y')\n    assert Pow(x, y).is_Mul is True\n    assert Pow(1, x).is_Mul is True\n    assert Pow(x, -1).is_Mul is True\n    assert Pow(x, 0).is_Mul is False\n    assert Pow(x, 1).is_Mul is False\n", "def test_pow_as_base_exp_unsympified():\n    b, e = 3, -4\n    p = Pow(b, e)\n    sympified_p = Pow(sympify(b), sympify(e))\n    assert p.as_base_exp() == sympified_p.as_base_exp()\n\n", "def test_pow_special_cases():\n    from sympy import Pow, oo, zoo, S, re, pi, sqrt, Rational, im\n    from sympy.core.expr import unchanged\n    from sympy.core.numbers import One, Zero\n\n    # Test Pow(zero, zero) and Pow(zero, infinity) and Pow(infinity, zero)\n    assert unchanged(Pow(0, 0)) == One\n    assert unchanged(Pow(0, oo)) == zoo\n    assert unchanged(Pow(oo, 0)) == One\n\n    # Test Pow(zero, -zero) and Pow(-zero, -zero) and Pow(zero, infinity) and Pow(infinity, -zero)\n    assert unchanged(Pow(0, -0)) == One\n    assert unchanged(Pow(-0, -0)) == One\n    assert unchanged(Pow(0, oo)) == zoo\n    assert unchanged(Pow(-oo, -0)) == zoo\n\n    # Test Pow(0, even) and Pow(-0, even)\n    assert unchanged(Pow(0, 2)) == 0\n    assert unchanged(Pow(-0, 2)) == 0\n\n    # Test Pow(0, odd) and Pow(-0, odd)\n    assert unchanged(Pow(0, 3)) == 0\n    assert unchanged(Pow(-0, 3)) == 0\n\n    # Test Pow(1, negative infinity)\n    assert unchanged(Pow(1, -oo)) == One\n\n    # Test Pow(infinity, negative infinity)\n    assert unchanged(Pow(oo, -oo)) == zoo\n\n    # Test Pow(infinity, infinity)\n    assert unchanged(Pow(oo, oo)) == oo\n\n    # Test Pow(zero, infinity) and Pow(infinity, zero)\n    assert unchanged(Pow(0, oo)) == zoo\n    assert unchanged(Pow(oo, 0)) == One\n\n    # Test Pow(negative infinity, even)\n    assert unchanged(Pow(-oo, 2)) == 0\n\n    # Test Pow(negative infinity, odd)\n    assert unchanged(Pow(-oo, 3)) == 0\n\n    # Test Pow(negative infinity, odd and negative)\n    assert unchanged(Pow(-oo, -3)) == zoo\n\n    # Test Pow(negative infinity, even and negative)\n    assert unchanged(Pow(-oo, -2)) == zoo\n\n"], "sample_293": ["    def test_reverse_with_language_prefix(self):\n        urlconf = get_resolver('test_urlconf')\n        response = urlconf.reverse('nested_view', language_code='fr')\n        self.assertEqual(response, '/fr/nested/')\n", "    def test_resolve_with_nested_app_and_namespace(self):\n        # Test that we can resolve a URL with a nested app and namespace\n        resolver = get_resolver(urlconf='urlpatterns_reverse.urlconf_nested')\n        resolved = resolver.resolve('/nested_app/inner_app/normal/42/37/')\n        self.assertEqual(resolved.func, views.empty_view)\n        self.assertEqual(resolved.args, ())\n        self.assertEqual(resolved.kwargs, {'arg1': '42', 'arg2': '37'})\n        self.assertEqual(resolved.url_name, 'nested-app:inner-app:normal-view')\n        self.assertEqual(resolved.route, 'nested_app:inner_app:normal/42/37/')\n        self.assertEqual(resolved.tried, [])\n", "    def test_resolve(self):\n        resolver = get_resolver()\n        with self.assertRaises(ImproperlyConfigured):\n            resolver.resolve('/non-existant-url/')\n", "    def test_nested_namespaces(self):\n        from django.urls import include, path\n        from .views import empty_view\n\n        # Test that nested namespaces are correctly resolved.\n        urlconf = [\n            path('test/', include('testapp.urls')),\n            path('test/', include('testapp.subapp.urls')),\n        ]\n        resolver = get_resolver(urlconf)\n\n        self.assertEqual(\n            resolver.resolve('/test/test2/inner/42/37/'),\n            ResolverMatch(\n                empty_view,\n                (),\n                {'arg1': '42', 'arg2': '37'},\n                'test-ns3:testapp:urlobject-view',\n                ['testapp', 'testapp.subapp'],\n                ['test-ns1', 'test-ns1:test-ns3'],\n                'test/test2/test3/inner/42/37/',\n                []\n            )\n        )\n\n        self.assertEqual(\n            resolver.resolve('/test/test3/inner/42/37/'),\n            ResolverMatch(\n                empty_view,\n                (),\n                {'arg1': '42', 'arg2': '37'},\n                'test-ns3:urlobject-view',\n                ['testapp'],\n                ['test-ns1', 'test-ns1:test-ns3'],\n                'test/test3/inner/42/37/',\n                []\n            )\n        )\n\n        self.assertEqual(\n            resolver.resolve('/test/test4/inner/42/37/'),\n            ResolverMatch(\n                empty_view,\n                (),\n                {'arg1': '42', 'arg2': '37'},\n                'test-ns2:urlobject-view',\n                ['testapp.subapp'],\n                ['test-ns1:test-ns2'],\n                'test/test4/inner/42/37/',\n                []\n            )\n        )\n", "    def test_cache_invalidation(self):\n        urlconf = 'urlpatterns_reverse.urlconf_outer'\n        resolver1 = get_resolver(urlconf)\n        self.assertEqual(resolver1, get_resolver(urlconf))\n        self.assertEqual(get_resolver.cache_info(), (0, 0))\n        get_resolver.cache_clear()\n        self.assertEqual(get_resolver.cache_info(), (0, 0))\n        resolver2 = get_resolver(urlconf)\n        self.assertNotEqual(resolver1, resolver2)\n        self.assertEqual(get_resolver.cache_info(), (0, 0))\n        self.assertEqual(resolver2, get_resolver(urlconf))\n        self.assertEqual(get_resolver.cache_info(), (1, 0))\n        self.assertEqual(resolver2, get_resolver(urlconf))\n        self.assertEqual(get_resolver.cache_info(), (1, 0))\n        resolver2._populate = lambda self: None  # invalidate cache\n        self.assertEqual(resolver2, get_resolver(urlconf))\n        self.assertEqual(get_resolver.cache_info(), (2, 0))\n        self.assertEqual(resolver2, get_resolver(urlconf))\n        self.assertEqual(get_resolver.cache_info(), (2, 0))\n"], "sample_479": ["    def test_add_index_on_field(self):\n        operations = [\n            migrations.RunPython(lambda x, y: None),\n            operations.AddIndex(\n                table=\"my_table\",\n                name=\"my_index\",\n                field=models.IntegerField(),\n            ),\n            migrations.RunPython(lambda x, y: None),\n        ]\n        expected = [\n            migrations.RunPython(lambda x, y: None),\n            operations.AddConstraint(\n                model_name=\"my_table\",\n                constraint=models.Index(fields=[\"my_field\"]),\n            ),\n            migrations.RunPython(lambda x, y: None),\n        ]\n        self.assertOptimizesTo(operations, expected, exact=1)\n", "    def test_rename_field_optimization(self):\n        # Test rename_field operation is reduced to create_field operation\n        operations = [\n            operations.RenameField(\n                model_name=\"test_model\",\n                old_name=\"old_field\",\n                new_name=\"new_field\",\n            )\n        ]\n        expected = [\n            operations.CreateField(\n                model_name=\"test_model\",\n                name=\"new_field\",\n                field=models.CharField(max_length=255),\n            )\n        ]\n        self.assertOptimizesTo(operations, expected, app_label=\"migrations\")\n", "    def test_optimize_add_index(self):\n        operation = operations.CreateModel(\n            name=\"Model\",\n            fields=[\n                (models.CharField(\"name\"), models.CharField(max_length=10)),\n            ],\n            options={},\n        )\n        expected = [\n            operations.CreateModel(\n                name=\"Model\",\n                fields=[\n                    (models.CharField(\"name\"), models.CharField(max_length=10)),\n                ],\n                options={},\n            ),\n            operations.AddIndex(\n                model_name=\"Model\",\n                index=models.Index(fields=[\"name\"], name=\"name_idx\"),\n            ),\n        ]\n        self.assertOptimizesTo([operation], expected)\n", "    def test_reduces_to_no_operation_when_model_does_not_exist(self):\n        operation = AddIndex(model_name=\"does_not_exist\", index=models.Index(fields=[\"field\"]))\n        self.assertEqual(operation.reduce(operations.CreateModel(\"does_not_exist\", []), \"migrations\"), [])\n", "    def test_rename_proxy_model(self):\n        # Create a migration that renames a model with a proxy\n        migration = operations.CreateModel(\n            \"app_name\",\n            [\n                (\"field1\", models.IntegerField()),\n                (\"field2\", models.CharField(max_length=255)),\n            ],\n            options={\"proxy\": True},\n        )\n        migration = operations.RenameModel(\"old_name\", \"new_name\")\n        operations = [migration]\n        result, iterations = self.optimize(operations, app_label=\"migrations\")\n\n        # Serialize the result\n        result = [self.serialize(f) for f in result]\n        expected = [\n            migrations.CreateModel(\n                name=\"new_name\",\n                fields=[\n                    (\"field1\", models.IntegerField()),\n                    (\"field2\", models.CharField(max_length=255)),\n                ],\n                options={\"proxy\": True},\n                bases=[\"django.db.models.Model\"],\n                managers=[],\n            )\n        ]\n        expected = [self.serialize(f) for f in expected]\n\n        # Assert the result is correct\n        self.assertEqual(expected, result)\n"], "sample_344": ["    def test_state_apps_clone(self):\n        apps = Apps()\n        apps.all_models = {\n            'app': {\n                'Model': models.Model\n            }\n        }\n        state_apps = StateApps([], apps.all_models, ignore_swappable=True)\n        cloned_state_apps = state_apps.clone()\n        self.assertIsNot(cloned_state_apps, state_apps)\n        self.assertIsNot(cloned_state_apps.all_models, state_apps.all_models)\n        self.assertIsNot(cloned_state_apps.app_configs, state_apps.app_configs)\n", "    def test_from_model_no_bases(self):\n        model = models.Model()\n        state = ModelState.from_model(model)\n        self.assertEqual(state.bases, (models.Model,))\n", "    def test_cloning(self):\n        model = ModelState(\n            'myapp', 'MyModel',\n            {\n                'field1': models.CharField(max_length=100),\n                'field2': models.CharField(max_length=100),\n            },\n            options={'indexes': [models.Index(fields=['field1'])]},\n            bases=(models.Model,),\n            managers=[('objects', models.Manager())],\n        )\n        original_model = model.clone()\n        self.assertEqual(model, original_model)\n", "    def test_model_state_fields(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        model_state = ModelState.from_model(MyModel)\n        self.assertEqual(model_state.fields, {'field1': model_state.get_field('field1'), 'field2': model_state.get_field('field2')})\n", "    def test_cloning(self):\n        state = ModelState(\n            app_label=\"test_app\",\n            name=\"TestModel\",\n            fields={\"id\": models.AutoField(primary_key=True)},\n            options={\"indexes\": [models.Index(fields=[\"id\"])]},\n            managers=[(\"objects\", models.Manager())],\n        )\n        cloned_state = state.clone()\n        self.assertEqual(state, cloned_state)\n"], "sample_389": ["    def test_get_host(self):\n        request = HttpRequest()\n        request.META = {\"HTTP_HOST\": \"example.com\"}\n        self.assertEqual(request.get_host(), \"example.com\")\n\n        request.META = {\"HTTP_HOST\": \"example.com:80\"}\n        self.assertEqual(request.get_host(), \"example.com\")\n\n        request.META = {\"HTTP_HOST\": \"example.com:443\"}\n        self.assertEqual(request.get_host(), \"example.com\")\n\n        request.META = {\"HTTP_HOST\": \"example.com:4433\"}\n        self.assertEqual(request.get_host(), \"example.com:4433\")\n\n        request.META = {\"HTTP_HOST\": \"invalid_host\"}\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n        request.META = {\"HTTP_HOST\": \"localhost\"}\n        self.assertEqual(request.get_host(), \"localhost\")\n\n        request.META = {\"HTTP_HOST\": \"127.0.0.1\"}\n        self.assertEqual(request.get_host(), \"127.0.0.1\")\n\n        request.META = {\"HTTP_HOST\": \"[::1]\"}\n        self.assertEqual(request.get_host(), \"[::1]\")\n\n        request.META = {\"HTTP_HOST\": \"example.com:80, secure\"}\n        self.assertEqual(request.scheme, \"https\")\n", "    def setUp(self):\n        self.request = HttpRequest()\n", "    def test_get_host(self):\n        request = HttpRequest()\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n        request.META = {\"HTTP_HOST\": \"invalid-host\"}\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n        request.META = {\"HTTP_HOST\": \"example.com\"}\n        request.settings = SimpleNamespace(ALLOWED_HOSTS=[\"example.com\"])\n        request.get_host()\n\n        request.META = {\"HTTP_HOST\": \".example.com\"}\n        request.settings = SimpleNamespace(ALLOWED_HOSTS=[\"example.com\"])\n        request.get_host()\n\n        request.META = {\"HTTP_HOST\": \"*\"}\n        request.settings = SimpleNamespace(ALLOWED_HOSTS=[\"example.com\"])\n        request.get_host()\n", "    def test_get_host(self):\n        request = HttpRequest()\n        request.META = {\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": 80,\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"HTTP_X_FORWARDED_HOST\": \"example.com\",\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": 80,\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": \"443\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"SERVER_NAME\": \"example.com\",\n            \"SERVER_PORT\": 443,\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {}\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n        request.META = {\n            \"SERVER_NAME\": \"example.com\",\n            \"HTTP_HOST\": \"example.com\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"HTTP_HOST\": \"example.com\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"HTTP_HOST\": \"example.com:80\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"HTTP_HOST\": \"example.com:443\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n\n        request.META = {\n            \"HTTP_HOST\": \"example.com:123\",\n        }\n        with self.assertRaises(DisallowedHost):\n            request.get_host()\n\n        request.META = {\n            \"HTTP_HOST\": \"example.com:443\",\n            \"SERVER_PORT\": \"443\",\n        }\n        request.get_host()\n        self.assertIn(\"example.com\", request.get_host())\n", "    def test_init(self):\n        req = HttpRequest()\n        self.assertIsInstance(req.GET, QueryDict)\n        self.assertIsInstance(req.POST, QueryDict)\n        self.assertIsInstance(req.COOKIES, dict)\n        self.assertIsInstance(req.META, HttpHeaders)\n        self.assertIsInstance(req.FILES, MultiValueDict)\n        self.assertIsNone(req.path)\n        self.assertIsNone(req.path_info)\n        self.assertIsNone(req.method)\n        self.assertIsNone(req.resolver_match)\n        self.assertIsNone(req.content_type)\n        self.assertIsNone(req.content_params)\n        self.assertIsInstance(req.headers, HttpHeaders)\n"], "sample_278": ["    def test_combine(self):\n        q1 = Q(name=\"John\")\n        q2 = Q(salary__gt=1000)\n        combined = q1 & q2\n        self.assertEqual(combined.connector, Q.AND)\n        self.assertEqual(combined.children, [(q1, ), (q2, )])\n        \n        combined = q1 | q2\n        self.assertEqual(combined.connector, Q.OR)\n        self.assertEqual(combined.children, [(q1, ), (q2, )])\n", "    def test_filtered_relation_eq(self):\n        # Test that __eq__ works correctly\n        rel1 = FilteredRelation('my_relation', condition=Q(first_name='John'))\n        rel2 = FilteredRelation('my_relation', condition=Q(first_name='John'))\n        self.assertTrue(rel1 == rel2)\n        self.assertFalse(rel1 == Q())\n", "    def setUpTestData(cls):\n        cls.relation = FilteredRelation('company_employee', condition=Q(id__gt=0))\n        cls.relation_clone = cls.relation.clone()\n", "    def setUpTestData(cls):\n        cls.companies = [\n            Company.objects.create(\n                name=\"Company A\", num_employees=10, num_chairs=5\n            ),\n            Company.objects.create(\n                name=\"Company B\", num_employees=20, num_chairs=3\n            ),\n            Company.objects.create(\n                name=\"Company C\", num_employees=30, num_chairs=2\n            ),\n        ]\n", "    def test_Case_expression(self):\n        expr = Case(When(age__gt=18, then=Value(1), else=Value(0))\n        result = expr.resolve_expression(query=None, allow_joins=True, reuse=None, summarize=False, for_save=False)\n        self.assertEqual(result, Case(When(age__gt=18, then=1, else=0))\n"], "sample_397": ["def test_render_to_string_with_context_processors(self):\n    context_processors = [\"django.template.context_processors.debug\"]\n    engine = self.engine_class(\n        context_processors=context_processors,\n    )\n    template_name = \"template.html\"\n    template = engine.from_string(template_name)\n    context = {\n        'variable': 'value',\n    }\n    rendered_template = engine.render_to_string(template_name, context)\n    self.assertEqual(rendered_template, 'template.html'.format(**context))\n", "    def test_engine_init_default_dirs(self):\n        engine = Engine(dirs=['/path/to/dir'])\n        self.assertEqual(engine.dirs, ['/path/to/dir'])\n        self.assertFalse(engine.app_dirs)\n", "    def test_template_loaders_configured(self):\n        engine = self.engine_class()\n        self.assertIn(\"django.template.loaders.filesystem.Loader\", [loader.__module__ for loader in engine.template_loaders])\n", "    def test_find_template_loader_invalid_configuration(self):\n        engine = Engine(dirs=[\"/path/to/dir\"])\n        with self.assertRaises(ImproperlyConfigured):\n            engine.find_template_loader(\"invalid.loader\")\n", "    def test_get_default(self):\n        # Test that get_default returns the first configured engine\n        # (in this case, DjangoTemplates)\n        from django.template import engines\n        engine = Engine.get_default()\n        self.assertIsInstance(engine, Engine)\n        self.assertIsInstance(engine, DjangoTemplates)\n"], "sample_698": ["def test_colored_level_formatter(caplog: Any):\n    \"\"\"Test the ColoredLevelFormatter.\"\"\"\n    handler = logging.StreamHandler()\n    formatter = ColoredLevelFormatter(TerminalWriter(), \"%(levelname)s %(message)s\", \"HH:MM:SS\")\n    formatter._style._fmt = \"%(levelname)s %(message)s\"\n    handler.setFormatter(formatter)\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n    logger.debug(\"debug message\")\n    logger.info(\"info message\")\n    logger.warning(\"warning message\")\n    logger.error(\"error message\")\n    logger.critical(\"critical message\")\n    assert \"debug\" in caplog.text\n    assert \"info\" in caplog.text\n    assert \"warning\" in caplog.text\n    assert \"error\" in caplog.text\n    assert \"critical\" in caplog.text\n", "def test_log_capture_handler_reset():\n    \"\"\"Test if LogCaptureHandler resets correctly.\"\"\"\n    from _pytest.logging import LogCaptureHandler\n\n    # Create a logger and handler\n    logger = logging.getLogger(\"test\")\n    handler = LogCaptureHandler()\n    logger.setLevel(logging.DEBUG)\n    handler.setLevel(logging.DEBUG)\n    logger.addHandler(handler)\n\n    # Log a message\n    logger.debug(\"Test message 1\")\n\n    # Reset the handler\n    handler.reset()\n\n    # Check that the log records are cleared\n    assert handler.records == []\n\n    # Check that the log text is cleared\n    assert handler.stream.getvalue() == \"\"\n", "def test_color_level_formatter():\n    # Test that the level name is correctly colored\n    terminal_writer = TerminalWriter()\n    formatter = ColoredLevelFormatter(terminal_writer, \"%(levelname)s\", \"%H:%M:%S\")\n    record = logging.LogRecord(\n        name=\"test_module\",\n        level=logging.INFO,\n        process=1234,\n        pathname=\"test_module.py\",\n        lineno=1,\n        msg=\"Test message\",\n        args=None,\n        exc_info=None,\n    )\n    formatted = formatter.format(record)\n    assert terminal_writer.strip_colors(formatted) == \" green test_module.py:1 Test message\"\n", "def test_create_formatter_by_default(auto_use_color: bool, capsys) -> None:\n    \"\"\"Ensure that the logger formatter is correctly initialized by default.\"\"\"\n    config = _pytest.config.Config()\n    plugin = _pytest.logging.LoggingPlugin(config)\n    formatter = plugin._create_formatter(\n        get_option_ini(config, \"log_format\"),\n        get_option_ini(config, \"log_date_format\"),\n        get_option_ini(config, \"log_auto_indent\"),\n    )\n    assert isinstance(formatter, logging.Formatter)\n    assert formatter.datefmt == get_option_ini(config, \"log_date_format\")\n", "def test_get_log_level_for_setting_default():\n    \"\"\"Test get_log_level_for_setting() when no settings are defined.\"\"\"\n    config = pytest.config\n    assert get_log_level_for_setting(config) is None\n\n"], "sample_515": ["def test_colorbar_extension_length_auto_spacing():\n    \"\"\"\n    Test that when auto spacing is used, the extensions are equal to the\n    interior boxes when spacing is uniform, and equal to the adjacent\n    interior boxes when spacing is proportional.\n\n    Helper function for test_colorbar_extension_length_auto_spacing.\n    \"\"\"\n    # Create a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=4)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        # Create a subplot.\n        cax = fig.add_subplot(4, 1, i + 1)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendfrac='auto',\n                 orientation='horizontal', spacing='uniform')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n\n", "def test_colorbar_extension_length_fractions():\n    \"\"\"\n    Check colorbar length when extendfrac is a tuple or sequence of fractions.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate(([0.1, 0.2], [0.2, 0.1], [0.1, 0.3])):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*3 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing='uniform')\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_fraction():\n    \"\"\"\n    Test that the colorbar's fraction affects the size of the colorbar.\n    \"\"\"\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=4)\n    # Get a colormap and appropriate norm for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a subplot.\n    cax = fig.add_subplot(1, 1, 1)\n    # Generate the colorbar with different fractions.\n    for fraction in (0.05, 0.15, 0.25):\n        cbar = Colorbar(cax, cmap=cmap, norm=norms['neither'],\n                        boundaries=norms['neither'].boundaries,\n                        extend='neither', orientation='horizontal',\n                        fraction=fraction)\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_orientation():\n    # Create a figure and a set of subplots.\n    fig, axs = plt.subplots(1, 4, figsize=(15, 3))\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a colorbar with different orientations and extensions.\n    for i, orientation in enumerate(['vertical', 'vertical', 'horizontal', 'horizontal']):\n        ax = axs[i]\n        norm = norms['neither']\n        boundaries = values = norm.boundaries\n        boundaries = boundaries[:-1]  # Remove the last boundary to test with 'neither'\n        values = values[:-1]\n        Colorbar(ax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend='neither', orientation=orientation)\n        ax.set_title(orientation)\n        ax.set_axis_off()\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_ticklabel_position():\n    \"\"\"\n    Test that the tick label position in a colorbar is correct.\n    \"\"\"\n    fig, ax = plt.subplots()\n    img = ax.imshow(np.random.rand(10, 10))\n    cb = fig.colorbar(img, ax=ax)\n    cb.set_ticks([0.1, 0.5, 0.9])\n    cb.set_ticklabels(['Low', 'Medium', 'High'])\n    cb.ax.set_xticklabels(cb.ax.get_xticklabels(), ha='right')\n    expected_label_positions = ['left', 'right']\n    actual_label_positions = [label.get_ha() for label in cb.ax.get_xticklabels()]\n    assert expected_label_positions == actual_label_positions\n"], "sample_46": ["    def test_uuidfield_iexact_lookup(self):\n        uuid_obj = UUIDModel.objects.create(uuid=uuid.uuid4())\n        uuid_str = str(uuid_obj.uuid)\n        uuid_str_lower = uuid_str.lower()\n        self.assertTrue(UUIDModel.objects.filter(uuid__iexact=uuid_obj.uuid).exists())\n        self.assertTrue(UUIDModel.objects.filter(uuid__iexact=uuid_str).exists())\n        self.assertTrue(UUIDModel.objects.filter(uuid__iexact=uuid_str_lower).exists())\n        self.assertFalse(UUIDModel.objects.filter(uuid__iexact='random_string').exists())\n", "    def test_uuid_iexact_lookup(self):\n        # Create a UUIDField instance\n        model = models.Model(uuid_id=UUIDField())\n        instance = model(uuid_id=uuid.uuid4())\n        instance.save()\n\n        # Create a QuerySet to test the lookup\n        qs = model.objects.filter(uuid_id__uuid_iexact=instance.uuid_id)\n\n        # Assert that the query is correct\n        self.assertEqual(qs.query.where_clauses[0][0].rhs, instance.uuid_id)\n", "    def test_uuid_exact_lookup(self):\n        # Create a model with a UUID field\n        class UUIDModel(models.Model):\n            uuid_field = models.UUIDField(unique=True)\n\n        # Create a UUID instance\n        uuid_value = uuid.uuid4()\n        uuid_instance = UUIDModel(uuid_field=uuid_value)\n\n        # Save the instance\n        uuid_instance.save()\n\n        # Query the instance using the exact lookup\n        instance = UUIDModel.objects.filter(uuid_field__exact=uuid_value).first()\n\n        # Check that the instance is returned\n        self.assertEqual(instance, uuid_instance)\n\n        # Query the instance using the exact lookup with a UUID instance\n        instance = UUIDModel.objects.filter(uuid_field__exact=Value(uuid_value)).first()\n\n        # Check that the instance is returned\n        self.assertEqual(instance, uuid_instance)\n", "    def test_uuid_field_regex(self):\n        model = models.Model\n        field = CharField(max_length=32)\n        model.add_to_class('uuid_field', field)\n\n        lookup = Regex(Lookup(lhs=field, rhs=\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\"))\n        self.assertEqual(lookup.get_db_prep_lookup(\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\", connection), ('%s', [\"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$\"]))\n\n        # Test the function with a uuid object\n        lookup = Regex(Lookup(lhs=field, rhs=uuid.UUID(\"123e4567-e89b-12d3-a456-426655440000\")))\n        self.assertEqual(lookup.get_db_prep_lookup(uuid.UUID(\"123e4567-e89b-12d3-a456-426655440000\"), connection), ('%s', [\"123e4567-e89b-12d3-a456-426655440000\"]))\n\n        # Test the function with a uuid string\n        lookup = Regex(Lookup(lhs=field, rhs=\"123e4567-e89b-12d3-a456-426655440000\"))\n        self.assertEqual(lookup.get_db_prep_lookup(\"123e4567-e89b-12d3-a456-426655440000\", connection), ('%s', [\"123e4567-e89b-12d3-a456-426655440000\"]))\n", "    def test_uuid_exact_lookup(self):\n        # Test exact lookup on UUIDField\n        model = UUIDModel.objects.create(id=1, uuid=uuid.uuid4())\n        query = UUIDModel.objects.filter(uuid=uuid.uuid4())\n        self.assertEqual(query.count(), 0)\n        \n        # Test exact lookup on UUIDField with existing record\n        query = UUIDModel.objects.filter(uuid=model.uuid)\n        self.assertEqual(query.count(), 1)\n        \n        # Test exact lookup on UUIDField with empty value\n        query = UUIDModel.objects.filter(uuid=None)\n        self.assertEqual(query.count(), 0)\n        \n        # Test exact lookup on UUIDField with string value\n        query = UUIDModel.objects.filter(uuid=str(model.uuid))\n        self.assertEqual(query.count(), 1)\n"], "sample_1043": ["def test_mathematica_code_special_functions():\n    assert mcode(exp(x)) == \"Exp[x]\"\n    assert mcode(log(x)) == \"Log[x]\"\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n", "def test_mathematica_code_trigonometric_functions():\n    # Test trigonometric functions\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(cos(x)) == 'Cos[x]'\n    assert mcode(tan(x)) == 'Tan[x]'\n    assert mcode(cot(x)) == 'Cot[x]'\n    assert mcode(asin(x)) == 'ArcSin[x]'\n    assert mcode(acos(x)) == 'ArcCos[x]'\n    assert mcode(atan(x)) == 'ArcTan[x]'\n    assert mcode(sinh(x)) == 'Sinh[x]'\n    assert mcode(cosh(x)) == 'Cosh[x]'\n    assert mcode(tanh(x)) == 'Tanh[x]'\n    assert mcode(coth(x)) == 'Coth[x]'\n    assert mcode(sech(x)) == 'Sech[x]'\n    assert mcode(csch(x)) == 'Csch[x]'\n    assert mcode(asinh(x)) == 'ArcSinh[x]'\n    assert mcode(acosh(x)) == 'ArcCosh[x]'\n    assert mcode(atanh(x)) == 'ArcTanh[x]'\n    assert mcode(acoth(x)) == 'ArcCoth[x]'\n    assert mcode(asech(x)) == 'ArcSech[x]'\n    assert mcode(acsch(x)) == 'ArcCsch[x]'\n", "def test_complex_infinity():\n    expr = oo\n    expected = 'ComplexInfinity'\n    assert mcode(expr) == expected\n", "def test_mcode():\n    # Test known functions with arguments\n    assert mcode(sin(x)) == 'Sin[x]'\n    assert mcode(exp(x)) == 'Exp[x]'\n    assert mcode(log(x)) == 'Log[x]'\n    assert mcode(Max(x, y)) == 'Max[x, y]'\n    assert mcode(Min(x, y)) == 'Min[x, y]'\n\n    # Test known functions with multiple arguments\n    assert mcode(sin(x, y)) == 'Sin[x, y]'\n    assert mcode(exp(x, y)) == 'Exp[x, y]'\n\n    # Test known functions with a single argument that is a list\n    assert mcode(sin([x, y])) == 'Sin[{x, y}]'\n\n    # Test known functions with a single argument that is a tuple\n    assert mcode(sin((x, y))) == 'Sin[{x, y}]'\n\n    # Test known functions with a single argument that is a list with a single element\n    assert mcode(sin([x])) == 'Sin[x]'\n\n    # Test known functions with a single argument that is a tuple with a single element\n    assert mcode(sin((x,))) == 'Sin[x]'\n\n    # Test known functions with a single argument that is a Derivative\n    assert mcode(Derivative(sin)(x)) == 'Derivative[1][Sin][x]'\n\n    # Test _print_Integral\n    assert mcode(Integral(sin(x), x)) == 'Hold[Integrate[Sin[x], x]]'\n\n    # Test _print_Sum\n    assert mcode(Sum(sin(x), (x, 0, 10))) == 'Hold[Sum[Sin[x], {x, 0, 10}]]'\n\n    # Test _print_Derivative\n    assert mcode(Derivative(sin)(x)) == 'Hold[D[Sin[x], x]]'\n\n    # Test _get_comment\n    assert MCodePrinter()._get_comment('This is a comment') == '(* This is a comment *)'\n\n    # Test _print_Mul\n    assert MCodePrinter().doprint(2*x) == '2*x'\n\n    # Test _print_Pow\n    assert MCodePrinter().doprint(x**2) == 'x^2'\n\n    # Test _print_Zero, _print_One, _print_NegativeOne, _print_half\n    assert M", "def test_mcode_integrals_with_multiple_variables():\n    from sympy.integrals import Integral\n    from sympy.functions import exp\n    from sympy import symbols\n\n    x, y, z = symbols('x,y,z')\n    expr = Integral(exp(x + y + z), x, y, z)\n    expected_output = \"Hold[Integrate[Exp[x + y + z],{x, y, z}]\"\n    assert mathematica_code(expr) == expected_output\n"], "sample_894": ["def test_forest_base_equality():\n    \"\"\"Check that the base class correctly tracks equality.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[\"RandomForestClassifier\"]\n    ForestRegressor = FOREST_REGRESSORS[\"RandomForestRegressor\"]\n\n    clf1 = ForestClassifier(n_estimators=10, random_state=1)\n    clf2 = ForestClassifier(n_estimators=10, random_state=1)\n    assert clf1 == clf2\n\n    clf1 = ForestRegressor(n_estimators=10, random_state=1)\n    clf2 = ForestRegressor(n_estimators=10, random_state=1)\n    assert clf1 == clf2\n\n    clf1 = ForestClassifier(n_estimators=10, random_state=1)\n    clf2 = ForestRegressor(n_estimators=10, random_state=1)\n    assert not clf1 == clf2\n", "def test_forest_regressor_default_criterion(name):\n    \"\"\"Test the default criterion of forest regressors.\"\"\"\n    ForestRegressor = FOREST_CLASSIFIERS_REGRESSORS[name]\n    clf = ForestRegressor(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n    assert clf.criterion == \"squared_error\"\n", "def test_regressor_fit_transform(name):\n    \"\"\"Test fit and transform methods of regressor estimators.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=42)\n\n    forest = ForestRegressor(n_estimators=10, random_state=42)\n    forest.fit_transform(X, y)\n    assert forest.n_features_in_ == X.shape[1]\n    assert forest._n_features_out == 10\n\n    X_transformed = forest.transform(X)\n    assert isinstance(X_transformed, csr_matrix)\n    assert X_transformed.shape == (X.shape[0], forest.n_outputs_)\n    assert forest.n_outputs_ == 1\n\n    # test fit and transform with sample_weight\n    sample_weight = np.random.rand(X.shape[0])\n    forest.fit_transform(X, y, sample_weight=sample_weight)\n    X_transformed = forest.transform(X)\n    assert isinstance(X_transformed, csr_matrix)\n    assert X_transformed.shape == (X.shape[0], forest.n_outputs_)\n    assert forest.n_outputs_ == 1\n\n", "def test_random_trees_embedding():\n    \"\"\"Check the RandomTreesEmbedding.\"\"\"\n    # create a toy sample\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([-1, -1, -1, 1, 1, 1])\n\n    # test it on a toy sample\n    with ignore_warnings(category=FutureWarning):\n        embed = RandomTreesEmbedding(n_estimators=10, random_state=1)\n    embed.fit(X, y)\n    assert embed.n_outputs_ == 10\n    assert embed.n_features_in_ == X.shape[1]\n    assert embed.feature_names_in_.shape == (X.shape[1],)\n    assert embed._n_features_out == 10\n    assert embed.sparse_output is True\n\n    # test transform\n    X_transformed = embed.transform(X)\n    assert X_transformed.shape == (len(X), 10)\n\n    # test transform with a large sample\n    large_embed = RandomTreesEmbedding(n_estimators=100, random_state=1)\n    large_embed.fit(X, y)\n    X_large = np.tile(X, 100)\n    X_large_transformed = large_embed.transform(X_large)\n    assert X_large_transformed.shape == (len(X) * 100, 100)\n\n    # test fit_transform\n    X_transformed_fit = embed.fit_transform(X, y)\n    assert X_transformed_fit.shape == (len(X), 10)\n    assert X_transformed_fit.equals(X_transformed)\n\n    # test get_feature_names_out\n    assert embed.get_feature_names_out().shape == (10,)\n    assert len(embed.get_feature_names_out()) == 10\n    assert embed.get_feature_names_out([\"feature1\", \"feature2\"]) == embed.get_feature_names_out()\n", "def test_forest_regressor_toy(name):\n    \"\"\"Test regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_CLASSIFIERS_REGRESSORS[name]\n\n    # toy sample\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n\n    # create a toy forest regressor\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n\n    # fit the regressor\n    reg.fit(X, y)\n\n    # assert that the regressor has the correct number of estimators\n    assert 10 == len(reg)\n\n    # make some new data\n    new_X = [[-2, 0], [-1, 1], [1, -1], [0, 1], [0, -1]]\n\n    # predict on the new data\n    new_y = reg.predict(new_X)\n\n    # assert that the predictions are correct\n    assert_array_almost_equal(new_y, [-1, -1, 1, 1, 1])\n\n    # also test apply\n    leaf_indices = reg.apply(new_X)\n    assert leaf_indices.shape == (len(new_X), reg.n_estimators)\n\n    # test that the regressor has the correct number of outputs\n    assert reg.n_outputs_ == 1\n\n    # test that the regressor has the correct attribute names\n    assert_array_equal(reg.feature_names_in_, [])\n\n    # test that the regressor has the correct feature importances\n    assert_array_almost_equal(reg.feature_importances_, np.zeros(X.shape[1]))\n\n    # test that the regressor has the correct oob score\n    # since we're not providing a scoring function, it should raise an error\n    with pytest.raises(NotFittedError):\n        reg.oob_score_\n\n    # test that the regressor has the correct oob predictions\n    with pytest.raises(NotFittedError):\n        reg.oob_prediction_\n\n    # test that the regressor has the correct decision function\n    with pytest.raises(NotFittedError):\n        reg.oob_decision_function_\n"], "sample_1031": ["def test_planck_units():\n    planck_length = Quantity(\"planck_length\")\n    assert planck_length.dimension == length\n    assert planck_length.scale_factor == sqrt(hbar*G/speed_of_light**3)\n\n    planck_mass = Quantity(\"planck_mass\")\n    assert planck_mass.dimension == mass\n    assert planck_mass.scale_factor == sqrt(hbar*speed_of_light/G)\n\n    planck_energy = Quantity(\"planck_energy\")\n    assert planck_energy.dimension == energy\n    assert planck_energy.scale_factor == planck_mass * speed_of_light**2\n\n    planck_force = Quantity(\"planck_force\")\n    assert planck_force.dimension == force\n    assert planck_force.scale_factor == planck_energy / planck_length\n\n    planck_power = Quantity(\"planck_power\")\n    assert planck_power.dimension == power\n    assert planck_power.scale_factor == planck_energy / planck_time\n\n    planck_density = Quantity(\"planck_density\")\n    assert planck_density.dimension == mass/length**3\n    assert planck_density.scale_factor == planck_mass / planck_length**3\n\n    planck_intensity = Quantity(\"planck_intensity\")\n    assert planck_intensity.dimension == mass * time**(-3)\n    assert planck_intensity.scale_factor == planck_energy_density * speed_of_light\n\n    planck_current = Quantity(\"planck_current\")\n    assert planck_current.dimension == current\n    assert planck_current.scale_factor == planck_charge / planck_time\n\n    planck_voltage = Quantity(\"planck_voltage\")\n    assert planck_voltage.dimension == voltage\n    assert planck_voltage.scale_factor == planck_energy / planck_charge\n\n    planck_impedance = Quantity(\"planck_impedance\")\n    assert planck_impedance.dimension == impedance\n    assert planck_impedance.scale_factor == planck_voltage / planck_current\n\n    planck_acceleration = Quantity(\"planck_acceleration\")\n    assert planck_acceleration.dimension == acceleration\n    assert planck_acceleration.scale_factor == speed_of_light / planck_time\n", "def test_electric_constant():\n    electric_constant = Quantity.vacuum_permittivity\n    assert electric_constant.name == \"vacuum_permittivity\"\n    assert electric_constant.dimension == Dimension(capacitance/length)\n    assert electric_constant.scale_factor == 1/(Quantity.u0 * Quantity.c**2)\n    assert Quantity(electric_constant).dimensionally_equal(Quantity(1/(Quantity.u0 * Quantity.c**2)))\n", "def test_planck_units():\n    # Test that planck_mass has the correct dimensions\n    assert Quantity.get_dimensional_expr(planck_mass) == mass\n    \n    # Test that planck_time has the correct dimensions\n    assert Quantity.get_dimensional_expr(planck_time) == time\n    \n    # Test that planck_temperature has the correct dimensions\n    assert Quantity.get_dimensional_expr(planck_temperature) == temperature\n    \n    # Test that planck_length has the correct dimensions\n    assert Quantity.get_dimensional_expr(planck_length) == length\n    \n    # Test that planck_charge has the correct dimensions\n    assert Quantity.get_dimensional_expr(planck_charge) == current\n    \n    # Test that derived Planck units are correctly scaled\n    assert planck_mass.is_dimensionless == False\n    assert planck_time.is_dimensionless == False\n    assert planck_temperature.is_dimensionless == False\n    assert planck_length.is_dimensionless == False\n    assert planck_charge.is_dimensionless == False\n    \n    # Test that derived Planck units can be converted to SI units\n    assert (planck_mass.to('kg')).is_dimensionless == False\n    assert (planck_time.to('s')).is_dimensionless == False\n    assert (planck_temperature.to('K')).is_dimensionless == False\n    assert (planck_length.to('m')).is_dimensionless == False\n    assert (planck_charge.to('C')).is_dimensionless == False\n    \n    # Test that derived Planck units can be used in calculations\n    assert (planck_mass * planck_length).is_dimensionless == False\n    assert (planck_time * planck_temperature).is_dimensionless == False\n    ", "def test_quantity_get_dimensional_expr():\n    # Test with base units\n    m_value = 10 * m\n    assert m_value.get_dimensional_expr() == m\n\n    kg_value = 5 * kg\n    assert kg_value.get_dimensional_expr() == kg\n\n    s_value = 7 * s\n    assert s_value.get_dimensional_expr() == s\n\n    A_value = 3 * A\n    assert A_value.get_dimensional_expr() == A\n\n    K_value = 9 * K\n    assert K_value.get_dimensional_expr() == K\n\n    # Test with derived units\n    N_value = 11 * N\n    assert N_value.get_dimensional_expr() == N\n\n    J_value = 13 * J\n    assert J_value.get_dimensional_expr() == J\n\n    W_value = 17 * W\n    assert W_value.get_dimensional_expr() == W\n\n    # Test with SI derived units\n    C_value = 19 * C\n    assert C_value.get_dimensional_expr() == C\n\n    V_value = 23 * V\n    assert V_value.get_dimensional_expr() == V\n\n    ohm_value = 29 * ohm\n    assert ohm_value.get_dimensional_expr() == ohm\n\n    # Test with other derived units\n    katal_value = 31 * katal\n    assert katal_value.get_dimensional_expr() == katal\n\n    gray_value = 37 * gray\n    assert gray_value.get_dimensional_expr() == gray\n\n    becquerel_value = 41 * Bq\n    assert becquerel_value.get_dimensional_expr() == Bq\n\n    # Test with custom derived units\n    ly_value = 43 * ly\n    assert ly_value.get_dimensional_expr() == ly\n\n    au_value = 47 * au\n    assert au_value.get_dimensional_expr() == au\n\n    # Test with base units with prefixes\n    km_value = 51 * km\n    assert km_value.get_dimensional_expr() == m\n\n    mg_value = 53 * mg\n    assert mg_value.get_dimensional_expr() == kg\n\n    us_value = 57 * us\n    assert us_value.get_dimensional_expr() == s\n\n    # Test with derived units with prefixes\n    mW_value = 61 * mW\n    assert mW_value.get_dimensional_expr() == W\n\n    mJ_value = 65 *", "def test_quantity_scale_factors():\n    # Test that the scale factors are correctly defined for all units\n    for unit in Quantity.SI_quantity_scale_factors.values():\n        assert unit.has(unit.base_dim)\n        for scale_factor in unit.args:\n            assert scale_factor.is_number\n            assert scale_factor.is_positive\n\n    # Test that the scale factors have the correct dimension\n    for unit in Quantity.SI_quantity_scale_factors.values():\n        dimex = Quantity.get_dimensional_expr(unit)\n        if dimex != 1:\n            assert dimsys_default.equivalent_dims(unit.dimension, Dimension(dimex))\n"], "sample_37": ["def test_to_fits(self):\n    with NumpyRNGContext():\n        # test basic WCS\n        wcs = wcs.WCS()\n        hdulist = wcs.to_fits()\n        assert hdulist[0].header['CTYPE1'] == 'RA---TAN'\n        assert hdulist[0].header['CTYPE2'] == 'DEC--TAN'\n        assert_array_equal(hdulist[0].header['CRVAL1'], [12.5, 13.5])\n        assert_array_equal(hdulist[0].header['CRVAL2'], [14.5, 15.5])\n        assert_array_equal(hdulist[0].header['CDELT1'], [1.0, 1.0])\n        assert_array_equal(hdulist[0].header['CDELT2'], [1.0, 1.0])\n        assert_array_equal(hdulist[0].header['CTYPEA'], ['RA---TAN', 'DEC--TAN'])\n        assert_array_equal(hdulist[0].header['CTYPEB'], ['RA---TAN', 'DEC--TAN'])\n        assert_array_equal(hdulist[0].header['CTYPEC'], ['RA---TAN', 'DEC--TAN'])\n\n        # test WCS with distortion\n        wcs = wcs.WCS(get_pkg_data_contents('wcs2.hdr'))\n        hdulist = wcs.to_fits()\n        assert_array_equal(hdulist[0].header['CTYPE1'], ['RA---TAN-SIP', 'DEC--TAN'])\n        assert_array_equal(hdulist[0].header['CTYPE2'], ['RA---TAN-SIP', 'DEC--TAN'])\n\n        # test WCS with non-standard header\n        with pytest.warns(FITSFixedWarning):\n            wcs = wcs.WCS(get_pkg_data_contents('wcs3.hdr'))\n            hdulist = wcs.to_fits()\n            assert_array_equal(hdulist[0].header['CTYPE1'], ['RA---TAN-SIP', 'DEC--TAN'])\n            assert_array_equal(hdulist[0].header['CTYPE2'], ['RA---TAN-SIP', 'DEC--TAN'])\n\n        # test WCS with no header\n        wcs = wcs.WCS(None)\n        hdulist = wcs.to_fits()\n        assert hdulist[0].header['CTYPE1'] == 'RA---TAN'\n        assert hdulist[", "    def test_to_fits(self):\n        with NumpyRNGContext(0):\n            wcs = wcs.WCS(\n                fits.Header({'CTYPE1': 'RA---TAN', 'CTYPE2': 'DEC--TAN',\n                            'CRVAL1': 10.0, 'CRVAL2': 20.0,\n                            'CRPIX1': 30.0, 'CRPIX2': 40.0,\n                            'CDELT1': 1.0, 'CDELT2': 2.0,\n                            'PC1_1': 1.0, 'PC1_2': 2.0,\n                            'PC2_1': 3.0, 'PC2_2': 4.0,\n                            'CTYPE1': 'LAT--TAN', 'CTYPE2': 'LON--TAN',\n                            'NAXIS1': 2, 'NAXIS2': 2,\n                            'NAXIS3': 2, 'NAXIS4': 2,\n                            'SIMPLE': True, 'BITPIX': 32,\n                            'EQUINOX': 2000.0, 'LATPOLE': 45.0,\n                            'LONPOLE': 0.0, 'CTYPE1': 'LAT--TAN',\n                            'CTYPE2': 'LON--TAN', 'CTYPE3': 'SPECT-':\n                            'LAT--TAN', 'CTYPE4': 'LON--TAN',\n                            'CRVAL1': 100.0, 'CRVAL2': 200.0,\n                            'CRVAL3': 300.0, 'CRVAL4': 400.0,\n                            'CRPIX1': 10.0, 'CRPIX2': 20.0,\n                            'CRPIX3': 30.0, 'CRPIX4': 40.0,\n                            'CDELT1': 0.1, 'CDELT2': 0.2,\n                            'CDELT3': 0.3, 'CDELT4': 0.4,\n                            'CTYPE1': 'LAT--TAN', 'CTYPE2': 'LON--TAN',\n                            'CTYPE3': 'SPECT-', 'CTYPE4': 'LON--TAN',\n                            'CTYPE5': 'SPECT-',", "    def test_sub(self):\n        # tests that the sub method returns a new WCS object with the same\n        # number of axes but with the specified axes removed\n        for filename in self._file_list:\n            with fits.open(filename) as hdulist:\n                wcs_object = wcs.WCS(hdulist[0].header, hdulist)\n                wcs_object_sub = wcs_object.sub([0, 1])\n                self.assertEqual(wcs_object_sub.naxis, 1)\n                self.assertEqual(wcs_object.naxis, 2)\n", "    def test_footprint_to_file(self):\n        # create a WCS object\n        hdu = fits.PrimaryHDU()\n        hdu.header['NAXIS1'] = 5\n        hdu.header['NAXIS2'] = 3\n        wcs_obj = wcs.WCS(hdu.header)\n\n        # create a footprint\n        corners = np.array([[1, 1],\n                            [1, 3],\n                            [5, 3],\n                            [5, 1]], dtype=np.float64)\n        footprint = wcs_obj.calc_footprint(corners)\n\n        # write the footprint to a file\n        with catch_warnings(record=True) as warn_list:\n            with pytest.raises(ValueError):\n                wcs_obj.footprint_to_file(filename='footprint.reg',\n                                          color='red', width=5)\n            assert len(warn_list) > 0\n\n        # test the written file\n        with open('footprint.reg', mode='r') as f:\n            contents = f.readlines()\n            assert contents[0].startswith('# Region file format: DS9 version 4.0 ')\n            assert contents[1].startswith('# global color=red font=\"helvetica 12 bold select=1 highlite=1 edit=1 move=1 delete=1 include=1 fixed=0 source')\n            assert contents[2].startswith('polygon(')\n            assert_array_equal(footprint, np.genfromtxt(io.StringIO(contents[3]), skip_header=0))\n            assert contents[4].startswith(') # color=red width=5')\n            os.remove('footprint.reg')\n", "    def test_wcs_prettyprint(self):\n        \"\"\"\n        Test pretty printing of WCS objects.\n        \"\"\"\n        # Set up a test WCS\n        filename = get_pkg_data_filename(\"data/j94f05bgq_flt.fits\")\n        hdulist = fits.open(filename)\n        wcs_obj = wcs.WCS(hdulist[('sci',1)].header, hdulist)\n\n        # Test pretty printing of WCS object\n        expected_output = (\n            \"WCS Keywords\\n\"\n            \"Number of WCS axes: 2\\n\"\n            \"CTYPE  :  ['RA---TAN-SIP', 'DEC--TAN-SIP']\\n\"\n            \"CRVAL  :  [ 5.52645724, -72.05171758]\\n\"\n            \"CRPIX  :  [ 1.00000238, 0.99999996]\\n\"\n            \"PC     :  [[ 1., 0.], [ 0., 1.]]\\n\"\n            \"CDELT  :  [  1.0,  1.0]\\n\"\n            \"NAXIS  :  [2, 2]\\n\"\n        )\n        with NumpyRNGContext(42):\n            # Test that the pretty print matches the expected output\n            actual_output = repr(wcs_obj)\n            assert actual_output == expected_output\n"], "sample_41": ["def test_unit_decompose():\n    # Test the decompose method for various cases\n    # Test 1: Decompose a single unit\n    u = u.m\n    assert u.decompose() == u.m\n\n    # Test 2: Decompose a unit with a scale factor\n    u = 2 * u.m\n    assert u.decompose() == u.m\n\n    # Test 3: Decompose a unit with a negative scale factor\n    u = -2 * u.m\n    assert u.decompose() == u.m\n\n    # Test 4: Decompose a composite unit\n    u = u.m * u.s\n    assert u.decompose() == u.m * u.s\n\n    # Test 5: Decompose a unit with a power\n    u = u.m**2\n    assert u.decompose() == u.m**2\n\n    # Test 6: Decompose a unit with a negative power\n    u = u.m**-2\n    assert u.decompose() == u.m**-2\n\n    # Test 7: Decompose a composite unit with a power\n    u = u.m**2 * u.s\n    assert u.decompose() == u.m**2 * u.s\n\n    # Test 8: Decompose a unit with a power and a scale factor\n    u = 2 * u.m**2\n    assert u.decompose() == u.m**2\n\n    # Test 9: Decompose a unit with a negative power and a scale factor\n    u = -2 * u.m**-2\n    assert u.decompose() == u.m**-2\n\n    # Test 10: Decompose a composite unit with a power, a scale factor, and\n    #          another power\n    u = 2 * u.m**2 * u.s\n    assert u.decompose() == u.m**2 * u.s\n\n    # Test 11: Decompose a unit with a power, a negative scale factor, and\n    #          another power\n    u = -2 * u.m**2 * u.s\n    assert u.decompose() == u.m**2 * u.s\n\n    # Test 12: Decompose a composite unit with a power, a negative scale factor,\n    #          and another power\n    u = -2 * u.m**2 * u.s\n    assert u.decompose() == u.m**2", "def test_fractions():\n    # Test the resolve_fractions function\n    assert resolve_fractions(1, 2) == (1, 1)\n    assert resolve_fractions(Fraction(1, 2), 2) == (Fraction(1, 2), 1)\n    assert resolve_fractions(1, Fraction(1, 2)) == (1, Fraction(1, 2))\n    assert resolve_fractions(1, 3) == (1, 1)\n    assert resolve_fractions(2, 3) == (2, 1)\n    assert resolve_fractions(1, 4) == (1, Fraction(1, 2))\n\n    # Test error handling\n    with pytest.raises(ValueError):\n        resolve_fractions(1, 'a')\n    with pytest.raises(ValueError):\n        resolve_fractions('a', 1)\n    with pytest.raises(ValueError):\n        resolve_fractions(1, 0)\n    with pytest.raises(ValueError):\n        resolve_fractions(1, 1.2)\n    with pytest.raises(ValueError):\n        resolve_fractions(1, -1)\n", "def test_irreducible_units():\n    # Test decompose() on an irreducible unit returns itself.\n    assert u.m.decompose() is u.m\n\n    # Test decompose() on an irreducible unit with a base unit that is not in the\n    # namespace raises a UnitsError.\n    with catch_warnings(record=True) as w:\n        assert raises(UnitConversionError)(u.m.decompose, bases=[u.mm])\n    assert len(w) == 1\n    assert \"Unit m can not be decomposed into the requested bases\" in str(w[-1].message)\n\n    # Test IrreducibleUnit.__reduce__() returns the correct arguments.\n    unit = IrreducibleUnit(['m'])\n    assert unit.__reduce__() == (IrreducibleUnit, (['m'], False, True))\n    assert unit.__reduce__() == _recreate_irreducible_unit(IrreducibleUnit, ['m'], True)\n    assert unit.__reduce__() == (IrreducibleUnit, (['m'], True, True))\n", "def test_add_prefixes():\n    m = u.m\n    _add_prefixes(m, namespace={})\n\n    assert m.name == 'meter'\n    assert len(m.names) == 4\n    assert m.names == ['meter', 'milli meter', 'centi meter', 'micro meter']\n\n    # Check if the base unit is the same as the unit with no prefixes\n    assert m.base_unit is m\n\n    # Check if the aliases are correctly set\n    assert m.aliases == ['millimeter', 'centimeter', 'micrometer']\n\n    # Check if the to_string method correctly handles prefixes\n    assert m.to_string(format='latex') == r'\\metre'\n    assert m.to_string(format='unicode') == 'metre'\n    assert m.to_string(format='fits') == 'meter'\n\n    # Check if the get_format_name method correctly handles prefixes\n    assert m.get_format_name('latex') == 'metre'\n    assert m.get_format_name('unicode') == 'metre'\n    assert m.get_format_name('fits') == 'meter'\n", "def test_flatten_units_collection_module():\n    class TestModule:\n        units = u.m\n    units = [u.m, u.s, TestModule, {'a': u.m, 'b': u.s}]\n    flattened_units = _flatten_units_collection(units)\n    assert isinstance(flattened_units, set)\n    assert len(flattened_units) == 4\n    assert u.m in flattened_units\n    assert u.s in flattened_units\n    assert u.m in TestModule.units\n    assert u.s in TestModule.units\n    assert u.m in flattened_units\n    assert u.s in flattened_units\n"], "sample_785": ["    def test_leave_one_group_out(self):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([1, 2, 1, 2])\n        groups = np.array([1, 1, 2, 2])\n        logo = LeaveOneGroupOut()\n        assert len(logo.get_n_splits(X, y, groups)) == 2\n", "    def test_stratified_split_size_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            stratified_split = StratifiedShuffleSplit(test_size=0.1)\n            assert_warns_message(UserWarning,\n                                CV_WARNING,\n                                stratified_split.split, np.zeros(10), np.ones(10))\n", "    def test_get_n_splits(self):\n        X = np.ones(10)\n        y = np.arange(10) // 2\n        cv = RepeatedKFold(n_splits=2, n_repeats=2)\n        assert cv.get_n_splits(X, y) == 4\n", "    def test_get_n_splits_default_value(self):\n        \"\"\"Test that BaseCrossValidator returns the number of samples when no\n        value is given\"\"\"\n        cv = BaseCrossValidator()\n        assert_equal(cv.get_n_splits(X=np.ones(10)), 10)\n        with pytest.raises(ValueError):\n            cv.split(X=np.ones(10), y=np.ones(10))\n        with pytest.raises(ValueError):\n            cv.split(X=None, y=np.ones(10))\n", "    def test_split_n_samples(self):\n        X = np.ones((10, 1))\n        y = np.arange(10) // 2\n        groups = np.arange(10)\n        cv = LeaveOneGroupOut()\n        assert cv.get_n_splits(X, y, groups) == 5\n"], "sample_896": ["def test_nmf_init_with_random_matrices():\n    # Initialize NMF with random matrices\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    nmf = NMF(\n        n_components=2,\n        solver=\"cd\",\n        init=\"random\",\n        random_state=0,\n        beta_loss=\"frobenius\",\n        tol=1e-4,\n        max_iter=200,\n        alpha_W=0.0,\n        alpha_H=\"same\",\n        l1_ratio=0.0,\n        verbose=0,\n    )\n    nmf.fit(X)\n\n    # Check the initialized matrices\n    assert np.all(nmf.components_ >= 0)\n    assert np.all(nmf.components_ >= 0)\n", "def test_nmf_init():\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    nmf = NMF(n_components=2, init=\"random\", random_state=0)\n    W, H = nmf.fit_transform(X)\n    assert_allclose(np.sum(W, axis=0), np.array([1.0208514, 1.0208514]), atol=1e-8)\n    assert_allclose(np.sum(H, axis=1), np.array([2.104337, 2.104337]), atol=1e-8)\n\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    nmf = NMF(n_components=2, init=\"nndsvd\", random_state=0)\n    W, H = nmf.fit_transform(X)\n    assert_allclose(np.sum(W, axis=0), np.array([1.0208514, 1.0208514]), atol=1e-8)\n    assert_allclose(np.sum(H, axis=1), np.array([2.104337, 2.104337]), atol=1e-8)\n\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    nmf = NMF(n_components=2, init=\"nndsvda\", random_state=0)\n    W, H = nmf.fit_transform(X)\n    assert_allclose(np.sum(W, axis=0), np.array([1.0208514, 1.0208514]), atol=1e-8)\n    assert_allclose(np.sum(H, axis=1), np.array([2.104337, 2.104337]), atol=1e-8)\n\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    nmf = NMF(n_components=2, init=\"n", "def test_nmf_init_n_components_auto(Estimator, solver):\n    estimator = Estimator(**solver)\n    # Test for the case when n_components is set to 'auto'\n    X = np.random.rand(10, 20)\n    estimator.fit_transform(X)\n    assert estimator.n_components == X.shape[1]\n    # Test for the case when n_components is set to 'auto' but the shape of W or H is already defined\n    estimator = Estimator(**solver)\n    estimator.components_ = np.random.rand(10, 20)\n    estimator._check_params(X)\n    assert estimator.n_components == 10\n", "def test_nmf_init_random_zero_mean():\n    rng = np.random.default_rng(0)\n    X = csc_matrix(np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]]))\n    X.data *= 2\n    init_W = np.abs(rng.random((X.shape[0], 2)) * np.sqrt(X.mean(0) / 2))\n    init_H = np.abs(rng.random((2, X.shape[1]) * np.sqrt(X.mean(0) / 2)))\n    model = NMF(\n        init=\"random\",\n        solver=\"cd\",\n        random_state=0,\n        n_components=2,\n    )\n    model._W = init_W\n    model._H = init_H\n    X_check = X.toarray()\n    model.fit_transform(X_check)\n    assert_array_almost_equal(model._W, init_W, decimal=5)\n    assert_array_almost_equal(model._H, init_H, decimal=5)\n    model = NMF(\n        init=\"random\",\n        solver=\"mu\",\n        random_state=0,\n        n_components=2,\n    )\n    model._W = init_W\n    model._H = init_H\n    X_check = X.toarray()\n    model.fit_transform(X_check)\n    assert_array_almost_equal(model._W, init_W, decimal=5)\n    assert_array_almost_equal(model._H, init_H, decimal=5)\n", "def test_nmf_init_random_state():\n    rng = np.random.RandomState(0)\n    X = np.random.rand(100, 20)\n    model = NMF(n_components=5, init=\"random\", random_state=rng)\n    model.fit_transform(X)\n\n    rng2 = np.random.RandomState(0)\n    model2 = NMF(n_components=5, init=\"random\", random_state=rng2)\n    model2.fit_transform(X)\n\n    assert_array_equal(model.components_.tolist(), model2.components_.tolist())\n"], "sample_1022": ["def test_convert_xor():\n    assert parse_expr('x^y', transformations=(convert_xor,)) == sympy.Symbol('x')**sympy.Symbol('y')\n    assert parse_expr('x^y^z', transformations=(convert_xor,)) == sympy.Symbol('x')**sympy.Symbol('y')**sympy.Symbol('z')\n    assert parse_expr('x^y^z^w', transformations=(convert_xor,)) == sympy.Symbol('x')**(sympy.Symbol('y')**(sympy.Symbol('z')**sympy.Symbol('w')))\n", "def test_split_symbols_custom():\n        if symbol not in ('a', 'b', 'c'):\n            return _token_splittable(symbol)\n        return False\n\n    transformation = split_symbols_custom(can_split)\n    s = 'a b c'\n    result = implicit_multiplication_application(stringify_expr(s, {}, {}), {}, {})\n    assert result == [('NAME', 'a'), ('*', 'b'), ('*', 'c')]\n", "def test_split_symbols_custom_reverts_to_default():\n    \"\"\"Test that split_symbols_custom reverts to default behavior if predicate is not provided\"\"\"\n    from sympy.parsing.sympy_parser import stringify_expr, standard_transformations\n    expr = stringify_expr('unsplittable', {}, {}, standard_transformations)\n    assert expr == 'unsplittable'\n", "def test_function_exponentiation_transforms_correctly():\n    # Test that function_exponentiation handles various cases of exponentiation\n    # correctly.\n    # Make sure the transform is idempotent\n    transformation = function_exponentiation\n    tokens = [(NAME, 'sin'), (OP, '**'), (NUMBER, '2'), (OP, '('), (NAME, 'x'), (OP, ')')]\n    result = transformation(tokens, {}, {})\n    assert result == [(NAME, 'sin**'), (OP, '('), (NAME, 'x'), (OP, ')**2')]\n\n    # Test that function_exponentiation handles nested exponentiation correctly\n    transformation = function_exponentiation\n    tokens = [(NAME, 'sin'), (OP, '**'), (NUMBER, '2'), (OP, '('), (NAME, 'x'), (OP, ')'), (OP, '**'), (NUMBER, '3')]\n    result = transformation(tokens, {}, {})\n    assert result == [(NAME, 'sin**'), (OP, '('), (NAME, 'x'), (OP, ')**'), (OP, '**3')]\n\n    # Test that function_exponentiation handles implicit multiplication correctly\n    transformation = function_exponentiation\n    tokens = [(NAME, 'sin'), (OP, '**'), (NUMBER, '2'), (OP, '('), (NAME, 'x'), (OP, '*'), (NUMBER, 'y'), (OP, ')')]\n    result = transformation(tokens, {}, {})\n    assert result == [(NAME, 'sin**'), (OP, '('), (NAME, 'x*y'), (OP, ')**2')]\n\n    # Test that function_exponentiation handles complex cases of exponentiation correctly\n    transformation = function_exponentiation\n    tokens = [(NAME, 'sin'), (OP, '**'), (NUMBER, '2'), (OP, '('), (NAME, 'x'), (OP, '+'), (NUMBER, 'y'), (OP, ')')]\n    result = transformation(tokens, {}, {})\n    assert result == [(NAME, 'sin**'), (OP, '('), (NAME, 'x+y'), (OP, ')**2')]\n\n    # Test that function_exponentiation handles cases with different precedence of operators correctly\n    transformation = function_exponentiation\n    tokens = [(NAME, 'sin'), (OP, '**'), (NUMBER, '2'), (OP, '('), (NAME, 'x'), (", "def test_rationalize():\n    # Test that rationalize works correctly on negative numbers\n    assert parse_expr(\"1/3\").simplify() == sympy.Rational(1,3)\n    assert parse_expr(\"-2/5\").simplify() == sympy.Rational(-2,5)\n    \n    # Test that rationalize works correctly on zero\n    assert parse_expr(\"0/3\").simplify() == sympy.Rational(0,3)\n    assert parse_expr(\"-0/2\").simplify() == sympy.Rational(0,2)\n    \n    # Test that rationalize works correctly on positive numbers\n    assert parse_expr(\"2/3\").simplify() == sympy.Rational(2,3)\n    assert parse_expr(\"1/2\").simplify() == sympy.Rational(1,2)\n    \n    # Test that rationalize works correctly on complex numbers\n    assert parse_expr(\"1/2 + 3/4*j\").simplify() == sympy.Rational(2, 1) + sympy.Rational(3, 2)*sympy.I\n    assert parse_expr(\"-2/3 + 1/4*j\").simplify() == sympy.Rational(-2, 3) + sympy.Rational(1, 4)*sympy.I\n    \n    # Test that rationalize fails with non-numeric inputs\n    with raises(SympifyError):\n        parse_expr(\"a/3\")\n        \n    # Test that rationalize works correctly with variable inputs\n    assert parse_expr(\"1/x\").simplify() == 1/x\n    \n    # Test that rationalize works correctly with multiple numbers\n    assert parse_expr(\"1/2 + 1/3\").simplify() == sympy.Rational(5,6)\n    assert parse_expr(\"-2/5 + 1/3\").simplify() == sympy.Rational(-1,15)\n"], "sample_508": ["def test_get_tightbbox_for_layout_only():\n    # Test get_tightbbox_for_layout_only with valid input\n    fig, ax = plt.subplots()\n    path = mpatches.Rectangle((0.1, 0.1), 0.8, 0.8)\n    ax.add_patch(path)\n    assert _get_tightbbox_for_layout_only(path, for_layout_only=True) is None\n    assert _get_tightbbox_for_layout_only(path, for_layout_only=False) is not None\n\n    # Test get_tightbbox_for_layout_only with non-Axes object\n    with pytest.raises(TypeError):\n        _get_tightbbox_for_layout_only(\"hello\", for_layout_only=True)\n\n    # Test get_tightbbox_for_layout_only with invalid argument\n    with pytest.raises(TypeError):\n        _get_tightbbox_for_layout_only(ax, \"hello\", for_layout_only=True)\n", "def test_update():\n    # Test updating an artist's properties using the update method\n    fig, ax = plt.subplots()\n    line = ax.axhline(y=0.5, color='red')\n    \n    # Initial state\n    assert line.get_color() == 'red'\n    assert line.get_linewidth() == 1.0\n    \n    # Update the line's properties\n    line.update({'color': 'blue', 'linewidth': 2.0})\n    \n    # Check the updated properties\n    assert line.get_color() == 'blue'\n    assert line.get_linewidth() == 2.0\n    \n    # Test updating an artist's properties using the set method\n    line.set(**{'color': 'green', 'linestyle': '--'})\n    \n    # Check the updated properties\n    assert line.get_color() == 'green'\n    assert line.get_linestyle() == '--'\n    \n    # Test updating an artist's properties using the setp method\n    setp(line, 'color', 'yellow', 'linestyle', '--')\n    \n    # Check the updated properties\n    assert line.get_color() == 'yellow'\n    assert line.get_linestyle() == '--'\n    \n    # Test updating an artist's properties using the setp method with multiple properties\n    setp(line, 'color', 'green', 'linestyle', '--', 'linewidth', 3.0)\n    \n    # Check the updated properties\n    assert line.get_color() == 'green'\n    assert line.get_linestyle() == '--'\n    assert line.get_linewidth() == 3.0\n    \n    # Test updating an artist's properties using the set method with multiple properties\n    line.set(**{'color': 'red', 'linestyle': '-', 'linewidth': 1.0})\n    \n    # Check the updated properties\n    assert line.get_color() == 'red'\n    assert line.get_linestyle() == '-'\n    assert line.get_linewidth() == 1.0\n    \n    # Test updating an artist's properties using the set method with a dictionary\n    line.set({'color': 'blue', 'linestyle': '--'})\n    \n    # Check the updated properties\n    assert line.get_color() == 'blue'\n    assert line.get_linestyle() == '--'\n", "def test_get_tightbbox_for_layout_only():\n    fig, ax = plt.subplots()\n    ax.add_patch(mpatches.Rectangle((0, 0), 1, 1))\n\n    # Create an artist with a non-zero tightbbox\n    path = mpath.Path.unit_rectangle()\n    trans = mtransforms.IdentityTransform()\n    clip_path = mpath.Path.unit_rectangle()\n    clip_path_transform = mtransforms.IdentityTransform()\n    artist = martist.Artist()\n    artist.set_clip_path((path, trans))\n    artist.set_transform(trans)\n\n    # Check that get_tightbbox_for_layout_only returns the same result as get_tightbbox\n    assert _get_tightbbox_for_layout_only(artist) == artist.get_tightbbox(fig.canvas)\n\n    # Check that get_tightbbox_for_layout_only returns an empty bbox when the artist is clipped to the axes\n    # and can be skipped in layout calculations\n    artist.set_clip_box(fig.bbox)\n    artist.set_clip_path(clip_path)\n    assert _get_tightbbox_for_layout_only(artist) == Bbox([[0, 0], [0, 0]])\n", "def test_sticky_edges():\n    # Test the sticky edges attribute\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_title(\"Test Sticky Edges\")\n    ax.set_xlabel(\"X Label\")\n    ax.set_ylabel(\"Y Label\")\n\n    line = ax.axvline(0, color='red', linestyle='--')\n    line.set_sticky_edges({'x': [-1, 1]})\n\n    # Check if the sticky edges are applied correctly\n    assert line.get_sticky_edges().x == [-1, 1]\n\n    # Test that adding new artist to axes does not update sticky edges\n    ax.axvline(0.5, color='blue')\n    assert line.get_sticky_edges().x == [-1, 1]\n\n    # Test that setting sticky edges to None resets the current values\n    line.set_sticky_edges({'x': []})\n    assert line.get_sticky_edges().x == []\n\n    # Test that setting sticky edges to a value different from the current value\n    # does not reset the current values\n    line.set_sticky_edges({'x': [1]})\n    assert line.get_sticky_edges().x == [1]\n\n    # Test that sticky edges are copied correctly when an artist is copied\n    line_copy = line.copy()\n    assert line_copy.get_sticky_edges().x == [1]\n", "def test_setp_getp():\n    fig, axs = plt.subplots(2, 1)\n\n    line = mlines.Line2D([0, 1], [0, 1])\n    line.set_color('blue')\n\n    text = martist.Text(0.5, 0.5, 'Hello, world!')\n    text.set_rotation(45)\n\n    ax1, ax2 = axs\n    ax1.add_line(line)\n    ax2.text(0.5, 0.5, 'Hello, world!')\n\n    assert getp(line, 'color') == 'blue'\n    assert getp(text, 'rotation') == 45\n    assert getp(ax1, 'xticklabels') is None\n    assert getp(ax2, 'text') == text\n\n    getp(line)\n\n    setp(line, 'color', 'red')\n    setp(text, 'rotation', 90)\n    setp(ax1, 'xticklabels', ['x1', 'x2'])\n    setp(ax2, 'text', 'Goodbye, world!')\n\n    assert getp(line, 'color') == 'red'\n    assert getp(text, 'rotation') == 90\n    assert getp(ax1, 'xticklabels') == ['x1', 'x2']\n    assert getp(ax2, 'text') == 'Goodbye, world!'\n\n    setp(line, 'linestyle', '--')\n\n    assert getp(line, 'linestyle') == '--'\n\n    setp(line, 'linewidth', 2)\n\n    assert getp(line, 'linewidth') == 2\n\n    setp(line, 'markerfacecolor', 'green')\n\n    assert getp(line, 'markerfacecolor') == 'green'\n\n    # Test that invalid property raises an error\n    with pytest.raises(AttributeError):\n        setp(line, 'invalid_property')\n\n    # Test that unknown property raises a warning\n    with pytest.warns(UserWarning):\n        setp(line, 'markerfacecolor', 'red', unknown_property='unknown')\n\n    # Test that properties with aliases can be set using either the full name or the alias\n    setp(line, 'markerfacecolor', 'blue')\n    assert getp(line, 'markerfacecolor') == 'blue'\n    setp(line, 'mfc', 'red')\n    assert getp(line, 'markerfacecolor') == 'red'\n\n   "], "sample_349": ["    def test_media(self):\n        form = AlbumForm()\n        widget = form.fields['band'].widget\n        self.assertIn('admin/js/vendor/jquery/jquery.min.js', widget.media.js)\n        self.assertIn('admin/js/vendor/select2/select2.min.js', widget.media.js)\n        self.assertIn('admin/js/vendor/select2/i18n/en.js', widget.media.js)\n        self.assertIn('admin/js/jquery.init.js', widget.media.js)\n        self.assertIn('admin/js/autocomplete.js', widget.media.js)\n        self.assertIn('admin/css/vendor/select2/select2.min.css', widget.media.css)\n        self.assertIn('admin/css/autocomplete.css', widget.media.css)\n", "    def test_optgroups_with_no_selected_choices(self):\n        field = Album._meta.get_field('band')\n        widget = AutocompleteSelect(field, admin.site)\n        name = 'band'\n        value = []\n        optgroups = widget.optgroups(name, value)\n        self.assertEqual(len(optgroups), 2)\n        self.assertEqual(optgroups[0][1][0].attrs['selected'], 'selected')\n", "    def test_empty_queryset(self):\n        class AlbumForm(forms.ModelForm):\n            class Meta:\n                model = Album\n                fields = ['band']\n                widgets = {\n                    'band': AutocompleteSelect(\n                        Album._meta.get_field('band'),\n                        admin.site,\n                        choices=[],\n                    )\n                }\n\n        form = AlbumForm()\n        self.assertEqual(form['band'].widget.optgroups('band', None, {'class': 'my-class'})[0][1], [self.empty_option])\n", "    def test_widget_context(self):\n        from django.contrib.admin import site\n        album_field = Album._meta.get_field('band')\n        autocomplete_widget = AutocompleteSelect(album_field, site, attrs={'class': 'my-class'})\n        context = autocomplete_widget.get_context('album_band', None, {'class': 'form-control'})\n        self.assertEqual(context['widget']['attrs']['class'], 'my-class form-control')\n        self.assertIn('data-ajax--cache', context['widget']['attrs'])\n", "    def test_autocomplete_required_field(self):\n        with translation.override('en'):\n            form = NotRequiredBandForm({'band': ''})\n            self.assertTrue(form.is_valid())\n            self.assertEqual(form.errors['band'], [])\n\n        form = RequiredBandForm({'band': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['band'], [forms.ValidationError('This field is required.')])\n"], "sample_1070": ["def test_exp_eval():\n    x = symbols('x')\n    assert exp(x).as_numer_denom() == (exp(x), 1)\n    assert exp(x).as_numer_denom(x) == (exp(x), 1)\n", "    def test_log_eval(self):\n        # Test log function evaluation with various inputs\n        assert log(1).evalf() == 0\n        assert log(2).evalf() == log(2)\n        assert log(3).evalf() == log(3)\n        assert log(4).evalf() == log(4)\n        assert log(nan).evalf() == nan\n        assert log(zoo).evalf() == zoo\n        assert log(-2).evalf() == I*pi + log(2)\n", "    def test_log_inverse(self):\n        # test log inverse function\n        self.assertEqual(log(exp(x)).args[0], x)\n        self.assertEqual(exp(log(x)).args[0], x)\n", "def test_exp_eval():\n    x, y = symbols('x y')\n    assert exp(x + y).expand() == exp(x).subs(x, x + y)\n    assert exp(x + y).as_base_exp() == (S.Exp1, Mul(x, y))\n", "    def test_exp_polar_to_exp(self):\n        from sympy import exp_polar, exp, I, pi\n        arg = exp_polar(2*I*pi)\n        assert exp(arg) == 1\n        assert exp_polar(arg.args[0]) == exp_polar(2*I*pi)\n"], "sample_495": ["    def test_validate_number(self):\n        paginator = Paginator([1, 2, 3], 2)\n        # Test valid numbers\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n        self.assertEqual(paginator.validate_number(3), 3)\n        self.assertEqual(paginator.validate_number(4), 4)\n        # Test invalid numbers\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(2.5)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number('a')\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(None)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(True)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(False)\n        # Test page number out of range\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(-1)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(5)\n", "    def test_get_page(self):\n        \"\"\"\n        Test that get_page returns a valid page object, even if the page argument\n        isn't a number or isn't in range.\n        \"\"\"\n        # Test with an integer\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n        page = paginator.get_page(3)\n        self.assertIsInstance(page, Page)\n        self.assertEqual(page.number, 3)\n        self.assertEqual(page.object_list, [5, 6, 7])\n\n        # Test with a float\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n        page = paginator.get_page(3.5)\n        self.assertIsInstance(page, Page)\n        self.assertEqual(page.number, 3)\n        self.assertEqual(page.object_list, [5, 6, 7])\n\n        # Test with a non-integer\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.get_page('three')\n\n        # Test with out-of-range value\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(0)\n\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(11)\n\n        # Test with allow_empty_first_page=False\n        paginator = Paginator([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2, allow_empty_first_page=False)\n        with self.assertRaises(EmptyPage):\n            paginator.get_page(0)\n", "    def test_page_validate_number(self):\n        \"\"\"\n        Test the validate_number method in the Paginator class.\n        \"\"\"\n        paginator = Paginator([], 10, orphans=1)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number('a')\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(3.14)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(0)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(5)\n        self.assertEqual(paginator.validate_number(1), 1)\n        self.assertEqual(paginator.validate_number(2), 2)\n", "    def test_get_page_empty_list(self):\n        # Test empty list\n        paginator = Paginator([], 10)\n        self.assertIsInstance(paginator.get_page(1), Page)\n        self.assertIsInstance(paginator.get_page(2), Page)\n        self.assertIsInstance(paginator.get_page(3), Page)\n", "    def test_validate_number_invalid_input(self):\n        paginator = Paginator([], per_page=10)\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number('abc')\n        with self.assertRaises(PageNotAnInteger):\n            paginator.validate_number(3.14)\n        with self.assertRaises(EmptyPage):\n            paginator.validate_number(-1)\n        with self.assertRaises(InvalidPage):\n            paginator.validate_number(1000)\n"], "sample_367": ["    def test_cache_page_decorator(self):\n        # Test cache_page decorator without key_prefix\n        @cache_page(60 * 15)\n            return HttpResponse('Test page')\n\n        request = HttpRequest()\n        request.META = {}\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n\n        # Test cache_page decorator with key_prefix\n        @cache_page(60 * 15, key_prefix='my_app')\n            return HttpResponse('Test page')\n\n        request = HttpRequest()\n        request.META = {}\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n\n        # Test cache_page decorator with cache_alias\n        @cache_page(60 * 15, cache='default')\n            return HttpResponse('Test page')\n\n        request = HttpRequest()\n        request.META = {}\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n\n        # Test cache_page decorator with invalid timeout\n        with self.assertRaises(ValueError):\n            @cache_page(-1)\n                return HttpResponse('Test page')\n\n        # Test cache_page decorator with invalid cache_alias\n        with self.assertRaises(ValueError):\n            @cache_page(60 * 15, cache=None)\n                return HttpResponse('Test page')\n", "    def test_cache_control(self):\n        request = HttpRequest()\n        response = HttpResponse('Hello, world!')\n        decorated_response = cache_control(private=False)(lambda r: response)(request)\n        self.assertEqual(decorated_response['Cache-Control'], 'private')\n        response = HttpResponse('Hello, world!')\n        decorated_response = cache_control(private=True)(lambda r: response)(request)\n        self.assertEqual(decorated_response['Cache-Control'], 'private, max-age=0')\n        response = HttpResponse('Hello, world!')\n        decorated_response = cache_control(max_age=60)(lambda r: response)(request)\n        self.assertEqual(decorated_response['Cache-Control'], 'max-age=60')\n", "    def test_never_cache_headers_added(self):\n        request = HttpRequest()\n        response = never_cache(fully_decorated)(request)\n        self.assertIn('Cache-Control', response.headers)\n        self.assertIn('Cache-Control', response['Cache-Control'])\n        self.assertIn('no-cache', response['Cache-Control'].lower())\n        self.assertIn('no-store', response['Cache-Control'].lower())\n        self.assertIn('must-revalidate', response['Cache-Control'].lower())\n        self.assertIn('max-age=0', response['Cache-Control'].lower())\n", "    def test_cache_page_decorator(self):\n        request = HttpRequest()\n        view_func = lambda request: HttpResponse('Test page')\n        decorated_view = cache_page(60 * 15)(view_func)\n\n        cached_response = decorated_view(request)\n        self.assertEqual(cached_response.status_code, 200)\n        self.assertEqual(cached_response.content, b'Test page')\n\n        # Test that the cache is updated correctly\n        view_func = lambda request: HttpResponse('Updated page', status=200)\n        decorated_view = cache_page(60 * 15)(view_func)\n        updated_response = decorated_view(request)\n        self.assertEqual(updated_response.status_code, 200)\n        self.assertEqual(updated_response.content, b'Updated page')\n\n        # Test that the cache is cleared when the timeout expires\n        from django.core.cache import cache\n        cache.delete(decorated_view.cache_key(request))\n        self.assertEqual(decorated_view(request).status_code, 200)\n        self.assertEqual(decorated_view(request).content, b'Updated page')\n", "def test_cache_page_no_timeout():\n    \"\"\"\n    Test cache_page decorator without timeout.\n    \"\"\"\n        return HttpResponse('Hello, world!')\n\n    decorated_view = cache_page()(view_func)\n    request = HttpRequest()\n    response = decorated_view(request)\n    assert response.status_code == 200\n    assert response.content == b'Hello, world!'\n"], "sample_378": ["    def setUpTestData(cls):\n        cls.notes = [\n            Note.objects.create(note=str(i), misc=str(i))\n            for i in range(10)\n        ]\n", "    def setUpTestData(cls):\n        cls.article = Article.objects.create(title='Article title')\n        cls.article2 = Article.objects.create(title='Article title2')\n        cls.note = Note.objects.create(note='Note', misc='Misc')\n        cls.note2 = Note.objects.create(note='Note2', misc='Misc2')\n        cls.related_object = RelatedObject.objects.create(article=cls.article, note=cls.note)\n        cls.related_object2 = RelatedObject.objects.create(article=cls.article2, note=cls.note2)\n", "    def test_bulk_update_fails_if_primary_key_is_in_update_fields(self):\n        with self.assertRaises(ValueError):\n            Article.objects.bulk_update([Article.objects.get(id=1)], ['id'])\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create()\n        cls.related = RelatedObject.objects.create(user=cls.user)\n        cls.user.related = cls.related\n        cls.user.save()\n", "    def setUpTestData(cls):\n        cls.member = Member.objects.create(\n            id=1,\n            first_name='John',\n            last_name='Doe',\n        )\n        cls.note = Note.objects.create(note='Note 1', misc='Misc 1')\n        cls.note.related_object = RelatedObject.objects.create(\n            member=cls.member,\n            note=cls.note,\n        )\n        cls.note.save()\n"], "sample_260": ["    def test_renames_model_to_itself(self):\n        model = models.Model(name='test_model')\n        operation = RenameModel('test_model', 'test_model')\n        expected = [operation]\n        self.assertOptimizesTo([operation], expected, exact=1)\n", "    def test_ModelOptionOperation_reduce(self):\n        create_model = CreateModel(\n            'test_model',\n            [\n                ('field1', models.CharField(max_length=10)),\n                ('field2', models.CharField(max_length=10)),\n            ],\n        )\n        alter_options = AlterModelOptions('test_model', {\n            'default_manager_name': 'my_manager',\n        })\n        result = create_model.reduce(alter_options, 'migrations')\n        self.assertEqual(len(result), 1)\n        self.assertIsInstance(result[0], CreateModel)\n        self.assertEqual(result[0].options['default_manager_name'], 'my_manager')\n", "    def test_alter_model_options(self):\n        class TestModel(models.Model):\n            class Meta:\n                ordering = ('field',)\n\n        migration = operations.AlterModelOptions(\n            name='testapp.TestModel',\n            options={'ordering': ('field2',)},\n        )\n        self.assertEqual(migration.name, 'testapp.TestModel')\n        self.assertEqual(migration.options, {'ordering': ('field2',)})\n\n        serialized_migration = self.serialize(migration)\n        self.assertEqual(serialized_migration, {\n            'OperationType': 'AlterModelOptions',\n            'name': 'testapp.TestModel',\n            'options': {'ordering': ('field2',)},\n        })\n", "    def test_remove_constraint_reduction(self):\n        model_name = 'unicode'\n        constraint_name = 'my_constraint'\n        operations = [\n            migrations.CreateModel(\n                name=model_name,\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=255)),\n                ],\n                options={\n                    'constraints': [\n                        migrations.UniqueConstraint(fields=['name'], name=constraint_name)\n                    ]\n                }\n            ),\n            migrations.RemoveConstraint(\n                model_name=model_name,\n                name=constraint_name\n            )\n        ]\n\n        self.assertOptimizesTo(operations, [\n            migrations.AlterModelOptions(\n                name=model_name,\n                options={\n                    'constraints': []\n                }\n            )\n        ], app_label='migrations')\n", "    def test_alter_model_options(self):\n        model = models.Model()\n        migration = AlterModelOptions('test_model', {'managed': False})\n        self.assertEqual(migration.name, 'test_model')\n        self.assertEqual(migration.options, {'managed': False})\n\n        serialized_migration = self.serialize(migration)\n        self.assertEqual(serialized_migration['migration'], 'AlterModelOptions')\n        self.assertEqual(serialized_migration['name'], 'test_model')\n        self.assertEqual(serialized_migration['options'], {'managed': False})\n\n        migration2 = AlterModelOptions('test_model', {'managed': True, 'verbose_name': 'test model'})\n        self.assertEqual(migration2.name, 'test_model')\n        self.assertEqual(migration2.options, {'managed': True, 'verbose_name': 'test model'})\n\n        serialized_migration2 = self.serialize(migration2)\n        self.assertEqual(serialized_migration2['migration'], 'AlterModelOptions')\n        self.assertEqual(serialized_migration2['name'], 'test_model')\n        self.assertEqual(serialized_migration2['options'], {'managed': True, 'verbose_name': 'test model'})\n"], "sample_411": ["    def test_output_wrapper(self):\n        with captured_stderr() as stderr:\n            wrapper = OutputWrapper(StringIO())\n            wrapper.write(\"Test message\", style_func=lambda x: x.upper())\n            self.assertEqual(stderr.getvalue(), \"TEST MESSAGE\\n\")\n            self.assertEqual(wrapper.ending, \"\\n\")\n", "    def test_CommandError(self):\n        # Test that CommandError returns a sensible message\n        error = CommandError(\"Test error message\")\n        self.assertEqual(str(error), \"Error: Test error message\")\n", "    def test_handle(self):\n        command = BaseCommand()\n        with self.assertRaises(NotImplementedError):\n            command.handle()\n", "    def test_handle_default_options(self):\n        \"\"\"\n        Test that handle_default_options sets DJANGO_SETTINGS_MODULE correctly.\n        \"\"\"\n        cmd = management.BaseCommand()\n        options = management.OptionParser(standalone=False)\n        handle_default_options(options)\n        self.assertEqual(os.environ[\"DJANGO_SETTINGS_MODULE\"], \"None\")\n        options.settings = \"settings\"\n        handle_default_options(options)\n        self.assertEqual(os.environ[\"DJANGO_SETTINGS_MODULE\"], \"settings\")\n", "    def test_base_command_help(self):\n        # Test that the help output is properly formatted\n        base_command = BaseCommand()\n        output = StringIO()\n        base_command.print_help(\"\", \"subcommand\")\n        output.write(base_command.stdout.getvalue())\n        expected = (\n            \"usage: subcommand [options] [args...]\\n\\n\"\n            \"optional arguments:\\n\"\n            \"  -h, --help            show this help message and exit\\n\"\n            \"  -v, --verbosity         Verbosity level; 0=minimal output, 1=normal output, 2=verbose output, 3=very verbose output\\n\"\n            \"  --settings             The Python path to a settings module, e.g. 'myproject.settings.main'. If this isn\\'t provided, the DJANGO_SETTINGS_MODULE environment variable will be used.\\n\"\n            \"  --pythonpath           A directory to add to the Python path, e.g. '/home/djangoprojects/myproject'.\\n\"\n            \"  --traceback           Raise on CommandError exceptions.\\n\"\n            \"  --no-color            Don't colorize the command output.\\n\"\n            \"  --force-color         Force colorization of the command output.\\n\"\n            \"  --skip-checks         Skip system checks.\\n\"\n        )\n        self.assertEqual(output.getvalue(), expected)\n"], "sample_1098": ["def test_hyper_convergence():\n    from sympy.functions.special.hyper import hyper, hyperexpand\n    from sympy.abc import x, y\n    from sympy.testing.randtest import random_complex_number as randcplx\n\n    for z in [randcplx() for _ in range(5)]:\n        for c in [-2, -1, 0, 1, 2]:\n            hyper(z, [1, 2], c).radius_of_convergence\n            hyper([1, 2], z, c).radius_of_convergence\n            hyper(z, z, c).radius_of_convergence\n            hyper([1, 2], [3, 4], c).radius_of_convergence\n            hyper(z, [1, 2], [3, 4]).radius_of_convergence\n            hyper([1, 2], z, [3, 4]).radius_of_convergence\n            hyper(z, [1, 2], z).radius_of_convergence\n            hyper([1, 2], [3, 4], z).radius_of_convergence\n            hyper(z, z, z).radius_of_convergence\n            hyper([1, 2], [3, 4], [5, 6]).radius_of_convergence\n            hyper(z, [1, 2], [3, 4], c).radius_of_convergence\n            hyper([1, 2], z, [3, 4], c).radius_of_convergence\n            hyper(z, z, [3, 4]).radius_of_convergence\n            hyper([1, 2], [3, 4], z, c).radius_of_convergence\n            hyper(z, z, z, c).radius_of_convergence\n            hyper([1, 2], [3, 4], [5, 6], c).radius_of_convergence\n            hyper(z, [1, 2], [3, 4], [5, 6]).radius_of_convergence\n            hyper([1, 2], z, [3, 4], [5, 6]).radius_of_convergence\n            hyper(z, z, [3, 4], [5, 6]).radius_of_convergence\n            hyper([1, 2], [3, 4], z, [5, 6]).radius_of_convergence\n            hyper", "def test_hyperexpand_hyperrep():\n    from sympy import hyperexpand, S\n    # Test that `hyperexpand` uses `HyperRep_power1`\n    h = hyper([], [], x)\n    assert h._eval_rewrite_as_nonrep() == h\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    h = hyper([1], [], x)\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    # Test that `hyperexpand` uses `HyperRep_power2`\n    h = hyper([S(1)/2, S(1)/2], [2], x)\n    assert h._eval_rewrite_as_nonrep() == h\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    # Test that `hyperexpand` uses `HyperRep_log1`\n    h = -x * hyper([1, 1], [2], x)\n    assert h._eval_rewrite_as_nonrep() == h\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    # Test that `hyperexpand` uses `HyperRep_atanh`\n    h = hyper([S(1)/2, 1], [3/2], x)\n    assert h._eval_rewrite_as_nonrep() == h\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    # Test that `hyperexpand` uses `HyperRep_asin1`\n    h = hyper([S(1)/2, 1/2], [3/2], x)\n    assert h._eval_rewrite_as_nonrep() == h\n    assert hyperexpand(h)._eval_rewrite_as_nonrep() == hyperexpand(h)\n    assert hyperexpand(h)._eval_rewrite_as_Sum() == h\n    # Test that `hyperexpand` uses `HyperRep_sqrts1`\n    h = hyper([-S(1)/2, 1/2 - S(1)/2], [1/2], x)\n    assert h._eval_rewrite_as_nonrep() ==", "    def test_hyper_diff(self):\n        z = symbols('z')\n        hyper_ = hyper([1, 2, 3], [4, 5], z)\n        self.assertEqual(hyper_.fdiff(3), 1/(z*6))\n        self.assertEqual(hyper_.fdiff(2), 1/(z*5))\n        self.assertEqual(hyper_.fdiff(1), 1/(z*4))\n        self.assertEqual(hyper_.fdiff(0), 0)\n        self.assertEqual(hyper_.fdiff(4), -1/(z**2))\n        self.assertEqual(hyper_.fdiff(5), -1/(z**3))\n        self.assertEqual(hyper_.fdiff(6), -1/(z**4))\n", "def test_hyper_expansion_with_mellin_transform():\n    # http://functions.wolfram.com/HypergeometricFunctions/MeijerG/06/02/03/0001/\n    from sympy import expand_func, gamma, hyperexpand\n    from sympy.abc import z\n    a, b, c, d = symbols('a b c d')\n    expr = expand_func(meijerg([a], [], [c], [d], z))\n    expected = gamma(c)*gamma(c - a - d)/gamma(c - a)/gamma(c - d)\n    assert expr.equals(expected)\n\n    # http://functions.wolfram.com/HypergeometricFunctions/MeijerG/06/02/03/0002/\n    expr = hyperexpand(meijerg([a, a], [a + 1, a + 2], [c], [d], z))\n    expected = (a + 1)*(a + c)*gamma(a + c + 1)*gamma(c - a - d)/gamma(c - a)/gamma(c - d)\n    assert expr.equals(expected)\n\n    # http://functions.wolfram.com/HypergeometricFunctions/MeijerG/06/02/03/0003/\n    expr = hyperexpand(meijerg([], [a], [c], [d], z))\n    expected = exp(z)\n    assert expr.equals(expected)\n\n    # test with non-integer parameters\n    expr = hyperexpand(meijerg([a, b], [c, d], [e], [f], z))\n    expected = (c - d)*(e - f)*gamma(c - d + e - f)*hyper([c - d + e - f], [c - d, e - f], z)\n    assert expr.equals(expected)\n", "def test_hyper_unevaluated():\n    from sympy import hyper\n    from sympy.abc import x, y\n    assert hyper(x, y, z).is_Function\n    assert hyper([x], [y], z).is_Function\n    assert hyper([x], (y,), z).is_Function\n    assert hyper((x,), (y,), z).is_Function\n    assert hyper(x, y, z).convergence_statement.is_BooleanFunction\n"], "sample_706": ["def test_empty_expression():\n    assert not evaluate(\"\", lambda _: True)\n", "def test_not_negated_matcher():\n    \"\"\"Test that not negates the matcher function.\"\"\"\n    assert evaluate(\"not True\", lambda x: True) is False\n    assert evaluate(\"not False\", lambda x: True) is True\n    assert evaluate(\"not None\", lambda x: True) is False\n    assert evaluate(\"not not True\", lambda x: True) is True\n", "def test_invalid_matcher_type():\n    with pytest.raises(ParseError):\n        evaluate(\"a or b\", lambda x: None)\n", "def test_empty_expression():\n    # Test case: Empty expression\n    assert evaluate(\"\", lambda x: True) is False\n\n", "def test_empty_expression():\n    # Empty expression should evaluate to False\n    assert not evaluate(\"\", lambda x: True)\n\n"], "sample_1020": ["def test_mathematica_code_with_trigonometric_functions():\n    # Test with single trigonometric functions\n    assert mcode(sin(x)) == \"Sin[x]\"\n    assert mcode(cos(x)) == \"Cos[x]\"\n    assert mcode(tan(x)) == \"Tan[x]\"\n    assert mcode(cot(x)) == \"Cot[x]\"\n    assert mcode(sinh(x)) == \"Sinh[x]\"\n    assert mcode(cosh(x)) == \"Cosh[x]\"\n    assert mcode(tanh(x)) == \"Tanh[x]\"\n    assert mcode(coth(x)) == \"Coth[x]\"\n    assert mcode(sech(x)) == \"Sech[x]\"\n    assert mcode(csch(x)) == \"Csch[x]\"\n\n    # Test with inverse trigonometric functions\n    assert mcode(asin(x)) == \"ArcSin[x]\"\n    assert mcode(acos(x)) == \"ArcCos[x]\"\n    assert mcode(atan(x)) == \"ArcTan[x]\"\n    assert mcode(asinh(x)) == \"ArcSinh[x]\"\n    assert mcode(acosh(x)) == \"ArcCosh[x]\"\n    assert mcode(atanh(x)) == \"ArcTanh[x]\"\n    assert mcode(acoth(x)) == \"ArcCoth[x]\"\n    assert mcode(asech(x)) == \"ArcSech[x]\"\n    assert mcode(acsch(x)) == \"ArcCsch[x]\"\n\n    # Test with Max and Min functions\n    assert mcode(Max(x, y)) == \"Max[x, y]\"\n    assert mcode(Min(x, y)) == \"Min[x, y]\"\n\n    # Test with multiple arguments for Max and Min functions\n    assert mcode(Max(x, y, z)) == \"Max[x, y, z]\"\n    assert mcode(Min(x, y, z)) == \"Min[x, y, z]\"\n\n    # Test with pow function\n    assert mcode(x**2) == \"x^2\"\n\n    # Test with negative numbers\n    assert mcode(-x) == \"-x\"\n\n    # Test with zero and one\n    assert mcode(x**0) == \"1\"\n    assert mcode(x**1) == \"x\"\n\n    # Test with infinity and negative infinity\n    assert mcode(oo) == \"Infinity\"\n    assert mcode(-oo) == \"-Infinity\"\n\n    # Test with pi\n", "def test_print_Pi():\n    assert mcode(pi) == 'Pi'\n", "def test_mathematica_code_with_sech():\n    from sympy import sech, csch, acsch\n    assert mathematica_code(sech(x)) == \"Sech[x]\"\n    assert mathematica_code(csch(x)) == \"Csch[x]\"\n    assert mathematica_code(acsch(x)) == \"ArcSech[x]\"\n", "def test_integral_multiple_variables():\n    from sympy import mathematica_code as mcode\n    x, y = symbols('x,y')\n    expr = Integral(x**2 + y**2, x, y)\n    assert mcode(expr) == \"Hold[Integrate[(x**2 + y**2), x, y]]\"\n", "def test_print_Mul_negative():\n    from sympy.core import S\n    assert mathematica_code(-S(1)) == '-1'\n"], "sample_999": ["compilation error", "def test_MatMul_latex():\n    m1 = Matrix([[1, 2], [3, 4]])\n    m2 = Matrix([[5, 6], [7, 8]])\n    m3 = Matrix([[9, 10], [11, 12]])\n\n    expected_latex = r'\\left[\\begin{matrix}5 & 6 \\\\ 7 & 8\\end{matrix}\\right] \\left[\\begin{matrix}9 & 10 \\\\ 11 & 12\\end{matrix}\\right]'\n\n    assert latex(m1 * m2 * m3) == expected_latex\n\n    m4 = MatrixSymbol('M', 2, 2)\n\n    expected_latex = r'\\left[\\begin{matrix}M_{00} & M_{01} \\\\ M_{10} & M_{11}\\end{matrix}\\right] \\left[\\begin{matrix}M_{00} & M_{01} \\\\ M_{10} & M_{11}\\end{matrix}\\right]'\n\n    assert latex(m4 * m4) == expected_latex\n\n    m5 = block_matrix([[Matrix([[1, 2], [3, 4]]), Matrix([[5, 6], [7, 8]])])\n\n    expected_latex = r'\\left[\\begin{matrix}1 & 2 & 5 & 6 \\\\ 3 & 4 & 7 & 8\\end{matrix}\\right]'\n\n    assert latex(m5) == expected_latex\n", "def test_printing_latex_ordering():\n    from sympy.printing.latex import latex, PRECEDENCE\n\n        latex_printer = LatexPrinter({'order': order})\n        return latex_printer.doprint(expr)\n\n    # Test that the ordering works as expected\n    assert _print_with_order(x + y, 'lex') == r'x + y'\n    assert _print_with_order(x + y, 'grlex') == r'x + y'\n    assert _print_with_order(x + y, 'grevlex') == r'x + y'\n    assert _print_with_order(x + y, 'old') == r'+x+y'\n\n    # Test that the ordering doesn't affect Mul objects\n    assert _print_with_order(x * y, 'lex') == r'x * y'\n    assert _print_with_order(x * y, 'grlex') == r'x * y'\n    assert _print_with_order(x * y, 'grevlex') == r'x * y'\n    assert _print_with_order(x * y, 'old') == r'x*y'\n\n    # Test that Pow objects are handled correctly\n    assert _print_with_order(x**2 + y, 'lex') == r'x**2 + y'\n    assert _print_with_order(x**2 + y, 'grlex') == r'x**2 + y'\n    assert _print_with_order(x**2 + y, 'grevlex') == r'x**2 + y'\n    assert _print_with_order(x**2 + y, 'old') == r'x**2+y'\n\n    # Test that the 'none' ordering doesn't change anything\n    assert _print_with_order(x + y, 'none') == r'x + y'\n    assert _print_with_order(x * y, 'none') == r'x * y'\n    assert _print_with_order(x**2 + y, 'none') == r'x**2 + y'\n", "compilation error", "def test_issue_16121():\n    m = ImmutableDenseNDimArray([[1, 2, 3], [4, 5, 6]])\n    assert latex(m) == r'\\begin{array}{ccc}1 & 2 & 3 \\\\4 & 5 & 6\\end{array}'\n    m = ImmutableSparseNDimArray([[1, 2, 3], [4, 5, 6]])\n    assert latex(m) == r'\\begin{array}{ccc}1 & 2 & 3 \\\\4 & 5 & 6\\end{array}'\n    m = MutableSparseNDimArray([[1, 2, 3], [4, 5, 6]])\n    assert latex(m) == r'\\begin{array}{ccc}1 & 2 & 3 \\\\4 & 5 & 6\\end{array}'\n    m = MutableDenseNDimArray([[1, 2, 3], [4, 5, 6]])\n    assert latex(m) == r'\\begin{array}{ccc}1 & 2 & 3 \\\\4 & 5 & 6\\end{array}'\n"], "sample_707": ["def test_iterparentnodeids(path: Path, expected: List[str]) -> None:\n    nodeid = str(path)\n    assert list(iterparentnodeids(nodeid)) == expected\n", "def test_listextrakeywords(nodeid, expected, test_data):\n    node = nodes.Node(\n        name=\"test1\",\n        path=test_data.path,\n        config=None,\n        session=None,\n        nodeid=nodeid,\n    )\n    node.extra_keyword_matches = test_data.extra_keywords\n    assert list(node.listextrakeywords()) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    node = nodes.Node(name=\"test\", nodeid=nodeid)\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids_nodeid_with_multiple_colons_and_slashes(\n    nodeid: str,", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    parent_ids = list(iterparentnodeids(nodeid))\n    assert parent_ids == expected\n"], "sample_939": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_visit_IfExp():\n    code = \"\"\"\n    if x > 5: 'yes' else 'no'\n    \"\"\"\n    expected = \"x > 5 and 'yes' or 'no'\"\n    assert visitor.visit(ast.parse(code)) == expected\n"], "sample_767": ["def test_ColumnTransformer_remainder_passthrough():\n    # Test that the remainder is correctly processed when remainder='passthrough'\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n\n    ct = ColumnTransformer(\n        [('norm1', Normalizer(norm='l1'), [0, 1]),\n         ('norm2', Normalizer(norm='l1'), slice(2, 4))],\n        remainder='passthrough')\n\n    ct.fit(X)\n    X_t = ct.transform(X)\n    expected_X_t = np.hstack([np.array([[0., 1.], [0.5, 0.5]]), X[:, 2:]])\n    assert_array_equal(X_t, expected_X_t)\n\n    ct = ColumnTransformer(\n        [('norm1', Normalizer(norm='l1'), [0, 1]),\n         ('norm2', Normalizer(norm='l1'), slice(2, 4))],\n        remainder='passthrough',\n        n_jobs=1)\n    X_t = ct.fit_transform(X)\n    expected_X_t = np.hstack([np.array([[0., 1.], [0.5, 0.5]]), X[:, 2:]])\n    assert_array_equal(X_t, expected_X_t)\n", "def test_column_transformer_fit_transform_drop():\n    \"\"\"Test ColumnTransformer with remainder='drop' and fit_transform\"\"\"\n    # Create a DataFrame with 3 columns\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([0, 1])\n    \n    # Create a ColumnTransformer with one transformer that transforms column 0\n    ct = ColumnTransformer([('trans', Trans(), [0])], remainder='drop')\n    \n    # Fit and transform the data\n    ct.fit_transform(X, y)\n    \n    # Check that the transformed data is a 1D array\n    expected = np.array([3, 6])\n    assert_array_equal(ct.transform(X), expected)\n\n", "def test_column_transformer_sparse_output():\n    X = np.random.rand(10, 10)\n    trans = ColumnTransformer([('sparse', SparseMatrixTrans(), [0])])\n    trans.fit(X)\n    # We must check that the output is not sparse\n    assert not sparse.issparse(trans.transform(X))\n", "def test_fit_transform_return_type():\n    # Make a ColumnTransformer\n    transformer = make_column_transformer(\n        ('scaler', StandardScaler(), 'all'),\n        ('encoder', OneHotEncoder(), [0]),\n        remainder='passthrough')\n    # Fit and transform a dataframe with different types of data\n    X = np.array([[1, 2], [3, 4]], dtype=float)\n    df = pd.DataFrame(X, columns=['a', 'b'])\n    X_transformed = transformer.fit_transform(df)\n    assert isinstance(X_transformed, np.ndarray)\n    assert X_transformed.dtype == float\n\n    # Test that ColumnTransformer returns a dense output when all transformers return dense outputs\n    transformer = make_column_transformer(\n        ('scaler', StandardScaler(), 'all'),\n        ('encoder', OneHotEncoder(), [0]),\n        remainder='passthrough')\n    # Fit and transform a dataframe with all dense data\n    X = np.array([[1, 2], [3, 4]], dtype=float)\n    df = pd.DataFrame(X, columns=['a', 'b'])\n    X_transformed = transformer.fit_transform(df)\n    assert not sparse.issparse(X_transformed)\n\n    # Test that ColumnTransformer returns a sparse output when one transformer returns a sparse output\n    transformer = make_column_transformer(\n        ('scaler', StandardScaler(), 'all'),\n        ('encoder', OneHotEncoder(), [0]),\n        remainder='passthrough')\n    # Fit and transform a dataframe with one sparse column\n    X = np.array([[1, 2], [3, 4]], dtype=float)\n    df = pd.DataFrame(X, columns=['a', 'b'])\n    df['c'] = sparse.csr_matrix([[1, 0], [0, 1]])\n    X_transformed = transformer.fit_transform(df)\n    assert sparse.issparse(X_transformed)\n", "def test_pass_through_transformer():\n    \"\"\"Check that 'passthrough' transformers are properly copied to the\n    ``transformers_`` list.\n\n    \"\"\"\n    transformer = FunctionTransformer(validate=False)\n    transformer.fit_transform = lambda X, y: X\n    transformer.transform = lambda X, y: X\n    X = np.array([1, 2])\n    ct = ColumnTransformer([('transformer', transformer, slice(None))])\n\n    # no sparse data\n    result = ct.fit_transform(X)\n    assert_array_equal(result, X)\n    assert_array_equal(ct.transformers_[0], ('transformer', transformer, slice(None)))\n\n    # sparse data\n    X = sparse.csr_matrix([[1], [2]])\n    result = ct.fit_transform(X)\n    assert_allclose_dense_sparse(result, X)\n    assert_array_equal(ct.transformers_[0], ('transformer', transformer, slice(None)))\n"], "sample_790": ["def test_kernel_pca_inverse_transform():\n    # Test inverse transform for rbf kernel\n    X, _ = make_circles(n_samples=100, factor=0.3, noise=0.05, random_state=42)\n    transformer = KernelPCA(n_components=1, kernel=\"rbf\", fit_inverse_transform=True)\n    X_transformed = transformer.fit_transform(X)\n    X_reconstructed = transformer.inverse_transform(X_transformed)\n    assert_allclose(X, X_reconstructed, atol=1e-3)\n\n    # Test inverse transform for precomputed kernel\n    K = rbf_kernel(X)\n    transformer = KernelPCA(n_components=1, kernel=\"precomputed\", fit_inverse_transform=True)\n    X_transformed = transformer.fit_transform(K)\n    X_reconstructed = transformer.inverse_transform(X_transformed)\n    assert_allclose(K, X_reconstructed, atol=1e-3)\n\n    # Test inverse transform for non-square kernel matrix\n    K = rbf_kernel(X[:, :10])\n    transformer = KernelPCA(n_components=1, kernel=\"precomputed\", fit_inverse_transform=True)\n    X_transformed = transformer.fit_transform(K)\n    X_reconstructed = transformer.inverse_transform(X_transformed)\n    assert_allclose(K, X_reconstructed, atol=1e-3)\n\n    # Test inverse transform with different kernel parameters\n    K = rbf_kernel(X, gamma=1.5)\n    transformer = KernelPCA(n_components=1, kernel=\"rbf\", gamma=1.5, fit_inverse_transform=True)\n    X_transformed = transformer.fit_transform(K)\n    X_reconstructed = transformer.inverse_transform(X_transformed)\n    assert_allclose(K, X_reconstructed, atol=1e-3)\n", "def test_kernel_pca_transform_always_centered():\n    X, _ = make_circles(n_samples=100, factor=.2, noise=.05)\n    transformer = KernelPCA(n_components=2, kernel=\"rbf\", gamma=15)\n    X_transformed = transformer.fit_transform(X)\n    X_centered = transformer._centerer.transform(X)\n    X_transformed_centered = transformer._centerer.transform(X_transformed)\n    assert_array_almost_equal(X_transformed_centered, X_centered)\n", "def test_KernelPCA_fit_inverse_transform_large_X():\n    # Test the inverse transform with large X\n    X, _ = make_circles(n_samples=10000, factor=0.3, noise=0.05, random_state=0)\n    kernel_pca = KernelPCA(kernel=\"rbf\", n_components=3, fit_inverse_transform=True)\n    kernel_pca.fit(X)\n    X_transformed = kernel_pca.transform(X)\n    X_reconstructed = kernel_pca.inverse_transform(X_transformed)\n    assert_allclose(X, X_reconstructed)\n", "def test_fit_inverse_transform_with_none_n_components():\n    X, y = make_circles(n_samples=100, factor=0.3, noise=0.05)\n    kpca = KernelPCA(n_components=None, fit_inverse_transform=True)\n    X_transformed = kpca.fit_transform(X)\n    assert_raises(NotFittedError, kpca.inverse_transform, X_transformed)\n", "def test_inverse_transform_zero_components():\n    # Create a test dataset with a linearly separable decision boundary\n    X, _ = make_circles(n_samples=100, factor=.2, noise=.05, random_state=0)\n\n    # Create a KPCA object with 0 components\n    kPCA = KernelPCA(n_components=0, kernel='rbf', gamma=15, fit_inverse_transform=True)\n    kPCA.fit(X)\n\n    # Check that inverse_transform returns an empty array\n    X_inv = kPCA.inverse_transform(kPCA.transform(X))\n    assert_array_almost_equal(X_inv, np.zeros((X.shape[0], X.shape[1])))\n\n    # Check that inverse_transform fails when fit_inverse_transform is False\n    kPCA.fit(X, y=None, fit_inverse_transform=False)\n    assert_raises(NotFittedError, kPCA.inverse_transform, X)\n"], "sample_900": ["def test_solver_validation():\n    mlp = MLPClassifier(solver='lbfgs')\n    with pytest.raises(ValueError):\n        mlp.fit(X_digits_binary, y_digits_binary)\n\n    mlp = MLPRegressor(solver='lbfgs')\n    with pytest.raises(ValueError):\n        mlp.fit(Xboston, yboston)\n\n    mlp = MLPClassifier(solver='invalid_solver')\n    with pytest.raises(ValueError):\n        mlp.fit(X_digits_binary, y_digits_binary)\n\n    mlp = MLPRegressor(solver='invalid_solver')\n    with pytest.raises(ValueError):\n        mlp.fit(Xboston, yboston)\n\n", "def test_mlp_predict_output_activation(activation):\n    X = np.array([[1, 2], [3, 4]])\n    mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=activation,\n                        solver='lbfgs', max_iter=100)\n    mlp.fit(X, np.array([0, 1]))\n    y_pred = mlp.predict(X)\n    assert y_pred.shape == (2,)\n\n", "def test_identity_activation():\n    mlp = MLPClassifier(hidden_layer_sizes=(100,), activation='identity')\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    mlp.fit(X, y)\n    y_pred = mlp.predict(X)\n    assert_array_equal(y_pred, np.dot(X, mlp.coefs_[0]) + mlp.intercepts_[0])\n", "def test_MLPClassifier_stochastic_solver_learning_rate_adaptive():\n    X, y = classification_datasets[0]\n    clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n                       solver='sgd', alpha=0.0001, batch_size=10,\n                       learning_rate='adaptive',\n                       learning_rate_init=0.01, max_iter=10, tol=1e-3,\n                       verbose=1, shuffle=True, random_state=None,\n                       n_iter_no_change=10)\n    with warnings.catch_warnings(record=True):\n        clf.fit(X, y)\n    assert clf.n_iter_ <= 10\n\n", "def test_MLPRegressor_categorical_target():\n    X, y = make_regression(n_samples=1000, n_features=20, n_informative=10,\n                           n_targets=1, random_state=42, target_func='linear')\n    y = np.where(y > 0, 1, 0)\n    \n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    mlp_regressor = MLPRegressor(hidden_layer_sizes=(100,), activation=\"relu\",\n                                 solver='adam', alpha=0.0001,\n                                 batch_size='auto', learning_rate=\"constant\",\n                                 learning_rate_init=0.001,\n                                 power_t=0.5, max_iter=200, shuffle=True,\n                                 random_state=42, tol=1e-4,\n                                 verbose=False, warm_start=False, momentum=0.9,\n                                 nesterovs_momentum=True, early_stopping=False,\n                                 validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n                                 epsilon=1e-8, n_iter_no_change=10, max_fun=15000)\n    \n    mlp_regressor.fit(X_scaled, y)\n\n    y_pred = mlp_regressor.predict(X_scaled)\n    assert_array_equal(y_pred.shape, y.shape)\n\n    # Check if the prediction is either 0 or 1\n    assert np.all(np.isclose(y_pred, np.round(y_pred), atol=1e-5))\n"], "sample_1030": ["def test_closest_points_rational():\n    from sympy.geometry.util import closest_points\n    from sympy.geometry import Point2D\n    p = [Point2D(i) for i in [(0, 0), (10, 0), (0, 10), (10, 10)]]\n    assert closest_points(*p) == {(Point2D(0, 0), Point2D(0, 10))}\n", "def test_are_coplanar_with_3_points():\n    from sympy.geometry import Point3D\n    from sympy.geometry.util import are_coplanar\n\n    p1, p2, p3 = Point3D(0, 0, 0), Point3D(1, 1, 1), Point3D(2, 2, 2)\n    assert are_coplanar(p1, p2, p3)\n", "def test_farthest_points_set_with_ties():\n    p1 = Point2D(0, 0)\n    p2 = Point2D(0, 1)\n    p3 = Point2D(1, 1)\n    p4 = Point2D(1, 0)\n\n    points = [p1, p2, p3, p4]\n    result = farthest_points(*points)\n    expected_result = {(p1, p4)}\n    assert result == expected_result\n", "def test_convex_hull_error_handling():\n    from sympy.geometry.util import convex_hull\n    from sympy.geometry.point import Point\n    points = [(1, 1), (1, 2), (3, 1), (-5, 2)]\n    # Test that a ValueError is raised when the points cannot be sorted.\n    raises(ValueError, convex_hull, *points, polygon=False)\n    # Test that a ValueError is raised when there is a mix of Point and non-Point.\n    raises(ValueError, convex_hull, points[0], Point(1, 2))\n    # Test that a ValueError is raised when there are less than two points.\n    raises(ValueError, convex_hull, points[0])\n    # Test that a ValueError is raised when the points are not 2D.\n    raises(ValueError, convex_hull, Point(1, 2, 3))\n", "def test_convex_hull_polygon_and_point():\n    from sympy.geometry import Point, Segment, Polygon, convex_hull, Point2D\n    p = Polygon((0, 0), (1, 1), (2, 1), (1, 0))\n    q = Point(0, 0)\n    assert isinstance(convex_hull(p, q), Polygon)\n\n"], "sample_301": ["    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.tempdir)\n", "    def test_autoreload_started_signal(self):\n        with mock.patch.object(autoreload, 'logger') as mock_logger:\n            reloader = get_reloader()\n            reloader.run_loop = lambda: None\n            reloader.run(django_main_thread=None)\n            mock_logger.info.assert_called_once_with('%s notified as changed. Signal results: %s.', 'non_existent_file', [])\n", "    def setUp(self):\n        self.test_module = tempfile.mkstemp()[1]\n        with open(self.test_module, 'w') as f:\n            f.write('def func(): pass')\n        self.addCleanup(os.remove, self.test_module)\n", "    def test_check_availability(self):\n        # Test that check_availability returns False without Watchman available\n        with mock.patch.object(pywatchman, 'client', side_effect=[OSError('Connection refused')]):\n            with self.assertRaises(WatchmanUnavailable):\n                autoreload.WatchmanReloader.check_availability()\n", "    def test_check_availability(self):\n        with mock.patch('django.utils.autoreload.pywatchman.client'):\n            with mock.patch('django.utils.autoreload.get_version_tuple'):\n                WatchmanReloader.check_availability()\n                # No exception is raised.\n"], "sample_960": ["def test_parse_annotation():\n    env = Mock()\n    annotation = 'List[int]'\n    expected = ['List', '[', 'int', ']']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = 'int | str'\n    expected = ['int', ' ', addnodes.desc_sig_punctuation('', '|'), ' ', 'str']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = 'None'\n    expected = [type_to_xref(annotation, env)]\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = 'Union[int, str]'\n    expected = ['Union', '[', 'int', ',', 'str', ']']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n\n    annotation = 'List[Union[int, str]]'\n    expected = ['List', '[', 'Union', '[', 'int', ',', 'str', ']', ']']\n    result = _parse_annotation(annotation, env)\n    assert_node(result, expected)\n", "def test_parse_invalid_signature():\n    # Test with invalid signature\n    signode = addnodes.desc_signature('invalid_signature', '')\n    signode.astext()\n    with pytest.raises(ValueError):\n        _pseudo_parse_arglist(signode, 'invalid signature')\n\n    # Test with signature without arguments\n    signode = addnodes.desc_signature('function()', '')\n    assert parse('function()') == 'function()'\n\n    # Test with signature with single argument\n    signode = addnodes.desc_signature('function(arg1)', '')\n    assert parse('function(arg1)') == 'function(arg1)'\n\n    # Test with signature with multiple arguments\n    signode = addnodes.desc_signature('function(arg1, arg2)', '')\n    assert parse('function(arg1, arg2)') == 'function(arg1, arg2)'\n\n    # Test with signature with default argument\n    signode = addnodes.desc_signature('function(arg1, arg2=arg2_default)', '')\n    assert parse('function(arg1, arg2=arg2_default)') == 'function(arg1, arg2=arg2_default)'\n\n    # Test with signature with multiple default arguments\n    signode = addnodes.desc_signature('function(arg1, arg2=arg2_default, arg3=arg3_default)', '')\n    assert parse('function(arg1, arg2=arg2_default, arg3=arg3_default)') == 'function(arg1, arg2=arg2_default, arg3=arg3_default)'\n\n    # Test with signature with arguments with parentheses\n    signode = addnodes.desc_signature('function(arg1(arg2))', '')\n    assert parse('function(arg1(arg2))') == 'function(arg1(arg2))'\n\n    # Test with signature with return type\n    signode = addnodes.desc_signature('function(arg1) -> return_type', '')\n    assert parse('function(arg1) -> return_type') == 'function(arg1) -> return_type'\n\n    # Test with signature with multiple return types\n    signode = addnodes.desc_signature('function(arg1) -> return_type1, return_type2', '')\n    assert parse('function(arg1) -> return_type1, return_type2') == 'function(arg1) -> return_type1, return_type2'\n\n    # Test with signature with return type and arguments\n    signode = addnodes.desc_signature('function(arg1) -> return_type', '')\n   ", "def test_handle_signature_method():\n    env = Mock()\n    env.ref_context['py:module'] = 'module1'\n    env.ref_context['py:class'] = 'class1'\n    env.ref_context['py:classes'] = ['class1', 'class2']\n    env.ref_context['py:modules'] = ['module1', 'module2']\n\n    # Test method\n    sig = 'method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyMethod()\n    method.handle_signature(sig, signode)\n    expected = 'async static method method'\n    assert signode.astext().startswith(expected)\n\n    # Test abstract method\n    sig = 'abstractmethod method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyMethod()\n    method.handle_signature(sig, signode)\n    expected = 'abstract async static method method'\n    assert signode.astext().startswith(expected)\n\n    # Test class method\n    sig = 'classmethod method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyMethod()\n    method.handle_signature(sig, signode)\n    expected = 'class async static method method'\n    assert signode.astext().startswith(expected)\n\n    # Test property method\n    sig = 'property method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyMethod()\n    method.handle_signature(sig, signode)\n    expected = 'property abstract async static method method'\n    assert signode.astext().startswith(expected)\n\n    # Test property method without abstract\n    sig = 'property method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyProperty()\n    method.handle_signature(sig, signode)\n    expected = 'property class method method'\n    assert signode.astext().startswith(expected)\n\n    # Test class property\n    sig = 'property method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method = PyProperty()\n    method.handle_signature(sig, signode)\n    expected = 'property abstract async static method method'\n    assert signode.astext().startswith(expected)\n\n    # Test method without prefix\n    sig = 'method(self, arg1, arg2)'\n    signode = addnodes.desc_signature(sig, '')\n    method", "def test_parse_annotation():\n    # Test that _parse_annotation parses annotations correctly\n    env = Mock()\n    env.ref_context = {'py:module': 'mymodule'}\n    annotation = '(int, str) -> bool'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, [('int', nodes.Text, ''), ('str', nodes.Text, ''), ('bool', nodes.Text, '')])\n\n    # Test that _parse_annotation handles nested annotations\n    annotation = '(int, (str, (list, str))) -> bool'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, [('int', nodes.Text, ''), ('str', nodes.Text, ''),\n                        ('list', nodes.Text, ''), ('str', nodes.Text, '')])\n\n    # Test that _parse_annotation handles ellipsis\n    annotation = '... -> bool'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, [addnodes.desc_sig_punctuation('', \"...\")])\n\n    # Test that _parse_annotation handles unsupported syntax\n    annotation = 'int | str'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, [nodes.Text(annotation)])\n\n    # Test that _parse_annotation handles unsupported syntax (Python 3.8+)\n    annotation = 'int | str'\n    result = _parse_annotation(annotation, env, env=env)\n    assert_node(result, [nodes.Text(annotation)])\n\n    # Test that _parse_annotation raises a SyntaxError\n    annotation = 'int | '\n    with pytest.raises(SyntaxError):\n        _parse_annotation(annotation, env)\n", "def test_handle_signature_with_classname_in_signature():\n    signode = addnodes.desc_signature('class MyClass(MyParent):', '')\n    obj = PyClasslike('class')\n    fullname, prefix = obj.handle_signature(signode.astext(), signode)\n    assert fullname == 'MyClass(MyParent)'\n    assert prefix == 'class '\n"], "sample_1077": ["    def test_complex_region_unions(self):\n        x, y = symbols('x y')\n        a = Interval(2, 3)\n        b = Interval(4, 5)\n        c = Interval(1, 7)\n        region1 = ComplexRegion(a*b)\n        region2 = ComplexRegion(a*c)\n        region3 = ComplexRegion(Union(a*b, a*c))\n        region4 = ComplexRegion(Union(a*b, b*c))\n        assert region1.contains(2.5 + 4.5*I)\n        assert region1.contains(2.5 + 6.5*I) == False\n        assert region3.contains(2.5 + 4.5*I)\n        assert region3.contains(2.5 + 6.5*I) == False\n        assert region4.contains(2.5 + 4.5*I)\n        assert region4.contains(2.5 + 6.5*I) == False\n        assert region1 == region1\n        assert region1 != region3\n", "def test_range_iteration():\n    r = Range(10)\n    gen = iter(r)\n    expected = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert list(gen) == expected\n", "def test_ImageSet():\n    from sympy import S, Interval, Rational, pi\n\n        return x**2 + 2*x + 1\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), r)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), i)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), r, polar=False)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), i, polar=False)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), r, polar=True)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), i, polar=True)\n    assert it.measure == 2\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), r, polar=False)\n    assert it.a_interval == i\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), i, polar=False)\n    assert it.b_interval == i\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), r, polar=True)\n    assert it.a_interval == r\n\n    r = Range(0, 2, 1)\n    i = Interval(-1, 1)\n    it = ImageSet(Lambda(x, f(x)), i, polar=True)\n    assert it.b_interval == i", "def test_normalize_theta_set():\n    # Test a simple Interval\n    assert normalize_theta_set(Interval(0, 2*pi)) == Interval(0, 2*pi)\n    # Test an interval with one cycle\n    assert normalize_theta_set(Interval(0, 10*pi)) == Interval(0, 2*pi)\n    # Test an interval with two cycles\n    assert normalize_theta_set(Interval(0, 20*pi)) == Interval(0, 2*pi)\n    # Test an interval with three cycles\n    assert normalize_theta_set(Interval(0, 30*pi)) == Interval(0, 2*pi)\n    # Test an interval that is not a multiple of pi\n    assert normalize_theta_set(Interval(0, 10)) == Interval(0, 10)\n    # Test an interval that is negative\n    assert normalize_theta_set(Interval(-10*pi, 0)) == Interval(-2*pi, 0)\n    # Test an interval that is a multiple of pi but does not start from 0\n    assert normalize_theta_set(Interval(-10*pi, -20*pi)) == Interval(-2*pi, 0)\n    # Test a FiniteSet\n    assert normalize_theta_set(FiniteSet(0, pi, 2*pi)) == FiniteSet(0, pi)\n    # Test a Union of Intervals\n    assert normalize_theta_set(Union(Interval(0, 10*pi), Interval(20*pi, 30*pi))) == Union(Interval(0, 2*pi), Interval(0, 2*pi))\n    # Test a Union of a FiniteSet and an Interval\n    assert normalize_theta_set(Union(FiniteSet(0, pi), Interval(20*pi, 30*pi))) == Union(FiniteSet(0, pi), Interval(0, 2*pi))\n    # Test a FiniteSet that is not a subset of Reals\n    assert raises(ValueError, lambda: normalize_theta_set(FiniteSet(1, 2, 3)))\n", "def test_ImageSet_lamda():\n    from sympy import symbols, pi, oo, Eq, Eq\n    from sympy.sets.fancysets import ImageSet\n    from sympy.sets.sets import FiniteSet, Interval, Union, Contains, ProductSet, imageset\n    x = symbols('x')\n    n = symbols('n', integer=True)\n    soln = ImageSet(Lambda(x, x**2 + 1), Interval(-oo, oo))\n    expected = Union(FiniteSet(1), FiniteSet(-1))\n    assert soln.equals(expected)\n"], "sample_506": ["def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = Spines.linear_spine(ax, 'bottom')\n    spine.set_bounds(0, 5)\n    assert spine.get_bounds() == (0, 5)\n\n    spine.set_bounds(5)\n    assert spine.get_bounds() == (5, 1)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds('invalid')\n\n    spine.set_bounds(0)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds(None, 5)\n    assert spine.get_bounds() == (0, 5)\n\n    spine.set_bounds(0, None)\n    assert spine.get_bounds() == (0, 1)\n\n    spine.set_bounds((0, 5))\n    assert spine.get_bounds() == (0, 5)\n\n    spine.set_bounds(5, (0, 5))\n    assert spine.get_bounds() == (0, 5)\n", "def test_spine_set_position():\n    fig, ax = plt.subplots()\n    spines = ax.spines\n    spines.left.set_position(('data', 1.0))\n    assert spines.left.get_position() == ('data', 1.0)\n    spines.left.set_position(('axes', 0.5))\n    assert spines.left.get_position() == ('axes', 0.5)\n    spines.left.set_position(('outward', 10))\n    assert spines.left.get_position() == ('outward', 10)\n", "def test_spine_bounds_no_change(spine_type):\n    # Create a new figure\n    fig, ax = plt.subplots()\n\n    # Create a spine for the specified type\n    spine = Spines.linear_spine(ax, spine_type)\n\n    # Set the spine bounds\n    spine.set_bounds(0, 1)\n\n    # Check that the bounds haven't changed\n    assert spine.get_bounds() == (0, 1)\n\n    # Check that the spine's position hasn't changed\n    assert spine.get_position() == ('outward', 0.0)\n\n", "def test_spine_set_bounds():\n    fig, ax = plt.subplots()\n    spine = ax.spines['left']\n    assert spine.get_bounds() == (None, None)\n\n    spine.set_bounds(0, 10)\n    assert spine.get_bounds() == (0, 10)\n\n    spine.set_bounds(0)\n    assert spine.get_bounds() == (0, None)\n\n    spine.set_bounds(high=10)\n    assert spine.get_bounds() == (None, 10)\n\n    spine.set_bounds((0, 10))\n    assert spine.get_bounds() == (0, 10)\n\n    spine.set_bounds((0, 10), 20)\n    assert spine.get_bounds() == (0, 20)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds('invalid')\n\n    spine.set_bounds(1, 2, 3, 4)\n    assert spine.get_bounds() == (1, 2)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds(1, 2, 3)\n\n    with pytest.raises(ValueError):\n        spine.set_bounds(1, 2, high=4)\n\n    spine.set_bounds(None, high=10)\n    assert spine.get_bounds() == (None, 10)\n", "def test_spine_position():\n    # Test setting spine positions\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.set_xlim([0, 5])\n    ax.set_ylim([0, 5])\n\n    # Test setting position to 'outward' with a value\n    ax.spines['bottom'].set_position(('outward', 10))\n\n    # Test setting position to 'axes' with a value\n    ax.spines['left'].set_position(('axes', 0.2))\n\n    # Test setting position to 'data' with a value\n    ax.spines['right'].set_position(('data', 1))\n\n    # Test setting position to 'center' and 'zero'\n    ax.spines['top'].set_position('center')\n    ax.spines['bottom'].set_position('zero')\n\n    # Test that spine position can be set with different units\n    ax.spines['left'].set_position(('outward', 10.5))\n    ax.spines['left'].set_position(('outward', 10.5, 'points'))\n    ax.spines['left'].set_position(('outward', 10.5, 'inches'))\n\n    plt.show()\n"], "sample_629": ["def test_get_python_path():\n    # Test with existing __init__.py\n    assert get_python_path(\"/path/to/project/subdir/__init__.py\") == \"/path/to/project/subdir\"\n    # Test with non-existing __init__.py\n    assert get_python_path(\"/path/to/project/subdir\") == \"/path/to/project\"\n    # Test with __init__.py in parent directory\n    assert get_python_path(\"/path/to/project\") == \"/path/to/project\"\n    # Test with __init__.py in current working directory\n    assert get_python_path(\".\") == \".\"\n", "def test_get_python_path():\n    # Test case 1: get_python_path returns the current working directory when the directory does not exist\n    assert get_python_path(\"/non/existent/directory\") == os.getcwd()\n\n    # Test case 2: get_python_path returns the parent directory when the current directory does not exist\n    dirname = os.path.dirname(\"/non/existent/directory\")\n    assert get_python_path(dirname) == dirname\n\n    # Test case 3: get_python_path returns the directory when the __init__.py exists\n    dirname = Path(__file__).parent.absolute()\n    assert get_python_path(dirname) == dirname\n\n    # Test case 4: get_python_path returns the parent directory when the __init__.py does not exist\n    parent_dirname = dirname.parent\n    assert get_python_path(parent_dirname) == parent_dirname\n", "def test_expand_modules_errors_handling():\n    ignore_list = [\"__init__.py\", \"test1.py\"]\n    ignore_list_re = [re.compile(r\"^test2\\.py$\")]\n    ignore_list_paths_re = [re.compile(r\"^test3$\")]\n    files_or_modules = [\"./test4.py\", \"./test5.py\", \"./test6.py\", \"./test7.py\", \"./test8.py\"]\n    expected_errors = [\n        {\"key\": \"fatal\", \"mod\": \"test5\", \"ex\": ImportError},\n        {\"key\": \"fatal\", \"mod\": \"test7\", \"ex\": SyntaxError},\n    ]\n\n    result, errors = expand_modules(files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re)\n\n    assert len(result) == 3\n    assert len(errors) == 2\n    for error in errors:\n        assert error in expected_errors\n    assert result[0][\"path\"] == Path(\"./test4.py\").absolute()\n    assert result[0][\"name\"] == \"test4\"\n    assert result[1][\"path\"] == Path(\"./test6.py\").absolute()\n    assert result[1][\"name\"] == \"test6\"\n    assert result[2][\"path\"] == Path(\"./test8.py\").absolute()\n    assert result[2][\"name\"] == \"test8\"\n", "def test_expand_modules_ignore_path_re():\n    \"\"\"Test that expand_modules ignores paths based on the ignore_list_paths_re\"\"\"\n    ignore_list_paths_re = [re.compile(r\"^/var/lib/\")]\n    files_or_modules = [\"/var/lib/python3.9/somefile.py\"]\n    result, errors = expand_modules(\n        files_or_modules, [], [], ignore_list_paths_re\n    )\n    assert len(result) == 0\n\n", "def test_get_python_path():\n    \"\"\"Test that get_python_path returns the correct python path\"\"\"\n    # Test case where the file is in the current working directory\n    assert get_python_path(\"\") == os.getcwd()\n    \n    # Test case where the file is in a subdirectory\n    file_path = Path(\"path/to/file.py\")\n    assert get_python_path(str(file_path)) == str(file_path.parent)\n    \n    # Test case where the file does not have an __init__.py\n    # (should return the parent directory)\n    file_path = Path(\"path/to/directory\")\n    assert get_python_path(str(file_path)) == str(file_path.parent)\n    \n    # Test case where the file has an __init__.py\n    file_path = Path(\"path/to/directory/__init__.py\")\n    assert get_python_path(str(file_path)) == str(file_path.parent)\n    \n    # Test case where the file is in a subdirectory of a package\n    file_path = Path(\"path/to/package/subdirectory/__init__.py\")\n    assert get_python_path(str(file_path)) == str(file_path.parent)\n"], "sample_262": ["    def test_lazy_object_init(self):\n            return object()\n        lazy_obj = SimpleLazyObject(func)\n        self.assertEqual(lazy_obj._wrapped, empty)\n        self.assertEqual(lazy_obj._setupfunc, func)\n", "    def test_lazy_object_repr(self):\n            return object()\n\n        lazy_obj = SimpleLazyObject(setup_func)\n        self.assertEqual(repr(lazy_obj), '<SimpleLazyObject: <object object at 0x...>')\n", "    def test_lazy_object_repr(self):\n        class TestLazyObject(LazyObject):\n                self._wrapped = \"test\"\n\n        lazy_obj = TestLazyObject()\n        self.assertEqual(repr(lazy_obj), \"<SimpleLazyObject: 'test'>\")\n\n        class TestLazyObjectInit(LazyObject):\n                self._wrapped = \"test\"\n\n                pass\n\n        lazy_obj = TestLazyObjectInit()\n        self.assertEqual(repr(lazy_obj), \"<TestLazyObjectInit: 'test'>\")\n", "    def test_lazy_object_repr(self):\n            return object()\n\n        lazy_object = SimpleLazyObject(get_object)\n        self.assertEqual(repr(lazy_object), \"<SimpleLazyObject: <function get_object at 0x...>\")\n", "    def test_lazy_unpickle_lazyobject(self):\n        wrapped = object()\n        self.assertEqual(unpickle_lazyobject(wrapped), wrapped)\n"], "sample_670": ["def test_parse_error_unexpected_char():\n    with pytest.raises(ParseError):\n        evaluate(\"a b\", lambda x: x == \"a\")\n", "def test_empty_expression():\n    with pytest.raises(ParseError):\n        evaluate(\"\", lambda s: False)\n", "def test_not_with_eof():\n    input_str = \"~\"\n    with pytest.raises(ParseError) as excinfo:\n        evaluate(input_str, lambda x: True)\n    assert excinfo.value.column == 1\n    assert excinfo.value.message == \"expected identifier; got end of input\"\n", "def test_expression_empty_input(mocker):\n    matcher = mocker.Mock(return_value=True)\n    assert not evaluate(\"\", matcher)\n\n", "def test_not_expr_errors():\n    \"\"\"Test that not_expr raises an exception when given an invalid input.\"\"\"\n    with pytest.raises(ParseError):\n        evaluate(\"not not\", lambda x: True)\n    with pytest.raises(ParseError):\n        evaluate(\"not (\", lambda x: True)\n    with pytest.raises(ParseError):\n        evaluate(\"( not\", lambda x: True)\n    with pytest.raises(ParseError):\n        evaluate(\"not not\", lambda x: True)\n"], "sample_327": ["    def test_uuid_field_valid_input(self):\n        field = UUIDField()\n        self.assertEqual(field.to_python('123e4567-e89b-12d3-a456-426655440000'), uuid.UUID('123e4567-e89b-12d3-a456-426655440000'))\n        self.assertEqual(field.to_python(''), None)\n        self.assertEqual(field.to_python(None), None)\n", "    def test_json_field_encoder(self):\n        json_field = JSONField(encoder=DjangoJSONEncoder)\n        data = {'a': 1, 'b': 2}\n        serialized = json_field.prepare_value(data)\n        self.assertIsInstance(serialized, str)\n        self.assertEqual(json.loads(serialized, cls=DjangoJSONEncoder), data)\n", "    def test_valid_choice(self):\n        field = ChoiceField(choices=((1, 'One'), (2, 'Two'), (3, 'Three')))\n        self.assertEqual(field.to_python('One'), 1)\n        self.assertEqual(field.to_python('Two'), 2)\n        self.assertEqual(field.to_python('Three'), 3)\n        self.assertEqual(field.to_python('Four'), '')\n        self.assertEqual(field.to_python(None), '')\n        self.assertEqual(field.to_python(''), '')\n", "    def test_Field_init(self):\n        field = Field(required=False, label='my_label')\n        self.assertFalse(field.required)\n        self.assertEqual(field.label, 'my_label')\n        self.assertEqual(field.widget, TextInput)\n        self.assertEqual(field.hidden_widget, HiddenInput)\n", "    def test_json_field_default_encoder(self):\n        # Test that the default JSONField uses the standard JSONEncoder\n        json_field = JSONField()\n        data = {'foo': 'bar'}\n        serialized_data = json_field.to_python(json.dumps(data))\n        self.assertIsInstance(serialized_data, dict)\n"], "sample_429": ["def test_decimal_validator_max_digits_none():\n    validator = DecimalValidator(max_digits=None, decimal_places=2)\n    try:\n        validator(Decimal(\"10000000000\"))\n        assert False, \"Expected ValueError when max_digits is None\"\n    except ValidationError as e:\n        assert e.code == \"max_digits\"\n\n", "compilation error", "def test_decimal_validator_without_max_digits_and_max_decimal_places(self):\n    with self.assertRaises(TypeError):\n        DecimalValidator()\n", "def test_decimal_validator_invalid_max_digits():\n    validator = DecimalValidator(max_digits=3, decimal_places=2)\n    with mock.patch.object(Decimal, \"MAX_DIGITS\", 2):\n        try:\n            validator(Decimal(\"742403889818000000\"))\n        except ValidationError as e:\n            assert e.code == \"max_digits\"\n            assert e.params[\"max\"] == 2\n        else:\n            assert False, \"Expected ValidationError\"\n", "compilation error"], "sample_502": ["def test_xticks_set_zero():\n    fig, ax = plt.subplots()\n    ax.set_xticks([0, 1, 2])\n    ax.set_xticks([3, 4, 5])\n    assert np.array_equal(ax.get_xticks(), np.array([0, 1, 2, 3, 4, 5]))\n", "def test_xlim_set_and_get():\n    ax = plt.gca()\n    assert ax.get_xlim() == (0, 1)\n    ax.set_xlim(2, 3)\n    assert ax.get_xlim() == (2, 3)\n    assert ax.get_xlim(2) == (2, 3)\n    assert ax.get_xlim(right=3) == (2, 3)\n", "def test_cmap_deprecation_warning():\n    # Test that setting a custom colormap through `set_cmap` raises a deprecation\n    # warning\n    with pytest.warns(MatplotlibDeprecationWarning):\n        plt.set_cmap('viridis')\n    plt.set_cmap('viridis')  # This should not raise a warning\n    with pytest.warns(None):\n        plt.set_cmap('viridis')  # This should not raise a warning\n\n    # Test that setting a custom colormap through `set_cmap` works as expected\n    plt.set_cmap('viridis')\n    assert plt.get_cmap().name == 'viridis'\n\n    # Test that setting a custom colormap through `set_cmap` does not interfere\n    # with the default colormap\n    plt.set_cmap('viridis')\n    plt.gca().imshow(np.random.rand(10, 10))\n    assert plt.get_cmap() is plt.gca().images[0].cmap\n\n    # Test that setting a custom colormap through `set_cmap` does not interfere\n    # with the colormap of a specific image\n    plt.set_cmap('viridis')\n    img = plt.imshow(np.random.rand(10, 10))\n    plt.gca().set_title('Default colormap')\n    assert img.cmap == plt.get_cmap()\n", "def test_xkcd_turns_off_usetex():\n    with plt.xkcd(scale=1, length=100, randomness=2):\n        with pytest.raises(RuntimeError):\n            plt.xticks(rotation=90)\n    plt.xticks(rotation=90)\n", "def test_matshow_autoaspect():\n    # Create some data\n    data = np.arange(4*4).reshape(4, 4)\n\n    # Create a new figure with auto aspect ratio\n    fig = plt.figure(figsize=(3, 3))\n\n    # Show that the aspect ratio is not preserved when saving\n    with pytest.warns(UserWarning):\n        plt.matshow(data)\n        plt.savefig('matshow_autoaspect.png')\n\n    # Show that the aspect ratio is preserved when not saving\n    fig.delaxes()\n    with pytest.warns(UserWarning):\n        plt.matshow(data)\n\n    # Show that a figure with an aspect ratio other than 1 is not modified\n    fig = plt.figure(figsize=(6, 3))\n    plt.matshow(data)\n    assert fig.get_size_inches() == (6, 3)\n"], "sample_751": ["def test_ExtraTreesClassifier_default_parameters():\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [\"foo\", \"foo\", \"foo\", 1, 1, 1]\n    et = ExtraTreesClassifier()\n    et.fit(X, y)\n    assert et.n_estimators == 10\n    assert et.max_depth is None\n    assert et.min_samples_split == 2\n    assert et.min_samples_leaf == 1\n    assert et.min_weight_fraction_leaf == 0.0\n    assert et.max_features == \"auto\"\n    assert et.max_leaf_nodes is None\n    assert et.min_impurity_decrease == 0.0\n    assert et.min_impurity_split is None\n", "def test_ForestClassifier():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n    forest = ForestClassifier(base_estimator=None, n_estimators=1)\n    with assert_raises(ValueError):\n        forest.fit(X, y)\n", "def test_RandomForestClassifier_default_parameters():\n    X, y = datasets.load_iris(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n    forest = RandomForestClassifier()\n    forest.fit(X_train, y_train)\n    y_pred = forest.predict(X_test)\n    assert_equal(len(y_pred), X_test.shape[0])\n", "    def test_init(self):\n        embedding = RandomTreesEmbedding(n_estimators=10)\n        assert embedding.sparse_output == True\n        assert embedding.n_jobs == 1\n        assert embedding.random_state == None\n        assert embedding.verbose == 0\n        assert embedding.warm_start == False\n", "def test_base_forest__base_estimator_not_fitted():\n    # Create a random forest regressor with a non-fitted estimator\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=0)\n    forest = RandomForestRegressor(base_estimator=DecisionTreeRegressor())\n    forest.fit(X, y)\n    assert_raises(NotFittedError, forest.predict, X)\n"], "sample_474": ["    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.author_with_full_name = Author.objects.create(\n            name=\"John Smith\", alias=\"smithj\"\n        )\n        cls.author_with_empty_string = Author.objects.create(alias=\"\")\n        cls.author_with_none_value = Author.objects.create(alias=None)\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n", "    def setUpTestData(cls):\n        cls.john = Author.objects.create(name=\"John Smith\", alias=\"smithj\")\n        cls.elena = Author.objects.create(name=\"\u00c9lena Jordan\", alias=\"elena\")\n        cls.rhonda = Author.objects.create(name=\"Rhonda\")\n"], "sample_333": ["    def test_media_attribute(self):\n        form = Form()\n        media = form.media\n        self.assertTrue(isinstance(media, Media))\n\n        form = Person()\n        media = form.media\n        self.assertTrue(isinstance(media, Media))\n        for field in form.fields.values():\n            self.assertIn(media, field.widget.media)\n\n        form = PersonNew()\n        media = form.media\n        self.assertTrue(isinstance(media, Media))\n        for field in form.fields.values():\n            self.assertIn(media, field.widget.media)\n", "    def test_bound_field_name(self):\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        bound_field = form['first_name']\n        self.assertEqual(bound_field.name, 'first_name')\n        self.assertEqual(bound_field.html_name, 'first_name_id')\n", "    def test_default_renderer(self):\n        class TestForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n\n        test_form = TestForm()\n        self.assertEqual(test_form.renderer, get_default_renderer())\n\n        test_form = TestForm(renderer=DjangoTemplates())\n        self.assertEqual(test_form.renderer, DjangoTemplates())\n", "    def test_render_form(self):\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '1990-01-01'})\n        rendered_form = form.as_table()\n        self.assertIn('John', rendered_form)\n        self.assertIn('Doe', rendered_form)\n        self.assertIn('1990-01-01', rendered_form)\n", "    def test_fields_order(self):\n        form = PersonNew(initial={'first_name': 'John'})\n        self.assertEqual(form.fields['first_name'].widget.attrs['id'], 'first_name_id')\n        self.assertNotEqual(form.fields['last_name'].widget.attrs.get('id'), 'last_name_id')\n        self.assertEqual(form.fields['birthday'].widget.attrs.get('id'), None)\n\n        form.order_fields(['first_name', 'last_name', 'birthday'])\n        self.assertEqual(form.fields['first_name'].widget.attrs['id'], 'first_name_id')\n        self.assertEqual(form.fields['last_name'].widget.attrs['id'], 'last_name_id')\n        self.assertEqual(form.fields['birthday'].widget.attrs.get('id'), 'birthday_id')\n"], "sample_326": ["    def test_urlize_with_nofollow(self):\n        self.check_output(urlize, 'http://example.com', '<a href=\"http://example.com\">http://example.com</a>')\n        self.check_output(urlize, 'https://example.com', '<a href=\"https://example.com\">https://example.com</a>')\n        self.check_output(urlize, 'www.example.com', '<a href=\"http://www.example.com\">www.example.com</a>')\n        self.check_output(urlize, 'example.com', '<a href=\"http://example.com\">example.com</a>')\n", "def test_trim_url_limit(self):\n    \"\"\"\n    Test the trim_url function used within urlize.\n\n    \"\"\"\n    self.assertEqual(trim_url(\"short_url\"), \"short_url\")\n    self.assertEqual(trim_url(\"long_url_longer_than_trim_limit\", 10), \"long_...\")\n\n    with self.assertRaises(TypeError):\n        trim_url(\"test\")\n        trim_url(\"test\", \"invalid_limit\")\n        trim_url(\"test\", None)\n        trim_url(\"test\", 10, \"additional_arg\")\n", "    def test_json_script(self):\n        value = {'key': 'value'}\n        element_id = 'example-id'\n        expected_output = '<script id=\"example-id\" type=\"application/json\">&nbsp;=&quot;value&quot;</script>'\n        self.check_output(json_script, value, expected_output)\n", "    def test_strip_spaces_between_tags_with_empty_input(self):\n        self.assertEqual(strip_spaces_between_tags(''), '')\n", "    def check_output(self, function, value, output=None):\n        \"\"\"\n        function(value) equals output. If output is None, function(value)\n        equals value.\n        \"\"\"\n        if output is None:\n            output = value\n        self.assertEqual(function(value), output)\n"], "sample_563": ["def test_packer_mode():\n    # Test fixed mode\n    packer = VPacker(pad=1, sep=1, mode='fixed')\n    packer.width = 100\n    packer.height = 100\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='red'))\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='blue'))\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='green'))\n    packer.draw(plt.gcf().canvas.get_renderer())\n    assert packer.get_bbox(plt.gcf().canvas.get_renderer()).x1 == 4\n\n    # Test expand mode\n    packer = HPacker(pad=1, sep=1, mode='expand')\n    packer.width = 100\n    packer.height = 100\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='red'))\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='blue'))\n    packer.add_artist(mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='green'))\n    packer.draw(plt.gcf().canvas.get_renderer())\n    assert packer.get_bbox(plt.gcf().canvas.get_renderer()).x1 == 100\n\n    # Test equal mode\n    packer = VPacker(pad=1, sep=1, mode='equal', children=[mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='red'),\n                                                         mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='blue'),\n                                                         mpatches.FancyBboxPatch((0, 0, 1, 1), facecolor='green')])\n    packer.width = 100\n    packer.height = 100\n    packer.draw(plt.gcf().canvas.get_renderer())\n    assert packer.get_bbox(plt.gcf().canvas.get_renderer()).y1 == 100\n\n    # Test invalid mode\n    packer = VPacker(pad=1, sep=1, mode='invalid')\n    with pytest.raises(ValueError):\n       ", "def test_packed_offsets():\n    # Verify that _get_packed_offsets works correctly\n    # for the 'fixed' mode\n    offsets, total = _get_packed_offsets([10, 20, 30], None, 1)\n    assert_allclose(offsets, [0, 11, 21])\n    assert_allclose(total, 32)\n\n    # Verify that _get_packed_offsets works correctly\n    # for the 'expand' mode\n    offsets, total = _get_packed_offsets([10, 20, 30], None, 1, mode='expand')\n    assert_allclose(offsets, [0, 2, 4])\n    assert_allclose(total, 6)\n\n    # Verify that _get_packed_offsets works correctly\n    # for the 'equal' mode\n    offsets, total = _get_packed_offsets([10, 20, 30], None, 1, mode='equal')\n    assert_allclose(offsets, [10, 20, 30])\n    assert_allclose(total, 60)\n\n    # Verify that _get_packed_offsets works correctly\n    # when total is given\n    offsets, total = _get_packed_offsets([10, 20, 30], 100, 1, mode='equal')\n    assert_allclose(offsets, [10, 30, 50])\n    assert_allclose(total, 90)\n\n    # Verify that _get_packed_offsets raises a ValueError\n    # when sep is None and total is None\n    with pytest.raises(ValueError):\n        _get_packed_offsets([10, 20, 30], None, None, mode='equal')\n\n    # Verify that _get_packed_offsets raises a ValueError\n    # when total is given and sep is None\n    with pytest.raises(ValueError):\n        _get_packed_offsets([10, 20, 30], 100, None, mode='equal')\n\n    # Verify that _get_packed_offsets raises a ValueError\n    # when total is given and sep is None and mode is 'equal'\n    with pytest.raises(ValueError):\n        _get_packed_offsets([10, 20, 30], 100, None, mode='equal', total=None)\n\n    # Verify that _get_packed_offsets raises a ValueError\n    # when mode is not 'fixed', 'expand' or 'equal'\n    with pytest.raises(ValueError):\n        _get_packed_offsets([10", "def test_paddedbox_pad_value(pad, expected_bbox):\n    fig, ax = plt.subplots()\n    box = PaddedBox(child=mpatches.Circle((0, 0), 10), pad=pad)\n    box.set_figure(fig)\n    box.set_axes(ax)\n    bbox = box.get_bbox(ax.get_renderer())\n    assert bbox == expected_bbox\n", "def test_get_packed_offsets():\n    widths = [10, 20, 30]\n    total, offsets = _get_packed_offsets(widths, 100, 2, 'fixed')\n    assert_allclose(total, 62.0)\n    assert_allclose(offsets, [0, 12, 24])\n\n    total, offsets = _get_packed_offsets(widths, 100, 2, 'expand')\n    assert_allclose(total, 100)\n    assert_allclose(offsets, [0, 20, 40])\n\n    total, offsets = _get_packed_offsets(widths, 100, 2, 'equal')\n    assert_allclose(total, 120)\n    assert_allclose(offsets, [20, 40, 60])\n\n    with pytest.raises(ValueError):\n        _get_packed_offsets(widths, None, 2, 'equal')\n\n    with pytest.raises(ValueError):\n        _get_packed_offsets(widths, 100, None, 'equal')\n", "def test_hpacker():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    box1 = mpatches.Rectangle((0, 0), 2, 1, facecolor='red', edgecolor='black')\n    box2 = mpatches.Rectangle((0, 2), 2, 1, facecolor='green', edgecolor='black')\n    box3 = mpatches.Rectangle((0, 4), 2, 1, facecolor='blue', edgecolor='black')\n    hp = HPacker(\n        pad=0.5,\n        sep=1,\n        width=5,\n        mode='fixed',\n        children=[box1, box2, box3]\n    )\n    hp.set_figure(fig)\n    ax.add_artist(hp)\n    hp.set_offset((2, 3))\n    hp.draw(fig.canvas.get_renderer())\n\n    # test packed boxes\n    assert_allclose(hp.get_children()[0].get_bbox(fig.canvas.get_renderer()).bounds, (1.2, -1.2, 2, 1))\n    assert_allclose(hp.get_children()[1].get_bbox(fig.canvas.get_renderer()).bounds, (1.2, 0.8, 2, 1))\n    assert_allclose(hp.get_children()[2].get_bbox(fig.canvas.get_renderer()).bounds, (1.2, 2.2, 2, 1))\n\n    # test packing with given total width\n    hp = HPacker(\n        pad=0.5,\n        sep=1,\n        width=6,\n        mode='fixed',\n        children=[box1, box2, box3]\n    )\n    hp.set_figure(fig)\n    ax.add_artist(hp)\n    hp.set_offset((2, 3))\n    hp.draw(fig.canvas.get_renderer())\n\n    # test packed boxes\n    assert_allclose(hp.get_children()[0].get_bbox(fig.canvas.get_renderer()).bounds, (1.2, -1.2, 2, 1))\n    assert_allclose(hp.get_children()[1].get_bbox(fig.canvas.get_renderer()).bounds, (2.2, 0.8, 2, 1))\n    assert_allclose(hp.get_children()[2].get_bbox(fig.canvas.get_renderer()).bounds, (3.2, 2.2, 2, 1"], "sample_1080": ["def test_refine_Pow_real_base_and_even_exponent():\n    x = Symbol('x', real=True)\n    expr = ((-1)**x)\n    assert refine(expr, Q.real(x) & Q.even(x)) == 1\n    expr = ((-1)**(x+1))\n    assert refine(expr, Q.real(x) & Q.even(x)) == -1\n    expr = ((-1)**(x+2))\n    assert refine(expr, Q.real(x) & Q.even(x)) == 1\n", "def test_refine_exp():\n    x = Symbol('x', real = True)\n    assert refine(exp(x), Q.positive(x)) == exp(x)\n    assert refine(exp(x), Q.negative(x)) == exp(x)\n    assert refine(exp(x), Q.zero(x)) == exp(x)\n    assert refine(exp(x), Q.real(x)) == exp(x)\n    assert refine(exp(x), Q.imaginary(x)) == exp(x)\n\n    y = Symbol('y', imaginary = True)\n    assert refine(exp(y), Q.positive(im(y))) == exp(y)\n    assert refine(exp(y), Q.negative(im(y))) == exp(y)\n    assert refine(exp(y), Q.zero(im(y))) == exp(y)\n    assert refine(exp(y), Q.real(y)) == exp(y)\n    assert refine(exp(y), Q.imaginary(y)) == exp(y)\n\n    assert refine(exp(x), Q.positive(x) & Q.imaginary(x)) == exp(x)\n    assert refine(exp(x), Q.negative(x) & Q.imaginary(x)) == exp(x)\n    assert refine(exp(x), Q.zero(x) & Q.imaginary(x)) == exp(x)\n    assert refine(exp(x), Q.real(x) & Q.imaginary(x)) == exp(x)\n    assert refine(exp(x), Q.real(x) & Q.positive(x) & Q.imaginary(x)) == exp(x)\n\n    assert refine(exp(x + y), Q.real(x + y)) == exp(x + y)\n    assert refine(exp(x + y), Q.imaginary(x + y)) == exp(x + y)\n    assert refine(exp(x + y), Q.positive(x + y)) == exp(x + y)\n    assert refine(exp(x + y), Q.negative(x + y)) == exp(x + y)\n    assert refine(exp(x + y), Q.zero(x + y)) == exp(x + y)\n    assert refine(exp(x + y), Q.real(x + y) & Q.imaginary(x + y)) == exp(x + y)\n", "def test_refine_Pow_n_zero():\n    x = Symbol('x', real=True)\n    expr = S.NegativeOne ** (x + 1)\n    expected = S.NegativeOne\n    assert refine(expr, Q.positive(x)) == expected\n    assert refine(expr, Q.negative(x)) == expected\n", "def test_refine_Pow_with_zero_power():\n    x = Symbol('x', real=True)\n    expr = x**0\n    assumptions = [Q.real(x)]\n    result = refine(expr, assumptions)\n    assert result == S.One\n", "def test_refine_Pow_even_exponent():\n    x, y = symbols('x y')\n    expr = (-1)**(x + y)\n    result = refine(expr, Q.even(x) & Q.odd(y))\n    assert result == (-1)**y\n"], "sample_7": ["def test_column_attrs_equal():\n    # Test that attrs_equal returns True when comparing two columns with the same\n    # attributes, and False otherwise\n    c1 = table.Column(data=[1, 2, 3], name='test', unit=u.m, format='%0.2f')\n    c2 = table.Column(data=[1, 2, 3], name='test', unit=u.m, format='%0.2f')\n    assert c1.attrs_equal(c2)\n\n    c3 = table.Column(data=[1, 2, 3], name='test', unit=u.m, format='%.3f')\n    assert not c1.attrs_equal(c3)\n\n    c4 = table.Column(data=[1, 2, 3], name='test', unit=u.s, format='%0.2f')\n    assert not c1.attrs_equal(c4)\n\n    c5 = table.Column(data=[1, 2, 3], name='other', unit=u.m, format='%0.2f')\n    assert not c1.attrs_equal(c5)\n\n    with pytest.raises(ValueError):\n        table.Column(data=[1, 2, 3], name='test', unit=u.m, format='%0.2f').attrs_equal(5)\n\n    with pytest.raises(ValueError):\n        table.Column(data=[1, 2, 3], name='test', unit=u.m, format='%0.2f').attrs_equal(table.Column(data=[1, 2, 3], name='test', unit=u.s, format='%0.2f'))\n", "    def test_attrs_equal_scalar(self):\n        col = table.Column(np.arange(10))\n        new_col = col.copy()\n        assert col.attrs_equal(new_col)\n", "    def test_copy(self):\n        col = table.Column(np.arange(5), name='test_col')\n        col.meta = {'key': 'value'}\n        col.indices = [table.ColumnIndex([1, 2])]\n\n        new_col = table.Column.copy(col, copy=True)\n        assert not new_col is col\n        assert new_col.meta == {'key': 'value'}\n        assert new_col.indices == [table.ColumnIndex([1, 2])]\n        assert_array_equal(new_col.data, col.data)\n\n        new_col = table.Column.copy(col, copy=False)\n        assert not new_col is col\n        assert new_col.meta == {}\n        assert new_col.indices == []\n        assert_array_equal(new_col.data, col.data)\n", "    def test_copy_with_mask(self):\n        # Test copying a column with a mask.\n        data = np.ma.array([1, 2, 3], mask=[False, True, False])\n        col = table.Column(data=data, name='test', dtype=float)\n        col_copy = col.copy()\n        self.assertTrue(np.ma.allclose(col.data.mask, col_copy.data.mask))\n", "def test_column_convert_unit_to():\n    # Create a simple column\n    c = table.Column([1, 2, 3], name='test', unit=u.m, dtype=float)\n\n    # Test unit conversion\n    new_unit = u.km\n    expected = np.array([1e3, 2e3, 3e3])\n    assert np.array_equal(c.convert_unit_to(new_unit).value, expected)\n\n    # Test unit conversion with equivalencies\n    new_unit = u.km\n    expected = np.array([1e3, 2e3, 3e3])\n    equivalencies = [u.spectral, u.spectral_density(1e-2)]\n    assert np.array_equal(c.convert_unit_to(new_unit, equivalencies=equivalencies).value, expected)\n\n    # Test unit conversion with invalid unit\n    with pytest.raises(u.UnitsError):\n        c.convert_unit_to('invalid_unit')\n\n    # Test unit conversion with invalid equivalencies\n    with pytest.raises(u.UnitsError):\n        c.convert_unit_to(new_unit, equivalencies=['invalid_equivalence'])\n\n    # Test unit conversion with None unit\n    c.unit = None\n    with pytest.raises(ValueError):\n        c.convert_unit_to(new_unit)\n\n    # Test unit conversion with no unit and no equivalencies\n    c.unit = None\n    c.equivalencies = []\n    c._set_cache = None\n    new_unit = u.km\n    c.to(new_unit, equivalencies=c.equivalencies)\n    expected = np.array([1e3, 2e3, 3e3])\n    assert np.array_equal(c.quantity.value, expected)\n"], "sample_955": ["compilation error", "def test_visit_ListComp(source, expected):\n    node = ast.parse(source).body[0]\n    unparser = _UnparseVisitor()\n    assert unparser.visit(node) == expected\n", "compilation error", "compilation error", "compilation error"], "sample_605": ["def test_groupby_quantile_dataset():\n    ds = xr.Dataset(\n        {\"a\": ((\"x\",), np.array([1, 2, 3, 4]))},\n        {\"x\": [0, 1, 2, 3]},\n    )\n\n    groupby = ds.groupby(\"x\")\n\n    da = groupby.quantile(0.5)\n    expected = xr.DataArray(\n        [2.0, 3.0], dims=[\"x\"], coords={\"x\": [0, 1, 2, 3]}\n    )\n    assert_identical(da, expected)\n\n    da = groupby.quantile([0, 0.5, 1])\n    expected = xr.DataArray(\n        [[1.0, 2.0, 3.0],\n         [2.0, 3.0, 4.0],\n         [3.0, 4.0, 5.0]], dims=[\"x\", \"quantile\"], coords={\"x\": [0, 1, 2, 3], \"quantile\": [0.0, 0.5, 1.0]}\n    )\n    assert_identical(da, expected)\n", "def variable():\n    coords = {\"x\": [\"a\", \"b\", \"c\"], \"y\": [1, 2, 3, 4]}\n    dims = [\"x\", \"y\"]\n    data = np.random.randn(3, 4)\n    var = xr.DataArray(data, dims=dims, coords=coords)\n    return var\n\n", "def test_groupby_dataset_reduce():\n    ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\", \"z\"), np.random.randn(3, 4, 2))},\n        {\"x\": [\"a\", \"b\", \"c\"], \"y\": [1, 2, 3, 4], \"z\": [1, 2]},\n    )\n    ds[\"boo\"] = ((\"z\", \"y\"), [[\"f\", \"g\", \"h\", \"j\"]] * 2)\n\n    g = ds.groupby(\"z\")\n\n    result = g.reduce(np.mean, dim=\"z\", keep_attrs=True)\n    assert_identical(result, ds[\"foo\"].mean(dim=\"z\"), \"Reduction should preserve attrs\")\n\n    result = g.reduce(np.mean, dim=\"z\", keep_attrs=False)\n    assert_identical(result, ds[\"foo\"].mean(dim=\"z\"), \"Reduction should not preserve attrs\")\n\n    result = g.reduce(np.mean, dim=\"y\", keep_attrs=True)\n    assert_identical(result, ds[\"foo\"].mean(dim=\"y\"), \"Reduction should preserve attrs\")\n\n    result = g.reduce(np.mean, dim=\"y\", keep_attrs=False)\n    assert_identical(result, ds[\"foo\"].mean(dim=\"y\"), \"Reduction should not preserve attrs\")\n\n    result = g.reduce(lambda x: x.sum(dim=\"y\") * 2, dim=\"z\")\n    assert_identical(result, ds[\"foo\"].sum(dim=\"y\").sum(dim=\"z\") * 2, \"Custom reduction should work\")\n\n    with raises_regex(ValueError, r\"can't specify both `grouper` and `bins`\"):\n        g.reduce(lambda x: x.sum(dim=\"y\"), dim=\"z\", bins=[1, 2, 3])\n\n    with raises_regex(ValueError, r\"cannot reduce over dimensions\"):\n        g.reduce(lambda x: x.sum(dim=\"y\"), dim=[\"z\", \"y\"])\n\n    result = g.reduce(lambda x: x.sum(dim=\"y\"), dim=\"z\")\n    assert_identical(result, ds[\"foo\"].sum(dim=\"y\"), \"Reduction should work on single dimension\")\n\n    ds[\"foo\"].coords[\"new_coord\"] = (\"x\", [\"a\", \"b\", \"c\"])\n    result = g.reduce(lambda x: x.sum(dim=\"y\"), dim=\"z\")\n    assert_identical(result, ds[\"foo\"].sum(dim=\"y\"), \"Reduction should work", "def test_groupby_quantile_array():\n    \"\"\"Test that `GroupBy.quantile` works correctly with array inputs.\"\"\"\n    # Test that quantile returns scalar\n    array = xr.DataArray(np.array([1, 2, 3]))\n    grouped = array.groupby(\"x\")\n    assert grouped.quantile(0.5).values == 2\n\n    # Test that quantile returns array\n    array = xr.DataArray(np.array([[1, 2], [3, 4]]))\n    grouped = array.groupby(\"x\")\n    result = grouped.quantile([0, 0.5, 1])\n    expected = xr.DataArray([[1, 1], [3, 3]])\n    assert_equal(result, expected)\n\n    # Test that quantile raises error with invalid quantile value\n    array = xr.DataArray(np.array([1, 2, 3]))\n    with raises_regex(ValueError, r\"quantile must be between 0 and 1\"):\n        grouped = array.groupby(\"x\")\n        grouped.quantile(2)\n\n    # Test that quantile with multiple dimensions works\n    array = xr.DataArray(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\n    grouped = array.groupby(\"x\")\n    result = grouped.quantile([0, 0.5, 1], dim=\"y\")\n    expected = xr.DataArray([[[1, 1], [3, 3]], [[5, 5], [7, 7]]])\n    assert_equal(result, expected)\n\n    # Test that quantile with invalid dim raises error\n    array = xr.DataArray(np.array([1, 2, 3]))\n    with raises_regex(ValueError, r\"Dimension 'z' not found in\"):\n        grouped = array.groupby(\"x\")\n        grouped.quantile([0, 0.5, 1], dim=\"z\")\n", "def test_groupby_quantile(dataset, array):\n    ds = dataset.groupby(\"y\")\n    da = ds.quantile([0.0, 0.5, 1.0])\n\n    assert_equal(ds[\"foo\"].dims, da.dims)\n    assert_equal(ds[\"foo\"].coords, da.coords)\n    assert_equal(ds[\"foo\"].attrs, da.attrs)\n\n    # Test quantile on a dataset with a variable as groupby dimension\n    ds = dataset.groupby(\"foo\")\n    da = ds.quantile([0.0, 0.5, 1.0])\n    assert_equal(ds[\"foo\"].dims, da.dims)\n    assert_equal(ds[\"foo\"].coords, da.coords)\n    assert_equal(ds[\"foo\"].attrs, da.attrs)\n\n    # Test that groups with no data in them are included in the output\n    # Create a dataset with a variable that is all NaN\n    ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\", \"z\"), np.random.randn(3, 4, 2))},\n        {\"x\": [\"a\", \"b\", \"c\"], \"y\": [1, 2, 3, 4], \"z\": [1, 2]},\n        attrs={\"foo\": {\"foo\": 0}}\n    )\n    ds[\"boo\"] = ((\"z\", \"y\"), [[\"f\", \"g\", \"h\", \"j\"]] * 2)\n    ds[\"foo\"] = ((\"z\", \"y\"), np.full((2, 4), np.nan))\n\n    ds = ds.groupby(\"y\")\n    da = ds.quantile([0.0, 0.5, 1.0])\n    assert_equal(ds[\"foo\"].dims, da.dims)\n    assert_equal(ds[\"foo\"].coords, da.coords)\n    assert_equal(ds[\"foo\"].attrs, da.attrs)\n\n    # Test that quantile is called on the correct variable\n    ds = dataset.groupby(\"foo\")\n    da = ds.quantile([0.0, 0.5, 1.0])\n    assert_equal(ds[\"foo\"].dims, da.dims)\n    assert_equal(ds[\"foo\"].coords, da.coords)\n    assert_equal(ds[\"foo\"].attrs, da.attrs)\n\n    # Test that quantile raises an error when q is out of range\n    ds = dataset.groupby(\"foo\")\n    with raises_regex(ValueError, r\"q must be between 0"], "sample_130": ["    def test_build_lookup_with_transform(self):\n        query = Query(Author)\n        query.add_fields(['name'])\n        with self.assertRaises(FieldError):\n            query.build_lookup(['iexact'], query.select[0], 'russell')\n", "    def test_add_fields_and_select_related(self):\n        # Test that adding fields and select related works when using a previous\n        # select_related call\n        q = Query(model=Author)\n        q.add_select_related('items')\n        q.add_fields(['name', 'items__name'])\n        expected_sql = 'SELECT \"author\".\"name\", \"author_items\".\"name\" FROM \"author\" LEFT OUTER JOIN \"author_items\" ON \"author_items\".\"author_id\" = \"author\".\"id\"'\n        self.assertEqual(str(q), expected_sql)\n", "    def test_add_annotation(self):\n        q = Query(Author)\n        q.add_annotation(Lower('name'), alias='lower_name')\n        self.assertIn('lower_name', q.annotation_select)\n        self.assertIn(Lower('name'), q.annotations.values())\n        self.assertEqual(q.annotation_select_mask, {'lower_name'})\n        q.clear_annotation_mask()\n        self.assertNotIn('lower_name', q.annotation_select)\n        self.assertNotIn(Lower('name'), q.annotations.values())\n", "    def test_build_lookup_and_filter(self):\n        # Test that build_lookup returns the correct lookup object for 'isnull' lookup.\n        q = Query(Author)\n        q.build_lookup(['isnull'], F('id'), True)\n        self.assertIsInstance(q.where.children[0].children[0], RelatedIsNull)\n\n        # Test that build_lookup returns the correct lookup object for 'isnull'\n        # with Negated lookup.\n        q = Query(Author)\n        q.where.children[0].negated = True\n        q.build_lookup(['isnull'], F('id'), True)\n        self.assertIsInstance(q.where.children[0].children[0], RelatedIsNull)\n        self.assertTrue(q.where.children[0].children[0].negated)\n\n        # Test that build_lookup handles None values correctly.\n        q = Query(Author)\n        with self.assertRaises(FieldError):\n            q.build_lookup(['isnull'], F('id'), None)\n\n        # Test that build_lookup handles empty string values correctly for Oracle.\n        q = Query(Author)\n        connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls = True\n        q.build_lookup(['exact'], F('id'), '')\n        self.assertIsInstance(q.where.children[0].children[0], RelatedIsNull)\n\n        # Test that build_lookup handles empty string values correctly for non-Oracle databases.\n        q = Query(Author)\n        connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls = False\n        with self.assertRaises(FieldError):\n            q.build_lookup(['exact'], F('id'), '')\n\n        # Test that build_lookup raises a FieldError when a lookup is not supported.\n        q = Query(Author)\n        with self.assertRaises(FieldError):\n            q.build_lookup(['exact'], F('id'), 'invalid')\n\n        # Test that build_lookup raises a FieldError when a lookup is not supported with Negated lookup.\n        q = Query(Author)\n        q.where.children[0].negated = True\n        with self.assertRaises(FieldError):\n            q.build_lookup(['exact'], F('id'), 'invalid')\n\n        # Test that build_filter raises a FieldError when a lookup is not supported.\n        q = Query(Author)\n        with self.assertRaises(FieldError):\n            q.build_filter((Q(id=F('id')), 'invalid'))\n\n        # Test that build_filter raises a FieldError when a lookup is not supported with Negated lookup.\n        q = Query(Author)\n        q", "    def test_join_promotion(self):\n        query = Query(Author.objects.model)\n        query.add_fields(['id', 'name'])\n        query.add_filter('id__exact', 1)\n        query.add_filter('name__startswith', 'A')\n        query._filtered_relations['id'] = FilteredRelation(\n            condition=Q(id__exact=2),\n            relation_name='id',\n            join_type=INNER,\n        )\n        query._lookup_joins = ['id']\n        self.assertEqual(query.alias_map['id'].join_type, LOUTER)\n        self.assertEqual(query.alias_map['name'].join_type, INNER)\n\n        # Test that filterable expressions do not get converted to Not(Exact)\n        # in the WHERE clause.\n        query.add_filter('id__gt', 1)\n        self.assertEqual(query.where[0][0].lookup_name, 'exact')\n        self.assertEqual(query.where[0][0].rhs, 1)\n\n        # Test that annotations are joined correctly.\n        query.add_annotation(F('name').upper(), 'name_upper')\n        query.add_filter('name_upper__exact', 'A')\n        self.assertEqual(query.where[0][0].lookup_name, 'exact')\n        self.assertEqual(query.where[0][0].rhs, 'A')\n\n        # Test that the set_group_by method correctly expands the GROUP BY\n        # clause.\n        query.add_fields(['id', 'name'])\n        query.add_annotation(F('name').upper(), 'name_upper')\n        query.set_group_by()\n        self.assertEqual(query.group_by, ('id', 'name_upper'))\n        self.assertEqual(query.annotation_select, {'name_upper': F('name').upper()})\n"], "sample_574": ["    def x(self):\n        return pd.Series([1, 3, 9, 27], name=\"x\", dtype=float)\n", "def test_continuous_label_formatter_like(self, x):\n\n    s = Continuous().label(like=\"%.2f\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    expected = np.array([\"1.00\", \"3.00\", \"9.00\"])\n    assert_series_equal(labels, expected)\n\n    s = Continuous().label(like=\"${x:.2f}\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    expected = np.array([\"$1.00\", \"$3.00\", \"$9.00\"])\n    assert_series_equal(labels, expected)\n\n    s = Continuous().label(like=lambda x: f\"X{x:.2f}\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    expected = np.array([\"X1.00\", \"X3.00\", \"X9.00\"])\n    assert_series_equal(labels, expected)\n\n    s = Continuous().label(like=lambda x: f\"{x:.2f}\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    expected = np.array([\"1.00\", \"3.00\", \"9.00\"])\n    assert_series_equal(labels, expected)\n\n    s = Continuous().label(like=\"%.2f\", base=10)._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    expected = np.array([\"10.00\", \"30.00\", \"90.00\"])\n    assert_series_equal(labels, expected)\n", "def test_continuous_label_like(self):\n\n    x = pd.Series([1, 3, 9], name=\"x\", dtype=float)\n\n    s = Continuous(label=Continuous().label(like=\"d\"))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert_array_equal(labels, [0.0, 1.0, 2.0])\n\n    s = Continuous(label=Continuous().label(like=lambda x: str(x**2)))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert_array_equal(labels, ['1.0', '9.0', '81.0'])\n\n    s = Continuous(label=Continuous().label(like=\"{x:.2f}\"))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert_array_equal(labels, ['1.00', '3.00', '9.00'])\n\n    s = Continuous(label=Continuous().label(like=\"{pos:.2f}\"))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert_array_equal(labels, ['0.00', '0.33', '1.00'])\n\n    s = Continuous(label=Continuous().label(like=\"{x:.0f}\"))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n\n    assert_array_equal(labels, ['1', '3', '9'])\n\n    s = Continuous(label=Continuous().label(like=\"{x:.2f}e\"))._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0,", "def test_continuous_label_formatter_like(self):\n    x = pd.Series([1, 3, 9], name=\"x\", dtype=float)\n    s = Continuous().label(like=\".2f\")._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    locs = a.major.locator()\n    labels = a.major.formatter.format_ticks(locs)\n    expected = np.array([\"1.00\", \"3.00\", \"9.00\"])\n    assert_series_equal(labels, expected)\n", "def test_continuous_label_base(self):\n    x = pd.Series([1, 3, 9], name=\"x\", dtype=float)\n    s = Continuous().label(base=2)._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    formatter = a.major.formatter\n    assert formatter.get_offset() == 0\n    assert formatter.get_scientific() == False\n    assert formatter.get_use_offset() == False\n"], "sample_321": ["    def test_origin_verification_with_multiple_allowed_schemes(self):\n        request = self._get_GET_csrf_cookie_request()\n        request.META['HTTP_ORIGIN'] = 'https://allowed.example.com'\n        request.META['HTTP_HOST'] = 'allowed.example.com'\n        response = CsrfViewMiddleware().process_request(request)\n        self.assertIsNone(response)\n\n        request.META['HTTP_HOST'] = 'allowed.example.com:8080'\n        response = CsrfViewMiddleware().process_request(request)\n        self.assertIsNone(response)\n\n        request.META['HTTP_HOST'] = 'example.com'\n        with override_settings(CSRF_TRUSTED_ORIGINS=['https://allowed.example.com', 'http://allowed.example.com', 'http://example.com']):\n            response = CsrfViewMiddleware().process_request(request)\n            self.assertIsNone(response)\n", "    def test_csrf_token_length_valid(self):\n        \"\"\"Test that the token is correctly validated.\"\"\"\n        req = self._get_GET_csrf_cookie_request(cookie=self._csrf_id_cookie)\n        req.META['HTTP_REFERER'] = \"https://example.com\"\n        middleware = CsrfViewMiddleware()\n        with self.assertRaises(InvalidTokenFormat):\n            middleware._get_token(req)\n        with self.assertRaises(InvalidTokenFormat):\n            middleware._get_token(req)\n\n        # Test that an invalid character in the token raises an exception\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = \"abc1defghi2jklmno3pqrstu4vwxyz\"\n        with self.assertRaises(InvalidTokenFormat):\n            middleware._get_token(req)\n\n        # Test that an incorrect length token raises an exception\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = \"abc123\"\n        with self.assertRaises(InvalidTokenFormat):\n            middleware._get_token(req)\n\n        # Test that an empty token raises an exception\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = \"\"\n        with self.assertRaises(InvalidTokenFormat):\n            middleware._get_token(req)\n", "    def test_no_csrf_cookie_for_session_based_token(self):\n        \"\"\"Ensures no CSRF cookie is set when using session-based tokens.\"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        response = CsrfViewMiddleware().process_response(req, HttpResponse())\n        self.assertFalse(response['Set-Cookie'])\n", "    def test_csrf_use_sessions_view_no_csrf_cookie(self):\n        # CSRF_USE_SESSIONS is enabled, but there is no cookie, so the session\n        # should be used instead.\n        req = self._get_POST_no_csrf_cookie_request()\n        middleware = CsrfViewMiddleware()\n        try:\n            middleware.process_view(req, lambda x: None, (), {})\n        except RejectRequest as exc:\n            self.assertEqual(exc.reason, REASON_NO_CSRF_COOKIE)\n        else:\n            self.fail('RejectRequest was not raised')\n", "    def test_csrf_insecure_referer_https(self):\n        # Test that an HTTPS request is rejected when the referer is HTTP\n        # even when the current host matches the referer.\n        req = self._get_GET_csrf_cookie_request()\n        req.META['HTTP_REFERER'] = 'http://example.com/'\n        req.META['HTTP_ORIGIN'] = 'https://example.com'\n        req.is_secure = True\n        middleware = CsrfViewMiddleware()\n        with self.assertRaises(RejectRequest):\n            middleware.process_view(req, lambda x: None, (), {})\n        self.assertTrue(req.is_secure())\n        self.assertFalse(req.META['HTTP_REFERER'] == 'https://example.com/')\n"], "sample_1102": ["def test_poly__is_univariate():\n    # testing polymultivariate polynomials\n    assert Poly(x**2 + x + 1, x, y).is_univariate\n    assert Poly(x**2 + x + 1, x, y).is_univariate\n    assert Poly(x**2 + y*x + 1, x).is_univariate\n    assert not Poly(x**2 + y*x + 1, x, y).is_univariate\n    assert not Poly(x**2 + y**2 + 1, x, y).is_univariate\n    assert not Poly(x**2 + y**2 + z**2 + 1, x, y, z).is_univariate\n\n    # testing univariate polynomials\n    assert Poly(x**2 + x + 1, x).is_univariate\n    assert Poly(x**2 + x + 1, x).is_univariate\n    assert Poly(x**2 + y*x + 1, x).is_univariate\n    assert not Poly(x**2 + y*x + 1, x, y).is_univariate\n    assert not Poly(x**2 + y**2 + 1, x).is_univariate\n    assert not Poly(x**2 + y**2 + z**2 + 1, x).is_univariate\n", "    def test_is_ground(self):\n        x = Symbol('x')\n        f = Poly(x + 1, x)\n        assert not f.is_ground\n        assert Poly(2, x).is_ground\n        assert Poly(x, x).is_ground\n        assert Poly(x + 1, x, y).is_ground\n", "def test_polynomial__degree():\n    from sympy import sqrt\n\n    f = Poly(x**3 - 2*x**2 - x + 1, x, extension=2)\n    assert f.degree() == 3\n    assert f.degree(x) == 3\n    assert f.degree(y) == 0\n    assert f.degree(z) == 0\n    assert f.degree(z**3) == 0\n\n    f = Poly(x**4 - 2*x**3 - x**2 + x - 1, x, extension=[sqrt(2)])\n    assert f.degree() == 4\n    assert f.degree(x) == 4\n    assert f.degree(y) == 0\n    assert f.degree(sqrt(2)) == 0\n\n    f = Poly(x**2 + y**2 + x*y + y + 1, x, y)\n    assert f.degree(x) == 2\n    assert f.degree(y) == 2\n    assert f.degree(x, y) == 2\n    assert f.degree(y, x) == 2\n    assert f.degree(x, z) == 0\n    assert f.degree(z, x) == 0\n    assert f.degree(y, z) == 0\n    assert f.degree(z, y) == 0\n\n    f = Poly(x**2 + y**2 + x*y + y + 1, x, y)\n    assert f.degree_list() == (2, 2)\n\n    f = Poly(x**5 + 2*x**4 + 3*x**3 + 2*x**2 + x + 1, x)\n    assert f.total_degree() == 5\n\n    f = Poly(x**2 + y**2 + x*y + y + 1, x, y)\n    assert f.total_degree() == 2\n    assert f.total_degree(x) == 2\n    assert f.total_degree(y) == 2\n    assert f.total_degree(x, y) == 2\n    assert f.total_degree(x, z) == 0\n    assert f.total_degree(z, x) == 0\n    assert f.total_degree(y, z) == 0\n    assert f.total_degree(z, y) == 0\n\n    f = Poly(x**5 + y**5 + x**4*y + y**4 + x**3", "def test_poly_unevaluated_in_rdiv():\n    # Testing Poly is unevaluated in rdiv\n    p = Poly(x)\n    q = Poly(x**2 + 1)\n    expected = Poly(0, x, domain='QQ')\n    assert p.rdiv(q) == expected\n\n", "def test_poly_from_expr_expansion():\n    from sympy import Poly, sin\n\n    # Test that the expand argument is used when the argument is a poly with multiple variables.\n    p = Poly(sin(x*y + 1), x, y, expand=False)\n    assert p.as_expr() == sin(x*y + 1)\n\n    # Test that the expand argument is not used when the argument is a poly with a single variable.\n    p = Poly(sin(x + 1), x, expand=False)\n    assert p.as_expr() == sin(x + 1).expand()\n\n    # Test that the expand argument is not used when the argument is not a poly.\n    assert Poly(sin(x + 1), x, expand=False).as_expr() == sin(x + 1).expand()\n"], "sample_451": ["    def test_replace_metacharacters(self):\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\|bar\\)\"),\n            r\"\\(foo\\|bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\|bar\\)\"),\n            r\"\\(foo\\|bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\?bar\\)\"),\n            r\"\\(foo?bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\*bar\\)\"),\n            r\"\\(foo*bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\^bar\\)\"),\n            r\"\\(foo^bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\(foo\\$bar\\)\"),\n            r\"\\(foo$bar\\)\"\n        )\n        self.assertEqual(\n            replace_metacharacters(r\"\\\\\\(foo\\)\"),\n            r\"\\\\(foo\\)\"\n        )\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(\"Hello\\\\? World\\\\*\"), \"Hello? World*\")\n        self.assertEqual(replace_metacharacters(\"\\\\?Hello World\"), \"\\\\?Hello World\")\n        self.assertEqual(replace_metacharacters(\"(Hello World\\\\*\"), \"(Hello World*\")\n        self.assertEqual(replace_metacharacters(\"(Hello World)\"), \"(Hello World)\")\n        self.assertEqual(replace_metacharacters(\"(?Hello World)\"), \"(?Hello World)\")\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a|b\\)\\.c\"),\n            r\"\\(a|b\\).c\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a|b\\)c\"),\n            r\"\\(a|b\\)c\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a|b\\)(c\"),\n            r\"\\(a|b\\)(c\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\|b\\)(c\"),\n            r\"\\(a\\|b\\)(c\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\|b\\)(c\"),\n            r\"\\(a\\)\\|b\\)(c\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\b\"),\n            r\"\\(a\\)\\b\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\B\"),\n            r\"\\(a\\)\\B\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\W\"),\n            r\"\\(a\\)\\W\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\W\"),\n            r\"\\(a\\)\\W\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\w\"),\n            r\"\\(a\\)\\w\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\W\"),\n            r\"\\(a\\)\\W\",\n        )\n\n        self.assertEqual(\n            replace_metacharacters(r\"\\(a\\)\\W\"),\n            r\"\\(a\\)\\W\",\n        )\n", "    def test_replace_metacharacters(self):\n        self.assertEqual(replace_metacharacters(\"^a\\\\?b*\"), \"^a?b*\")\n        self.assertEqual(replace_metacharacters(\"\\\\?^\"), \"\\\\^\")\n        self.assertEqual(replace_metacharacters(\"\\\\( \\\\) \\\\{ \\\\} \\\\[ \\\\]\"), \"( ( { } [ ]\")\n", "    def test_replace_metacharacters(self):\n        test_cases = [\n            r\"\\\\\",\n            r\"\\\\?\",\n            r\"\\\\[\",\n            r\"\\\\]\",\n            r\"\\\\*\",\n            r\"\\\\+\",\n            r\"\\\\^\",\n            r\"\\$\",\n            r\"(\\\\[a-zA-Z])\",\n            r\"(\\\\[a-zA-Z\\\\])\",\n            r\"(\\\\[a-zA-Z\\\\a-zA-Z])\",\n            r\"(?:[a-zA-Z])\",\n            r\"([a-zA-Z])\",\n            r\"(?:[a-zA-Z])\",\n            r\"([a-zA-Z])\",\n            r\"(?P<test>\\\\[a-zA-Z])\",\n            r\"(?P<test>\\\\[a-zA-Z\\\\])\",\n            r\"(?P<test>\\\\[a-zA-Z\\\\a-zA-Z])\",\n            r\"(?:(?P<test>[a-zA-Z])\",\n            r\"(?P<test>[a-zA-Z])\",\n            r\"(?:(?P<test>[a-zA-Z])\",\n            r\"([a-zA-Z])\",\n        ]\n        for pattern in test_cases:\n            self.assertEqual(replace_metacharacters(pattern), pattern.replace(\"\\\\\", \"\"))\n"], "sample_1127": ["compilation error", "    def test_composition_series(self):\n        from sympy.combinatorics.named_groups import CyclicGroup\n        C = CyclicGroup(6)\n        series = C.composition_series()\n        self.assertEqual(len(series), 3)\n        self.assertEqual(series[0].order(), 6)\n        self.assertEqual(series[1].order(), 3)\n        self.assertEqual(series[2].order(), 2)\n        S = SymmetricGroup(6)\n        series = S.composition_series()\n        self.assertEqual(len(series), 4)\n        self.assertEqual(series[0].order(), 6)\n        self.assertEqual(series[1].order(), 3)\n        self.assertEqual(series[2].order(), 2)\n        self.assertEqual(series[3].order(), 1)\n        D = DihedralGroup(8)\n        series = D.composition_series()\n        self.assertEqual(len(series), 3)\n        self.assertEqual(series[0].order(), 8)\n        self.assertEqual(series[1].order(), 4)\n        self.assertEqual(series[2].order(), 2)\n", "compilation error", "def test_order():\n    # Define a permutation group\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2))\n\n    # Check if the order of the group is correct\n    assert G.order() == 6\n\n    # Check if the order of the group is correct when the group is\n    # generated by two cycles of length 3\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(1, 2, 0))\n    assert G.order() == 6\n\n    # Check if the order of the group is correct when the group is\n    # generated by two transpositions\n    G = PermutationGroup(Permutation(0, 1), Permutation(0, 2))\n    assert G.order() == 6\n\n    # Check if the order of the group is correct for a group with\n    # multiple generators\n    G = PermutationGroup(Permutation(0, 1, 2), Permutation(0, 2, 1),\n                        Permutation(0, 1))\n    assert G.order() == 6\n\n    # Check if the order of the group is correct for a group with\n    # multiple generators and cycles of different lengths\n    G = PermutationGroup(Permutation(0, 1, 2, 3), Permutation(0, 1),\n                        Permutation(1, 2))\n    assert G.order() == 24\n\n    # Test the `index` method\n    G1 = PermutationGroup(Permutation(0, 1, 2))\n    G2 = PermutationGroup(Permutation(1), Permutation(2))\n    assert G1.index(G2) == 3\n\n    # Test the `index` method when the subgroups are not subgroups\n    G1 = PermutationGroup(Permutation(0, 1, 2))\n    G2 = PermutationGroup(Permutation(0, 2))\n    with XFAIL('Subgroup index is not correctly calculated for non-subgroups'):\n        assert G1.index(G2) == 3\n", "compilation error"], "sample_351": ["    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n", "    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n\n        cls.author1 = Author.objects.create(name='Author 1')\n        cls.author2 = Author.objects.create(name='Author 2')\n        cls.author3 = Author.objects.create(name='Author 3')\n\n        cls.book1 = Book.objects.create(\n            title='Book 1',\n            authors=[cls.author1],\n            categories=[cls.c1]\n        )\n        cls.book2 = Book.objects.create(\n            title='Book 2',\n            authors=[cls.author2],\n            categories=[cls.c2]\n        )\n        cls.book3 = Book.objects.create(\n            title='Book 3',\n            authors=[cls.author3],\n            categories=[cls.c3]\n        )\n\n        cls.article1 = Article.objects.create(title='Article 1', authors=[cls.author1], categories=[cls.c1])\n        cls.article2 = Article.objects.create(title='Article 2', authors=[cls.author2], categories=[cls.c2])\n        cls.article3 = Article.objects.create(title='Article 3', authors=[cls.author3], categories=[cls.c3])\n", "    def setUpTestData(cls):\n        cls.writer = Writer.objects.create(name='Test Writer')\n        cls.writer_book = Book.objects.create(title='Test Book', writer=cls.writer)\n        cls.category = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.category_book = Book.objects.create(title='Test Book 2', category=cls.category)\n", "def test_ModelChoiceField_empty_label(self):\n    form = forms.Form({\n        'field': '',\n    }, {'field': 'some_value'})\n    field = ModelChoiceField(self.c1, empty_label='No choice')\n    with self.assertRaises(ValidationError):\n        field.clean(form.cleaned_data['field'])\n\n    field = ModelChoiceField(self.c1, empty_label=None)\n    field.clean(form.cleaned_data['field'])\n\n    field = ModelChoiceField(self.c1, empty_label='')\n    field.clean(form.cleaned_data['field'])\n\n    field = ModelChoiceField(self.c1, empty_label='   ')\n    field.clean(form.cleaned_data['field'])\n\n    field = ModelChoiceField(self.c1, empty_label='0')\n    field.clean(form.cleaned_data['field'])\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(\n            first_name='Test Author',\n            last_name='Test Last Name',\n            age=30,\n        )\n        cls.book = Book.objects.create(\n            title='Test Book',\n            publication_date=datetime.date.today(),\n            author=cls.author,\n            category=Category.objects.first(),\n            isbn='978-1234567890'\n        )\n"], "sample_178": ["    def make_customformset(\n            self, formset_data=None, formset_class=ChoiceFormSet,\n            total_forms=None, initial_forms=0, max_num_forms=0, min_num_forms=0, **kwargs):\n        \"\"\"\n        Make a ChoiceFormset from the given formset_data.\n        The data should be given as a list of (choice, votes) tuples.\n        \"\"\"\n        kwargs.setdefault('prefix', 'choices')\n        kwargs.setdefault('auto_id', False)\n        kwargs.setdefault('form_kwargs', {})\n\n        if formset_data is None:\n            return formset_class(**kwargs)\n\n        if total_forms is None:\n            total_forms = len(formset_data)\n\n            args = (kwargs['prefix'],) + args\n            return '-'.join(args)\n\n        data = {\n            prefixed('TOTAL_FORMS'): str(total_forms),\n            prefixed('INITIAL_FORMS'): str(initial_forms),\n            prefixed('MAX_NUM_FORMS'): str(max_num_forms),\n            prefixed('MIN_NUM_FORMS'): str(min_num_forms),\n        }\n        for i, (choice, votes) in enumerate(formset_data):\n            data[prefixed(str(i), 'choice')] = choice\n            data[prefixed(str(i), 'votes')] = votes\n\n        return formset_class(data, **kwargs)\n", "    def test_management_form_is_bound(self):\n        management_form = ManagementForm(auto_id='id_test')\n        self.assertFalse(management_form.is_bound)\n", "    def test_add_fields_with_deletion_and_ordering(self):\n        formset_class = formset_factory(CustomKwargForm, formset=BaseFormSet, can_order=True, can_delete=True)\n        formset = formset_class(extra=2)\n\n        form = formset.forms[0]\n        self.assertIn('custom_kwarg', form.fields)\n        self.assertIn('ORDER', form.fields)\n        self.assertIn('DELETE', form.fields)\n\n        form = formset.forms[1]\n        self.assertIn('custom_kwarg', form.fields)\n        self.assertIn('ORDER', form.fields)\n        self.assertIn('DELETE', form.fields)\n", "    def test_form_kwargs(self):\n        formset = ChoiceFormSet(prefix='test', auto_id=False, extra=2)\n        self.assertEqual(formset.get_form_kwargs(0), {'auto_id': 'test', 'prefix': 'test-0'})\n        self.assertEqual(formset.get_form_kwargs(1), {'auto_id': 'test', 'prefix': 'test-1'})\n        self.assertEqual(formset.get_form_kwargs(2), {'auto_id': 'test', 'prefix': 'test-2'})\n", "    def test_initial_forms_property(self):\n        formset = ChoiceFormSet()\n        with self.subTest(msg='no forms'):\n            self.assertEqual(formset.initial_forms, [])\n        formset = ChoiceFormSet(initial=[('choice1', 1), ('choice2', 2)])\n        with self.subTest(msg='some forms'):\n            self.assertEqual(formset.initial_forms, [formset.forms[0]])\n            self.assertEqual(len(formset.initial_forms), 1)\n"], "sample_533": ["def test_contour_labeler_too_close():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.linspace(0, 1, 10), np.linspace(0, 1, 10))\n    z = np.sin(x) * np.cos(y)\n    cs = ax.contour(x, y, z, levels=10)\n\n    labeler = cs.clabel()\n    labeler.add_label_near(0.5, 0.5, inline=True)\n    labeler.add_label_near(0.6, 0.6, inline=True)\n\n    assert len(labeler.labelXYs) == 1\n    assert not labeler.too_close(0.5, 0.5, 10)\n\n    # Test too close with a second label\n    assert labeler.too_close(0.5, 0.5, 10)  # This should be True\n\n    labeler.pop_label(0)\n    assert not labeler.too_close(0.5, 0.5, 10)\n", "def test_contour_labeler_inline_spacing():\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    fig, ax = plt.subplots()\n    ax.contour(x, y, y, 5, inline_spacing=1)\n    plt.show()\n", "def test_contour_labeler_manual():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2 * np.pi, 100)\n    y = np.sin(x)\n    cs = ax.contour(x, y, 6)\n    with rc_context(rc={'savefig.dpi': 300}):\n        im = ax.imshow(np.sin(x[:, np.newaxis] + np.linspace(0, 2 * np.pi, 100)), extent=(0, 2 * np.pi, 0, 2 * np.pi))\n        fig.colorbar(im)\n    ax.set_title('Test of Contour Labeler Manual')\n    cs.clabel(manual=[(0.5, 0.5), (0.7, 0.7)])\n    plt.show()\n    # Save the figure to a file for comparison\n    test_dir = plt.get_current_fig_manager().savefig_defaults['fname'].split('.')[0]\n    fig.savefig(test_dir + '_contour_labeler_manual.png', bbox_inches='tight')\n", "def test_contour_labeling_manual_mode():\n    # Test that labels are added correctly in manual mode\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    z = np.sin(x) + np.sin(y)\n    fig, ax = plt.subplots()\n    cs = ax.contour(x, y, z, levels=5, zorder=0)\n    cs.clabel(manual=True)\n\n        for label in cs.ax.texts:\n            if np.allclose(label.get_position(), (x, y)):\n                assert label.get_text() == label_text\n                return True\n        return False\n\n    # Check that the first click adds a label\n    click_event = plt.gcf().canvas.mpl_connect(\"button_press_event\", lambda event: None)\n    cs.add_label_near(5, 0.5, False, 5)\n    plt.canvas.draw_idle()\n    assert check_label(5, 0.5, '2.0')\n    plt.canvas.mpl_disconnect(click_event)\n\n    # Check that the second click ends the labeling\n    click_event = plt.gcf().canvas.mpl_connect(\"button_press_event\", lambda event: None)\n    cs.add_label_near(5, 0.5, False, 5)\n    plt.canvas.draw_idle()\n    assert not check_label(5, 0.5, '2.0')\n    plt.canvas.mpl_disconnect(click_event)\n\n    # Check that removing the last label works\n    cs.pop_label(-1)\n    assert not check_label(5, 0.5, '2.0')\n\n    # Check that the third click removes the last label\n    click_event = plt.gcf().canvas.mpl_connect(\"button_press_event\", lambda event: None)\n    cs.add_label_near(5, 0.5, False, 5)\n    plt.canvas.draw_idle()\n    assert check_label(5, 0.5, '2.0')\n    click_event = plt.gcf().canvas.mpl_connect(\"button_press_event\", lambda event: None)\n    cs.add_label_near(5, 0.5, False, 5)\n    plt.canvas.draw_idle()\n    assert not check_label(5, 0.5, '2.0')\n    plt.canvas.mpl_disconnect(click_event)\n\n    # Check that entering the keyboard ends the labeling\n    click_event = plt", "def test_contour_set_autolev(levels, expected_levels):\n    with rc_context(rc={'contour.linewidth': 1, 'contour.negative_linestyle': 'solid'}):\n        ax = plt.gca()\n        quad_contour_set = ContourSet(ax, levels, np.arange(10), np.arange(10).reshape(10, 1))\n        quad_contour_set._autolev(5)\n        assert_array_almost_equal(quad_contour_set.levels, expected_levels)\n"], "sample_856": ["def test_time_series_split_with_default_max_train_size():\n    \"\"\"Check if TimeSeriesSplit uses the default max_train_size value\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([1, 2, 3, 4])\n    tscv = TimeSeriesSplit()\n    indices = np.arange(4)\n    expected = [(indices[:1], indices[1:2]),\n                (indices[:2], indices[2:3]),\n                (indices[:3], indices[3:4])]\n    for train, test in tscv.split(X):\n        assert_array_equal(train, expected.pop(0))\n    assert not expected\n", "    def test_grid_search_cv(self):\n        X, y = make_classification(n_samples=100, n_features=20, n_informative=10)\n        param_grid = {'n_estimators': [10, 20, 30]}\n        grid_search = GridSearchCV(Ridge(), param_grid, cv=KFold(n_splits=5))\n        grid_search.fit(X, y)\n        assert grid_search.best_score_ > 0.5\n        assert len(grid_search.best_params_) == 1\n", "def test_leave_p_out_with_p_larger_than_samples():\n    with pytest.raises(ValueError):\n        LeavePOut(p=10)\n", "    def test_init(self):\n        test_fold = [0, 1, 2, 0, 1]\n        ps = PredefinedSplit(test_fold)\n        assert ps.test_fold.shape == (5,)\n        assert ps.test_fold[1] == 1\n        assert ps.unique_folds.shape == (3,)\n        assert ps.unique_folds[0] == 0\n", "    def test_init(self):\n        # Test that KFold raises a ValueError if n_splits is not an integer.\n        with assert_raise_message(ValueError,\n                               \"The number of folds must be of Integral type.\",\n                               \"n_splits\"):\n            KFold(n_splits='3')\n\n        # Test that KFold raises a ValueError if n_splits is less than 2.\n        with assert_raise_message(ValueError, \"k-fold cross-validation requires at least one\",\n                                \"n_splits\"):\n            KFold(n_splits=1)\n\n        # Test that KFold raises a TypeError if shuffle is not a boolean.\n        with assert_raise_message(TypeError, \"shuffle must be True or False;\",\n                                \"shuffle\"):\n            KFold(n_splits=3, shuffle='True')\n"], "sample_796": ["def test_huber_regressor_fit_intercept_false():\n    \"\"\"Test HuberRegressor with fit_intercept=False\"\"\"\n    X, y = make_regression_with_outliers(n_samples=100, n_features=20)\n    huber = HuberRegressor(fit_intercept=False)\n    huber.fit(X, y)\n    assert huber.coef_.shape == (20,)\n    assert huber.intercept_ == 0.0\n    assert huber.scale_ > 0.0\n    assert huber.outliers_.shape == (100,)\n\n    # Test with different values of epsilon\n    for epsilon in [1.0, 1.35, 1.5]:\n        huber = HuberRegressor(epsilon=epsilon, fit_intercept=False)\n        huber.fit(X, y)\n        assert huber.coef_.shape == (20,)\n        assert huber.intercept_ == 0.0\n        assert huber.scale_ > 0.0\n        assert huber.outliers_.shape == (100,)\n\n    # Test with different values of alpha\n    for alpha in [0.0, 0.0001, 0.001]:\n        huber = HuberRegressor(alpha=alpha, fit_intercept=False)\n        huber.fit(X, y)\n        assert huber.coef_.shape == (20,)\n        assert huber.intercept_ == 0.0\n        assert huber.scale_ > 0.0\n        assert huber.outliers_.shape == (100,)\n\n", "def test_huber_regressor_fit_intercept_zero():\n    # Create a dataset with outliers and fit_intercept set to False\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    huber = HuberRegressor(fit_intercept=False)\n    huber.fit(X, y)\n    assert huber.intercept_ == 0.0\n    assert huber.scale_ > 0.0\n    assert huber.coef_.shape == (X.shape[1],)\n    assert huber.outliers_.shape == y.shape\n    assert_greater(huber.score(X, y), 0.0)\n    # Check if the model is robust to outliers\n    X_outliers = X.copy()\n    X_outliers[:10, :] = 10 * np.random.rand(10, X.shape[1])\n    y_outliers = 10 * np.random.rand(10)\n    huber.fit(X_outliers, y_outliers)\n    assert_greater(huber.score(X_outliers, y_outliers), 0.0)\n", "def test_huber_regressor_fit_intercept_false():\n    X, y = make_regression_with_outliers(n_samples=100, n_features=20)\n    # Ensure that fit_intercept=False case is tested with some data.\n    model = HuberRegressor(fit_intercept=False)\n    model.fit(X, y)\n    assert model.coef_.shape == (X.shape[1],)\n    assert model.intercept_ == 0.0\n    assert model.scale_ > 0.0\n    assert model.outliers_.shape == y.shape\n", "def test_huberRegressor_score():\n    # Generate data with 10 outliers\n    X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n    # Replace 10% of the samples with noise to make them outliers\n    num_noise = int(0.1 * X.shape[0])\n    outliers = np.random.choice(X.shape[0], num_noise, replace=False)\n    X[outliers, :] = 2.0 * np.random.normal(0, 1, (num_noise, X.shape[1]))\n    y[outliers] = np.random.uniform(-10, 10, num_noise)\n    \n    # Create a Huber regressor with a small epsilon to emphasize outliers\n    huber = HuberRegressor(epsilon=1.0)\n    huber.fit(X, y)\n\n    # The score should be lower than the R-squared score of a linear regression\n    # on the inliers, since the Huber regressor tries to minimize the median\n    # absolute error, not the mean squared error\n    linear = LinearRegression().fit(X, y)\n    assert huber.score(X, y) < linear.score(X[y < 0], y[y < 0])\n    \n    # Check that the score is not greater than 1\n    assert_greater(huber.score(X, y), -1)\n", "def test_huber_regression_on_sparse_matrix():\n    \"\"\"Test HuberRegressor with a sparse matrix\"\"\"\n    # Create a regression problem with a sparse matrix\n    X, y, coef = make_regression(\n        n_samples=100, n_features=20, noise=0.05, random_state=0, n_informative=10)\n    X_sparse = sparse.csr_matrix(X)\n    y = 2.0 * y\n\n    # Create a Huber regressor\n    huber = HuberRegressor(max_iter=100, warm_start=False)\n\n    # Fit the model on the sparse matrix\n    huber.fit(X_sparse, y)\n    # The Huber regressor should not have any issues with sparse matrices\n    assert huber.coef_.shape == (20,)\n    assert huber.intercept_ == 0.0\n    assert huber.scale_ > 0.0\n\n    # Now, create a sparse matrix with only 2 features and check that the\n    # model can still fit\n    X_sparse_2feat = sparse.csr_matrix(np.random.rand(100, 2))\n    y = np.random.rand(100)\n    huber = HuberRegressor(max_iter=100, warm_start=False)\n    huber.fit(X_sparse_2feat, y)\n    assert huber.coef_.shape == (2,)\n"], "sample_697": ["def test_get_user():\n    \"\"\"Test the get_user function\"\"\"\n    # Test case where getpass.getuser() is available\n    import getpass\n    expected_user = getpass.getuser()\n    assert get_user() == expected_user\n\n    # Test case where getpass.getuser() is not available\n    try:\n        del sys.modules['getpass']\n        assert get_user() is None\n    finally:\n        sys.modules['getpass'] = getpass\n\n    # Test case where getpass.getuser() raises a KeyError\n    try:\n        getpass.getuser = lambda: None\n        assert get_user() is None\n    finally:\n        del getpass.getuser\n\n    # Test case where getpass.getuser() raises an ImportError\n    try:\n        getpass = None\n        assert get_user() is None\n    finally:\n        getpass = getpass\n", "def test_get_user(monkeypatch: MonkeyPatch) -> None:\n    \"\"\"Test that get_user() returns the expected user name.\"\"\"\n    # Test a real user name.\n    user_name = \"test_user\"\n    monkeypatch.setenv(\"USER\", user_name)\n    assert get_user() == user_name\n\n    # Test that get_user() returns None when getuser() fails.\n    monkeypatch.setenv(\"USER\", \"\")\n    assert get_user() is None\n\n    # Test that get_user() returns None when getpass.getuser() raises an exception.\n    monkeypatch.setenv(\"USER\", \"test_user\")\n    getpass_mock = monkeypatch.setattr(\"getpass.getuser\", side_effect=RuntimeError(\"Mocked exception\"))\n    assert get_user() is None\n    getpass_mock.assert_called_once()\n\n    # Test that get_user() returns None when getpass.getuser() returns an empty string.\n    monkeypatch.setenv(\"USER\", \"\")\n    assert get_user() is None\n", "def test_get_user_env_non_empty(tmp_path: Path) -> None:\n    \"\"\"Test get_user() when environment variable PYTEST_DEBUG_TEMPROOT is set.\"\"\"\n    user_name = \"test_user\"\n    tmp_path.mkdir()\n    os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = str(tmp_path)\n    assert get_user() == user_name\n    os.environ.pop(\"PYTEST_DEBUG_TEMPROOT\")\n\n", "def test_get_user(monkeypatch, capsys):\n    \"\"\"Check that get_user() returns the correct user name when getuser() is available.\"\"\"\n    monkeypatch.setattr(\"getpass.getuser\", lambda: \"test_user\")\n    assert get_user() == \"test_user\"\n\n    monkeypatch.setattr(\"getpass.getuser\", lambda: \"test_user\")\n    monkeypatch.setattr(\"getpass.getuser\", lambda: \"test_user\")\n    assert get_user() is None\n\n    monkeypatch.setattr(\"getpass.getuser\", lambda: None)\n    assert get_user() is None\n", "def test_temp_path_factory_config(tmp_path_factory: TempPathFactory, capsys) -> None:\n    \"\"\"Test TempPathFactory's behavior when created from config.\"\"\"\n    # Arrange\n    config = Config()\n    config.option.basetemp = tmp_path_factory.getbasetemp().joinpath(\"custom_basetemp\")\n\n    # Act\n    factory = TempPathFactory.from_config(config)\n\n    # Assert\n    assert factory._given_basetemp == config.option.basetemp\n\n    # Test with a custom base temp directory\n    custom_basetemp = factory.getbasetemp()\n    assert custom_basetemp.name == \"custom_basetemp\"\n    assert custom_basetemp.exists()\n\n    # Test with a custom base temp directory that doesn't exist\n    config.option.basetemp = Path(\"/non/existent/path\")\n    with pytest.raises(FileNotFoundError):\n        TempPathFactory.from_config(config)\n\n    # Test with an empty base temp directory\n    config.option.basetemp = \"\"\n    with pytest.raises(ValueError):\n        TempPathFactory.from_config(config)\n"], "sample_1167": ["def test_latex_printing_of_vector_field():\n    field = CoordSys3D('R').x\n    latex = LatexPrinter()\n    assert latex._print(field) == r'\\mathbf{{x}}'\n", "def test_latex_BesselFunctions():\n    assert latex(_hprint_BesselBase(besselj(0, x), 1)) == r'J^{1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(besselj(x, 0), 1)) == r'J_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(besselk(0, x), 1)) == r'K^{1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(bessely(0, x), 1)) == r'Y^{1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(besseli(0, x), 1)) == r'I^{1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(jn(0, x), 1)) == r'J_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(jn(x, 0), 1)) == r'J_{x}\\left(0\\right)'\n    assert latex(_hprint_BesselBase(hankel1(0, x), 1)) == r'H^{(1)1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(hankel2(0, x), 1)) == r'H^{(2)1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(hn1(0, x), 1)) == r'h^{(1)1}_{0}\\left(x\\right)'\n    assert latex(_hprint_BesselBase(hn2(0, x), 1)) == r'h^{(2)1}_{0}\\left(x\\right)'\n\n", "def test_latex_printer_default_latex():\n    assert latex(latex(sympify('x**2 + 3*x - 4'))._latex_) == r'x^{2} + 3 x - 4'\n", "def test_LatexPrinter_complex_arithmetic():\n    assert latex(1 + 2 * I) == r'1 + 2 \\text{i}'\n", "def test_LatexPrinter_dirichlet_eta():\n    assert LatexPrinter()._print(dirichlet_eta(zeta(n))) == r\"\\eta^0\\left(\\zeta(n)\\right)\"\n    assert LatexPrinter()._print(dirichlet_eta(zeta(n), 2)) == r\"\\eta^{2}\\left(\\zeta(n)\\right)\"\n    assert LatexPrinter()._print(dirichlet_eta(zeta(n), -2)) == r\"\\left(\\eta^2\\left(\\zeta(n)\\right)\\right)\"\n"], "sample_143": ["    def test_unescape_string_literal_non_string_literal(self):\n        # Test that a non-string literal raises a ValueError\n        with self.assertRaises(ValueError):\n            text.unescape_string_literal('not a string literal')\n", "def test_normalize_newlines_preserves_trailing_newline(self):\n    \"\"\"Test that normalize_newlines preserves trailing newlines.\"\"\"\n    text = \"Hello\\r\\nWorld\\r\\n\"\n    expected_output = \"Hello\\nWorld\\n\"\n    self.assertEqual(normalize_newlines(text), expected_output)\n", "    def test_capfirst_preserves_case_of_nonalpha_characters(self):\n        self.assertEqual(capfirst('abc'), 'Abc')\n        self.assertEqual(capfirst('abc.'), 'Abc.')\n        self.assertEqual(capfirst('abc!'), 'Abc!')\n", "    def test_get_valid_filename_invalid_string(self):\n        self.assertEqual(get_valid_filename(''), '')\n        self.assertEqual(get_valid_filename(None), '')\n        self.assertEqual(get_valid_filename(123), '')\n        self.assertEqual(get_valid_filename(True), '')\n        self.assertEqual(get_valid_filename(1.2), '')\n        self.assertEqual(get_valid_filename([1, 2, 3]), '')\n        self.assertEqual(get_valid_filename({'a': 1, 'b': 2}), '')\n", "    def test_compress_sequence_generator(self):\n        # Test that compress_sequence yields the entire sequence\n        sequence = ['hello', 'world']\n        expected_compressed = b'hello' + b'world'\n        self.assertEqual(next(compress_sequence(sequence)), expected_compressed)\n\n        # Test that compress_sequence yields individual strings when the sequence\n        # is large\n        large_sequence = [b'hello' for _ in range(100)]\n        compressed = b''\n        for chunk in compress_sequence(large_sequence):\n            compressed += chunk\n        self.assertEqual(compressed, expected_compressed)\n"], "sample_880": ["def test_multiclass_multioutput_shape():\n    # Test that type_of_target correctly identifies multiclass-multioutput format with shape (n_samples, n_outputs)\n    # and that class_distribution returns the correct class distribution\n    X, y = datasets.make_classification(n_classes=3, n_samples=10, n_features=10)\n    y = np.array(y).reshape(-1, 3)  # Reshape to (n_samples, n_outputs)\n\n    assert type_of_target(y) == \"multiclass-multioutput\"\n\n    classes, n_classes, class_prior = class_distribution(y)\n    assert classes == [[0, 1, 2], [0, 1, 2], [0, 1, 2]]\n    assert n_classes == [3, 3, 3]\n    assert np.allclose(class_prior, np.array([[0.25, 0.25, 0.5], [0.25, 0.25, 0.5], [0.25, 0.25, 0.5]]))\n\n    # Test that type_of_target correctly identifies multiclass-multioutput format with shape (n_samples,)\n    y = y.ravel()\n    assert type_of_target(y) == \"multiclass\"\n\n    # Test that class_distribution returns the correct class distribution for multiclass format\n    classes, n_classes, class_prior = class_distribution(y[:, np.newaxis])\n    assert classes == [[0, 1, 2]]\n    assert n_classes == [3]\n    assert np.allclose(class_prior, np.array([0.25, 0.25, 0.5]))\n", "def test_type_of_target_empty_array():\n    \"\"\"Check that type_of_target correctly identifies empty arrays as continuous.\"\"\"\n    assert type_of_target(np.array([])) == \"continuous\"\n\n", "def test_is_multilabel_sparse_matrix_with_empty_rows():\n    matrix = csr_matrix(np.array([[0, 1], [1, 0]]))\n    # Check that it is considered as a multilabel format\n    assert is_multilabel(matrix) is True\n\n    # Check that it is considered as a multilabel format even if some rows are empty\n    matrix = csr_matrix(np.array([[0, 1], [0, 0]]))\n    assert is_multilabel(matrix) is True\n", "def test_class_distribution_sparse():\n    # Testing for multilabel-indicator sparse data\n    y = csr_matrix(np.array([[1, 0], [0, 1], [1, 0, 1]])\n                   ).tolil().tocsc()\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0], [1]])\n    assert_array_equal(n_classes, [1, 1])\n    assert_array_equal(class_prior, [[0.5], [0.5]])\n\n    # Testing for multilabel-indicator sparse data with explicit zero\n    y = sparse_multilable_explicit_zero.tocsc()\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0], [1]])\n    assert_array_equal(n_classes, [1, 1])\n    assert_array_equal(class_prior, [[1.0], [0.0]])\n\n    # Testing for multilabel-indicator sparse data with multiple classes\n    y = csr_matrix(np.array([[1, 0, 1], [0, 1, 0], [1, 1, 0]])\n                   ).tolil().tocsc()\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[0], [1], [2]])\n    assert_array_equal(n_classes, [1, 1, 1])\n    assert_array_equal(\n        class_prior,\n        [[0.5], [0.5], [0.0]]\n    )\n\n    # Testing for multiclass-multioutput sparse data\n    y = csr_matrix(np.array([[1, 0, 2], [1, 0, 2], [1, 0, 2]])\n                   ).tolil().tocsc()\n    classes, n_classes, class_prior = class_distribution(y)\n    assert_array_equal(classes, [[1], [0], [2]])\n    assert_array_equal(n_classes, [1, 1, 1])\n    assert_array_equal(class_prior, [[1.0], [0.0], [0.0]])\n\n    # Testing for multiclass-multioutput sparse data with multiple classes\n    y = csr_matrix(np.array([[1, 0, 2], [1, 0, 2], [0, 1, 2]])\n                   ).tolil().tocsc()\n    classes, n_classes, class", "def test_unique_labels_partial_fit():\n    \"\"\"Test that unique_labels can work with partial_fit.\"\"\"\n    from sklearn.utils.multiclass import unique_labels\n    from sklearn.linear_model import LogisticRegression\n\n    X, y = datasets.load_iris(return_X_y=True)\n    clf = LogisticRegression(max_iter=1000)\n    clf.partial_fit(X[:10], y[:10])\n    assert_array_equal(unique_labels(clf.classes_), unique_labels(y[:10]))\n\n    # Test that unique_labels is not affected by the order of classes\n    y = np.array([0, 1, 1, 2, 2])\n    assert_array_equal(unique_labels(y), unique_labels(y[[1, 2, 0, 3, 4]]))\n\n    # Test that unique_labels returns the correct result when a class is repeated\n    y = np.array([0, 0, 1, 1])\n    assert_array_equal(unique_labels(y), unique_labels(y[1:][::-1]))\n\n    # Test that unique_labels raises an error for a non-array input\n    with pytest.raises(ValueError):\n        unique_labels(1)\n"], "sample_323": ["    def test_apply_success_called_with_migration_instance(self):\n        with mock.patch.object(self.executor, 'progress_callback') as mock_callback:\n            self.executor.apply_migration(self.state, self.migration)\n            mock_callback.assert_called_once_with('apply_success', self.migration, fake=False)\n", "    def test_detect_soft_applied_initial_migration(self):\n        self.recorder.clear_applied_migrations()\n\n        # Create a state with some applied migrations.\n        state = ProjectState(real_apps=list(self.loader.unmigrated_apps))\n        for migration in self.loader.graph.forwards_plan((\"migrations\", \"0001_initial\")):\n            if migration not in self.loader.applied_migrations:\n                continue\n            state = migration.mutate_state(state, preserve=True)\n        self.recorder.record_applied(\"migrations\", \"0001_initial\")\n\n        # The initial migration is considered implicitly applied.\n        self.assertTrue(self.executor.detect_soft_applied(state, self.loader.graph.nodes[\"migrations.0001_initial\"])[0])\n", "    def test_replacements(self):\n        initial_migration = migrations.RunPython('test_function')\n        migration = migrations.RunPython('test_function')\n        replacement_migration = migrations.SquashMigration(\n            replace=migrations.RunPython('test_function'),\n            replaces=[('app_label', 'initial_migration_name')],\n            atomic=True\n        )\n        self.loader.replacements['app_label.initial_migration_name'] = [replacement_migration]\n        migration_plan = self.executor.migration_plan([('app_label', 'initial_migration_name')])\n        self.assertEqual(migration_plan, [(replacement_migration, True)])\n", "    def test_migrate_unmigrated_apps(self):\n        \"\"\"\n        Test that unmigrated apps are detected and migrated in the correct order.\n        \"\"\"\n        # Create a migration in migrations2 app that creates a model in migrations app\n        self.create_migration('0013_initial', {'migrations': 'create_model'})\n        self.create_migration('0014_initial', {'migrations2': 'create_model'})\n\n        # Create a project state with migrations2 unmigrated\n        executor = MigrationExecutor(connection)\n        state = executor._create_project_state()\n\n        # Test that the unmigrated apps are detected and migrated in the correct order\n        targets = [('migrations2', None)]\n        executor.migrate(targets, state=state)\n\n        # Check that the migrations were applied in the correct order\n        self.assertEqual(self.recorder.applied_migrations(), {'migrations': ['0013_initial'], 'migrations2': ['0014_initial']})\n\n        # Test that the unmigrated apps are still unmigrated in the final state\n        self.assertEqual(self.recorder.unapplied_migrations(), {'migrations2': ['0014_initial']})\n", "    def test_migrate_unapplied_migration_to_applied_migration(self):\n        # Ensure the migration is unapplied before migrating\n        self.loader.unapplied_migrations['migrations.m0002_initial'] = True\n\n        executor = MigrationExecutor(connection, progress_callback=mock.MagicMock())\n        state = executor._create_project_state()\n\n        executor.migrate(targets=[('migrations', 'm0002_initial')])\n\n        self.assertEqual(executor.recorder.applied_migrations(), {'migrations.m0001_initial', 'migrations.m0002_initial'})\n\n        # Verify the migration is applied\n        state = executor._create_project_state(with_applied_migrations=True)\n        self.assertEqual(state.apps.get_model('migrations', 'MyModel')._meta.db_table, 'migrations_mymodel')\n"], "sample_53": ["    def test_autocomplete_select_repr(self):\n        widget = AutocompleteSelect(Album._meta.get_field('band').remote_field, admin.site)\n        self.assertEqual(\n            repr(widget),\n            \"<AutocompleteSelect(admin.site, Album._meta.get_field('band').remote_field)>\"\n        )\n", "    def test_required_attribute_is_not_used_when_first_option_has_value(self):\n        form = RequiredBandForm({'band': 'some_band'})\n        # Check that the rendered HTML does not contain 'required'\n        self.assertNotContains(\n            form.as_p(),\n            'required',\n        )\n", "    def test_render(self):\n        date = datetime.date.today()\n        widget = DateInput(attrs={'placeholder': 'Test date'})\n        html = widget.render('date', date)\n        self.assertInHTML('<input type=\"date\"', html)\n        self.assertIn('placeholder=\"Test date\"', html)\n        self.assertIn(date.strftime('%Y-%m-%d'), html)\n", "    def test_rendering(self):\n        form = AlbumForm()\n        widget = form['band'].field.widget\n        rendered_widget = widget.render('band', None)\n        expected_widget = \"\"\"", "    def test_media(self):\n        widget = AutocompleteSelect(\n            Album._meta.get_field('band').remote_field,\n            admin.site,\n            attrs={'class': 'my-class'},\n        )\n        self.assertIsInstance(widget.media, Media)\n        self.assertIn('css', widget.media._css)\n        self.assertIn('js', widget.media._js)\n        self.assertIn('admin.widgets.select2.css', widget.media._css['css'][0])\n        self.assertIn('admin.widgets.select2.js', widget.media._js)\n"], "sample_447": ["    def test_combined_expression_equality(self):\n        from django.db.models import IntegerField, Sum\n        from django.db.models.expressions import CombinedExpression\n\n        expr1 = CombinedExpression(\n            F(\"field1\"), \"add\", F(\"field2\"), output_field=IntegerField()\n        )\n        expr2 = CombinedExpression(\n            F(\"field1\"), \"add\", F(\"field2\"), output_field=IntegerField()\n        )\n        self.assertEqual(expr1, expr2)\n\n        expr3 = CombinedExpression(\n            F(\"field1\"), \"add\", F(\"field2\"), output_field=FloatField()\n        )\n        self.assertNotEqual(expr1, expr3)\n", "    def test_window_expression_with_partition_by_and_order_by(self):\n        qs = Employee.objects.annotate(\n            average_salary=Window(\n                Avg(\"salary\"),\n                partition_by=Q(department=1),\n                order_by=F(\"age\").desc(),\n            )\n        )\n        sql, params = qs.query.as_sql(None, connection=connection)\n        self.assertIn(\"PARTITION BY\", sql)\n        self.assertIn(\"ORDER BY\", sql)\n        self.assertIn(\"age\", sql)\n", "    def setUpTestData(cls):\n        cls.store = Store.objects.create(\n            name=\"Mamma and Pappa's Books\",\n            original_opening=datetime.datetime(2022, 1, 1, 9, 0, 0),\n            friday_night_closing=datetime.time(22, 0),\n        )\n        cls.books = [\n            Book.objects.create(\n                isbn=\"1234567890\",\n                name=\"Test Book\",\n                pages=200,\n                rating=3.0,\n                price=Decimal(\"10.99\"),\n                contact=Author.objects.create(name=\"John Doe\", age=30),\n                publisher=Publisher.objects.create(name=\"Test Publisher\", num_awards=0),\n                pubdate=datetime.date(2022, 1, 1),\n            ),\n            Book.objects.create(\n                isbn=\"2345678901\",\n                name=\"Test Book 2\",\n                pages=300,\n                rating=4.0,\n                price=Decimal(\"20.99\"),\n                contact=Author.objects.create(name=\"Jane Doe\", age=25),\n                publisher=Publisher.objects.create(name=\"Test Publisher\", num_awards=0),\n                pubdate=datetime.date(2022, 1, 15),\n            ),\n            Book.objects.create(\n                isbn=\"3456789012\",\n                name=\"Test Book 3\",\n                pages=400,\n                rating=5.0,\n                price=Decimal(\"30.99\"),\n                contact=Author.objects.create(name=\"Bob Smith\", age=35),\n                publisher=Publisher.objects.create(name=\"Test Publisher\", num_awards=0),\n                pubdate=datetime.date(2022, 1, 30),\n            ),\n        ]\n        for i, book in enumerate(cls.books):\n            for j in range(i + 1, len(cls.books)):\n                book.related = cls.books[j]\n                cls.books[j].related = book\n", "    def setUpTestData(cls):\n        cls.model = model = SimpleModel(\n            raw_sql_field=RawSQL(\n                \"\"\"\n                    SELECT 'hello' AS field\n                \"\"\",\n                (),\n                output_field=CharField(),\n            )\n        )\n        model._meta.model_fields.append(model.raw_sql_field)\n", "    def test_WindowWithBooleanExpression(self):\n        # Regression test for bug #26758.\n        # This test case ensures that a Window with a boolean expression\n        # can be created and used in a query.\n        queryset = Book.objects.annotate(\n            is_picked_by_all_authors=Window(\n                expression=Sum(Case(When(authors__count=OuterRef(\"id\"), then=1), default=0)),\n                output_field=BooleanField(),\n            )\n        )\n        self.assertEqual(\n            list(queryset.values_list(\"id\", \"is_picked_by_all_authors\")),\n            [(1, True), (2, False), (3, True), (4, True), (5, True), (6, False)],\n        )\n"], "sample_277": ["def test_register_lookup(self):\n    class TestModel(models.Model):\n        class Meta:\n            app_label = 'testapp'\n            verbose_name = 'Test Model'\n\n    class TestLookup(Lookup):\n        lookup_name = 'test_lookup'\n        lookup_type = 'test_type'\n\n    TestModel.register_lookup(TestLookup())\n\n    self.assertEqual(TestModel._meta.get_field('test_lookup').lookup, TestLookup())\n    self.assertEqual(TestModel._get_lookup('test_lookup'), TestLookup())\n    self.assertIsNone(TestModel._get_lookup('non_existent_lookup'))\n\n    class TestModel2(TestModel):\n        pass\n\n    self.assertEqual(TestModel2._get_lookup('test_lookup'), TestLookup())\n\n    TestModel._unregister_lookup(TestLookup())\n\n    self.assertIsNone(TestModel._get_lookup('test_lookup'))\n    self._clear_cached_lookups()\n\n    self.assertEqual(TestModel._get_lookup('test_lookup'), TestLookup())\n", "    def test_Q_repr(self):\n        q = Q(a=1, b=2)\n        self.assertIn('a', repr(q))\n        self.assertIn('b', repr(q))\n        self.assertIn('AND', repr(q))\n", "    def test_Q_deconstruct(self):\n        q = Q(name__startswith='a') & Q(age__gt=18)\n        path, args, kwargs = q.deconstruct()\n        self.assertEqual(path, 'Q')\n        self.assertEqual(args, (Q(name__startswith='a'), Q(age__gt=18)))\n        self.assertEqual(kwargs, {'connector': 'AND'})\n", "    def test_resolve_expression(self):\n        q = Q(a__exact=1, b__gte=2)\n        qs = Q()\n        qs.add(q, Q.AND)\n        query = None  # We don't really care about the actual query object\n        allow_joins = False\n        reuse = {}\n        summarize = False\n        for_save = False\n        clause, joins = qs.resolve_expression(query, allow_joins, reuse, summarize, for_save)\n        self.assertTrue(clause is not None)\n        self.assertTrue(joins is not None)\n", "    def test_Q_resolve_expression(self):\n        # Test that resolve_expression() returns an Expression when allow_joins is True\n        q = Q(a__gt=F('b'))\n        expr = q.resolve_expression(allow_joins=True)\n        self.assertIsInstance(expr, Query)\n\n        # Test that resolve_expression() returns None when allow_joins is False\n        expr = q.resolve_expression(allow_joins=False)\n        self.assertIsNone(expr)\n\n        # Test that resolve_expression() raises an error when reuse is True\n        with self.assertRaises(ValueError):\n            q.resolve_expression(reuse=True)\n\n        # Test that resolve_expression() raises an error when query is None\n        with self.assertRaises(ValueError):\n            q.resolve_expression(query=None)\n"], "sample_205": ["    def test_init_with_string_message(self):\n        error = ValidationError('Invalid email')\n        self.assertEqual(error.message, 'Invalid email')\n        self.assertIsNone(error.code)\n        self.assertIsNone(error.params)\n        self.assertEqual(error.error_list, [error])\n", "def test_validation_error_init_message_and_code(self):\n    error = ValidationError('This is a message', code='my_code')\n    self.assertEqual(error.message, 'This is a message')\n    self.assertEqual(error.code, 'my_code')\n", "    def test_validation_error_init(self):\n        message = \"Test error message\"\n        code = \"test_code\"\n        params = {\"param1\": \"value1\", \"param2\": \"value2\"}\n        validation_error = ValidationError(message, code, params)\n        self.assertEqual(validation_error.message, message)\n        self.assertEqual(validation_error.code, code)\n        self.assertEqual(validation_error.params, params)\n\n        dict_error = {\"field1\": [\"error1\", \"error2\"], \"field2\": [\"error3\"]}\n        validation_error = ValidationError(dict_error)\n        self.assertEqual(validation_error.error_dict, dict_error)\n\n        list_error = [\"error1\", \"error2\", \"error3\"]\n        validation_error = ValidationError(list_error)\n        self.assertEqual(validation_error.error_list, list_error)\n", "    def test_init_with_code_and_params(self):\n        error = ValidationError('Test error message', code='test_code', params={'key': 'value'})\n        self.assertEqual(error.code, 'test_code')\n        self.assertEqual(error.params, {'key': 'value'})\n        self.assertEqual(error.message, 'Test error message')\n", "    def test_init_with_dict(self):\n        # Test that ValidationError can be created with a dictionary\n        error_dict = {'field1': ['error1', 'error2'], 'field2': 'error'}\n        val_err = ValidationError(error_dict)\n        self.assertEqual(val_err.error_dict, error_dict)\n"], "sample_224": ["    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='Adrian Holovaty', age=34)\n        cls.a2 = Author.objects.create(name='Jacob Kaplan-Moss', age=35)\n        cls.a3 = Author.objects.create(name='Brad Dayley', age=45)\n        cls.a4 = Author.objects.create(name='James Bennett', age=29)\n        cls.a5 = Author.objects.create(name='Jeffrey Forcier', age=37)\n        cls.a6 = Author.objects.create(name='Paul Bissex', age=29)\n        cls.a7 = Author.objects.create(name='Wesley J. Chun', age=25)\n        cls.a8 = Author.objects.create(name='Peter Norvig', age=57)\n        cls.a9 = Author.objects.create(name='Stuart Russell', age=46)\n        cls.a1.friends.add(cls.a2, cls.a4)\n        cls.a2.friends.add(cls.a1, cls.a7)\n        cls.a4.friends.add(cls.a1)\n        cls.a5.friends.add(cls.a6, cls.a7)\n        cls.a6.friends.add(cls.a5, cls.a7)\n        cls.a7.friends.add(cls.a2, cls.a5, cls.a6)\n        cls.a8.friends.add(cls.a9)\n        cls.a9.friends.add(cls.a8)\n\n        cls.p1 = Publisher.objects.create(name='Apress', num_awards=3, duration=datetime.timedelta(days=1))\n        cls.p2 = Publisher.objects.create(name='Sams', num_awards=1, duration=datetime.timedelta(days=2))\n        cls.p3 = Publisher.objects.create(name='Prentice Hall', num_awards=7)\n        cls.p4 = Publisher.objects.create(name='Morgan Kaufmann', num_awards=9)\n        cls.p5 = Publisher.objects.create(name=\"Jonno's House of Books\", num_awards=0)\n\n        cls.b1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), contact=cls.a1, publisher=cls.p1,\n            pubdate=datetime.date(2007, 12, 6)\n        )\n        cls.b2 = Book.objects.create(\n            isbn='067232959', name='Sams Teach Yourself Django in", "    def test_empty_queryset_aggregation(self):\n        qs = Book.objects.none()\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Sum('price'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Min('price'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Max('price'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Avg('price'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Sum('price'))\n        self.assertRaises(NotSupportedError, qs.aggregate, Count('id'), Func('AVG', 'price'))\n", "    def test_avg_function_with_decimal_field(self):\n        result = Book.objects.aggregate(avg_rating=Avg('rating'))\n        self.assertEqual(result['avg_rating'], Approximate(4.033333333))\n", "    def test_aggregate_expressions(self):\n        # Test aggregate expressions with various types of fields\n        author_count = Author.objects.aggregate(count=Count('id'))\n        self.assertEqual(author_count['count'], 9)\n\n        total_pages = Book.objects.aggregate(total_pages=Sum('pages'))\n        self.assertEqual(total_pages['total_pages'], 2679)\n\n        min_page_count = Book.objects.aggregate(min_pages=Min('pages'))\n        self.assertEqual(min_page_count['min_pages'], 300)\n\n        avg_rating = Book.objects.aggregate(avg_rating=Avg('rating'))\n        self.assertAlmostEqual(avg_rating['avg_rating'], 4.032, places=3)\n\n        max_rating = Book.objects.aggregate(max_rating=Max('rating'))\n        self.assertEqual(max_rating['max_rating'], 5.0)\n\n        # Test aggregate expressions with a Filter\n        max_rating = Book.objects.filter(contact__name='Wesley J. Chun').aggregate(max_rating=Max('rating'))\n        self.assertEqual(max_rating['max_rating'], 5.0)\n\n        # Test aggregate expressions with an Exclude\n        max_rating = Book.objects.exclude(contact__name='Wesley J. Chun').aggregate(max_rating=Max('rating'))\n        self.assertEqual(max_rating['max_rating'], 5.0)\n\n        # Test aggregate expressions with an Order By\n        max_rating = Book.objects.order_by('rating').aggregate(max_rating=Max('rating'))\n        self.assertEqual(max_rating['max_rating'], 5.0)\n\n        # Test aggregate expressions with a Group By\n        book_count = Book.objects.values('publisher').annotate(book_count=Count('id')).values('book_count')\n        self.assertEqual(list(book_count), [{'book_count': 1}, {'book_count': 1}, {'book_count': 2}, {'book_count': 1}, {'book_count': 1}])\n\n        # Test aggregate expressions with a Raw Query\n        raw_query = Book.objects.raw('SELECT COUNT(*) FROM books_book')\n        self.assertEqual(list(raw_query)[0]['COUNT(*)'], 6)\n", "    def test_group_by(self):\n        \"\"\"\n        Test that group_by() is working correctly.\n        \"\"\"\n        query = Author.objects.values('friends__name').annotate(count=Count('friends__name')).values('count')\n        expected_results = [\n            {'count': 1},\n            {'count': 2},\n            {'count': 3},\n            {'count': 1},\n            {'count': 1},\n            {'count': 3},\n            {'count': 2},\n            {'count': 1}\n        ]\n        self.assertEqual(list(query), expected_results)\n\n        query = Author.objects.values('friends__name').annotate(count=Count('friends__name')).values('count').order_by('count')\n        expected_results = [\n            {'count': 1},\n            {'count': 1},\n            {'count': 1},\n            {'count': 1},\n            {'count': 2},\n            {'count': 2},\n            {'count': 3},\n            {'count': 3}\n        ]\n        self.assertEqual(list(query), expected_results)\n"], "sample_766": ["def test_sparse_encode_single_atom():\n    n_components = 1\n    dictionary = rng_global.randn(n_components, n_features)\n    with TempMemmap((n_samples, n_features)) as X:\n        X[:] = X\n        code = sparse_encode(X, dictionary)\n        assert code.shape == (n_samples, n_components)\n        assert code[0, 0] > 0\n", "def test_dict_learning_n_components_none():\n    \"\"\"Test that dict_learning returns an array of shape\n    (n_samples, n_components) when n_components is None.\"\"\"\n    X = np.random.rand(100, 10)\n    alpha = 0.1\n    code, dict_ = dict_learning(X, None, alpha)\n    assert code.shape == (100, X.shape[1])\n    assert dict_.shape == (X.shape[1], 10)\n\n", "def test_dict_learning_sparse():\n    \"\"\"Check the sparsity of the sparse code\"\"\"\n    n_samples, n_features = 10, 10\n    X = rng_global.randn(n_samples, n_features)\n    n_components = 5\n    alpha = 1\n    max_iter = 100\n    tol = 1e-8\n    method = 'lars'\n    n_jobs = 1\n    code_init = np.random.rand(n_samples, n_components)\n    dict_init = np.random.rand(n_components, n_features)\n\n    sparse_code, dictionary, errors, _ = dict_learning(X, n_components,\n                                                       alpha, max_iter,\n                                                       tol, method, n_jobs,\n                                                       code_init=code_init,\n                                                       dict_init=dict_init)\n\n    assert_less(sparse_code.sum(axis=1).max(), 0.9 * n_components,\n                'Sparse code is not sparse enough')\n    assert_equal(sparse_code.shape, (n_samples, n_components),\n                'Wrong sparse code shape')\n    assert_equal(dictionary.shape, (n_components, n_features),\n                'Wrong dictionary shape')\n", "def test_sparse_encode_zeros(X):\n    \"\"\"Test sparse_encode with zero data\"\"\"\n    D = rng_global.randn(n_features, 1)\n    transform = SparseCoder(dictionary=D)\n    code = transform.transform(X)\n    expected_code = np.zeros(X.shape[0])\n    assert_array_equal(code, expected_code)\n", "def test_dict_learning_online_n_jobs():\n    \"\"\"Check that dict_learning_online works with n_jobs > 1.\"\"\"\n    n_samples, n_features = 10, 8\n    X = rng_global.randn(n_samples, n_features)\n\n    n_components = 4\n    alpha = 1\n\n    X_train, X_test = X[:n_samples // 2], X[n_samples // 2:]\n    dl = MiniBatchDictionaryLearning(n_components=n_components, alpha=alpha,\n                                    n_jobs=-1, random_state=rng_global)\n    dl.fit(X_train)\n    code = dl.transform(X_test)\n\n    dl_n_jobs = MiniBatchDictionaryLearning(n_components=n_components,\n                                           alpha=alpha, n_jobs=4,\n                                           random_state=rng_global)\n    dl_n_jobs.fit(X_train)\n    code_n_jobs = dl_n_jobs.transform(X_test)\n\n    assert_array_almost_equal(code, code_n_jobs)\n"], "sample_719": ["def test_count_vectorizer_fit_transform_non_fitted():\n    # Test that calling fit_transform() on a non-fitted vectorizer raises\n    # a ValueError\n    vectorizer = CountVectorizer()\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform(JUNK_FOOD_DOCS)\n\n", "def test_hashing_vectorizer():\n    vectorizer = HashingVectorizer()\n    X = vectorizer.fit_transform([\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"])\n    assert_equal(vectorizer.get_feature_names(), ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'lazy', 'dog'])\n    assert_equal(X.toarray().shape, (1, vectorizer.n_features))\n    assert_equal(X.nnz, 8)\n", "def test_HashingVectorizer_doc_not_analyzed_directly():\n    X = HashingVectorizer().fit_transform(ALL_FOOD_DOCS)\n    assert isinstance(X, sparse.csr_matrix)\n", "def test_HashingVectorizer_get_feature_names():\n    vectorizer = HashingVectorizer()\n    X = vectorizer.fit_transform(['the pizza is good', 'the pizza is good'])\n    X_expected = sparse.csr_matrix([[1, 1], [1, 1]], dtype=int)\n    assert_equal(vectorizer.get_feature_names(), ['1', '2'])\n    assert_equal(X.toarray(), X_expected.toarray())\n", "def test_TfidfVectorizer():\n    vectorizer = TfidfVectorizer(max_features=10, stop_words=None)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    assert_equal(len(vectorizer.get_feature_names_out()), 10)\n    assert_equal(X.shape[1], 10)\n    assert_true((X > 0).sum(axis=0).sum() > 0)\n"], "sample_592": ["def test_maybe_truncate():\n    # Test that maybe_truncate truncates strings correctly\n    assert maybe_truncate(\"hello world\") == \"hello worl...\"\n    assert maybe_truncate(\"hello world\", maxlen=10) == \"hello worl...\"\n    assert maybe_truncate(\"hello\", maxlen=5) == \"hello\"\n\n    # Test that maybe_truncate leaves strings unchanged if they fit within the limit\n    assert maybe_truncate(\"hello\", maxlen=10) == \"hello\"\n\n    # Test that maybe_truncate leaves strings unchanged if they are already shorter than the limit\n    assert maybe_truncate(\"a\", maxlen=10) == \"a\"\n\n    # Test that maybe_truncate handles empty strings correctly\n    assert maybe_truncate(\"\", maxlen=10) == \"\"\n\n    # Test that maybe_truncate handles very long strings correctly\n    long_string = \"a\" * 1000\n    assert maybe_truncate(long_string, maxlen=10) == long_string[:7] + \"...\"\n", "def test_format_timestamp():\n    # Test a valid timestamp\n    timestamp = pd.Timestamp(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T12:00:00\"\n\n    # Test a timestamp beyond the valid range\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(np.datetime64(\"9999-12-31 23:59:59\"))\n\n    # Test a timestamp with no time component\n    timestamp = pd.Timestamp(\"2022-01-01\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01\"\n\n    # Test a NaT (not a time) value\n    na_t = pd.NaT\n    assert formatting.format_timestamp(na_t) == \"NaT\"\n\n    # Test a custom datetime object\n    custom_datetime = datetime(2022, 1, 1, 12, 0, 0)\n    assert formatting.format_timestamp(custom_datetime) == \"2022-01-01T12:00:00\"\n\n    # Test a numpy datetime64 value\n    numpy_datetime = np.datetime64(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(numpy_datetime) == \"2022-01-01T12:00:00\"\n", "def test_format_timestamp():\n    # Test that format_timestamp handles Datetime objects\n    dt = pd.Timestamp(\"2020-01-01 12:00:00\")\n    assert formatting.format_timestamp(dt) == \"2020-01-01T12:00:00\"\n\n    # Test that format_timestamp handles NaT\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test that format_timestamp handles out-of-bounds values\n    assert formatting.format_timestamp(\"9999-12-31\") == \"9999-12-31\"\n\n    # Test that format_timestamp handles non-Datetime objects\n    assert formatting.format_timestamp(12345) == \"12345\"\n\n", "def test_format_timedelta():\n    # Test a simple case\n    timedelta_str = pd.Timedelta(days=1)\n    assert formatting.format_timedelta(timedelta_str) == \"1 days\"\n\n    # Test a case with \"time\" format\n    timedelta_str = pd.Timedelta(hours=1)\n    assert formatting.format_timedelta(timedelta_str, timedelta_format=\"time\") == \"01:00:00\"\n\n    # Test a case with \"date\" format\n    timedelta_str = pd.Timedelta(days=1)\n    assert formatting.format_timedelta(timedelta_str, timedelta_format=\"date\") == \"1 days\"\n\n    # Test an invalid timedelta_format\n    with pytest.raises(ValueError):\n        formatting.format_timedelta(timedelta_str, timedelta_format=\"invalid\")\n\n    # Test an empty timedelta\n    assert formatting.format_timedelta(np.timedelta64(0, 'ns')) == \"0 ns\"\n\n    # Test an NaT timedelta\n    assert formatting.format_timedelta(pd.NaT) == \"NaT\"\n\n    # Test a timedelta with a different unit\n    assert formatting.format_timedelta(np.timedelta64(1, 'D')) == \"1 days\"\n    assert formatting.format_timedelta(np.timedelta64(1, 'h')) == \"1 hours\"\n    assert formatting.format_timedelta(np.timedelta64(1, 'm')) == \"1 minutes\"\n    assert formatting.format_timedelta(np.timedelta64(1, 's')) == \"1 seconds\"\n    assert formatting.format_timedelta(np.timedelta64(1, 'ms')) == \"1 milliseconds\"\n    assert formatting.format_timedelta(np.timedelta64(1, 'us')) == \"1 microseconds\"\n    assert formatting.format_timedelta(np.timedelta64(1, 'ns')) == \"1 nanoseconds\"\n\n    # Test a timedelta with a very large value\n    timedelta_str = pd.Timedelta(days=100000)\n    assert formatting.format_timedelta(timedelta_str) == \"10958904000000000000000000000 days\"\n\n    # Test a timedelta with a very small value\n    timedelta_str = pd.Timedelta(milliseconds=1)\n    assert formatting.format_timedelta(timedelta_str) == \"0.001 seconds\"\n", "def test_maybe_truncate():\n    # Test that maybe_truncate truncates the string as expected\n    assert maybe_truncate(\"short string\") == \"short string\"\n    assert maybe_truncate(\"very long string that is too long to fit in 10 characters\") == \"very long...\"\n    # Test that the truncation point is exactly at the maxlen\n    assert maybe_truncate(\"very long string that is too long to fit in 12 characters\", maxlen=12) == \"very long string...\"\n    # Test that the ellipsis is exactly three characters long\n    assert maybe_truncate(\"very long string that is too long to fit in 11 characters\", maxlen=11) == \"very long str...\"\n\n    # Test that maybe_truncate works with an empty string\n    assert maybe_truncate(\"\") == \"\"\n\n    # Test that maybe_truncate works with a string that is already short enough\n    assert maybe_truncate(\"short enough\") == \"short enough\"\n\n    # Test that maybe_truncate works with a string that is too long to fit in the maximum display width\n    assert maybe_truncate(\"a\" * 1000) == \"a...\"\n\n"], "sample_1074": ["    def test_polycyclic_group(self):\n        # Test that the polycyclic group method returns a PolycyclicGroup object\n        G = PermutationGroup([Permutation([0, 2, 1])])\n        pg = G.polycyclic_group()\n        self.assertIsInstance(pg, PolycyclicGroup)\n\n        # Test that the polycyclic group method returns the correct PolycyclicGroup object\n        self.assertEqual(pg.pc_sequence, [Permutation([1, 0, 2])])\n        self.assertEqual(pg.pc_series, [PermutationGroup([Permutation([0, 2, 1])])])\n        self.assertEqual(pg.relative_order, [2])\n\n        # Test that the polycyclic group method raises an error if the group is not solvable\n        G = PermutationGroup([Permutation([0, 1, 2, 3, 4, 5])])\n        with self.assertRaises(ValueError):\n            G.polycyclic_group()\n", "def test_polycyclic_group():\n    G = PermutationGroup([Permutation([0, 1, 2]), Permutation([0, 2, 1])])\n    PG = G.polycyclic_group()\n    assert PG.is_polycyclic\n    assert PG[0].order() == 2\n    assert PG[1].order() == 2\n    assert PG[2].order() == 2\n    assert PG[3].order() == 2\n    assert PG.order() == 4\n\n    G = PermutationGroup([Permutation([0, 2, 1, 3]), Permutation([1, 0, 2, 3])])\n    PG = G.polycyclic_group()\n    assert PG.is_polycyclic\n    assert PG[0].order() == 2\n    assert PG[1].order() == 2\n    assert PG[2].order() == 4\n    assert PG.order() == 8\n\n    G = PermutationGroup([Permutation([0, 2, 1, 3, 4]), Permutation([1, 0, 2, 3, 4])])\n    PG = G.polycyclic_group()\n    assert PG.is_polycyclic\n    assert PG[0].order() == 2\n    assert PG[1].order() == 2\n    assert PG[2].order() == 2\n    assert PG[3].order() == 2\n    assert PG[4].order() == 5\n    assert PG.order() == 40\n\n    G = PermutationGroup([Permutation([0, 2, 1, 3]), Permutation([1, 0, 2, 3])])\n    PG = G.polycyclic_group()\n    assert PG.is_polycyclic\n    assert PG[0].order() == 2\n    assert PG[1].order() == 2\n    assert PG[2].order() == 2\n    assert PG[3].order() == 2\n    assert PG.order() == 8\n", "    def test_PermutationGroup_direct_product(self):\n        from sympy.combinatorics.permutations import Permutation\n        G = SymmetricGroup(3)\n        H = CyclicGroup(2)\n        GH = G*H\n        assert GH.degree == 6\n        assert len(GH) == 12\n        assert GH.order() == 12\n        assert GH.is_transitive()\n        assert not GH.is_abelian\n        assert GH.is_polycyclic\n        assert GH.is_solvable\n        assert GH.is_nilpotent\n        assert GH.is_primitive()\n        assert GH.is_elementary(2)\n        assert GH.is_elementary(3)\n        assert GH.center().is_subgroup(G)\n        assert GH.center().is_subgroup(H)\n        assert GH.centralizer(G).is_subgroup(H)\n        assert GH.centralizer(H).is_subgroup(G)\n        assert GH.normal_closure(GH.generators).is_subgroup(G)\n        assert GH.normal_closure(H.generators).is_subgroup(H)\n        assert GH.normal_closure(GH.center().generators) == GH.center()\n        assert GH.sylow_subgroup(2).is_subgroup(G)\n        assert GH.sylow_subgroup(3).is_subgroup(H)\n        assert GH.sylow_subgroup(5).is_subgroup(CyclicGroup(5))\n        assert GH.sylow_subgroup(7).is_subgroup(CyclicGroup(7))\n        assert GH.sylow_subgroup(11).is_subgroup(CyclicGroup(11))\n        assert GH.sylow_subgroup(13).is_subgroup(CyclicGroup(13))\n        assert GH.sylow_subgroup(17).is_subgroup(CyclicGroup(17))\n        assert GH.sylow_subgroup(19).is_subgroup(CyclicGroup(19))\n        assert GH.sylow_subgroup(23).is_subgroup(CyclicGroup(23))\n        assert GH.sylow_subgroup(29).is_subgroup(CyclicGroup(29))\n        assert GH.sylow_subgroup(31).is_subgroup(CyclicGroup(31))\n        assert GH.sylow_subgroup(37).is_subgroup(CyclicGroup(37))\n        assert GH.sylow_subgroup(41).is_subgroup(CyclicGroup(41))\n        assert GH.sylow_subgroup(43).is_subgroup(Cyclic", "def test_permutation_group_contains():\n    # Test if contains method is working correctly for a single permutation\n    p = Permutation([0, 2, 1])\n    G = PermutationGroup(p)\n    assert G.contains(p)\n    assert not G.contains(Permutation([0, 1, 2]))\n\n    # Test if contains method is working correctly for a list of permutations\n    p1 = Permutation([0, 2, 1])\n    p2 = Permutation([1, 0, 2])\n    G = PermutationGroup([p1, p2])\n    assert G.contains(p1)\n    assert G.contains(p2)\n    assert not G.contains(Permutation([0, 1, 3]))\n\n    # Test if contains method is working correctly when the size of the permutation\n    # is different from the size of the group\n    p = Permutation([0, 1])\n    G = PermutationGroup(Permutation([0, 2, 1, 3]))\n    assert G.contains(p, strict=False)\n    assert not G.contains(p, strict=True)\n\n    # Test if contains method is working correctly for a permutation group\n    p = Permutation([0, 1, 2])\n    G = PermutationGroup([p])\n    assert G.contains(PermutationGroup([p]))\n    assert not G.contains(PermutationGroup([Permutation([0, 2, 1])]))\n", "def test_schreier_sims_incremental():\n    # test a group on 7 points\n    a = Permutation([1, 2, 4, 5, 6, 0, 3])\n    b = Permutation([1, 4, 5, 6, 0, 3, 2])\n    G = PermutationGroup([a, b])\n    base, strong_gens = G.schreier_sims_incremental()\n    assert base == [2, 3]\n    assert len(strong_gens) == 3\n\n    # test a group on 10 points\n    a = Permutation([3, 7, 9, 2, 6, 4, 0, 1, 8, 5])\n    b = Permutation([1, 7, 9, 0, 6, 8, 5, 4, 2, 3])\n    c = Permutation([3, 7, 9, 0, 6, 4, 2, 1, 8, 5])\n    G = PermutationGroup([a, b, c])\n    base, strong_gens = G.schreier_sims_incremental()\n    assert base == [0, 2]\n    assert len(strong_gens) == 4\n\n    # test that the algorithm doesn't use elements that fix the base\n    a = Permutation([1, 0, 4, 2, 3])\n    b = Permutation([0, 1, 4, 2, 3])\n    c = Permutation([0, 2, 3, 1])\n    G = PermutationGroup([a, b, c])\n    base, strong_gens = G.schreier_sims_incremental()\n    assert base == [1]\n    assert len(strong_gens) == 3\n\n    # test a group with trivial subgroup\n    a = Permutation([1, 2, 0])\n    G = PermutationGroup([a])\n    base, strong_gens = G.schreier_sims_incremental()\n    assert base == []\n    assert len(strong_gens) == 1\n\n    # test a group with identity in generators\n    a = Permutation([1, 2, 0])\n    G = PermutationGroup([a"], "sample_177": ["    def test_model_state_equality(self):\n        model_state1 = ModelState.from_model(models.Model())\n        model_state2 = ModelState.from_model(models.Model())\n        self.assertTrue(model_state1 == model_state2)\n        self.assertFalse(model_state1 != model_state2)\n", "    def test_from_model(self):\n        model = models.Model()\n        model._meta.app_label = 'my_app'\n        model._meta.object_name = 'my_model'\n        model._meta.local_fields = [\n            models.IntegerField(name='id'),\n            models.CharField(name='name', max_length=255)\n        ]\n        model._meta.local_many_to_many = [\n            models.ManyToManyField(name='tags', related_name='my_model')\n        ]\n        model._meta.managers = [models.Manager(), models.Manager(name='objects')]\n        model._meta.default_manager = models.Manager(name='default_manager')\n        model._meta.base_manager = models.Manager()\n        model._meta.constraints = [models.CheckConstraint(name='test_constraint')]\n        model._meta.indexes = [models.Index(name='test_index')]\n        model._meta.unique_together = [('name',)]\n        model._meta.index_together = [('name',)]\n        model._meta.original_attrs = {\n            'unique_together': [('name',)],\n            'index_together': [('name',)],\n            'indexes': [models.Index(name='test_index')],\n            'constraints': [models.CheckConstraint(name='test_constraint')]\n        }\n        model._meta.private_fields = [models.IntegerField(name='private_field')]\n        model._bases = (models.Model,)\n        model._default_manager = models.Manager(name='default_manager')\n        model._base_manager = models.Manager()\n        \n        model_state = ModelState.from_model(model)\n        self.assertEqual(model_state.app_label, 'my_app')\n        self.assertEqual(model_state.name, 'my_model')\n        self.assertEqual(model_state.fields, {'id': models.IntegerField(name='id'), 'name': models.CharField(max_length=255)})\n        self.assertEqual(model_state.options, {'indexes': [models.Index(name='test_index')], 'constraints': [models.CheckConstraint(name='test_constraint')], 'unique_together': {('name',)}, 'index_together': {('name',)}})\n        self.assertEqual(model_state.bases, (models.Model,))\n        self.assertEqual(model_state.managers, [('objects', models.Manager()), ('default_manager', models.Manager(name='default_manager'))])\n        \n        model_state2 = ModelState.from_model(model, exclude_rels=True)\n        self.assertEqual(model_state2.app_label, 'my_app')\n        self.assertEqual(model_state2.name, 'my_model')\n        self.assertEqual(model_state2.fields, {'id': models.IntegerField(name='id'), 'name': models.CharField(max_length=255", "    def test_from_model_exclude_rels(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n        model_state = ModelState.from_model(MyModel, exclude_rels=True)\n        self.assertEqual(model_state.fields, {'name': models.CharField(max_length=255)})\n        self.assertEqual(model_state.options, {})\n", "    def test_from_model_concrete_model(self):\n        # Test that a ModelState can be created from a concrete model.\n        model = models.Model()\n        model._meta.app_label = 'testapp'\n        model._meta.model_name = 'concrete'\n        state = ModelState.from_model(model)\n        self.assertEqual(state.app_label, 'testapp')\n        self.assertEqual(state.name, 'concrete')\n        self.assertEqual(state.bases, (models.Model,))\n", "    def test_model_state_clone(self):\n        model = UnicodeModel()\n        model_state = ModelState.from_model(model)\n        cloned_state = model_state.clone()\n\n        self.assertEqual(model_state, cloned_state)\n        self.assertIsNot(model_state, cloned_state)\n\n        # Check that fields are not the same instances after cloning\n        self.assertNotEqual(model_state.fields['name'], cloned_state.fields['name'])\n"], "sample_680": ["    def test_evaluate_condition_string_condition(self):\n        item = pytest.Item()\n        mark = pytest.Mark()\n        condition = \"os.name == 'nt'\"\n        result, reason = evaluate_condition(item, mark, condition)\n        assert result is True\n        assert reason == \"condition: os.name == 'nt'\"\n", "    def test_evaluate_condition_string(self):\n        item = Item(\"test_eval\", \"module\")\n        mark = Mark(\"skipif\", {\"reason\": \"test reason\"})\n        condition = \"True\"\n        result, reason = evaluate_condition(item, mark, condition)\n        assert result is False\n        assert reason == \"condition: True\"\n", "    def test_evaluate_condition_string_condition(self):\n        item = pytest.Item()\n        item.config = pytest.Config()\n        mark = pytest.Mark(name=\"skipif\")\n        condition = \"os.name == 'nt'\"\n        result, reason = evaluate_condition(item, mark, condition)\n        assert result is False\n        assert reason == \"condition: os.name == 'nt'\"\n", "def test_evaluate_condition_with_string_condition():\n    item = pytest.Item(\"test name\", node=\"node name\")\n    mark = pytest.Mark(\"skipif\")\n    mark.kwargs = {\"condition\": \"os.name == 'nt'\"}\n    result, reason = evaluate_condition(item, mark, mark.kwargs[\"condition\"])\n    assert result == (sys.platform == \"win32\")\n    assert reason == \"condition: os.name == 'nt'\"\n", "    def test_evaluate_condition_string_eval(self):\n        item = pytest.Item(\"test item\", config=pytest.Config())\n        mark = pytest.Mark(\"xfail\", {\"condition\": \"sys.platform == 'win32'\"})\n        condition = mark.kwargs[\"condition\"]\n        result, reason = evaluate_condition(item, mark, condition)\n        assert result is False\n        assert reason == \"condition: sys.platform == 'win32'\"\n"], "sample_83": ["    def test_tag_decorator_register(self):\n        library = Library()\n        @library.tag()\n            pass\n        self.assertIn('test_func', library.tags)\n        self.assertEqual(library.tags['test_func'], test_func)\n", "    def setUp(self):\n        self.library = Library()\n", "    def setUp(self):\n        self.library = Library()\n", "    def test_tag(self):\n        @self.library.tag()\n            return 'hello'\n\n        self.assertIn('hello', self.library.tags)\n", "    def setUp(self):\n        self.library = Library()\n"], "sample_101": ["    def test_path_info_empty_string(self):\n        environ = {\n            'PATH_INFO': '',\n            'REQUEST_METHOD': 'GET'\n        }\n        request = WSGIRequest(environ)\n        self.assertEqual(request.path_info, '/')\n", "    def test_init_path_info_empty(self):\n        environ = {\n            'REQUEST_METHOD': 'GET',\n            'SCRIPT_NAME': '/admin/',\n            'PATH_INFO': '',\n        }\n        request = WSGIRequest(environ)\n        self.assertEqual(request.path_info, '/')\n", "def test_limited_stream(self):\n    stream = BytesIO(b'1234567890')\n    limited_stream = LimitedStream(stream, 5)\n    self.assertEqual(limited_stream.read(10), b'1234567')\n    self.assertEqual(limited_stream.read(1), b'8')\n    self.assertEqual(limited_stream.read(1), b'9')\n    self.assertEqual(limited_stream.read(1), b'0')\n    self.assertEqual(limited_stream.read(1), b'')\n    self.assertEqual(limited_stream.read(), b'')\n", "    def test_init_with_empty_environ(self):\n        environ = {}\n        request = WSGIRequest(environ)\n        self.assertEqual(request.META, environ)\n", "    def test_wsgi_handler_request_class(self):\n        request = self.request_factory.get('/test/')\n        request.META = {\n            'wsgi.url_scheme': 'https',\n            'CONTENT_LENGTH': '10',\n            'wsgi.input': BytesIO(b'hello'),\n        }\n        handler = WSGIHandler()\n        response = handler(request)\n        self.assertIsInstance(response, FileResponse)\n        self.assertIn('Content-Length', response.headers)\n        self.assertIn('Content-Type', response.headers)\n"], "sample_651": ["def test_WarningsRecorder_len(tmp_path):\n    \"\"\"Test len() method of WarningsRecorder.\"\"\"\n    with WarningsRecorder() as wrec:\n        warnings.warn(\"test warning\", UserWarning)\n        warnings.warn(\"test warning2\", UserWarning)\n    assert len(wrec) == 2\n    assert len(wrec.list) == 2\n", "def test_warnings_checker_matches_with_re() -> None:\n    \"\"\"Test that WarningsChecker matches with regex.\"\"\"\n    with pytest.warns(UserWarning, match=r'^value must be \\d+$'):\n        warnings.warn('value must be 42', UserWarning)\n        warnings.warn('value must be 23', UserWarning)\n        warnings.warn('value must be 0', DeprecationWarning)\n\n    checker = pytest.warns(UserWarning, match=r'^value must be \\d+$')\n    assert len(checker.list) == 2\n    assert checker.matches(checker[0])\n    assert not checker.matches(checker[1])\n", "def test_WarningsRecorder_clear():\n    \"\"\"Test that WarningsRecorder.clear() works.\"\"\"\n    with WarningsRecorder() as wrec:\n        wrec.list.append(warnings.WarningMessage(\n            category=DeprecationWarning, message='test message', filename='test.py', lineno=42\n        ))\n        assert len(wrec.list) == 1\n        wrec.clear()\n        assert len(wrec.list) == 0\n\n", "def test_WarningsChecker_matches():\n    \"\"\"Test WarningsChecker.matches method.\"\"\"\n    with warnings.catch_warnings():\n        warnings.warn(\"Test message\", UserWarning)\n    wrec = WarningsRecorder()\n    with wrec:\n        warnings.warn(\"Test message\", UserWarning)\n    checker = WarningsChecker(UserWarning)\n    assert checker.matches(wrec.list[0])\n    assert not checker.matches(wrec.list[0], match=\"Invalid message\")\n", "def test_warningschecker_matches_empty():\n    \"\"\"Test if WarningsChecker matches no warnings.\"\"\"\n    with WarningsRecorder() as rec:\n        with WarningsChecker() as check:\n            pass\n        assert check.list == []\n        assert check.matches(None) is False\n\n"], "sample_462": ["    def test_uuid_field_required(self):\n        field = UUIDField(required=True)\n        with self.assertRaises(ValidationError):\n            field.clean(\"\")\n", "    def test_invalid_max_length(self):\n        form = Form({'field': 'a' * 1001})\n        with self.assertRaises(ValidationError):\n            CharField(max_length=1000).clean(form.cleaned_data['field'])\n", "    def test_typed_multiple_choice_field_with_required_option(self):\n        class TypedMultipleChoiceFieldForm(Form):\n            field = TypedMultipleChoiceField(\n                choices=[(\"choice1\", \"Choice 1\"), (\"choice2\", \"Choice 2\")],\n                choices_func=lambda: [(\"choice3\", \"Choice 3\")],\n                coerce=str,\n                required=True,\n            )\n\n        form = TypedMultipleChoiceFieldForm({\"field\": [\"choice1\"]})\n        self.assertTrue(form.is_valid())\n\n        form = TypedMultipleChoiceFieldForm({\"field\": []})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"This field is required.\", form.errors[\"field\"])\n\n        form = TypedMultipleChoiceFieldForm({\"field\": [\"choice1\", \"choice2\"]})\n        self.assertTrue(form.is_valid())\n\n        form = TypedMultipleChoiceFieldForm({\"field\": [\"choice1\", \"choice4\"]})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"Select a valid choice. choice4 is not one of the available choices.\", form.errors[\"field\"])\n", "    def test_compress(self):\n        field = MultiValueField(fields=(IntegerField(), IntegerField()))\n        with self.assertRaises(NotImplementedError):\n            field.compress([\"1\", \"2\"])\n", "    def test_choice_coerce(self):\n        field = TypedChoiceField(choices=[(\"apple\", \"Apple\"), (\"banana\", \"Banana\")], coerce=int)\n        field.empty_value = ''\n        self.assertEqual(field.clean(\"apple\"), 1)\n        self.assertEqual(field.clean(\"1\"), 1)\n        self.assertEqual(field.clean(\"banana\"), 2)\n        self.assertEqual(field.clean(1), 1)\n        self.assertEqual(field.clean(2), 2)\n        with self.assertRaises(ValidationError):\n            field.clean(\"orange\")\n"], "sample_764": ["def test_ColumnTransformer_missing_fitted_transformers():\n    \"\"\"Test that a DataConversionWarning is raised when transform is called\n    before fit.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    ct = ColumnTransformer(transformers=[('trans', Trans(), [0])])\n\n    with pytest.warns(DataConversionWarning):\n        ct.transform(X)\n", "def test_fit_transform_passthrough():\n    # Check that fit_transform does not throw a warning when using passthrough\n    X = [[1, 2, 3], [4, 5, 6]]\n    y = [1, 2]\n    transformer = ColumnTransformer(\n        [('passthrough', 'passthrough', [0, 1, 2])], remainder='passthrough')\n    transformer.fit_transform(X, y)\n", "def test_remainder_transformer_is_fitted():\n    X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n    ct = ColumnTransformer(transformers=[('trans', Trans(), [0, 2])], remainder='passthrough')\n    ct.fit(X)\n    assert hasattr(ct.transformers_[0][1], 'transform')\n    # verify that it was actually called\n    assert ct.transform(X).shape == (2, 2)\n", "def test_column_transformer_transformer_weights():\n    \"\"\"Test the transformer_weights argument.\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([1, 2])\n    scaler = StandardScaler()\n    onehot = OneHotEncoder()\n    transformer_list = [\n        ('scaler', scaler, [0]),\n        ('onehot', onehot, [1]),\n    ]\n    column_transformer = ColumnTransformer(\n        transformer_list, transformer_weights={'scaler': 2., 'onehot': 0.5},\n    )\n    # Test that the output is correct\n    result = column_transformer.fit_transform(X, y)\n    assert_allclose_dense_sparse(result, 2 * np.array([[0., 3., 6.],\n                                                     [8., 15., 18.]]))\n\n    # Test that the weights are applied correctly\n    expected_result = np.array([[0., 1.5, 3.],\n                               [2., 7.5, 9.]])\n    assert_array_equal(result, expected_result)\n\n    # Test that the weights can be updated\n    column_transformer.set_params(transformer_weights={'scaler': 1., 'onehot': 1.})\n    result = column_transformer.transform(X)\n    assert_allclose_dense_sparse(result, np.array([[0., 2., 3.],\n                                                 [4., 10., 6.]]))\n\n    # Test that the weights can be set to None\n    column_transformer.set_params(transformer_weights=None)\n    result = column_transformer.transform(X)\n    assert_allclose_dense_sparse(result, np.array([[0., 2., 3.],\n                                                 [4., 10., 6.]]))\n\n    # Test that a warning is raised when the transformer weights sum is not 1\n    with assert_warns(UserWarning):\n        column_transformer = ColumnTransformer(\n            transformer_list, transformer_weights={'scaler': 1., 'onehot': 2.},\n        )\n        column_transformer.transform(X)\n", "def test_sparse_output_switches_to_dense():\n    \"\"\"\n    Check that the ColumnTransformer switches to dense output when output is\n    supposed to be dense.\n\n    \"\"\"\n    X = np.array([[1., 2., 3.], [4., 5., 6.]])\n    ct = ColumnTransformer([('scaler', StandardScaler(), [0, 1]), ('passthrough', 'passthrough', [2])])\n    ct.fit(X)\n    X_sparse = sparse.csr_matrix(X)\n    X_sparse_output = ct.transform(X_sparse)\n    assert_array_equal(X_sparse_output.toarray(), ct.transform(X))\n"], "sample_340": ["    def test_build_graph_with_replacing_migrations(self):\n        with self.settings(MIGRATION_MODULES={\n            'app1': 'path.to.migrations',\n            'app2': 'path.to.migrations',\n        }):\n            loader = MigrationLoader(connections['default'])\n            # Create some migrations with replaces\n            loader.disk_migrations[('app1', '0001_initial')] = mock.Mock()\n            loader.disk_migrations[('app1', '0002_replace')] = mock.Mock(replaces=['0001_initial'])\n            loader.disk_migrations[('app2', '0001_initial')] = mock.Mock()\n            loader.build_graph()\n            self.assertIn(('app1', '0002_replace'), loader.replacements)\n            self.assertEqual(loader.replacements[('app1', '0002_replace')].replaces, ['0001_initial'])\n", "    def test_build_graph(self):\n        # Create two apps with migrations\n        self.create_app('app1', ['001_initial.py', '002_create_table.py'])\n        self.create_app('app2', ['001_initial.py', '002_create_table.py'])\n\n        # Create a migration in app1 that depends on a migration in app2\n        app1_migration = MigrationLoader.get_migration('app1', '002_create_table')\n        app2_migration = MigrationLoader.get_migration('app2', '002_create_table')\n        app1_migration.dependencies.append(('app2', '001_initial'))\n\n        # Build the graph\n        loader = MigrationLoader(connection=self.connection, load=True)\n        loader.build_graph()\n\n        # Check that the graph is consistent\n        loader.graph.validate_consistency()\n\n        # Check that the nodes are added to the graph correctly\n        self.assertIn(('app1', '002_create_table'), loader.graph.nodes)\n        self.assertIn(('app2', '002_create_table'), loader.graph.nodes)\n\n        # Check that the dependencies are added to the graph correctly\n        app1_node = loader.graph.nodes[('app1', '002_create_table')]\n        self.assertEqual(app1_node.dependencies, [('app2', '001_initial')])\n", "    def test_build_graph_without_migration_file(self):\n        # Arrange\n        app_config = apps.get_app_config('test_app')\n        app_path = os.path.dirname(app_config.path)\n        migration_path = os.path.join(app_path, 'migrations')\n        if os.path.exists(migration_path):\n            os.rmdir(migration_path)\n        # Act\n        loader = MigrationLoader(connections['default'])\n        # Assert\n        self.assertEqual(loader.disk_migrations, {})\n        self.assertEqual(loader.unmigrated_apps, {'test_app'})\n", "    def test_build_graph_with_replace_migrations_enabled(self):\n        from django.conf import settings\n        settings.MIGRATION_MODULES = {'app1': 'tests.app1.migrations'}\n        settings.ENABLE_MIGRATION_REPLACEMENTS = True\n\n        MigrationLoader(connection=connections['default'])\n        self.assertTrue(MigrationLoader.replace_migrations)\n", "    def test_build_graph_loads_migration_with_replacing_dependencies(self):\n        self.connection = connections['default']\n        MigrationRecorder(connections['default']).delete_migrations()\n        self.assertListEqual(MigrationRecorder(connections['default']).applied_migrations(), [])\n\n        # Create migrations\n        self.load_migrations([\n            \"0001_initial.py\",\n            \"0002_replace_0001.py\",\n            \"0003_unrelated.py\",\n        ])\n        MigrationLoader(connections['default']).load_disk()\n\n        # Create replacing migration\n        self.assertTrue(MigrationLoader(connections['default']).graph.nodes[('testapp', '__replace__')])\n        # Migration 0002_replace_0001 replaces 0001_initial\n"], "sample_713": ["def test_ridge_solver_wrong_solver():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    alpha = 1.0\n    with assert_warns(UserWarning):\n        ridge_regression(X, y, alpha, solver='invalid_solver')\n", "    def test_ridge_regression_dtype(self):\n        X = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n        y = np.array([0, 1], dtype=np.float32)\n        model = Ridge()\n        model.fit(X, y)\n        assert_equal(model.coef_.dtype, np.float32)\n", "def test_Ridge_fit_intercept_behavior():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    y = rng.rand(10)\n    estimator = Ridge(fit_intercept=False)\n    estimator.fit(X, y)\n    assert_equal(estimator.coef_.shape, (5,))\n", "    def test_gcv_mode_eigen(self):\n        # Test with a matrix where the number of features is larger than the number\n        # of samples\n        X = np.random.rand(100, 1000)\n        y = np.random.rand(100)\n        gcv = _RidgeGCV()\n        gcv.fit(X, y)\n        assert_equal(gcv.gcv_mode, 'eigen')\n", "def test_RidgeCV_tolerance():\n    from sklearn.linear_model import RidgeCV\n    from sklearn.metrics import make_scorer\n\n    # Make a random dataset\n    X, y = make_regression(n_samples=100, n_features=5, random_state=0)\n\n    # Define a scorer that allows for different tolerance\n    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n\n    # Create a RidgeCV object with a tolerance of 0.01\n    ridgecv = RidgeCV(cv=KFold(), alphas=[0.1, 1.0, 10.0], scoring=scorer,\n                      tol=0.01)\n\n    # Fit the RidgeCV object to the data\n    ridgecv.fit(X, y)\n\n    # Check that the tolerance is taken into account in the scoring\n    assert_greater(ridgecv.score(X, y), 0.01)\n\n    # Check that the alpha_ attribute is set correctly\n    assert_equal(len(ridgecv.alphas_), 1)\n    assert_almost_equal(ridgecv.alpha_, 1.0)\n\n    # Check that the best score is greater than the minimum possible score\n    best_score = ridgecv.best_score_\n    min_score = min(mean_squared_error(y, np.mean(y)) for _ in range(10))\n    assert_greater(best_score, min_score)\n\n    # Check that the best alpha is the one with the highest score\n    best_alpha = ridgecv.best_alpha_\n    alphas = np.array([0.1, 1.0, 10.0])\n    assert_almost_equal(best_alpha, alphas[np.argmax(ridgecv.scores_)])\n"], "sample_470": ["    def test_lazy_proxy_equality(self):\n        @lazy\n            return 5\n\n        proxy1 = get_value()\n        proxy2 = get_value()\n\n        self.assertEqual(proxy1, proxy2)\n", "def test_lazy_object_copy(self):\n        return 42\n\n    lazy_obj = SimpleLazyObject(create_lazy_object)\n    copy_obj = lazy_obj.__copy__()\n    self.assertEqual(id(lazy_obj._wrapped), id(copy_obj._wrapped))\n    self.assertEqual(lazy_obj._wrapped, copy_obj._wrapped)\n\n        return [1, 2, 3]\n\n    lazy_obj = SimpleLazyObject(create_lazy_object)\n    copy_obj = lazy_obj.__deepcopy__({})\n    self.assertIsNot(lazy_obj._wrapped, copy_obj._wrapped)\n    self.assertEqual(lazy_obj._wrapped, copy_obj._wrapped)\n\n        raise ValueError(\"Test error\")\n\n    lazy_obj = SimpleLazyObject(create_lazy_object)\n    with self.assertRaises(ValueError):\n        lazy_obj.__copy__()\n    with self.assertRaises(ValueError):\n        lazy_obj.__deepcopy__({})\n", "    def test_lazy_object_repr(self):\n        # Test that the repr of a lazy object without an evaluated wrapped object\n        # provides a meaningful representation\n        lazy_obj = SimpleLazyObject(lambda: object())\n        self.assertIn(\"object\", repr(lazy_obj))\n        self.assertIn(\"LazyObject\", repr(lazy_obj))\n\n        # Test that the repr of a lazy object with an evaluated wrapped object\n        # provides a meaningful representation\n        lazy_obj._wrapped = object()\n        self.assertIn(\"object\", repr(lazy_obj))\n", "    def test_lazy_object_attribute_access(self):\n        class MyLazyObject(LazyObject):\n                self._wrapped = \"Hello, World!\"\n\n        lazy_obj = MyLazyObject()\n        with self.assertRaises(AttributeError):\n            lazy_obj.attribute  # This should raise an AttributeError\n\n        # Now let's access the attribute with the correct proxy method\n        lazy_obj.__getattr__(\"attribute\")  # This should not raise an error\n", "    def test_lazy_str(self):\n        lazy_str = lazy(str, str)\n        self.assertEqual(lazy_str(\"hello\"), \"hello\")\n        self.assertEqual(repr(lazy_str(\"hello\")), repr(\"hello\"))\n"], "sample_878": ["def test_fit_transform_with_empty_transformers():\n    # Test fit_transform when transformers is empty\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers)\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError):\n        ct.fit_transform(X)\n\n    # Test fit_transform when transformers is empty and remainder='passthrough'\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers, remainder='passthrough')\n    X = np.array([[1, 2], [3, 4]])\n    result = ct.fit_transform(X)\n    assert_array_equal(result, np.array([[1, 2], [3, 4]]))\n\n    # Test fit_transform when transformers is empty and remainder='drop'\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers, remainder='drop')\n    X = np.array([[1, 2], [3, 4]])\n    result = ct.fit_transform(X)\n    assert_array_equal(result, np.zeros((2, 0)))\n\n    # Test fit_transform with empty transformers and custom output\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers).set_output(transform='pandas')\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError):\n        ct.fit_transform(X)\n\n    # Test fit_transform with empty transformers and custom output\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers, remainder='passthrough').set_output(transform='pandas')\n    X = np.array([[1, 2], [3, 4]])\n    result = ct.fit_transform(X)\n    assert isinstance(result, pd.DataFrame)\n    assert result.shape == (2, 2)\n\n    # Test fit_transform with empty transformers and custom output\n    transformers = []\n    ct = ColumnTransformer(transformers=transformers, remainder='drop').set_output(transform='pandas')\n    X = np.array([[1, 2], [3, 4]])\n    result = ct.fit_transform(X)\n    assert isinstance(result, pd.DataFrame)\n    assert result.shape == (2, 0)\n\n    # Test fit_transform with custom transformers and custom output\n    transformers = [('transformer', Trans(), [0])]\n    ct = ColumnTransformer(transformers=transformers, remainder='passthrough').set_output(transform='pandas')\n    X = np.array([[1, 2], [3, 4]])\n   ", "def test_column_transformer_sparse_output():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    Y = ColumnTransformer(\n        transformers=[\n            (\"scaler\", StandardScaler(), [0]),\n            (\"encoder\", OneHotEncoder(), [1]),\n            (\"passthrough\", \"passthrough\", [2]),\n        ],\n        remainder=\"passthrough\",\n        sparse_threshold=0.0,\n        verbose=False,\n        verbose_feature_names_out=True,\n    )\n    Y.fit(X)\n    X_transformed = Y.transform(X)\n    assert sparse.issparse(X_transformed)\n    assert X_transformed.shape == (2, 5)\n    assert len(Y.transformers_) == 3\n", "def test_column_transformer_transformer_weights():\n    \"\"\"Test that transformer weights are correctly applied.\"\"\"\n    # Test that weights are applied in transform method\n    X = np.array([[1, 2], [3, 4]])\n    transformer = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [\"feature1\"]),\n         (\"trans2\", OneHotEncoder(), [\"feature2\"])],\n        transformer_weights={\"trans1\": 2, \"trans2\": 3},\n    )\n    transformer.fit_transform(X)\n    transformed_X = transformer.transform(X)\n    assert transformed_X.shape == (2, 4)  # (2, 4) because trans1 scales and trans2 one-hot encodes\n    assert transformed_X[0, 0] == 2 * 0.0  # scaled by 2\n    assert transformed_X[0, 1] == 0.0  # not scaled\n    assert transformed_X[0, 2] == 1.0  # one-hot encoded\n    assert transformed_X[0, 3] == 1.0  # one-hot encoded\n\n    # Test that weights are applied in fit_transform method\n    X = np.array([[1, 2], [3, 4]])\n    transformer = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [\"feature1\"]),\n         (\"trans2\", OneHotEncoder(), [\"feature2\"])],\n        transformer_weights={\"trans1\": 2, \"trans2\": 3},\n    )\n    transformed_X = transformer.fit_transform(X)\n    assert transformed_X.shape == (2, 4)  # (2, 4) because trans1 scales and trans2 one-hot encodes\n    assert transformed_X[0, 0] == 2 * 0.0  # scaled by 2\n    assert transformed_X[0, 1] == 0.0  # not scaled\n    assert transformed_X[0, 2] == 1.0  # one-hot encoded\n    assert transformed_X[0, 3] == 1.0  # one-hot encoded\n\n    # Test that weights are applied correctly when using sparse matrices\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    transformer = ColumnTransformer(\n        [(\"trans1\", StandardScaler(), [\"feature1\"]),\n         (\"trans2\", OneHotEncoder(), [\"feature2\"])],\n        transformer_weights", "def test_column_transformer_feature_names_out_prefixing():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    ct = ColumnTransformer(\n        [(\"norm1\", Normalizer(norm=\"l1\"), [0]), (\"norm2\", Normalizer(norm=\"l1\"), [1])],\n        verbose_feature_names_out=True,\n    )\n    ct.fit(X, y)\n    result = ct.transform(X)\n    assert_array_equal(result.columns.tolist(), [\"norm1__0\", \"norm2__1\"])\n    assert_array_equal(result.columns.tolist(), [\"norm1__0\", \"norm2__1\"])\n", "def test_ColumnTransformer_remainder_none():\n    \"\"\"Test that ColumnTransformer's remainder=None does not raise an error\n    when used with a transformer that requires 1D input.\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    scaler = make_column_transformer(\n        (StandardScaler(), [\"features\"]),\n        remainder=None,\n        transformers=[\n            (\"passthrough\", Trans(), \"passthrough\"),\n            (\"passthrough\", TransNo2D(), \"features\"),\n        ],\n    )\n    assert scaler.transform(X).shape == (2, 3)\n"], "sample_633": ["def test_hash_lineset():\n    \"\"\"Test hash_lineset function with a single line of code\"\"\"\n    lineset = LineSet(\"test_file\", [\"line1\"])\n    hash_to_index, index_to_lines = hash_lineset(lineset)\n    assert len(hash_to_index) == 0\n    assert len(index_to_lines) == 0\n", "def test_ignore_signatures_multiple_functions():\n    linter = PyLinter()\n    reporter = Reporter(linter, report_progress=False)\n    reporter.write = lambda x: None  # Avoid writing to stdout\n    sim = similar.Similar(\n        min_lines=1,\n        ignore_comments=True,\n        ignore_docstrings=True,\n        ignore_imports=True,\n        ignore_signatures=True,\n    )\n    sim.append_stream(\"file1\", open(SIMILAR_CLS_A, encoding=\"utf-8\"))\n    sim.append_stream(\"file2\", open(SIMILAR_CLS_B, encoding=\"utf-8\"))\n    sim.run()\n    # Check that no message is emitted\n    assert reporter.messages == []\n", "def test_compare_empty_linesets():\n    \"\"\"Test that when there are no common lines between two files, the checker correctly\n    doesn't report any duplicate lines.\"\"\"\n    with StringIO() as f:\n        with redirect_stdout(f):\n            linter = PyLinter()\n            linter.current_name = \"test_file\"\n            similar.Checker(linter).close()\n            output = f.getvalue()\n\n    assert \"R0801\" not in output\n    assert \"similar lines in 0 files\" not in output\n    assert \"TOTAL lines=0 duplicates=0 percent=0.00\" in output\n", "def test_hash_lineset():\n    similar = Similar(\n        min_lines=4,\n        ignore_comments=False,\n        ignore_docstrings=False,\n        ignore_imports=False,\n        ignore_signatures=False,\n    )\n    with open(SIMILAR1, encoding=\"utf-8\") as stream:\n        similar.append_stream(SIMILAR1, stream)\n    similar.run()\n\n    # check hash_lineset is called with correct parameters\n    assert similar.min_lines == 4\n    lineset = similar.linesets[0]\n    hash_to_index, index_to_lines = hash_lineset(lineset, 4)\n    assert len(hash_to_index) == 9\n    assert len(index_to_lines) == 9\n    assert hash_to_index[LinesChunk(lineset.name, 0, \"line1\", \"line2\", \"line3\", \"line4\")][0] == Index(0)\n    assert hash_to_index[LinesChunk(lineset.name, 1, \"line2\", \"line3\", \"line4\", \"line5\")][0] == Index(1)\n    assert index_to_lines[Index(0)].start == 0\n    assert index_to_lines[Index(0)].end == 4\n    assert index_to_lines[Index(1)].start == 1\n    assert index_to_lines[Index(1)].end == 5\n", "def test_process_module(tmp_path):\n    \"\"\"Test process_module\"\"\"\n    file_path = tmp_path / \"test.py\"\n    file_path.write_text(\"def f(): pass\")\n    with open(file_path, encoding=\"utf-8\") as stream:\n        checker = similar.SimilarChecker(PyLinter())\n        checker.process_module(astroid.Module(file_path, stream))\n    assert checker.linesets[0].stripped_lines == [\n        similar.LineSpecifs(text=\"def f():\", line_number=1),\n    ]\n"], "sample_668": ["def test_DEPRECATED_EXTERNAL_PLUGINS():\n    assert isinstance(DEPRECATED_EXTERNAL_PLUGINS, dict)\n    assert isinstance(DEPRECATED_EXTERNAL_PLUGINS[\"pytest_catchlog\"], str)\n    assert isinstance(DEPRECATED_EXTERNAL_PLUGINS[\"pytest_capturelog\"], str)\n    assert isinstance(DEPRECATED_EXTERNAL_PLUGINS[\"pytest_faulthandler\"], str)\n", "def test_DEPRECATED_EXTERNAL_PLUGINS():\n    # Test DEPRECATED_EXTERNAL_PLUGINS is a set\n    assert isinstance(DEPRECATED_EXTERNAL_PLUGINS, set)\n    # Test the set has the correct items\n    assert DEPRECATED_EXTERNAL_PLUGINS == {\"pytest_catchlog\", \"pytest_capturelog\", \"pytest_faulthandler\"}\n", "def test_junit_xml_default_family():\n    \"\"\"Test that the JUnit XML default family warning is correctly issued.\"\"\"\n    # Act\n    with pytest.warns(PytestDeprecationWarning, match=\"The 'junit_family' default value will\"):\n        # Act and Assert\n        assert (\n            pytest.main([\"-junit-xml\", \"--junit-xml\", \"-v\", \"test_fixtures\"]) is None\n        )\n\n", "def test_node_use_from_parent():\n    # Given\n    class TestNode:\n            self.name = name\n\n    # When\n    with pytest.warns(UnformattedWarning, match=NODE_USE_FROM_PARENT.message):\n        node = TestNode(\"MyNode\")\n\n    # Then\n    assert isinstance(node, nodes.Node)\n", "def test_node_use_from_parent():\n    # Create a Node instance\n    node = nodes.Node('test_node')\n\n    # Attempt to use deprecated Node construction\n    with pytest.warns(UnformattedWarning):\n        nodes.Node('test_node_2')\n\n    # Assert the warning is raised\n    assert node.name == 'test_node'\n"], "sample_111": ["    def test_get_queryset_ordering(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=255)\n            number = models.IntegerField()\n\n        class MyAdmin(admin.ModelAdmin):\n            list_display = ('name', 'number')\n            list_display_links = ('name',)\n            list_filter = ('number',)\n\n        request = self._mocked_authenticated_request(reverse('admin:myapp_mymodel_changelist'), self.superuser)\n        change_list = ChangeList(request, MyModel, MyAdmin.list_display, MyAdmin.list_display_links,\n                                MyAdmin.list_filter, None, [], False, 10, None, MyAdmin.list_editable, MyAdmin, None)\n        self.assertEqual(change_list.get_ordering(request, MyModel.objects.all()), ['number', 'name'])\n", "def test_get_ordering_field_with_callable(self):\n    # Test case for get_ordering_field with a callable\n    class BandAdmin(admin.ModelAdmin):\n        ordering = ['name']\n            return 'name'\n\n    band_admin = BandAdmin(Musician, self.site)\n    cl = ChangeList(\n        request=self._mocked_authenticated_request('/admin/musician/list/', self.superuser),\n        model=Musician,\n        list_display=['name', 'my_method'],\n        list_display_links=None,\n        list_filter=[],\n        date_hierarchy=None,\n        search_fields=[],\n        list_select_related=None,\n        list_per_page=None,\n        list_max_show_all=None,\n        list_editable=None,\n        model_admin=band_admin,\n        sortable_by=None,\n    )\n    self.assertEqual(cl.get_ordering_field('my_method'), 'name')\n", "    def test_get_ordering_field_columns(self):\n        # Create test data\n        User.objects.create_superuser(username='admin', email='admin@example.com', password='password')\n        user = User.objects.create(username='test', email='test@example.com', password='password')\n        user.save()\n        self.model_admin = admin.ModelAdmin(model=User, admin_site=custom_site)\n        request = self._mocked_authenticated_request(reverse('admin:auth_user_changelist'), self.superuser)\n        change_list = ChangeList(request, User, ['username', 'email'], [], [], None, [], None, 10, 100, [], self.model_admin, ['username'])\n        self.assertEqual(change_list.get_ordering_field_columns(), {0: 'asc', 1: 'asc'})\n\n        # Test ordering with multiple columns\n        request = self._mocked_authenticated_request(reverse('admin:auth_user_changelist'), self.superuser)\n        request.GET = self.client.get(reverse('admin:auth_user_changelist')).META['QUERY_STRING']\n        request.GET = request.GET.split('&')[0] + '&o=username&ot=desc&' + request.GET.split('&')[1]\n        change_list = ChangeList(request, User, ['username', 'email'], [], [], None, [], None, 10, 100, [], self.model_admin, ['username'])\n        self.assertEqual(change_list.get_ordering_field_columns(), {0: 'desc', 1: 'asc'})\n\n        # Test ordering with single column\n        request = self._mocked_authenticated_request(reverse('admin:auth_user_changelist'), self.superuser)\n        request.GET = self.client.get(reverse('admin:auth_user_changelist')).META['QUERY_STRING']\n        request.GET = request.GET.split('&')[0] + '&o=username&' + request.GET.split('&')[1]\n        change_list = ChangeList(request, User, ['username', 'email'], [], [], None, [], None, 10, 100, [], self.model_admin, ['username'])\n        self.assertEqual(change_list.get_ordering_field_columns(), {0: 'asc'})\n\n        # Test ordering with invalid column\n        request = self._mocked_authenticated_request(reverse('admin:auth_user_changelist'), self.superuser)\n        request.GET = self.client.get(reverse('admin:auth_user_changelist')).META['QUERY_STRING']\n", "    def test_change_list_filtering(self):\n        # Create some objects\n        event = Event.objects.create(name='Event 1', start_date=datetime.date(2022, 1, 1))\n        band = Band.objects.create(name='Band 1', description='Desc 1')\n        musician = Musician.objects.create(name='Musician 1', email='musician1@example.com')\n        Concert.objects.create(event=event, band=band, musician=musician)\n\n        # Create a request with a GET parameter for filtering\n        request = self._mocked_authenticated_request(reverse('admin:concert_changelist'), self.superuser)\n        request.GET._mutable = True\n        request.GET['q'] = 'musician1'\n        request.GET._mutable = False\n\n        # Create a ChangeList instance\n        change_list = ChangeList(request, Concert, ('name', 'event__name'), None, None, None, ('name', 'event__name'), True, 10, None, None, ConcertAdmin, None)\n\n        # Test filtering\n        (filter_specs, has_filters, lookup_params, use_distinct) = change_list.get_filters(request)\n        self.assertTrue(has_filters)\n        self.assertEqual(len(filter_specs), 1)\n        self.assertEqual(filter_specs[0].field, Concert._meta.get_field('name'))\n        self.assertEqual(lookup_params, {'name__icontains': 'musician1'})\n\n        # Test filtering with date_hierarchy\n        change_list = ChangeList(request, Concert, ('name', 'event__name'), None, None, 'start_date', ('name', 'event__name'), True, 10, None, None, ConcertAdmin, None)\n        (filter_specs, has_filters, lookup_params, use_distinct) = change_list.get_filters(request)\n        self.assertTrue(has_filters)\n        self.assertEqual(len(filter_specs), 1)\n        self.assertEqual(filter_specs[0].field, Concert._meta.get_field('start_date'))\n        self.assertEqual(lookup_params, {'start_date__year__gte': 2022, 'start_date__month__gte': 1, 'start_date__day__gte': 1})\n", "def test_change_list_get_ordering(self):\n    request = self._mocked_authenticated_request(reverse('admin:admin_changelist'), self.superuser)\n    change_list = ChangeList(request, models.User, ['name'], [], [], None, ['name'], [], 10, None, None, admin.site.admin, ['name', 'email'])\n    self.assertEqual(change_list.get_ordering(request, change_list.queryset), ['name', 'email'])\n\n    # Test ordering specified in query string\n    request.GET = self.factory.get(reverse('admin:admin_changelist')).GET\n    request.GET._mutable = True\n    request.GET[ORDER_VAR] = '0.name,-1.email'\n    request.GET._mutable = False\n    change_list = ChangeList(request, models.User, ['name'], [], [], None, ['name'], [], 10, None, None, admin.site.admin, ['name', 'email'])\n    self.assertEqual(change_list.get_ordering(request, change_list.queryset), ['-name', 'email'])\n\n    # Test ordering specified in query string with invalid index\n    request.GET = self.factory.get(reverse('admin:admin_changelist')).GET\n    request.GET._mutable = True\n    request.GET[ORDER_VAR] = '0.name,-1.invalid'\n    request.GET._mutable = False\n    change_list = ChangeList(request, models.User, ['name'], [], [], None, ['name'], [], 10, None, None, admin.site.admin, ['name', 'email'])\n    self.assertEqual(change_list.get_ordering(request, change_list.queryset), ['name', 'email'])\n\n    # Test ordering specified in query string with non-existent field\n    request.GET = self.factory.get(reverse('admin:admin_changelist')).GET\n    request.GET._mutable = True\n    request.GET[ORDER_VAR] = '0.name,-1.invalidfield'\n    request.GET._mutable = False\n    change_list = ChangeList(request, models.User, ['name'], [], [], None, ['name'], [], 10, None, None, admin.site.admin, ['name', 'email'])\n    self.assertEqual(change_list.get_ordering(request, change_list.queryset), ['name', 'email'])\n\n    # Test ordering specified in ModelAdmin\n    admin_class = admin.AdminSite().unregistered(models.User)\n    admin_class.ordering = ['email', 'name']\n    change_list = ChangeList(request, models.User, ['name'], [], [], None, ['name'], [],"], "sample_812": ["    def test_nmf_repr(self):\n        nmf = NMF(n_components=10, init='random', solver='cd',\n                  beta_loss='frobenius', tol=1e-5, max_iter=250,\n                  random_state=42, alpha=1.0, l1_ratio=0.5, verbose=2,\n                  shuffle=True)\n        expected_repr = (\n            \"NMF(n_components=10, init='random', solver='cd', \"\n            \"beta_loss='frobenius', tol=1e-5, max_iter=250, \"\n            \"random_state=42, alpha=1.0, l1_ratio=0.5, \"\n            \"verbose=2, shuffle=True)\"\n        )\n        self.assertEqual(_EstimatorPrettyPrinter().pprint(nmf), expected_repr)\n", "def test_safe_repr_list_estimators():\n    pp = _EstimatorPrettyPrinter(indent=1, width=80, depth=None,\n                                 stream=None, compact=False, indent_at_name=True,\n                                 n_max_elements_to_show=5)\n    lr = LogisticRegression()\n    pp._dispatch[BaseEstimator.__repr__] = _pprint_estimator\n    assert pp._safe_repr([lr, lr], {}, 10, 0, changed_only=False)[0] == \"['<__main__.LogisticRegression object at 0x...>']\"\n\n", "def test_estimator_pretty_printing_with_changed_only():\n    estimator = LogisticRegression(penalty='l2', dual=True, tol=1e-3, C=0.5)\n    pp = _EstimatorPrettyPrinter(changed_only=True)\n    actual = pp._safe_repr(estimator, {}, 10, 0, changed_only=True)\n    expected = (\"LogisticRegression(dual=True, tol=1e-03, C=0.5)\", True, False)\n    assert actual == expected\n", "def test_EstimatorPrettyPrinter_changed_params_repr():\n    # Test that the changed parameters are correctly represented\n    pp = _EstimatorPrettyPrinter(width=50)\n    estimator = LogisticRegression(C=1.0, dual=False, tol=1e-5)\n    expected_output = \"LogisticRegression(C=1.0, dual=False, tol=1e-5)\"\n    assert pp.format(estimator, None, 0, 0) == (expected_output, True, False)\n\n    estimator = LogisticRegression(C=1.0, dual=False, tol=1e-5)\n    estimator.tol = 1e-4\n    pp = _EstimatorPrettyPrinter(width=50)\n    expected_output = \"LogisticRegression(C=1.0, dual=False, tol=1e-4)\"\n    assert pp.format(estimator, None, 0, 0) == (expected_output, True, False)\n\n    # Test that non-changed parameters are not represented\n    estimator = LogisticRegression(C=1.0, dual=False, tol=1e-4)\n    pp = _EstimatorPrettyPrinter(width=50)\n    expected_output = \"LogisticRegression(C=1.0, dual=False)\"\n    assert pp.format(estimator, None, 0, 0) == (expected_output, True, False)\n\n", "def test_pprint_PCA():\n    pp = _EstimatorPrettyPrinter(width=100, indent_at_name=False)\n    pp._changed_only = False\n\n    pca = PCA(n_components=0.5, whiten=True, svd_solver='arpack', tol=0.001)\n    print(pp.pformat(pca))\n    assert \"PCA(n_components=0.5, whiten=True, svd_solver='arpack', tol=0.001)\" in repr(pca)\n\n    pca = PCA(n_components=None, copy=False, whiten=True, svd_solver='arpack', tol=0.001)\n    print(pp.pformat(pca))\n    assert \"PCA(n_components=None, whiten=True, svd_solver='arpack', tol=0.001)\" in repr(pca)\n\n"], "sample_99": ["    def test_extract_datetime_field(self):\n        model = DTModel.objects.create(\n            name='model',\n            start_datetime=datetime(2020, 1, 1, 12, 0, 0, tzinfo=datetime_timezone('UTC')),\n            end_datetime=datetime(2020, 1, 1, 13, 0, 0, tzinfo=datetime_timezone('UTC')),\n        )\n        dt = Extract('start_datetime', 'year')\n        result = dt.resolve_expression(query=Q(pk=model.pk))\n        self.assertEqual(result.lhs, model.start_datetime)\n        self.assertEqual(result.output_field, IntegerField())\n        self.assertEqual(result.lookup_name, 'year')\n", "    def create_model(self, start_datetime, end_datetime):\n        return DTModel.objects.create(\n            name=start_datetime.isoformat() if start_datetime else 'None',\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            start_date=start_datetime.date() if start_datetime else None,\n            end_date=end_datetime.date() if end_datetime else None,\n            start_time=start_datetime.time() if start_datetime else None,\n            end_time=end_datetime.time() if end_datetime else None,\n            duration=(end_datetime - start_datetime) if start_datetime and end_datetime else None,\n        )\n", "    def test_extract_duration(self):\n        # Create some sample data\n        model = DTModel.objects.create(\n            name='None',\n            start_datetime=datetime(2020, 1, 1, 10, 0, 0),\n            end_datetime=datetime(2020, 1, 1, 12, 0, 0),\n            duration=timedelta(hours=2),\n        )\n        self.assertEqual(Extract(OuterRef('duration')).resolve_expression().as_sql(None, None)[0], 'EXTRACT(EPOCH FROM duration)')\n", "    def create_model(self, start_datetime, end_datetime):\n        return DTModel.objects.create(\n            name=start_datetime.isoformat() if start_datetime else 'None',\n            start_datetime=start_datetime,\n            end_datetime=end_datetime,\n            start_date=start_datetime.date() if start_datetime else None,\n            end_date=end_datetime.date() if end_datetime else None,\n            start_time=start_datetime.time() if start_datetime else None,\n            end_time=end_datetime.time() if end_datetime else None,\n            duration=(end_datetime - start_datetime) if start_datetime and end_datetime else None,\n        )\n", "    def test_datetime_trunc_sql(self):\n        from django.db import connection\n\n        with connection.cursor() as cursor:\n            query = ExtractHour(\"datetime_field\")\n            sql, params = query.as_sql(cursor.connection)\n            cursor.execute(sql, params)\n            self.assertEqual(cursor.fetchall()[0][0], 'EXTRACT(HOUR FROM datetime_field)')\n"], "sample_306": ["    def test_parse_duration_iso8601_with_seconds_fraction(self):\n        # Test parsing ISO 8601 duration with seconds fraction\n        self.assertEqual(\n            parse_duration('P1D2H3M4.56789S'),\n            timedelta(days=1, seconds=1804.56789)\n        )\n", "    def test_parse_duration_iso8601_with_negative_seconds(self):\n        # Test ISO 8601 representation with negative seconds.\n        self.assertEqual(parse_duration(\"P1DT-1.000001S\"), timedelta(days=1, seconds=-1))\n", "def test_parse_duration_iso8601_with_seconds(self):\n    \"\"\"Test that parse_duration() handles an ISO 8601 duration with seconds.\"\"\"\n    value = \"P0DT1S\"\n    result = parse_duration(value)\n    self.assertIsInstance(result, timedelta)\n    self.assertEqual(result.days, 0)\n    self.assertEqual(result.seconds, 1)\n", "    def test_parse_duration_iso8601_with_seconds(self):\n        self.assertEqual(parse_duration('P1S'), timedelta(seconds=1))\n        self.assertEqual(parse_duration('PT1S'), timedelta(seconds=1))\n        self.assertEqual(parse_duration('P1.000S'), timedelta(seconds=1))\n        self.assertEqual(parse_duration('PT1.000S'), timedelta(seconds=1))\n", "    def test_parse_datetime_tz_offset(self):\n        self.assertEqual(\n            parse_datetime('2022-01-01 12:00:00+00:00'),\n            datetime(2022, 1, 1, 12, 0, 0, tzinfo=utc)\n        )\n        self.assertEqual(\n            parse_datetime('2022-01-01 12:00:00-01:00'),\n            datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(-60))\n        )\n        self.assertEqual(\n            parse_datetime('2022-01-01 12:00:00+01:00'),\n            datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(60))\n        )\n"], "sample_921": ["def test_signature_from_str_invalid():\n    \"\"\"Test invalid input to signature_from_str.\"\"\"\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, *, c, d): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, *args, c, d): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c=2, d=3): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, *, d=3): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, **kwargs): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, *args, **kwargs): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c=2, d, e): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, *, d, e): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e, **kwargs): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e, *args, **kwargs): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e, f): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c=2, d, e, f): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, *, d, e, f): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e, f, **kwargs): pass:\")\n\n    with pytest.raises(ValueError):\n        inspect.signature_from_str(\"def func(a, b, c, d, e, f, *args, **kwargs): pass:\")\n", "def test_isclassmethod():\n    class MyClass:\n            pass\n    my_method = MyClass().my_method\n    assert inspect.isclassmethod(my_method) == True\n", "def test_isbuiltin():\n    # Test if isbuiltin() correctly identifies built-in functions\n    assert inspect.isbuiltin(int)  # Built-in function\n    assert inspect.isbuiltin(len)  # Built-in function\n\n    # Test if isbuiltin() correctly identifies built-in classes\n    assert not inspect.isbuiltin(int)  # Not a built-in class\n    assert not inspect.isbuiltin(str)  # Not a built-in class\n\n    # Test if isbuiltin() correctly handles wrapped functions\n        return x\n    assert not inspect.isbuiltin(wrapper)  # Wrapped function is not built-in\n", "def test_isenumclass():\n    class MyEnum(enum.Enum):\n        A = 1\n        B = 2\n\n    class MyType(type):\n            super().__init__(*args, **kwargs)\n\n    assert inspect.isenumclass(MyEnum)\n    assert not inspect.isenumclass(MyType())\n\n    assert inspect.isenumclass(enum.Enum)\n    assert not inspect.isenumclass(str)\n\n    class MyEnumLike(type):\n            super().__init__(*args, **kwargs)\n\n    assert not inspect.isenumclass(MyEnumLike)\n", "def test_isbuiltin_class_method():\n    assert is_builtin_class_method(list, '__init__')\n    assert is_builtin_class_method(dict, '__init__')\n    assert not is_builtin_class_method(object, '__init__')\n    assert not is_builtin_class_method(int, '__dict__')\n"], "sample_410": ["    def test_make_random_password(self):\n        manager = User.objects._default_manager\n        with ignore_warnings(category=RemovedInDjango51Warning):\n            password = manager.make_random_password()\n            self.assertEqual(len(password), 10)\n            self.assertEqual(set(password), set(manager.allowed_chars))\n", "    def test_normalize_username(self):\n        user = AbstractBaseUser(username='abc')\n        user.save()\n        self.assertEqual(user.get_username(), 'abc')\n\n        # Test case with non-ASCII characters\n        user = AbstractBaseUser(username=' caf\u00e9')\n        user.save()\n        self.assertEqual(user.get_username(), ' caf\u00e9')\n\n        # Test case with empty string\n        user = AbstractBaseUser(username='')\n        user.save()\n        self.assertEqual(user.get_username(), '')\n\n        # Test case with None\n        user = AbstractBaseUser(username=None)\n        user.save()\n        self.assertEqual(user.get_username(), None)\n", "    def test_get_session_auth_hash(self):\n        \"\"\"\n        Test that get_session_auth_hash returns the correct HMAC.\n        \"\"\"\n        user = get_user_model().objects.create_user(username=\"testuser\", email=\"test@example.com\", password=\"password123\")\n        expected_hash = user.get_session_auth_hash()\n        self.assertIsInstance(expected_hash, str)\n        self.assertEqual(len(expected_hash), 64)  # SHA-256 hash length\n", "    def test_normalize_email(self):\n        \"\"\"Test that normalize_email method works correctly.\"\"\"\n        # Test with an email address with uppercase domain\n        email = \"test@EXAMPLE.com\"\n        expected = \"test@example.com\"\n        self.assertEqual(BaseUserManager.normalize_email(email), expected)\n\n        # Test with an email address without a domain\n        email = \"test\"\n        self.assertEqual(BaseUserManager.normalize_email(email), email)\n\n        # Test with an invalid email address\n        email = \"test@\"\n        self.assertEqual(BaseUserManager.normalize_email(email), email)\n\n        # Test with a None email\n        self.assertEqual(BaseUserManager.normalize_email(None), \"\")\n", "    def test_is_anonymous_and_is_authenticated_properties(self):\n        user = get_user_model().objects.create_user(username=\"testuser\", password=\"password123\")\n        self.assertFalse(user.is_anonymous)\n        self.assertTrue(user.is_authenticated)\n"], "sample_480": ["    def test_has_key_lookup(self):\n        model = JSONModel.objects.create(data={\"name\": \"John\", \"age\": 30})\n        queryset = JSONModel.objects.filter(data__has_key=\"name\")\n        self.assertEqual(queryset.count(), 1)\n", "    def test_custom_decoder(self):\n        json_model = JSONModel.objects.create(data={'key': 'value'})\n        decoded_data = CustomJSONDecoder().object_hook(json.loads(json_model.data))\n        self.assertEqual(decoded_data, {'key': 'value'})\n", "    def test_custom_decoder(self):\n        model = JSONModel()\n        model.json_field = {\"key\": \"value\"}\n        json_field = model.json_field\n        custom_decoder = CustomJSONDecoder()\n        try:\n            json.loads(json_field, cls=custom_decoder)\n            self.fail(\"JSONDecodeError was expected\")\n        except json.JSONDecodeError:\n            pass\n", "    def test_jsonfield_from_db_value_on_none(self):\n        \"\"\"\n        Regression test for #21711: Calling from_db_value on a JSONField instance\n        with a null value should return None.\n        \"\"\"\n        model = JSONModel()\n        json_field = JSONField()\n        self.assertIsNone(json_field.from_db_value(None, model, connection))\n", "    def test_custom_encoder(self):\n        with warnings.catch_warnings(record=True) as warning_record:\n            warnings.simplefilter(\"always\")\n            class CustomEncoder(json.JSONEncoder):\n                    return \"custom encoded value\"\n\n            json_field = JSONField(encoder=CustomEncoder)\n            json_field.clean({\"test\": \"value\"})\n            self.assertEqual(len(warning_record), 0)\n        self.assertEqual(len(warning_record), 0)\n"], "sample_1130": ["def test_point_partial_velocity():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    p = Point('p')\n    u1, u2 = dynamicsymbols('u1 u2')\n    p.set_vel(N, u1 * N.x + u2 * A.y)\n    assert p.partial_velocity(N, u1) == N.x\n    assert p.partial_velocity(N, u1, u2) == (N.x, A.y)\n    p2 = Point('p2')\n    p2.set_vel(N, u1 * N.x + u2 * A.y)\n    assert p2.partial_velocity(N, u1, u2) == (N.x, A.y)\n", "    def test_pos_from(self):\n        N = ReferenceFrame('N')\n        p1 = Point('p1')\n        p2 = Point('p2')\n        p1.set_pos(p2, 10 * N.x)\n        assert p1.pos_from(p2) == 10 * N.x\n        assert p2.pos_from(p1) == -10 * N.x\n        with raises(ValueError):\n            p2.pos_from(p1)  # Should fail because it is trying to calculate the distance from a point to itself\n", "def test_locatenew_with_different_frames():\n    N = ReferenceFrame('N')\n    M = ReferenceFrame('M')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    P3 = P1.locatenew('P3', 5 * M.x)\n    \n    # Check if P2 is in N frame\n    assert P2.pos_from(P1) == 10 * N.x\n    \n    # Check if P3 is in M frame\n    assert P3.pos_from(P1) == 5 * M.x\n    \n    # Check if P2 is not in M frame\n    assert P2.pos_from(P1) != 5 * M.x\n    \n    # Check if P3 is not in N frame\n    assert P3.pos_from(P1) != 10 * N.x\n", "def test_locatenew_pos():\n    N = ReferenceFrame('N')\n    P = Point('P')\n    Q = P.locatenew('Q', 10 * N.x)\n    assert Q.pos_from(P) == 10 * N.x\n    assert P.pos_from(Q) == -10 * N.x\n    assert Q.pos_from(Q) == Vector(0)\n", "def test_vel_pos_consistency():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    q = dynamicsymbols('q')\n    p1.set_vel(N, 10 * N.x)\n    p2.set_pos(p1, q * N.x)\n    assert p2.pos_from(p1) == q * N.x\n    assert p2.vel(N) == 10 * N.x + q * N.x\n    assert p2.vel(N) != 10 * N.x\n"], "sample_1": ["def test_is_separable_shift_and_scale():\n    \"\"\"Test that Shift and Scale are separable.\"\"\"\n    sh1 = models.Shift(1, name='shift1')\n    scl1 = models.Scale(1, name='scale1')\n    assert is_separable(sh1) == np.array([True])\n    assert is_separable(scl1) == np.array([True])\n    assert is_separable(sh1 | scl1) == np.array([True, True])\n    assert is_separable(sh1 & scl1) == np.array([False])\n", "def test_separability_matrix_independent_inputs():\n    sh1 = models.Shift(1, name='shift1')\n    sh2 = models.Shift(2, name='sh2')\n    scl1 = models.Scale(1, name='scl1')\n    scl2 = models.Scale(2, name='scl2')\n    map1 = Mapping((0, 1, 0, 1), name='map1')\n    \n    # Test separability matrix for independent inputs\n    cm = sh1 & sh2 | scl1 & scl2 | map1\n    expected_separability_matrix = np.array([[True, True, False, False],\n                                            [True, True, False, False],\n                                            [False, False, True, False],\n                                            [False, False, False, True]])\n    assert_allclose(separability_matrix(cm), expected_separability_matrix)\n\n    # Test separability matrix for independent inputs with no mixing\n    cm = scl1 & scl2 | map1\n    expected_separability_matrix = np.array([[True, False, False],\n                                            [False, True, False],\n                                            [False, False, True]])\n    assert_allclose(separability_matrix(cm), expected_separability_matrix)\n\n    # Test separability matrix for independent inputs with mixing\n    cm = sh1 & sh2 | map1\n    expected_separability_matrix = np.array([[True, False, False],\n                                            [False, True, False],\n                                            [True, False, True]])\n    assert_allclose(separability_matrix(cm), expected_separability_matrix)\n", "def test__compute_n_outputs():\n    # Test cases for n_outputs calculation\n    # When both models are simple models\n    assert _compute_n_outputs(sh1, sh2) == sh1.n_outputs + sh2.n_outputs\n    # When both models are ndarrays\n    assert _compute_n_outputs(np.ones((3, sh1.n_inputs)), np.ones((3, sh2.n_inputs))) == sh1.n_outputs + sh2.n_outputs\n    # When left is a model and right is an ndarray\n    assert _compute_n_outputs(sh1, np.ones((3, sh2.n_inputs))) == sh1.n_outputs + sh2.n_outputs\n    # When left is an ndarray and right is a model\n    assert _compute_n_outputs(np.ones((3, sh1.n_inputs)), sh2) == sh1.n_outputs + sh2.n_outputs\n", "def test_separability_arithmetic_operations():\n    \"\"\"\n    Tests for arithmetic operators on models with different n_inputs and n_outputs\n    \"\"\"\n    sh = models.Shift(1, name='sh')\n    scl = models.Scale(2, name='scl')\n    p1 = models.Polynomial1D(1, name='p1')\n    p2 = models.Polynomial1D(2, name='p2')\n\n    # Test with models having different n_inputs and n_outputs\n    # Addition\n    result = sh + scl\n    expected_result = _arith_oper(sh, scl)\n    assert np.array_equal(separability_matrix(result), np.array([[False, False])))\n    # Subtraction\n    result = sh - scl\n    expected_result = _arith_oper(sh, scl)\n    assert np.array_equal(separability_matrix(result), np.array([[False, False])))\n    # Multiplication\n    result = sh * scl\n    expected_result = _arith_oper(sh, scl)\n    assert np.array_equal(separability_matrix(result), np.array([[False, False])))\n    # Division\n    result = sh / scl\n    expected_result = _arith_oper(sh, scl)\n    assert np.array_equal(separability_matrix(result), np.array([[False, False])))\n    # Exponentiation\n    result = sh ** scl\n    expected_result = _arith_oper(sh, scl)\n    assert np.array_equal(separability_matrix(result), np.array([[False, False]))\n\n", "def test_separability_matrix_polynomial_multiply():\n    # Test polynomial multiplication\n    p11 = models.Polynomial1D(1)\n    p21 = models.Polynomial1D(2)\n    p12 = models.Polynomial1D(1)\n    p22 = models.Polynomial1D(2)\n    p1m = p11 * p21\n    p2m = p12 * p22\n\n    # Check that multiplication of two polynomials produces a separable output\n    is_separable = is_separable(p1m)\n    assert np.all(is_separable == np.array([True]))\n\n    # Check that the separability matrix of a polynomial multiplication is correct\n    expected = np.array([[True, False, True, False, True, False, True, False, True, False],\n                        [False, True, False, True, False, True, False, True, False, True],\n                        [True, False, True, False, True, False, True, False, True, False],\n                        [False, True, False, True, False, True, False, True, False, True],\n                        [True, False, True, False, True, False, True, False, True, False],\n                        [False, True, False, True, False, True, False, True, False, True],\n                        [True, False, True, False, True, False, True, False, True, False],\n                        [False, True, False, True, False, True, False, True, False, True],\n                        [True, False, True, False, True, False, True, False, True, False],\n                        [False, True, False, True, False, True, False, True, False, True]])\n\n    matrix = separability_matrix(p1m)\n    assert_allclose(matrix, expected)\n"], "sample_546": ["def test_subplots_adjust_fails_with_incompatible_engine():\n    # Create a figure with a layout engine that is not compatible with subplots_adjust\n    fig = Figure()\n    fig.set_layout_engine(layout=\"constrained\")\n    fig.subplotpars.update(left=0.1, bottom=0.1, right=0.9, top=0.9)\n\n    # Try to adjust the subplot layout, which should raise a warning\n    with pytest.warns(UserWarning):\n        fig.subplots_adjust(left=0.2, bottom=0.2, right=0.8, top=0.8)\n", "def test_Figure__get_dpi():\n    fig = Figure()\n    assert fig.dpi == 100\n    fig.dpi = 200\n    assert fig.dpi == 200\n    fig.dpi = 50\n    assert fig.dpi == 50\n", "    def test_add_subplots(self):\n        fig = Figure(figsize=(8, 6))\n        ax1 = fig.add_subplot(221)\n        ax2 = fig.add_subplot(222)\n        ax3 = fig.add_subplot(223)\n        ax4 = fig.add_subplot(224)\n\n        self.assertEqual(len(fig.axes), 4)\n", "def test_set_layout_engine_deprecation():\n    \"\"\"Verify that deprecation warnings are raised when setting layout engine.\"\"\"\n    # Set up a figure with a default layout engine\n    fig = Figure()\n    # Try to set a new layout engine\n    with pytest.warns(UserWarning, match=_EXPECT_DEPRECATION_MESSAGE):\n        fig.set_layout_engine('tight')\n    # Check that the warning message is as expected\n    assert \"Use `layout='tight'` instead\" in str(fig._layout_engine)\n\n    # Try to set a new layout engine with specific parameters\n    with pytest.warns(UserWarning, match=_EXPECT_DEPRECATION_MESSAGE):\n        fig.set_layout_engine('tight', pad=0.5, h_pad=0.2, w_pad=0.1)\n", "def test_figure_layout(layout, tight_layout, constrained_layout):\n    fig, ax = plt.subplots()\n    fig.set_layout_engine(layout)\n    fig.tight_layout(tight=tight_layout)\n    fig.constrained_layout(constrained=constrained_layout)\n    assert fig.get_layout_engine() is not None\n    assert fig.get_layout_engine().layout == layout\n"], "sample_943": ["def test_apidoc_no_excludes(apidoc):\n    assert apidoc.coderoot == 'test-root'\n    assert apidoc.outdir == 'tempdir/out'\n", "def test_apidoc_module_first(apidoc):\n    modules = apidoc.outdir / 'index.rst'\n    with open(modules, 'r') as f:\n        content = f.read()\n    assert 'Module' in content\n    assert 'Submodule' in content\n", "def test_recurse_tree_excludes_directory(apidoc):\n    assert apidoc.coderoot == 'test-root'\n    assert apidoc.outdir == path(tempdir / 'out')\n    # Check that the excluded directory is not present in the output\n    assert not (apidoc.outdir / 'exclude').exists()\n", "def test_apidoc__excludes_multiple_top_level_modules(apidoc):\n    \"\"\"Test excluding multiple top-level modules.\"\"\"\n    code = (apidoc.coderoot / 'module1.py').write_text('def foo(): pass')\n    code = (apidoc.coderoot / 'module2.py').write_text('def bar(): pass')\n    excludes = [apidoc.coderoot / 'module1.py', apidoc.coderoot / 'module2.py']\n    apidoc_main(['-o', apidoc.outdir, '-F', apidoc.coderoot] + excludes)\n    assert not (apidoc.outdir / 'module1.rst').exists()\n    assert not (apidoc.outdir / 'module2.rst').exists()\n    assert (apidoc.outdir / 'module1__init__.rst').exists()\n    assert (apidoc.outdir / 'module2__init__.rst').exists()\n    assert (apidoc.outdir / 'modules.rst').exists()\n", "def test_apidoc_module_first_true(tmpdir, apidoc):\n    \"\"\"Test apidoc --module-first.\"\"\"\n    # Check if there is a module file generated in the output directory\n    modules = [f for f in apidoc.outdir.listdir() if f.endswith('.rst')]\n    assert len(modules) > 0\n\n    # Check if the module file contains the module name in the first line\n    with open(apidoc.outdir / modules[0]) as f:\n        content = f.read()\n    assert apidoc.coderoot.name in content\n\n"], "sample_1048": ["def test_parabola_eccentricity():\n    # Test that the eccentricity of a parabola is 1\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.eccentricity == 1\n\n    # Test that the eccentricity of a parabola is 1 for a horizontal parabola\n    p2 = Parabola(Point(0, 0), Line(Point(0, 5), Point(0, 7)))\n    assert p2.eccentricity == 1\n\n    # Test that the eccentricity of a parabola is 1 for a vertical parabola\n    p3 = Parabola(Point(0, 0), Line(Point(5, 0), Point(7, 0)))\n    assert p3.eccentricity == 1\n", "def test_parabola_eccentricity():\n    # Test if the eccentricity of a parabola is always 1\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.eccentricity == S(1)\n", "def test_parabola_eccentricity():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.eccentricity == 1\n", "def test_parabola_focal_length():\n    from sympy import Rational, oo, sqrt, S\n    x, y = symbols('x y', real=True)\n    p1 = Parabola(Point(0, 0), Line(Point(1, -1), Point(1, 1)))\n    assert p1.focal_length == 0\n    p2 = Parabola(Point(0, 0), Line(Point(1, 0), Point(-1, 0)))\n    assert p2.focal_length == 1\n    p3 = Parabola(Point(0, 0), Line(Point(0, 1), Point(0, -1)))\n    assert p3.focal_length == 1\n    p4 = Parabola(Point(1, 0), Line(Point(0, 1), Point(0, -1)))\n    assert p4.focal_length == sqrt(2)\n", "def test_parabola_focal_length_symbolic_input():\n    # Test focal length with symbolic input\n    x = symbols('x')\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    p1_directrix = p1.directrix\n    assert p1.focal_length == 4\n    p2 = Parabola(Point(0, 0), Line(Point(x, 8), Point(x+1, 8)))\n    assert p2.focal_length == 1\n"], "sample_1129": ["def test_SparseMatrix_printing():\n    from sympy.codegen import Assignment\n    from sympy.codegen.ast import none\n    from sympy.codegen.scipy_nodes import cosm1\n    from sympy.core import Expr, Mod, symbols, Eq, Le, Gt, zoo, oo, Rational, Pow\n    from sympy.core.numbers import pi\n    from sympy.core.singleton import S\n    from sympy.functions import acos, KroneckerDelta, Piecewise, sign, sqrt\n    from sympy.logic import And, Or\n    from sympy.matrices import SparseMatrix, MatrixSymbol, Identity\n    from sympy.printing.pycode import (\n        MpmathPrinter, NumPyPrinter, PythonCodePrinter, pycode, SciPyPrinter,\n        SymPyPrinter\n    )\n    from sympy.testing.pytest import raises\n    from sympy.tensor import IndexedBase\n    from sympy.external import import_module\n    from sympy.functions.special.gamma_functions import loggamma\n\n    x, y, z = symbols('x y z')\n    p = IndexedBase(\"p\")\n\n    # Test Mpmath printer\n    mpmath_expr = SparseMatrix([[pi, 0], [0, pi])\n    mpmath_printer = MpmathPrinter()\n    mpmath_output = mpmath_printer.doprint(mpmath_expr)\n    assert mpmath_output == \"mpmath.mpmath.matrix([[3.14159265359, 0], [0, 3.14159265359]])\"\n\n    # Test NumPy printer\n    numpy_expr = SparseMatrix([[pi, 0], [0, pi])\n    numpy_printer = NumPyPrinter()\n    numpy_output = numpy_printer.doprint(numpy_expr)\n    assert numpy_output == \"numpy.array([[3.14159265359, 0], [0, 3.14159265359]])\"\n\n    # Test SciPy printer\n    scipy_expr = SparseMatrix([[pi, 0], [0, pi])\n    scipy_printer = SciPyPrinter()\n    scipy_output = scipy_printer.doprint(scipy_expr)\n    assert scipy_output == \"scipy.sparse.coo_matrix(([3.14159265359, 0], ([0, 1], [0, 1])), shape=(2, 2))\"\n\n    # Test SymPy printer\n    sympy_expr = SparseMatrix([[pi, 0], [0,", "def test_PythonCodePrinter_print_SparseMatrix():\n    from sympy.codegen import Assignment\n    from sympy.codegen.ast import none\n    from sympy.codegen.cfunctions import expm1, log1p\n    from sympy.codegen.scipy_nodes import cosm1\n    from sympy.codegen.matrix_nodes import MatrixSolve\n    from sympy.core import Expr, Mod, symbols, Eq, Le, Gt, zoo, oo, Rational, Pow\n    from sympy.core.numbers import pi\n    from sympy.core.singleton import S\n    from sympy.functions import acos, KroneckerDelta, Piecewise, sign, sqrt\n    from sympy.logic import And, Or\n    from sympy.matrices import SparseMatrix, MatrixSymbol, Identity\n    from sympy.printing.pycode import (\n        MpmathPrinter, NumPyPrinter, PythonCodePrinter, pycode, SciPyPrinter,\n        SymPyPrinter\n    )\n    from sympy.testing.pytest import raises\n    from sympy.tensor import IndexedBase\n    from sympy.external import import_module\n    from sympy.functions.special.gamma_functions import loggamma\n\n    x, y, z = symbols('x y z')\n    p = IndexedBase(\"p\")\n    m = SparseMatrix(((1, 2), (3, 4)))\n    assert pycode(m, fully_qualified_modules=False) == \"numpy.array([[1, 2], [3, 4]])\"\n    assert pycode(m, fully_qualified_modules=True) == \"numpy.array([[1, 2], [3, 4]], dtype=int32)\"\n    assert pycode(m, fully_qualified_modules=True, standard=\"python3\") == \"numpy.array([[1, 2], [3, 4]], dtype=int32)\"\n    assert pycode(m, fully_qualified_modules=False, standard=\"python3\") == \"numpy.array([[1, 2], [3, 4]])\"\n", "    def test_print_pow_with_negative_integer_exp(self):\n        printer = PythonCodePrinter()\n        x = symbols('x')\n        expr = Pow(x, -2)\n        result = printer.doprint(expr)\n        assert result == '1/x**2'\n", "def test_Piecewise_with_NaN_in_cond():\n    from sympy import Piecewise, sin, oo, nan, S\n    from sympy.printing.pycode import PythonCodePrinter\n    # Test Piecewise with a condition that equals NaN\n    pw = Piecewise((sin(x), x > 0), (nan, x == oo))\n    printer = PythonCodePrinter({'standard': 'python3'})\n    assert printer.doprint(pw) == '(math.sin(x) if x > 0 else nan)'\n    # Test Piecewise with a condition that equals NaN in NumPy\n    pw = Piecewise((sin(x), x > 0), (nan, x == oo))\n    printer = NumPyPrinter()\n    assert printer.doprint(pw) == '(np.where(x > 0, np.sin(x), np.nan))'\n", "def test_print_Piecewise_with_default_value():\n    from sympy import Piecewise, S, cos, sin\n\n    p = Piecewise(\n        (sin(x), x < 0),\n        (cos(x), x > 0),\n        (0, True)\n    )\n\n    printer = PythonCodePrinter({'standard':'python3'})\n    assert printer.doprint(p) == 'piecewise(x < 0, sin(x), x > 0, cos(x), True, 0)'\n\n    printer = NumPyPrinter()\n    assert printer.doprint(p) == 'np.select([x < 0, x > 0], [np.sin(x), np.cos(x)], default=0)'\n\n    printer = SciPyPrinter()\n    assert printer.doprint(p) == 'scipy.special.select([x < 0, x > 0], [sp.sin(x), sp.cos(x)], default=0)'\n\n    printer = SymPyPrinter()\n    assert printer.doprint(p) == 'sp.select([x < 0, x > 0], [sp.sin(x), sp.cos(x)], default=0)'\n"], "sample_1085": ["def test_simplify_rational():\n    assert simplify(Rational(2, 3)) == Rational(2, 3)\n    assert simplify(Rational(3, 5)) == Rational(3, 5)\n    assert simplify(Rational(4, 5)) == Rational(4, 5)\n    assert simplify(Rational(5, 7)) == Rational(5, 7)\n    assert simplify(Rational(6, 8)) == Rational(3, 4)\n    assert simplify(Rational(5, 12)) == Rational(5, 12)\n    assert simplify(Rational(7, 8)) == Rational(7, 8)\n    assert simplify(Rational(9, 12)) == Rational(3, 4)\n    assert simplify(Rational(11, 12)) == Rational(11, 12)\n    assert simplify(Rational(13, 20)) == Rational(13, 20)\n    assert simplify(Rational(15, 20)) == Rational(3, 4)\n    assert simplify(Rational(17, 20)) == Rational(17, 20)\n    assert simplify(Rational(19, 20)) == Rational(19, 20)\n    assert simplify(Rational(21, 28)) == Rational(3, 4)\n    assert simplify(Rational(23, 28)) == Rational(23, 28)\n    assert simplify(Rational(25, 28)) == Rational(25, 28)\n    assert simplify(Rational(27, 28)) == Rational(27, 28)\n    assert simplify(Rational(29, 36)) == Rational(29, 36)\n    assert simplify(Rational(31, 36)) == Rational(31, 36)\n    assert simplify(Rational(33, 36)) == Rational(11, 12)\n    assert simplify(Rational(35, 36)) == Rational(35, 36)\n    assert simplify(Rational(37, 44)) == Rational(37, 44)\n    assert simplify(Rational(39, 44)) == Rational(39, 44)\n    assert simplify(Rational(41, 44)) == Rational(41, 44)\n    assert simplify(Rational(43, 44)) == Rational(43, 44)\n    assert simplify(Rational(45, 56)) == Rational(15, 28)\n    assert", "def test_rational_mod():\n    assert Rational(11, 3).mod(7) == Rational(1, 3)\n    assert Rational(11, 3).mod(-7) == Rational(2, 3)\n    assert Rational(11, 3).mod(8) == Rational(4, 3)\n    assert Rational(11, 3).mod(-8) == Rational(1, 3)\n    assert Rational(11, 3).mod(9) == Rational(1, 3)\n    assert Rational(11, 3).mod(-9) == Rational(4, 3)\n    assert Rational(11, 3).mod(10) == Rational(3, 3)\n    assert Rational(11, 3).mod(-10) == Rational(2, 3)\n    assert Rational(11, 3).mod(11) == Rational(0, 3)\n    assert Rational(11, 3).mod(-11) == Rational(2, 3)\n    assert Rational(11, 3).mod(12) == Rational(4, 3)\n    assert Rational(11, 3).mod(-12) == Rational(1, 3)\n    assert Rational(11, 3).mod(0) == Rational(11, 3)\n\n    assert Rational(1, 3).mod(7) == Rational(1, 3)\n    assert Rational(1, 3).mod(-7) == Rational(2, 3)\n    assert Rational(1, 3).mod(8) == Rational(4, 3)\n    assert Rational(1, 3).mod(-8) == Rational(1, 3)\n    assert Rational(1, 3).mod(9) == Rational(1, 3)\n    assert Rational(1, 3).mod(-9) == Rational(4, 3)\n    assert Rational(1, 3).mod(10) == Rational(3, 3)\n    assert Rational(1, 3).mod(-10) == Rational(2, 3)\n    assert Rational(1, 3).mod(11) == Rational(0, 3)\n    assert Rational(1, 3).mod(-11) == Rational(2, 3)\n    assert Rational(1, 3).mod(12) ==", "def test_mpf_norm():\n    a = Float('1.23456789', 15)\n    b = Float('1.23456789', 15)\n    assert same_and_same_prec(mpf_norm(a._mpf_, 15), mpf_norm(b._mpf_, 15))\n", "def test_mod_inverse():\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(-3, 11) == 7\n    assert mod_inverse(2, 4) == None\n    assert raises(ValueError, mod_inverse, 2, 0)\n    assert raises(ValueError, mod_inverse, S(2)/7, S(5)/2)\n", "    def test_Float_power(self):\n        f = Float(2, 15)\n        assert f**S.Half == Float(1.4142135623730951, 15)\n        assert f**Rational(3, 4) == Float(1.6817928305074295, 15)\n        assert f**Rational(1, 4) == Float(1.1892071150027213, 15)\n        assert f**Rational(1, 5) == Float(1.1486984211723232, 15)\n        assert f**Rational(1, 6) == Float(1.1224620483093722, 15)\n        assert f**Rational(1, 7) == Float(1.102941677312354, 15)\n        assert f**Rational(1, 8) == Float(1.0830925659292254, 15)\n        assert f**Rational(1, 9) == Float(1.065723077305472, 15)\n        assert f**Rational(1, 10) == Float(1.0500355299801113, 15)\n        assert f**S.Infinity == S.Infinity\n        assert f**S.NegativeInfinity == S.Infinity\n        assert f**(-S.Infinity) == S.NegativeInfinity\n        assert f**(-S.Half) == Float(0.7071067811865475, 15)\n        assert f**(-Rational(3, 4)) == Float(0.3535533905932737, 15)\n        assert f**(-Rational(1, 4)) == Float(0.7071067811865475, 15)\n        assert f**(-Rational(1, 5)) == Float(0.4472135954999584, 15)\n        assert f**(-Rational(1, 6)) == Float(0.4082482904638509, 15)\n        assert f**(-Rational(1, 7)) == Float(0.3779641496150126, 15)\n        assert f**(-Rational(1, 8)) == Float(0.3535533905932737, 15)\n       "], "sample_1107": ["def test_igraph():\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('a', 'c'), ('b', 'd')])\n    assert topological_sort(G) == ['a', 'b', 'c', 'd']\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('b', 'd'), ('c', 'b')])\n    with raises(ValueError):\n        topological_sort(G)\n    assert connected_components(G) == [['a', 'b', 'c', 'd']]\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('b', 'c'), ('c', 'd')])\n    assert connected_components(G) == [['a', 'b', 'c', 'd']]\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('b', 'c'), ('c', 'a')])\n    with raises(ValueError):\n        connected_components(G)\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('b', 'c'), ('c', 'a'), ('a', 'd')])\n    with raises(ValueError):\n        connected_components(G)\n    G = (['a', 'b', 'c', 'd'], [('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'a')])\n    with raises(ValueError):\n        connected_components(G)\n", "def test_flatten():\n    # Test case 1\n    assert flatten([1, 2, 3]) == [1, 2, 3]\n    assert flatten([1, 2, [3]]) == [1, 2, 3]\n\n    # Test case 2\n    assert flatten([1, [2, 3], [4, 5]]) == [1, 2, 3, 4, 5]\n\n    # Test case 3\n    assert flatten([1, 1, 1, 2, 2, 3], levels=1) == [1, 2, 3, 1, 2]\n    assert flatten([1, 1, 3, 2, 2, 1], levels=1) == [1, 1, 3, 2, 2]\n\n    # Test case 4\n    assert flatten([1, 1, 3, 2, 2, 1], cls=Basic) == [1, 1, 3, 2, 2]\n\n    # Test case 5\n    assert flatten([1.0, 2, (1, None)]) == [1.0, 2, 1, None]\n\n    # Test case 6\n    assert flatten([1, 2, 3], levels=0) == [1, 2, 3]\n    assert flatten([1, 2, 3], levels=2) == [1, 2, 3]\n\n    # Test case 7\n    assert flatten([1, 2, [3, 4]], levels=2) == [1, 2, 3, 4]\n\n    # Test case 8\n    assert flatten([[1], [2], [3]], levels=1) == [1, 2, 3]\n\n    # Test case 9\n    assert flatten([1, 2, 3], cls=Basic) == [1, 2, 3]\n\n    # Test case 10\n    assert flatten([1.0, 2, (1, None)], cls=Basic) == [1.0, 2, 1, None]\n", "def test_kbins():\n    # Test for various cases of kbins\n    assert list(kbins(list(range(5)), 2)) == [\n        [[0], [1, 2, 3, 4]],\n        [[0, 1], [2, 3, 4]],\n        [[0, 1, 2], [3, 4]],\n        [[0, 1, 2, 3], [4]]\n    ]\n    assert list(kbins([1, 1, 1, 1], 3)) == [\n        [[1], [1], [1]],\n        [[1], [1, 1]],\n        [[1], [1, 1, 1]]\n    ]\n    assert list(kbins([1, 2, 3, 4], 1)) == [\n        [[1, 2, 3, 4]]\n    ]\n    assert list(kbins([1, 2, 3, 4], 5)) == []\n    assert list(kbins([1, 2, 3, 4], 2, ordered=0)) == [\n        [[0, 1], [2, 3, 4]],\n        [[0, 2], [1, 3, 4]],\n        [[0, 3], [1, 2, 4]],\n        [[0, 4], [1, 2, 3]]\n    ]\n    assert list(kbins([1, 2, 3, 4], 2, ordered=11)) == [\n        [[1], [2, 3, 4]],\n        [[1, 2], [3, 4]],\n        [[1, 3], [2, 4]],\n        [[1, 4], [2, 3]],\n        [[2], [1, 3, 4]],\n        [[2, 1], [3, 4]],\n        [[2, 3], [1, 4]],\n        [[2, 4], [1, 3]],\n        [[3], [1, 2, 4]],\n        [[3, 1], [2, 4]],\n        [[3, 2], [1, 4]],\n        [[3, 4], [1, 2]],\n        [[4], [1, 2", "def test_rotate_right():\n    # Check rotation of an empty list\n    assert rotate_right([], 5) == []\n    # Check rotation of a list of one element\n    assert rotate_right([1], 5) == [1]\n    # Check rotation of a list of multiple elements\n    assert rotate_right([1, 2, 3], 5) == [3, 1, 2]\n    # Check rotation of a list by a number of steps larger than its length\n    assert rotate_right([1, 2, 3], 7) == [3, 1, 2]\n    # Check rotation of a list by a number of steps smaller than its length\n    assert rotate_right([1, 2, 3], 1) == [2, 3, 1]\n    # Check rotation of a list by a negative number of steps\n    assert rotate_right([1, 2, 3], -1) == [2, 3, 1]\n    # Check rotation of a list by a multiple of its length\n    assert rotate_right([1, 2, 3], 3) == [2, 3, 1]\n    # Check rotation of a list by a multiple of its length plus one\n    assert rotate_right([1, 2, 3], 4) == [2, 3, 1]\n", "compilation error"], "sample_176": ["    def test_manager_deconstruction(self):\n        class TestModel(models.Model):\n            objects = models.Manager()\n            all_objects = models.Manager()\n\n        before_state = ModelState(\"testapp\", \"TestModel\", [(\"id\", models.AutoField(primary_key=True))])\n        after_state = ModelState(\"testapp\", \"TestModel\", [(\"id\", models.AutoField(primary_key=True))], managers=[\n            ('objects', TestModel.objects),\n            ('all_objects', TestModel.all_objects),\n        ])\n        changes = self.get_changes([before_state], [after_state])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['AlterModelManagers'])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"TestModel\", managers=[\n            ('objects', TestModel.objects),\n            ('all_objects', TestModel.all_objects),\n        ])\n", "def test_rename_model_with_renamed_options(self):\n    # Test that a model is renamed with renamed options\n    before = self.make_project_state([\n        self.author_options,\n    ])\n    after = self.make_project_state([\n        self.author_renamed_with_db_table_options,\n    ])\n    changes = self.get_changes(before.models, after.models)\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertMigrationDependencies(changes, 'testapp', 0, [(None, '__first__')])\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='NewAuthor')\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, preserve_default=True)\n", "    def deconstruct(self):\n        return (\n            self.__module__ + '.' + self.__class__.__name__,\n            self.__dict__,\n            {},\n        )\n", "    def test_add_operation_dependencies(self):\n        before_states = [self.author_empty, self.author_dates_of_birth_auto_now_add]\n        after_states = [self.author_empty, self.author_dates_of_birth_auto_now_add]\n        autodetector = MigrationAutodetector(\n            self.make_project_state(before_states),\n            self.make_project_state(after_states),\n        )\n        changes = autodetector._detect_changes()\n        self.assertEqual(autodetector.generated_operations, {})\n        self.assertEqual(autodetector.migrations, {})\n        self.add_operation(autodetector, \"testapp\", operations.CreateModel(\n            name=\"CustomModel\",\n            fields=[],\n            options={\"verbose_name\": \"Custom Model\"},\n            bases=[],\n            managers=[],\n        ))\n        self.assertEqual(autodetector.generated_operations[\"testapp\"], [autodetector.add_operation.call_args[1][\"operation\"]])\n        self.assertEqual(autodetector.migrations[\"testapp\"], [autodetector.add_operation.call_args[1][\"operation\"]])\n", "    def test_add_default_null_field(self):\n        author_name_null = self.author_name_null.clone()\n        author_name_null.fields[1] = models.CharField(max_length=200, null=True, default=123)\n        author_name_null_default_null = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, null=True, default=123)),\n        ])\n        changes = self.get_changes([author_name_null], [author_name_default_null])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n"], "sample_413": ["    def test_check_string_if_invalid_is_string(self):\n        self.assertEqual(\n            len(check_string_if_invalid_is_string([])),\n            1,\n        )\n", "    def test_string_if_invalid_not_string(self):\n        self.assertEqual(len(check_string_if_invalid_is_string([])), 2)\n", "    def test_check_string_if_invalid_is_string(self):\n        self.assertEqual(len(check_string_if_invalid_is_string([])), 1)\n\n", "    def test_check_string_if_invalid_is_string(self):\n        with self.assertWarns(None):\n            with override_settings(TEMPLATES=self.TEMPLATES_INVALID_STRING_IF_INVALID):\n                check_string_if_invalid_is_string([], {})\n", "    def test_duplicate_template_tags(self):\n        self.assertEqual(len(check_for_template_tags_with_the_same_name([])), 1)\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name([self.TEMPLATES_DUPLICATE_TEMPLATE_TAGS[0]]),\n            [Error(E003.msg.format(repr(\"mylib\"), \"path.to.mylib1, path.to.mylib2\"), id=E003.id)]\n        )\n        self.assertEqual(\n            check_for_template_tags_with_the_same_name(\n                [self.TEMPLATES_DUPLICATE_TEMPLATE_TAGS[0], self.TEMPLATES_DUPLICATE_TEMPLATE_TAGS[1]]\n            ),\n            [Error(E003.msg.format(repr(\"mylib\"), \"path.to.mylib1, path.to.mylib2\"), id=E003.id)]\n        )\n\n"], "sample_478": ["    def test_date_hierarchy(self):\n        class MyModel(Model):\n            name = models.CharField(max_length=255)\n\n        class MyAdmin(ModelAdmin):\n            date_hierarchy = \"name\"\n\n        self.assertIsInvalid(\n            MyAdmin,\n            MyModel,\n            \"'date_hierarchy' refers to 'name', which is not a DateField or \"\n            \"DateTimeField.\",\n            id=\"admin.E128\",\n        )\n", "    def test_date_hierarchy(self):\n        class ModelAdmin(admin.ModelAdmin):\n            date_hierarchy = \"non_existent_field\"\n\n        self.assertIsInvalid(\n            ModelAdmin,\n            Model,\n            \"The value of 'date_hierarchy' refers to 'non_existent_field', which does not refer to a Field.\",\n            id=\"admin.E127\",\n        )\n", "    def test_prepopulated_fields_valid(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            model = User\n            prepopulated_fields = {\"username\": (\"first_name\", \"last_name\")}\n\n        self.assertIsValid(MyModelAdmin, User)\n", "    def test_check_save_as(self):\n        class ModelAdmin(admin.ModelAdmin):\n            save_as = True  # default value\n            model = ValidationTestModel\n\n        self.assertIsValid(ModelAdmin, ValidationTestModel)\n", "    def test_fieldsets_invalid_fieldset_format(self):\n        # Test that _check_fieldsets_item raises an error when the fieldset is not a list of length 2.\n        model_admin = admin.ModelAdmin\n        model = Model\n        self.assertIsInvalid(\n            model_admin,\n            model,\n            \"The value of 'fieldsets' refers to '%s', which is not a list of length 2.\"\n            % type(model_admin).__name__,\n            id=\"admin.E008\",\n        )\n"], "sample_487": ["    def test_date_hierarchy_missing_field(self):\n        class InvalidDateHierarchyAdmin(admin.ModelAdmin):\n            date_hierarchy = \"invalid_field\"\n\n        self.assertIsInvalid(\n            InvalidDateHierarchyAdmin, ValidationTestModel, \"The value of 'date_hierarchy' refers to 'invalid_field', which does not refer to a Field.\"\n        )\n", "    def test_date_hierarchy(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            date_hierarchy = \"title\"\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            model=User,\n            msg=\"The value of 'date_hierarchy' refers to 'title', which is not a DateField or DateTimeField.\",\n            id=\"admin.E128\",\n        )\n", "    def test_filter_vertical(self):\n        class ModelAdmin(ModelAdmin):\n            list_display = ('field1', 'field2')\n            filter_vertical = ('field1',)\n\n        self.assertIsInvalid(\n            ModelAdmin,\n            Model,\n            \"The value of 'filter_vertical' refers to 'field1', which is not \"\n            \"contained in 'list_display'.\",\n            id=\"admin.E122\",\n        )\n", "    def test_exclude_is_not_none(self):\n        model_admin = admin.ModelAdmin()\n        self.assertIsInvalid(model_admin, ValidationTestModel, 'exclude is required')\n", "    def test_empty_ordering(self):\n        class TestModelAdmin(admin.ModelAdmin):\n            ordering = None\n        self.assertIsValid(TestModelAdmin, TestModel)\n"], "sample_341": ["    def test_formset_factory_defaults(self):\n        ChoiceFormSet = formset_factory(Choice)\n        self.assertEqual(ChoiceFormSet.min_num, 0)\n        self.assertEqual(ChoiceFormSet.max_num, 1000)\n        self.assertEqual(ChoiceFormSet.absolute_max, 2000)\n", "    def test_base_formset_init(self):\n        formset = BaseFormSet(auto_id='id_%s', prefix='my_prefix', error_class=ErrorList)\n        self.assertEqual(formset.auto_id, 'id_%s')\n        self.assertEqual(formset.prefix, 'my_prefix')\n        self.assertEqual(formset.error_class, ErrorList)\n", "    def test_management_form_clean(self):\n        # Test that ManagementForm.clean() works as expected\n        form_data = {'TOTAL_FORMS': '1', 'INITIAL_FORMS': '1', 'MIN_NUM_FORMS': '1', 'MAX_NUM_FORMS': '1'}\n        form = ManagementForm(form_data)\n        form.full_clean()\n        self.assertTrue(form.is_valid())\n\n        # Test that ManagementForm.clean() raises ValidationError when TOTAL_FORMS is missing\n        form_data = {'INITIAL_FORMS': '1', 'MIN_NUM_FORMS': '1', 'MAX_NUM_FORMS': '1'}\n        form = ManagementForm(form_data)\n        with self.assertRaises(ValidationError):\n            form.full_clean()\n\n        # Test that ManagementForm.clean() raises ValidationError when INITIAL_FORMS is missing\n        form_data = {'TOTAL_FORMS': '1', 'MIN_NUM_FORMS': '1', 'MAX_NUM_FORMS': '1'}\n        form = ManagementForm(form_data)\n        with self.assertRaises(ValidationError):\n            form.full_clean()\n", "    def test_base_formset_management_form_attrs(self):\n        BaseFormSet = formset_factory(form=Form)\n        base_formset = BaseFormSet()\n        self.assertIn('TOTAL_FORMS', base_formset.management_form.base_fields)\n        self.assertIn('INITIAL_FORMS', base_formset.management_form.base_fields)\n        self.assertIn('MIN_NUM_FORMS', base_formset.management_form.base_fields)\n        self.assertIn('MAX_NUM_FORMS', base_formset.management_form.base_fields)\n", "    def test_management_form_clean(self):\n        formset = ChoiceFormSet(data={\n            'choices-INITIAL_FORMS': '0',\n            'choices-TOTAL_FORMS': '2',\n            'choices-0-choice': 'A',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'B',\n            'choices-1-votes': '1'\n        })\n        with self.assertRaises(ValidationError):\n            formset.management_form.is_valid()\n"], "sample_414": ["    def test_formfield_for_dbfield(self):\n        # Test that formfield_for_dbfield returns the correct formfield for a DateTimeField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            DateTimeField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminSplitDateTime)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a DateField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            DateField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminDateWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a TimeField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            TimeField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminTimeWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a TextField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            TextField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminTextareaWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a URLField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            URLField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminURLFieldWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for an IntegerField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            IntegerField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminIntegerFieldWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a BigIntegerField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            BigIntegerField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminBigIntegerFieldWidget)\n\n        # Test that formfield_for_dbfield returns the correct formfield for a CharField\n        ff = admin.ModelAdmin(formfield_overrides={}).formfield_for_dbfield(\n            CharField(), request=None\n        )\n        self.assertIsInstance(ff.widget, admin.widgets.AdminTextInputWidget)\n\n        # Test that formfield_for_dbfield returns the correct", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.a = Album.objects.create(name=\"test\")\n        cls.b = Band.objects.create(name=\"test\")\n        cls.i = Individual.objects.create(name=\"test\")\n", "    def test_save_formset(self):\n        model_admin = admin.ModelAdmin()\n        model_admin.form = forms.ModelForm\n        formset = model_admin.get_changelist_formset(None)\n        formset = formset(formset.data or {}, instance=None)\n        self.assertTrue(formset.is_valid())\n\n        # Create a new model admin\n        model_admin = admin.ModelAdmin()\n        model_admin.model = Car\n        model_admin.form = forms.ModelForm\n        formset = model_admin.get_changelist_formset(None)\n        formset = formset(formset.data or {}, instance=None)\n        formset.forms[0].save(commit=True)\n        self.assertTrue(formset.is_valid())\n", "    def test_inline_model_field_overrides(self):\n        # Test that formfield_overrides work on inline models\n        class MyInline(admin.StackedInline):\n            model = Profile\n            formfield_overrides = {\"first_name\": {\"widget\": forms.TextInput()}}\n\n        ma = MyInline(User, admin.site)\n        ff = ma.formfield_for_dbfield(User._meta.get_field(\"first_name\"), request=None)\n        self.assertIsInstance(ff.widget, forms.TextInput)\n", "    def test_get_inline_instances(self):\n        from .models import Company, Individual, Member, School\n\n        model_admin = admin.ModelAdmin(Company, admin.site)\n        inline_admin = InlineModelAdmin(model=Individual, fk_name=\"company\", admin=widget_admin_site)\n        self.assertEqual(model_admin.get_inline_instances(request=None, obj=None), [])\n\n        inline_instances = model_admin.get_inline_instances(request=None, obj=Company.objects.get(id=1))\n        self.assertIn(inline_admin, inline_instances)\n        self.assertIn(inline_admin.model, inline_instances[0].get_queryset(None))\n        self.assertIn(\"company\", inline_admin.model._meta.get_fields(include_hidden=True))\n\n        inline_admin = InlineModelAdmin(model=Member, fk_name=\"school\", admin=widget_admin_site)\n        inline_instances = model_admin.get_inline_instances(request=None, obj=School.objects.get(id=1))\n        self.assertIn(inline_admin, inline_instances)\n        self.assertIn(inline_admin.model, inline_instances[0].get_queryset(None))\n        self.assertIn(\"school\", inline_admin.model._meta.get_fields(include_hidden=True))\n"], "sample_618": ["def test_unified_dim_sizes():\n    \"\"\"Tests the unified_dim_sizes function.\"\"\"\n    da_a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    da_b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n    da_c = xr.DataArray([7, 8, 9], dims=[\"y\"])\n\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da_a, da_b, da_c])\n\n    unified_dim_sizes([da_a, da_a])\n    unified_dim_sizes([da_a, da_b])\n\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da_a, da_b, da_c], exclude_dims={\"x\"})\n\n    unified_dim_sizes([da_a, da_b], exclude_dims={\"x\"})\n\n    da_a = xr.DataArray(\n        [[1, 2, 3], [4, 5, 6]], dims=[\"x\", \"y\"], coords={\"x\": [0, 1]}\n    )\n    da_b = xr.DataArray(\n        [[7, 8, 9], [10, 11, 12]], dims=[\"x\", \"y\"], coords={\"x\": [0, 1]}\n    )\n\n    unified_dim_sizes([da_a, da_b])\n    unified_dim_sizes([da_a, da_b], exclude_dims={\"x\"})\n\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da_a, da_b], exclude_dims={\"y\"})\n", "def test_align_chunks():\n    \"\"\"Test align_chunks functionality\"\"\"\n    from . import raises, requires_dask\n    from xarray.testing import assert_identical\n\n    with requires_dask():\n        x = xr.DataArray([1, 2, 3], dims=['x'], chunks=2)\n        y = xr.DataArray([4, 5, 6], dims=['x'], chunks=3)\n        z = xr.DataArray([7, 8, 9], dims=['x'], chunks=2)\n\n        assert identical(align(x, y, z, join='outer', exclude=['x']), xr.align(x, y, z, join='outer', exclude=['x']))\n\n        x = xr.DataArray([1, 2, 3], dims=['x'], chunks=2)\n        y = xr.DataArray([4, 5, 6], dims=['x'], chunks=3)\n        z = xr.DataArray([7, 8, 9], dims=['x'], chunks=2)\n\n        with raises(ValueError):\n            align(x, y, z, join='outer', exclude=['x'], raise_on_invalid=True)\n\n        x = xr.DataArray([1, 2, 3], dims=['x'], chunks=2)\n        y = xr.DataArray([4, 5, 6], dims=['x'], chunks=3)\n\n        with raises(ValueError):\n            align(x, y, z, join='outer', exclude=['x'])\n\n        x = xr.DataArray([1, 2, 3], dims=['x'], chunks=2)\n        y = xr.DataArray([4, 5, 6], dims=['x'], chunks=3)\n\n        with raises(ValueError):\n            align(x, y, z, join='outer', exclude=['y'])\n\n        x = xr.DataArray([1, 2, 3], dims=['x'], chunks=2)\n\n        with raises(ValueError):\n            align(x, y, z, join='outer', exclude=['x', 'y'])\n", "def test_cov_vectorize(vectorize):\n    data_a = xr.DataArray(np.arange(10), dims=[\"x\"], attrs={\"a\": 1})\n    data_b = xr.DataArray(np.arange(10), dims=[\"x\"], attrs={\"b\": 2})\n\n        return np.cov(x, y, ddof=1)\n\n    result = apply_ufunc(\n        cov_func,\n        data_a,\n        data_b,\n        input_core_dims=[[(\"x\", \"x\")]],\n        vectorize=vectorize,\n    )\n    expected = np.cov(np.arange(10), np.arange(10), ddof=1)\n    assert_identical(result, expected)\n\n", "def test_apply_ufunc_broadcast_dimensions():\n    \"\"\"Test that apply_ufunc broadcasts dimensions correctly\"\"\"\n    da = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    da_aligned = da.copy()\n    da_aligned = da_aligned.broadcast_like(da_aligned + da)\n    result = apply_ufunc(np.add, da_aligned, da_aligned)\n    assert_identical(result, da_aligned + da_aligned)\n\n", "def test_polyval_fill_value():\n    import numpy as np\n    import dask.array as da\n    import dask\n    import xarray as xr\n\n    dask_version = dask.__version__\n\n    coords = xr.DataArray(\n        np.linspace(0, 10, 10), dims=[\"x\"], coords={\"x\": np.linspace(0, 10, 10)}\n    )\n    da_coords = da.from_array(coords.data, chunks={\"x\": 5})\n    da_coords = da_coords.rechunk({\"x\": -1})\n    da_coords = da_coords.transpose(\"x\")\n\n    coords = coords.rechunk({\"x\": -1})\n    coords = coords.transpose(\"x\")\n\n    da_coeffs = xr.DataArray(\n        np.array([1, 2, 3, 4, 5]), dims=[\"degree\"], coords={\"degree\": np.arange(5)}\n    ).chunk({\"degree\": -1})\n\n    ds = xr.Dataset({\"x\": coords, \"coeffs\": da_coeffs})\n    ds_da = xr.Dataset({\"x\": da_coords, \"coeffs\": da_coeffs})\n\n    coeffs = xr.DataArray(\n        np.array([1, 2, 3, 4, 5]), dims=[\"degree\"], coords={\"degree\": np.arange(5)}\n    )\n    da_coeffs = da.from_array(coeffs.data, chunks={\"degree\": 5})\n    da_coeffs = da_coeffs.rechunk({\"degree\": -1})\n\n    da_coeffs = da_coeffs.transpose(\"degree\")\n\n    da_poly = da.polyval(da_coeffs, da_coords, \"degree\")\n    poly = xr.polyval(coeffs, coords, \"degree\")\n    da_poly = da_poly.compute()\n    poly = poly.compute()\n\n    assert_identical(poly, da_poly)\n\n    ds_poly = xr.polyval(ds, \"x\", \"degree\")\n    ds_poly_da = xr.polyval(ds_da, \"x\", \"degree\")\n    ds_poly = ds_poly.compute()\n    ds_poly_da = ds_poly_da.compute()\n\n    assert_identical(ds_poly, ds_poly_da)\n\n    da_poly = da.polyval(da_coeffs, da_coords, \"degree\", fill_value=0)\n    poly = xr.polyval(coeffs, coords, \"degree\", fill_value=0)\n    da_poly = da_poly.compute()\n    poly = poly.compute()\n\n    assert_identical(poly, da_poly)\n\n    ds_poly = xr.polyval(ds, \"x\", \"degree\", fill_value=0)\n    ds_poly_da ="], "sample_284": ["    def test_manifest_strict_fails_on_missing_manifest_entry(self):\n        \"\"\"\n        Test that ManifestStaticFilesStorage raises an error when manifest_strict is True and a file\n        has a missing entry in the manifest.\n        \"\"\"\n        # Create a test file\n        file_path = TEST_ROOT / 'test.txt'\n        with open(file_path, 'w') as f:\n            f.write('Hello, world!')\n\n        # Set up the staticfiles storage to use ManifestStaticFilesStorage\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n\n        # Set manifest_strict to True\n        settings.STATICFILES_MANIFEST_STRICT = True\n\n        # Call collectstatic to create the manifest\n        call_command('collectstatic', verbosity=0)\n\n        # Try to access a file with a missing manifest entry\n        with self.assertRaises(ValueError):\n            storage.staticfiles_storage.stored_name('test.txt')\n\n        # Clean up\n        os.remove(file_path)\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.StaticFilesStorage'\n        del settings.STATICFILES_MANIFEST_STRICT\n", "    def test_manifest_version(self):\n        storage = storage.ManifestStaticFilesStorage()\n        self.assertEqual(storage.manifest_version, '1.0')\n", "    def test_manifest_save_and_load(self):\n        storage = ManifestStaticFilesStorage()\n        self.assertFalse(os.path.exists(storage.manifest_name))\n\n        # Test saving the manifest\n        storage.hashed_files = {'file1': 'hashed_file1', 'file2': 'hashed_file2'}\n        storage.save_manifest()\n        self.assertTrue(os.path.exists(storage.manifest_name))\n        self.assertEqual(storage.read_manifest(), json.dumps({'paths': {'file1': 'hashed_file1', 'file2': 'hashed_file2'}, 'version': '1.0'}))\n\n        # Test loading the manifest\n        storage.hashed_files.clear()\n        storage.read_manifest()\n        self.assertEqual(storage.hashed_files, {'file1': 'hashed_file1', 'file2': 'hashed_file2'})\n\n        # Test strict mode\n        storage = ManifestStaticFilesStorage(manifest_strict=True)\n        with self.assertRaises(ValueError):\n            storage.read_manifest()\n        storage.hashed_files = {'file1': 'hashed_file1'}\n        with self.assertRaises(ValueError):\n            storage.stored_name('file2')\n", "    def setUp(self):\n        super().setUp()\n        self.storage = storage.staticfiles_storage\n", "    def test_stored_name_nonexistent_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            os.makedirs(os.path.join(tmpdir, 'static'), exist_ok=True)\n            with open(os.path.join(tmpdir, 'static', 'file.txt'), 'w') as f:\n                f.write('content')\n            storage = storage.staticfiles_storage\n            storage.location = tmpdir\n            storage.base_location = tmpdir\n            self.assertEqual(storage.stored_name('file.txt'), 'file.txt')\n            self.assertEqual(storage.stored_name('static/file.txt'), 'static/file.txt')\n            storage.delete('static/file.txt')\n            self.assertIsNone(storage.stored_name('file.txt'))\n"], "sample_258": ["    def test_disconnect_no_weakref(self):\n        a_signal.connect(receiver_1_arg, weak=False)\n        self.assertTrue(a_signal.has_listeners())\n        self.assertEqual(a_signal.receivers, [(id(receiver_1_arg), id(None))])\n        a_signal.disconnect(receiver_1_arg)\n        self.assertFalse(a_signal.has_listeners())\n        self.assertEqual(a_signal.receivers, [])\n        garbage_collect()\n", "    def test_connect_multiple_receivers_with_different_uids(self):\n        @receiver(a_signal)\n            pass\n\n        @receiver(a_signal, dispatch_uid='uid1')\n            pass\n\n        @receiver(a_signal, dispatch_uid='uid2')\n            pass\n\n        self.assertEqual(len(a_signal.receivers), 3)\n        a_signal.disconnect(receiver2, dispatch_uid='uid1')\n        self.assertEqual(len(a_signal.receivers), 2)\n", "    def test_disconnect_multiple_receivers(self):\n        d_signal.connect(receiver_1_arg, dispatch_uid='uid1')\n        d_signal.connect(receiver_1_arg, dispatch_uid='uid1')\n        self.assertTrue(d_signal.disconnect(receiver_1_arg, dispatch_uid='uid1'))\n        self.assertFalse(d_signal.disconnect(receiver_1_arg, dispatch_uid='uid1'))\n        self.assertFalse(d_signal.disconnect(receiver_1_arg))\n", "    def test_weak_references(self):\n        weak_receiver = weakref.ref(receiver_1_arg)\n        a_signal.connect(weak_receiver)\n        self.assertIsNotNone(a_signal._live_receivers(None))\n        a_signal.disconnect(weak_receiver)\n        self.assertIsNone(a_signal._live_receivers(None))\n        garbage_collect()\n", "    def test_disconnect_duplicate_connection(self):\n        # Test that you can't connect a receiver to a signal multiple times\n        # using the same dispatch_uid.\n        # Note that the receiver should be a callable that accepts keyword\n        # arguments.\n        self.assertFalse(a_signal.has_listeners())\n        a_signal.connect(receiver_1_arg, dispatch_uid='test-id')\n        self.assertTrue(a_signal.has_listeners())\n        a_signal.connect(receiver_1_arg, dispatch_uid='test-id')\n        self.assertTrue(a_signal.has_listeners())\n        # When disconnect is called, it should remove the last connection\n        a_signal.disconnect(receiver_1_arg, dispatch_uid='test-id')\n        self.assertFalse(a_signal.has_listeners())\n"], "sample_210": ["    def test_get_redirect(self):\n        view = RedirectView()\n        view.url = '/redirect_url'\n        view.permanent = True\n        response = view.get(self.rf.get('/'))\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response.url, '/redirect_url')\n", "    def _assert_template(self, response):\n        self.assertEqual(response.status_code, 200)\n", "    def test_template_response_mixin_render_to_response(self):\n        view = AboutTemplateView()\n        context = {'title': 'Hello'}\n        response = view.render_to_response(context)\n        self.assertIsInstance(response, TemplateResponse)\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('template_name', response._template._template)\n", "    def test_context_kwargs_deprecation_warning(self):\n        url_kwargs = {'artist_id': 123}\n        view = AboutTemplateView()\n        response = view.get(self.rf.get('/about/'))\n        self.assertIn('artist_id', response.context)\n        self.assertIn('artist_id', view.kwargs)\n        with self.assertWarns(RemovedInDjango40Warning):\n            view.get(self.rf.get('/about/', **url_kwargs))\n", "    def test_http_method_not_allowed(self):\n        view = View()\n        request = self.rf.get('/')\n        with self.assertRaises(HttpResponseNotAllowed):\n            view.dispatch(request)\n"], "sample_775": ["def test_changed_params_with_default_value():\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.utils._pprint import _EstimatorPrettyPrinter\n\n    # Test that _changed_params returns the correct parameters when the default value is NaN\n    lr = LogisticRegression(max_iter=100, C=0.1)\n    expected_changed_params = {'max_iter': 100, 'C': 0.1}\n    changed_params = _changed_params(lr)\n    assert changed_params == expected_changed_params\n\n    # Test that _changed_params returns the correct parameters when the default value is a scalar\n    ss = StandardScaler(with_mean=True)\n    expected_changed_params = {'with_mean': True}\n    changed_params = _changed_params(ss)\n    assert changed_params == expected_changed_params\n\n    # Test that _changed_params returns an empty dictionary when all parameters have their default values\n    lr_default = LogisticRegression()\n    expected_changed_params = {}\n    changed_params = _changed_params(lr_default)\n    assert changed_params == expected_changed_params\n", "def test_repr_unsupported_objects():\n    # Test class without __repr__ method\n    class UnsupportedClass:\n        pass\n\n    unsupported_repr, readable, recursive = _safe_repr(UnsupportedClass(), None, 10, 0, changed_only=False)\n    assert repr(UnsupportedClass()) == unsupported_repr\n    assert not readable\n    assert not recursive\n\n    # Test instance of unsupported class\n    unsupported_instance = UnsupportedClass()\n    unsupported_repr, readable, recursive = _safe_repr(unsupported_instance, None, 10, 0, changed_only=False)\n    assert repr(unsupported_instance) == unsupported_repr\n    assert not readable\n    assert not recursive\n", "    def test_changed_only_parameter(self):\n        # Test that the changed_only parameter works correctly\n        pp = _EstimatorPrettyPrinter(changed_only=True)\n        estimator = make_pipeline(\n            StandardScaler(),\n            LogisticRegression()\n        )\n        estimator.set_params(logisticregression__C=0.5)\n        print(pp.pformat(estimator))\n        assert \"C=0.5\" in str(pp.pformat(estimator))\n        assert \"C=1.0\" not in str(pp.pformat(estimator))\n", "def test_estimator_pretty_printer_with_changed_only_true():\n    \"\"\"Test the _EstimatorPrettyPrinter with changed_only=True.\"\"\"\n    # Create a Logistic Regression estimator with default parameters\n    lr = LogisticRegression()\n\n    # Create a dictionary with the default parameters and the changed parameters\n    params = lr.get_params(deep=False)\n    changed_params = _changed_params(lr)\n    all_params = {**params, **changed_params}\n\n    # Create an instance of _EstimatorPrettyPrinter with changed_only=True\n    pp = _EstimatorPrettyPrinter(changed_only=True)\n\n    # Print the estimator with changed_only=True\n    print(pp.format(lr))\n\n    # Check if the printed string contains the changed parameters\n    printed_str = str(pp.format(lr))\n    for param, value in changed_params.items():\n        assert param in printed_str\n        assert f\"{param}={value}\" in printed_str\n\n    # Check if the printed string does not contain the default parameters\n    for param, value in params.items():\n        if param != 'solver' and param != 'max_iter':\n            assert f\"{param}={value}\" not in printed_str\n", "    def test_estimator_pretty_printing_changed_params(self):\n        est = make_pipeline(LogisticRegression(max_iter=1000))\n        est.fit([[1, 2], [3, 4]], [0, 1])\n        est = make_pipeline(LogisticRegression(max_iter=1000))\n        est.fit([[1, 2], [3, 4]], [0, 1])\n        with self.assertRaises(TypeError):\n            _EstimatorPrettyPrinter()._format_params(est.get_params(deep=True), None, 0, 0, {}, 0)\n"], "sample_1149": ["def test_SingletonRegistry_register_class():\n    # Test that a class can be registered with the SingletonRegistry\n    class TestClass:\n        pass\n\n    S.register(TestClass)\n\n    # Test that the class is now an attribute of S\n    assert TestClass in dir(S)\n\n    # Test that the class has been installed as a singleton\n    test_instance = S.TestClass()\n    singleton_instance = S.TestClass()\n    assert test_instance is singleton_instance\n\n    # Test that re-registering a class overwrites the old instance\n    S.register(TestClass)\n    assert test_instance is not S.TestClass()\n", "def test_SingletonRegistry_registration_order():\n    class TestClass:\n        pass\n\n    class AnotherClass:\n        pass\n\n    S.register(TestClass)\n    S.register(AnotherClass)\n\n    assert S.TestClass is TestClass()\n    assert S.AnotherClass is AnotherClass()\n\n    S.register(TestClass)\n    assert S.TestClass is TestClass()\n", "def test_SingletonRegistry_register_register_same_class(self):\n    class TestSingleton(Basic, metaclass=Singleton):\n        pass\n\n    S.register(TestSingleton)\n    S.register(TestSingleton)\n\n    self.assertEqual(id(S.TestSingleton), id(S.TestSingleton))\n", "def test_SingletonRegistry_register():\n    \"\"\"Test that register and __getattr__ methods in SingletonRegistry work correctly\"\"\"\n    class TestClass:\n        pass\n    S.register(TestClass)\n    try:\n        S.__getattr__('TestClass')\n    except AttributeError:\n        assert False, \"TestClass should be installed\"\n    try:\n        S.register(TestClass)\n        assert False, \"TestClass should not be registered twice\"\n    except AttributeError:\n        pass\n    try:\n        S.__getattr__('NonExistentClass')\n        assert False, \"NonExistentClass should not be installed\"\n    except AttributeError:\n        pass\n    try:\n        S.__getattr__('TestClass')\n        assert TestClass is S.TestClass\n    except Exception as e:\n        assert False, f\"Expected TestClass to be S.TestClass, got {e}\"\n", "def test_singleton_registry_retrieval():\n    # Test retrieving a registered class from the SingletonRegistry\n    assert S.Basic is Basic\n\n    # Test retrieving a non-registered class from the SingletonRegistry raises AttributeError\n    with pytest.raises(AttributeError):\n        S.NonRegistered\n\n    # Test retrieving a registered class after it has been unregistered raises AttributeError\n    S.Basic = None  # Unregister the Basic class\n    with pytest.raises(AttributeError):\n        S.Basic\n    S.register(Basic)  # Re-register the Basic class\n    assert S.Basic is Basic\n\n    # Test retrieving a class from the SingletonRegistry using its name as a string\n    class MySingleton(Basic, metaclass=Singleton):\n        pass\n    assert S.MySingleton is MySingleton()\n\n    # Test retrieving a class from the SingletonRegistry using its name as a string after it has been unregistered raises AttributeError\n    S.MySingleton = None  # Unregister the MySingleton class\n    with pytest.raises(AttributeError):\n        S.MySingleton\n    S.register(MySingleton)  # Re-register the MySingleton class\n    assert S.MySingleton is MySingleton()\n"], "sample_608": ["def test_format_timestamp():\n    # Test that we can format a valid datetime\n    t = pd.Timestamp(\"2022-07-25\")\n    assert formatting.format_timestamp(t) == \"2022-07-25T00:00:00\"\n\n    # Test that we can format a valid datetime with a time component\n    t = pd.Timestamp(\"2022-07-25 12:00:00\")\n    assert formatting.format_timestamp(t) == \"2022-07-25T12:00:00\"\n\n    # Test that we can format a NaT (Not a Time)\n    t = pd.NaT\n    assert formatting.format_timestamp(t) == \"NaT\"\n\n    # Test that we can format a datetime that's outside the valid range\n    with pytest.raises(OutOfBoundsDatetime):\n        t = pd.Timestamp(\"9999-12-31\")\n        formatting.format_timestamp(t)\n", "def test_diff_array_repr():\n    arr1 = xr.DataArray(np.array([1, 2, 3]))\n    arr2 = xr.DataArray(np.array([4, 5, 6]))\n    assert (\n        diff_array_repr(arr1, arr2, \"identical\")\n        == \"Left and right DataArray objects are not identical\\n    Differing dimensions:\\n        (2) != (2)\\n    Differing values:\\nL\\n[1 2 3]\\nR\\n[4 5 6]\"\n    )\n\n    arr3 = xr.DataArray(np.array([1, 2, 3]))\n    arr4 = xr.DataArray(np.array([1, 2, 3]))\n    assert diff_array_repr(arr3, arr4, \"identical\") == \"\"\n\n    arr5 = xr.DataArray(np.array([1.0, 2.0, 3.0]))\n    arr6 = xr.DataArray(np.array([1.0, 2.0, 3.0]))\n    assert diff_array_repr(arr5, arr6, \"allclose\") == \"\"\n\n    arr7 = xr.DataArray(np.array([1, 2, 3]))\n    arr8 = xr.DataArray(np.array([4, 5, 6]))\n    assert diff_array_repr(arr7, arr8, \"equals\") == \"Left and right DataArray objects are not equal\\n    Differing dimensions:\\n        (3) != (3)\\n    Differing values:\\nL\\n[1 2 3]\\nR\\n[4 5 6]\"\n\n    arr9 = xr.DataArray(np.array([1, 2, 3]))\n    arr10 = xr.DataArray(np.array([4, 5, 6]))\n    assert diff_array_repr(arr9, arr10, lambda x, y: x == y) == \"Left and right DataArray objects are not identical\\n    Differing dimensions:\\n        (3) != (3)\\n    Differing values:\\nL\\n[1 2 3]\\nR\\n[4 5 6]\"\n", "def test_format_timestamp():\n    # Test with a valid timestamp\n    timestamp = pd.Timestamp(\"2022-01-01\")\n    assert formatting.format_timestamp(timestamp) == \"2022-01-01T00:00:00\"\n\n    # Test with a timestamp out of bounds\n    with pytest.raises(pd.errors.OutOfBoundsDatetime):\n        formatting.format_timestamp(pd.Timestamp(\"0001-01-01\"))\n\n    # Test with a NaT value\n    assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n    # Test with a string that can't be converted to a timestamp\n    assert formatting.format_timestamp(\"invalid\") == \"invalid\"\n", "    def test_inline_variable_array_repr(self):\n        ds = xr.Dataset(\n            {\n                \"x\": (\"y\", np.random.rand(5)),\n                \"y\": (\"x\", np.random.rand(5)),\n            }\n        )\n        result = formatting.inline_variable_array_repr(ds, 20)\n        assert result == \"np.ndarray\"\n", "def test_inline_dask_repr():\n    # Test that inline_dask_repr returns the correct string representation\n    da = xr.DataArray(np.random.rand(10), dims=[\"x\"])\n    chunks = da.chunks\n    expected_chunks = f\"chunksize={chunks[0]}\"\n    assert inline_dask_repr(da) == f\"dask.array<chunksize={chunks[0]}\"\n\n    # Test that inline_dask_repr handles meta attribute\n    meta = xr.DataArray(np.random.rand(10), dims=[\"x\"], attrs={\"foo\": \"bar\"})\n    da = da._meta = meta\n    assert inline_dask_repr(da).startswith(\"dask.array<chunksize=\")\n\n    # Test that inline_dask_repr handles unknown types\n    class UnknownType:\n        pass\n\n    array = np.random.rand(10)\n    unknown_array = UnknownType()\n    unknown_array.__array_function__ = np.array\n    unknown_array.__array_interface__ = array.__array_interface__\n    unknown_array._meta = meta\n    with pytest.raises(ImportError):\n        inline_dask_repr(unknown_array)\n\n    # Test that inline_dask_repr handles sparse arrays\n    sparse_array = xr.DataArray(np.random.rand(10), dims=[\"x\"], chunks=chunks)\n    sparse_array._data = sparse_array_type(np.random.rand(10))\n    assert inline_sparse_repr(sparse_array) == (\n        \"<sparse.COO: nnz=10, fill_value=!s>\"\n    ).format(sparse_array._data.fill_value)\n"], "sample_974": ["def test_piecewise_with_no_default_term():\n    from sympy.core import symbols\n    x = symbols('x')\n    expr = Piecewise((x + 1, x > 0), (x, x < 0))\n    expected_output = \"if (x > 0) {x + 1} else {x};\"\n    assert ccode(expr, human=False) == expected_output\n", "def test_piecewise_to_conditional():\n    from sympy import Piecewise, symbols\n    x = symbols('x')\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    result = ccode(expr)\n    expected = \"\"\"if (x > 0) {", "def test_CCodePrinter_with_MultivariateIndexed():\n    from sympy import IndexedBase, Idx, Matrix, MatrixSymbol, symbols\n    from sympy.printing.ccode import CCodePrinter\n\n    A = IndexedBase('A', shape=(3, 3))\n    i, j = Idx('i', 3), Idx('j', 3)\n    A_expr = A[i, j]\n\n    CCodePrinter.contract = False\n\n    code = CCodePrinter().doprint(A_expr)\n    assert code == 'A[i, j]'\n\n    # Test contract=True\n    CCodePrinter.contract = True\n\n    code = CCodePrinter().doprint(A_expr)\n    expected_code = 'for (int i=0; i<3; i++){\\n  for (int j=0; j<3; j++){\\n    A[i, j];\\n  }\\n}'\n    assert code == expected_code\n\n    # Test with a matrix symbol\n    A_sym = MatrixSymbol('A', 3, 3)\n    code = CCodePrinter().doprint(A_expr, assign_to=A_sym)\n    expected_code = 'A_sym[0, 0] = A[0, 0];\\nA_sym[0, 1] = A[0, 1];\\nA_sym[0, 2] = A[0, 2];\\nA_sym[1, 0] = A[1, 0];\\nA_sym[1, 1] = A[1, 1];\\nA_sym[1, 2] = A[1, 2];\\nA_sym[2, 0] = A[2, 0];\\nA_sym[2, 1] = A[2, 1];\\nA_sym[2, 2] = A[2, 2];'\n    assert code == expected_code\n\n    # Test with a matrix symbol and contract=False\n    CCodePrinter.contract = False\n    code = CCodePrinter().doprint(A_expr, assign_to=A_sym)\n    expected_code = 'A_sym[0, 0] = A[0, 0];\\nA_sym[0, 1] = A[0, 1];\\nA_sym[0, 2] = A[0, 2];", "def test_ccode_derefence():\n    x, y, z = symbols('x,y,z')\n    # test that dereferencing works\n    ccode_expr = ccode(x + 2*y + 3*z)\n    assert ccode_expr == \"x + 2*y + 3*z\"\n    # test that dereferencing works with variables in the right order\n    ccode_expr = ccode(x + 2*y + 3*z, dereference=['z', 'y', 'x'])\n    assert ccode_expr == \"x + 2*y + 3*z\"\n    # test that dereferencing works with variables in the wrong order\n    ccode_expr = ccode(x + 2*y + 3*z, dereference=['x', 'y', 'z'])\n    assert ccode_expr == \"(*x) + 2*(*y) + 3*(*z)\"\n", "def test_ccode_with_Piecewise():\n    # Test if the code generation for Piecewise works with different logic\n    from sympy import Piecewise, sin, cos, Abs, exp, ceiling, sqrt, gamma\n    from sympy.sets import Range\n    from sympy.logic import ITE\n    x, y, z = symbols('x,y,z')\n\n    piecewise_expr1 = Piecewise((sin(x), x < 0), (cos(x), True))\n    ccode_expr1 = ccode(piecewise_expr1)\n    assert ccode_expr1 == \"if (x < 0) {\\n  cos(x);\\n}\\nelse {\\n  sin(x);\\n}\"\n\n    piecewise_expr2 = Piecewise((Abs(x), x > 0), (x, True))\n    ccode_expr2 = ccode(piecewise_expr2)\n    assert ccode_expr2 == \"if (x > 0) {\\n  Abs(x);\\n}\\nelse {\\n  x;\\n}\"\n\n    piecewise_expr3 = Piecewise((exp(x), True), (x, True))\n    ccode_expr3 = ccode(piecewise_expr3)\n    assert ccode_expr3 == \"if (True) {\\n  exp(x);\\n}\\nelse {\\n  x;\\n}\"\n\n    # Test if the code generation for Piecewise works with AugmentedAssignments\n    from sympy.codegen import aug_assign\n    y = symbols('y')\n    aug_assign_expr = aug_assign(y, sin(x), x**2)\n    piecewise_aug_assign_expr = Piecewise((aug_assign_expr, x < 0), (y, True))\n    ccode_expr4 = ccode(piecewise_aug_assign_expr)\n    assert ccode_expr4 == \"if (x < 0) {\\ny = sin(x);\\n}\\nelse {\\ny = y;\\n}\"\n\n    # Test if the code generation for Piecewise works with ITE\n    ite_expr = ITE(x < 0, sin(x), cos(x))\n    ccode_expr5 = ccode(ite_expr)\n    assert ccode_expr5 == \"((x < 0) ? cos(x) : sin(x))\"\n\n    # Test if the code generation for Piecewise works with Matrix\n    from sympy import Matrix\n    mat = Matrix([x**2, sin(x), x])\n    piecewise_mat = Piecewise((mat"], "sample_549": ["    def test_delete_masked_points_1D_masked(self):\n        # Test with 1-D masked array\n        a = np.ma.array([1, 2, 3, np.nan, 4, 5])\n        b = np.ma.array([1, 2, 3, np.nan, 4, 5])\n        c = np.ma.array([1, 2, 3, 4, 5, np.nan])\n        result = delete_masked_points(a, b, c)\n        assert_array_equal(result[0], np.ma.array([1, 2, 3, 4, 5]))\n        assert_array_equal(result[1], np.ma.array([1, 2, 3, 4, 5]))\n        assert_array_equal(result[2], np.ma.array([1, 2, 3, 4, 5]))\n", "    def test_delete_masked_points_single_masked_array(self, safe_masked_invalid_mock):\n        masked_array = np.ma.array([1, 2, np.nan, 4, 5], mask=[False, False, True, False, False])\n        non_masked_array = np.array([1, 2, 3, 4, 5])\n        result = delete_masked_points(masked_array, non_masked_array)\n        assert result == (np.ma.array([1, 2, 4, 5], mask=[False, False, False, False]),)\n        assert safe_masked_invalid_mock.call_count == 1\n", "    def test_delete_masked_points_with_list(self):\n        # Test that delete_masked_points works with lists\n        x1 = np.array([1, 2, 3, 4, 5])\n        x2 = np.array([1, 2, np.nan, 4, 5])\n        x3 = np.array([1, 2, 3, np.nan, 5])\n\n        result1, result2, result3 = delete_masked_points(x1, x2, x3)\n        assert_array_equal(result1, [1, 2, 3, 4, 5])\n        assert_array_equal(result2, [1, 2, 4, 5])\n        assert_array_equal(result3, [1, 2, 3, 5])\n", "def test_delete_masked_points_array_of_arrays():\n    # Given\n    arrays = np.array([[1, 2, 3], [4, 5, 6]])\n    mask = np.array([[True, False, False], [True, True, False]])\n\n    # When\n    result = delete_masked_points(*_reshape_2D(arrays, \"test_delete_masked_points_array_of_arrays\"))\n\n    # Then\n    assert_array_equal(result, np.array([[1, 3], [4, 6]])\n", "    def test_no_masked_points(self):\n        # Check that unmasked points are returned unchanged.\n        x, y = np.array([1, 2, 3]), np.array([4, 5, 6])\n        result = delete_masked_points(x, y)\n        assert_array_equal(result, (x, y))\n"], "sample_73": ["    def test_hash_key(self):\n        storage = ManifestStaticFilesStorage()\n        self.assertEqual(storage.hash_key('test.css'), 'test.css')\n        self.assertEqual(storage.hash_key('test.css'), 'test.css')\n        self.assertEqual(storage.hash_key('test.js'), 'test.js')\n        self.assertEqual(storage.hash_key('test.js'), 'test.js')\n", "    def test_manifest_loads_correctly(self):\n        manifest = self.manifest_storage.read_manifest()\n        self.assertIsInstance(manifest, str)\n", "    def test_manifest_version(self):\n        # Create a storage instance with the default settings\n        storage = ManifestStaticFilesStorage()\n        self.assertEqual(storage.manifest_version, '1.0')\n        self.assertEqual(storage.manifest_name, 'staticfiles.json')\n        self.assertTrue(storage.manifest_strict)\n", "    def test_hashed_files_storage_clear(self):\n        storage = storage.staticfiles_storage\n        self.assertEqual(storage.hashed_files, {})\n        storage.post_process([], dry_run=True)\n        self.assertEqual(storage.hashed_files, {})\n        storage.post_process([], dry_run=False)\n        self.assertNotEqual(storage.hashed_files, {})\n", "    def test_manifest_strict(self):\n        self.assertTrue(storage.staticfiles_storage.manifest_strict)\n        with override_settings(STATICFILES_STORAGE='ManifestStaticFilesStorage'):\n            self.assertTrue(storage.staticfiles_storage.manifest_strict)\n"], "sample_62": ["    def setUp(self):\n        self.site = CustomSite()\n", "    def test_register(self):\n        self.site.register(Location, NameAdmin)\n        self.assertTrue(self.site.is_registered(Location))\n        self.assertIsInstance(self.site._registry[Location], admin.ModelAdmin)\n", "    def test_register_unregister(self):\n        self.site.register(Location, NameAdmin)\n        self.assertTrue(self.site.is_registered(Location))\n        self.site.unregister(Location)\n        self.assertFalse(self.site.is_registered(Location))\n", "    def setUp(self):\n        self.site = CustomSite()\n", "    def setUp(self):\n        self.site = CustomSite()\n"], "sample_221": ["    def test_values(self):\n        obj = MyEvent.objects.create(event_name='test')\n        self.assertEqual(list(MyEvent.objects.values('event_name')), [{'event_name': 'test'}])\n        self.assertEqual(list(MyEvent.objects.values('event_name').filter(event_name='test')), [{'event_name': 'test'}])\n", "    def setUpTestData(cls):\n        class MyModel(models.Model):\n            pass\n\n        class MyQuerySet(models.QuerySet):\n            class Meta:\n                model = MyModel\n\n        MyModel._default_manager = MyQuerySet\n\n        class MyModelManager(models.Manager.from_queryset(MyQuerySet)):\n            pass\n\n        MyModel._default_manager = MyModelManager()\n\n        cls.MyModel = MyModel\n        cls.MyModel.objects = MyModel._default_manager\n", "    def test_iterable_return_values(self):\n        qs = Happening.objects.all()\n        self.assertPickles(qs)\n        self.assertEqual(list(qs), list(qs.iterator()))\n", "    def setUpTestData(cls):\n        cls.model = models.Model()\n        cls.model._meta.concrete_fields = [models.CharField(max_length=255), models.IntegerField()]\n", "    def test_iterable_class(self):\n        qs = Happening.objects.all()\n        self.assertEqual(qs._iterable_class, ModelIterable)\n        qs.values_list()\n        self.assertEqual(qs._iterable_class, ValuesListIterable)\n        qs.values_list(flat=True)\n        self.assertEqual(qs._iterable_class, FlatValuesListIterable)\n        qs.values_list(named=True)\n        self.assertEqual(qs._iterable_class, NamedValuesListIterable)\n"], "sample_330": ["    def test_typecast_timestamp(self):\n        # Test typecast_timestamp with valid timestamp string\n        timestamp = '2022-01-01 12:00:00.000000-05'\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 1, 1, 12, 0, 0))\n\n        # Test typecast_timestamp with invalid timestamp string\n        self.assertIsNone(typecast_timestamp('Invalid Timestamp'))\n\n        # Test typecast_timestamp with null timestamp\n        self.assertIsNone(typecast_timestamp(None))\n\n        # Test typecast_timestamp with timestamp string without timezone information\n        timestamp = '2022-01-01 12:00:00'\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 1, 1, 12, 0, 0))\n\n        # Test typecast_timestamp with timestamp string with fractional seconds\n        timestamp = '2022-01-01 12:00:00.123456-05'\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 1, 1, 12, 0, 0, 123456))\n", "    def test_timestamp_quoting(self):\n        # Test typecast_timestamp function\n        timestamp = \"2022-07-25 14:30:00.123456\"\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 7, 25, 14, 30, 0, 123456))\n\n        # Test typecast_timestamp function with time zone information\n        timestamp = \"2022-07-25 14:30:00-05:00\"\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 7, 25, 14, 30, 0, 0))\n\n        # Test typecast_timestamp function with seconds without fractional part\n        timestamp = \"2022-07-25 14:30:00\"\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 7, 25, 14, 30, 0))\n\n        # Test typecast_timestamp function with seconds and fractional part\n        timestamp = \"2022-07-25 14:30:00.123\"\n        self.assertEqual(typecast_timestamp(timestamp), datetime.datetime(2022, 7, 25, 14, 30, 0, 123000))\n\n        # Test typecast_timestamp function with empty string\n        self.assertIsNone(typecast_timestamp(\"\"))\n\n        # Test typecast_timestamp function with None\n        self.assertIsNone(typecast_timestamp(None))\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2022-01-01'), datetime.date(2022, 1, 1))\n        self.assertEqual(typecast_date('2022-01-01 00:00:00'), datetime.date(2022, 1, 1))\n        self.assertIsNone(typecast_date(None))\n        self.assertIsNone(typecast_date(''))\n", "    def test_truncate_name_empty_string(self):\n        self.assertEqual(truncate_name(''), '')\n", "    def test_truncate_name(self):\n        self.assertEqual(truncate_name('TABLE'), 'TABLE')\n        self.assertEqual(truncate_name('TABLE', length=10), 'TABLE')\n\n        self.assertEqual(truncate_name('USER.TABLE', length=10), 'USER.TABLE')\n        self.assertEqual(truncate_name('USER.TABLE', length=10), 'USER.TABLE')\n\n        self.assertEqual(truncate_name('USER.TABLE', length=5), 'USER.T')\n        self.assertEqual(truncate_name('USER.TABLE', length=5, hash_len=4), 'USER.TB66CC')\n\n        self.assertEqual(truncate_name('USER.TABLE', length=5, hash_len=6), 'USER.TB66CC')\n"], "sample_248": ["    def test_handle_shell_import_error(self):\n        with self.assertRaises(CommandError):\n            call_command('shell', '--interface', 'non_existent_interface')\n", "    def test_python_startup_and_pythonrc_py(self):\n        with captured_stdout() as stdout:\n            with captured_stdin() as stdin:\n                stdin.write('import sys; print(sys.executable)\\n')\n                stdin.seek(0)\n                call_command('shell', stdin=stdin, stdout=stdout)\n                self.assertIn(sys.executable, stdout.getvalue())\n\n        # Test .pythonrc.py existence\n        with captured_stdout() as stdout:\n            with captured_stdin() as stdin:\n                stdin.write('import sys; print(os.path.exists(\"~/.pythonrc.py\"))\\n')\n                stdin.seek(0)\n                call_command('shell', stdin=stdin, stdout=stdout)\n                self.assertTrue(stdout.getvalue().startswith('True'))\n\n        # Test .pythonrc.py execution\n        with captured_stdout() as stdout:\n            with captured_stdin() as stdin:\n                stdin.write('import sys; print(sys.path)\\n')\n                stdin.seek(0)\n                call_command('shell', stdin=stdin, stdout=stdout)\n                self.assertIn('/home/user/.pythonrc.py', stdout.getvalue())\n", "    def test_python_startup_file_error(self):\n        with captured_stdout():\n            with captured_stdin('import sys\\nsys.exit(1)\\n') as stdin:\n                call_command('shell', interface='python')\n        self.assertRegex(captured_stdout(), r\"Traceback \\(most recent call last\\)\\n.*\")\n", "    def test_no_startup(self):\n        # Test that not using IPython and not setting no_startup doesn't run the startup code.\n        script = 'print(\"__name__\" in globals())'\n        with captured_stdout() as stdout:\n            call_command('shell', '--interface=python', '--no-startup')\n            self.assertNotIn('Executing .pythonrc.py', stdout.getvalue())\n        with captured_stdout() as stdout:\n            call_command('shell', '--interface=python')\n            self.assertIn('Executing .pythonrc.py', stdout.getvalue())\n", "    def setUp(self):\n        self.stdio = captured_stdin(captured_stdout())\n        self.stdio.__enter__()\n        self.addCleanup(self.stdio.__exit__, None, None, None)\n"], "sample_624": ["def test_calc_max_rows_first_last():\n    assert calc_max_rows_first(10) == 5\n    assert calc_max_rows_first(9) == 4\n    assert calc_max_rows_first(1) == 1\n    assert calc_max_rows_first(0) == 0\n\n    assert calc_max_rows_last(10) == 5\n    assert calc_max_rows_last(9) == 4\n    assert calc_max_rows_last(1) == 0\n    assert calc_max_rows_last(0) == 0\n", "def test_format_timestamp():\n    # Test valid timestamps\n    t = pd.Timestamp(\"2022-01-01 12:00:00\")\n    assert formatting.format_timestamp(t) == \"2022-01-01T12:00:00\"\n\n    # Test invalid timestamps (out of bounds)\n    with pytest.raises(pd.errors.OutOfBoundsDatetime):\n        formatting.format_timestamp(\"9999-12-31 12:00:00\")\n\n    # Test NaT\n    t = pd.NaT\n    assert formatting.format_timestamp(t) == \"NaT\"\n\n    # Test strings\n    t = \"2022-01-01 12:00:00\"\n    assert formatting.format_timestamp(t) == \"2022-01-01 12:00:00\"\n", "def test_format_timestamp():\n    # Test with valid timestamp\n    t = pd.Timestamp(\"2022-01-01 00:00:00\")\n    assert formatting.format_timestamp(t) == \"2022-01-01 00:00:00\"\n\n    # Test with time zone\n    t = pd.Timestamp(\"2022-01-01 00:00:00+00:00\")\n    assert formatting.format_timestamp(t) == \"2022-01-01T00:00:00+00:00\"\n\n    # Test with invalid timestamp\n    t = pd.Timestamp(\"9999-12-31 23:59:59\")\n    assert formatting.format_timestamp(t) == \"9999-12-31 23:59:59\"\n\n    # Test with NaT\n    t = pd.NaT\n    assert formatting.format_timestamp(t) == \"NaT\"\n\n    # Test with datetime object\n    t = datetime(2022, 1, 1)\n    assert formatting.format_timestamp(t) == \"2022-01-01 00:00:00\"\n\n    # Test with non- datetime object\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(100)\n", "    def test_inline_variable_array_repr(self):\n        # Test inline_variable_array_repr without _repr_inline_\n        var = xr.Variable(\"test\", np.array([1, 2, 3]))\n        max_width = 20\n        result = formatting.inline_variable_array_repr(var, max_width)\n        expected = \"np.ndarray shape=(3,) dtype=float64\"\n        assert result == expected\n\n        # Test inline_variable_array_repr with _repr_inline_\n        class Var:\n                return \"custom\"\n\n        var = Var()\n        max_width = 20\n        result = formatting.inline_variable_array_repr(var, max_width)\n        expected = \"custom\"\n        assert result == expected\n\n        # Test inline_variable_array_repr with dask array\n        da = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n        max_width = 20\n        result = formatting.inline_variable_array_repr(da, max_width)\n        expected = \"dask.array<chunksize=(3,)>\"\n        assert result == expected\n\n        # Test inline_variable_array_repr with sparse array\n        sa = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"], coords={\"x\": [1, 2, 3]})\n        max_width = 20\n        result = formatting.inline_variable_array_repr(sa, max_width)\n        expected = \"<COO: nnz=3, fill_value=nan>\"\n        assert result == expected\n", "    def test_format_timestamp_datetime_object(self):\n        # Test that a datetime object is properly formatted\n        dt = pd.Timestamp(\"2022-01-01 12:00:00\")\n        assert formatting.format_timestamp(dt) == \"2022-01-01T12:00:00\"\n"], "sample_541": ["def test_slider_label_empty_string():\n    with pytest.raises(ValueError):\n        widgets.Slider(ax=get_ax(), label='', valmin=0, valmax=10,\n                      valinit=5)\n", "def test_lasso_selector_selection(ax):\n    # Initialize the lasso selector\n        assert len(verts) == 2\n        assert_allclose(verts, [[0.5, 0.5], [1.0, 1.0]])\n    lasso = widgets.LassoSelector(ax, onselect)\n\n    # Draw a rectangle\n    line = Line2D([0.3, 0.7], [0.3, 0.7], color='k', lw=2)\n    ax.add_line(line)\n\n    # Perform a lasso selection\n    click_and_drag(ax, 0.4, 0.4, 1.0, 1.0)\n    assert len(lasso.verts) == 2\n    assert_allclose(lasso.verts, [[0.5, 0.5], [1.0, 1.0]])\n\n    # Perform another lasso selection\n    click_and_drag(ax, 0.4, 0.4, 1.1, 1.1)\n    assert len(lasso.verts) == 4\n    assert_allclose(lasso.verts, [[0.5, 0.5], [1.0, 1.0], [0.5, 0.5], [1.0, 1.0]])\n", "def test_LassoSelector_draw_bounding_box():\n    fig, ax = plt.subplots()\n    ax.add_patch(Rectangle((0, 0), 10, 10, facecolor='red'))\n    lasso = widgets.LassoSelector(ax, lambda *args: None,\n                                  draw_bounding_box=True, interactive=False)\n    assert lasso._draw_box\n    assert lasso._box is None\n\n    ax.set_xlim([0, 10])\n    ax.set_ylim([0, 10])\n    lasso.onmove(mock_event(MouseEvent(1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 'button_press',\n                                      1, 0, 0, None, None, 'left',\n                                      'button', 'motion', None, None, None,\n                                      None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None, None, None, None, None,\n                                      None, None", "def test_Button_on_clicked():\n    ax = get_ax()\n    button = widgets.Button(ax, 'test button')\n        return event.inaxes\n    cid = button.on_clicked(callback)\n    button.disconnect(cid)\n    assert button.get_active() == False\n    button.set_active(True)\n    assert button.get_active() == True\n    assert button.ignore(mock_event('button_press_event', ax=ax)) == False\n    assert button.ignore(mock_event('button_release_event', ax=ax)) == False\n    assert button.ignore(mock_event('motion_notify_event', ax=ax)) == False\n    assert button.ignore(mock_event('button_press_event', ax=None)) == True\n    button.set_active(False)\n    assert button.get_active() == False\n    assert button.ignore(mock_event('button_press_event', ax=ax)) == True\n    ax.canvas.draw()\n    ax.canvas.flush_events()\n    with click_and_drag(button, ax, 'motion_notify_event') as (event, mouse_event):\n        assert button.get_active() == True\n        button.set_active(False)\n        assert button.get_active() == False\n        button.set_active(True)\n        assert button.get_active() == True\n        assert button.ignore(mock_event('motion_notify_event', ax=ax)) == False\n    assert button.get_active() == True\n    ax.canvas.draw()\n    ax.canvas.flush_events()\n    with click_and_drag(button, ax, 'motion_notify_event') as (event, mouse_event):\n        button.set_active(False)\n        assert button.get_active() == False\n        button.set_active(True)\n        assert button.get_active() == True\n        assert button.ignore(mock_event('motion_notify_event', ax=ax)) == False\n        button.set_active(False)\n        assert button.get_active() == False\n    assert button.get_active() == False\n    assert button.ignore(mock_event('motion_notify_event', ax=ax)) == True\n", "def test_slider():\n    fig, ax = plt.subplots()\n    slider = widgets.Slider(ax, 'slider', 0, 10)\n    # test slider range\n    assert slider.valmin == 0\n    assert slider.valmax == 10\n    assert slider.val == 0\n    # test on_changed\n        pass\n    slider.on_changed(test)\n    slider.val = 5\n    slider.on_changed(test)()\n    # test set_val\n    slider.set_val(7)\n    assert slider.val == 7\n    # test _update\n        pass\n    slider.on_changed(test_update)\n    event = mock.Mock()\n    slider._update(event)\n    slider.val = 3\n    slider.set_val(7)\n    # test _value_in_bounds\n    assert slider._value_in_bounds(15) == 10\n    assert slider._value_in_bounds(-5) == -5\n    assert slider._value_in_bounds(12) == 10\n    # test sliderstep\n    slider.valstep = 3\n    slider.val = 2\n    assert slider.val == 3\n    # test slidermin and slidermax\n    slider.slidermin = widgets.Slider(ax, 'min', 0, 10)\n    slider.slidermax = widgets.Slider(ax, 'max', 0, 10)\n    slider.val = 15\n    assert slider.val == 10\n    # test initcolor and track_color\n    slider.initcolor = 'r'\n    slider.track_color = 'b'\n    slider.val = 5\n    # test valfmt\n    slider.valfmt = '%.2f'\n    slider.set_val(5)\n    # test dragging\n    slider.dragging = False\n    slider.set_val(7)\n    slider._update(event)\n    assert slider.val == 7\n"], "sample_54": ["    def test_status_code_setting(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        response.status_code = 404\n        self.assertEqual(response.status_code, 404)\n", "    def test_HTTPResponse_repr(self):\n        response = HttpResponse(content=b'Hello, World!')\n        self.assertEqual(str(response), '<HttpResponse status_code=200>')\n", "    def test_set_headers_filelike_with_name(self):\n        with tempfile.NamedTemporaryFile() as filelike:\n            response = FileResponse(filelike, filename='example.txt')\n            self.assertEqual(response.get('Content-Length'), os.path.getsize(filelike.name))\n            self.assertEqual(response.get('Content-Type'), 'application/octet-stream')\n", "    def test_reason_phrase_property(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n        response.status_code = 404\n        self.assertEqual(response.reason_phrase, 'Not Found')\n        response._reason_phrase = 'Custom Reason'\n        self.assertEqual(response.reason_phrase, 'Custom Reason')\n", "    def test_reject_non_string_value_for_reason_phrase(self):\n        response = HttpResponse()\n        with self.assertRaises(TypeError):\n            response.reason_phrase = 123\n"], "sample_824": ["def test_check_pairwise_arrays_sparse():\n    X = csr_matrix(np.array([[1, 2], [3, 4]])\n    Y = csr_matrix(np.array([[5, 6], [7, 8]])\n\n    with ignore_warnings():\n        with pytest.raises(ValueError):\n            check_pairwise_arrays(X, Y, precomputed=True)\n\n    with ignore_warnings():\n        with pytest.raises(ValueError):\n            check_pairwise_arrays(X, Y, precomputed=False)\n\n    X = csr_matrix(np.array([[1, 2], [3, 4]])\n    Y = csr_matrix(np.array([[5, 6], [7, 8]])\n\n    X, Y = check_pairwise_arrays(X, Y)\n    assert issparse(X)\n    assert issparse(Y)\n\n    X = csr_matrix(np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n    X, Y = check_pairwise_arrays(X, Y)\n    assert not issparse(X)\n    assert not issparse(Y)\n\n    X = np.array([[1, 2], [3, 4]])\n    Y = csr_matrix(np.array([[5, 6], [7, 8]])\n\n    X, Y = check_pairwise_arrays(X, Y)\n    assert not issparse(X)\n    assert issparse(Y)\n\n    X = csr_matrix(np.array([[1, 2], [3, 4]])\n    Y = csr_matrix(np.array([[5, 6], [7, 8]]))\n\n    X, Y = check_pairwise_arrays(X, Y, precomputed=True)\n    assert issparse(X)\n    assert issparse(Y)\n\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n    X, Y = check_pairwise_arrays(X, Y, precomputed=True)\n    assert not issparse(X)\n    assert not issparse(Y)\n", "def test_pairwise_distances_chunked_generator():\n    X = np.random.rand(100, 10)\n    with pytest.raises(TypeError):\n        list(pairwise_distances_chunked(X))\n\n    with ignore_warnings():\n        gen = pairwise_distances_chunked(X, reduce_func=lambda x: None)\n        assert isinstance(gen, GeneratorType)\n\n    # Check that the generator can be iterated over multiple times\n    gen = pairwise_distances_chunked(X, reduce_func=lambda x: None)\n    for _ in gen:\n        pass\n    for _ in gen:\n        pass\n\n    # Check that the generator can be iterated over with a reduce function\n    gen = pairwise_distances_chunked(X, reduce_func=lambda x: None)\n    for _ in gen:\n        pass\n    gen = pairwise_distances_chunked(X, reduce_func=lambda x: None)\n    for _ in gen:\n        pass\n", "def test_check_pairwise_arrays_precomputed():\n    X = [[1, 2], [3, 4]]\n    Y = None\n    safe_X, safe_Y = check_pairwise_arrays(X, Y, precomputed=True)\n    assert safe_X is safe_Y\n    assert_array_equal(safe_X, X)\n    assert safe_Y is X\n", "def test_manhattan_distances_scalar():\n    X = np.array([[0, 1], [1, 1]])\n    Y = np.array([[0, 1], [2, 1]])\n\n    expected = np.array([[0., 1.], [1., 0.]])\n\n    assert_array_almost_equal(manhattan_distances(X, Y), expected)\n\n", "def test_manhattan_distances():\n    # Test for all zeros\n    X = np.zeros((10, 5))\n    assert_array_equal(manhattan_distances(X, X), np.zeros((10, 10)))\n\n    # Test for all ones\n    X = np.ones((10, 5))\n    assert_array_equal(manhattan_distances(X, X), np.ones((10, 10)))\n\n    # Test for some zeros and ones\n    X = np.array([[0, 1, 0, 1, 0],\n                  [1, 0, 1, 0, 1],\n                  [0, 1, 0, 1, 0],\n                  [1, 0, 1, 0, 1],\n                  [0, 1, 0, 1, 0]])\n    assert_array_equal(manhattan_distances(X, X), np.array([[0, 2, 0, 2, 0],\n                                                            [2, 0, 2, 0, 2],\n                                                            [0, 2, 0, 2, 0],\n                                                            [2, 0, 2, 0, 2],\n                                                            [0, 2, 0, 2, 0]]))\n\n    # Test for matrix with different shapes\n    X = np.array([[0, 1, 0, 1, 0],\n                  [1, 0, 1, 0, 1]])\n    Y = np.array([[0, 1, 0, 1, 0,\n                   1, 0, 1, 0, 1]])\n    assert_array_equal(manhattan_distances(X, Y), np.array([[0, 2, 0, 2, 0, 2],\n                                                          [2, 0, 2, 0, 2, 0]]))\n\n    # Test for vector with different shapes\n    X = np.array([[0, 1, 0, 1, 0]])\n    Y = np.array([[0, 1, 0, 1, 0,\n                   1, 0, 1, 0, 1]])\n    assert_array_equal(manhattan_distances(X, Y), np.array([[0, 2, 0, 2, 0, 2]]))\n\n    # Test for ValueError\n    X = np.array([[0,"], "sample_716": ["    def test_ridge_regression_sparse_input(self):\n        X, y = make_regression(n_samples=10, n_features=5, random_state=0)\n        X_sparse = sp.csr_matrix(X)\n        ridge = Ridge(alpha=0.1)\n        ridge.fit(X_sparse, y)\n        assert_array_almost_equal(ridge.coef_, ridge.fit(X, y).coef_)\n", "    def test_ridge_classifiers_equal_weights(self):\n        rng = check_random_state(0)\n        X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                             n_classes=3, n_labels=2,\n                                             rng=rng, random_state=0)\n        clf = RidgeClassifier()\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        y_pred_proba = clf.predict_proba(X)\n        assert_array_equal(y_pred, y)\n        assert_array_almost_equal(y_pred_proba.sum(axis=1), 1, decimal=2)\n", "    def test_ridge_regression_sparse_kernel(self):\n        X, y = make_regression(n_samples=100, n_features=100, n_informative=50, random_state=42)\n        X = sp.csr_matrix(X)\n        X = X.todense()\n        y = y[:, np.newaxis]\n        alpha = np.array([0.01, 0.1, 1.0])\n        coef = ridge_regression(X, y, alpha, solver='cholesky')\n        assert_array_almost_equal(ridge_regression(X, y, alpha, solver='lsqr'), coef)\n        assert_array_almost_equal(ridge_regression(X, y, alpha, solver='svd'), coef)\n", "def test_RidgeClassifierCV_partial_fit():\n    X, y = make_regression(n_samples=10, n_features=5, n_informative=3,\n                           random_state=0)\n    y = np.where(y > 0, 1, 0)\n\n    # One-vs-one strategy\n    clf = RidgeClassifierCV(alphas=0.1, cv=2, n_jobs=-1)\n    clf.partial_fit(X, y, classes=np.unique(y))\n    assert_equal(clf.classes_, np.array([0, 1]))\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    # One-vs-rest strategy\n    clf = RidgeClassifierCV(alphas=0.1, cv=2, n_jobs=-1, multi_class='ovr')\n    clf.partial_fit(X, y, classes=np.unique(y))\n    assert_equal(clf.classes_, np.array([0, 1]))\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    # One-vs-rest strategy with multiclass\n    X, y = make_multilabel_classification(n_samples=10, n_features=5,\n                                         n_classes=3, n_labels=2,\n                                         random_state=0)\n    clf = RidgeClassifierCV(alphas=0.1, cv=2, n_jobs=-1, multi_class='ovr')\n    clf.partial_fit(X, y)\n    assert_equal(clf.classes_, np.array([0, 1, 2]))\n    y_pred = clf.predict(X)\n    assert_array_equal(y_pred, y)\n\n    # Test that partial fit doesn't change the parameters\n    X, y = make_regression(n_samples=10, n_features=5, n_informative=3,\n                           random_state=0)\n    y = np.where(y > 0, 1, 0)\n    clf = RidgeClassifierCV(alphas=[0.1, 0.5, 1])\n    clf.partial_fit(X, y)\n    assert_array_equal(clf.alphas_, [0.1, 0.5, 1])\n\n    # Test that partial fit doesn't change the n_iter_\n    X, y = make_regression(n_samples=10, n_features=5, n_informative=3,\n                           random_state=0)\n    y = np.where(y > 0, 1, 0)\n    clf = RidgeClassifierCV(alphas=0.1, cv", "def test_ridge_regression_sparse(self):\n    n_samples = 20\n    n_features = 5\n    X = sp.rand(n_samples, n_features, density=0.9, random_state=0)\n    y = np.dot(X, np.random.rand(n_features)) + np.random.randn(n_samples)\n    X = DENSE_FILTER(X)\n\n    alphas = [0.1, 1.0, 10.0]\n    alpha_1 = 0.001\n    alpha_2 = 1e10\n\n    estimator = Ridge(alpha=alpha_1, fit_intercept=False)\n    estimator.fit(X, y)\n    y_pred_1 = estimator.predict(X)\n\n    estimator = Ridge(alpha=alpha_2, fit_intercept=False)\n    estimator.fit(X, y)\n    y_pred_2 = estimator.predict(X)\n\n    y_pred_avg = (y_pred_1 + y_pred_2) / 2\n\n    assert_greater(np.abs(np.mean(y - y_pred_avg)), 0.05)\n"], "sample_25": ["def test_keyword_to_rvkc():\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    assert card.field_specifier == \"AXIS.1\"\n    assert card.keyword == \"TESTKEY.AXIS.1\"\n    assert card.value == 2\n    assert card.rawvalue == \"AXIS.1: 2\"\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.field_specifier = \"AXIS.2\"\n    assert card.field_specifier == \"AXIS.2\"\n    assert card.keyword == \"TESTKEY.AXIS.2\"\n    assert card.value == 2\n    assert card.rawvalue == \"AXIS.2: 2\"\n\n    with pytest.raises(ValueError):\n        card.field_specifier = \"\"\n\n    with pytest.raises(AttributeError):\n        del card.field_specifier\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.field_specifier = \"AXIS.1\"\n    with pytest.raises(AttributeError):\n        del card.field_specifier\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.value = 3\n    assert card.field_specifier is None\n    assert card.keyword == \"TESTKEY\"\n    assert card.value == 3\n    assert card.rawvalue == \"3\"\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.value = 3.0\n    assert card.field_specifier is None\n    assert card.keyword == \"TESTKEY\"\n    assert card.value == 3.0\n    assert card.rawvalue == \"3.0\"\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.value = 3.0\n    with pytest.raises(ValueError):\n        card.value = None\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.value = \"3\"\n    assert card.field_specifier is None\n    assert card.keyword == \"TESTKEY\"\n    assert card.value == \"3\"\n    assert card.rawvalue == \"3\"\n\n    card = fits.Card(\"TESTKEY\", \"DP1.AXIS.1: 2\")\n    card.value = \"3.0\"\n", "    def test_keyword_length(self):\n        card = Card()\n        card.keyword = \"ABCDE\"\n        self.assertEqual(card.keyword, \"ABCDE\")\n        with pytest.raises(ValueError):\n            card.keyword = \"ABCDEF\"\n        with pytest.raises(ValueError):\n            card.keyword = \"AB CDE\"\n        with pytest.raises(ValueError):\n            card.keyword = \"AB12DE\"\n\n        card = Card()\n        card.keyword = \"HIERARCH AB\"\n        self.assertEqual(card.keyword, \"HIERARCH AB\")\n        with pytest.raises(ValueError):\n            card.keyword = \"HIERARCH ABC\"\n\n        card = Card()\n        card.keyword = \"HIERARCHAB\"\n        self.assertEqual(card.keyword, \"HIERARCHAB\")\n", "def test_card_card_fromstring_bytes():\n    card = fits.Card.fromstring(b'SIMPLE  =T / written by test')\n    assert card.keyword == 'SIMPLE'\n    assert card.value == True\n    assert card.comment == 'written by test'\n    assert card.image == 'SIMPLE  =T / written by test'\n    assert len(card) == 3\n\n    card = fits.Card.fromstring(b'COMMENT  This is a comment line')\n    assert card.keyword == 'COMMENT'\n    assert card.value == 'This is a comment line'\n    assert card.comment is None\n    assert card.image == 'COMMENT  This is a comment line'\n    assert len(card) == 3\n\n    card = fits.Card.fromstring(b'HIERARCH HIERARCH TELESCOP=HST / HST telescope')\n    assert card.keyword == 'TELESCOP'\n    assert card.value == 'HST / HST telescope'\n    assert card.comment is None\n    assert card.image == 'HIERARCH HIERARCH TELESCOP=HST / HST telescope'\n    assert len(card) == 3\n\n    card = fits.Card.fromstring(b'    ')\n    assert card.keyword == ''\n    assert card.value == ''\n    assert card.comment == ''\n    assert card.image == '      '\n    assert len(card) == 3\n", "    def test_card__repr(self):\n        card = Card(\"SIMPLE  =        T\")\n        self.assertEqual(repr(card), (\"SIMPLE\", \"True\", \"\"))\n        self.assertEqual(str(card), \"SIMPLE  =        T\")\n", "def test_fix_value():\n    # Test that fix_value raises a ValueError when the card is already valid\n    card = Card.fromstring(\"CARD=1.234567890123456789\")\n    with pytest.raises(ValueError):\n        card.fix_value()\n\n    # Test that fix_value raises an AttributeError when the card has an invalid state\n    card = Card.fromstring(\"CARD=1.234567890123456789\")\n    card._invalid = True\n    with pytest.raises(AttributeError):\n        card.fix_value()\n\n    # Test that fix_value can fix a value string with a sign\n    card = Card.fromstring(\"CARD=-1.234567890123456789\")\n    card._fix_value()\n    assert card.image == \"CARD =  -1.234567890123456789\"\n    assert card._value == -1.234567890123456789\n\n    # Test that fix_value can fix a value string with a decimal\n    card = Card.fromstring(\"CARD= 1.234567890123456789\")\n    card._fix_value()\n    assert card.image == \"CARD = 1.234567890123456789\"\n    assert card._value == 1.234567890123456789\n\n    # Test that fix_value can fix a value string with an exponent\n    card = Card.fromstring(\"CARD=1.234567890123456789e-10\")\n    card._fix_value()\n    assert card.image == \"CARD = 1.234567890123456789E-10\"\n    assert card._value == 1.234567890123456789e-10\n\n    # Test that fix_value raises a ValueError when the card value cannot be converted\n    card = Card.fromstring(\"CARD= invalid value\")\n    with pytest.raises(ValueError):\n        card._fix_value()\n\n    # Test that fix_value can fix a long value string by truncating it\n    card = Card.fromstring(\n        \"CARD=123456789012345678901234567890123456789012345678901234567890\"\n    )\n    card._fix_value()\n    assert card.image == \"CARD = 1234567890123456789012345678901&\"\n    assert card._value == 1.2345678901234568e+22\n\n    # Test that fix_value raises an AssertionError when the value is already modified\n    card = Card.fromstring(\"CARD=1.234567890123456789\")\n    card"], "sample_752": ["def test_isolation_forest_max_samples_not_float():\n    \"\"\"Check that max_samples is not a float.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    X_sparse = csc_matrix([[1, 2], [3, 4]])\n    n_samples = 2\n    max_samples = 1.5\n    with pytest.raises(ValueError):\n        IsolationForest(max_samples=max_samples).fit(X)\n    with pytest.raises(ValueError):\n        IsolationForest(max_samples=max_samples).fit(X_sparse)\n    max_samples = 1.5 * n_samples\n    with pytest.raises(ValueError):\n        IsolationForest(max_samples=max_samples).fit(X)\n    with pytest.raises(ValueError):\n        IsolationForest(max_samples=max_samples).fit(X_sparse)\n", "def test_isolation_forest_max_features():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    iforest = IsolationForest(max_features=2, contamination=0.1, random_state=42)\n    iforest.fit(X)\n    assert_equal(iforest.n_features_in_, 2)\n    assert_equal(iforest._max_features, 2)\n    assert_equal(iforest.estimators_features_[0].size, 2)\n", "def test_isolation_forest_contamination_auto():\n    \"\"\"Test that isolation forest works with contamination='auto'\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 1, 0, 1])\n    clf = IsolationForest(contamination='auto')\n    clf.fit(X, y)\n    assert clf.offset_ == -0.5\n    assert clf.decision_function(X) < 0\n", "def test_isolation_forest_ensemble_size():\n    # Create a dataset with 10 features and 1000 samples\n    X = np.random.rand(1000, 10)\n    y = np.zeros(1000)\n    y[:100] = 1  # 10% of the samples are outliers\n\n    # Train an Isolation Forest with different ensemble sizes\n    for n_estimators in [10, 50, 100, 500]:\n        iforest = IsolationForest(n_estimators=n_estimators, random_state=0)\n        iforest.fit(X, y)\n        scores = iforest.score_samples(X)\n        assert_greater(np.mean(scores[y == 0]), np.mean(scores[y == 1]))\n", "def test_score_samples():\n    \"\"\"Check that the score_samples method works on a clean dataset.\n\n    \"\"\"\n    X = np.random.rand(100, 10)\n    clf = IsolationForest(contamination=0.1, random_state=0)\n    clf.fit(X)\n    score = clf.score_samples(X)\n    assert_array_almost_equal(score, np.ones(100))\n"], "sample_202": ["    def test_update_cookie_with_empty_data(self):\n        storage = self.storage_class()\n        response = self.response\n        storage._update_cookie('', response)\n        self.assertNotIn(CookieStorage.cookie_name, response.cookies)\n", "    def test_store_and_retrieve_message_with_extra_tags(self):\n        # Create a message with extra tags\n        message = Message('info', 'Test message', extra_tags='tag1;tag2')\n        storage = self.storage_class(self.request)\n\n        # Store the message\n        response = self.response_class()\n        set_cookie_data(storage, [message])\n        storage.save_messages(response)\n\n        # Retrieve the message\n        storage.request = self.request\n        messages, _ = storage._get()\n        self.assertEqual(len(messages), 1)\n        self.assertEqual(messages[0].level, message.level)\n        self.assertEqual(messages[0].message, message.message)\n        self.assertEqual(messages[0].extra_tags, message.extra_tags)\n", "    def test_stored_messages_not_finished(self):\n        storage = self.storage_class(self.request)\n        messages = [Message('level', 'message')]\n        response = SimpleCookie()\n        stored_messages = storage._store(messages, response)\n        self.assertEqual(stored_messages, [])\n        self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n        self.assertEqual(response.cookies.get('messages')['max-age'], 0)\n", "    def test_empty_messages_are_encoded(self):\n        storage = self.storage_class()\n        set_cookie_data(storage, [])\n        self.assertTrue(storage._decode(storage.request.COOKIES.get(CookieStorage.cookie_name)))\n", "    def test_store_and_decode_messages_with_empty_messages(self):\n        storage = self.get_storage()\n        set_cookie_data(storage, [])\n        self.assertEqual(stored_cookie_messages_count(storage, self.response), 0)\n"], "sample_294": ["    def _get_GET_csrf_cookie_request(self):\n        req = TestingHttpRequest()\n        req.method = 'GET'\n        req.META['HTTP_COOKIE'] = f\"{settings.CSRF_COOKIE_NAME}={self._csrf_id}\"\n        return req\n", "    def test_no_csrf_token_check_for_GET_request(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(req, None, (), {})\n        self.assertIsNone(response)\n", "    def test_csrf_trusted_origins_hosts(self):\n        with self.settings(CSRF_TRUSTED_ORIGINS=['https://example.com', 'http://sub.example.com']):\n            middleware = CsrfViewMiddleware()\n            self.assertEqual(middleware.csrf_trusted_origins_hosts, ['example.com', 'sub.example.com'])\n            self.assertEqual(middleware.allowed_origins_exact, {'example.com'})\n            self.assertEqual(middleware.allowed_origin_subdomains, {'https': ['sub.example.com']})\n", "    def test_CSRF_USE_SESSIONS_and_CSRF_COOKIE_NAME_not_set(self):\n        with override_settings(\n            CSRF_USE_SESSIONS=True,\n            CSRF_COOKIE_NAME='some_other_cookie_name',\n        ):\n            with self.assertRaises(ImproperlyConfigured):\n                CsrfViewMiddleware()\n", "    def _test_csrf_cookie_secure_cookie(self):\n        # Test that a secure cookie is set when the HTTPS scheme is used\n        req = TestingHttpRequest()\n        req.is_secure = True\n        req.META = {'HTTP_HOST': 'example.com'}\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_request(req)\n        self.assertEqual(response, None)\n        cookie = req.COOKIES.get(settings.CSRF_COOKIE_NAME)\n        self.assertTrue(cookie.startswith('Secure'))\n"], "sample_866": ["def test_affinity_propagation_damping():\n    \"\"\"Test Affinity Propagation with different damping values.\"\"\"\n    clustering = AffinityPropagation(damping=0.9, max_iter=100, convergence_iter=5)\n    clustering.fit(X)\n    cluster_centers_indices, labels, _ = clustering.cluster_centers_indices_, clustering.labels_, clustering.n_iter_\n    assert cluster_centers_indices.shape[0] == 3\n", "def test_affinity_propagation_convergence_warning_precomputed_affinity():\n    # Create a precomputed affinity matrix with a single cluster\n    n_samples = 10\n    affinity_matrix = np.ones((n_samples, n_samples))\n    affinity_matrix.flat[::(n_samples + 1)] = 0  # Set the diagonal to 0\n\n    # Create an affinity propagation instance with a preference of 0\n    ap = AffinityPropagation(preference=0, affinity=\"precomputed\")\n\n    # Fit the affinity propagation instance with the precomputed affinity matrix\n    with pytest.warns(ConvergenceWarning, match=\"Affinity propagation did not converge\"):\n        ap.fit(affinity_matrix)\n\n    # Check if the warning message is correct\n    assert_warns_message(ConvergenceWarning,\n                         \"Affinity propagation did not converge, this model \"\n                         \"will not have any cluster centers.\",\n                         ap.fit, affinity_matrix)\n\n    # Check that the cluster centers are empty\n    assert ap.cluster_centers_.size == 0\n\n    # Check that all points are labeled as -1\n    assert np.all(ap.labels_ == -1)\n", "def test_affinity_propagation_preferences():\n    # Test with 1 cluster\n    ap = AffinityPropagation(preference=np.median(euclidean_distances(X)))\n    cluster_centers_indices, labels = ap.fit_predict(X)\n    assert len(cluster_centers_indices) == 1\n    assert len(np.unique(labels)) == 1\n\n    # Test with 3 clusters\n    ap = AffinityPropagation(preference=np.max(euclidean_distances(X)))\n    cluster_centers_indices, labels = ap.fit_predict(X)\n    assert len(cluster_centers_indices) == 3\n    assert len(np.unique(labels)) == 3\n\n    # Test with all clusters\n    ap = AffinityPropagation(preference=np.min(euclidean_distances(X)))\n    cluster_centers_indices, labels = ap.fit_predict(X)\n    assert len(cluster_centers_indices) == 3\n    assert len(np.unique(labels)) == 60\n\n    # Test with all clusters with precomputed affinity matrix\n    ap = AffinityPropagation(affinity='precomputed')\n    ap.fit(euclidean_distances(X))\n    cluster_centers_indices, labels = ap.predict(X)\n    assert len(cluster_centers_indices) == 3\n    assert len(np.unique(labels)) == 60\n\n    # Test with warnings\n    ap = AffinityPropagation(preference=np.median(euclidean_distances(X)))\n    with assert_warns(ConvergenceWarning):\n        cluster_centers_indices, labels = ap.fit_predict(X)\n    assert len(cluster_centers_indices) == 1\n    assert len(np.unique(labels)) == 1\n\n    # Test with empty preference array\n    ap = AffinityPropagation(preference=np.array([]))\n    with assert_warns(ConvergenceWarning):\n        cluster_centers_indices, labels = ap.fit_predict(X)\n    assert len(cluster_centers_indices) == 1\n    assert len(np.unique(labels)) == 1\n", "def test_affinity_propagation_max_iter_warning():\n    \"\"\"Test that max_iter is correctly checked when convergence is not reached.\"\"\"\n    ap = AffinityPropagation(max_iter=1, verbose=False)\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    with assert_warns(ConvergenceWarning):\n        ap.fit(X)\n\n", "def test_affinity_propagation_all_equal_similarities():\n    \"\"\"Test that affinity propagation clustering works when all samples have\n    equal similarities and equal preferences.\n\n    We create a small dataset with three points and set the preference to be\n    equal to the maximum similarity, which means that all points are their own\n    cluster centers.\n    \"\"\"\n    from sklearn.cluster import AffinityPropagation\n    n_clusters = 3\n    points = np.array([[1, 1], [1, 1], [1, 1]])\n    ap = AffinityPropagation(preference=1.0)\n    ap.fit(points)\n    labels = ap.labels_\n    assert_array_equal(labels, np.array([0, 0, 0]))\n"], "sample_68": ["    def test_is_active(self):\n        self.assertTrue(get_exception_reporter_filter(None).is_active(None))\n        self.assertFalse(get_exception_reporter_filter(None).is_active(True))\n", "    def test_get_traceback_data(self):\n        # Test normal case\n        request = RequestFactory().get('/test')\n        request.user = User()\n        reporter = ExceptionReporter(request, Exception, Exception(), None)\n        data = reporter.get_traceback_data()\n        self.assertIn('request', data)\n        self.assertIn('user_str', data)\n        self.assertIn('filtered_POST_items', data)\n\n        # Test template debug\n        reporter = ExceptionReporter(request, TemplateDoesNotExist, TemplateDoesNotExist(), None)\n        data = reporter.get_traceback_data()\n        self.assertIn('template_info', data)\n        self.assertIn('template_does_not_exist', data)\n\n        # Test postmortem\n        reporter = ExceptionReporter(request, Exception, Exception().__cause__(), None)\n        data = reporter.get_traceback_data()\n        self.assertIn('postmortem', data)\n\n        # Test unicode error\n        reporter = ExceptionReporter(request, UnicodeError, UnicodeError(), None)\n        data = reporter.get_traceback_data()\n        self.assertIn('unicode_hint', data)\n", "    def test_get_post_parameters_active(self):\n        reporter = ExceptionReporter(None, None, None, None)\n        filter = SafeExceptionReporterFilter()\n        self.assertTrue(filter.is_active(None))\n        post_params = filter.get_post_parameters(None)\n        self.assertEqual(post_params, {})\n", "    def setUp(self):\n        self.request = RequestFactory().get('/')\n", "    def test_is_active(self):\n        self.assertTrue(SafeExceptionReporterFilter().is_active(None))\n        self.assertTrue(SafeExceptionReporterFilter().is_active(False))\n        self.assertFalse(SafeExceptionReporterFilter().is_active(True))\n"], "sample_1160": ["def test_intersection_sets_image_set_interval_real():\n    from sympy.simplify.ratsimp import numer\n\n    a = ImageSet(Lambda(x, x**2), S.Integers)\n    b = Interval(0, 4, left_open=True, right_open=False)\n    expected = Interval(0, 2, left_open=True, right_open=False)\n    assert intersection_sets(a, b) == expected\n\n    a = ImageSet(Lambda(x, x**2), S.Integers)\n    b = Interval(-4, 0, left_open=False, right_open=True)\n    expected = Interval(0, 2, left_open=True, right_open=False)\n    assert intersection_sets(a, b) == expected\n\n    a = ImageSet(Lambda(x, x**2), S.Integers)\n    b = Interval(-4, 0, left_open=True, right_open=True)\n    expected = None\n    assert intersection_sets(a, b) == expected\n\n    a = ImageSet(Lambda(x, x**2), S.Integers)\n    b = Interval(0, 4, left_open=False, right_open=False)\n    expected = Interval(0, 2, left_open=False, right_open=False)\n    assert intersection_sets(a, b) == expected\n\n    a = ImageSet(Lambda(x, x**2), S.Integers)\n    b = Interval(-4, 0, left_open=False, right_open=False)\n    expected = None\n    assert intersection_sets(a, b) == expected\n", "def test_intersection_sets_complex_region():\n    # Test intersection with Real\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    assert intersection_sets(c1, c2).args[0] == Interval(0, 1, left_open=True, right_open=False)\n    assert intersection_sets(c1, c2).args[1] == Interval(0, pi, left_open=True, right_open=False)\n    assert intersection_sets(c2, c1).args[1] == Interval(0, pi, left_open=True, right_open=False)\n    assert intersection_sets(c2, c1).args[0] == Interval(0, 1, left_open=True, right_open=False)\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(2*pi, 3*pi), polar=False)\n    assert intersection_sets(c1, c2) is S.EmptySet\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(2, 3)*Interval(0, pi), polar=False)\n    assert intersection_sets(c1, c2) is S.EmptySet\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(-pi, 0), polar=False)\n    assert intersection_sets(c1, c2) is S.EmptySet\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(2, 3)*Interval(-pi, 0), polar=False)\n    assert intersection_sets(c1, c2) is S.EmptySet\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    c2 = ComplexRegion(Interval(0, 1)*Interval(pi, 2*pi), polar=False)\n    assert intersection_sets(c1, c2) is S.EmptySet\n\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, pi),", "def test_intersection_sets_real_interval_and_complex_interval():\n    from sympy import Interval, ComplexRegion, S\n\n    # Test intersection between real interval and complex interval\n    r1 = Interval(1, 5)\n    c1 = ComplexRegion(Interval(0, 5), Interval(0, pi))\n    result = intersection_sets(r1, c1)\n    assert result == Interval(1, 5)\n\n    # Test intersection between complex interval and real interval\n    c1 = ComplexRegion(Interval(0, 5), Interval(0, pi))\n    r1 = Interval(1, 5)\n    result = intersection_sets(c1, r1)\n    assert result == Interval(1, 5)\n\n    # Test intersection between complex interval and complex interval\n    c1 = ComplexRegion(Interval(0, 5), Interval(0, pi))\n    c2 = ComplexRegion(Interval(3, 7), Interval(pi/2, 3*pi/2))\n    result = intersection_sets(c1, c2)\n    assert result == ComplexRegion(Intersection(Interval(3, 5), Interval(3*pi/2, pi)),\n                                 Intersection(Interval(0, pi), Interval(pi/2, 3*pi/2)))\n\n    # Test intersection with unbounded complex interval\n    c1 = ComplexRegion(Interval(0, 5), Interval(0, pi))\n    c2 = ComplexRegion(Interval(S.NegativeInfinity, S.Infinity), Interval(0, pi))\n    result = intersection_sets(c1, c2)\n    assert result == ComplexRegion(Interval(0, 5), Interval(0, pi))\n\n    # Test intersection with complex interval with unbounded real part\n    c1 = ComplexRegion(Interval(S.NegativeInfinity, S.Infinity), Interval(0, pi))\n    c2 = ComplexRegion(Interval(0, 5), Interval(0, pi))\n    result = intersection_sets(c1, c2)\n    assert result == ComplexRegion(Interval(0, 5), Interval(0, pi))\n\n", "def test_intersection_sets_interval_polar():\n    # Test intersection of an Interval in polar form with a Naturals\n    a = Interval(2, 3, False, True, polar=True)\n    b = Naturals(0)\n    expected = Interval(2, 3, False, True, polar=True)\n    assert intersection_sets(a, b) == expected\n\n    # Test intersection of an Interval in polar form with an Interval\n    a = Interval(2, 3, False, True, polar=True)\n    b = Interval(3, 5, False, True)\n    expected = Interval(3, 3, False, False)\n    assert intersection_sets(a, b) == expected\n\n    # Test intersection of an Interval in polar form with a ComplexRegion\n    a = Interval(2, 3, False, True, polar=True)\n    b = ComplexRegion(Interval(3, 5), Interval(0, pi))\n    expected = ComplexRegion(Interval(3, 3), Interval(0, pi))\n    assert intersection_sets(a, b) == expected\n\n    # Test intersection of an Interval in polar form with an Interval in polar form\n    a = Interval(2, 3, False, True, polar=True)\n    b = Interval(3, 5, False, True, polar=True)\n    expected = ComplexRegion(Interval(3, 3), Interval(0, pi))\n    assert intersection_sets(a, b) == expected\n", "def test_intersection_image_set_and_finite_set():\n    x = symbols('x')\n    i = ImageSet(Lambda(x, x**2), S.Reals)\n    f = FiniteSet(1, 4, 9)\n    result = intersection_sets(i, f)\n    assert result == FiniteSet(1, 4, 9)\n\n    # Test that intersection of ImageSet with FiniteSet returns an empty set if\n    # there are no elements in the FiniteSet that satisfy the condition of the\n    # ImageSet.\n    i = ImageSet(Lambda(x, x**2), S.Reals)\n    f = FiniteSet(2, 3, 5)\n    result = intersection_sets(i, f)\n    assert result == FiniteSet()\n\n    # Test that intersection of ImageSet with an empty FiniteSet returns an empty set\n    i = ImageSet(Lambda(x, x**2), S.Reals)\n    f = FiniteSet()\n    result = intersection_sets(i, f)\n    assert result == FiniteSet()\n\n    # Test that intersection of FiniteSet with an empty ImageSet returns an empty set\n    f = FiniteSet(1, 4, 9)\n    i = ImageSet(Lambda(x, x**2), S.Reals)\n    result = intersection_sets(f, i)\n    assert result == FiniteSet()\n"], "sample_759": ["def test_handle_unknown_warning():\n    \"\"\"Test handle_unknown parameter warning.\"\"\"\n    X = [[1, 2], [3, 4]]\n\n    with ignore_warnings(category=FutureWarning):\n        encoder = OneHotEncoder(handle_unknown='ignore', categories='auto')\n        encoder.fit(X)\n        encoder._handle_deprecations(X)\n    assert_raises_regex(UserWarning, \"The handling of integer data will change in version \"\n                                \"0.22.\", encoder._handle_deprecations, X)\n\n    with ignore_warnings(category=FutureWarning):\n        encoder = OneHotEncoder(handle_unknown='ignore', categories='auto')\n        encoder.fit(X)\n        encoder._handle_deprecations(X)\n    assert_raises_regex(UserWarning, \"The handling of integer data will change in version \"\n                                \"0.22.\\nIf you want the future behaviour and \"\n                                \"silence this warning, you can specify \"\n                                \"\\\"categories='auto'\\\".\\nIn case you used a \"\n                                \"LabelEncoder before this OneHotEncoder to convert \"\n                                \"the categories to integers, then you can now \"\n                                \"use the OneHotEncoder directly.\", encoder._handle_deprecations, X)\n\n    with pytest.raises(ValueError):\n        encoder = OneHotEncoder(handle_unknown='error')\n        encoder.fit(X)\n", "def test_onehot_encoder_sparse_output_shape():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    encoder = OneHotEncoder(sparse=True)\n    X = encoder.fit_transform(X)\n    assert_equal(X.shape, (3, 4))\n", "def test_handle_unknown_error_ordinal_encoder():\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2], ['Unknown', 2]])\n    encoder = OrdinalEncoder()\n    with assert_raises_regex(ValueError, \"Found unknown categories\"):\n        encoder.fit_transform(X)\n", "def test_OrdinalEncoder_inverse_transform_ignores_unknown_categories():\n    enc = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    transformed_X = enc.transform([['Female', 3], ['Male', 1]])\n    # Test that it ignores unknown categories and assigns None to the\n    # corresponding feature in the inverse transform\n    expected_inverse_transformed_X = np.array([[0., 2.], [1., 0.],\n                                              [None, None]])\n    assert_array_equal(enc.inverse_transform(transformed_X), expected_inverse_transformed_X)\n", "def test_transform_unknown_with_fit():\n    \"\"\"Test that transform() raises an error when an unknown category is\n    encountered and handle_unknown='error' and _legacy_mode is False.\"\"\"\n    encoder = OneHotEncoder(handle_unknown='error')\n    X = np.array([['Male', 1], ['Female', 3]])\n    encoder.fit(X)\n    with pytest.raises(ValueError):\n        encoder.transform(np.array([['Male', 1], ['Female', 4]]))\n"], "sample_865": ["def test_tree_base_decision_tree_fit(self):\n    # Test fit method of BaseDecisionTree\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n\n    tree = BaseDecisionTree(criterion=\"gini\", max_depth=2)\n    with pytest.raises(NotFittedError):\n        tree.predict(X)\n\n    tree.fit(X, y)\n    assert tree.get_depth() == 2\n    assert tree.get_n_leaves() == 3\n    assert_array_equal(tree.tree_.feature, np.array([0, 1]))\n    assert_array_equal(tree.tree_.threshold, np.array([2, 4]))\n\n    # Test fit method of BaseDecisionTree with invalid max_depth\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", max_depth=0).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid min_samples_split\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", min_samples_split=1).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid min_samples_leaf\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", min_samples_leaf=0).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid max_features\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", max_features=-1).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid min_weight_fraction_leaf\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", min_weight_fraction_leaf=0.5).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid max_leaf_nodes\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", max_leaf_nodes=-1).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid min_impurity_decrease\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", min_impurity_decrease=-1).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid min_impurity_split\n    with pytest.raises(ValueError):\n        BaseDecisionTree(criterion=\"gini\", min_impurity_split=-1).fit(X, y)\n\n    # Test fit method of BaseDecisionTree with invalid class_weight\n    with", "def test_pruning_path_zero_ccp_alpha():\n    # Create a DecisionTreeClassifier with ccp_alpha == 0.0\n    clf = DecisionTreeClassifier(ccp_alpha=0.0)\n    X, y = DATASETS[\"clf_small\"][\"X\"], DATASETS[\"clf_small\"][\"y\"]\n    clf.fit(X, y)\n\n    # Compute the pruning path\n    ccp_path = clf.cost_complexity_pruning_path(X, y)\n\n    # Check that ccp_alphas and impurities are empty\n    assert_array_equal(ccp_path.ccp_alphas, [])\n    assert_array_equal(ccp_path.impurities, [])\n", "def test_minimal_cost_complexity_pruning_clf(criterion):\n    X = DATASETS[\"clf_small\"][\"X\"]\n    y = DATASETS[\"clf_small\"][\"y\"]\n    tree = DecisionTreeClassifier(criterion=criterion)\n    tree.fit(X, y)\n    X_pruned, tree_pruned = tree.cost_complexity_pruning_path(X, y)\n    assert_almost_equal(tree_pruned.tree_.node_count, tree.tree_.node_count)\n    assert_almost_equal(tree_pruned.tree_.impurity, tree.tree_.impurity)\n    assert_array_equal(tree_pruned.tree_.n_node_samples, tree.tree_.n_node_samples)\n    assert_array_equal(tree_pruned.tree_.children_left, tree.tree_.children_left)\n    assert_array_equal(tree_pruned.tree_.children_right, tree.tree_.children_right)\n    assert_array_equal(tree_pruned.tree_.feature, tree.tree_.feature)\n    assert_array_equal(tree_pruned.tree_.threshold, tree.tree_.threshold)\n    assert_array_almost_equal(tree_pruned.tree_.value, tree.tree_.value)\n    assert_almost_equal(tree_pruned.tree_.n_node_samples.sum(), tree.tree_.n_node_samples.sum())\n\n", "def test_DecisionTreeRegressor_min_impurity_decrease():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([2, 4])\n    clf = DecisionTreeRegressor(min_impurity_decrease=0.2, random_state=42)\n    clf.fit(X, y)\n\n    # The model should not have pruned the tree\n    assert clf.tree_.node_count > 2\n    assert_tree_equal(clf.tree_, DecisionTreeRegressor(tree_.get_tree(),\n                                                     random_state=42).tree_,\n                      \"DecisionTreeRegressor with min_impurity_decrease\")\n\n    # The model should have a non-zero value for the feature importances\n    assert np.all(clf.feature_importances_ > 0)\n\n", "def test_tree_equality():\n    # test equal trees\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y)\n\n    t = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"])\n    assert t == \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y <= 3.\\n\\tprediction: 2\"\n\n    d = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"], max_depth=0)\n    assert d == \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y <= 3.\\n\\tprediction: 2\"\n\n    d = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"], max_depth=1)\n    assert d == \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y <= 3.\\n\\tprediction: 2\"\n\n    d = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"], max_depth=2)\n    assert d == \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y <= 3.\\n\\tprediction: 2\"\n\n    # test unequal trees\n    clf.fit(X, y, random_state=42)\n    t = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"])\n    assert t != \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y <= 3.\\n\\tprediction: 2\"\n\n    # test trees with categorical features\n    X = np.array([[1, \"a\"], [2, \"b\"]])\n    y = np.array([1, 2])\n    clf = DecisionTreeClassifier()\n    clf.fit(X, y)\n\n    t = tree.export_text(clf.tree_, feature_names=[\"x\", \"y\"])\n    assert t == \"feature: x <= 2.\\n\\tprediction: 1\\nfeature: y == b.\\n\\tprediction: 2\"\n\n    # test trees with missing values\n    X = np.array([[1, 2], [np.nan, 4]])\n    y = np.array([1, 2"], "sample_155": ["    def test_http_response_init(self):\n        response = HttpResponse()\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.reason_phrase, 'OK')\n        self.assertEqual(response.charset, 'utf-8')\n", "    def test_reason_phrase(self):\n        response = HttpResponse()\n        self.assertEqual(response.reason_phrase, 'OK')\n        response.status_code = 500\n        self.assertEqual(response.reason_phrase, 'Internal Server Error')\n        response.reason_phrase = 'Custom Reason Phrase'\n        self.assertEqual(response.reason_phrase, 'Custom Reason Phrase')\n", "    def test_headers(self):\n        response = HttpResponse()\n        response['Content-Type'] = 'text/plain'\n        response['X-Custom-Header'] = 'custom value'\n        self.assertEqual(response['Content-Type'], 'text/plain')\n        self.assertEqual(response['x-custom-header'], 'custom value')\n        self.assertEqual(response['X-Invalid-Header'], None)\n        self.assertIn('X-Custom-Header', response._headers)\n        self.assertEqual(len(response._headers), 2)\n        response._headers['X-Custom-Header'] = None\n        self.assertEqual(len(response._headers), 1)\n", "    def test_streaming_content_iterable_empty(self):\n        response = StreamingHttpResponse()\n        self.assertEqual(list(response.streaming_content), [])\n", "    def test_repr_includes_status_code_content_type_and_url(self):\n        response = HttpResponse(status=404, reason='Not Found', url='http://example.com')\n        expected = '<HttpResponse status_code=404, url=\"http://example.com\">'\n        self.assertEqual(repr(response), expected)\n"], "sample_776": ["def test_lars_path_residues_Xy(X, y):\n    Xy = np.dot(X.T, y)\n    result = _lars_path_residues(X, y, X, y, Xy=Xy, method='lar')\n    assert_array_almost_equal(result[0], result[0])\n    assert_array_almost_equal(result[1], result[1])\n    assert_array_almost_equal(result[2], result[2])\n    assert_array_almost_equal(result[3], result[3])\n", "def test_LassoLarsIC_fit():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    model = LassoLarsIC(criterion='bic')\n    model.fit(X, y)\n    assert_equal(model.n_iter_, 3)\n    assert_equal(model.coef_.shape, (2,))\n    assert_equal(model.intercept_, None)\n    assert_greater(model.alpha_, 0)\n    assert_equal(model.criterion_, None)\n", "def test_lars_cv_n_jobs():\n    X, y = diabetes.data, diabetes.target\n    lars_cv = linear_model.LarsCV(n_jobs=2)\n    with ignore_warnings(category=ConvergenceWarning):\n        lars_cv.fit(X, y)\n    assert lars_cv.n_jobs == 2\n", "def test_LassoLarsCV_predict():\n    \"\"\"Test the prediction of LassoLarsCV.\"\"\"\n    n_samples = 100\n    n_features = 10\n    n_targets = 1\n    np.random.seed(0)\n    X = np.random.rand(n_samples, n_features)\n    y = np.random.rand(n_samples)\n\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n\n    # Check that the prediction is correct\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with n_jobs = -1\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   n_jobs=-1)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with n_jobs = None\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   n_jobs=None)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with verbose = True\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   verbose=True)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with max_iter = 1000\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   max_iter=1000)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with precompute = True\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   precompute=True)\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np.abs(pred.mean()), 1e-2)\n\n    # Test with precompute = 'auto'\n    reg = linear_model.LassoLarsCV(cv=2, fit_intercept=False, normalize=False,\n                                   precompute='auto')\n    reg.fit(X, y)\n    pred = reg.predict(X)\n    assert_less(np", "def test_lars_path_non_zero_alpha_min():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([2, 4])\n    with ignore_warnings(category=DeprecationWarning):\n        alphas, active, coefs = lars_path(X, y, alpha_min=1.0)\n        assert alphas[-1] == 0\n        assert np.all(active == [0, 1])\n        assert np.all(coefs == np.zeros((1, 2)))\n"], "sample_216": ["    def test_field_references_through(self):\n        book = ModelState(\"otherapp\", \"Book\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"title\", models.CharField(max_length=200)),\n        ])\n        through = ModelState(\"otherapp\", \"BookAuthor\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n        ])\n        self.assertTrue(field_references((\"otherapp\", \"Book\"), book.fields[\"author\"], (\"otherapp\", \"BookAuthor\")))\n", "    def test_field_references_through(self):\n        # Create a model with a field that references another model through a through model\n        through_model = ModelState(\"testapp\", \"ThroughModel\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n            (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n        ])\n        author_model = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"books\", models.ManyToManyField(\"otherapp.Book\", through=through_model)),\n        ])\n        # Test that field_references returns a FieldReference when the field references the provided context\n        self.assertTrue(field_references(resolve_relation(through_model, \"testapp\", \"ThroughModel\"), through_model._meta.get_field(\"author\"), resolve_relation(author_model, \"testapp\", \"Author\")))\n        # Test that field_references returns False when the field does not reference the provided context\n        self.assertFalse(field_references(resolve_relation(through_model, \"testapp\", \"ThroughModel\"), through_model._meta.get_field(\"id\"), resolve_relation(author_model, \"testapp\", \"Author\")))\n", "def test_field_references_through(self):\n    # Test that `field_references` returns `False` when there's no through model.\n    self.assertFalse(field_references((\"testapp\", \"Author\"), models.ForeignKey(\"testapp.Publisher\", models.CASCADE), (\"testapp\", \"Publisher\")))\n\n    # Test that `field_references` returns `FieldReference` when there's a through model.\n    reference = field_references((\"testapp\", \"Author\"), models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"), (\"testapp\", \"Publisher\"))\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, None)\n    self.assertEqual(reference.through, (models.ForeignKey(\"testapp.Publisher\", models.CASCADE), [\"author_id\", \"publisher_id\"]))\n\n    # Test that `field_references` returns `False` when the through model is not related.\n    self.assertFalse(field_references((\"testapp\", \"Author\"), models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Deal\"), (\"testapp\", \"Publisher\")))\n\n    # Test that `field_references` returns `FieldReference` when the through model is related and the field is specified.\n    reference = field_references((\"testapp\", \"Author\"), models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\"), (\"testapp\", \"Contract\"), \"author_id\")\n    self.assertIsInstance(reference, FieldReference)\n    self.assertEqual(reference.to, (models.ForeignKey(\"testapp.Publisher\", models.CASCADE), [\"publisher_id\"]))\n    self.assertEqual(reference.through, (models.ForeignKey(\"testapp.Author\", models.CASCADE), [\"author_id\", \"publisher_id\"]))\n", "def test_field_references_m2m_through(self):\n    \"\"\"\n    Test if field_references handles ManyToManyField through models correctly.\n    \"\"\"\n    m2m_model = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"publishers\", models.ManyToManyField(\"testapp.Publisher\", through=\"testapp.Contract\")),\n    ])\n    field = m2m_model.fields[\"publishers\"]\n    reference_model_tuple = (\"testapp\", \"Contract\")\n    self.assertTrue(field_references((\"testapp\", \"Author\"), field, reference_model_tuple))\n    self.assertEqual(field_references((\"testapp\", \"Author\"), field, reference_model_tuple).to, (field.remote_field, [field.remote_field.through_fields[0]]))\n    self.assertEqual(field_references((\"testapp\", \"Author\"), field, reference_model_tuple).through, (field.remote_field, field.remote_field.through_fields))\n", "    def test_field_references_proxy(self):\n        state = self.make_project_state([\n            self.author_proxy_metadata,\n            self.author,\n        ])\n        self.assertTrue(field_is_referenced(state, (\"testapp\", \"Author\"), ()))\n        self.assertTrue(field_is_referenced(state, (\"testapp\", \"Author\"), (\"id\",)))\n"], "sample_957": ["def test_restify_py37_special_forms():\n    # Test restify for py37 SpecialForm\n    assert restify(MyList) == ':class:`typing.List`'\n    assert restify(MyInt) == ':obj:`~typing.NewType`'\n    assert restify(T) == ':obj:`~typing.TypeVar`'\n", "def test_stringify_NewType():\n    \"\"\"Test stringify() with NewType.\"\"\"\n    MyInt = NewType('MyInt', int)\n    assert stringify(MyInt) == 'MyInt'\n\n", "def test_restify_NewType():\n    assert restify(MyInt) == ':obj:`MyInt`'\n    assert restify(NewType('MyInt', int)) == ':obj:`NewType`'\n    assert restify(NewType('MyInt', str)) == ':obj:`NewType`'\n    assert restify(NewType('MyInt', int, module='custom_module')) == ':obj:`custom_module.MyInt`'\n", "def test_typing_restify_stringify_generics():\n    from sphinx.util.typing import restify, stringify\n\n    # Test Generic\n    GenericClass = Generic[str, int]\n    assert restify(GenericClass) == ':class:`~typing.Generic`[str, int]'\n    assert stringify(GenericClass) == 'Generic[str, int]'\n\n    # Test Generic with Union as origin\n    UnionClass = Union[str, int]\n    assert restify(UnionClass) == ':obj:`~typing.Union`[str, int]'\n    assert stringify(UnionClass) == 'Union[str, int]'\n\n    # Test Generic with Generic as origin\n    GenericGenericClass = Generic[GenericClass]\n    assert restify(GenericGenericClass) == ':class:`~typing.Generic`[~typing.Generic[str, int]]'\n    assert stringify(GenericGenericClass) == 'Generic[Generic[str, int]]'\n\n    # Test Generic with ellipsis as one of the type parameters\n    EllipsisGenericClass = Generic[int, ...]\n    assert restify(EllipsisGenericClass) == ':class:`~typing.Generic`[int, ...]'\n    assert stringify(EllipsisGenericClass) == 'Generic[int, ...]'\n\n    # Test Generic with Invalid type parameters\n    BrokenGenericClass = Generic[BrokenType]\n    assert restify(BrokenGenericClass) == ':class:`~typing.Generic`[~typing.BrokenType]'\n    assert stringify(BrokenGenericClass) == 'Generic[BrokenType]'\n\n    # Test Generic with TypeVar as type parameters\n    T = TypeVar('T')\n    GenericClassWithT = Generic[T]\n    assert restify(GenericClassWithT) == ':class:`~typing.Generic`[~typing.TypeVar(``T``)]'\n    assert stringify(GenericClassWithT) == 'Generic[TypeVar(``T``)]'\n\n    # Test Generic with TypeVar and Union as type parameters\n    UnionT = Union[T, int]\n    GenericClassWithUnionT = Generic[UnionT]\n    assert restify(GenericClassWithUnionT) == ':class:`~typing.Generic`[~typing.Union[~typing.TypeVar(``T``), int]]'\n    assert stringify(GenericClassWithUnionT) == 'Generic[Union[TypeVar(``T``), int]]'\n", "def test_restify_invalid_builtin_class():\n    \"\"\"Check restify() with invalid builtin class.\"\"\"\n    # Test 1: Struct\n    assert restify(Struct) == ':class:`struct.Struct`'\n\n    # Test 2: TracebackType\n    assert restify(TracebackType) == ':class:`types.TracebackType`'\n\n    # Test 3: Invalid class with __module__ None\n    class InvalidClass:\n        pass\n    InvalidClass.__module__ = None\n    assert restify(InvalidClass) == ':obj:`InvalidClass`'\n"], "sample_847": ["def test_l1_ratio_validation():\n    \"\"\"Check that l1_ratio is validated in ElasticNetCV.\"\"\"\n    # Test a valid l1_ratio\n    model = ElasticNetCV(l1_ratio=0.5)\n    # Test a l1_ratio outside of the valid range\n    with pytest.raises(ValueError):\n        ElasticNetCV(l1_ratio=1.1)\n    with pytest.raises(ValueError):\n        ElasticNetCV(l1_ratio=-0.1)\n    # Test a list of l1_ratio values\n    model = ElasticNetCV(l1_ratio=[0.1, 0.5, 0.9])\n    # Test a list of l1_ratio values with non-numeric elements\n    with pytest.raises(TypeError):\n        ElasticNetCV(l1_ratio=[0.1, 0.5, 'string'])\n    # Test that l1_ratio is not validated in ElasticNet\n    model = ElasticNet(l1_ratio=0.5)\n    # Test that l1_ratio is validated in MultiTaskElasticNetCV\n    model = MultiTaskElasticNetCV(l1_ratio=0.5)\n    # Test that l1_ratio is not validated in MultiTaskElasticNet\n    model = MultiTaskElasticNet(l1_ratio=0.5)\n", "def test_lasso_path_approximation():\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 10\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples)\n    model = Lasso(alpha=0.1)\n    alphas, coefs, _ = lasso_path(X, y)\n    approx_path = np.zeros_like(coefs)\n    for i in range(len(alphas)):\n        model.alpha = alphas[i]\n        model.fit(X, y)\n        approx_path[:, i] = model.coef_\n    assert_almost_equal(coefs, approx_path)\n", "def test_lasso_path_duplicate_alphas(eps):\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    l1_ratio = 1.0\n    n_alphas = 100\n    alphas = np.array([0.01, 0.1, 0.5, 0.9, 0.99, 1])\n    alphas_duplicate = np.sort(alphas)[::-1]\n    np.testing.assert_array_equal(lasso_path(X, y, eps=eps, n_alphas=n_alphas,\n                                           alphas=alphas, precompute='auto',\n                                           Xy=None, copy_X=True,\n                                           coef_init=None, verbose=False,\n                                           return_n_iter=False, positive=False, **{'X_offset': 0, 'X_scale': 1})['alphas'],\n                                 alphas_duplicate)\n    np.testing.assert_array_almost_equal(lasso_path(X, y, eps=eps, n_alphas=n_alphas,\n                                                  alphas=alphas_duplicate,\n                                                  precompute='auto', Xy=None,\n                                                  copy_X=True, coef_init=None,\n                                                  verbose=False, return_n_iter=False, positive=False, **{'X_offset': 0, 'X_scale': 1})['coefs'],\n                                 lasso_path(X, y, eps=eps, n_alphas=n_alphas,\n                                            alphas=alphas_duplicate, precompute='auto',\n                                            Xy=None, copy_X=True, coef_init=None,\n                                            verbose=False, return_n_iter=False,\n                                            positive=False, **{'X_offset': 0, 'X_scale': 1})['coefs'])\n    np.testing.assert_array_almost_equal(lasso_path(X, y, eps=eps, n_alphas=n_alphas,\n                                                  alphas=alphas_duplicate,\n                                                  precompute='auto', Xy=None,\n                                                  copy_X=True, coef_init=None,\n                                                  verbose=False, return_n_iter=False, positive=False, **{'X_offset': 0, 'X_scale': 1})['dual_gaps'],\n                                 lasso_path(X, y, eps=eps, n_alphas=n_alphas,\n                                            alphas=alphas_duplicate, precompute='auto',\n                                            Xy=None, copy_X=True, coef_init=None,\n                                            verbose=False, return_n_iter", "    def test_default_settings(self):\n        X, y = load_boston(return_X_y=True)\n        with ignore_warnings(category=ConvergenceWarning):\n            model = MultiTaskLassoCV()\n            model.fit(X, y)\n            assert_array_almost_equal(model.coef_.shape, (y.shape[1], X.shape[1]))\n            assert_array_almost_equal(model.intercept_.shape, (y.shape[1],))\n            assert_almost_equal(model.alpha_, 0.0352)\n            assert_almost_equal(model.n_iter_, 100)\n", "def test_lars_path():\n    X, y = load_boston(return_X_y=True)\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', verbose=0)\n    assert coefs_lars.shape == (100, X.shape[1])\n    # Test that lars_path returns a valid array\n    assert_array_almost_equal(np.sum(coefs_lars, axis=0), 0, decimal=5)\n    # Check that the same path is returned when giving a specific list of alphas\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', alphas=coefs_lars[:, 0])\n    assert_array_almost_equal(coefs_lars, lars_path(X, y, method='lasso')[1])\n    # Check that we get the same path when using a higher or lower value of eps\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', eps=1e-4)\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', eps=1e-5)\n    assert_array_almost_equal(coefs_lars, lars_path(X, y, method='lasso')[1])\n    # Test that we get the same path when setting a different number of alphas\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', n_alphas=50)\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', n_alphas=100)\n    assert_array_almost_equal(coefs_lars, lars_path(X, y, method='lasso')[1])\n    # Test that we get the same path when setting a different list of alphas\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', alphas=coefs_lars[:, 0])\n    _, coefs_lars, _ = lars_path(X, y, method='lasso', alphas=np.linspace(0, 1, 100))\n    assert_array_almost_equal(coefs_lars, lars_path(X, y, method='lasso')[1])\n    # Check that we get the same path when giving a specific list of alphas and n_alphas\n    _, coefs_lars, _ = lars_path(X, y"], "sample_1183": ["def test_fractions():\n    from sympy.polys.domains import ZZ\n    from sympy import sin\n\n    K, x, y = field(\"x,y\", ZZ)\n\n    f = (x + y) / (x - y)\n    assert f.is_rational_function() == True\n    assert f.is_polynomial() == False\n\n    g = f.subs(x, y)\n    assert g.is_rational_function() == True\n    assert g.is_polynomial() == False\n\n    h = g.subs(y, x)\n    assert h.is_rational_function() == True\n    assert h.is_polynomial() == False\n\n    i = f * g\n    assert i.is_rational_function() == True\n    assert i.is_polynomial() == False\n\n    j = f / g\n    assert j.is_rational_function() == True\n    assert j.is_polynomial() == False\n\n    k = sin(f)\n    assert k.is_rational_function() == True\n    assert k.is_polynomial() == False\n\n    l = sin(g)\n    assert l.is_rational_function() == True\n    assert l.is_polynomial() == False\n\n    m = sin(h)\n    assert m.is_rational_function() == True\n    assert m.is_polynomial() == False\n\n    n = sin(i)\n    assert n.is_rational_function() == True\n    assert n.is_polynomial() == False\n\n    o = sin(j)\n    assert o.is_rational_function() == True\n    assert o.is_polynomial() == False\n\n    p = sin(k)\n    assert p.is_rational_function() == True\n    assert p.is_polynomial() == False\n\n    q = sin(l)\n    assert q.is_rational_function() == True\n    assert q.is_polynomial() == False\n\n    r = sin(m)\n    assert r.is_rational_function() == True\n    assert r.is_polynomial() == False\n\n    s = sin(n)\n    assert s.is_rational_function() == True\n    assert s.is_polynomial() == False\n\n    t = sin(o)\n    assert t.is_rational_function() == True\n    assert t.is_polynomial() == False\n\n    u = sin(p)\n    assert u.is_rational_function() == True\n    assert u.is_polynomial() == False\n\n    v = sin(q)\n    assert v.is_rational_function() == True\n    assert v.is", "    def test_field_new(self):\n        # Test FracField(field_new) with various inputs\n        K = FracField(QQ, order=lex)\n        self.assertIsInstance(K.field_new(1), FracElement)\n        self.assertIsInstance(K.field_new(1/2), FracElement)\n        self.assertIsInstance(K.field_new(QQ(3)), FracElement)\n        self.assertIsInstance(K.field_new(QQ(3)/2), FracElement)\n\n        with raises(CoercionFailed):\n            K.field_new('a')\n\n        with raises(CoercionFailed):\n            K.field_new(1.0)\n", "def test_fric_element_substituition(self):\n    # Test FracElement substitution\n    ZZ = ZZ_gmpy if HAS_GMPY else ZZ_python\n    f, g, h = field(\"x,y,z\", ZZ)\n    x, y, z = f.gens\n\n    frac1 = f.new(x**2 + y**2, x**2 - y**2)\n    frac2 = f.new(y**2 + z**2, y**2 - z**2)\n\n    result1 = frac1.subs(x, y)\n    result2 = frac1.subs(y, z)\n\n    self.assertEqual(result1, f.new(y**2 + y**2, y**2 - y**2))\n    self.assertEqual(result2, f.new(x**2 + z**2, x**2 - z**2))\n\n    result = frac1.subs(x, y).subs(y, z)\n    expected = f.new(z**2 + z**2, z**2 - z**2)\n    self.assertEqual(result, expected)\n\n    # Test substitution with a list of pairs\n    result = frac1.subs([(x, y), (y, z)])\n    self.assertEqual(result, f.new(y**2 + y**2, y**2 - y**2))\n\n    # Test substitution with a dictionary\n    result = frac1.subs({x: y, y: z})\n    self.assertEqual(result, f.new(y**2 + y**2, y**2 - y**2))\n\n    # Test substitution with a single value\n    result = frac1.subs(x, y)\n    self.assertEqual(result, f.new(y**2 + y**2, y**2 - y**2))\n\n    # Test substitution with a value that doesn't exist in the expression\n    result = frac1.subs(z, 1)\n    self.assertEqual(result, f.new(x**2 + y**2, x**2 - y**2))\n\n    # Test substituition of FracElement with a PolyElement\n    frac3 = f.new(x + y, x**2 - y**2)\n\n    with self.assertRaises(CoercionFailed):\n        frac1.subs(x, frac3)\n\n    # Test substituition of FracElement with a FracElement from a different field\n    g = field(\"u,v\", ZZ)\n    u, v = g.gens\n    frac4 = g.new(u**2", "def test_fractions():\n    from sympy import symbols\n    from sympy.polys.rings import PolyRing\n    from sympy.polys.domains import ZZ\n\n    _, x, y = symbols('x,y')\n    K, x, y = field('x,y', ZZ)\n    field_new = K.new\n\n    # Test addition\n    f1 = field_new(2*x**2, x**2)\n    f2 = field_new(x + 2*y, 3*x)\n    assert (f1 + f2).as_expr() == (2*x**2*(3*x)/(x**2) + (x + 2*y)/(3*x)*(x**2))\n\n    # Test subtraction\n    f1 = field_new(2*x**2, x**2)\n    f2 = field_new(x + 2*y, 3*x)\n    assert (f1 - f2).as_expr() == (2*x**2*(3*x)/(x**2) - (x + 2*y)/(3*x)*(x**2))\n\n    # Test multiplication\n    f1 = field_new(2*x**2, x**2)\n    f2 = field_new(x + 2*y, 3*x)\n    assert (f1 * f2).as_expr() == (2*x**2*(x + 2*y)/(3*x))\n\n    # Test division\n    f1 = field_new(2*x**2, x**2)\n    f2 = field_new(x + 2*y, 3*x)\n    assert (f1 / f2).as_expr() == (2*x**2*(3*x)/(x**2) / (x + 2*y)/(3*x))\n\n    # Test power\n    f = field_new(x + 2*y, x**2)\n    assert f**2.as_expr() == ((x + 2*y)**2)/(x**4)\n\n    # Test different order of elements\n    f = field_new(y, x)\n    g = field_new(x, y)\n    assert f.as_expr() == y/x\n    assert g.as_expr() == x/y\n\n    # Test variable substitution\n    f = field_new(2*x, x**2)\n    assert f.subs(x, x+1).as_expr() == 2*(x + 1)/((x + 1)**2)\n\n    # Test polynomial evaluation\n    f", "def test_unify_poly_ring_to_field():\n    \"\"\"Test unify_poly_ring_to_field. \"\"\"\n\n    ZZ_gmpy = ZZ_gmpy if HAS_GMPY else ZZ_python\n\n    ZZ, x, y = field(\"x,y\", ZZ)\n    Q, x, y = field(\"x,y\", QQ)\n    Qx, x, y = field(\"x,y\", QQ, order=grlex)\n\n    assert unify(ZZx, Q).field == Q\n    assert unify(Qx, ZZ).field == Q\n\n    ZZx = x*ZZ\n    R = ZZx.to_ring()\n    assert unify(ZZx, R).field == Q\n    assert unify(R, ZZx).field == Q\n\n    _, x, y, z = field(\"x,y,z\", QQ)\n    assert unify(ZZ, ZZ).field == ZZ\n    assert unify(ZZ, Q).field == Q\n    assert unify(Q, ZZ).field == Q\n    assert unify(Q, Q).field == Q\n\n    _, x, y, z = field(\"x,y,z\", ZZ)\n    assert unify(Q, ZZ).field == Q\n    assert unify(ZZ, Q).field == Q\n\n    assert unify(Q, ZZ).field == Q\n    assert unify(ZZ, Q).field == Q\n\n    # This is a regression test, see issue 15385\n    ZZx, x, y = field(\"x,y\", ZZ)\n    ZZx_y, x, y = field(\"x,y\", ZZ)\n\n    assert unify(ZZx, ZZx_y).field == ZZx.field\n    assert unify(ZZx_y, ZZx).field == ZZx.field\n"], "sample_16": ["    def test_shape(self):\n        self.check(np.shape)\n", "    def test_shape(self):\n        o = np.shape(self.q)\n        expected = np.shape(self.q.value) * self.q.unit\n        assert o.shape == expected.shape\n        assert np.all(o == expected)\n", "    def test_shape(self):\n        out = np.shape(self.q)\n        expected = np.shape(self.q.value)\n        assert out == expected\n", "    def setup_method(self):\n        self.q = np.arange(9.0).reshape(3, 3) / 4.0 * u.m\n", "    def test_merge_arrays(self):\n        # Test that merge_arrays can merge two structured quantities with different units.\n        # Note that this is not the most comprehensive test of merge_arrays, but it covers one of the main use cases.\n        # We'll need to add more tests if this passes.\n        from astropy.units.quantity_helper.function_helpers import merge_arrays\n        from astropy.units.quantity import Quantity\n        from astropy.units import StructuredUnit\n\n        # Define two structured units\n        u1 = StructuredUnit({'field1': u.m, 'field2': u.s})\n        u2 = StructuredUnit({'field1': u.km, 'field3': u.Hz})\n\n        # Create two quantities with the structured units\n        q1 = Quantity([(1, 2), (3, 4)], u1)\n        q2 = Quantity([(5, 6), (7, 8)], u2)\n\n        # Merge the quantities\n        q_out = merge_arrays((q1, q2))\n\n        # The merged quantity should have a structured unit with the fields from both original units\n        expected_unit = StructuredUnit({'field1': u.m, 'field2': u.s, 'field3': u.Hz})\n        assert q_out.unit == expected_unit\n\n        # The values of the merged quantity should be correct\n        expected_values = [(1, 2), (3, 4), (5, 6), (7, 8)]\n        assert q_out.value.tolist() == expected_values\n"], "sample_90": ["    def test_formfield_callback(self):\n        class MyModel(models.Model):\n            field = models.CharField(max_length=10)\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = MyModel\n                fields = ('field',)\n                formfield_callback = lambda f: forms.CharField(max_length=20)\n\n        form = MyForm(data={'field': 'value'})\n        self.assertEqual(form.fields['field'].widget.attrs['maxlength'], 20)\n", "    def test_modelform_factory_with_all_fields(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n            description = models.TextField()\n\n        ModelForm = modelform_factory(model=TestModel, fields=['name', 'description'])\n        self.assertTrue(modelform_defines_fields(ModelForm))\n", "    def test_model_form_with_empty_fields(self):\n        class EmptyForm(forms.ModelForm):\n            class Meta:\n                model = models.Model\n                fields = ()\n\n        form = EmptyForm()\n        self.assertEqual(form.fields, {})\n        self.assertEqual(form._meta.fields, ())\n", "    def test_construct_instance_instance_none(self):\n        form = BookForm({'title': 'Test book', 'author': 'Test author'})\n        instance = construct_instance(form, None)\n        self.assertIsNone(instance.pk)\n", "    def test_formfield_callback_default(self):\n        form = modelform_factory(model=Article, fields='__all__')\n        form_fields = form.base_fields\n        self.assertIn('id', form_fields)\n        self.assertIn('title', form_fields)\n        self.assertIn('content', form_fields)\n"], "sample_307": ["    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.datetime = datetime.now()\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.utc_time = datetime(2003, 10, 7, 11, 39, tzinfo=timezone.utc)\n        self.local_time = datetime(2003, 10, 7, 11, 39, tzinfo=get_default_timezone())\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.now = make_aware(datetime.now(get_default_timezone()), get_default_timezone())\n        self.datetime = self.now\n"], "sample_402": ["    def test_should_redirect_with_slash_append_slash_false(self):\n        request = self.rf.get('/')\n        self.assertFalse(CommonMiddleware().should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_false(self):\n        request = self.rf.get(\"/test\")\n        middleware = CommonMiddleware()\n        self.assertFalse(middleware.should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash_trailing_slash(self):\n        request = self.rf.get(\"/test/\")\n        self.assertTrue(CommonMiddleware().should_redirect_with_slash(request))\n", "    def test_prepend_www_redirect(self):\n        request = self.rf.get(\"/path\")\n        request.META[\"HTTP_HOST\"] = \"example.com\"\n        middleware = CommonMiddleware()\n        response = middleware.process_request(request)\n        self.assertEqual(response.url, \"http://www.example.com/path/\")\n", "    def test_should_redirect_with_slash_false(self):\n        request = self.rf.get('/')\n        self.assertFalse(self.rf.common_middleware.should_redirect_with_slash(request))\n"], "sample_1021": ["def test_from_axis_angle():\n    # Test that from_axis_angle returns a normalized quaternion\n    from sympy.algebras.quaternion import Quaternion\n    from sympy import symbols, pi, sqrt\n    q = Quaternion.from_axis_angle((sqrt(3)/3, sqrt(3)/3, sqrt(3)/3), 2*pi/3)\n    assert q.norm() == 1\n\n    # Test that from_axis_angle returns the correct quaternion for a vector and angle\n    q = Quaternion.from_axis_angle((1, 0, 0), pi/2)\n    assert q == (0 + 1*i + 0*j + 0*k)\n\n    # Test that from_axis_angle raises an error for a zero vector\n    from sympy.utilities.pytest import raises\n    raises(ValueError, lambda: Quaternion.from_axis_angle((0, 0, 0), 1))\n\n    # Test that from_axis_angle returns the correct quaternion for an angle of pi\n    q = Quaternion.from_axis_angle((1, 0, 0), pi)\n    assert q == (-1 + 0*i + 0*j + 0*k)\n\n    # Test that from_axis_angle returns the correct quaternion for an angle of -pi\n    q = Quaternion.from_axis_angle((1, 0, 0), -pi)\n    assert q == (-1 + 0*i + 0*j + 0*k)\n", "def test_exp():\n    # Test exp(q) for q = 1\n    q = Quaternion(1, 0, 0, 0)\n    expected = E\n    assert trigsimp(q.exp()) == expected\n\n    # Test exp(q) for q = i\n    q = Quaternion(0, 1, 0, 0)\n    expected = I * E\n    assert trigsimp(q.exp()) == expected\n\n    # Test exp(q) for q = j\n    q = Quaternion(0, 0, 1, 0)\n    expected = E * j\n    assert trigsimp(q.exp()) == expected\n\n    # Test exp(q) for q = k\n    q = Quaternion(0, 0, 0, 1)\n    expected = I * E\n    assert trigsimp(q.exp()) == expected\n\n    # Test exp(q) for a non-real quaternion\n    q = Quaternion(1, 2, 3, 4)\n    expected = (E*cos(sqrt(29)) + 2*sqrt(29)*E*sin(sqrt(29))/29*I +\n                3*sqrt(29)*E*sin(sqrt(29))/29*j +\n                4*sqrt(29)*E*sin(sqrt(29))/29*k)\n    assert trigsimp(q.exp()) == expected\n\n    # Test exp(q) for a non-real quaternion with real_field=False\n    q = Quaternion(1, 2, 3, 4, real_field=False)\n    expected = (E*cos(sqrt(29)) + 2*sqrt(29)*E*sin(sqrt(29))/29*I +\n                3*sqrt(29)*E*sin(sqrt(29))/29*j +\n                4*sqrt(29)*E*sin(sqrt(29))/29*k)\n    assert trigsimp(q.exp()) == expected\n", "def test_to_axis_angle():\n    q = Quaternion(0, 0, 0, 0)\n    assert q.to_axis_angle() == ((0, 0, 0), 0)\n\n    q = Quaternion(1, 0, 0, 0)\n    assert q.to_axis_angle() == ((1, 0, 0), 0)\n\n    q = Quaternion(0, 1, 0, 0)\n    assert q.to_axis_angle() == ((0, 1, 0), 0)\n\n    q = Quaternion(0, 0, 1, 0)\n    assert q.to_axis_angle() == ((0, 0, 1), 0)\n\n    q = Quaternion(0, 0, 0, 1)\n    assert q.to_axis_angle() == ((0, 0, 0), 0)\n\n    q = Quaternion(0.5, 0.5, 0.5, 0.5)\n    assert q.to_axis_angle() == ((1/sqrt(3), 1/sqrt(3), 1/sqrt(3)), pi/3)\n\n    q = Quaternion(1, 1, 0, 0)\n    assert q.to_axis_angle() == ((-1/sqrt(2), 1/sqrt(2), 0), pi/4)\n\n    q = Quaternion(1, 0, 0, 1)\n    assert q.to_axis_angle() == ((0, 0, 1), pi/2)\n\n    q = Quaternion(-1, 0, 0, 1)\n    assert q.to_axis_angle() == ((0, 0, 1), -pi/2)\n\n    q = Quaternion(-1, 0, 0, -1)\n    assert q.to_axis_angle() == ((0, 0, -1), -pi/2)\n\n    q = Quaternion(0, -1, 0, 0)\n    assert q.to_axis_angle() == ((0, 1, 0), -pi/2)\n\n    q = Quaternion(0, -1, 0, 0)\n    assert q.to_axis_angle() == ((0, -1, 0), pi/2)\n\n    q = Quaternion(0, 0, -1, 0)\n    assert q.to_axis_angle() == ((0, 0, -1", "def test_quaternion_diff():\n    # Test differentiating a quaternion over real field\n    q = Quaternion(x, 0, 0, 0)\n    assert q.diff(x) == Quaternion(1, 0, 0, 0)\n\n    # Test differentiating a quaternion over complex field\n    q = Quaternion(x, y, 0, 0, real_field = False)\n    assert q.diff(x) == Quaternion(1, 0, 0, 0)\n\n    # Test differentiating a quaternion with complex coefficients\n    q = Quaternion(x + 3*I, y, 0, 0)\n    assert q.diff(x) == Quaternion(1, 0, 0, 0)\n\n    # Test differentiating a quaternion with multiple variables\n    q = Quaternion(x + y, x + y, 0, 0)\n    assert q.diff(x) == Quaternion(1, 1, 0, 0)\n    assert q.diff(y) == Quaternion(1, 1, 0, 0)\n\n    # Test differentiating a quaternion with a non-commutative variable\n    q = Quaternion(x, I, 0, 0)\n    raises(ValueError, lambda: q.diff(I))\n\n    # Test differentiating a quaternion with a non-real field\n    q = Quaternion(x, I, 0, 0)\n    assert q.diff(x) == Quaternion(1, 0, 0, 0)\n", "def test_from_axis_angle():\n    q = Quaternion(0, 0, 1, 0)\n    v = q.to_axis_angle()\n    assert v == (1, pi/2)\n\n    # Test from_axis_angle with a normalized axis\n    q = Quaternion(0, 0, 1/3, 0)\n    v = q.to_axis_angle()\n    assert v == (1/3, pi/2)\n\n    # Test from_axis_angle with a negative angle\n    q = Quaternion(0, 0, 1, 0)\n    v = q.to_axis_angle()\n    assert v == (1, pi/2)\n\n    # Test from_axis_angle with a quaternion not in the rotation subgroup\n    q = Quaternion(0, 1, 0, 0)\n    v = q.to_axis_angle()\n    assert v == (1, pi/2)\n"], "sample_337": ["    def test_origin_verified_with_fully_qualified_domain(self):\n        request = HttpRequest()\n        request.META['HTTP_ORIGIN'] = 'https://example.com'\n        middleware = CsrfViewMiddleware()\n        self.assertTrue(middleware._origin_verified(request))\n", "    def test_process_request_with_no_csrf_token_and_no_cookie(self):\n        request = HttpRequest()\n        middleware = CsrfViewMiddleware()\n        request.method = 'POST'\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        with self.assertRaises(RejectRequest) as cm:\n            middleware.process_request(request)\n        self.assertEqual(cm.exception.reason, REASON_NO_CSRF_COOKIE)\n", "    def test_invalid_origin_rejected(self):\n        request = HttpRequest()\n        request.META['HTTP_ORIGIN'] = 'http://bad-origin.com'\n        middleware = CsrfViewMiddleware()\n        with self.assertRaises(RejectRequest):\n            middleware.process_view(request, lambda x: None, (), {})\n", "    def test_process_view_bad_origin(self):\n        request = HttpRequest()\n        request.META['HTTP_ORIGIN'] = 'http://badorigin.com'\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(request, lambda x: None, (), {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.reason_phrase, 'Forbidden')\n", "    def test_cookie_rotation(self):\n        request = HttpRequest()\n        middleware = CsrfViewMiddleware()\n        request.META['CSRF_COOKIE'] = MASKED_TEST_SECRET1\n        middleware.process_request(request)\n        self.assertNotEqual(request.META['CSRF_COOKIE'], MASKED_TEST_SECRET1)\n        self.assertIn('CSRF_COOKIE_USED', request.META)\n"], "sample_545": ["def test_figure_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    assert np.allclose(fig.subplotpars.left, 0.1)\n    assert np.allclose(fig.subplotpars.right, 0.9)\n    assert np.allclose(fig.subplotpars.top, 0.9)\n    assert np.allclose(fig.subplotpars.bottom, 0.1)\n\n    fig.subplots_adjust(wspace=0.2, hspace=0.4)\n    assert np.allclose(fig.subplotpars.wspace, 0.2)\n    assert np.allclose(fig.subplotpars.hspace, 0.4)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\", UserWarning)\n        fig.subplots_adjust(wspace=0.8, hspace=0.9)\n\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2, hspace=0.4)\n", "def test_subplots_adjust():\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1,\n                      wspace=0.4, hspace=0.4)\n\n    # Ensure the adjustment has been applied\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.wspace == 0.4\n    assert fig.subplotpars.hspace == 0.4\n\n    # Ensure the adjustment has been applied to the axes\n    assert ax1.get_position().x0 == 0.1\n    assert ax1.get_position().y0 == 0.1\n    assert ax1.get_position().width == 0.8\n    assert ax1.get_position().height == 0.8\n    assert ax2.get_position().x0 == 0.1\n    assert ax2.get_position().y0 == 0.1\n    assert ax2.get_position().width == 0.8\n    assert ax2.get_position().height == 0.8\n", "def test_set_constrained_layout_pads():\n    # Create a figure with constrained layout\n    fig = plt.figure(constrained_layout=True)\n    fig.subplots_adjust(top=0.9, bottom=0.1)\n    fig.set_constrained_layout_pads(w_pad=1.0, h_pad=2.0)\n    \n    # Check that the layout engine has the correct padding values\n    assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n    assert fig.get_layout_engine().get_info() == {'w_pad': 1.0, 'h_pad': 2.0}\n\n    # Clear the figure\n    fig.clear()\n\n    # Try to set constrained layout pads on a figure without constrained layout\n    with pytest.raises(RuntimeError):\n        fig.set_constrained_layout_pads(w_pad=1.0, h_pad=2.0)\n\n    # Try to set constrained layout pads on a figure with a different layout engine\n    fig.set_layout_engine('tight')\n    with pytest.raises(RuntimeError):\n        fig.set_constrained_layout_pads(w_pad=1.0, h_pad=2.0)\n\n    # Check that the padding values are not set when passing in None\n    fig.set_layout_engine('constrained')\n    fig.set_constrained_layout_pads(w_pad=None, h_pad=None)\n    assert fig.get_layout_engine().get_info() == {'w_pad': 3.0, 'h_pad': 3.0}\n\n    # Check that the padding values are updated when passing in a dict\n    fig.set_constrained_layout_pads(w_pad={'w_pad': 1.0}, h_pad={'h_pad': 2.0})\n    assert fig.get_layout_engine().get_info() == {'w_pad': 1.0, 'h_pad': 2.0}\n\n    # Check that the padding values are updated when passing in a dict with all values\n    fig.set_constrained_layout_pads(w_pad={'w_pad': 1.0, 'h_pad': 2.0, 'wspace': 0.5, 'hspace': 0.5})\n    assert fig.get_layout_engine().get_info() == {'w_pad': 1.0, 'h_pad': 2.0, 'wspace': 0.5, 'hspace': 0.5}\n", "    def test_figure_savefig_transparency(self, transparent):\n        fig = plt.figure()\n        fig.patch.set_facecolor('red')\n        ax = fig.add_subplot(111)\n        ax.plot([1, 2, 3])\n        ax.patch.set_facecolor('blue')\n        fig.savefig('test.png', transparent=transparent)\n        ax.set_visible(False)\n        ax2 = fig.add_subplot(111)\n        ax2.plot([1, 2, 3])\n        fig.savefig('test.png', transparent=transparent)\n", "def test_figure_bounding_box_inches():\n    fig = plt.figure(figsize=(6, 4), dpi=100)\n    fig.text(0.5, 0.9, 'Caption', ha='center', fontsize=10)\n    ax = fig.add_subplot(111)\n    ax.text(0.5, 0.9, 'Axes caption', ha='center', fontsize=10)\n    ax.set_xticks([1, 2, 3])\n    ax.set_yticks([1, 2, 3])\n\n    # The bounding box should be large enough to contain the figure and axes titles\n    fig.savefig('test.png', bbox_inches='tight')\n\n    # Save the figure with the default bounding box\n    fig.savefig('test_default.png')\n\n    # Compare the bounding boxes of the two saved figures\n    assert (np.abs(fig.get_tightbbox(None).bounds - [0, 0, 6.0, 4.0]) < 1e-5).all()\n    assert (np.abs(fig.bbox_inches.bounds - [0, 0, 6.0, 4.0]) < 1e-5)\n\n    # Create a figure with a subfigure\n    fig_subfig = fig.subfigures(1, 1)[0]\n    ax_subfig = fig_subfig.subplots(1, 1)\n    ax_subfig.text(0.5, 0.9, 'Subfigure caption', ha='center', fontsize=10)\n    ax_subfig.set_xticks([1, 2, 3])\n    ax_subfig.set_yticks([1, 2, 3])\n\n    # The bounding box should be large enough to contain the subfigure and its title\n    fig.savefig('test_subfig.png', bbox_inches='tight')\n\n    # Save the figure with the default bounding box\n    fig.savefig('test_default_subfig.png')\n\n    # Compare the bounding boxes of the two saved figures\n    assert (np.abs(fig.get_tightbbox(None).bounds - [0, 0, 6.0, 4.0]) < 1e-5).all()\n    assert (np.abs(fig.bbox_inches.bounds - [0, 0,"], "sample_1209": ["def test_prefix_division():\n    # Test prefix division\n    assert (kilo / kibi).scale_factor == 2\n    assert (kilo / kibi).name == \"kibi\"\n    assert (kilo / kibi).abbrev == 'Ki'\n\n    # Test prefix division with multiple digits\n    assert (mebi / kibi).scale_factor == 2\n    assert (mebi / kibi).name == \"kibi\"\n    assert (mebi / kibi).abbrev == 'Ki'\n\n    # Test prefix division with base 2\n    assert (kilo / kibi).scale_factor == 2\n    assert (kilo / kibi).name == \"kibi\"\n    assert (kilo / kibi).abbrev == 'Ki'\n\n    # Test prefix division with result equal to one\n    assert (kibi / kibi).scale_factor == 1\n    assert (kibi / kibi).name == \"kibi\"\n    assert (kibi / kibi).abbrev == 'Ki'\n\n    # Test prefix division with result equal to one and base 2\n    assert (kibi / kibi).scale_factor == 1\n    assert (kibi / kibi).name == \"kibi\"\n    assert (kibi / kibi).abbrev == 'Ki'\n", "def test_prefix_division_with_non_prefix_quantity():\n    from sympy.physics.units import Quantity, meter, Prefix\n    unit = Quantity('meter', 'm')\n    kilo = Prefix('kilo', 'k', 3)\n    \n    result = kilo / unit\n    assert result == S(1) / 1000\n    \n    # Test with different units\n    result = kilo / meter\n    assert result == S(1) / 1000\n    \n    result = kilo / Quantity('meter', 'm', base_unit=True)\n    assert result == S(1) / 1000\n", "def test_prefix_division():\n    # Test division of two prefixes\n    result = (Prefix('kilo', 'k', 3) / Prefix('deci', 'd', -1))\n    assert result == Prefix('hecto', 'h', 2)\n    assert result.scale_factor == 10**2\n\n    # Test division of prefix by a quantity\n    result = (Prefix('kilo', 'k', 3) / meter)\n    assert result == kilo * meter / meter\n    assert result.scale_factor == 10**3\n\n    # Test division of quantity by a prefix\n    result = (meter / Prefix('kilo', 'k', 3))\n    assert result == meter * (1 / kilo)\n    assert result.scale_factor == 1 / (10**3)\n\n    # Test division of a prefix by another prefix with different base\n    result = (Prefix('kibi', 'Ki', 10, 2) / Prefix('kilo', 'k', 3))\n    assert result == Prefix('mebi', 'Mi', 20, 2)\n    assert result.scale_factor == 2**20 / (10**3)\n\n    # Test division of a prefix by another prefix with the same base and exponent\n    result = (Prefix('kilo', 'k', 3) / Prefix('kilo', 'k', 3))\n    assert result == S.One\n\n    # Test division of a prefix by a unit with the same dimension\n    result = (Prefix('kilo', 'k', 3) / meter)\n    assert result == kilo * meter / meter\n    assert result.scale_factor == 10**3\n\n    # Test division of a prefix by a unit with different dimension\n    result = (Prefix('kilo', 'k', 3) / W)\n    assert result == Prefix('kilo', 'k', 3) * W**-1\n    assert result.scale_factor == 10**3\n", "def test_prefix_multiplication_and_division():\n    # Test multiplication with same base and exponent\n    assert (kilo * kilo).scale_factor == 1000**2\n    assert (kilo * kilo).abbrev == \"kkm\"\n    assert (kilo * kilo).name == \"kilokilo\"\n\n    # Test multiplication with different base and exponent\n    assert (kilo * mebi).scale_factor == 1000 * 1024\n    assert (kilo * mebi).abbrev == \"Ym\"\n    assert (kilo * mebi).name == \"kilokibi\"\n\n    # Test division with same base and exponent\n    assert (kilo / kilo).scale_factor == 1\n    assert (kilo / kilo).abbrev == \"k\"\n    assert (kilo / kilo).name == \"kilo\"\n\n    # Test division with different base and exponent\n    assert (kilo / mebi).scale_factor == 1000 / 1024\n    assert (kilo / mebi).abbrev == \"mY\"\n    assert (kilo / mebi).name == \"kikibi\"\n\n    # Test prefix multiplication with base not equal to 10\n    assert (micro * micro).scale_factor == 10**-6 * 10**-6\n    assert (micro * micro).abbrev == \"\\mu\\mu\"\n    assert (micro * micro).name == \"micromicro\"\n\n    # Test prefix division with base not equal to 10\n    assert (micro / micro).scale_factor == 1\n    assert (micro / micro).abbrev == \"\\mu\"\n    assert (micro / micro).name == \"micro\"\n", "def test_prefix_mul(self):\n    # Test prefix multiplication\n    # Test cases from https://en.wikipedia.org/wiki/Prefix_for_binary_quantities\n    assert(kibi * kilo).scale_factor == 1024\n    assert(kibi * kilo == kibi * Prefix('kilo', 'k', 3, 10))\n    assert(kibi * kilo == Prefix('Y', 'Y', 13, 2))\n    \n    # Test prefix multiplication with non-Prefix object\n    assert(kibi * meter == kibi * meter)\n    assert(kibi * S(1024) == kibi * S(1024))\n\n    # Test prefix multiplication with Quantity object\n    assert(kibi * Quantity('1 meter') == kibi * meter)\n\n    # Test prefix multiplication with multiple prefixes\n    assert(kibi * kilo * kilo == Prefix('Y', 'Y', 16, 2))\n    assert(kibi * kibi == Prefix('Yi', 'Yi', 20, 2))\n\n    # Test prefix multiplication with negative exponents\n    assert(milli * milli == Prefix('m', 'm', -6, 10))\n    assert(milli * micro == Prefix('m', 'm', -3, 10))\n\n    # Test prefix multiplication with zero exponent\n    assert(kilo ** 0 == S.One)\n    assert(kilo ** 0 * kilo == S.One)\n\n    # Test prefix multiplication with negative exponent\n    assert(kilo ** (-1) == milli)\n    assert(kilo ** (-1) * kilo == S.One)\n"], "sample_735": ["    def test_init_params(self):\n        # Test case 1: 'kmeans' initialization\n        gm = GaussianMixture(n_components=2, init_params='kmeans')\n        self.assertEqual(gm.init_params, 'kmeans')\n\n        # Test case 2: 'random' initialization\n        gm = GaussianMixture(n_components=2, init_params='random')\n        self.assertEqual(gm.init_params, 'random')\n\n        # Test case 3: Invalid initialization\n        with self.assertRaises(ValueError):\n            GaussianMixture(n_components=2, init_params='invalid')\n", "    def test_init_params_kmeans(self):\n        gmm = GaussianMixture(n_components=2, init_params='kmeans')\n        self.assertEqual(gmm.init_params, 'kmeans')\n", "    def test_initialization_with_invalid_covariance_type(self):\n        model = GaussianMixture(covariance_type='unknown')\n        with self.assertRaises(ValueError):\n            model._check_parameters(None)\n", "    def test_init_params_method(self):\n        gmm = GaussianMixture(n_components=2, covariance_type='diag',\n                              n_init=1, init_params='kmeans',\n                              random_state=42)\n        X, _ = make_spd_matrix(2, random_state=42)\n        gmm.fit(X)\n\n        # Check that weights, means and precisions have been initialized\n        assert_array_almost_equal(gmm.weights_, np.array([0.5, 0.5]))\n        assert_array_almost_equal(gmm.means_, np.array([[0., 0.], [0., 0.]]))\n\n        # Check that precisions have been initialized\n        assert_array_almost_equal(gmm.precisions_cholesky_, np.array([[1., 0.], [0., 1.]]))\n\n        # Check that precisions have been initialized with the right shape\n        assert_equal(gmm.precisions_.shape, (2, 2))\n", "    def __init__(self, rng, n_samples=500, n_components=2, n_features=2,\n                 scale=50):\n        super().__init__(rng, n_samples, n_components, n_features, scale)\n"], "sample_2": ["def test_uncertainty_array_with_non_equivalent_unit():\n    # Create a CCDData object with StdDevUncertainty and units of ADU.\n    data = _random_array.copy()\n    ccd = CCDData(data, unit=u.adu, uncertainty=StdDevUncertainty(data))\n    \n    # Create a new CCDData object with the same data but different units.\n    ccd_diff_unit = CCDData(data, unit=u.mJy, uncertainty=StdDevUncertainty(data))\n    \n    # Try to assign the uncertainty from the first CCDData object to the second.\n    with pytest.raises(ValueError):\n        ccd_diff_unit.uncertainty = ccd.uncertainty\n", "def test_ccddata_arithmetic_operations():\n    \"\"\"\n    Test that the _arithmetic decorator works as expected.\n    \"\"\"\n    ccd_data = create_ccd_data()\n    new_data = ccd_data + 5\n    assert np.all(new_data.data == ccd_data.data + 5)\n    assert new_data.unit == u.adu\n\n    ccd_data += 5\n    assert np.all(ccd_data.data == ccd_data.data + 5)\n    assert ccd_data.unit == u.adu\n\n    ccd_data -= 5\n    assert np.all(ccd_data.data == ccd_data.data - 5)\n    assert ccd_data.unit == u.adu\n\n    ccd_data *= 5\n    assert np.all(ccd_data.data == ccd_data.data * 5)\n    assert ccd_data.unit == u.adu\n\n    ccd_data /= 5\n    assert np.all(ccd_data.data == ccd_data.data / 5)\n    assert ccd_data.unit == u.adu\n\n    with pytest.raises(TypeError):\n        ccd_data._data = None\n        ccd_data += 5\n", "def test_ccddata_reader_unit_checking():\n    \"\"\"\n    Test that the CCDData reader checks for and complains about unit inconsistencies.\n    \"\"\"\n\n    # Test case 1: data unit is None\n    with pytest.raises(ValueError):\n        fits_ccddata_reader(get_pkg_data_filename('ccddata_reader_testfile.fits'))\n\n    # Test case 2: data unit and uncertainty unit are different\n    ccd = create_ccd_data()\n    ccd.uncertainty = StdDevUncertainty(_random_array + 1 * u.adu)\n    with pytest.raises(ValueError):\n        fits_ccddata_reader(get_pkg_data_filename('ccddata_reader_testfile.fits'))\n\n    # Test case 3: uncertainty unit is None\n    ccd = create_ccd_data()\n    ccd.uncertainty = StdDevUncertainty(_random_array)\n    ccd.uncertainty.unit = None\n    with pytest.raises(ValueError):\n        fits_ccddata_reader(get_pkg_data_filename('ccddata_reader_testfile.fits'))\n\n    # Test case 4: uncertainty unit and data unit are different\n    ccd = create_ccddata(1 * u.m)\n    ccd.uncertainty = StdDevUncertainty(_random_array + 1 * u.m)\n    with pytest.raises(ValueError):\n        fits_ccddata_reader(get_pkg_data_filename('ccddata_reader_testfile.fits'))\n", "def test_arithmetic_decorator_unit_removal():\n    from astropy.nddata.ccddata import CCDData\n    from astropy import units as u\n    from astropy.nddata.nduncertainty import StdDevUncertainty\n\n    # Create a CCDData instance with a unit\n    ccd = CCDData(_random_array, unit=u.adu)\n\n    # Try creating a new CCDData instance with a unit that is different from\n    # the original unit. This should raise a ValueError.\n    with pytest.raises(ValueError):\n        CCDData(_random_array, unit=u.m)\n\n    # Now use the _arithmetic decorator to temporarily disable the unit\n    # requirement.\n        _config_ccd_requires_unit = False\n        result = self._prepare_then_do_arithmetic(np.add, operand,\n                                                  operand2)\n        _config_ccd_requires_unit = True\n        return result.__class__(result)\n\n    CCDData._arithmetic = _arithmetic_add\n\n    # Create a new CCDData instance with a unit that is different from the\n    # original unit. This should not raise a ValueError.\n    new_ccd = CCDData(_random_array, unit=u.m)\n\n    # Now restore the original _arithmetic method\n    CCDData._arithmetic = _arithmetic(np.add)\n\n    # Check that the new CCDData instance has the correct unit\n    assert new_ccd.unit.is_equivalent(u.m)\n\n    # Now use the _arithmetic decorator to temporarily disable the unit\n    # requirement and create a new CCDData instance with no unit. This should\n    # not raise a ValueError.\n    new_ccd = CCDData(_random_array)\n\n    # Check that the new CCDData instance has no unit\n    assert new_ccd.unit is None\n", "def test_ccddata_known_invalid_fits_unit_strings():\n    \"\"\"\n    Test the known_invalid_fits_unit_strings dictionary.\n    \"\"\"\n    known_invalid_fits_unit_strings = CCDData.known_invalid_fits_unit_strings\n    for fits_unit_string, unit in known_invalid_fits_unit_strings.items():\n        with pytest.raises(ValueError):\n            u.Unit(fits_unit_string)\n        try:\n            CCDData([1], unit=fits_unit_string)\n        except ValueError as err:\n            if str(err) != f\"invalid unit: {fits_unit_string}\":\n                raise AssertionError(\n                    f\"Expected ValueError with the string '{fits_unit_string}' \"\n                    f\"but got: {str(err)}\")\n        if fits_unit_string == 'ELECTRONS/S':\n            # This one should not raise an error because it should be mapped\n            # to the equivalent unit u.electron/u.s\n            ccd_data = CCDData([1], unit=fits_unit_string)\n            assert ccd_data.unit == u.electron/u.s\n        else:\n            ccd_data = CCDData([1], unit=fits_unit_string)\n            assert ccd_data.unit == unit\n"], "sample_1198": ["def test_parse_mathematica_Pi_translation():\n    # Test the translation of Pi to pi\n    mma_expr = \"Pi\"\n    sympy_expr = parse_mathematica(mma_expr)\n    assert sympy_expr == pi\n", "def test_parse_sqrt():\n    expression = parse_mathematica(\"Sqrt[x]\")\n    expected = sympify(\"sqrt(x)\")\n    assert expression == expected\n", "def test_parse_mathematica_with_function():\n    m = MathematicaParser()\n    sympy_expr = m.parse(\"f[x_, y_, z_] := x^2 + y^2 + z^2\")\n    expected_expr = Lambda((x, y, z), x**2 + y**2 + z**2)\n    assert sympy_expr == expected_expr\n", "def test_parse_mathematica_Sqrt():\n    expression = parse_mathematica(\"Sqrt[2]\")\n    assert expression == sympify('sqrt(2)')\n", "def test_parse_mathematica_with_multiple_quantities():\n    from sympy import Matrix\n    w, x, y, z = symbols('w x y z')\n    m = Matrix([[1, 2], [3, 4]])\n    m = Matrix(m)\n    m = parse_mathematica(\"Matrix[{{1, 2}, {3, 4}}]\")\n    assert m == m\n\n"], "sample_524": ["def test_figure_get_constrained_layout_pads_relative():\n    \"\"\"\n    Test get_constrained_layout_pads with relative=True\n    \"\"\"\n    # Create a figure with constrained layout\n    fig = plt.figure(constrained_layout=True)\n    \n    # Set the dpi of the figure to 100\n    fig.set_dpi(100)\n    \n    # Get the constrained layout pads\n    w_pad, h_pad, wspace, hspace = fig.get_constrained_layout_pads(relative=True)\n    \n    # Check that the pads are in figure relative units\n    assert np.allclose(w_pad, 3.0/72.0)\n    assert np.allclose(h_pad, 3.0/72.0)\n    \n    # Check that the wspace and hspace are 0\n    assert np.allclose(wspace, 0)\n    assert np.allclose(hspace, 0)\n", "def test_figure_add_subplot():\n    # Test add_subplot with no arguments\n    fig = plt.figure()\n    ax = fig.add_subplot()\n    assert len(fig.axes) == 1\n    assert ax is fig.gca()\n\n    # Test add_subplot with a single argument\n    ax = fig.add_subplot(2, 1, 1)\n    assert len(fig.axes) == 2\n    assert ax is fig.gca()\n\n    # Test add_subplot with multiple arguments\n    ax = fig.add_subplot(2, 2, 1)\n    assert len(fig.axes) == 3\n    assert ax is fig.gca()\n\n    # Test add_subplot with projection\n    ax = fig.add_subplot(2, 1, 1, projection='polar')\n    assert len(fig.axes) == 2\n    assert isinstance(ax, mpl.projections.polar.PolarAxes)\n\n    # Test add_subplot with axes_class\n    class CustomAxes(mpl.axes.Axes):\n        pass\n    ax = fig.add_subplot(2, 1, 1, axes_class=CustomAxes)\n    assert len(fig.axes) == 2\n    assert isinstance(ax, CustomAxes)\n\n    # Test add_subplot with sharex and sharey\n    ax1 = fig.add_subplot(2, 1, 1)\n    ax2 = fig.add_subplot(2, 1, 2, sharex=ax1, sharey=ax1)\n    assert len(fig.axes) == 2\n    assert ax1.get_shared_axes()[0] is ax2\n\n    # Test add_subplot with label\n    ax = fig.add_subplot(2, 1, 1, label='custom_label')\n    assert len(fig.axes) == 2\n    assert ax.get_label() == 'custom_label'\n\n    # Test add_subplot with invalid arguments\n    with pytest.raises(ValueError):\n        fig.add_subplot(2, 2, 3)\n    with pytest.raises(ValueError):\n        fig.add_subplot(2, 2, 3, projection='invalid_projection')\n", "def test_subplots_adjust_with_constrained_layout():\n    fig, axs = plt.subplots(2, 2)\n    fig.tight_layout()\n\n    # adjust the subplots\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    # the subplots should remain unchanged\n    assert np.allclose(axs[0, 0].get_position().get_points(), [[0.125, 0.925], [0.875, 0.925]])\n    assert np.allclose(axs[0, 1].get_position().get_points(), [[0.125, 0.775], [0.875, 0.775]])\n    assert np.allclose(axs[1, 0].get_position().get_points(), [[0.125, 0.225], [0.875, 0.225]])\n    assert np.allclose(axs[1, 1].get_position().get_points(), [[0.125, 0.075], [0.875, 0.075]])\n\n    # use constrained layout to adjust the subplots\n    fig.set_layout_engine('constrained')\n    fig.tight_layout()\n\n    # the subplots should have changed\n    assert np.allclose(axs[0, 0].get_position().get_points(), [[0.175, 0.775], [0.825, 0.775]])\n    assert np.allclose(axs[0, 1].get_position().get_points(), [[0.175, 0.525], [0.825, 0.525]])\n    assert np.allclose(axs[1, 0].get_position().get_points(), [[0.175, 0.275], [0.825, 0.275]])\n    assert np.allclose(axs[1, 1].get_position().get_points(), [[0.175, 0.025], [0.825, 0.025]])\n\n    # use constrained layout with custom padding\n    fig.set_layout_engine('constrained')\n    fig.set_constrained_layout_pads(w_pad=0.1, h_pad=0.1)\n    fig.tight_layout()\n\n    # the subplots should have changed with the custom padding\n    assert np.allclose(axs[0, 0].get_position().get_points(), [[0.175, 0.", "def test_figure_subplot_mosaic():\n    fig, axs = plt.subplots(1, 2)\n    fig.subplots_adjust(wspace=0.4)\n    fig.subplotpars.update(right=0.9)\n    sfigs = fig.subplots(1, 2, squeeze=True, wspace=0.2)\n    axsL = sfigs[0].subplots(1, 2)\n    axsR = sfigs[1].subplots(2, 1)\n\n    # Check that the SubFigure axes were created correctly\n    assert len(sfigs[0].axes) == 2\n    assert len(sfigs[1].axes) == 2\n\n    # Check that the main figure axes were created correctly\n    assert len(fig.axes) == 4\n\n    # Check that the subplot parameters were updated correctly\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.wspace == 0.4\n\n    # Check that the subfigure subplot parameters were updated correctly\n    assert sfigs[0].subplotpars.wspace == 0.2\n    assert sfigs[1].subplotpars.wspace == 0.2\n\n    # Check that the axis positions were calculated correctly\n    assert axsL[0].get_position().get_bounds() == (0, 0, 0.45, 1)\n    assert axsL[1].get_position().get_bounds() == (0.45, 0, 0.45, 1)\n    assert axsR[0].get_position().get_bounds() == (0, 0, 1, 0.45)\n    assert axsR[1].get_position().get_bounds() == (0, 0.45, 1, 0.45)\n\n    # Check that the axis layout was updated correctly\n    fig.align_labels()\n\n    # Check that the axis labels were updated correctly\n    assert axsL[0].get_xlabel() == ''\n    assert axsL[1].get_xlabel() == ''\n    assert axsR[0].get_ylabel() == ''\n    assert axsR[1].get_ylabel() == ''\n\n    # Check that the subfigure layout was updated correctly\n    sfigs[0].align_labels()\n    sfigs[1].align_labels()\n\n    # Check that the subfigure labels were updated correctly\n    assert sfigs[0].suptitle.get_text", "def test_tight_layout():\n    # Create a figure with tight layout.\n    fig = plt.figure(tight_layout=True)\n    ax1 = fig.add_subplot(2, 2, 1)\n    ax1.plot([1, 2, 3])\n    ax2 = fig.add_subplot(2, 2, 2)\n    ax2.plot([2, 3, 4])\n    ax3 = fig.add_subplot(2, 2, 3)\n    ax3.plot([3, 4, 5])\n    ax4 = fig.add_subplot(2, 2, 4)\n    ax4.plot([4, 5, 6])\n\n    # Test that tight_layout is called when drawing.\n    with rc_context(rc={'figure.autolayout': True}):\n        fig.draw()\n\n    # Test that axes are not repeated when creating a new subplot with the same\n    # position.\n    ax5 = fig.add_subplot(2, 2, 1)\n    assert ax5 is ax1\n\n    # Test that tight_layout works with a custom layout engine.\n    class CustomLayoutEngine:\n            pass\n\n    with rc_context(rc={'figure.autolayout': True,\n                       'figure.constrained_layout.use': True}):\n        fig.set_layout_engine(CustomLayoutEngine())\n\n    # Test that tight_layout works with a figure that has a custom layout engine.\n    with rc_context(rc={'figure.autolayout': True}):\n        fig.set_layout_engine(CustomLayoutEngine())\n        fig.draw()\n\n    # Test that tight_layout works with a figure that has a constrained layout.\n    with rc_context(rc={'figure.autolayout': False,\n                       'figure.constrained_layout.use': True}):\n        fig.draw()\n\n    # Test that tight_layout raises an error when trying to set a non-boolean value\n    # for tight_layout.\n    with pytest.raises(ValueError):\n        fig.set_layout_engine(TightLayoutEngine(tight=False))\n"], "sample_639": ["    def test_add_message_without_linter(self):\n        with self.assertRaises(MissingArgumentManager):\n            OtherBasicChecker().add_message(\"W0001\")\n", "    def check_consistency(self):\n        with self.assertRaises(InvalidMessageError):\n            super().check_consistency()\n", "def test_create_message_definition_from_tuple_invalid_msg_tuple():\n    \"\"\"Test that create_message_definition_from_tuple raises a InvalidMessageError when the message tuple has too few elements.\"\"\"\n    checker = OtherBasicChecker()\n    with pytest.raises(InvalidMessageError):\n        checker.create_message_definition_from_tuple(\"W0001\", (\"message\",))\n", "    def test__init__(self):\n        \"\"\"Test the initialization of the BaseChecker class.\"\"\"\n        checker = BaseChecker()\n        assert checker.name is None\n        assert checker.linter is None\n        checker = BaseChecker(linter=object())\n        assert checker.linter == object()\n        with pytest.raises(MissingArgumentManager):\n            BaseChecker()\n", "def test_check_consistency_valid(self):\n    \"\"\"Test that BaseChecker's check_consistency method raises an InvalidMessageError when\n    there are two messages with different checker ids and ids that do not match the\n    pattern 'xx'.\n\n    This test is for the issue #1466\n    \"\"\"\n    basic_checker = OtherBasicChecker(linter=None)\n    basic_checker.msgs = {\n        \"W1234\": (\n            \"message\",\n            \"message-symbol\",\n            \"Message description with detail.\",\n        ),\n        \"W4321\": (\n            \"message\",\n            \"message-symbol\",\n            \"Message description with detail.\",\n        ),\n    }\n    basic_checker.check_consistency()\n    self.assertRaises(InvalidMessageError, basic_checker.check_consistency)\n"], "sample_877": ["def test_isotonic_regression_out_of_bounds_clip():\n    X, y = make_regression(n_samples=10, n_features=1, random_state=42)\n    iso_reg = IsotonicRegression(out_of_bounds=\"clip\")\n    iso_reg.fit(X, y)\n    y_pred = iso_reg.predict(X)\n    assert np.all(np.isfinite(y_pred))\n\n", "def test_isotonic_regression_out_of_bounds_warning():\n    \"\"\"Test the case where out_of_bounds is set to 'raise'.\"\"\"\n    X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    iso_reg = IsotonicRegression(out_of_bounds=\"raise\")\n    with pytest.warns(None):\n        iso_reg.fit(X, y)\n    with pytest.raises(ValueError):\n        iso_reg.predict(np.array([1.0, 1.1, 1.2]))\n", "def test_isotonic_regression_interpolation():\n    X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    iso_reg = IsotonicRegression().fit(X, y)\n    x_test = np.linspace(X.min(), X.max(), 10)\n    y_pred = iso_reg.transform(x_test)\n    assert_array_almost_equal(y_pred, iso_reg.predict(x_test))\n", "def test_isotonic_regression_out_of_bounds_nans():\n    \"\"\"Test that isotonic regression correctly handles the 'nan' out of bounds setting.\n\n    The test case checks that the 'nan' setting is applied correctly when the\n    value to predict is outside the range of the training data.\n\n    Parameters\n    ----------\n    None\n    \"\"\"\n    X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    iso_reg = IsotonicRegression(out_of_bounds=\"nan\")\n    iso_reg.fit(X, y)\n    T = np.array([np.min(X) - 1, np.max(X) + 1])\n    y_pred = iso_reg.predict(T)\n    expected_y_pred = np.array([np.nan, np.nan])\n    assert_array_almost_equal(y_pred, expected_y_pred)\n\n", "def test_isotonic_regression_handle_out_of_bounds():\n    \"\"\"Regression: Out-of-bounds values.\"\"\"\n    X, y = make_regression(n_samples=10, n_features=1, random_state=41)\n    iso = IsotonicRegression(out_of_bounds=\"clip\")\n\n    X_out_of_bounds = np.array([1e-10, 2, 2, 4, 5, 7, 9, 10])\n    y_out_of_bounds = np.array([1e-10, 2, 2, 3, 4, 5, 6, 10])\n\n    # Make sure the fitted model is cliping the out of bounds values\n    iso.fit(X, y)\n    y_pred = iso.predict(X_out_of_bounds)\n\n    # Test that the out of bounds values were clipped\n    assert_array_almost_equal(np.min(y_pred), 2.0)\n    assert_array_almost_equal(np.max(y_pred), 6.0)\n\n    # Test that the in-bounds values were preserved\n    assert_array_almost_equal(np.min(y_pred[1:3]), 2.0)\n    assert_array_almost_equal(np.max(y_pred[5:7]), 6.0)\n\n    # Test the same with y_min and y_max\n    iso = IsotonicRegression(out_of_bounds=\"clip\", y_min=1.0, y_max=5.0)\n    iso.fit(X, y)\n    y_pred = iso.predict(X_out_of_bounds)\n\n    # Test that the out of bounds values were clipped\n    assert_array_almost_equal(np.min(y_pred), 1.0)\n    assert_array_almost_equal(np.max(y_pred), 5.0)\n\n    # Test that the in-bounds values were preserved\n    assert_array_almost_equal(np.min(y_pred[1:3]), 1.0)\n    assert_array_almost_equal(np.max(y_pred[5:7]), 5.0)\n"], "sample_949": ["def test_man_page_builder_init_warning(caplog):\n    \"\"\"Test the warning when no 'man_pages' config value found.\"\"\"\n    config = Config()\n    config.man_pages = None\n    builder = ManualPageBuilder(config)\n    builder.init()\n    assert caplog.records[-1].levelname == 'WARNING'\n    assert caplog.records[-1].message.startswith(__('no \"man_pages\" config value found; no manual pages will be written'))\n", "def test_manual_page_builder_init_config_value_not_found(caplog):\n    \"\"\"Test handling of missing \"man_pages\" config value.\"\"\"\n    config = Config()\n    builder = ManualPageBuilder(config)\n    assert builder.init() == None\n    assert caplog.records[-1].levelname == 'WARNING'\n    assert caplog.records[-1].message == \"no 'man_pages' config value found; no manual pages will be written\"\n", "def test_init_method_logger_warning(caplog):\n    caplog.set_level(logging.WARNING)\n    builder = ManualPageBuilder()\n    builder.init()\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in caplog.text\n", "def test_manual_page_builder_init_warning(caplog):\n    \"\"\" Test that a warning is raised when no \"man_pages\" config value is found. \"\"\"\n    config = Config()\n    builder = ManualPageBuilder(config)\n    assert caplog.record_tuples == []\n    builder.init()\n    assert caplog.record_tuples == [\n        (builder.name, logging.WARNING, \"no 'man_pages' config value found; no manual pages will be written\")\n    ]\n", "def test_manual_page_builder_write(tmpdir):\n    # setup a sphinx application\n    conf_dir = tmpdir.mkdir('conf')\n    app = Sphinx(srcdir=str(tmpdir), outdir=str(tmpdir), confdir=str(conf_dir))\n\n    # setup configuration\n    config = Config()\n    config.project = 'my_project'\n    config.release = '1.0'\n    config.author = 'John Doe'\n    config.man_pages = default_man_pages(config)\n    config.man_make_section_directory = True\n    app.config = config\n\n    # setup manual page builder\n    builder = ManualPageBuilder(app)\n    builder.init()\n\n    # write a document\n    from sphinx.environment import Env\n    from sphinx.environment import toctree\n    env = Env(app)\n    toctree = toctree()\n    env.all_docs = {'doc1': toctree}\n    env.get_doctree = lambda docname: toctree\n    builder.write('doc1')\n\n    # check written file\n    assert path.exists(builder.outdir / 'man1' / 'doc1.1')\n"], "sample_338": ["    def test_rename_model_destructible_args(self):\n        \"\"\"Test renaming a model with deconstructible arguments\"\"\"\n        before_state = self.make_project_state([\n            self.author_name_deconstructible_1,\n        ])\n        after_state = self.make_project_state([\n            self.author_name_deconstructible_2,\n        ])\n        changes = self.get_changes(before_state.models, after_state.models)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['RenameModel'])\n", "def test_rename_field_in_renamed_model(self):\n    \"\"\"Test that a field rename in a model that has been renamed doesn't raise.\"\"\"\n    before = self.make_project_state([\n        self.author_renamed_with_book,\n    ])\n    after = self.make_project_state([\n        self.author_renamed_with_book,\n    ])\n    autodetector = MigrationAutodector(\n        before,\n        after,\n    )\n    changes = autodetector._detect_changes()\n    self.assertEqual(len(changes.get(\"testapp\", [])), 0)\n", "def test_alter_unique_together(self):\n    \"\"\"\n    Test that altering unique_together works correctly.\n    \"\"\"\n    author_name_foo_together = ModelState(\"testapp\", \"Author\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n        (\"foo\", models.CharField(max_length=200)),\n    ], {\n        \"index_together\": {(\"name\", \"foo\")},\n        \"unique_together\": {(\"foo\", \"name\")},\n    })\n    after_states = [\n        self.author_name_foo_together,\n        self.book,\n        self.book_foo_together,\n        self.book_foo_together_2,\n        self.book_foo_together_3,\n        self.book_foo_together_4,\n    ]\n    before_states = [self.author_name]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterUniqueTogether\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\", unique_together=[(\"foo\", \"name\")])\n", "    def test_rename_field_and_alter_field(self):\n        from django.db.migrations.operations import RenameField, AlterField\n        from django.db.migrations.operations.models import AlterModelOptions\n\n        author_name_renamed = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        author_name_renamed_field = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"names\", models.CharField(max_length=200)),\n        ])\n        author_name_altered = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=300)),\n        ])\n\n        changes = self.get_changes(\n            [author_name_renamed],\n            [author_name_renamed_field, author_name_altered]\n        )\n\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['RenameField', 'AlterField'])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"rename_name\", old_name='name', new_name='names')\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name='rename_name', new_field=models.CharField(max_length=200))\n        self.assertOperationAttributes(changes, \"testapp\", 1, 0, name='rename_names', field=models.CharField(max_length=300))\n        self.assertOperationAttributes(changes, \"testapp\", 1, 0, name='rename_names', field=models.CharField(max_length=300))\n        self.assertOperationAttributes(changes, \"testapp\", 1, 0, preserve_default=True)\n", "def test_rename_model_maintains_related_field_names(self):\n    before_states = [\n        self.author_empty,\n        self.contract,\n    ]\n    after_states = [\n        self.contract_renamed,\n        self.publisher,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        0,\n        name=\"0001_squashed_0002_auto_20230221_1208\",\n        dependencies=(\"testapp\", \"__first__\"),\n        operations=[\n            operations.RenameModel(\n                old_name=\"testapp.Contract\",\n                new_name=\"testapp.Deal\",\n            ),\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        0,\n        1,\n        name=\"0001_squashed_0002_auto_20230221_1208\",\n        dependencies=(\"testapp\", \"0001_squashed_0002_auto_20230221_1208\"),\n        operations=[\n            operations.AddField(\n                model_name=\"Deal\",\n                name=\"author\",\n                field=models.ForeignKey(\n                    \"testapp.Author\",\n                    models.CASCADE,\n                ),\n                preserve_default=True,\n            ),\n            operations.AddField(\n                model_name=\"Deal\",\n                name=\"publisher\",\n                field=models.ForeignKey(\n                    \"testapp.Publisher\",\n                    models.CASCADE,\n                ),\n                preserve_default=True,\n            ),\n        ],\n    )\n    self.assertOperationAttributes(\n        changes,\n        \"testapp\",\n        1,\n        0,\n        name=\"0003_auto_20230221_1209\",\n        dependencies=(\"testapp\", \"0001_squashed_0002_auto_20230221_1208\"),\n        operations=[\n            operations.AddField(\n                model_name=\"Publisher\",\n                name=\"author\",\n                field=models.ForeignKey(\n                    \"testapp.Author\",\n                    models.CASCADE,\n                ),\n                preserve_default=True,\n            ),\n        ],\n    )\n"], "sample_199": ["    def setUpTestData(cls):\n        cls.author = Author.objects.create(name='Test Author')\n        cls.books = Book.objects.create(\n            isbn='1234567890', name='Test Book', pages=100, rating=3.0,\n            price=Decimal('10.00'), contact=cls.author, pubdate=datetime.date(2010, 1, 1)\n        )\n        cls.author2 = Author.objects.create(name='Test Author 2')\n        cls.books2 = Book.objects.create(\n            isbn='0987654321', name='Test Book 2', pages=200, rating=4.0,\n            price=Decimal('20.00'), contact=cls.author2, pubdate=datetime.date(2011, 1, 1)\n        )\n        cls.books.authors.add(cls.author, cls.author2)\n        cls.books2.authors.add(cls.author, cls.author2)\n", "    def test_temporal_subtraction(self):\n        with self.assertRaises(FieldError):\n            TemporalSubtraction(F('pubdate'), F('pubdate'))\n", "    def test_conditional_expression(self):\n        Author.objects.create(name='John Doe', age=30)\n        Author.objects.create(name='Jane Doe', age=25)\n        results = Author.objects.annotate(\n            is_adult=Case(\n                When(age__gte=18, then=True),\n                default=False,\n                output_field=fields.BooleanField()\n            )\n        ).filter(is_adult=True)\n        self.assertEqual(list(results), [Author.objects.get(name='John Doe')])\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), contact=Author.objects.create(name='John Doe', age=35),\n            publisher=Publisher.objects.create(name='Apress', num_awards=3),\n            pubdate=datetime.date(2007, 12, 6)\n        )\n        cls.book2 = Book.objects.create(\n            isbn='067232959', name='Sams Teach Yourself Django in 24 Hours',\n            pages=528, rating=3.0, price=Decimal('23.09'), contact=Author.objects.create(name='Jane Doe', age=30),\n            publisher=Publisher.objects.create(name='Sams', num_awards=1),\n            pubdate=datetime.date(2008, 3, 3)\n        )\n", "    def test_window_expression_with_partition_by(self):\n        qs = Employee.objects.annotate(\n            avg_salary=Window(\n                Avg('salary'),\n                partition_by=['department']\n            )\n        )\n        qs = qs.values('department__name').annotate(avg_salary=Avg('avg_salary'))\n        self.assertEqual(qs.count(), 3)\n        self.assertEqual(qs[0]['avg_salary'], 50000)\n        self.assertEqual(qs[1]['avg_salary'], 30000)\n        self.assertEqual(qs[2]['avg_salary'], 75000)\n"], "sample_1124": ["def test_FracElement_from_expr_with_exp():\n    from sympy.polys.fields import FracField, FracElement\n\n    x, y = symbols('x y')\n    f = FracField((x, y), ZZ, lex)\n\n    expr = exp(x) / (y + 1)\n    frac = FracElement.from_expr(expr)\n    assert frac.field == f\n    assert frac.numer == exp(x)\n    assert frac.denom == y + 1\n\n    expr = (x**2 + y**2) / (x + y)\n    frac = FracElement.from_expr(expr)\n    assert frac.field == f\n    assert frac.numer == x**2 + y**2\n    assert frac.denom == x + y\n\n    with XFAIL(\"FracElement.from_expr() does not handle invalid expressions\"):\n        expr = 1 / (x + y)\n        frac = FracElement.from_expr(expr)\n", "def test_fracelelement_hash():\n    from sympy.polys.rings import ring\n    from sympy.polys.domains import ZZ\n    x, y = symbols('x y')\n    field, X, Y = field('x,y', ZZ)\n\n    f = FracElement(X**2, X**3 + 2)\n    assert hash(f) == hash((field, X**2, X**3 + 2))\n", "def test_diff():\n    X, Y, Z = symbols('X Y Z')\n    f = FracElement((X**2 + Y)/(Z + 1), 1)\n    assert f.diff(X).as_expr() == 2*X/(Z + 1)\n    assert f.diff(Y).as_expr() == 1/(Z + 1)\n    assert f.diff(Z).as_expr() == (-2*X*(X + Y))/(Z + 1)**2\n", "def test_field_new():\n    from sympy.polys.rings import ring\n    from sympy.polys.fields import field\n    X, Y = symbols('X Y')\n\n    # test ground_new\n    field = field(\"x,y\", ZZ)\n    K = field()[0]\n    x = K.ground_new(X)\n    assert x.as_expr() == X\n\n    # test field_new\n    field = field(\"x,y\", ZZ)\n    K = field()[0]\n    x = K.field_new(X)\n    assert x.as_expr() == X\n\n    # test ground_new with polynomial ring\n    field = field(\"x,y\", QQ)\n    K = field()[0]\n    x = K.ground_new(X**2 + Y**2)\n    assert x.as_expr() == X**2 + Y**2\n\n    # test field_new with polynomial ring\n    field = field(\"x,y\", QQ)\n    K = field()[0]\n    x = K.field_new(X**2 + Y**2)\n    assert x.as_expr() == X**2 + Y**2\n\n    # test field_new with frac field\n    field = field(\"x,y\", ZZ)\n    K = field()[0]\n    x = K.field_new(X/(Y+1))\n    assert x.as_expr() == X/(Y+1)\n\n    # test ground_new with frac field\n    field = field(\"x,y\", ZZ)\n    K = field()[0]\n    x = K.ground_new(X/(Y+1))\n    assert x.as_expr() == X/(Y+1)\n", "def test_frac_element_pow():\n    from sympy.polys.domains import ZZ, QQ\n    from sympy.polys.fields import FracField, FracElement\n    from sympy.polys.rings import ring\n    from sympy import sqrt\n\n    X, Y = symbols('x y')\n\n    K, x, y = FracField((X, Y), ZZ, lex)\n    f = K(1/X**2 + 2/X**3)\n    g = K(1/X**4 + 2/X**5)\n\n    assert f**2 == FracElement((1/X**4 + 4/X**6 + 4/X**7 + 4/X**8), 1)\n\n    with raises(ZeroDivisionError):\n        f**(-1)\n\n    with raises(TypeError):\n        f**'a'\n\n    # Test negative power\n    f = K(1/X + 2/X**2)\n    assert f**(-1) == FracElement(X**2 + 2*X, 1)\n\n    # Test negative power with negative denominator\n    f = K(1/X - 2/X**2)\n    assert f**(-1) == FracElement(X**2 - 2*X, 1)\n"], "sample_691": ["def test_get_timeout_config_value():\n    \"\"\"Test that get_timeout_config_value returns the correct timeout value.\"\"\"\n    config = Config()\n    config._store = {fault_handler_stderr_key: io.StringIO()}\n\n    # Test with no timeout value\n    assert FaultHandlerHooks.get_timeout_config_value(config) == 0.0\n\n    # Test with timeout value\n    config._store[fault_handler_stderr_key] = io.StringIO(\"timeout=5\")\n    assert FaultHandlerHooks.get_timeout_config_value(config) == 5.0\n\n    # Test with invalid timeout value\n    config._store[fault_handler_stderr_key] = io.StringIO(\"timeout=not_a_number\")\n    assert FaultHandlerHooks.get_timeout_config_value(config) == 0.0\n\n    # Test with faulthandler module enabled\n    config._store[fault_handler_stderr_key] = io.StringIO(\"timeout=5\")\n    import faulthandler\n    faulthandler.is_enabled = lambda: True\n    assert FaultHandlerHooks.get_timeout_config_value(config) == 0.0\n", "def test_faulthandler_configure_existing_faulthandler_configured_before_pytest():\n    \"\"\"Test that configuring faulthandler before pytest does not crash and does not\n    override the faulthandler_timeout option.\"\"\"\n\n    with Pytester(ttemp=True) as t:\n        t.makeini(\"\"\"\n            [pytest]\n            faulthandler_timeout = 0.5\n        \"\"\")\n        t.makeconftest(\"\"\"\n            import faulthandler\n            faulthandler.enable()\n        \"\"\")\n        t.makefile(\"\"\"\n                pass\n        \"\"\")\n        captured = t.runpytest()\n        assert \"faulthandler module enabled before pytest configuration step, 'faulthandler_timeout' option ignored\" in captured.stdout\n", "def test_faulthandler_stderr_duplicate_copy():\n    with pytest.raises(FileExistsError):\n        FaultHandlerHooks().pytest_configure(Pytester().config)\n", "def test_faulthandler_configure_calls_register_method():\n    \"\"\"Ensure that pytest_configure calls register method of FaultHandlerHooks.\"\"\"\n    pytester = Pytester()\n    config = pytester.getconfig()\n\n    class DummyFaultHandlerHooks:\n            assert config.pluginmanager.register.called_once_with(self, \"faulthandler-hooks\")\n\n            pass\n\n            pass\n\n            pass\n\n            pass\n\n    plugin = DummyFaultHandlerHooks()\n    config.pluginmanager.register(plugin, \"faulthandler-hooks\")\n    config.pluginmanager.unregister(plugin)\n\n    with pytest.raises(AttributeError):\n        FaultHandlerHooks().pytest_configure(config)\n\n    config.pluginmanager.register(FaultHandlerHooks(), \"faulthandler-hooks\")\n    FaultHandlerHooks().pytest_configure(config)\n", "def test_faulthandler_configure_configuring_faulthandler_before_pytest_configuration():\n    \"\"\"Test that if faulthandler is enabled before pytest configuration step, it does not\n    override the configuration.\"\"\"\n    with Pytester.assertRaises(pytest.PytestConfigWarning) as warning:\n        pytest.main([ \"--faulthandler\", \"--faulthandler-timeout=5\"])\n        warning.match(\"faulthandler module enabled before pytest configuration step, 'faulthandler_timeout' option ignored\")\n"], "sample_965": ["def test_getannotations():\n    class A:\n        a: int\n        b: str\n\n    assert inspect.getannotations(A) == {'a': int, 'b': str}\n\n    class B:\n            pass\n\n    assert inspect.getannotations(B) == {'__init__': {'a': int, 'b': str}}\n\n    class C:\n        class D:\n            d: int\n            e: str\n\n    assert inspect.getannotations(C.D) == {'d': int, 'e': str}\n\n    class E:\n        @classmethod\n            pass\n\n    assert inspect.getannotations(E) == {'__init__': {'a': int, 'b': str}}\n\n    class F:\n        @staticmethod\n            pass\n\n    assert inspect.getannotations(F) == {'__init__': {'a': int, 'b': str}}\n\n    class G:\n            pass\n\n    class H(G):\n        pass\n\n    assert inspect.getannotations(H) == {'__init__': {'a': int, 'b': str}}\n\n    class I:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(I) == {'a': int, 'b': str}\n\n    class J:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(J()) == {'a': int, 'b': str}\n\n    class K:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(K()) == {'a': int, 'b': str}\n\n    class L:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(L()) == {'a': int, 'b': str}\n\n    class M:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(M()) == {'a': int, 'b': str}\n\n    class N:\n            pass\n\n    assert inspect.getannotations(N) == {'__init__': {'a': int, 'b': str}}\n\n    class O:", "def test_getannotations():\n    class TestEnum(enum.Enum):\n        A = 1\n        B = 2\n\n    class TestClass:\n            self.__annotations__ = {\n                'a': int,\n                'b': str\n            }\n\n            pass\n\n    assert inspect.getannotations(TestEnum) == {}\n    assert inspect.getannotations(TestEnum.A) == {}\n    assert inspect.getannotations(TestClass) == {'a': int, 'b': str}\n    assert inspect.getannotations(TestClass.method) == {'a': int, 'b': str}\n", "def test_is_singledispatch_method():\n    \"\"\"Check is_singledispatch_method()\"\"\"\n    class TestClass:\n        @singledispatch\n            pass\n\n    test_func = TestClass().test_func\n    assert is_singledispatch_method(test_func) is True\n\n    class TestClass2:\n        @singledispatchmethod\n            pass\n\n    try:\n        import functools\n        from functools import singledispatchmethod\n    except ImportError:\n        pytest.skip(\"singledispatchmethod is not available in Python 3.6 and 3.7\")\n\n    test_func = TestClass2().test_func\n    assert is_singledispatch_method(test_func) is True\n\n    test_func = TestClass().test_func\n    assert is_singledispatch_method(test_func) is False\n", "def test_getannotations():\n    class A:\n        pass\n\n    class B(A):\n        __annotations__ = {\"a\": int}\n        a: int\n\n    assert getannotations(B) == {\"a\": int}\n", "def test_getglobals():\n        \"\"\"docstring\"\"\"\n        global z  # type: ignore\n        z = 1\n    assert getglobals(func) == {'x': 1, 'y': 1, 'z': 1}\n"], "sample_875": ["    def test_confusion_matrix_binary_labels(self):\n        y_true = np.array([0, 0, 1, 1])\n        y_pred = np.array([0, 1, 0, 1])\n        conf_mat = confusion_matrix(y_true, y_pred)\n        assert_array_equal(conf_mat, np.array([[2, 0], [0, 2]]))\n", "def test_brier_score_loss_one():\n    # Check that brier score is 0 when y_prob is equal to y_true\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0, 1, 1, 0])\n    assert_allclose(brier_score_loss(y_true, y_prob), 0)\n", "def test_class_likelihood_ratios_binary_to_multiclass():\n    y_true = [0, 1, 0, 1]\n    y_pred = [0, 1, 0, 0]\n\n    with pytest.raises(ValueError):\n        class_likelihood_ratios(y_true, y_pred)\n\n", "def test_fbeta_score_zero_division(pos_label, zero_division):\n    y_true = [0, 1, 2, 0, 1, 2]\n    y_pred = [0, 2, 1, 0, 0, 1]\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", UndefinedMetricWarning)\n        assert_allclose(\n            fbeta_score(y_true, y_pred, beta=1, pos_label=pos_label, zero_division=zero_division),\n            0,\n        )\n    if zero_division != 0:\n        with pytest.raises(UndefinedMetricWarning):\n            fbeta_score(y_true, y_pred, beta=1, pos_label=pos_label, zero_division=zero_division)\n", "def test_hinge_loss_binary():\n    y_true = np.array([-1, 1, 1])\n    pred_decision = np.array([-2, 2, 0.5])\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n        with pytest.raises(TypeError):\n            hinge_loss(y_true, \"not a float\")\n    with pytest.raises(ValueError):\n        hinge_loss(y_true, pred_decision, labels=[0, 1, 3])\n\n    pred_decision = np.array([-1, 1, 2])  # non-normalized\n    with pytest.raises(ValueError):\n        hinge_loss(y_true, pred_decision, labels=[0, 1, 2])\n    with pytest.raises(ValueError):\n        hinge_loss(y_true, pred_decision, labels=[0, 1])\n\n    pred_decision = np.array([-1, 1, 2])  # non-normalized\n    hinge_loss(y_true, pred_decision, labels=[0, 1, 2])\n\n    # Test labels not in y_true\n    hinge_loss(y_true, pred_decision, labels=[0, 3, 2])\n\n    # Test when y_true and pred_decision are not 1D\n    y_true = np.array([[0], [-1]])\n    pred_decision = np.array([[-2], [2]])\n    hinge_loss(y_true, pred_decision)\n\n    # Test multiclass hinge loss\n    y_true = np.array([0, 2, 1])\n    labels = np.array([0, 1, 2])\n    hinge_loss(y_true, np.array([[1, 0, -1], [0, 1, -1], [0, 0, 1]]), labels=labels)\n    hinge_loss(y_true, np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]), labels=labels)\n\n    # Test multiclass hinge loss with y_true not in labels\n    y_true = np.array([0, 2, 1])\n    hinge_loss(y_true, np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]))\n"], "sample_664": ["def test_deprecated_external_plugins():\n    \"\"\"Test the DEPRECATED_EXTERNAL_PLUGINS set.\"\"\"\n    assert DEPRECATED_EXTERNAL_PLUGINS == {\n        \"pytest_catchlog\",\n        \"pytest_capturelog\",\n        \"pytest_faulthandler\",\n    }\n", "def test_deprecated_external_plugins():\n    \"\"\"Test if DEPRECATED_EXTERNAL_PLUGINS has the expected list of plugins\"\"\"\n    assert DEPRECATED_EXTERNAL_PLUGINS == {\n        \"pytest_catchlog\",\n        \"pytest_capturelog\",\n        \"pytest_faulthandler\",\n    }\n", "def test_deprecated_external_plugins():\n    \"\"\"Test that the DEPRECATED_EXTERNAL_PLUGINS set contains only plugins that are integrated into the core.\"\"\"\n    for plugin in DEPRECATED_EXTERNAL_PLUGINS:\n        assert plugin in [\"pytest_catchlog\", \"pytest_capturelog\", \"pytest_faulthandler\"]\n\n", "def test_deprecated_external_plugins():\n    \"\"\"Test the DEPRECATED_EXTERNAL_PLUGINS set\"\"\"\n    assert DEPRECATED_EXTERNAL_PLUGINS == {\n        \"pytest_catchlog\",\n        \"pytest_capturelog\",\n        \"pytest_faulthandler\",\n    }\n", "def test_dep_to_be_removed():\n    with pytest.warns(PytestDeprecationWarning, match=\"The 'junit_family' default value will change\"):\n        deprecated.JUNIT_XML_DEFAULT_FAMILY\n"], "sample_1086": ["def test_PowerPrinter_str():\n    # Test _print_Pow function\n    assert sstr(2**3) == '2**3'\n    assert sstr(2**-3) == '2**-3'\n    assert sstr(2**(3/2)) == '2**(3/2)'\n    assert sstr(sqrt(2)) == 'sqrt(2)'\n    assert sstr(1/sqrt(2)) == '1/sqrt(2)'\n    assert sstr(Pow(x, 1/2)) == 'x**(1/2)'\n    assert sstr(Pow(x, -1/2)) == 'x**(-1/2)'\n    assert sstr(Pow(x, -1)) == '1/x'\n    assert sstr(Pow(x, S.Half)) == 'sqrt(x)'\n    assert sstr(Pow(x, -S.Half)) == '1/sqrt(x)'\n    # Test _print_Pow with rational=True\n    assert sstr(Pow(x, 1/2), rational=True) == 'x**(1/2)'\n", "def test_StrPrinter_MatrixMul():\n    from sympy import Matrix, MatrixSymbol\n\n    M = Matrix([[1, 2], [3, 4]])\n    printer = StrPrinter()\n    assert printer.doprint(M) == \"Matrix([[1, 2], [3, 4]])\"\n\n    M = Matrix([[1, 2], [3, 4]]) * Matrix([[5, 6], [7, 8]])\n    assert printer.doprint(M) == \"Matrix([[19, 22], [43, 50]])\"\n\n    M = Matrix([[1, 2], [3, 4]]) * Matrix([[5, 6], [7, 8]) * Matrix([[9, 10], [11, 12]])\n    assert printer.doprint(M) == \"Matrix([[(9*19 + 10*43), (9*22 + 10*50)], [(7*19 + 8*43), (7*22 + 8*50)]]))\"\n", "def test_StrPrinter__print_MatrixBase():\n    matrix = Matrix([[1, 2], [3, 4]])\n    printer = StrPrinter()\n    assert printer._print(matrix) == str(matrix)\n", "def test_Printer_StrPrinter():\n    # Test _print_Pow\n    assert StrPrinter()._print(Pow(x, 2)) == 'x**2'\n    assert StrPrinter()._print(Pow(x, 3)) == 'x**3'\n    assert StrPrinter()._print(Pow(x, Rational(1, 2))) == 'x**(1/2)'\n    assert StrPrinter()._print(Pow(x, Rational(1, 3))) == 'x**(1/3)'\n    assert StrPrinter()._print(Pow(x, Rational(-1, 2))) == '1/x**2'\n    assert StrPrinter()._print(Pow(x, Rational(-1, 3))) == '1/x**(1/3)'\n\n    # Test _print_Pow with negative exponent\n    assert StrPrinter()._print(Pow(x, -2)) == '1/x**2'\n    assert StrPrinter()._print(Pow(x, -3)) == '1/x**3'\n\n    # Test _print_Pow with rational exponent\n    assert StrPrinter()._print(Pow(x, Rational(1, 2))) == 'x**(1/2)'\n    assert StrPrinter()._print(Pow(x, Rational(2, 3))) == 'x**(2/3)'\n\n    # Test _print_Pow with sympy integer exponent\n    assert StrPrinter()._print(Pow(x, 1)) == 'x'\n    assert StrPrinter()._print(Pow(x, 2)) == 'x**2'\n    assert StrPrinter()._print(Pow(x, 3)) == 'x**3'\n\n    # Test _print_Pow with negated exponent\n    assert StrPrinter()._print(Pow(x, -2)) == '1/x**2'\n    assert StrPrinter()._print(Pow(x, -3)) == '1/x**3'\n\n    # Test _print_Pow with Fraction exponent\n    assert StrPrinter()._print(Pow(x, Rational(1, 2))) == 'x**(1/2)'\n    assert StrPrinter()._print(Pow(x, Rational(2, 3))) == 'x**(2/3)'\n\n    # Test _print_Pow with square root\n    assert StrPrinter()._print(Pow(x, S.Half)) == 'sqrt(x)'\n\n    # Test _print_Pow with exp is negative half\n    assert StrPrinter()._print(Pow(x, -S.Half)) ==", "def test_Cycle():\n    from sympy.combinatorics.permutations import Cycle\n    from sympy.core import Expr\n    from sympy.printing import StrPrinter\n\n    p = StrPrinter()\n    c = Cycle((1, 2, 3))\n    assert p.doprint(c) == 'Cycle((1, 2, 3))'\n\n    c = Cycle((1, 2))\n    assert p.doprint(c) == 'Cycle((1, 2))'\n\n    c = Cycle((1,))\n    assert p.doprint(c) == 'Cycle((1))'\n\n    c = Cycle(())\n    assert p.doprint(c) == '()'\n\n    # Test with string representation\n    c = Cycle([1, 2, 3])\n    assert p.doprint(c) == 'Cycle((1, 2, 3))'\n\n    # Test with an expression that is not a Cycle\n    expr = Expr('x')\n    assert p.doprint(expr) == 'x'\n"], "sample_379": ["    def test_mark_safe_on_callable(self):\n            return \"Hello, <b>world</b>!\"\n\n        marked_hello_world = mark_safe(hello_world)\n        self.assertIsInstance(marked_hello_world, SafeString)\n\n        rendered = Template(\"{% autoescape off %}{{ hello_world }}{% endautoescape %}\").render(Context({'hello_world': marked_hello_world}))\n        self.assertEqual(rendered, \"Hello, <b>world</b>!\")\n", "    def test_mark_safe_callable(self):\n            return \"Hello, <b>World!</b>\"\n\n        self.assertIsInstance(mark_safe(func), SafeString)\n", "    def test_html_representation_of_SafeData(self):\n        safe_data = SafeData()\n        self.assertEqual(safe_data.__html__(), safe_data)\n        ", "    def test_mark_safe_decorator(self):\n        # Test that the mark_safe decorator correctly marks a function's return value as safe.\n        @mark_safe\n            return 'Hello, ' + name + '!'\n\n        self.assertIsInstance(hello('world'), SafeString)\n        self.assertIsInstance(mark_safe(lambda: 'Hello, world!')(), SafeString)\n", "    def assertRenderEqual(self, tpl, expected, **context):\n        context = Context(context)\n        tpl = Template(tpl)\n        self.assertEqual(tpl.render(context), expected)\n"], "sample_599": ["def test_CFScaleOffsetCoder_encode_decode_scale_factor():\n    \"\"\"Test that scale_factor is applied correctly when encoding and decoding.\"\"\"\n    x = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    variable = xr.Variable(\"x\", x, encoding={\"scale_factor\": 2})\n    coder = CFScaleOffsetCoder()\n    encoded = coder.encode(variable)\n    decoded = coder.decode(encoded)\n    assert_identical(decoded.data, xr.DataArray([2, 4, 6], dims=[\"x\"]))\n", "def test_unsigned_integer_coder_encode_decode():\n    da = xr.DataArray(\n        np.array([255, 256], dtype=np.uint8),\n        dims=[\"x\"],\n        attrs={\"_Unsigned\": \"true\", \"_FillValue\": 255},\n    )\n    variable = xr.Variable(\"x\", da)\n\n    coder = UnsignedIntegerCoder()\n    encoded_variable = coder.encode(variable, name=\"test_variable\")\n    decoded_variable = coder.decode(encoded_variable, name=\"test_variable\")\n\n    assert_equal(variable.data, decoded_variable.data)\n    assert_equal(variable.attrs, decoded_variable.attrs)\n    assert_equal(variable.encoding, decoded_variable.encoding)\n", "    def test_unsigned_integer_encode(self):\n        # Test encoding an unsigned integer array\n        arr = np.array([1, 2, 3], dtype=np.uint8)\n        variable = xr.DataArray(arr, dims=[\"x\"])\n        coder = UnsignedIntegerCoder()\n        encoded = coder.encode(variable)\n        assert isinstance(encoded.data, np.ndarray)\n        assert encoded.data.dtype == np.int8\n\n        # Test that the _Unsigned attribute is removed\n        assert \"Unsigned\" not in encoded.attrs\n", "def test_cfmaskcoder_encode_decode_roundtrip():\n    # Test that decoding and encoding a variable with fill values doesn't change the data\n    da = da.from_array(np.arange(4).reshape((2, 2))\n                      , chunks=(2, 2))\n    v = xr.Variable((\"x\", \"y\"), da)\n    v.encoding = {\"_FillValue\": 4, \"missing_value\": 5}\n    v.attrs[\"_FillValue\"] = 4\n    v.attrs[\"missing_value\"] = 5\n    coder = variables.CFMaskCoder()\n    encoded = coder.encode(v)\n    decoded = coder.decode(encoded)\n    assert_identical(v, decoded)\n", "def test_CFMaskCoder_fill_values(\n    fill_value, missing_value, expected_fill_value, expected_missing_value"], "sample_526": ["def test_date_ticker_factory_deprecated():\n    with _api.deprecation_warnings_as_errors():\n        with pytest.raises(UserWarning):\n            mdates.date_ticker_factory(1, tz=\"UTC\", numticks=5)\n", "def test_date_locator_interval_multiples():\n    # Test that interval_multiples=True works as expected for MicrosecondLocator\n    fig, ax = plt.subplots()\n    ax.plot_date(mdates.drange(datetime.datetime(1970, 1, 1),\n                              datetime.datetime(1970, 1, 2),\n                              mdates.timedelta(seconds=1)),\n                 np.random.randn(2))\n    locator = mdates.MicrosecondLocator(interval=5, interval_multiples=True)\n    locator.set_axis(ax.xaxis)\n    ticks = locator()\n    assert ticks.size == 2\n    assert np.allclose(ticks, [0, 5])\n\n    # Test that interval_multiples=True works as expected for AutoDateLocator\n    fig, ax = plt.subplots()\n    ax.plot_date(mdates.drange(datetime.datetime(1970, 1, 1),\n                              datetime.datetime(1970, 1, 1, 12, 0, 0),\n                              mdates.timedelta(hours=1)),\n                 np.random.randn(12))\n    locator = mdates.AutoDateLocator(interval_multiples=True)\n    locator.set_axis(ax.xaxis)\n    ticks = locator()\n    assert ticks.size == 12\n    assert np.allclose(ticks, np.arange(0, 12))\n\n    # Test that interval_multiples=False works as expected for AutoDateLocator\n    fig, ax = plt.subplots()\n    ax.plot_date(mdates.drange(datetime.datetime(1970, 1, 1),\n                              datetime.datetime(1970, 1, 1, 12, 0, 0),\n                              mdates.timedelta(hours=1)),\n                 np.random.randn(12))\n    locator = mdates.AutoDateLocator(interval_multiples=False)\n    locator.set_axis(ax.xaxis)\n    ticks = locator()\n    assert ticks.size == 12\n\n    # Test that interval_multiples=True works as expected for custom AutoDateLocator\n    fig, ax = plt.subplots()\n    ax.plot_date(mdates.drange(datetime.datetime(1970, 1, 1),\n                              datetime.datetime(1970, 1, 1, 12, 0, 0),\n                              mdates.timedelta(hours=1)),\n                 np.random.randn(12))\n    locator = mdates.AutoDateLocator(tz=dateutil.tz.gettz('US/Eastern'),\n                                    interval_multiples=True)\n    locator.set_axis(ax.xaxis)\n    ticks = locator()\n    assert ticks.size ==", "def test_auto_date_formatter_round_trip():\n    # Set default timezone\n    with rc_context(rc={'timezone': 'UTC'}):\n        # Create some dates\n        dates = mdates.drange(datetime.date(2000, 1, 1),\n                             datetime.date(2000, 12, 31),\n                             datetime.timedelta(days=1))\n\n        # Create a figure and axis\n        fig, ax = plt.subplots()\n        ax.plot(dates, np.random.rand(len(dates)))\n\n        # Create an auto date locator and formatter\n        locator = mdates.AutoDateLocator()\n        formatter = mdates.AutoDateFormatter(locator)\n\n        # Set the locator and formatter on the axis\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(formatter)\n\n        # Test that the formatter is working correctly\n        assert formatter.format_ticks(dates) == formatter.format_ticks(dates)\n\n        # Test that the formatter is working correctly with different intervals\n        locator = mdates.AutoDateLocator(interval_multiples=False)\n        formatter = mdates.AutoDateFormatter(locator)\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(formatter)\n        assert formatter.format_ticks(dates) == formatter.format_ticks(dates)\n", "def test_MicrosecondLocator():\n    # Test that MicrosecondLocator creates ticks at the correct interval\n    # and that it does not create ticks outside the view limits\n    locator = mdates.MicrosecondLocator(interval=100)\n\n    fig, ax = plt.subplots()\n    ax.plot_date(mdates.date2num([datetime.datetime(2022, 1, 1),\n                                 datetime.datetime(2022, 1, 2)]),\n                 [1, 2])\n\n    ax.xaxis.set_major_locator(locator)\n    locator.set_axis(ax)\n\n    ticks = locator()\n    assert len(ticks) == 1\n\n    # Test that MicrosecondLocator can handle large dates\n    # without losing precision\n    locator = mdates.MicrosecondLocator(interval=100)\n    locator = mdates.DateLocator(locator)\n    dates = mdates.drange(datetime.datetime(290301, 1, 1),\n                         datetime.datetime(294241, 1, 1),\n                         datetime.timedelta(seconds=1))\n    ax.plot_date(dates, [1] * len(dates))\n    ax.xaxis.set_major_locator(locator)\n    locator.set_axis(ax)\n    ticks = locator()\n    assert len(ticks) == 191816\n\n    # Test that MicrosecondLocator raises a warning when dealing with dates\n    # far from the epoch\n    with rc_context({'dates.epoch': '0000-01-01T00:00:00'}):\n        locator = mdates.MicrosecondLocator(interval=100)\n        locator = mdates.DateLocator(locator)\n        dates = mdates.drange(datetime.datetime(290301, 1, 1),\n                             datetime.datetime(294241, 1, 1),\n                             datetime.timedelta(seconds=1))\n        ax.plot_date(dates, [1] * len(dates))\n        ax.xaxis.set_major_locator(locator)\n        locator.set_axis(ax)\n        with pytest.warns(UserWarning):\n            locator()\n", "def test_date_ticker_factory_deprecation():\n    # Testing that date_ticker_factory raises a DeprecationWarning when used\n    with pytest.warns(FutureWarning):\n        locator, formatter = mdates.date_ticker_factory(10, tz='UTC')\n        assert isinstance(locator, mdates.YearLocator)\n        assert isinstance(formatter, mdates.DateFormatter)\n\n    # Testing that date_ticker_factory returns the correct types when using non-deprecated API\n    with rc_context(rc={'timezone': 'UTC'}):\n        locator, formatter = mdates.AutoDateLocator(tz='UTC'), mdates.AutoDateFormatter(mdates.AutoDateLocator(tz='UTC'))\n        assert isinstance(locator, mdates.AutoDateLocator)\n        assert isinstance(formatter, mdates.AutoDateFormatter)\n"], "sample_636": ["def test_line_set_init_invalid_args():\n    with pytest.raises(TypeError):\n        LineSet(\"name\", 1)\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\")\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_comments=\"str\")\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_docstrings=\"str\")\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_imports=\"str\")\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_signatures=\"str\")\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_comments=1.5)\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_docstrings=1.5)\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_imports=1.5)\n\n    with pytest.raises(TypeError):\n        LineSet(\"name\", \"lines\", ignore_signatures=1.5)\n", "    def test_regexp_for_lines_with_content(self, tmp_path):\n        # Test that the regex correctly identifies lines with content\n        text = \"import os\\nos.system('ls -l')\\n# comment\"\n        with tmp_path / \"file.txt\" as f:\n            f.write(text)\n            f.seek(0)\n            lineset = LineSet(\"file.txt\", [line for line in f.readlines()])\n            assert len(stripped_lines(lineset.stripped_lines, ignore_comments=True, ignore_docstrings=True, ignore_imports=True, ignore_signatures=True)) == 1\n", "    def test_Run(self):\n        with _patch_streams(StringIO()) as out:\n            Run([\"-c\", \"similar\", \"--ignore-comments\", \"--ignore-docstrings\", \"--ignore-imports\", \"--ignore-signatures\", \"data.py\"])\n\n            # Check the messages in the output\n            assert \"Similar lines in 1 files\\n==data.py:[4:6]\\n   def a()  # pass\\n   def a()  # pass\" in out.getvalue()\n            assert \"Similar lines in 1 files\\n==data.py:[4:6]\\n   def a()  # pass\\n   def a()  # pass\" in out.getvalue()\n            assert \"Similar lines in 2 files\\n==data.py:[4:6]\\n   def a()  # pass\\n   def a()  # pass\\n==data2.py:[4:6]\\n   def a()  # pass\\n   def a()  # pass\" not in out.getvalue()\n", "def test_similar_code_checker_report_duplicates_from_regrtest_data():\n    out = StringIO()\n    with _patch_streams(out):\n        Run([\"--output-format=parseable\", join(DATA, \"regrtest.py\")])\n    out.seek(0)\n    lines = out.readlines()\n    expected_duplicates = [\n        \"Similar lines in 2 files\\n==regrtest.py:[31:35]\\n     if a:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[8:11]\\n     if b:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[19:22]\\n     if c:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[13:16]\\n     if d:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[21:24]\\n     if e:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[15:18]\\n     if f:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[27:30]\\n     if g:\\n\",\n        \"Similar lines in 1 file\\n==regrtest.py:[32:35]\\n     if h:\\n\",\n    ]\n    expected_duplicates_str = \"\\n\".join(expected_duplicates)\n    assert all(line in expected_duplicates_str for line in lines)\n    assert len(lines) == len(expected_duplicates) + 1\n    assert lines[-1].startswith(\"TOTAL\")\n", "    def test_similar_code_checker_with_lint_results(self) -> None:\n        \"\"\"Test similar code checker with an existing linter.\"\"\"\n        # Create a temporary stream\n        stream = StringIO()\n\n        # Create a temporary directory\n        tmp_dir = tempfile.mkdtemp()\n        try:\n            # Create a file with a code with similar lines\n            with open(join(tmp_dir, \"file1.py\"), \"w\", encoding=\"utf-8\") as f:\n                f.write(\n                    \"\"\""], "sample_501": ["def test_legend_set_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot(np.random.rand(10))\n    leg = ax.legend()\n    assert leg.get_bbox_to_anchor().width == 1\n    assert leg.get_bbox_to_anchor().height == 1\n\n    # test with a tuple\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    assert leg.get_bbox_to_anchor().x0 == 0.5\n    assert leg.get_bbox_to_anchor().y0 == 0.5\n    assert leg.get_bbox_to_anchor().width == 1\n    assert leg.get_bbox_to_anchor().height == 1\n\n    # test with a BboxBase instance\n    leg.set_bbox_to_anchor(Bbox.from_bounds(0, 0, 2, 2))\n    assert leg.get_bbox_to_anchor().x0 == 0\n    assert leg.get_bbox_to_anchor().y0 == 0\n    assert leg.get_bbox_to_anchor().width == 2\n    assert leg.get_bbox_to_anchor().height == 2\n\n    # test with a None value\n    leg.set_bbox_to_anchor(None)\n    assert leg.get_bbox_to_anchor() is None\n", "def test_get_legend_handler_map():\n    # Test that get_legend_handler_map() correctly returns the default handler map.\n    legend = mlegend.Legend(mpl.Figure(), [], [])\n    handler_map = legend.get_legend_handler_map()\n    assert handler_map == mlegend.Legend._default_handler_map\n\n    # Test that get_legend_handler_map() correctly updates the default handler map.\n    mlegend.Legend.set_default_handler_map({'Line2D': 'custom_handler'})\n    legend = mlegend.Legend(mpl.Figure(), [], [])\n    handler_map = legend.get_legend_handler_map()\n    assert handler_map == {'Line2D': 'custom_handler', **mlegend.Legend._default_handler_map}\n\n    # Test that get_legend_handler_map() returns the custom handler map if provided.\n    legend = mlegend.Legend(mpl.Figure(), [], [])\n    legend._custom_handler_map = {'Line2D': 'custom_handler'}\n    handler_map = legend.get_legend_handler_map()\n    assert handler_map == {'Line2D': 'custom_handler', **mlegend.Legend._default_handler_map}\n\n    # Test that get_legend_handler_map() returns None if custom_handler_map is None.\n    legend = mlegend.Legend(mpl.Figure(), [], [])\n    legend._custom_handler_map = None\n    handler_map = legend.get_legend_handler_map()\n    assert handler_map == mlegend.Legend._default_handler_map\n\n    # Test that get_legend_handler_map() correctly returns the default handler map\n    # if custom_handler_map is None and Legend._default_handler_map is not None.\n    mlegend.Legend._default_handler_map = {'Line2D': 'custom_handler'}\n    legend = mlegend.Legend(mpl.Figure(), [], [])\n    handler_map = legend.get_legend_handler_map()\n    assert handler_map == {'Line2D': 'custom_handler'}\n", "def test_Legend_get_legend_handler_map():\n    # Test that the default handler map is a copy of the internal default handler map\n    legend = mlegend.Legend(None, [], [])\n    default_map = legend.get_default_handler_map()\n    legend_map = legend.get_legend_handler_map()\n    assert default_map is not legend_map\n    assert default_map == legend_map\n\n    # Test that the custom handler map is a copy of the internal custom handler map\n    custom_map = legend._custom_handler_map = {'test': 'handler'}\n    legend_map = legend.get_legend_handler_map()\n    assert custom_map == legend_map\n\n    # Test that the default handler map is updated when the custom handler map is updated\n    legend.update_default_handler_map({'test': 'handler'})\n    legend_map = legend.get_legend_handler_map()\n    assert 'test' in legend_map\n\n    # Test that the default handler map is not updated when the custom handler map is updated\n    legend._custom_handler_map['test'] = 'handler'\n    legend_map = legend.get_legend_handler_map()\n    assert 'test' in legend_map\n", "def test_DraggableLegend_update_bbox_to_anchor():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    dr = mlegend.DraggableLegend(ax.legend(), update=\"bbox\")\n    dr._update_bbox_to_anchor((1, 1))\n    assert dr.legend.get_bbox_to_anchor() == (1, 1)\n    assert dr.legend.get_bbox_to_anchor().width == 1\n    assert dr.legend.get_bbox_to_anchor().height == 1\n    dr._update_bbox_to_anchor((2, 2))\n    assert dr.legend.get_bbox_to_anchor() == (2, 2)\n    assert dr.legend.get_bbox_to_anchor().width == 1\n    assert dr.legend.get_bbox_to_anchor().height == 1\n", "def test_init_legend_box_custom_handler():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.set_title('Test Legend')\n    legend = mlegend.Legend(ax, [mlines.Line2D([0], [0])], ['Line'])\n    with rc_context({'legend.frameon': False}):\n        legend._init_legend_box([mlines.Line2D([0], [0])], ['Line'],\n                               markerfirst=False)\n\n    # Check if legend's frame is not drawn\n    assert not legend.get_frame_on()\n\n    # Test custom handler\n    with rc_context({'legend.frameon': True}):\n        # Create custom handler\n        class CustomHandler(mllegend.HandlerBase):\n                self.legend_artist = mock.Mock()\n\n                self.legend_artist.assert_called_once_with(legend, orig_handle,\n                                                         fontsize, offset)\n                return [mlines.Line2D([0], [0])]\n\n        # Set custom handler\n        legend_handler_map = {'Line2D': CustomHandler()}\n        legend._init_legend_box([mlines.Line2D([0], [0])], ['Line'],\n                               markerfirst=False, handler_map=legend_handler_map)\n\n        # Check if custom handler is used\n        assert legend.legendHandles[0] == legend.legendHandles[0]\n\n    # Test error handling for invalid handler\n    with rc_context({'legend.frameon': True}):\n        legend_handler_map = {'Line2D': 'InvalidHandler'}\n        with pytest.raises(TypeError):\n            legend._init_legend_box([mlines.Line2D([0], [0])], ['Line'],\n                                   markerfirst=False, handler_map=legend_handler_map)\n"], "sample_93": ["    def test_window_frame_rows(self):\n        Author.objects.annotate(window=Window(F('age'), partition_by=['name'], order_by=['age'], frame=RowRange(2, 5))).values('window')\n        self.assertEqual(len(connection.queries), 1)\n        self.assertIn('WINDOW', connection.queries[0]['sql'])\n", "    def test_window_expression(self):\n        Author.objects.create(name='John Doe', age=25)\n        authors = Author.objects.annotate(\n            avg_age=Window(\n                Avg('age'),\n                partition_by=ExpressionList('name'),\n            )\n        )\n        self.assertEqual(len(authors), 3)\n        self.assertEqual(authors[0].avg_age, None)\n        self.assertEqual(authors[1].avg_age, 35.0)\n        self.assertEqual(authors[2].avg_age, None)\n", "    def test_window_expression_partition_by(self):\n        with CaptureQueriesContext() as context:\n            query = Book.objects.annotate(\n                max_rating=Window(\n                    Max('rating'),\n                    partition_by=OuterRef('id'),\n                    order_by=F('rating').desc(),\n                )\n            ).values('id', 'max_rating')\n            result = list(query)\n            self.assertEqual(len(context.captured_queries), 1)\n            self.assertEqual(list(query), [\n                {'id': 1, 'max_rating': Decimal('4.5')},\n                {'id': 2, 'max_rating': Decimal('3.0')},\n                {'id': 3, 'max_rating': Decimal('4.0')},\n                {'id': 4, 'max_rating': Decimal('4.0')},\n                {'id': 5, 'max_rating': Decimal('4.0')},\n                {'id': 6, 'max_rating': Decimal('5.0')},\n            ])\n", "    def test_expression_equality(self):\n        # Check equality of expressions\n        self.assertEqual(2 + F('foo'), F('foo') + 2)\n        self.assertEqual(F('foo') + 2, 2 + F('foo'))\n", "    def test_case_expression(self):\n        # Test simple CaseExpression with two When objects.\n        queryset = Book.objects.annotate(\n            rating_value=Case(\n                When(rating__gt=4, then=Value(5)),\n                When(rating__lt=2, then=Value(1)),\n                default=Value(3),\n            )\n        )\n        self.assertEqual(list(queryset.values_list('rating_value', flat=True)), [5, 5, 5, 3, 5, 5, 3, 5, 3, 5])\n"], "sample_66": ["    def test_get_signed_cookie(self):\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer('salt').sign('value')\n        self.assertEqual(request.get_signed_cookie('signed_cookie', default=None, salt='salt'), 'value')\n        self.assertIsNone(request.get_signed_cookie('unsigned_cookie'))\n        self.assertEqual(request.get_signed_cookie('unsigned_cookie', default='default_value'), 'default_value')\n        with self.assertRaises(RawPostDataException):\n            request.get_signed_cookie('unsigned_cookie', default=RAISE_ERROR)\n", "    def test_get_host_with_x_forwarded_host(self):\n        request = HttpRequest()\n        request.META = {\n            'HTTP_X_FORWARDED_HOST': 'example.com',\n        }\n        self.assertEqual(request.get_host(), 'example.com')\n", "    def test_urlencode_with_safe(self):\n        q = QueryDict(mutable=True)\n        q['next'] = '/a&b/'\n        encoded = q.urlencode(safe='/')\n        self.assertEqual(encoded, 'next=/a%26b/')\n        self.assertIsInstance(encoded, str)\n", "    def test_query_dict_copy(self):\n        q = QueryDict(mutable=True)\n        q['a'] = '1'\n        q['b'] = '2'\n        q_copy = q.copy()\n        self.assertNotSameDict(q, q_copy)\n        q_copy['c'] = '3'\n        self.assertNotSameDict(q, q_copy)\n", "    def test_empty_get_data_after_post_parse_error(self):\n        \"\"\"\n        Test that GET data is empty after a POST parsing error.\n        \"\"\"\n        request = HttpRequest()\n        request.method = 'POST'\n        request.content_type = 'application/x-www-form-urlencoded'\n        request.body = b'invalid_data'\n        \n        try:\n            request.parse_file_upload(request.META, request)\n        except MultiPartParserError:\n            self.assertEqual(request.GET, QueryDict())\n"], "sample_557": ["def test_figure_image(dpi):\n    fig = Figure(figsize=(10, 10), dpi=dpi)\n    fig.figimage(np.random.rand(10, 10), cmap='viridis')\n    plt.show()\n", "def test_figure_savefig():\n    fig = Figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    ax.plot([1, 2, 3])\n    fig.savefig('test_savefig.png', bbox_inches='tight', pad_inches=0.1)\n\n    # Test that the figure was saved to the specified file\n    assert Path('test_savefig.png').is_file()\n\n    # Test that the file has the correct DPI\n    with Image.open('test_savefig.png') as img:\n        assert img.info['dpi'] == (fig.dpi, fig.dpi)\n\n    # Test that the file has the correct bounding box\n    with Image.open('test_savefig.png') as img:\n        assert img.size == (8 * fig.dpi, 6 * fig.dpi)\n        assert img.size == (8 * fig.dpi, 6 * fig.dpi)\n\n    # Test that the file has the correct metadata\n    with Image.open('test_savefig.png') as img:\n        assert img.info['tEXtCreator'] == 'Matplotlib'\n\n    # Test that the file has the correct orientation\n    with Image.open('test_savefig.png') as img:\n        assert img.size == (8 * fig.dpi, 6 * fig.dpi)\n\n    # Clean up the test file\n    Path('test_savefig.png').unlink()\n", "def test_set_canvas(tmp_path):\n    # Create a figure\n    fig = Figure(figsize=(6, 6))\n    canvas = FigureCanvasAgg(fig)\n    assert fig.canvas is None\n\n    # Set the canvas\n    fig.set_canvas(canvas)\n    assert fig.canvas is canvas\n\n    # Test that the canvas is not None when figure is pickled\n    buf = io.BytesIO()\n    pickle.dump(fig, buf)\n    unpickled_fig = pickle.load(buf)\n    assert unpickled_fig.canvas is canvas\n\n    # Test that the canvas is not None when figure is shown\n    fig.show()\n    assert fig.canvas is canvas\n\n    # Test that the canvas is not None when figure is saved\n    fig.savefig(tmp_path / 'test.png')\n    assert fig.canvas is canvas\n\n    # Test that the canvas is not None when figure is drawn\n    fig.draw()\n    assert fig.canvas is canvas\n", "def test_figure_pickle():\n    fig = plt.figure()\n    fig.set_size_inches(10, 5)\n    ax1 = fig.add_subplot(1, 1, 1)\n    ax1.plot([1, 2, 3])\n\n    with pytest.raises(TypeError):\n        pickle.dumps(fig)\n\n    # Test figure without canvas\n    fig.canvas = None\n    with pytest.raises(TypeError):\n        pickle.dumps(fig)\n\n    # Test figure with layout engine\n    fig.set_layout_engine('tight')\n    with pytest.raises(TypeError):\n        pickle.dumps(fig)\n\n    # Test figure with non-serializable attributes\n    fig.non_serializable_attr = object()\n    with pytest.raises(TypeError):\n        pickle.dumps(fig)\n\n    # Test figure with picklable attributes\n    fig.pylab_restored = False\n\n    # Test figure with legend\n    ax2 = fig.add_subplot(1, 1, 1)\n    fig.legend()\n\n    # Test figure with tight layout\n    fig.tight_layout()\n\n    # Test figure with constrained layout\n    fig.set_layout_engine('constrained')\n\n    # Test figure with subplots\n    fig.subplots(2, 2)\n\n    # Test figure with subfigures\n    fig.subfigures(2, 2)\n\n    # Test figure with tight bounding box\n    bbox = fig.get_tightbbox(fig.canvas.get_renderer())\n\n    # Test figure with layout engine that is not compatible with tight layout\n    fig.set_layout_engine(PlaceHolderLayoutEngine(True, False))\n    with pytest.raises(RuntimeError):\n        fig.tight_layout()\n\n    # Test figure with layout engine that is not compatible with constrained layout\n    fig.set_layout_engine(PlaceHolderLayoutEngine(True, False))\n    with pytest.raises(RuntimeError):\n        fig.set_layout_engine('constrained')\n", "def test_figure_clear_with_keep_observers():\n    fig = plt.figure()\n    ax1 = fig.add_subplot(111)\n    ax2 = fig.add_subplot(221)\n\n        assert ax in observers\n\n        assert ax not in observers\n\n        assert ax not in fig.axes\n\n    fig.clear(keep_observers=True)\n\n    assert len(fig._axobservers) == 1\n    check_kept_ax(fig._axobservers, ax1)\n    check_removed_ax(fig._axobservers, ax2)\n\n    check_ax_removed_from_figure(ax1)\n    check_ax_removed_from_figure(ax2)\n\n    fig.clear(keep_observers=False)\n\n    assert len(fig._axobservers) == 0\n    check_removed_ax(fig._axobservers, ax1)\n    check_removed_ax(fig._axobservers, ax2)\n\n    check_ax_removed_from_figure(ax1)\n    check_ax_removed_from_figure(ax2)\n"], "sample_269": ["    def test_get_context_data(self):\n        \"\"\"Test get_context_data method.\"\"\"\n        request = RequestFactory().get('/path/to/url')\n        view = JavaScriptCatalog()\n        context = view.get_context_data()\n        self.assertIn('catalog', context)\n        self.assertIn('formats', context)\n        self.assertIn('plural', context)\n", "    def _get_catalog(self):\n        \"\"\"Return a JavaScriptCatalog instance.\"\"\"\n        return JavaScriptCatalog()\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.url = reverse('js_catalog', args=['en'])\n", "    def test_get(self):\n        # Check that the view returns a 404 when locale is not set in the URL.\n        response = self.client.get(reverse('jsi18n'))\n        self.assertEqual(response.status_code, 404)\n", "    def _get_request(self, url=None, lang_code=None, packages=None):\n        \"\"\"Return a Django request object.\"\"\"\n        request_factory = RequestFactory()\n        if url is None:\n            url = reverse('javascript-catalog')\n        if lang_code is not None:\n            request_factory.POST = {LANGUAGE_QUERY_PARAMETER: lang_code}\n        if packages is not None:\n            request_factory.GET = {'packages': packages}\n        request = request_factory.get(url)\n        return request\n"], "sample_404": ["    def test_create_token(self):\n        lexer = Lexer(\"{{ test }}\")\n        tokens = lexer.tokenize()\n        self.assertEqual(len(tokens), 1)\n        self.assertEqual(tokens[0].token_type, TokenType.VAR)\n        self.assertEqual(tokens[0].contents, \"test\")\n        self.assertEqual(tokens[0].lineno, 1)\n", "    def setUp(self):\n        self.engine = self._engine()\n", "    def test_constant_string(self):\n        # Test that constant strings are correctly tokenized\n        template = Template(\"{% if _('Hello') == 'World' %}\")\n        tokens = template.nodelist[0].nodelist[0].token.split_contents()\n        self.assertEqual(tokens, [\"_('Hello') == 'World'\"])\n", "    def test_tokenize_verbatim_block(self):\n        template_string = \"{% verbatim %}Hello{% endverbatim %}\"\n        lexer = Lexer(template_string)\n        self.assertEqual(len(lexer.tokenize()), 4)\n        self.assertEqual(lexer.tokenize()[0].token_type, TokenType.TEXT)\n        self.assertEqual(lexer.tokenize()[1].token_type, TokenType.BLOCK)\n        self.assertEqual(lexer.tokenize()[2].token_type, TokenType.TEXT)\n        self.assertEqual(lexer.tokenize()[3].token_type, TokenType.BLOCK)\n", "    def test_filter_expression_compile(self):\n        template_string = '{% load some_library %}{{ my_filter|arg1:\"arg1 value\" arg2=\"arg2 value\" %}'\n        template = Template(template_string)\n        parser = Parser(template.tokenize(), libraries={\"some_library\": {\"tags\": {\"my_filter\": lambda x, y: x}}})\n        filter_expression = parser.compile_filter(template_string.split(VARIABLE_TAG_END)[1])\n        self.assertEqual(filter_expression.filters, [(parser.find_filter(\"my_filter\"), [(True, Variable(\"arg1:value\")), (False, Variable(\"arg1 value\")), (True, Variable(\"arg2:value\"))])])\n"], "sample_325": ["    def test_build_widget_attrs_copying_attributes(self):\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        field = form['first_name']\n        attrs = {'class': 'form-control'}\n        attrs_copy = field.build_widget_attrs(attrs)\n        self.assertEqual(attrs, attrs_copy)\n", "    def test_bound_field_init_method(self):\n        person = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2022-01-01'})\n        bound_field = person['first_name']\n        self.assertEqual(bound_field.form, person)\n        self.assertEqual(bound_field.field, person.fields['first_name'])\n        self.assertEqual(bound_field.name, 'first_name')\n        self.assertEqual(bound_field.html_name, 'first_name')\n        self.assertEqual(bound_field.html_initial_name, 'first_name')\n        self.assertEqual(bound_field.html_initial_id, '')\n        self.assertEqual(bound_field.label, 'First name')\n        self.assertEqual(bound_field.help_text, '')\n", "    def test_bound_field_auto_id(self):\n        form = PersonNew()\n        field = form['first_name']\n        self.assertEqual(field.auto_id, 'first_name_id')\n", "    def test_bound_field_widget_attrs(self):\n        form = PersonNew()\n        field = form.fields['first_name']\n        bound_field = BoundField(form, field, 'first_name')\n        attrs = {}\n        result = bound_field.build_widget_attrs(attrs)\n        self.assertEqual(result, attrs)\n        attrs['required'] = True\n        result = bound_field.build_widget_attrs(attrs, field.widget)\n        self.assertEqual(result, {'required': True})\n        field.required = False\n        result = bound_field.build_widget_attrs(attrs, field.widget)\n        self.assertEqual(result, {})\n        field.disabled = True\n        result = bound_field.build_widget_attrs(attrs, field.widget)\n        self.assertEqual(result, {'disabled': True})\n", "    def test_bound_field_label_tag(self):\n        form = Person()\n        field = BoundField(form, form.fields['first_name'], 'first_name')\n        label_tag = field.label_tag()\n        self.assertEqual(label_tag, mark_safe('<label>First name</label>'))\n        \n        field.label = 'Custom label'\n        label_tag = field.label_tag()\n        self.assertEqual(label_tag, mark_safe('<label>Custom label</label>'))\n\n        field.field.required = True\n        field.form.label_suffix = ':'\n        label_tag = field.label_tag(label_suffix='')\n        self.assertEqual(label_tag, mark_safe('<label>Custom label:</label>'))\n\n        field.field.required = False\n        field.form.label_suffix = ':'\n        label_tag = field.label_tag(label_suffix='')\n        self.assertEqual(label_tag, mark_safe('Custom label'))\n"], "sample_703": ["def test_not_expression():\n        return x == \"True\"\n\n    assert evaluate(\"not True\", matcher) == False\n    assert evaluate(\"not False\", matcher) == True\n", "def test_empty_expression():\n    \"\"\"Test empty expression.\"\"\"\n    assert evaluate(\"\", lambda x: True) is False\n", "def test_compile_invalid_syntax():\n    \"\"\"Test that invalid syntax raises a ParseError.\"\"\"\n    with pytest.raises(ParseError):\n        evaluate(\"a and\", lambda x: True)\n", "def test_compile_multiple_or():\n    matcher = lambda x: x == \"foo\"\n    assert evaluate(\"foo or bar\", matcher) == True\n    assert evaluate(\"foo and bar or baz\", matcher) == True\n    assert evaluate(\"foo or not bar\", matcher) == True\n", "def test_matcher_adapter():\n        return str.startswith('a')\n\n    adapter = MatcherAdapter(matcher)\n    assert adapter['a' + IDENT_PREFIX] == True\n    with pytest.raises(TypeError):\n        list(adapter)\n    with pytest.raises(TypeError):\n        len(adapter)\n"], "sample_146": ["def test_check_setting_language_code_valid(self):\n    with override_settings(LANGUAGE_CODE='en'):\n        result = check_setting_language_code([], None)\n        self.assertEqual(result, [])\n", "    def test_language_code_re(self):\n        # Test that language_code_re correctly matches language codes.\n        self.assertTrue(language_code_re.match('en'))\n        self.assertTrue(language_code_re.match('mas'))\n        self.assertTrue(language_code_re.match('sgn-ase'))\n        self.assertTrue(language_code_re.match('fr-CA'))\n        self.assertTrue(language_code_re.match('es-419'))\n        self.assertTrue(language_code_re.match('zh-Hans'))\n        self.assertTrue(language_code_re.match('ca-ES-valencia'))\n\n        # Test that language_code_re correctly does not match invalid language codes.\n        self.assertFalse(language_code_re.match(None))\n        self.assertFalse(language_code_re.match(123))\n        self.assertFalse(language_code_re.match(b'en'))\n        self.assertFalse(language_code_re.match('e\u00fc'))\n        self.assertFalse(language_code_re.match('en_US'))\n        self.assertFalse(language_code_re.match('en--us'))\n        self.assertFalse(language_code_re.match('-en'))\n        self.assertFalse(language_code_re.match('en-'))\n        self.assertFalse(language_code_re.match('en-US.UTF-8'))\n        self.assertFalse(language_code_re.match('en_US.UTF-8'))\n        self.assertFalse(language_code_re.match('ca_ES@valencia'))\n        self.assertFalse(language_code_re.match('sr@latin'))\n", "    def setUp(self):\n        self.valid_tags = (\n            'en',              # language\n            'mas',             # language\n            'sgn-ase',         # language+extlang\n            'fr-CA',           # language+region\n            'es-419',          # language+region\n            'zh-Hans',         # language+script\n            'ca-ES-valencia',  # language+region+variant\n            # FIXME: The following should be invalid:\n            'sr@latin',        # language+script\n        )\n        self.invalid_tags = (\n            None,              # invalid type: None.\n            123,               # invalid type: int.\n            b'en',             # invalid type: bytes.\n            'e\u00fc',              # non-latin characters.\n            'en_US',           # locale format.\n            'en--us',          # empty subtag.\n            '-en',             # leading separator.\n            'en-',             # trailing separator.\n            'en-US.UTF-8',     # language tag w/ locale encoding.\n            'en_US.UTF-8',     # locale format - language w/ region and encoding.\n            'ca_ES@valencia',  # locale format - language w/ region and variant.\n            # FIXME: The following should be invalid:\n            # 'sr@latin',      # locale instead of language tag.\n        )\n", "    def test_check_language_code_with_empty_value(self):\n        \"\"\"Error if LANGUAGE_CODE setting is empty.\"\"\"\n        with override_settings(LANGUAGE_CODE=''):\n            self.assertEqual(len(check_setting_language_code([])), 1)\n            self.assertIn(E001, check_setting_language_code([]))\n", "    def test_setting_languages_bidi_invalid_value(self):\n        \"\"\"Test LANGUAGES_BIDI setting is invalid.\"\"\"\n        with override_settings(LANGUAGES_BIDI=['invalid']):\n            check_setting_languages_bidi([self.config])\n            self.assertIn(E003, check_setting_languages_bidi([self.config])[0].__dict__)\n"], "sample_50": ["    def test_runshell_db_sigint_handler(self):\n        \"\"\"\n        Test if the SIGINT signal is properly restored after subprocess.run.\n        \"\"\"\n            self.subprocess_args = list(*args)\n            self.pgpassword = env.get('PGPASSWORD')\n            return subprocess.CompletedProcess(self.subprocess_args, 0)\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            original_sigint_handler = signal.signal(signal.SIGINT)\n            DatabaseClient.runshell_db({'host': 'localhost', 'database': 'test'})\n            self.assertEqual(signal.signal(signal.SIGINT), original_sigint_handler)\n", "    def test_runshell_db_empty_params(self):\n        \"\"\"\n        Test that runshell_db raises an error when given an empty connection params dictionary.\n        \"\"\"\n        with self.assertRaises(KeyError):\n            DatabaseClient.runshell_db({})\n        with self.assertRaises(KeyError):\n            DatabaseClient.runshell_db({'host': ''})\n", "    def test_runshell_db_invalid_conn_params(self):\n        # Test case to check that runshell_db handles invalid connection parameters.\n        dbinfo = {'host': 'localhost', 'database': 'mydb'}\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db(dbinfo)\n\n        dbinfo = {'host': 'localhost', 'port': 'non-numeric', 'database': 'mydb'}\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db(dbinfo)\n\n        dbinfo = {'host': 'localhost', 'user': 'myuser', 'password': 'mypassword', 'database': 'mydb'}\n        with self.assertRaises(subprocess.CalledProcessError):\n            DatabaseClient.runshell_db(dbinfo)\n", "    def test_runshell_db_sigint_handler(self):\n        \"\"\"\n        Test that the SIGINT handler is restored after running psql command.\n        \"\"\"\n            subprocess.signal(signal.SIGINT, signal.SIG_IGN)\n            return subprocess.CompletedProcess(args, 0)\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            DatabaseClient.runshell_db({'user': 'test_user', 'host': 'localhost', 'database': 'test_db', 'port': 5432})\n        self.assertEqual(signal.getsignal(signal.SIGINT), signal.SIG_IGN)\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n", "    def test_runshell_db_saves_original_sigint_handler(self):\n        \"\"\"\n        That function checks that the original SIGINT handler is restored after running the runshell_db method.\n        \"\"\"\n            raise AssertionError('Should not be called')\n\n            subprocess.CompletedProcess(self.subprocess_args, 0)\n\n        sigint_handler = signal.getsignal(signal.SIGINT)\n        with mock.patch('subprocess.run', new=_mock_subprocess_run):\n            signal.signal(signal.SIGINT, original_sigint_handler)\n            DatabaseClient.runshell_db({'database': 'test_db', 'user': 'test_user', 'password': 'test_password'})\n        self.assertEqual(signal.getsignal(signal.SIGINT), sigint_handler)\n"], "sample_97": ["    def test_cache_persistence(self):\n        filename = self.temporary_file('test_module.py')\n        with open(filename, 'w') as f:\n            f.write('pass')\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(filename)\n        with open(filename, 'w') as f:\n            f.write('pass 2')\n        self.clear_autoreload_caches()\n        self.assertFileFound(filename)\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().misses, 0)\n", "    def setUp(self):\n        self.client_mock = mock.patch('watchman.client.client').start()\n        self.client_mock.query.return_value = {'version': '4.9.0'}\n        self.client_mock.capabilityCheck.return_value = {'version': '4.9.0'}\n        self.client_mock.getSubscription.return_value = [{'subscription': 'test_subscription', 'files': ['test_file.py']}, {}]\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        ticker = reloader.tick()\n        self.assertIsInstance(ticker, types.GeneratorType)\n        self.assertEqual(ticker.__next__(), None)\n        reloader.stop()\n        self.assertRaises(StopIteration, ticker.__next__)\n", "    def test_iter_modules_and_files_extra_globs(self):\n        temp_dir = self.temporary_file('temp_dir')\n        temp_file = self.temporary_file('temp_file.py')\n        temp_file.write_text('import os')\n        temp_file.parent.mkdir(parents=True)\n        \n        with extend_sys_path(temp_dir):\n            with mock.patch('sys.path_importer_cache'):\n                self.import_and_cleanup('temp_file')\n                self.assertFileFound(temp_file)\n        \n        self.clear_autoreload_caches()\n        self.assertFileNotFound(temp_file)\n        \n        autoreload.BaseReloader().directory_globs[temp_dir] = ['**/*.py']\n        self.clear_autoreload_caches()\n        self.assertFileFound(temp_file)\n        \n        autoreload.BaseReloader().directory_globs.pop(temp_dir, None)\n        autoreload.BaseReloader().directory_globs[temp_dir] = ['temp_file.py']\n        self.clear_autoreload_caches()\n        self.assertFileFound(temp_file)\n        \n        autoreload.BaseReloader().directory_globs.pop(temp_dir, None)\n        self.clear_autoreload_caches()\n        self.assertFileNotFound(temp_file)\n        \n        autoreload.BaseReloader().directory_globs[temp_dir] = ['**/*.py', 'temp_file.py']\n        self.clear_autoreload_caches()\n        self.assertFileFound(temp_file)\n        \n        autoreload.BaseReloader().directory_globs.pop(temp_dir, None)\n        self.clear_autoreload_caches()\n        self.assertFileFound(temp_file)\n", "    def test_tick_no_changes(self):\n        reloader = autoreload.StatReloader()\n        # Create a temporary file and add it to sys.path\n        filename = self.temporary_file('test.py')\n        with open(filename, 'w') as f:\n            f.write('import os')\n        sys.path.append(str(filename))\n        self.import_and_cleanup('test')\n\n        # Set the file's modification time to a low value so it's picked up immediately\n        os.utime(filename, (0, 0))\n\n        with mock.patch.object(time, 'sleep') as mock_sleep:\n            reloader.tick()\n            reloader.tick()\n            self.assertEqual(mock_sleep.call_count, 2)\n"], "sample_67": ["    def test_fields_for_model_with_fields(self):\n        class ArticleForm(forms.ModelForm):\n            class Meta:\n                model = Article\n                fields = ['title', 'content']\n\n        fields = fields_for_model(Article, fields=['title', 'content'])\n        self.assertEqual(len(fields), 2)\n        self.assertEqual(set(fields.keys()), set(['title', 'content']))\n", "    def test_modelform_factory_fields_exclude(self):\n        form = modelform_factory(model=Article, fields=['title', 'content'])\n        self.assertEqual(form._meta.fields, ['title', 'content'])\n", "    def test_modelform_factory_formfield_callback(self):\n        with self.assertRaises(TypeError):\n            modelform_factory(model=Article, form=form_class=ModelForm, formfield_callback='invalid_callback')\n\n        class CallbackModelForm(ModelForm):\n            class Meta:\n                model = Article\n                fields = '__all__'\n                formfield_callback = 'invalid_callback'\n\n        with self.assertRaises(TypeError):\n            CallbackModelForm()\n\n        class CallbackModelForm2(ModelForm):\n            class Meta:\n                model = Article\n                fields = '__all__'\n                formfield_callback = lambda x: 'invalid_formfield'\n\n        with self.assertRaises(TypeError):\n            CallbackModelForm2()\n\n        class CallbackModelForm3(ModelForm):\n            class Meta:\n                model = Article\n                fields = '__all__'\n                formfield_callback = lambda x: x\n\n        form = CallbackModelForm3()\n        self.assertTrue(modelform_defines_fields(form))\n        self.assertEqual(form.base_fields, fields_for_model(Article, form.fields, form._meta.exclude))\n", "    def test_modelform_factory_with_str_fields_value(self):\n        form = modelform_factory(model=Writer, fields='name')\n        self.assertEqual(form.Meta.fields, ['name'])\n", "    def test_empty_meta_fields(self):\n        with self.assertRaises(ImproperlyConfigured):\n            class EmptyMetaModelForm(forms.ModelForm):\n                class Meta:\n                    model = Writer\n                pass\n"], "sample_1196": ["def test_contains_invalid_input():\n    # Test invalid input type for x\n    x = Symbol('x')\n    s = S.Naturals\n    with raises(TypeError, match='expecting Set, not Symbol'):\n        Contains(x, s)\n\n    # Test invalid input type for S\n    x = S.Symbol('x')\n    s = 1\n    with raises(TypeError, match='expecting Set, not int'):\n        Contains(x, s)\n\n    # Test invalid input type for both x and S\n    x = 1\n    s = 'set'\n    with raises(TypeError, match='expecting Set, not str'):\n        Contains(x, s)\n", "def test_contains_eval_not_set():\n    # Test that eval raises TypeError when the argument is not a Set\n    i = Symbol('i', integer=True)\n    raises(TypeError, lambda: Contains(i, 'not a set'))\n", "def test_contains_eval_set():\n    x = Symbol('x')\n    s = Set(x**2, x > 0)\n    assert Contains(x, s).eval() == Contains(x, s)\n", "def test_Contains_contains_element():\n    x = S.Symbol('x')\n    s = S.Integers\n    assert Contains(x, s).eval(x, s) == S.true\n    s = FiniteSet(1, 2, 3)\n    assert Contains(x, s).eval(x, 1) == S.true\n    assert Contains(x, s).eval(x, 2) == S.true\n    assert Contains(x, s).eval(x, 3) == S.true\n    assert Contains(x, s).eval(x, 4) == S.false\n    s = S.Integers\n    assert Contains(x, s).eval(x, oo) == S.false\n    s = S.Reals\n    assert Contains(x, s).eval(x, 2) == S.true\n    s = S.Naturals\n    assert Contains(x, s).eval(x, 0) == S.false\n    s = S.Naturals\n    assert Contains(x, s).eval(x, 1) == S.true\n    s = S.Naturals0\n    assert Contains(x, s).eval(x, 0) == S.true\n", "def test_contains_binary_symbols():\n    x = Symbol('x', integer=True)\n    y = Symbol('y', integer=True)\n    s = S.Naturals\n    contains = Contains(x, s)\n\n    assert contains.binary_symbols == set()\n\n    contains2 = Contains(Eq(x, y), s)\n    assert contains2.binary_symbols == set()\n\n    contains3 = Contains(Ne(x, y), s)\n    assert contains3.binary_symbols == {x, y}\n"], "sample_890": ["def testSequentialFeatureSelector_backward_selection_with_negative_tol():\n    \"\"\"Test the behavior of backward selection with a negative tolerance.\"\"\"\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    estimator = LinearRegression()\n    sfs = SequentialFeatureSelector(estimator, n_features_to_select=5, direction=\"backward\", tol=-0.1)\n    sfs.fit(X, y)\n    assert_array_equal(sfs.get_support(), np.array([True, True, True, True, True, False, False, False, False, False]))\n", "def test_fit_with_invalid_direction():\n    \"\"\"Ensure the correct ValueError is raised when `direction` is not a valid option.\"\"\"\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    with pytest.raises(ValueError):\n        SequentialFeatureSelector(estimator, direction=\"invalid\")\n", "def test_direction_backward():\n    \"\"\"Test backward direction.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=3)\n    model = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(model, direction=\"backward\", n_features_to_select=3)\n    sfs.fit(X, y)\n    assert_array_equal(sfs._get_support_mask(), [True, True, True, False, False, False, False, False, False, False])\n    # Test that the support mask is inverted\n    sfs = SequentialFeatureSelector(model, direction=\"backward\", n_features_to_select=3)\n    sfs.fit(X, y)\n    sfs.direction = \"forward\"\n    assert_array_equal(sfs._get_support_mask(), [False, False, False, True, True, True, True, True, True, True])\n", "def test_backward_selection_with_scikit_learn_default_cv():\n    # Generate a dataset with 10 features and 100 samples\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    selector = SequentialFeatureSelector(estimator, n_features_to_select=5, direction=\"backward\")\n    selector.fit(X, y)\n    support = selector.get_support()\n    assert_array_equal(support, selector.support_)\n", "def test_SequentialFeatureSelector_direction_backward():\n    # Regression test for issue #16584\n    X, y = make_regression(n_samples=100, n_features=10)\n    estimator = LinearRegression()\n    sfs = SequentialFeatureSelector(estimator, direction=\"backward\")\n    sfs.fit(X, y)\n    assert_array_equal(sfs.get_support(), np.array([False, False, False, False, False, False, False, False, False, True]))\n\n    # Test with a non-linearly separable dataset\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=2, n_redundant=8, n_repeated=0, random_state=42)\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(estimator, direction=\"backward\")\n    sfs.fit(X, y)\n    assert_array_equal(sfs.get_support(), np.array([True, True, False, False, False, False, False, False, False, False]))\n\n    # Test with a multi-class classification dataset\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=3, n_informative=2, n_redundant=8, random_state=42)\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(estimator, direction=\"backward\")\n    sfs.fit(X, y)\n    assert_array_equal(sfs.get_support(), np.array([True, True, False, False, False, False, False, False, False, False]))\n\n    # Test with a dataset that contains repeated features\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=2, n_redundant=8, n_repeated=2, random_state=42)\n    estimator = KNeighborsClassifier(n_neighbors=3)\n    sfs = SequentialFeatureSelector(estimator, direction=\"backward\")\n    sfs.fit(X, y)\n    assert_array_equal(sfs.get_support(), np.array([True, True, False, False, False, False, False, False, False, False]))\n\n    # Test with a dataset that contains correlated features\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=1, n_redundant=9, random_state=42)\n    estimator = KNeighborsClassifier(n_neighbors"], "sample_510": ["def test_figure_close():\n    # Create a figure\n    fig = plt.figure()\n    assert len(plt.get_fignums()) == 1\n\n    # Close the figure\n    plt.close(fig)\n    assert len(plt.get_fignums()) == 0\n\n    # Create another figure\n    fig2 = plt.figure()\n    assert len(plt.get_fignums()) == 1\n\n    # Close all figures\n    plt.close('all')\n    assert len(plt.get_fignums()) == 0\n\n    # Close a specific figure\n    plt.close(fig2)\n    assert len(plt.get_fignums()) == 0\n\n    # Create another figure\n    fig3 = plt.figure()\n    assert len(plt.get_fignums()) == 1\n\n    # Close the last figure\n    plt.close(fig3)\n    assert len(plt.get_fignums()) == 0\n", "def test_figure_kwargs(tmpdir):\n    fig = plt.figure(num=1, figsize=(6, 6), dpi=100, facecolor='blue',\n                     edgecolor='black', frameon=False, clear=True,\n                     FigureClass=mpl.figure.Figure, **{'key': 'value'})\n    assert fig.number == 1\n    assert fig.get_size_inches() == (6, 6)\n    assert fig.get_dpi() == 100\n    assert fig.get_facecolor() == 'blue'\n    assert fig.get_edgecolor() == 'black'\n    assert not fig.get_frame_on()\n    assert fig.get_label() == ''\n", "def test_xkcd_context_manager():\n    fig, ax = plt.subplots()\n    plt.xkcd()\n    ax.plot([1, 2, 3])\n    plt.xkcd.__enter__()\n    ax.plot([4, 5, 6])\n    plt.xkcd.__exit__(None, None, None)\n    assert ax.get_title() == ax.get_title()  # test if xkcd style is still active\n", "def test_rgrids():\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100))\n    ax.contourf(x, y, x**2 + y**2)\n    ax.set_theta_zero_location(\"N\")\n    ax.set_theta_direction(-1)\n    lines, labels = ax.rgrids(radii=(0.25, 0.5, 1.0), labels=('A', 'B', 'C'))\n    assert len(lines) == 3\n    assert len(labels) == 3\n    assert all(label.get_text() in ['A', 'B', 'C'] for label in labels)\n    ax.clear()\n    lines, labels = ax.thetagrids(angles=(45, 135, 225, 315), labels=('N', 'NE', 'E', 'SE'))\n    assert len(lines) == 4\n    assert len(labels) == 4\n    assert all(label.get_text() in ['N', 'NE', 'E', 'SE'] for label in labels)\n    plt.close('all')\n", "def test_colormaps():\n    \"\"\"Test the colormaps function.\"\"\"\n    colormaps = matplotlib.cm.colormaps\n    assert len(colormaps) > 0\n    assert 'viridis' in colormaps\n    assert 'plasma' in colormaps\n    assert 'inferno' in colormaps\n    assert 'magma' in colormaps\n    assert 'copper' in colormaps\n    assert 'pink' in colormaps\n    assert 'bone' in colormaps\n    assert 'cool' in colormaps\n    assert 'flag' in colormaps\n    assert 'gray' in colormaps\n    assert 'hot' in colormaps\n    assert 'hsv' in colormaps\n    assert 'jet' in colormaps\n    assert 'prism' in colormaps\n    assert 'spring' in colormaps\n    assert 'summer' in colormaps\n    assert 'winter' in colormaps\n    assert 'nipy_spectral' in colormaps\n    assert 'autumn' in colormaps\n"], "sample_893": ["def test_export_graphviz__out_file__str():\n    X = np.array([[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]])\n    y = np.array([-1, -1, -1, 1, 1, 1])\n    clf = DecisionTreeClassifier(random_state=42)\n    clf = clf.fit(X, y)\n    assert isinstance(export_graphviz(clf, \"test.dot\"), str)\n\n", "def test_export_text_decision_tree_regressor():\n    \"\"\"Test export_text function for decision tree regressor.\"\"\"\n    X = np.array([[0.5, 5], [2, 4], [6, 4], [2, 2], [8, 1], [1, 1]])\n    y = np.array([2, 4, 5, 5, 5, 6])\n    estimator = DecisionTreeRegressor(random_state=42)\n    estimator.fit(X, y)\n    report = export_text(estimator)\n    assert report\n", "def test_export_text_simple_regressor():\n    \"\"\"Test export_text with a simple regressor.\"\"\"\n    X = [[1, 2], [2, 3], [3, 4]]\n    y = [2, 3, 5]\n    clf = DecisionTreeRegressor(random_state=0).fit(X, y)\n\n    report = export_text(clf)\n    lines = report.split(\"\\n\")\n    assert len(lines) == 3\n    assert lines[0] == \"|--- x[0] <= 2\"\n    assert lines[1] == \"|   |--- x[1] <= 3\"\n    assert lines[2] == \"|   |   |--- value: 2.0\"\n\n", "def test_export_text_multi_output():\n    # Create a multi-output decision tree regressor\n    X = np.random.rand(10, 2)\n    y1 = np.random.rand(10)\n    y2 = np.random.rand(10)\n    dt = DecisionTreeRegressor().fit(X, [y1, y2])\n\n    # Test that the export_text function works correctly for a multi-output\n    # decision tree regressor\n    report = export_text(dt)\n    assert \"value:\" in report\n    assert \"weights:\" not in report\n\n    # Test that the export_text function works correctly for a multi-output\n    # decision tree regressor with show_weights=True\n    report = export_text(dt, show_weights=True)\n    assert \"value:\" in report\n    assert \"weights:\" in report\n\n    # Test that the export_text function raises a ValueError when the decision\n    # tree is not fitted\n    with pytest.raises(NotFittedError):\n        export_text(None)\n\n    # Test that the export_text function raises a ValueError when the feature\n    # names do not match the number of features in the decision tree\n    dt = DecisionTreeRegressor().fit(X, [y1, y2])\n    with pytest.raises(ValueError):\n        export_text(dt, feature_names=[\"feature_0\"])\n\n    # Test that the export_text function works correctly when the class_names\n    # parameter is a boolean\n    report = export_text(dt, class_names=True)\n    assert \"class:\" in report\n\n    # Test that the export_text function raises a ValueError when the class_names\n    # parameter is not None and does not match the number of classes in the\n    # decision tree\n    dt = DecisionTreeClassifier().fit(X, y)\n    with pytest.raises(ValueError):\n        export_text(dt, class_names=[1, 2])\n\n    # Test that the export_text function raises a ValueError when the max_depth\n    # parameter is not an integer\n    with pytest.raises(ValueError):\n        export_text(dt, max_depth=\"invalid\")\n\n    # Test that the export_text function raises a ValueError when the spacing\n    # parameter is not an integer\n    with pytest.raises(ValueError):\n        export_text(dt, spacing=\"invalid\")\n\n    # Test that the export_text function raises a ValueError when the decimals\n    # parameter is not an integer\n    with pytest.raises(ValueError):\n        export_text(dt, decimals=\"invalid\")\n\n    # Test that the export_text function raises", "def test_export_graphviz_classification_tree():\n    # Create a DecisionTreeClassifier instance and train it\n    clf = DecisionTreeClassifier(random_state=42)\n    clf.fit(X, y)\n    # Create a StringIO object to capture the output\n    out = StringIO()\n    # Export the tree using export_graphviz\n    export_graphviz(clf, out_file=out)\n    # Check if the exported tree is correct\n    graphviz_output = out.getvalue()\n    # Check if the tree has the correct nodes and edges\n    expected_nodes = [\"root\", \"feature_0 <= -1.0\", \"feature_1 <= -1.0\", \"feature_0 > -1.0\"]\n    expected_edges = [\n        \"root -> feature_0 <= -1.0\",\n        \"root -> feature_1 <= -1.0\",\n        \"feature_0 <= -1.0 -> feature_1 <= -1.0\",\n        \"feature_1 <= -1.0 -> feature_0 > -1.0\",\n    ]\n    # Use regular expressions to match the nodes and edges in the exported graph\n    for node in expected_nodes:\n        assert search(rf\"^{node} \\[.*\\] ;$\", graphviz_output)\n    for edge in expected_edges:\n        assert search(rf\"{edge} ;$\", graphviz_output)\n"], "sample_903": ["def test_tsne_init_args(self):\n    \"\"\"Test the initialization of TSNE with various arguments\"\"\"\n    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n\n    # Test with default arguments\n    tsne = TSNE()\n    assert_equal(tsne.n_components, 2)\n    assert_equal(tsne.perplexity, 30.0)\n    assert_equal(tsne.early_exaggeration, 12.0)\n    assert_equal(tsne.learning_rate, 200.0)\n    assert_equal(tsne.n_iter, 1000)\n    assert_equal(tsne.n_iter_without_progress, 300)\n    assert_equal(tsne.min_grad_norm, 1e-7)\n    assert_equal(tsne.metric, 'euclidean')\n    assert_equal(tsne.init, 'random')\n    assert_equal(tsne.verbose, 0)\n    assert_equal(tsne.random_state, None)\n    assert_equal(tsne.method, 'barnes_hut')\n    assert_equal(tsne.angle, 0.5)\n\n    # Test with custom arguments\n    tsne = TSNE(n_components=3, perplexity=20.0, early_exaggeration=10.0,\n                learning_rate=100.0, n_iter=2000, n_iter_without_progress=400,\n                min_grad_norm=1e-8, metric='cosine', init='pca',\n                verbose=1, random_state=42, method='exact', angle=0.3)\n    assert_equal(tsne.n_components, 3)\n    assert_equal(tsne.perplexity, 20.0)\n    assert_equal(tsne.early_exaggeration, 10.0)\n    assert_equal(tsne.learning_rate, 100.0)\n    assert_equal(tsne.n_iter, 2000)\n    assert_equal(tsne.n_iter_without_progress, 400)\n    assert_equal(tsne.min_grad_norm, 1e-8)\n    assert_equal(tsne.metric, 'cosine')\n    assert_equal(tsne.init, 'pca')\n    assert_equal(tsne.verbose, 1)\n    assert_equal(tsne.random_state, 42)\n    assert_equal(tsne.method, 'exact')\n    assert_equal(tsne.angle, 0.3)\n\n    # Test invalid arguments\n    with assert_raises(ValueError):\n        TSNE(n_components=-1", "def test_tsne_perplexity():\n    tsne = TSNE(n_components=2, perplexity=2.0)\n    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    tsne.fit_transform(X)\n    assert_warns(UserWarning, tsne.fit_transform, X, perplexity=2.0)\n", "    def test_tsne_error_message_on_invalid_n_iter(self):\n        # Test that the error message is correct when n_iter is too small\n        with assert_warns(UserWarning):\n            TSNE(n_iter=200).fit_transform(np.array([[1, 2], [3, 4]]))\n", "def test_tsne_fit_transform_with_sparse_matrix():\n    \"\"\"Test that t-SNE can be used with a sparse matrix as input.\"\"\"\n    # Generate some sample data\n    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    X_sparse = sp.csr_matrix(X)\n\n    # Create a TSNE object with the 'exact' method\n    tsne = TSNE(n_components=2, method='exact')\n\n    # Fit and transform the sparse matrix\n    X_embedded = tsne.fit_transform(X_sparse)\n\n    # Check that the result is correct\n    X_embedded_dense = tsne.fit_transform(X)\n    assert_array_almost_equal(X_embedded, X_embedded_dense)\n", "def test_tsne_trustworthiness_precomputed():\n    X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n    X_embedded = TSNE(n_components=2).fit_transform(X)\n    trust = trustworthiness(X, X_embedded, n_neighbors=5,\n                           precomputed=False, metric='euclidean')\n    trust_precomp = trustworthiness(X, X_embedded, n_neighbors=5,\n                                   precomputed=True, metric='euclidean')\n    assert_greater(trust, 0)\n    assert_greater(trust_precomp, 0)\n    assert_less(trust, 1)\n    assert_less(trust_precomp, 1)\n    assert_array_almost_equal(trust, trust_precomp)\n\n"], "sample_86": ["    def test_lazy_object_init(self):\n        # Test that LazyObject is properly initialized\n        lazy_obj = LazyObject()\n        self.assertIsNone(lazy_obj._wrapped)\n", "    def test_lazy_object_init(self):\n            return \"Test string\"\n\n        lazy_object = SimpleLazyObject(_setupfunc)\n        self.assertEqual(lazy_object._wrapped, empty)\n", "    def test_lazy_proxy_methods(self):\n        # Test that the proxy object acts like the original object\n        lazy_obj = lazy(lambda: [1, 2, 3])\n        proxy = lazy_obj()\n\n        self.assertEqual(proxy.__len__(), 3)\n        self.assertEqual(proxy.__getitem__(0), 1)\n        self.assertEqual(proxy.__getitem__(1), 2)\n        self.assertEqual(proxy.__getitem__(2), 3)\n        self.assertEqual(proxy.__len__(), 3)\n\n        # Test that __dir__ returns the methods of the original object\n        self.assertEqual(set(proxy.__dir__()), {'__len__', '__getitem__'})\n\n        # Test that __repr__ returns a meaningful representation\n        self.assertEqual(repr(proxy), '<lazy: <function __main__.<lambda>>')\n\n        # Test that __copy__ and __deepcopy__ work correctly\n        proxy_copy = copy.copy(proxy)\n        self.assertEqual(proxy_copy.__len__(), 3)\n        self.assertEqual(proxy_copy.__getitem__(0), 1)\n        self.assertEqual(proxy_copy.__getitem__(1), 2)\n        self.assertEqual(proxy_copy.__getitem__(2), 3)\n\n        proxy_deepcopy = copy.deepcopy(proxy)\n        self.assertEqual(proxy_deepcopy.__len__(), 3)\n        self.assertEqual(proxy_deepcopy.__getitem__(0), 1)\n        self.assertEqual(proxy_deepcopy.__getitem__(1), 2)\n        self.assertEqual(proxy_deepcopy.__getitem__(2), 3)\n", "    def test_lazy_object_delegates_methods(self):\n        class MyObject:\n                self.value = value\n\n                return self.value * 2\n\n        lazy_object = LazyObject()\n        lazy_object._setupfunc = lambda: MyObject(5)\n        self.assertEqual(lazy_object.my_method(), 10)\n", "    def test_lazy_object_init_with_empty_func(self):\n        with self.assertRaises(NotImplementedError):\n            lazyobject = LazyObject()\n            lazyobject._setup()\n"], "sample_933": ["def test_gettext_write_fails_when_versioning_method_is_set_to_text():\n    with pytest.raises(AttributeError):\n        app = Sphinx()\n        app.add_builder(MessageCatalogBuilder)\n        app.config.versioning_method = 'text'\n        app.config.gettext_uuid = False\n        setup(app)\n", "def test_i18n_builder_init():\n    \"\"\"Test I18nBuilder init method.\"\"\"\n    # Create a Sphinx application instance\n    app = Sphinx(app.builder_name='gettext', srcdir='test_src', outdir='test_out')\n    \n    # Initialize the i18n builder\n    builder = I18nBuilder()\n    builder.init(app)\n    \n    # Check that the catalogs dictionary is a defaultdict\n    assert isinstance(builder.catalogs, defaultdict)\n    \n    # Check that the tags instance is an instance of I18nTags\n    assert isinstance(builder.tags, I18nTags)\n    \n    # Check that the versioning method is set correctly\n    assert builder.versioning_method == 'text'\n    \n    # Check that the use_message_catalog flag is False\n    assert builder.use_message_catalog is False\n", "def test_build_gettext_setup(tmp_path):\n    # Test setup values in gettext configuration\n    config_file = tmp_path / 'conf.py'\n    config_file.write_text(\n        \"\"\"\n        project = 'Test Project'\n        copyright = '2022 Test Project'\n        version = '1.0.0'\n        gettext_compact = False\n        gettext_location = True\n        gettext_uuid = True\n        gettext_auto_build = True\n        gettext_additional_targets = ['index']\n        gettext_last_translator = 'Test Translator <translator@example.com>'\n        gettext_language_team = 'English <en@example.org>'\n        \"\"\"\n    )\n\n    os.environ['SOURCE_DATE_EPOCH'] = '1643723400'\n\n    app = Sphinx(srcdir='root-gettext', outdir=tmp_path, confoverrides={'conf.py': str(config_file)})\n    with pytest.raises(CalledProcessError):\n        app.builder_statuscode = subprocess.run(['sphinx-build', '-b', 'gettext', str(tmp_path), str(tmp_path)], stdout=PIPE, stderr=PIPE)\n\n    # Test default gettext_additional_targets\n    assert app.config.gettext_additional_targets == []\n", "def test_gettext_setup(app, status, warning, error):\n    app.builder = 'gettext'\n    app.builder.add_config_values({\n        'gettext_compact': False,\n        'gettext_location': False,\n        'gettext_uuid': False,\n        'gettext_auto_build': False,\n        'gettext_additional_targets': ['index.html'],\n        'gettext_last_translator': 'FULL NAME <EMAIL@ADDRESS>',\n        'gettext_language_team': 'LANGUAGE <LL@li.org>',\n    })\n    app.builder.build_all()\n    assert app.config.gettext_compact is False\n    assert app.config.gettext_location is False\n    assert app.config.gettext_uuid is False\n    assert app.config.gettext_auto_build is False\n    assert app.config.gettext_additional_targets == ['index.html']\n    assert app.config.gettext_last_translator == 'FULL NAME <EMAIL@ADDRESS>'\n    assert app.config.gettext_language_team == 'LANGUAGE <LL@li.org>'\n", "def test_build_gettext_empty_catalogs(tmpdir):\n    \"\"\"\n    Check that the gettext builder creates empty catalogs if no translatable messages.\n    \"\"\"\n    # Create a test project with a single doc file that has no translatable messages\n    with cd(tmpdir):\n        subprocess.run(['sphinx-apidoc', '-t', '-d', str(tmpdir), 'src'])\n\n    # Build the project with the gettext builder\n    subprocess.run(['sphinx-build', '-b', 'gettext', str(tmpdir), str(tmpdir)])\n\n    # Check that the catalogs are created but empty\n    catalogs = [f for f in os.listdir(str(tmpdir)) if f.endswith('.pot')]\n    assert len(catalogs) == 2  # catalogs for 'sphinx' and 'default' domains\n    for catalog in catalogs:\n        with open(os.path.join(tmpdir, catalog), 'r', encoding='utf-8') as f:\n            content = f.read()\n            assert re.search(r'^# SOME DESCRIPTIVE TITLE.$', content, re.MULTILINE)\n            assert re.search(r'^msgid \"\"$', content, re.MULTILINE)\n            assert re.search(r'^msgstr \"\"$', content, re.MULTILINE)\n            assert re.search(r'^Project-Id-Version: .*$', content, re.MULTILINE)\n            assert re.search(r'^PO-Revision-Date: .*$', content, re.MULTILINE)\n            assert re.search(r'^Last-Translator: .*$', content, re.MULTILINE)\n            assert re.search(r'^Language-Team: .*$', content, re.MULTILINE)\n            assert re.search(r'^MIME-Version: 1.0$', content, re.MULTILINE)\n            assert re.search(r'^Content-Type: text/plain; charset=UTF-8$', content, re.MULTILINE)\n            assert re.search(r'^Content-Transfer-Encoding: 8bit$', content, re.MULTILINE)\n            assert re.search(r'^\\n$', content, re.MULTILINE)\n            assert re.search(r'$', content, re.MULTILINE)\n"], "sample_71": ["    def test_format_large_number_with_force_grouping(self):\n        # Test formatting a large number with force grouping enabled\n        number = 12345678901234567890\n        self.assertEqual(format(number, '.', decimal_pos=2, force_grouping=True), '-12.345.678.901,23')\n", "def test_format_with_large_decimal_pos(self):\n    number = Decimal('12345678901234567890.987654321')\n    result = format(number, '.', 10)\n    self.assertEqual(result, '12.3456789.876543210')\n", "    def test_format_integer_with_decimal_separator(self):\n        result = format(12345.6789, '.', decimal_pos=3)\n        self.assertEqual(result, '12.345,678.790')\n", "    def test_decimal_pos_and_force_grouping(self):\n        self.assertEqual(format(1234567.89, '.', 2, 3, ',', True, False), '1.234,567.89')\n", "def test_large_number(self):\n    # Test case for large numbers with scientific notation\n    large_number = Decimal('1.23456789012345678901234567890123456789012345678901234567890')\n    expected_output = '{1.23456789e+29}'\n    self.assertEqual(format(large_number, '.', decimal_pos=28), expected_output)\n"], "sample_1023": ["def test_sieve_reset():\n    s = Sieve()\n    assert s._list == [2, 3, 5, 7, 11, 13]\n    assert s._tlist == [1, 1, 2, 2, 4, 6]\n    assert s._mlist == [0, 1, -1, -1, 0, -1]\n    s._reset(prime=True)\n    assert s._list == [2, 3, 5, 7, 11, 13]\n    s._reset(totient=True)\n    assert s._tlist == [1, 1, 2, 2, 4, 6]\n    s._reset(mobius=True)\n    assert s._mlist == [0, 1, -1, -1, 0, -1]\n", "def test_sieve_extend():\n    s = Sieve()\n    assert len(s._list) == 6\n    s.extend(10)\n    assert len(s._list) > 6\n    assert 2 in s\n    assert 10 in s\n    assert 11 in s\n    assert 13 in s\n    assert 7 not in s\n", "def test_sieve__reset():\n    s = Sieve()\n    s._reset(prime=True, totient=True, mobius=True)\n    assert s._list == _aset(2, 3, 5, 7, 11, 13)\n    assert s._tlist == _aset(1, 1, 2, 2, 4, 6)\n    assert s._mlist == _aset(0, 1, -1, -1, 0, -1)\n", "def test_sieve_reset():\n    \"\"\"Test reset method of Sieve.\"\"\"\n    sieve = Sieve()\n    sieve.extend(100)\n    assert len(sieve) == 25\n    sieve._reset()\n    assert len(sieve) == 6\n    sieve._reset(prime=True)\n    assert len(sieve) == 6\n", "def test_primepi():\n    from sympy import primepi\n    # Test that primepi returns the correct values for small inputs\n    assert primepi(1) == 0\n    assert primepi(2) == 1\n    assert primepi(3) == 2\n    assert primepi(5) == 3\n    # Test that primepi returns the correct values for larger inputs\n    assert primepi(100) == 25\n    assert primepi(1000) == 168\n    # Test that primepi returns the correct values for large inputs\n    assert primepi(10000) == 1229\n    # Test that primepi returns the correct values for inputs near a prime number\n    assert primepi(23) == 9\n    assert primepi(24) == 10\n    # Test that primepi returns the correct values for inputs near a prime number (continued)\n    assert primepi(26) == 11\n    assert primepi(27) == 12\n    # Test that primepi returns the correct values for inputs near a prime number (continued)\n    assert primepi(29) == 13\n    assert primepi(30) == 14\n    # Test that primepi raises a ValueError for negative inputs\n    raises(ValueError, lambda: primepi(-1))\n    # Test that primepi raises a ValueError for non-integer inputs\n    raises(ValueError, lambda: primepi(1.5))\n    # Test that primepi raises a ValueError for non-real inputs\n    raises(ValueError, lambda: primepi(1 + 2*I))\n    # Test that primepi raises a ValueError for non-numeric inputs\n    raises(ValueError, lambda: primepi(zoo))\n    # Test that primepi raises a ValueError for inputs that are not real\n    raises(ValueError, lambda: primepi(nan))\n    # Test that primepi returns S.Infinity for S.Infinity\n    assert primepi(S.Infinity) == S.Infinity\n    # Test that primepi returns S.Zero for S.NegativeInfinity\n    assert primepi(S.NegativeInfinity) == S.Zero\n"], "sample_158": ["    def test_foreign_key_field_constraints(self):\n        class MyModel(models.Model):\n            pass\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            field = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n            errors = field.check()\n            self.assertEqual(len(errors), 0)\n            self.assertEqual(field.unique, False)\n            self.assertEqual(field.remote_field.on_delete, models.CASCADE)\n", "    def test_m2m_field_db_table(self):\n        class RelatedModel(models.Model):\n            class Meta:\n                db_table = 'related_table'\n\n        class Model(models.Model):\n            related = models.ManyToManyField(RelatedModel)\n\n        with self.assertRaises(Error):\n            Model._meta.concrete_model._meta.db_table = 'model_table'\n            Model._meta.concrete_model._meta.auto_created._meta.db_table = 'm2m_table'\n            Model._meta.concrete_model._meta.apps.get_models(include_auto_created=True)\n            Model._meta.concrete_model._meta.db_table = 'clashing_table'\n", "    def test_many_to_many_field_unique_constraint(self):\n        with mock.patch('django.db.models.checks.get_models') as mocked_get_models:\n            mocked_get_models.return_value = [models.Model1, models.Model2]\n            class Model1(models.Model):\n                pass\n            class Model2(models.Model):\n                field1 = models.ManyToManyField(Model1, unique=True)\n            checks = Model2._meta.model_checks()\n            self.assertEqual(len(checks), 1)\n            self.assertIsInstance(checks[0], Error)\n            self.assertEqual(str(checks[0]), 'ManyToManyFields cannot be unique.')\n", "    def test_foreign_key_deconstruction_with_swapped_model(self):\n        # Act\n        with mock.patch('django.apps.apps.get_swappable_settings_name', return_value='swapped_model'):\n            with isolate_apps('invalid_models_tests'):\n                model = models.Model()\n                field = models.ForeignKey('swapped_model', on_delete=models.CASCADE)\n\n                # Assert\n                expected_path = 'invalid_models_tests.swapped_model'\n                expected_args = ('on_delete', models.CASCADE)\n                expected_kwargs = {\n                    'db_constraint': True,\n                    'related_name': None,\n                    'related_query_name': None,\n                    'limit_choices_to': None,\n                    'parent_link': False,\n                    'to_field': None,\n                }\n                expected_deconstructed = (field.name, field.path, expected_args, expected_kwargs)\n                self.assertEqual(field.deconstruct(), expected_deconstructed)\n", "    def test_foreign_object_check_recursion(self):\n        with override_settings(ROOT_URLCONF='tests.urls'):\n            with isolate_apps('invalid_models_tests'):\n                with mock.patch('django.apps.apps.check_models_ready') as mock_check_models_ready:\n                    # Create a model with a recursive ForeignObject\n                    class RecursiveModel(models.Model):\n                        field = models.ForeignKey('self', related_name='+', on_delete=models.CASCADE)\n                    mock_check_models_ready.return_value = None\n                    self.assertEqual(len(models.check(RecursiveModel().field), 0)\n"], "sample_1110": ["def test_PythonCodePrinter_CustomFunctions():\n    from sympy.core import Function\n    class CustomFunction(Function):\n            return S.Add(x, y)\n    x = symbols('x')\n    y = symbols('y')\n    expr = CustomFunction(x, y)\n    printer = PythonCodePrinter({'standard': 'python3', 'user_functions': {'CustomFunction': 'add'}})\n    assert printer.doprint(expr) == 'add(x, y)'\n", "def test_PythonCodePrinter_NaN():\n    expr = pi\n    printer = PythonCodePrinter()\n    result = printer._print_NaN(expr)\n    assert result == 'float(\"nan\")'\n", "def test_SparseMatrixPrinting():\n    A = SparseMatrix([[1, 2], [3, 4]])\n    A1 = SparseMatrix([[5, 0], [0, 6]])\n\n    # Test NumPy printer\n    printer = NumPyPrinter()\n    assert printer.doprint(A) == 'numpy.array([[1, 2], [3, 4]])'\n\n    # Test SciPy printer\n    printer = SciPyPrinter()\n    assert printer.doprint(A) == 'scipy.sparse.coo_matrix([[1, 2], [3, 4]], shape=(2, 2), dtype=float64)'\n\n    # Test PyCode printer\n    printer = PythonCodePrinter()\n    assert printer.doprint(A) == 'numpy.array([[1, 2], [3, 4]])'\n\n    # Test Mpmath printer\n    printer = MpmathPrinter()\n    assert printer.doprint(A) == 'mpmath.matrix([[1, 2], [3, 4]])'\n\n    # Test SymPy printer\n    printer = SymPyPrinter()\n    assert printer.doprint(A) == 'sympy.Matrix([[1, 2], [3, 4]])'\n\n    # Test printing of SparseMatrix equality\n    assert printer.doprint(Eq(A, A1)) == 'numpy.array_equal([[1, 2], [3, 4]], [[5, 0], [0, 6]])'\n    # Test printing of SparseMatrix inequality\n    assert printer.doprint(Lt(A, A1)) == 'numpy.greater_equal([[1, 2], [3, 4]], [[5, 0], [0, 6]])'\n    # Test printing of SparseMatrix greater than\n    assert printer.doprint(Gt(A, A1)) == 'numpy.less_equal([[1, 2], [3, 4]], [[5, 0], [0, 6]])'\n", "def test_print_ComplexInfinity():\n    printer = PythonCodePrinter()\n    expr = sympy.symbols('ComplexInfinity')\n    assert printer._print_ComplexInfinity(expr) == \"float('nan')\"\n", "def test_PythonCodePrinter_print_Pow():\n    # Test rational power with negative integer\n    assert pycode(x**-2) == \"1/x**2\"\n    assert pycode(x**-2, standard=\"python2\") == \"1.0/(x**2)\"\n\n    # Test with non-rational power\n    assert pycode(x**Rational(1, 2)) == \"math.sqrt(x)\"\n\n    # Test with non-rational power\n    assert pycode(x**pi) == \"math.exp(log(x)/pi)\"\n\n    # Test with large Rational power\n    assert pycode(x**Rational(100, 3)) == \"math.exp(log(x)*100/3)\"\n"], "sample_1001": ["def test_print_latex_Morphism():\n    from sympy import NamedMorphism\n    from sympy.categories import Category\n\n    A = Category(\"A\")\n    B = Category(\"B\")\n\n    phi = NamedMorphism(A, B, \"phi\")\n\n    printer = latex(phi, mode='inline')\n    expected = r\"\\mathbf{A}\\xrightarrow{\\mathrm{phi}}\\mathbf{B}\"\n    assert printer == expected\n\n    printer = latex(phi, mode='equation')\n    expected = r\"\\mathbf{A}\\xrightarrow{\\mathrm{phi}}\\mathbf{B}\"\n    assert printer == expected\n\n    printer = latex(phi, mode='equation*', itex=True)\n    expected = r\"\\mathbf{A}\\xrightarrow{\\mathrm{phi}}\\mathbf{B}\"\n    assert printer == expected\n", "    def test_latex_expression_with_indexed_base(self):\n        a = IndexedBase('a')\n        print(latex(a))\n", "def test_latex_settings():\n    from sympy import latex, symbols\n    x, y = symbols('x y')\n\n    # Test order\n    assert latex(x + y, order='lex') == r\"x + y\"\n\n    # Test mode\n    assert latex(x + y, mode='inline') == r\"$x + y$\"\n\n    # Test itex\n    assert latex(x + y, itex=True) == r\"$$x + y$$\"\n\n    # Test fold_frac_powers\n    assert latex(x**Rational(1, 2)) == r\"x^{1/2}\"\n\n    # Test fold_func_brackets\n    assert latex(x**sin(1)) == r\"x^{\\sin(1)}\"\n\n    # Test fold_short_frac\n    assert latex(x/y) == r\"\\frac{x}{y}\"\n\n    # Test long_frac_ratio\n    assert latex(Integral(x, x)/2/pi) == r\"\\frac{1}{2 \\pi} \\int x\\, dx\"\n\n    # Test mul_symbol\n    assert latex(x*y) == r\"x \\cdot y\"\n\n    # Test inv_trig_style\n    assert latex(asin(1)) == r\"\\operatorname{asin}(1)\"\n\n    # Test mat_str\n    assert latex(Matrix(2, 2, [x, y])) == r\"\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right]\"\n\n    # Test mat_delim\n    assert latex(Matrix(2, 2, [x, y]), mat_delim=\"(\") == r\"\\left(\\begin{matrix}x\\\\y\\end{matrix}\\right)\"\n\n    # Test symbol_names\n    assert latex(x**2, symbol_names={x:'x_i'}) == r\"x_i^2\"\n\n    # Test ln_notation\n    assert latex(log(10)) == r\"\\log{\\left (10 \\right )}\"\n\n    # Test various types\n    assert latex([x, y]) == r\"\\left [ x, \\quad y\\right ]\"\n    assert latex((x, y)) == r\"\\left ( x, \\quad y\\right )\"\n    assert latex({\"x\": x, \"y\": y}) == r\"\\left \\{ x: x, \\quad y: y\\right \\}\"\n\n    # Test latex of DiracDelta\n    assert latex(DiracDelta(x))", "def test_deal_with_super_sub():\n    printer = LatexPrinter()\n    assert printer._deal_with_super_sub('alpha') == r'\\alpha'\n    assert printer._deal_with_super_sub('alpha') == r'\\alpha'\n    assert printer._deal_with_super_sub('alphaHat') == r'\\hat{\\alpha}'\n    assert printer._deal_with_super_sub('alphaHatPrime') == r'{\\dot{\\hat{\\alpha}}}\\''\n    assert printer._deal_with_super_sub('alphaHatPrimePrime') == r'{\\ddot{\\hat{\\alpha}}\\'\\}'\n    assert printer._deal_with_super_sub('alphaHatPrimePrimePrime') == r'{\\dddot{\\hat{\\alpha}}\\'\\'\\}'\n\n", "def test_latex_multi_line_expr():\n    from sympy import sympify\n\n    s = 'x + y + z + w'\n    expr = sympify(s)\n    assert latex(expr) == s\n\n    s = 'x + y + z + w\\n + u + v'\n    expr = sympify(s)\n    assert latex(expr) == s\n\n    s = 'x + y + z + w\\n+ u + v'\n    expr = sympify(s)\n    assert latex(expr) == s\n"], "sample_148": ["    def setUp(self):\n        self.model = models.Model()\n        self.model._meta.add_field(models.CharField(name='name', max_length=10))\n", "    def setUp(self):\n        self.n = NestedObjects(using=DEFAULT_DB_ALIAS)\n        self.objs = [\n            Car.objects.create(model='Model A', owner=Vehicle.objects.create(name='Vehicle A')),\n            Car.objects.create(model='Model B', owner=Vehicle.objects.create(name='Vehicle B')),\n            Car.objects.create(model='Model C', owner=Vehicle.objects.create(name='Vehicle C')),\n            Vehicle.objects.create(name='Vehicle D'),\n            Vehicle.objects.create(name='Vehicle E')\n        ]\n        self.n.collect([self.objs[0]])\n", "    def test_model_format_dict(self):\n        self.assertEqual(model_format_dict(models.Model()), {\n            'verbose_name': 'Model', 'verbose_name_plural': 'Models'})\n", "    def test_collect_with_relations(self):\n        self.n = NestedObjects(using=DEFAULT_DB_ALIAS)\n        self.objs = [Car.objects.create(name='Car %d' % i) for i in range(5)]\n        self.objs.append(Site.objects.create(name='Site 1'))\n\n        self.objs[0].site = self.objs[1]\n        self.objs[1].vehicle = self.objs[0]\n        self.objs[2].vehicle = self.objs[0]\n        self.objs[3].vehicle = self.objs[0]\n        self.objs[4].vehicle = self.objs[0]\n\n        self.n.collect([self.objs[0]])\n\n        expected_nested = [\n            self.objs[0],\n            self.objs[1],\n            [self.objs[2], self.objs[3], self.objs[4]],\n        ]\n\n        self.assertEqual(self.n.nested(lambda obj: obj.name), expected_nested)\n", "    def test_display_for_field(self):\n        # Test display_for_field with various field types\n        field_types = [\n            models.BooleanField(),\n            models.CharField(max_length=255),\n            models.DateField(),\n            models.DateTimeField(),\n            models.DecimalField(max_digits=10, decimal_places=2),\n            models.EmailField(),\n            models.FileField(upload_to='uploads/'),\n            models.IntegerField(),\n            models.Manager(),\n            models.Model(),\n            models.OneToOneField('self', on_delete=models.CASCADE),\n            models.ForeignKey('self', on_delete=models.CASCADE),\n            models.ManyToManyField('self'),\n            models.URLField(max_length=200),\n            models.IntegerField(),\n            models.FloatField(),\n        ]\n\n        for field in field_types:\n            value = getattr(field, 'default', None)\n            if value is None:\n                value = field.max_length\n            elif isinstance(value, datetime.datetime):\n                value = field.auto_now\n            elif isinstance(value, (int, float)):\n                value = field.max_length\n\n            # Test with empty value\n            empty_value_display = 'Unknown'\n            self.assertEqual(display_for_field(None, field, empty_value_display), empty_value_display)\n\n            # Test with default value\n            self.assertEqual(display_for_field(value, field, empty_value_display), value)\n\n            # Test with non-empty value\n            if isinstance(field, (models.BooleanField, models.EmailField, models.URLField)):\n                self.assertEqual(display_for_field(True, field, empty_value_display), 'Yes')\n                self.assertEqual(display_for_field(False, field, empty_value_display), 'No')\n            else:\n                self.assertEqual(display_for_field(True, field, empty_value_display), '1')\n                self.assertEqual(display_for_field(False, field, empty_value_display), '0')\n\n            # Test with datetime\n            if isinstance(field, models.DateTimeField):\n                value = datetime.now()\n                self.assertEqual(display_for_field(value, field, empty_value_display), formats.localize(value))\n\n            # Test with date\n            if isinstance(field, models.DateField):\n                value = datetime.now().date()\n                self.assertEqual(display_for_field(value, field, empty_value_display), formats.localize(value))\n\n            # Test with time\n            if isinstance(field, models.TimeField):\n                value = datetime.now().time()\n                self.assertEqual(display_for_field(value, field, empty_value_display), formats.localize(value))\n\n            # Test with number\n            if isinstance(field, (models.IntegerField, models.FloatField, models.DecimalField)):\n                self.assertEqual(display_for_field(Decimal('12.34"], "sample_19": ["    def test_world2pix_singularity(self):\n        # Create a WCS with a singular matrix\n        with NumpyRNGContext(42):\n            header = fits.Header()\n            header[\"CTYPE1\"] = \"RA---TAN\"\n            header[\"CTYPE2\"] = \"DEC--TAN\"\n            header[\"CRVAL1\"] = 0\n            header[\"CRVAL2\"] = 0\n            header[\"CRPIX1\"] = 1\n            header[\"CRPIX2\"] = 1\n            header[\"CTYPE1_1\"] = 0\n            header[\"CTYPE1_2\"] = 0\n            header[\"CTYPE2_1\"] = 0\n            header[\"CTYPE2_2\"] = 0\n            header[\"CTYPE1_3\"] = 0\n            header[\"CTYPE2_3\"] = 0\n            header[\"CTYPE1_4\"] = 0\n            header[\"CTYPE2_4\"] = 0\n            header[\"CTYPE1_5\"] = 0\n            header[\"CTYPE2_5\"] = 0\n            header[\"CTYPE1_6\"] = 0\n            header[\"CTYPE2_6\"] = 0\n            header[\"CTYPE1_7\"] = 0\n            header[\"CTYPE2_7\"] = 0\n            header[\"CTYPE1_8\"] = 0\n            header[\"CTYPE2_8\"] = 0\n\n            wcs = wcs.WCS(header)\n            with pytest.raises(wcs.SingularMatrixError):\n                wcs.all_world2pix(np.array([1, 2, 3]), np.array([4, 5, 6]))\n", "    def test_all_world2pix_ra_dec_order(self):\n        # Test that ra_dec_order=True raises an error\n        with pytest.raises(ValueError):\n            wcs.WCS(fits.Header()).all_world2pix(ra=1, dec=2, ra_dec_order=True)\n        # Test that ra_dec_order=False does not raise an error\n        wcs.WCS(fits.Header()).all_world2pix(ra=1, dec=2, ra_dec_order=False)\n", "    def test_all_world2pix(self):\n        # A few trivial cases to ensure that _all_world2pix works\n        w = WCS({\"CTYPE1\": \"RA---TAN\", \"CTYPE2\": \"DEC--TAN\"}, fix=True)\n        pix = w.all_world2pix([[1, 1], [2, 2]], 1)\n        assert_array_equal(pix, [[1, 1], [2, 2]])\n\n        pix = w.all_world2pix([1, 1], [1, 1], 1)\n        assert_array_equal(pix, [1, 1])\n\n        w = WCS(\n            {\n                \"CTYPE1\": \"RA---TPV\",\n                \"CTYPE2\": \"DEC--TPV\",\n                \"CTYPE3\": \"FREQ\",\n                \"CTYPE4\": \"FREQ\",\n                \"CTYPE5\": \"STOKES\",\n                \"CTYPE6\": \"STOKES\",\n                \"CTYPE7\": \"STOKES\",\n            },\n            fix=True,\n        )\n        pix = w.all_world2pix(\n            [\n                [1, 1, 1, 1, 1],\n                [2, 2, 2, 2, 2],\n                [3, 3, 3, 3, 3],\n            ],\n            [1, 1, 1, 1, 1, 1, 1],\n            1,\n        )\n        assert_array_equal(pix, [[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [3, 3, 3, 3, 3]])\n\n        w = WCS(\n            {\n                \"CTYPE1\": \"RA---TPV\",\n                \"CTYPE2\": \"DEC--TPV\",\n                \"CTYPE3\": \"FREQ\",\n                \"CTYPE4\": \"FREQ\",\n                \"CTYPE5\": \"STOKES\",\n                \"CTYPE6\": \"STOKES\",\n                \"CTYPE7\": \"STOKES\",\n            },\n            fix=True,\n        )\n        pix = w.all_world2pix(\n            [\n                [[1, 1, 1, 1], [2, 2, 2, 2]],\n                [[3, 3, 3, 3], [4, 4, 4", "    def test_fix_translations(self):\n        from astropy.io import fits\n        from astropy.wcs import WCS\n\n        # Test fix() method\n        hdulist = fits.HDUList([\n            fits.PrimaryHDU(header=fits.Header({\n                'CTYPE1': 'RA---TAN',\n                'CTYPE2': 'DEC--TAN',\n                'CRVAL1': 12.5,\n                'CRVAL2': 23.5,\n                'CRPIX1': 25,\n                'CRPIX2': 35,\n                'CTYPE3': 'FREQ',\n                'CRVAL3': 1e6,\n                'CTYPE4': 'TIME',\n                'CRVAL4': datetime(2022, 1, 1, 12, 0, 0),\n                'CTYPE5': 'TIMEDEL',\n                'CRVAL5': 100,\n            }))\n        ])\n\n        wcs = WCS(hdulist[0].header, hdulist=hdulist)\n        wcs.fix(translate_units=\"shd\")\n\n        expected_header = fits.Header({\n            'CTYPE1': 'RA---TPV',\n            'CTYPE2': 'DEC--TPV',\n            'CTYPE3': 'FREQ',\n            'CTYPE4': 'TIME',\n            'CTYPE5': 'TIMEDEL',\n            'CRVAL1': 12.5,\n            'CRVAL2': 23.5,\n            'CRVAL3': 1e6,\n            'CRVAL4': datetime(2022, 1, 1, 12, 0, 0),\n            'CRVAL5': 100,\n            'CRPIX1': 25,\n            'CRPIX2': 35,\n            'CUNIT1': 'deg',\n            'CUNIT2': 'deg',\n            'CUNIT3': 'Hz',\n            'CUNIT4': 's',\n            'CUNIT5': 's',\n            'CTYPE1': 'RA---SIP',\n            'CTYPE2': 'DEC--SIP',\n            'A_ORDER': 1,\n            'B_ORDER': 1,\n            'AP_ORDER': 0,\n            'BP_ORDER': 0,\n            'A_0_0': 1,\n            'A_0_1': 0,\n            'A_1_0': 0,\n            'A_1_1': ", "    def test_det2im_forward(self):\n        with ctx_for_v71_dateref_warnings():\n            filename = get_pkg_data_filename(\"data/maps\", \"example_det2im_forward.hdr\")\n            with NumpyRNGContext(42):\n                hdulist = fits.open(filename)\n                wcs = wcs.WCS(hdulist[0].header, hdulist, key=\"A\", relax=True)\n                det2im = wcs.det2im(\n                    np.array([[0.0, 0.0], [1.0, 1.0]], dtype=np.float64),\n                    0,\n                )\n                assert_array_equal(det2im, np.array([[0.0, 0.0], [1.0, 1.0]], dtype=np.float64))\n                assert_allclose(det2im.crpix, [0.0, 0.0], atol=1e-8)\n                assert_allclose(det2im.crval, [0.0, 0.0], atol=1e-8)\n                assert_allclose(det2im.cdelt, [1.0, 1.0], atol=1e-8)\n\n                filename = get_pkg_data_filename(\"data/maps\", \"example_det2im_forward.fits\")\n                with NumpyRNGContext(42):\n                    with fits.open(filename) as hdulist:\n                        det2im_hdu = hdulist[\"D2IMARR\"]\n                        assert_array_equal(det2im_hdu.data, det2im.data)\n                        assert_quantity_allclose(det2im_hdu.header[\"CRPIX1\"], det2im.crpix[0])\n                        assert_quantity_allclose(det2im_hdu.header[\"CRVAL1\"], det2im.crval[0])\n                        assert_quantity_allclose(det2im_hdu.header[\"CDELT1\"], det2im.cdelt[0])\n                        assert_array_equal(det2im_hdu.header[\"CTYPE1\"], \"LOOKUP\")\n"], "sample_203": ["    def test_regex_validator(self):\n        validator = RegexValidator(regex=r'^hello$')\n        with self.assertRaises(ValidationError):\n            validator('world')\n        validator('hello')\n        \n        # Test with flags\n        validator = RegexValidator(regex=r'^hello$',\n                                 flags=re.IGNORECASE)\n        with self.assertRaises(ValidationError):\n            validator('WORLD')\n        validator('hello')\n", "    def test_file_extension_validator(self):\n        validator = FileExtensionValidator(allowed_extensions=['txt', 'pdf'])\n        with self.assertRaises(ValidationError):\n            validator('test.txt')\n            validator('test.pdf')\n        with self.assertRaises(ValidationError):\n            validator('test.doc')\n        self.assertEqual(validator('test.txt'), None)\n        self.assertEqual(validator('test.pdf'), None)\n", "    def test_slug_validator_with_empty_string(self):\n        validator = validate_slug\n        with self.assertRaises(ValidationError):\n            validator('')\n", "    def test_regex_validator(self):\n        validator = RegexValidator(regex='^hello$')\n        with self.assertRaises(ValidationError):\n            validator('world')\n        validator('hello')\n        with self.assertRaises(TypeError):\n            RegexValidator(regex=123, flags=0)\n", "    def test_MakeValidator_deconstruct(self):\n        validator = validators.IntegerValidator(_lazy_re_compile(r'^-?\\d+\\Z'))\n        deconstructed = validator.__dict__\n        new_validator = validators.IntegerValidator._make_validator(deconstructed)\n        self.assertEqual(validator, new_validator)\n"], "sample_163": ["    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.u2 = User.objects.create_user(\n            username=\"testuser\", password=\"password\", email=\"testuser@example.com\"\n        )\n        cls.admin_user = User.objects.create_superuser(\n            username=\"admin\", password=\"password\", email=\"admin@example.com\"\n        )\n", "    def test_login_view_redirects_to_next_page(self):\n        # Test that the login view redirects to the next page after successful login\n        url = reverse(\"login\")\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        response = self.login(url=url)\n        self.assertRedirects(response, reverse(\"password_change\"), status_code=302, target_status_code=200)\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.u1 = User.objects.create_user(\n            username=\"testclient\", password=\"password\", email=\"testclient@example.com\"\n        )\n", "    def test_password_reset_view(self):\n        # Test that a user can request a password reset\n        response = self.client.post(\n            \"/password_reset/\",\n            {\"email\": \"testclient@example.com\"},\n        )\n        self.assertEqual(response.status_code, 200)\n\n        # Test that a password reset email is sent\n        self.assertEqual(len(mail.outbox), 1)\n        self.assertIn(\"Password reset link\", mail.outbox[0].body)\n\n        # Test that the password reset link is valid\n        password_reset_confirm_url = reverse(\"password_reset_confirm\")\n        uid = urlsafe_base64_encode(\"testclient\".encode()).decode()\n        token = default_token_generator.make_token(self.u1)\n        url = f\"{password_reset_confirm_url}?uid={uid}&token={token}\"\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        # Test that a user can reset their password\n        response = self.client.post(url, {\"new_password1\": \"new_password\", \"new_password2\": \"new_password\"})\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(self.u1.check_password(\"new_password\"))\n", "    def test_redirect_authenticated_user_to_login_url(self):\n        self.client.login(username=\"testclient\", password=\"password\")\n        response = self.client.get(\"/login/\")\n        self.assertRedirects(response, \"/login/\")\n"], "sample_299": ["    def test_check_cache_location_not_exposed_with_media_root(self):\n        with override_settings(CACHES={\n            'default': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '/tmp/cache',\n            },\n            'other': {\n                'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                'LOCATION': '/tmp/cache',\n            },\n        }, MEDIA_ROOT='/tmp/media'):\n            errors = check_cache_location_not_exposed([], apps=None)\n            self.assertEqual(len(errors), 2)\n            self.assertEqual(errors[0].msg, \"Your 'default' cache configuration might expose your cache or lead to corruption of your data because its LOCATION matches 'MEDIA_ROOT'.\")\n            self.assertEqual(errors[1].msg, \"Your 'other' cache configuration might expose your cache or lead to corruption of your data because its LOCATION matches 'MEDIA_ROOT'.\")\n", "    def test_cache_location_is_absolute(self):\n        errors = check_default_cache_is_configured([])\n        self.assertEqual(errors, [])\n", "    def test_check_cache_location_not_exposed_valid_configuration(self):\n        with override_settings(**self.VALID_CACHES_CONFIGURATION):\n            self.assertEqual(check_cache_location_not_exposed([], {}), [])\n", "    def test_cache_location_not_exposed_to_media_root(self):\n        with override_settings(\n            CACHES=self.VALID_CACHES_CONFIGURATION,\n            MEDIA_ROOT=self.STATIC_ROOT,\n            STATIC_ROOT=self.STATIC_ROOT,\n            STATICFILES_DIRS=self.STATICFILES_DIRS,\n        ):\n            self.assertIn(\n                Warning(\n                    \"Your 'default' cache configuration might expose your cache \"\n                    \"or lead to corruption of your data because its LOCATION \"\n                    \"is inside STATIC_ROOT.\",\n                    id='caches.W002',\n                ),\n                check_cache_location_not_exposed([], {}),\n            )\n            self.assertNotIn(\n                Warning(\n                    \"Your 'other' cache configuration might expose your cache \"\n                    \"or lead to corruption of your data because its LOCATION \"\n                    \"matches STATIC_ROOT.\",\n                    id='caches.W002',\n                ),\n                check_cache_location_not_exposed([], {}),\n            )\n            self.assertNotIn(\n                Warning(\n                    \"Your 'third' cache configuration might expose your cache \"\n                    \"or lead to corruption of your data because its LOCATION \"\n                    \"matches STATIC_ROOT.\",\n                    id='caches.W002',\n                ),\n                check_cache_location_not_exposed([], {}),\n            )\n", "    def test_check_default_cache_is_configured_with_valid_cache(self):\n        with self.assertWarns(Warning, E001.id):\n            check_default_cache_is_configured(app_configs=[])\n"], "sample_578": ["def test_bar_alpha(self):\n    df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n    p = Plot(x=\"x\", y=\"y\", data=df).add(Bar(alpha=0.5)).plot()\n    ax = p._figure.axes[0]\n    bars = self.plot_bars({\"x\": \"x\", \"y\": \"y\"}, {\"color\": \"C0\"}, {})\n    bar = bars[0]\n    assert bar.get_alpha() == pytest.approx(0.5)\n", "    def test_bar_alpha(self):\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [10, 20, 30]})\n        variables = {\"x\": \"x\", \"y\": \"y\"}\n        mark_kws = {\"fill\": False, \"edgealpha\": 0.5}\n        layer_kws = {\"palette\": \"viridis\"}\n        bars = self.plot_bars(variables, mark_kws, layer_kws)\n        for bar in bars:\n            assert bar.get_alpha() == 0.5\n", "    def test_bar_alpha(self):\n\n        x = np.array([1, 2, 3])\n        y = np.array([4, 5, 6])\n        data = pd.DataFrame({\"x\": x, \"y\": y})\n        variables = {\"data\": data, \"x\": \"x\", \"y\": \"y\"}\n        mark_kws = {\"color\": \"#000000\", \"alpha\": 0.5, \"fill\": True}\n        layer_kws = {\"ax\": None}\n\n        bars = self.plot_bars(variables, mark_kws, layer_kws)\n        assert all(bar.get_alpha() == 0.5 for bar in bars)\n", "    def test_bars_no_width(self):\n\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [1, 2, 3]})\n        p = Plot(x=\"x\", y=\"y\").add(Bars()).plot()\n        ax = p._figure.axes[0]\n        bars = self.plot_bars({\"x\": \"x\", \"y\": \"y\"}, {}, {\"height\": 6})\n\n        assert len(bars) == 3\n        for bar in bars:\n            assert bar.get_width() == pytest.approx(1)\n", "    def test_bars_edgecolor_cycle(self):\n\n        data = pd.DataFrame({\n            \"x\": [1, 2, 3],\n            \"y\": [1, 2, 3],\n            \"value\": [10, 20, 30]\n        })\n\n        plot = Plot(x=\"x\", y=\"value\", hue=\"x\", data=data)\n        bars = Bars(color=\"C0\", edgecolor=\"C1\", width=0.8)\n        plot.add(bars).plot()\n\n        ax = plot._figure.axes[0]\n        bars = self.plot_bars({\"x\": \"x\", \"y\": \"value\", \"hue\": \"x\"}, {\"color\": \"C0\"}, {\"hue_order\": [1, 2, 3]})\n        assert len(bars) == 3\n        for i, bar in enumerate(bars):\n            assert bar.get_facecolor() == (0.0, 0.0, 1.0, 0.7)  # default fill color\n            assert bar.get_edgecolor() == to_rgba(\"C1\")\n            assert bar.get_linewidth() == 1.0  # default edge width\n"], "sample_310": ["    def test_bookmarklets_view(self):\n        response = self.client.get(reverse('admin:admin_doc:bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_context_data(self):\n        self.client.force_login(self.superuser)\n        model_name = 'Person'\n        app_label = 'tests'\n        self.client.get(reverse('admin_docs:model-detail', kwargs={'app_label': app_label, 'model_name': model_name}))\n        context = self.client.session['django_docutils_context']\n        self.assertIn('name', context)\n        self.assertIn('summary', context)\n        self.assertIn('description', context)\n        self.assertIn('fields', context)\n        self.assertIn('methods', context)\n", "    def test_bookmarklets_view(self):\n        response = self.client.get(reverse('admin_doc:bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n"], "sample_1094": ["def test_replace_general_usage():\n    from sympy import Wild, sin, cos\n    x = Wild('x')\n    y = Wild('y')\n    e = x**2 + sin(x*y)\n    q = x**2, sin(x*y)\n    v = x + 1\n    assert e.replace(q, v) == v\n    assert e.replace(q, v, simultaneous=False) == x**2 + sin(x*y)\n    assert e.replace(q, v, map=True) == ({x**2: x + 1, sin(x*y): x + 1}, x + 1)\n    assert e.replace(q, v, map=True, simultaneous=False) == ({x**2: x + 1, sin(x*y): x + 1}, x**2 + sin(x*y))\n    assert e.replace(q, v, map=True, exact=True) == ({x**2: x + 1}, x + 1)\n    assert e.replace(q, v, map=True, exact=False) == ({x**2: x + 1, sin(x*y): x + 1}, x + 1)\n    assert e.replace(q, v, map=True, simultaneous=True, exact=False) == ({x**2: x + 1, sin(x*y): x + 1}, x + 1)\n    assert e.replace(q, v, map=True, simultaneous=False, exact=False) == ({x**2: x + 1, sin(x*y): x + 1}, x**2 + sin(x*y))\n    assert e.replace(q, v, map=True, exact=True, simultaneous=False) == ({x**2: x + 1}, x**2 + sin(x*y))\n\n    q = x**2 + y, sin(x*y)\n    v = x + 1\n    assert e.replace(q, v) == x + 1 + sin(x*y)\n    assert e.replace(q, v, simultaneous=False) == x**2 + sin(x*y)\n    assert e.replace(q, v, map=True) == ({x**2 + y: x + 1}, x + 1 + sin(x*y))\n    assert e.replace(q, v, map=True, simultaneous=False) == ({x**2 + y: x + 1}, x**2 + sin(x*y))\n    assert e.replace(q, v, map=True, exact=True) == ({x**2 +", "def test_hashable_content():\n    a = Basic(1, 2, 3)\n    b = Basic(1, 2, 3)\n    c = Basic(1, 2, 4)\n\n    assert a._hashable_content() == (1, 2, 3)\n    assert a._hashable_content() is b._hashable_content()\n    assert a._hashable_content() != c._hashable_content()\n\n    class CustomBasic(Basic):\n            self.custom_attr = True\n            super(CustomBasic, self).__init__(*args)\n\n    d = CustomBasic(1, 2, 3)\n    e = CustomBasic(1, 2, 3)\n\n    assert d._hashable_content() == (1, 2, 3)\n    assert d._hashable_content() is e._hashable_content()\n", "def test_sort_key():\n    # Test that the sort key method is correctly overridden\n    # in the Basic class and that it can be used to sort a list of\n    # Basic objects.\n\n    a = S(2)\n    b = S(3)\n    c = S(1)\n\n    # Make sure that the sort key method returns the correct key.\n    assert a.sort_key() == b.sort_key()\n    assert c.sort_key() < a.sort_key()\n    assert b.sort_key() > a.sort_key()\n\n    # Make sure that the sort key method can be used to sort a list\n    # of Basic objects.\n    l = [a, b, c]\n    l.sort(key=lambda x: x.sort_key())\n    assert l == [c, a, b]\n\n    # Make sure that the sort key method works with Basic objects that\n    # have a custom sort key.\n    l = [a, b, c]\n    for i in l:\n        i.sort_key = lambda: 'c'  # Make sure this is overridden\n    l.sort(key=lambda x: x.sort_key())\n    assert l == [c, b, a]\n\n    # Make sure that the sort key method works when the order is given.\n    l = [a, b, c]\n    l.sort(key=lambda x: x.sort_key(order='lex'))\n    assert l == [a, b, c]\n\n    # Make sure that the sort key method works when the order is given\n    # and is not 'lex'.\n    l = [a, b, c]\n    l.sort(key=lambda x: x.sort_key(order='rev'))\n    assert l == [c, b, a]\n\n    # Make sure that the sort key method raises a ValueError when the\n    # order is not 'lex' or 'rev'.\n    l = [a, b, c]\n    with raises(ValueError):\n        l.sort(key=lambda x: x.sort_key(order='invalid'))\n\n    # Make sure that the sort key method raises a TypeError when the\n    # order is not a string.\n    l = [a, b, c]\n    with raises(TypeError):\n        l.sort(key=lambda x: x.sort_key(order=123))\n", "def test_basic_matches():\n    b = Basic(1, 2, 3)\n    w = Basic(Wild('x'))\n    f = Function('f')\n\n    # matches() is None when self and expr don't match\n    assert b.matches(w) is None\n\n    # matches() returns a dictionary when self and expr match\n    assert b.matches(Basic(1, 2, 3)) == {}\n    # the dictionary contains values in the order they were encountered\n    assert b.matches(Basic(1, 2, 3, 4)) == {w: Basic(4)}\n\n    # matches() returns a dictionary with all subexpressions matched\n    assert (b + w).matches(b + w) == {b: b, w: w}\n\n    # matches() returns a dictionary with all subexpressions matched\n    assert ((b + w) * (b + w)).matches((b + w) * (b + w)) == {\n        b: b, w: w}\n\n    # matches() can handle more than one Wild\n    a, b, c = Basic(Wild('a')), Basic(Wild('b')), Basic(Wild('c'))\n    assert (a + b + c).matches(a + b + c) == {a: a, b: b, c: c}\n\n    # matches() can handle a Wild that is an argument\n    assert (a + b * c).matches(a + b * c) == {a: a, b: b, c: c}\n\n    # matches() works with Function\n    assert f(b + w).matches(f(b + w)) == {f: f, b: b, w: w}\n\n    # matches() works with symbols\n    x, y, z = Symbol('x'), Symbol('y'), Symbol('z')\n    assert (x + y + z).matches(x + y + z) == {x: x, y: y, z: z}\n\n    # matches() works with derivatives\n    assert (f(b).diff(x)).matches(f(b).diff(x)) == {f: f, b: b}\n\n    # matches() returns a dictionary with the corresponding Wilds replaced\n    # by the actual values\n    assert b.matches(b) == {}\n    assert (b + w).matches(b + 2) == {w: 2}\n\n    # matches() returns a dictionary", "def test_subs():\n    from sympy.core.compatibility import default_sort_key\n\n    # test that the default_sort_key is used\n    a = symbols('a')\n    b = symbols('b')\n    s = symbols('s')\n    x = symbols('x')\n    y = symbols('y')\n    z = symbols('z')\n    expr = sin(x)*y + sin(x)*z + sin(a)*s\n\n        if arg.is_Symbol:\n            return arg.name\n        else:\n            return (arg.count_ops(), len(arg.args), default_sort_key(arg))\n\n    args = expr.args\n    default_args = ordered(args, default_key)\n    assert args != default_args\n    assert default_args == ordered(args)\n\n    # test that args can be sorted in different orders\n    expr = sin(x)*y + sin(x)*z + sin(y)*z + sin(a)*s\n    default_args = ordered(expr.args, default_key)\n    sorted_args = ordered(expr.args, lambda a: a.as_coeff_Mul()[0])\n\n    assert default_args != sorted_args\n    assert sorted_args == ordered(expr.args, lambda a: a.as_coeff_Mul()[0])\n\n    # test that all() is not needed\n    args = expr.args\n    assert not all(isinstance(i, Basic) for i in args)\n    assert all(isinstance(i, Basic) for i in ordered(args))\n\n    # test that _has() can find complex expressions\n    assert expr.has(sin(x)*z)\n    assert expr.has(sin(x)*z + sin(a)*s)\n    assert not expr.has(sin(y))\n\n    # test that _has() can find atoms\n    assert expr.has(x)\n    assert expr.has(y)\n    assert not expr.has(s)\n\n    # test that _has() is faster than has()\n        return any(expr._has(arg) for arg in expr.atoms())\n\n        return any(expr.has(arg) for arg in expr.atoms())\n\n    expr = sin(x)*y + sin(x)*z + sin(a)*s\n    expr2 = sin(x)*y + sin(x)*z + sin(x)*s\n\n    assert expr.has(sin(x)*z)\n    assert expr.has(sin(x))\n    assert expr.has(sin(x)*y)\n    assert not expr.has(sin(y))\n    assert expr.has(y)\n    assert expr.has(x)\n    assert not expr.has(s)\n\n    assert"], "sample_358": ["    def setUp(self):\n        self.reference = Expressions(\n            'table', [F('name'), Upper('name')], Query(connection), lambda value: value\n        )\n", "    def test_references_table_and_column(self):\n        reference = Table('table', lambda table: table.upper())\n        self.assertTrue(reference.references_table('table'))\n        self.assertFalse(reference.references_column('table', 'column'))\n", "    def setUp(self):\n        self.reference = ForeignKeyName('table', ['column'], 'other_table', ['other_column'], 'suffix', lambda table, columns, suffix: 'fk_table_columns_suffix')\n", "    def setUp(self):\n        self.reference = Table('table', lambda table: table.upper())\n\n        class Query(Query):\n                return []\n\n        class Compiler:\n                return '', ()\n\n        class Model:\n                self.expressions = []\n\n        self.compiler = Compiler()\n        self.query = Query(Model())\n", "    def setUp(self):\n        with connection.schema_editor() as schema_editor:\n            schema_editor.execute('''\n                CREATE TABLE person (\n                    id SERIAL PRIMARY KEY,\n                    first_name VARCHAR(255),\n                    last_name VARCHAR(255)\n                );\n            ''')\n            self.person_table = Table('person', lambda table: table)\n            self.name_columns = Columns('person', ['first_name', 'last_name'], lambda table: table)\n            self.query = Query(connection)\n            self.compiler = connection.ops.compiler\n"], "sample_1002": ["def test_igcd():\n    assert igcd(2, 4) == 2\n    assert igcd(5, 10, 15) == 5\n    with raises(ValueError):\n        igcd(2)\n    with raises(ValueError):\n        igcd(2, 3, 4, 5)\n    assert igcd(4, 6, 8) == 2\n    assert igcd(-4, 6, 8) == 2\n    assert igcd(4, -6, 8) == 2\n    assert igcd(-4, -6, 8) == 2\n    assert igcd(4, 6, -8) == 2\n    assert igcd(-4, 6, -8) == 2\n    assert igcd(4, -6, -8) == 2\n    assert igcd(-4, -6, -8) == 2\n", "def test_mod_inverse():\n    from sympy.core.numbers import mod_inverse\n    assert mod_inverse(2, 7) == 4\n    assert mod_inverse(-2, 7) == 3\n    assert mod_inverse(3, 7) == 5\n    assert mod_inverse(-3, 7) == 2\n    assert mod_inverse(2, 2) == 1\n    assert mod_inverse(-2, 2) == -1\n    assert mod_inverse(0, 7) == 0\n    assert mod_inverse(0, 0) == 0\n    assert raises(ZeroDivisionError, mod_inverse, 1, 0)\n    assert mod_inverse(1, 7) == 1\n    assert mod_inverse(-1, 7) == 6\n    assert mod_inverse(7, 7) == 1\n    assert mod_inverse(-7, 7) == 0\n    assert mod_inverse(2, -7) == 0\n    assert mod_inverse(-2, -7) == 0\n    assert mod_inverse(3, -7) == 4\n    assert mod_inverse(-3, -7) == 3\n    assert mod_inverse(7, -7) == 0\n    assert mod_inverse(-7, -7) == 6\n    assert mod_inverse(7, -1) == 0\n    assert mod_inverse(-7, -1) == 0\n    assert mod_inverse(2, 1) == 0\n    assert mod_inverse(-2, 1) == 0\n    assert mod_inverse(3, 1) == 0\n    assert mod_inverse(-3, 1) == 0\n    assert mod_inverse(7, 1) == 0\n    assert mod_inverse(-7, 1) == 0\n    assert mod_inverse(2, -1) == 0\n    assert mod_inverse(-2, -1) == 0\n    assert mod_inverse(3, -1) == 0\n    assert mod_inverse(-3, -1) == 0\n    assert mod_inverse(7, -1) == 0\n    assert mod_inverse(-7, -1) == 0\n    assert mod_inverse(5, 17) == 11\n    assert mod_inverse(17, 17) == 1\n", "def test_NaN_comparison():\n    assert nan == nan\n    with raises(TypeError, \"Invalid comparison nan < nan\"):\n        nan < nan\n    with raises(TypeError, \"Invalid comparison nan <= nan\"):\n        nan <= nan\n    with raises(TypeError, \"Invalid comparison nan > nan\"):\n        nan > nan\n    with raises(TypeError, \"Invalid comparison nan >= nan\"):\n        nan >= nan\n    with raises(TypeError, \"Invalid comparison nan != nan\"):\n        nan != nan\n    with raises(TypeError, \"Invalid comparison nan == nan\"):\n        nan == nan\n    assert nan is not S.NegativeInfinity\n    assert nan is not S.Infinity\n    assert nan is not 1\n    assert nan is not -1\n    assert nan is not S.Half\n    assert nan is not S.Exp1\n    assert nan is not pi\n    assert nan is not I\n", "def test_number_symbol_powers():\n    from sympy import S, oo, exp, Pow\n    assert S.exp1**0 == 1\n    assert S.exp1**oo == oo\n    assert S.pi**0 == 1\n    assert S.pi**oo == oo\n    assert S.pi**-oo == oo\n    assert S.pi**oo**S.pi**-oo == oo\n    assert S.pi**oo**S.pi == oo\n    assert S.pi**-oo**S.pi**-oo == S.pi**-oo\n    assert S.pi**oo**S.pi**-oo**S.pi**oo == oo\n    assert S.exp1**oo**S.pi**-oo == oo\n    assert S.pi**oo**S.exp1**-oo == oo\n    assert S.pi**oo**S.pi**oo**S.pi**-oo == S.pi**oo\n    assert S.exp1**-oo**S.pi**oo == S.exp1**-oo\n    assert S.pi**oo**S.pi**oo**S.pi**-oo**S.pi**oo == S.pi**oo\n    assert S.pi**oo**S.pi**oo**S.pi**-oo**S.pi**oo**S.pi**oo == oo\n    assert S.pi**-oo**S.pi**oo == oo\n    assert S.pi**-oo**S.pi**oo**S.pi**oo == oo\n    assert S.pi**-oo**S.pi**oo**S.pi**oo**S.pi**-oo == S.pi**oo\n    assert S.pi**oo**S.pi**oo**S.pi**-oo**S.pi**oo**S.pi**-oo == oo\n    assert S.pi**oo**S.pi**oo**S.pi**-oo**S.pi**oo**S.pi**-oo == oo\n    assert S.pi**-oo**S.pi**-oo == S.pi**oo\n    assert S.pi**oo**S.pi**oo == S.pi**oo\n    assert S.pi**oo**S.pi**-oo**S.pi**oo == oo\n    assert S.pi**-oo**S.pi**-oo**S.pi**-oo == oo\n    assert S.pi**oo**S.pi**-oo**S.pi**oo**S.pi**-oo == oo\n    assert S.pi**oo**S", "def test_infinity_mod_inverse():\n    from sympy import oo\n    assert mod_inverse(oo, 5) is oo\n    assert mod_inverse(oo, -5) is oo\n    assert mod_inverse(5, oo) == 0\n    assert mod_inverse(-5, oo) == 0\n    assert mod_inverse(oo, 0) == oo\n    assert raises(ZeroDivisionError, mod_inverse, 5, 0)\n    assert raises(ZeroDivisionError, mod_inverse, -5, 0)\n    assert raises(ZeroDivisionError, mod_inverse, oo, 0)\n    assert raises(ZeroDivisionError, mod_inverse, -oo, 0)\n"], "sample_513": ["def test_legend_title_font_properties():\n    \"\"\"\n    Test that the title font properties are correctly set from the title_fontproperties\n    argument, and that setting title_fontsize overrides title_fontproperties.\n    \"\"\"\n    fig, ax = plt.subplots()\n    ax.plot([1, 2])\n    ax.legend(title='Test', title_fontproperties=FontProperties(size=12))\n    assert ax.get_legend().get_title().get_size_in_points() == 12\n\n    ax.legend(title='Test', title_fontsize=10)\n    assert ax.get_legend().get_title().get_size_in_points() == 10\n\n    ax.legend(title='Test', title_fontproperties=FontProperties(size=12, family='serif'))\n    assert ax.get_legend().get_title().get_size_in_points() == 12\n    assert ax.get_legend().get_title().get_family() == 'serif'\n\n    with rc_context(rc={'legend.title_fontsize': 10}):\n        ax.legend(title='Test', title_fontproperties=FontProperties(size=12))\n        assert ax.get_legend().get_title().get_size_in_points() == 10\n\n    with rc_context(rc={'legend.title_fontproperties': FontProperties(size=12)}):\n        ax.legend(title='Test', title_fontsize=10)\n        assert ax.get_legend().get_title().get_size_in_points() == 10\n", "def test_legend_title_font_properties_none():\n    fig, ax = plt.subplots()\n    handles = [mlines.Line2D([0], [0], label='line1'),\n               mlines.Line2D([0], [0], label='line2')]\n    ax.legend(handles=handles, title='Test', title_fontproperties=None)\n    assert ax.get_legend().get_title().get_fontproperties().get_size_in_points() == 10\n", "def test_DraggableLegend():\n    with rc_context(rc={'font.size': 16}):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3], label='Line 1')\n        ax.plot([4, 5, 6], [4, 5, 6], label='Line 2')\n        legend = mlegend.Legend(ax, [mlines.Line2D([0], [0])], ['Line 1'], loc='upper right')\n        ax.add_artist(legend)\n        draggable_legend = legend.set_draggable(True)\n        fig.canvas.draw()\n        draggable_legend._draggable.start_drag(100, 100)\n        assert draggable_legend.get_bbox_to_anchor().get_xy()[1] > 0\n        draggable_legend._draggable.stop_drag()\n        draggable_legend.set_draggable(False)\n        draggable_legend._draggable = None\n", "    def test_set_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        legend = ax.legend([mlines.Line2D([0, 1], [0, 1])], ['line'])\n        legend.set_bbox_to_anchor((0.5, 0.5))\n        assert legend.get_bbox_to_anchor().x0 == 0.5\n        assert legend.get_bbox_to_anchor().y0 == 0.5\n        assert legend.get_bbox_to_anchor().width == 0\n        assert legend.get_bbox_to_anchor().height == 0\n", "def test_set_frame_on():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    leg = ax.legend()\n    assert leg.get_frame_on()\n    leg.set_frame_on(False)\n    assert not leg.get_frame_on()\n    leg.set_frame_on(True)\n    assert leg.get_frame_on()\n"], "sample_711": ["def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node()\n    assert list(iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    parent_nodeids = list(iterparentnodeids(nodeid))\n    assert parent_nodeids == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    \"\"\"Test the iterparentnodeids function.\"\"\"\n    node = nodes.Node(name=\"\", parent=None, nodeid=nodeid)\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node(name=\"test\", parent=None, nodeid=nodeid)\n    parent_nodeids = list(iterparentnodeids(node.nodeid))\n    assert parent_nodeids == expected\n", "def test_iterparentnodeids(separate_names):\n    # Test case: a file with several nodes and its parent path is not provided\n    # in the FSCollector.from_parent call.\n    collector = nodes.FSCollector.from_parent(\n        parent=nodes.FSCollector.from_parent(\n            parent=nodes.File.from_parent(\n                parent=nodes.FSCollector.from_parent(\n                    parent=nodes.File.from_parent(\n                        parent=nodes.FSCollector.from_parent(\n                            parent=nodes.File.from_parent(\n                                parent=nodes.FSCollector(\n                                    name=\"parent\",\n                                    path=Path(\"path/to/file1.py\"),\n                                )\n                            ),\n                            name=\"file2\",\n                            fspath=legacy_path(\"path/to/file2.py\"),\n                        )\n                    ),\n                    name=\"file3\",\n                    path=Path(\"path/to/file3.py\"),\n                )\n            ),\n            name=\"file4\",\n            path=Path(\"path/to/file4.py\"),\n        ),\n        name=\"file5\",\n        path=Path(\"path/to/file5.py\"),\n    )\n    assert list(iterparentnodeids(\"file5::test_func\")) == [\n        \"\",\n        \"path/to\",\n        \"path/to/file5.py\",\n        \"path/to/file5.py::file5\",\n        \"path/to/file5.py::file5::test_func\",\n    ]\n"], "sample_1203": ["def test_block_homomorphism():\n    # Test that the block homomorphism is correctly computed\n    # when the block system is trivial\n    group = AlternatingGroup(4)\n    blocks = list(range(4))\n    H = block_homomorphism(group, blocks)\n    assert isinstance(H, GroupHomomorphism)\n    assert H.domain == group\n    assert H.codomain == SymmetricGroup(4)\n    assert H.images[group.gen1] == Permutation([0, 1, 2, 3])\n    assert H.images[group.gen2] == Permutation([0, 2, 1, 3])\n\n    # Test that the block homomorphism is correctly computed\n    # when the block system has more blocks than the degree of the group\n    group = AlternatingGroup(3)\n    blocks = list(range(4))\n    with raises(ValueError):\n        block_homomorphism(group, blocks)\n\n    # Test that the block homomorphism is correctly computed\n    # when the block system has less blocks than the degree of the group\n    group = AlternatingGroup(4)\n    blocks = [0, 1, 2]\n    H = block_homomorphism(group, blocks)\n    assert isinstance(H, GroupHomomorphism)\n    assert H.domain == group\n    assert H.codomain == SymmetricGroup(4)\n    assert H.images[group.gen1] == Permutation([0, 1, 2, 3])\n    assert H.images[group.gen2] == Permutation([0, 2, 1, 3])\n\n    # Test that the block homomorphism is correctly computed\n    # when the block system is the identity permutation\n    group = AlternatingGroup(4)\n    blocks = [0, 1, 2, 3]\n    H = block_homomorphism(group, blocks)\n    assert isinstance(H, GroupHomomorphism)\n    assert H.domain == group\n    assert H.codomain == SymmetricGroup(4)\n    assert H.images[group.gen1] == Permutation([0, 1, 2, 3])\n    assert H.images[group.gen2] == Permutation([0, 2, 1, 3])\n", "def test_block_homomorphism_injective():\n    from sympy.combinatorics import Permutation\n    from sympy.combinatorics.perm_groups import PermutationGroup\n    from sympy.combinatorics.named_groups import DihedralGroup\n\n    # Define a DihedralGroup acting on 3 blocks\n    blocks = [0, 1, 0, 1, 0, 1, 0, 0]\n    group = DihedralGroup(8)\n    H = block_homomorphism(group, blocks)\n\n    # Check that the homomorphism is injective\n    assert H.is_injective()\n\n    # Check that the kernel of the homomorphism is trivial\n    kernel = H.kernel()\n    assert kernel.order() == 1\n", "def test_group_isomorphism_infinite_groups():\n    F, a = free_group('a')\n    G = FpGroup(F, [a**2])\n    H = FpGroup(F, [a**3])\n    assert not group_isomorphism(G, H)\n    assert not is_isomorphic(G, H)\n", "def test_holomorphism_kernel_and_image():\n    # Test that the kernel and image methods work correctly\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n\n    # Create a homomorphism from G to H\n    T = homomorphism(G, H, [a, b], [H((1, 3)), H((2, 4))])\n\n    # Test that the kernel method works correctly\n    assert T.kernel() == FpSubgroup(G, [H((0, 2, 3))])\n\n    # Test that the image method works correctly\n    assert T.image() == H\n\n    # Test that the invert method works correctly\n    assert T.invert(H((0, 2, 3))) == a*b**-1\n\n    # Test that the invert method works correctly for a list of elements\n    assert T.invert([H((0, 2, 3)), H((1, 4))]) == [a*b**-1, H((1, 4))]\n\n    # Test that the invert method works correctly for a permutation\n    assert T.invert(Permutation((0, 2, 3))) == a*b**-1\n\n    # Test that the invert method works correctly for a free group element\n    F, x, y = free_group(\"x, y\")\n    G = FpGroup(F, [x**3, y**3, (x*y)**2])\n    assert G.reduce(x**-1*y**-1).invert(x**-1*y**-1) == x**-1*y**-1\n", "def test_homomorphism_with_free_group():\n    F, a, b = free_group('a, b')\n    G = FpGroup(F, [a**2, b**2, (a*b)**2])\n    H = AlternatingGroup(4)\n    # The homomorphism should not be injective since it maps the generators to the same element\n    homomorphism = homomorphism(G, H, [a, b], [H((0, 2)), H((0, 2))])\n    assert not homomorphism.is_injective()\n"], "sample_24": ["    def check(self, func, *args, **kwargs):\n        if func in (np.histogram, np.histogram2d, np.histogramdd):\n            for kw in [\"weights\", \"density\"]:\n                if kw in kwargs:\n                    del kwargs[kw]\n        out = func(self.a, *args, **kwargs)\n        expected = func(self.a.value, *args, **kwargs)\n        if isinstance(out, tuple):\n            out = out[0]\n        assert_array_equal(out, expected)\n", "    def setup(self):\n        super().setup()\n        self.ma = np.ma.array([[1, 2, 3], [4, 5, 6]], mask=[[0, 1, 0], [0, 0, 1]])\n        self.a = np.array([[1, 2, 3], [4, 5, 6]])\n        self.mb = np.ma.array([[7, 8, 9], [10, 11, 12]], mask=[[0, 0, 1], [1, 0, 0]])\n        self.b = np.array([[7, 8, 9], [10, 11, 12]])\n", "    def test_choose(self):\n        # Test that choosing between quantities works\n        a = self.ma\n        b = Masked(np.array([1, 2, 3]), mask=np.array([True, False, True]))\n        expected = Masked(np.array([2, 3, 2]), mask=np.array([False, True, True]))\n        out = np.choose(a, [a, b], out=self.ma)\n        assert_masked_equal(out, expected)\n\n        # Test that choosing between quantities with different units works\n        a = self.ma * self.u2\n        b = Masked(np.array([1, 2, 3]), mask=np.array([True, False, True]))\n        expected = Masked(np.array([2, 3, 2]), mask=np.array([False, True, True]))\n        out = np.choose(a, [a, b], out=self.ma)\n        assert_masked_equal(out, expected)\n\n        # Test that choosing between a quantity and a non-quantity works\n        a = self.ma\n        b = 2\n        expected = Masked(np.array([2, 3, 2]), mask=np.array([False, True, True]))\n        out = np.choose(a, [a, b], out=self.ma)\n        assert_masked_equal(out, expected)\n", "    def check(self, func, *args, **kwargs):\n        out = func(self.ma)\n        expected = func(self.a)\n        assert_array_equal(out.shape, expected.shape)\n", "    def setup(self):\n        self.a = np.linspace(0, 10, 11)\n        self.ma = Masked(self.a, mask=[False, False, True, False, False, False, False, False, False, False, False])\n        self.mask_a = np.array([False, False, True, False, False, False, False, False, False, False, False])\n        self.xp = np.linspace(0, 10, 11)\n        self.fp = np.sin(self.a)\n"], "sample_660": ["def test_record_property_with_invalid_name():\n    \"\"\"Verify that record_property raises an error with an invalid parameter type.\"\"\"\n    testdir = py.path.local()\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    properties_node = dom.find_first_by_tag(\"properties\")\n    assert properties_node is None\n\n        record_property(123, 1)  # invalid parameter type\n\n    result, dom = runandparse(testdir, test_function)\n    assert result.ret != 0\n    properties_node = dom.find_first_by_tag(\"properties\")\n    assert properties_node is not None\n    assert_attr(properties_node, numtests=\"1\", errors=\"1\")\n", "def test_mangle_test_address():\n    report = BaseReport()\n    report.nodeid = \"path.to.module.TestClass.test_method\"\n    names = mangle_test_address(report.nodeid)\n    assert names == [\"path.to.module\", \"TestClass\", \"test_method\"]\n", "def test_LogXML_finalize():\n    testdir = py.test.TempdirFactory().mkdtemp()\n    resultpath = testdir.join(\"junit.xml\")\n    logxml = LogXML(resultpath)\n    report = BaseReport(\n        \"nodeid\",\n        \"test_function\",\n        testdir.join(\"test_function.py\"),\n        testdir.join(\"test_function.py\").abspath(),\n        None,\n        None,\n        None,\n        None,\n        False,\n        False,\n        None,\n        None,\n        None,\n    )\n    logxml.node_reporter(report).finalize()\n    result, domnode = runandparse(testdir)\n    testcases = domnode.find_by_tag(\"testcase\")\n    assert len(testcases) == 1\n    testcase = testcases[0]\n    assert_attr(testcase, name=\"test_function\", classname=\"nodeid\")\n    assert_attr(testcase, time=\"0.000\")\n    assert_attr(testcase, file=\"test_function.py\")\n    assert_attr(testcase, line=\"1\")\n", "    def test_init_properties(self):\n        xml = LogXML(\n            logfile=\"test_log.xml\",\n            prefix=\"test_prefix\",\n            suite_name=\"test_suite\",\n            logging=\"stdout\",\n            report_duration=\"call\",\n            family=\"xunit2\",\n            log_passing_tests=False,\n        )\n        assert_attr(xml, name=\"test_log.xml\", text=\"\")\n        assert_attr(xml, prefix=\"test_prefix\")\n        assert_attr(xml, suite_name=\"test_suite\")\n        assert_attr(xml, logging=\"stdout\")\n        assert_attr(xml, report_duration=\"call\")\n        assert_attr(xml, family=\"xunit2\")\n        assert_attr(xml, log_passing_tests=\"false\")\n", "def test_pytest_sessionfinish_suites_and_properties():\n    testdir = py.testinfra.frompath(py.path.local.path(__file__).dirname())\n    result, doc = runandparse(testdir)\n    assert len(doc.find_by_tag(\"testsuite\")) == 1\n    suite = doc.find_first_by_tag(\"testsuite\")\n    assert_attr(suite, name=\"pytest\", tests=0, failures=0, errors=0, skipped=0, time=\"0.000\")\n    props = suite.find_by_tag(\"properties\")\n    assert len(props) == 0\n"], "sample_405": ["    def test_reduce_delete_model(self):\n        with self.assertRaises(ValueError):\n            DeleteModel(\"myapp.MyModel\").reduce(CreateModel(\"myapp.MyModel\", []), \"myapp\")\n        self.assertEqual(DeleteModel(\"myapp.MyModel\").reduce(DeleteModel(\"myapp.MyModel\"), \"myapp\"), [])\n", "    def test_rename_index_with_fields(self):\n        UnicodeModel._meta.get_field(\"field2\").db_index = True\n        with transaction.atomic():\n            with CaptureQueriesContext(connection) as capture:\n                with transaction.atomic():\n                    migration = self._create_migration(\n                        self._create_project_state(\n                            app_label=\"unicode\", state=\"after\"\n                        ),\n                        \"rename_index\",\n                        [\n                            RenameIndex(\n                                model_name=\"unicode.unicode\",\n                                new_name=\"custom_index\",\n                                old_name=\"field2\",\n                                old_fields=[\"field2\"],\n                            )\n                        ],\n                    )\n                    migration.apply(project_state=self._create_project_state(\n                        app_label=\"unicode\", state=\"before\"\n                    ))\n\n                self.assertEqual(\n                    len(capture.captured_queries),\n                    1,\n                    \"Expected a single query to rename the index\",\n                )\n                self.assertEqual(\n                    capture.captured_queries[0],\n                    (\n                        \"ALTER TABLE unicode_unicode \"\n                        \"RENAME CONSTRAINT unicode_unicode_custom_index \"\n                        \"TO field2\"\n                    ),\n                )\n\n                with transaction.atomic():\n                    migration.apply(\n                        project_state=self._create_project_state(app_label=\"unicode\", state=\"after\")\n                    )\n\n                self.assertEqual(\n                    len(capture.captured_queries),\n                    0,\n                    \"Expected no queries to rename the index\",\n                )\n", "    def test_rename_index(self):\n        self.create_model(\n            \"RenameIndex\", \"field_1\", \"model1\", \"RenameIndexTest\"\n        )\n\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"new_index_name\",\n                \"old_index_name\",\n                [\"field_1\"],\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"new_index_name\",\n                None,\n                [\"field_1\"],\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n\n        # Test that renaming an index with no fields to a name that already exists\n        # raises IntegrityError.\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"old_index_name\",\n                \"old_index_name\",\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n\n        # Test that renaming an index with fields to a name that already exists\n        # raises IntegrityError.\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"old_index_name\",\n                \"old_index_name\",\n                [\"field_1\"],\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n\n        # Test that renaming an index with fields that does not exist raises\n        # IntegrityError.\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"new_index_name\",\n                \"non-existent-index\",\n                [\"field_1\"],\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n\n        # Test that renaming an index from a name that does not exist raises\n        # IntegrityError.\n        with self.assertRaises(IntegrityError):\n            RenameIndex(\n                \"RenameIndexTest\",\n                \"new_index_name\",\n                \"non-existent-index\",\n            ).apply(\n                connection,\n                self.Model._meta.db,\n            )\n", "    def test_alter_index_together_model_with_indexes(self):\n        # Create a model with indexes\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(text=\"test\")\n        UnicodeModel.objects.create(text=\"test2\")\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(text=\"test\")\n        UnicodeModel.objects.create(text=\"test2\")\n\n        # Create migration\n        with self.settings(INDEX_TOGETHER=[(\"text\",), (\"text\", \"id\")]):\n            migration = migration.CreateModel(\n                name=\"unicode\",\n                fields=[\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"text\", models.CharField(max_length=10)),\n                    (\"id2\", models.AutoField(primary_key=True)),\n                ],\n                options={\n                    \"indexes\": [\n                        models.Index(fields=[\"text\"]),\n                        models.Index(fields=[\"id2\"]),\n                    ],\n                },\n            )\n\n        # Run migration\n        migration.apply(project_state=ModelState(\n            app_label=\"tests\",\n            name=\"unicode\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"text\", models.CharField(max_length=10)),\n                (\"id2\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"indexes\": [\n                    models.Index(fields=[\"text\"]),\n                    models.Index(fields=[\"id2\"]),\n                ],\n            },\n            bases=(models.Model,),\n            managers=[],\n        ))\n\n        # Alter index_together\n        with self.settings(INDEX_TOGETHER=[(\"text\",)]):\n            migration = migration.AlterIndexTogether(\n                name=\"unicode\",\n                index_together=[\"text\"],\n            )\n\n        # Run migration\n        migration.apply(project_state=ModelState(\n            app_label=\"tests\",\n            name=\"unicode\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"text\", models.CharField(max_length=10)),\n                (\"id2\", models.AutoField(primary_key=True)),\n            ],\n            options={\n                \"indexes\": [\n                    models.Index(fields=[\"text\"]),\n                    models.Index(fields=[\"id2\"]),\n                    models.Index(fields=[\"text\"]),\n                ],\n            },\n            bases=(models.Model,),\n            managers=[],\n        ))\n\n        # Test database state\n        with CaptureQueriesContext() as queries:\n            with self.assertNumQueries(0):\n                UnicodeModel.objects.create(text=\"test\")\n            with self.assertNumQueries(0):\n                UnicodeModel.objects.create(text=\"test2\")\n\n        # Test database state after reversing migration\n        migration = migration.ReverseOperation(\n", "    def test_add_index(self):\n        # Create a new model with an index\n        with self.assertRaises(MigrationError):\n            with self.assertRaises(MigrationError):\n                with CaptureQueriesContext() as context:\n                    with transaction.atomic():\n                        Migration(\n                            \"0003_auto_20230201_1457\",\n                            initial=True,\n                        ).apply(project_state=ProjectState(self.apps, self.loader, [], [], [], {}, {}),\n                               schema_editor=MigrationTestStatefulSchemaEditor(self.connection),\n                               recorder=MigrationRecorder(self.loader))\n                self.assertEqual(len(context.captured_queries), 0)\n\n        # Create an index on a field that does not exist\n        with self.assertRaises(FieldDoesNotExist):\n            Migration(\n                \"0003_auto_20230201_1458\",\n                dependencies=[Migration(\"0002_auto_20230201_1457\")],\n            ).apply(project_state=ProjectState(self.apps, self.loader, [], [], [], {}, {}),\n                   schema_editor=MigrationTestStatefulSchemaEditor(self.connection),\n                   recorder=MigrationRecorder(self.loader))\n\n        # Create an index on a field that does exist\n        with self.assertRaises(MigrationError):\n            with self.assertRaises(MigrationError):\n                with CaptureQueriesContext() as context:\n                    with transaction.atomic():\n                        Migration(\n                            \"0003_auto_20230201_1459\",\n                            dependencies=[Migration(\"0002_auto_20230201_1457\")],\n                        ).apply(project_state=ProjectState(self.apps, self.loader, [], [], [], {}, {}),\n                               schema_editor=MigrationTestStatefulSchemaEditor(self.connection),\n                               recorder=MigrationRecorder(self.loader))\n                self.assertEqual(len(context.captured_queries), 0)\n\n        # Create an index on multiple fields\n        with self.assertRaises(MigrationError):\n            with self.assertRaises(MigrationError):\n                with CaptureQueriesContext() as context:\n                    with transaction.atomic():\n                        Migration(\n                            \"0003_auto_20230201_1460\",\n                            dependencies=[Migration(\"0002_auto_20230201_1457\")],\n                        ).apply(project_state=ProjectState(self.apps, self.loader, [], [], [], {}, {}),\n                               schema_editor=MigrationTestStatefulSchemaEditor(self.connection),\n                               recorder=MigrationRecorder(self.loader))\n                self.assertEqual(len(context.captured_queries), 0)\n\n        # Delete a model\n        with self.assertRaises(MigrationError):\n            with self.assertRaises(MigrationError):\n                with CaptureQueriesContext() as"], "sample_886": ["def test_auto_wrap_is_configured_with_global_config_set_to_pandas():\n    with config_context(get_config({\"transform_output\": \"pandas\"})):\n        class TestEstimator(_SetOutputMixin):\n                return X\n\n                return X\n\n        estimator = TestEstimator()\n        assert _auto_wrap_is_configured(estimator)\n        assert _auto_wrap_is_configured(estimator)._sklearn_auto_wrap_output_keys == {\"transform\"}\n        assert _auto_wrap_is_configured(estimator).get_feature_names_out is None\n        assert isinstance(_wrap_data_with_container(\"transform\", np.array([1, 2]), np.array([1, 2]), estimator), np.ndarray)\n", "def test_get_output_config_estimator_config_not_defined():\n    class Estimator:\n        pass\n\n    estimator = Estimator()\n    method = \"transform\"\n    output_config = _get_output_config(method, estimator)\n    assert output_config[\"dense\"] == \"default\"\n", "def test_get_output_config_none_estimator():\n    config = get_config()\n    config[\"transform_output\"] = \"pandas\"\n    with config_context(config):\n        result = _get_output_config(\"transform\")\n        assert result == {\"dense\": \"pandas\"}\n", "def test_get_output_config_defaults_to_global_config():\n    with config_context({\"transform_output\": \"pandas\"}):\n        estimator = _SetOutputMixin()\n        assert _get_output_config(\"transform\", estimator) == {\"dense\": \"pandas\"}\n\n    with config_context({\"transform_output\": \"default\"}):\n        estimator = _SetOutputMixin()\n        assert _get_output_config(\"transform\", estimator) == {\"dense\": \"default\"}\n\n    with pytest.raises(ValueError):\n        _get_output_config(\"transform\", estimator)  # uses global config\n\n    with pytest.raises(ValueError):\n        _get_output_config(\"transform\", estimator)  # uses global config with invalid value\n", "def test_safe_set_output_config_invalid_output_type():\n    \"\"\"Test that `_safe_set_output` raises an error if an invalid output type is provided.\"\"\"\n    with pytest.raises(ValueError):\n        _safe_set_output(\n            estimator=_SetOutputMixin(),\n            transform=\"invalid_output\",\n        )\n\n"], "sample_461": ["    def test_slug_field_with_empty_value(self):\n        field = SlugField()\n        with self.assertRaises(ValidationError):\n            field.run_validators('')\n", "    def test_url_field_blank_value(self):\n        field = URLField()\n        self.assertEqual(field.to_python(\"\"), \"\")\n        self.assertEqual(field.to_python(None), None)\n        self.assertEqual(field.to_python([]), \"\")\n        self.assertEqual(field.to_python({}), \"\")\n", "    def test_to_python_on_string(self):\n        field = CharField()\n        self.assertEqual(field.to_python('hello'), 'hello')\n        self.assertEqual(field.to_python('hello world'), 'hello world')\n        self.assertEqual(field.to_python('   hello  world   '), 'hello world')\n", "    def test_default_regex(self):\n        field = RegexField(regex='^[a-zA-Z0-9]+')\n        field.full_clean()\n        self.assertEqual(field.error_messages, {\n            \"invalid\": _(\"Enter a string of 1 to 255 characters.\"),\n        })\n", "    def test_disabled_json_field(self):\n        field = JSONField(disabled=True)\n        self.assertTrue(field.disabled)\n        value = '{\"key\": \"value\"}'\n        self.assertEqual(field.clean(value), value)\n        self.assertEqual(field.to_python(value), value)\n"], "sample_313": ["def test_get_template_directories_empty(self):\n    with mock.patch('django.template.engines.all') as engines_all:\n        engines_all.return_value = [\n            mock.Mock(spec=DjangoTemplates),\n            mock.Mock(spec=DjangoTemplates),\n        ]\n        for engine in engines_all.return_value:\n            engine.dirs = []\n            engine.engine.template_loaders = []\n        result = get_template_directories()\n        self.assertEqual(result, set())\n", "    def test_watch_for_template_changes(self):\n        with mock.patch('django.template.autoreload.watcher.watch_dir') as watch_dir_mock:\n            watch_for_template_changes(None)\n            watch_dir_mock.assert_called_once_with(Path.cwd(), '**/*')\n", "    def test_watch_for_template_changes_watches_correct_directories(self, mock_reset_loaders):\n        # Arrange\n        extra_template_dir = Path('path/to/extra/template/dir')\n        backend = engines.all()[1]  # DjangoTemplates backend\n        backend.engine.template_loaders[0].get_dirs.return_value = [str(extra_template_dir)]\n        directory = get_template_directories()\n\n        # Act\n        watch_for_template_changes(sender=autoreload, **kwargs={})\n\n        # Assert\n        mock_reset_loaders.assert_called_once()\n        self.assertIn(str(extra_template_dir), [str(dir) for dir in directory])\n", "    def test_watch_for_template_changes(self, mock_reset_loaders):\n        # Test that watch_for_template_changes correctly watches for changes in template directories\n        with mock.patch('django.template.autoreload.autoreload_started') as mock_autoreload_started:\n            watch_for_template_changes(None, None)\n            self.assertEqual(mock_autoreload_started.call_count, 1)\n            self.assertEqual(len(mock_autoreload_started.call_args_list[0][1]['watched_paths']), len(get_template_directories()))\n", "    def test_reset_loaders_is_called_when_template_file_changed(self, mock_reset_loaders):\n        # Arrange\n        file_path = EXTRA_TEMPLATES_DIR / \"template.txt\"\n        file_path.touch()\n\n        # Act\n        autoreload.file_changed(sender=None, file_path=str(file_path))\n\n        # Assert\n        mock_reset_loaders.assert_called_once()\n"], "sample_438": ["    def test_generic_foreign_key_pickling(self):\n        # Test that a GenericForeignKey can be pickled and unpickled.\n        obj = Post(content_object=Question(title='Test question'))\n        pickled_obj = json.loads(json.dumps(obj.__dict__))\n        unpickled_obj = Post(**pickled_obj)\n        self.assertEqual(obj.content_object, unpickled_obj.content_object)\n", "def test_custom_model_validation(self):\n    \"\"\"\n    Tests that the custom model's full_clean method is called when the model's\n    clean method raises a ValidationError.\n    \"\"\"\n    question = Question()\n    question.full_clean = lambda self: self.clean()  # override full_clean\n    with self.assertRaises(ValidationError):\n        question.full_clean()\n    self.assertTrue(hasattr(question, \"_full_clean_called\"))\n    self.assertEqual(question._full_clean_called, 1)\n", "    def test_generic_foreign_key_deferred(self):\n        answer = Answer.objects.create(text=\"test\")\n        answer.question = answer\n        self.assertEqual(answer.question, answer)\n        self.assertEqual(answer.question_id, answer.id)\n        answer.refresh_from_db(fields=[\"text\", \"question\"])\n        self.assertEqual(answer.question, answer)\n        self.assertEqual(answer.question_id, answer.id)\n", "    def test_model_save_fields_by_position(self):\n        post = Post(title=\"Test Post\", content=\"Test content\")\n        post.save()\n        post.refresh_from_db()\n        self.assertEqual(post.title, \"Test Post\")\n        self.assertEqual(post.content, \"Test content\")\n", "    def test_model_pickle_unpickle(self):\n        model = models.Model()\n        model.pk = 1\n        state = model.__getstate__()\n        unpickled_model = model_unpickle((model.__class__,))(None, {}, **state)\n        self.assertEqual(model.pk, unpickled_model.pk)\n"], "sample_219": ["    def test_str(self):\n        f = F('field')\n        self.assertEqual(str(f), \"F('field')\")\n", "    def setUpTestData(cls):\n        cls.start_2020 = datetime.datetime(2020, 1, 1)\n        cls.start_2021 = datetime.datetime(2021, 1, 1)\n        cls.max = Employee.objects.create(\n            firstname='Max', lastname='Mustermann', salary=30,\n            start_date=cls.start_2020, end_date=cls.start_2020 + datetime.timedelta(days=10)\n        )\n        cls.min = Employee.objects.create(\n            firstname='Min', lastname='Minmann', salary=30,\n            start_date=cls.start_2021, end_date=cls.start_2021 + datetime.timedelta(days=10)\n        )\n        cls.start = Employee.objects.create(\n            firstname='Start', lastname='Startmann', salary=30,\n            start_date=cls.start_2020, end_date=cls.start_2020 + datetime.timedelta(days=10)\n        )\n        cls.end = Employee.objects.create(\n            firstname='End', lastname='Endmann', salary=30,\n            start_date=cls.start_2020, end_date=cls.start_2020 + datetime.timedelta(days=10)\n        )\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n\n        # create some sample data\n        for i in range(1, 6):\n            Employee.objects.create(firstname=f\"John {i}\", lastname=\"Doe\", salary=100 + i)\n", "    def test_subquery_expression(self):\n        queryset = Employee.objects.values('id', 'firstname', 'lastname').filter(firstname='Joe')\n        subquery = Subquery(queryset)\n        self.assertEqual(subquery.resolve_expression(None).output_field, fields.IntegerField())\n", "    def test_window_expression_group_by(self):\n        # We're using the order_by_window test case. The window function\n        # is not included in the GROUP BY clause.\n        self._test_window_expression_group_by(expression=OrderBy(\n            F('salary'), descending=True\n        ))\n"], "sample_240": ["    def test_make_token_with_timestamp_legacy(self):\n        # Arrange\n        user = User.objects.create_user('test', 'test@example.com')\n        token_generator = PasswordResetTokenGenerator()\n        timestamp = token_generator._num_seconds(datetime.now())\n        expected_token = token_generator._make_token_with_timestamp(user, timestamp, legacy=True)\n        \n        # Act\n        actual_token = token_generator.make_token(user)\n        \n        # Assert\n        self.assertNotEqual(expected_token, actual_token)\n", "    def test_make_token_with_timestamp(self):\n        token_generator = MockedPasswordResetTokenGenerator(datetime(2022, 1, 1))\n        user = User.objects.create_user('testuser', 'testuser@example.com', 'password123')\n        token = token_generator.make_token(user)\n        self.assertIn('202201', token)  # timestamp should be in base36 format\n        self.assertEqual(len(token), 16)  # token length should be 16 characters\n", "    def test_make_token_with_timestamp_legacy_algorithm(self):\n        # Test that _make_token_with_timestamp generates a valid token when using the legacy algorithm\n        token_generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 1, 1))\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        timestamp = token_generator._num_seconds(datetime(2022, 1, 1))\n        token = token_generator._make_token_with_timestamp(user, timestamp, legacy=True)\n        self.assertEqual(len(token), 14)  # 6 digits for timestamp + 8 digits for hash\n", "    def test_make_hash_value(self):\n        user = User.objects.create_user(\n            username=\"testuser\",\n            email=\"test@example.com\",\n            password=\"password123\"\n        )\n        token_generator = MockedPasswordResetTokenGenerator(datetime(2022, 1, 1, 12, 0, 0))\n        self.assertIsNotNone(token_generator._make_hash_value(user, 1640995200))\n\n        # Test that hash value doesn't include empty email if email is not set\n        user = User.objects.create_user(\n            username=\"testuser\",\n            password=\"password123\"\n        )\n        self.assertNotIn(user.get_email_field_name(), token_generator._make_hash_value(user, 1640995200))\n\n        # Test that hash value includes password\n        self.assertIn(user.password, token_generator._make_hash_value(user, 1640995200))\n\n        # Test that hash value includes last login timestamp\n        user.last_login = datetime(2022, 1, 1, 12, 0, 0)\n        user.save()\n        self.assertIn(str(user.last_login), token_generator._make_hash_value(user, 1640995200))\n\n        # Test that hash value includes timestamp\n        self.assertIn(\"1640995200\", token_generator._make_hash_value(user, 1640995200))\n\n        # Test that hash value includes email\n        user.email = \"new_email@example.com\"\n        self.assertIn(user.email, token_generator._make_hash_value(user, 1640995200))\n", "    def test_make_token_with_timestamp_legacy_token(self):\n        # Create a user with a legacy password\n        user = User.objects.create_user(username='testuser', password='legacy_password')\n        # Simulate an older token (before Django 3.1)\n        now = datetime(2020, 1, 1)\n        generator = MockedPasswordResetTokenGenerator(now)\n        token = generator.make_token(user)\n        self.assertIsInstance(token, str)\n        # Check that we can still verify the token\n        self.assertTrue(generator.check_token(user, token))\n"], "sample_1081": ["def test_smoothness():\n    assert smoothness(2**7*3**2) == (3, 128)\n    assert smoothness(2**4*13) == (13, 16)\n    assert smoothness(2) == (2, 2)\n    assert smoothness(1) == (1, 1)\n", "def test_divisor_sigma():\n    assert divisor_sigma(18) == 38\n    assert divisor_sigma(39) == 56\n    assert divisor_sigma(12, 3) == 210\n    assert divisor_sigma(37) == 38\n    assert divisor_sigma(1) == 1\n    assert divisor_sigma(-5) == 7\n    assert divisor_sigma(0) == 1\n    assert divisor_sigma(0, 0) == 1\n    assert divisor_sigma(0, 1) == 0\n    assert divisor_sigma(1, 0) == 1\n    assert divisor_sigma(1, 1) == 1\n    assert divisor_sigma(1, 2) == 2\n    assert divisor_sigma(2, 0) == 2\n    assert divisor_sigma(2, 1) == 3\n    assert divisor_sigma(2, 2) == 5\n    assert divisor_sigma(3, 0) == 2\n    assert divisor_sigma(3, 1) == 4\n    assert divisor_sigma(3, 2) == 10\n    assert divisor_sigma(4, 0) == 3\n    assert divisor_sigma(4, 1) == 5\n    assert divisor_sigma(4, 2) == 15\n    assert divisor_sigma(5, 0) == 2\n    assert divisor_sigma(5, 1) == 6\n    assert divisor_sigma(5, 2) == 31\n    assert divisor_sigma(6, 0) == 4\n    assert divisor_sigma(6, 1) == 10\n    assert divisor_sigma(6, 2) == 61\n    assert divisor_sigma(7, 0) == 2\n    assert divisor_sigma(7, 1) == 8\n    assert divisor_sigma(7, 2) == 57\n    assert divisor_sigma(8, 0) == 4\n    assert divisor_sigma(8, 1) == 15\n    assert divisor_sigma(8, 2) == 97\n    assert divisor_sigma(9, 0) == 5\n    assert divisor_sigma(9, 1) == 13\n    assert divisor_sigma(9, 2) == 101\n    assert divisor_sigma(10, 0) == 4\n    assert divisor", "def test_primefactors():\n    assert primefactors(5) == [5]\n    assert primefactors(13) == [13]\n    assert primefactors(23) == [23]\n    assert primefactors(37) == [37]\n    assert primefactors(47) == [47]\n", "def test_core():\n    from sympy.ntheory.factor_ import core\n    assert core(16) == 2\n    assert core(32) == 2\n    assert core(64) == 2\n    assert core(81) == 3\n    assert core(100) == 2\n    assert core(256) == 2\n    assert core(624) == 3\n    assert core(1279) == 2\n    assert core(2176782337) == 3\n    assert core(2**31 - 1) == 2\n    assert core(2**32 - 1) == 2\n    assert core(2**63 - 1) == 2\n", "def test_primefactors():\n    assert primefactors(6) == [2, 3]\n    assert primefactors(-5) == [5]\n    assert primefactors(0) == []\n    assert primefactors(1) == []\n"], "sample_560": ["def test_set_loc_overrides_rcParams():\n    # Test that passing loc to the legend constructor overrides rcParams\n    with rc_context(rc={'legend.loc': 'best'}) as rc_context:\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        legend = mlegend.Legend(ax, [mlines.Line2D([0], [0])], [\"Label\"])\n        assert legend.get_loc() == \"best\"\n\n        legend.set_loc('upper right')\n        assert legend.get_loc() == \"upper right\"\n\n        legend2 = mlegend.Legend(ax, [mlines.Line2D([0], [0])], [\"Label\"],\n                                  loc='lower left')\n        assert legend2.get_loc() == \"lower left\"\n", "def test_set_bbox_to_anchor_transform():\n    fig, ax = plt.subplots()\n    line = mlines.Line2D([0, 1], [0, 1])\n    ax.add_line(line)\n\n    legend = mlegend.Legend(ax, [line], [line.get_label()])\n    legend.set_bbox_to_anchor((0.5, 0.5))\n    assert_allclose(legend.get_bbox_to_anchor().x0, 0.5)\n    assert_allclose(legend.get_bbox_to_anchor().y0, 0.5)\n\n    # Test that a valid Transform object is passed to BboxTransformTo\n    transform = mtransforms.IdentityTransform()\n    legend.set_bbox_to_anchor((0.5, 0.5), bbox_transform=transform)\n    assert isinstance(legend.get_bbox_to_anchor().get_transform(),\n                     BboxTransformTo)\n\n    # Test that None is passed to BboxTransformTo when bbox_transform is None\n    legend.set_bbox_to_anchor((0.5, 0.5), bbox_transform=None)\n    assert legend.get_bbox_to_anchor().get_transform() is None\n\n    # Test that an invalid Transform object raises an error\n    with pytest.raises(TypeError):\n        legend.set_bbox_to_anchor((0.5, 0.5), bbox_transform='not a transform')\n", "def test_set_bbox_to_anchor():\n    \"\"\"\n    Test that set_bbox_to_anchor works with valid and invalid inputs.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n\n        fig, ax = plt.subplots()\n        legend = mlegend.Legend(ax, [], [])\n\n        # Valid input\n        legend.set_bbox_to_anchor((0.5, 0.5))\n        assert legend.get_bbox_to_anchor() == Bbox.from_bounds(0.5, 0.5, 0, 0)\n\n        # Invalid input\n        with pytest.raises(ValueError):\n            legend.set_bbox_to_anchor((0, 0, 0, 0))\n\n        # Invalid input\n        with pytest.raises(TypeError):\n            legend.set_bbox_to_anchor(\"invalid\")\n\n        # Valid input with transform\n        transform = mtransforms.Affine2D().scale(2, 2)\n        legend.set_bbox_to_anchor((0.5, 0.5), transform=transform)\n        assert legend.get_bbox_to_anchor().transform == transform\n\n        # Invalid input with transform\n        with pytest.raises(ValueError):\n            legend.set_bbox_to_anchor(0.5)\n\n        # Invalid input with transform\n        with pytest.raises(TypeError):\n            legend.set_bbox_to_anchor((0, 0, 0, 0), transform=\"invalid\")\n\n    with rc_context(rc={'axes.facecolor': 'green'}):\n        fig, ax = plt.subplots()\n        legend = mlegend.Legend(ax, [], [])\n\n        # Test that set_bbox_to_anchor respects the parent bbox\n        legend.set_bbox_to_anchor(None)\n        assert legend.get_bbox_to_anchor() == Bbox.from_bounds(0, 0, 0, 0)\n\n    with rc_context(rc={'axes.facecolor': 'blue'}):\n        fig, ax = plt.subplots()\n        legend = mlegend.Legend(ax, [], [])\n\n        # Test that set_bbox_to_anchor respects the parent bbox\n        legend.set_bbox_to_anchor((0, 0, 0, 0))\n        assert legend.get_bbox_to_anchor() == Bbox.from_bounds(0, 0, 0, 0)\n", "def test_legend_draw_frame_with_alpha():\n    # test that framealpha is correctly applied to the legend's background\n    with rc_context({'legend.framealpha': 0.5}):\n        fig, ax = plt.subplots()\n        line = ax.plot([1, 2, 3])[0]\n        line.set_label('line')\n        ax.legend()\n\n    with rc_context({'legend.framealpha': None}):\n        fig, ax = plt.subplots()\n        line = ax.plot([1, 2, 3])[0]\n        line.set_label('line')\n        ax.legend()\n\n    # test that shadow and framealpha cannot be set at the same time\n    with pytest.raises(ValueError):\n        fig, ax = plt.subplots()\n        line = ax.plot([1, 2, 3])[0]\n        line.set_label('line')\n        ax.legend(shadow=True, framealpha=0.5)\n", "def test_draggable_legend():\n    \"\"\"Test that a draggable legend works as expected.\"\"\"\n    # Check that creating a draggable legend works\n    fig, ax = plt.subplots()\n    ax.plot([1, 2])\n    leg = ax.legend(title=\"Test Legend\", draggable=True)\n    assert leg.get_draggable()\n    assert leg.get_draggable().get_draggable()\n\n    # Check that we can drag the legend\n    leg = mlegend.Legend(ax, [mlines.Line2D([1, 2], [1, 2])], ['Line'])\n    leg.set_bbox_to_anchor((0.5, 0.5))\n    leg.set_loc('upper right')\n    leg.set_draggable(True)\n    leg.set_draggable(use_blit=True)\n    draggable = leg.get_draggable()\n    draggable.connect()\n    assert draggable.get_draggable()\n\n    # Check that we can't create a draggable legend without a parent\n    with pytest.raises(TypeError):\n        mlegend.Legend(None, [mlines.Line2D([1, 2], [1, 2])], ['Line']).get_draggable()\n\n    # Check that a non-existent legend handle raises a warning\n    with warnings.catch_warnings(record=True):\n        ax.legend().get_draggable()\n\n    # Check that we can't get the draggable legend if it's not draggable\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n\n    # Check that setting draggable to True on a non-draggable legend works\n    assert not leg.get_draggable()\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n    assert leg.get_draggable().get_draggable()\n\n    # Check that setting draggable to False on a draggable legend works\n    assert leg.get_draggable()\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n\n    # Check that setting draggable to True on a draggable legend and then\n    # setting it to False works\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n\n    # Check that setting draggable to False on a non-draggable legend and then\n    # setting it to True works\n    leg.set_draggable(False)\n    assert not leg.get_draggable()\n    leg.set_draggable(True)\n    assert leg.get_draggable()\n\n    # Check that we can get the legend after it's been dragged\n    fig, ax = plt.subplots()\n    ax.plot(["], "sample_871": ["def test_davies_bouldin_score_invalid_labels():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.raises(ValueError):\n        davies_bouldin_score(X, np.array([1, 1, 0]))\n", "def test_calinski_harabasz_score():\n    X, y = datasets.make_blobs(n_samples=100, centers=3, random_state=0)\n    score = calinski_harabasz_score(X, y)\n    assert 0 <= score <= 1\n    score = calinski_harabasz_score(X, y)  # same score for same data\n    assert score == pytest.approx(score)\n", "def test_davies_bouldin_score_centroid_distances_are_zero():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 0, 0])\n    assert_array_equal(davies_bouldin_score(X, labels), 0.0)\n", "def test_calinski_harabasz_score_vectorized():\n    \"\"\"Test that the vectorized version of calinski_harabasz_score works.\"\"\"\n    from sklearn.utils._testing import _assert_warns_message\n    from sklearn.utils._testing import _assert_warns_regex\n    from sklearn.datasets import make_blobs\n    from sklearn.cluster import KMeans\n\n    # Test with a single cluster\n    X, _ = make_blobs(n_samples=10, centers=1, n_features=5, random_state=0)\n    score = calinski_harabasz_score(X, KMeans(n_clusters=1).fit_predict(X))\n    assert_allclose(score, 0.0)\n\n    # Test with two clusters\n    X, _ = make_blobs(n_samples=10, centers=2, n_features=5, random_state=0)\n    score = calinski_harabasz_score(X, KMeans(n_clusters=2).fit_predict(X))\n    assert_allclose(score, 0.0)\n\n    # Test with three clusters\n    X, _ = make_blobs(n_samples=10, centers=3, n_features=5, random_state=0)\n    score = calinski_harabasz_score(X, KMeans(n_clusters=3).fit_predict(X))\n    assert_allclose(score, 0.0)\n\n    # Test with very close clusters\n    X, _ = make_blobs(n_samples=10, centers=2, cluster_std=0.01, random_state=0)\n    score = calinski_harabasz_score(X, KMeans(n_clusters=2).fit_predict(X))\n    assert_allclose(score, 0.0)\n\n    # Test with no clusters\n    X = np.zeros((10, 5))\n    score = calinski_harabasz_score(X, KMeans(n_clusters=2).fit_predict(X))\n    assert_allclose(score, 0.0)\n\n    # Test with empty input\n    score = calinski_harabasz_score(np.array([]), np.array([]))\n    assert score == 0.0\n\n    # Test with very large input\n    X = np.random.rand(100000, 5)\n    score = calinski_harabasz_score(X, KMeans(n_clusters=2).fit_predict(X))\n    assert score >= 0.0\n\n    # Test with non-finite input\n    X = np.array([[", "def test_silhouette_score_precomputed():\n    \"\"\"Test silhouette_score with precomputed distance matrix.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n    labels = np.zeros(100)\n    for i in range(10):\n        labels[i * 10 : (i + 1) * 10] = i\n\n    D = pairwise_distances(X)\n    D[10, 20] = 0.1  # Create non-zero diagonal element\n\n    with pytest.warns(UserWarning):\n        score = silhouette_score(D, labels)\n        assert np.isclose(score, 0.585006)\n"], "sample_582": ["def test_prepare_import(tmp_path):\n    # Test with relative path\n    module_name = prepare_import(str(tmp_path / \"module.py\"))\n    assert module_name == \"module\"\n\n    # Test with absolute path\n    module_name = prepare_import(str(tmp_path / \"absolute_module.py\"))\n    assert module_name == \"absolute_module\"\n\n    # Test with init file\n    module_name = prepare_import(str(tmp_path / \"module_init\"))\n    assert module_name == \"module\"\n\n    # Test with init file in parent directory\n    (tmp_path / \"parent_module\").mkdir()\n    with open((tmp_path / \"parent_module\" / \"__init__.py\"), \"w\"):\n        pass\n    (tmp_path / \"parent_module\" / \"module\").mkdir()\n    module_name = prepare_import(str(tmp_path / \"parent_module\" / \"module.py\"))\n    assert module_name == \"module.parent_module\"\n\n    # Test with parent directory not having init file\n    (tmp_path / \"parent_module\").mkdir()\n    module_name = prepare_import(str(tmp_path / \"parent_module\" / \"module.py\"))\n    assert module_name == \"module\"\n\n    # Test when current directory is in sys.path\n    sys.path.insert(0, str(tmp_path))\n    module_name = prepare_import(\"module.py\")\n    assert module_name == \"module\"\n\n    # Test when current directory is not in sys.path\n    sys.path.pop(0)\n    module_name = prepare_import(\"module.py\")\n    assert module_name == \"module\"\n", "def test_locate_app_file_path(runner):\n    app_file = test_path / \"app.py\"\n    app_file.write_text(\"from flask import Flask\\napp = Flask('test_app')\")\n    try:\n        result = runner.invoke(cli, [\"--app\", str(app_file)])\n        assert result.exit_code == 0\n    finally:\n        app_file.unlink()\n", "def test_prepare_import__with_init_py_file():\n    with cwd.as_cwd():\n        Path(\"test_apps/__init__.py\").touch()\n        Path(\"test_apps/submodule.py\").touch()\n\n        sys.path.insert(0, str(cwd))\n        assert prepare_import(\"test_apps.submodule\") == \"test_apps.submodule\"\n\n        # Make sure the __init__.py file is still accessible.\n        assert prepare_import(\"test_apps\") == \"test_apps\"\n\n        # Make sure the submodule is still accessible when the current working\n        # directory is the package.\n        os.chdir(cwd / \"test_apps\")\n        assert prepare_import(\"submodule\") == \"submodule\"\n", "def test_prepare_import(tmp_path):\n    with tmp_path.as_cwd():\n        # Test importing with no path\n        assert prepare_import(\"test_app\") == \"test_app\"\n        assert locate_app(\"test_app\", None) is None\n\n        # Test importing with a path and a module name\n        (tmp_path / \"test_app\").mkdir()\n        (tmp_path / \"test_app\" / \"__init__.py\").touch()\n        (tmp_path / \"test_app\" / \"app.py\").write_text(\n            \"from . import app as real_app\\nfrom flask import Flask\\napp = Flask(__name__)\\nreal_app.config.update(app.config)\"\n        )\n        assert prepare_import(\"test_app.app\") == \"test_app\"\n        assert locate_app(\"test_app\", \"app\") is current_app\n        assert locate_app(\"test_app\", \"non_existent\") is None\n\n        # Test importing with a path and a module name with arguments\n        (tmp_path / \"test_app\").mkdir()\n        (tmp_path / \"test_app\" / \"__init__.py\").touch()\n        (tmp_path / \"test_app\" / \"app.py\").write_text(\n            \"from . import app as real_app\\nfrom flask import Flask\\napp = Flask(__name__)\\ndef create_app():\\n    return real_app\"\n        )\n        assert prepare_import(\"test_app.create_app\") == \"test_app\"\n        assert locate_app(\"test_app\", \"create_app\") is current_app\n        assert locate_app(\"test_app\", \"non_existent\") is None\n\n        # Test importing with a path and a module name with arguments that raise an exception\n        (tmp_path / \"test_app\").mkdir()\n        (tmp_path / \"test_app\" / \"__init__.py\").touch()\n        (tmp_path / \"test_app\" / \"app.py\").write_text(\n            \"from . import app as real_app\\nfrom flask import Flask\\napp = Flask(__name__)\\ndef create_app(*args):\\n    raise ValueError('Test error')\"\n        )\n        assert prepare_import(\"test_app.create_app\") == \"test_app\"\n        with pytest.raises(NoAppException):\n            locate_app(\"test_app\", \"create_app\")\n\n        # Test importing with a path and a module name with arguments that do not raise an exception\n        (tmp_path / \"test_app\").mkdir()\n        (tmp_path / \"test_app\" / \"__init", "def test_prepare_import(tmp_path):\n    \"\"\"Tests for prepare_import.\n\n    \"\"\"\n    # Test that a module name can be prepended to a path with a file extension\n    module_name = prepare_import(str(tmp_path / \"example.py\"))\n    assert module_name == \"example\"\n\n    # Test that a module name can be prepended to a path with a directory structure\n    module_name = prepare_import(str(tmp_path / \"example\" / \"directory\" / \"__init__.py\"))\n    assert module_name == \"example.directory\"\n\n    # Test that an empty module name is returned when the path ends with __init__.py\n    module_name = prepare_import(str(tmp_path / \"example\" / \"__init__.py\"))\n    assert module_name == \"example\"\n\n    # Test that the current directory is added to sys.path if necessary\n    old_sys_path = list(sys.path)\n    prepare_import(str(tmp_path))\n    assert str(tmp_path) in sys.path and str(tmp_path) not in old_sys_path\n    sys.path = old_sys_path\n\n    # Test that a Path object is correctly converted to a string\n    module_name = prepare_import(tmp_path / \"example.py\")\n    assert module_name == \"example\"\n"], "sample_102": ["    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_union_distinct(self):\n        qs1 = Number.objects.filter(num__gt=4)\n        qs2 = Number.objects.filter(num__lt=6)\n        expected_numbers = [2, 3, 5]\n        self.assertNumbersEqual(qs1.union(qs2, all=True), expected_numbers, ordered=False)\n        self.assertNumbersEqual(qs1.union(qs2, all=True).distinct(), expected_numbers, ordered=False)\n", "    def test_update_returns_count(self):\n        Number.objects.create(num=1)\n        Number.objects.create(num=2)\n        Number.objects.create(num=3)\n\n        updated_count = Number.objects.filter(num=F('num') + 1).update(num=F('num') + 1)\n        self.assertEqual(updated_count, 3)\n"], "sample_695": ["def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node(name=\"\", nodeid=nodeid)\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids(path: Path, expected_nodeid: str) -> None:\n    node = nodes.Node.from_parent(None, path=path)\n    assert list(iterparentnodeids(node.nodeid)) == expected_nodeid.split(\"/\")\n", "compilation error", "def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node(name=\"test_node\", nodeid=nodeid)\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids_with_no_colons(nodeid: str) -> None:\n    \"\"\"Test that iterparentnodeids returns an empty iterator when there are no colons.\"\"\"\n    assert list(nodes.iterparentnodeids(nodeid)) == [\"\"]\n    assert list(nodes.iterparentnodeids(\"\")) == [\"\"]\n"], "sample_255": ["    def test_close_connection(self):\n        # Arrange\n        handler = ServerHandler(None, None, None, {'CONTENT_LENGTH': '10'})\n        handler.request_handler = Stub()\n\n        # Act\n        handler.close()\n\n        # Assert\n        self.assertTrue(handler.close_connection)\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.handler = ServerHandler(BytesIO(), BytesIO(), BytesIO(), self.request_factory.request('GET', 'http://example.com'))\n", "    def setUp(self):\n        self.server_address = ('127.0.0.1', 8000)\n        self.stub_socket = Stub()\n        self.stub_socket.bind = lambda: None\n        self.stub_socket.getpeername = lambda: self.server_address\n        self.stub_socket.getsockname = lambda: self.server_address\n        self.handler = WSGIRequestHandler(\n            self.stub_socket, '', self.server_address\n        )\n", "    def test_handle_error(self):\n        \"\"\"\n        Test WSGIRequestHandler.handle_error method\n        \"\"\"\n        handler = WSGIRequestHandler()\n        stderr = BytesIO()\n        with captured_stderr(stderr) as captured_stderr:\n            handler.handle_error()\n        self.assertEqual(stderr.getvalue(), '')\n\n        # Test with is_broken_pipe_error() True\n        with captured_stderr(stderr) as captured_stderr:\n            with self.assertRaises(socket.error):\n                handler.handle_error()\n        self.assertEqual(stderr.getvalue(), '- Broken pipe from (127.0.0.1, 12345)\\n')\n", "    def test_WSGIServer(self):\n        wsgi_app = get_wsgi_application()\n        server = WSGIServer(('localhost', 0), 'dummy')\n\n        # Test that server_address is set correctly\n        self.assertEqual(server.server_address, ('localhost', 0))\n\n        # Test that allow_reuse_address is set correctly\n        self.assertTrue(server.allow_reuse_address)\n\n        # Test that ipv6 is set correctly\n        server = WSGIServer(('localhost', 0), 'dummy', ipv6=True)\n        self.assertEqual(server.address_family, socket.AF_INET6)\n\n        # Test that handle_error sets the log message correctly\n        with captured_stderr() as stderr:\n            server.handle_error(Stub(), ('localhost', 0))\n            self.assertIn('Broken pipe from localhost', stderr.getvalue())\n"], "sample_485": ["def test_json_script(self):\n    \"\"\"\n    Test json_script() function with different encoder.\n    \"\"\"\n    value = {'name': 'John', 'age': 30}\n    encoder = lambda o: o\n    output = json_script(value, 'my_script', encoder)\n    expected = '<script id=\"my_script\" type=\"application/json\">{\"name\":\"John\",\"age\":30}</script>'\n    self.assertEqual(output, expected)\n\n    # Test with DjangoJSONEncoder\n    output = json_script(value, 'my_script')\n    expected = '<script id=\"my_script\" type=\"application/json\">{\"name\":\"John\",\"age\":30}</script>'\n    self.assertEqual(output, expected)\n\n    # Test with invalid value\n    value = None\n    with self.assertRaises(TypeError):\n        json_script(value, 'my_script')\n\n    # Test with empty string\n    value = ''\n    output = json_script(value, 'my_script')\n    expected = '<script id=\"my_script\" type=\"application/json\"></script>'\n    self.assertEqual(output, expected)\n\n    # Test with list\n    value = [1, 2, 3]\n    output = json_script(value, 'my_script')\n    expected = '<script id=\"my_script\" type=\"application/json\">[1,2,3]</script>'\n    self.assertEqual(output, expected)\n\n    # Test with dictionary with non-JSON values\n    value = {'name': 'John', 'age': None}\n    output = json_script(value, 'my_script')\n    expected = '<script id=\"my_script\" type=\"application/json\">{\"name\":\"John\",\"age\":null}</script>'\n    self.assertEqual(output, expected)\n", "    def check_output(self, function, value, output=None):\n        \"\"\"\n        function(value) equals output. If output is None, function(value)\n        equals value.\n        \"\"\"\n        if output is None:\n            output = value\n        self.assertEqual(function(value), output)\n", "    def test_conditional_escape_strips_unnecessary_quotes(self):\n        self.check_output(conditional_escape, \"Hello 'world'\")\n", "    def test_json_script_encoder(self):\n        encoder = lambda obj: \"key %s\" % obj\n        self.check_output(json_script, \"test data\", '<script type=\"application/json\">key test data</script>')\n", "    def test_json_script_encoder(self):\n        \"\"\"\n        Verify json_script() works with custom JSONEncoder.\n        \"\"\"\n        class CustomJSONEncoder(DjangoJSONEncoder):\n                if isinstance(obj, datetime):\n                    return obj.isoformat()\n                return super().default(obj)\n\n        value = {\"date\": datetime.now()}\n        output = '<script type=\"application/json\">{}</script>'\n        self.assertEqual(json_script(value, encoder=CustomJSONEncoder), output)\n"], "sample_1096": ["def test_Idx_upper():\n    i, j = symbols('i j', integer=True)\n    idx = Idx(i, 3)\n    assert idx.upper == 2\n    idx = Idx(i, (0, 5))\n    assert idx.upper == 4\n    idx = Idx(i)\n    assert idx.upper is None\n    idx = Idx(i, oo)\n    assert idx.upper is None\n", "    def test_infinite_range(self):\n        from sympy import IndexedBase, Idx\n        from sympy.abc import x, y\n        i = Idx('i', oo)\n        self.assertEqual(i.lower, 0)\n        self.assertEqual(i.upper, oo)\n        self.assertEqual(i.lower, i.range[0])\n        self.assertEqual(i.upper, i.range[1])\n        self.assertEqual(i, Idx('i', 0, oo))\n        A = IndexedBase('A', shape=(x, y))\n        self.assertEqual(A[i, i].shape, (x, y))\n", "def test_range_can_be_used_with_multiple_indices():\n    n = symbols('n', integer=True)\n    from sympy.tensor.indexed import IndexedBase\n\n    A = IndexedBase('A', shape=(n, n))\n\n    i = Idx('i', n)\n    j = Idx('j', n)\n\n    assert A[i, j].shape == (n, n)\n    assert A[i, j].ranges == [(0, n - 1), (0, n - 1)]\n", "def test_IndexedBase():\n    from sympy import IndexedBase, Idx, symbols\n\n    # Test that IndexedBase preserves assumptions\n    a_real = IndexedBase('a', real=True)\n    assert a_real.is_real\n    assert a_real.assumptions0 == {'real': True}\n    a_real = IndexedBase('a', integer=True)\n    assert a_real.is_integer\n    assert a_real.assumptions0 == {'integer': True}\n\n    # Test that IndexedBase can be created from a Symbol\n    a = symbols('a', real=True)\n    b = IndexedBase(a)\n    assert b.is_real\n    assert b.assumptions0 == {'real': True}\n\n    # Test that IndexedBase can be created from a MatrixBase object\n    from sympy.matrices import MatrixBase\n    M = MatrixBase('M')\n    B = IndexedBase(M)\n    assert B.is_commutative\n    assert B.assumptions0 == {'commutative': True}\n\n    # Test that IndexedBase preserves shape information\n    A = IndexedBase('A', shape=(2, 2))\n    assert A.shape == (2, 2)\n    assert A[0, 1].shape == (2, 2)\n\n    # Test that IndexedBase shape can be set to None\n    A = IndexedBase('A', shape=None)\n    assert A.shape is None\n    assert A[0, 1].shape is None\n\n    # Test that IndexedBase shape can be set to a tuple of None\n    A = IndexedBase('A', shape=(None, None))\n    assert A.shape == (None, None)\n    assert A[0, 1].shape == (None, None)\n\n    # Test that IndexedBase shape can be set to an invalid shape\n    A = IndexedBase('A', shape='invalid')\n    assert A.shape == (1,)\n    assert A[0, 1].shape == (1,)\n\n    # Test that IndexedBase preserves offset information\n    A = IndexedBase('A', offset=1)\n    assert A.offset == 1\n    assert A[0, 1].offset == 1\n\n    # Test that IndexedBase preserves strides information\n    A = IndexedBase('A', strides=(1, 2))\n    assert A.strides == (1, 2)\n    assert A[0, 1].strides == (1,", "def test_idx_init():\n    # Test labels\n    i, j, k = symbols('i j k', integer=True)\n    idx1 = Idx('test1')\n    idx2 = Idx('test2', range=(0, 5))\n    idx3 = Idx(i)\n    idx4 = Idx(j, 10)\n    idx5 = Idx(k, (0, oo))\n    idx6 = Idx(j, lower=2)\n\n    assert idx1.label == Symbol('test1')\n    assert idx2.label == Symbol('test2')\n    assert idx2.lower == 0\n    assert idx2.upper == 5\n    assert idx3.label == i\n    assert idx3.lower == 0\n    assert idx3.upper == i-1\n    assert idx4.label == j\n    assert idx4.lower == 0\n    assert idx4.upper == 9\n    assert idx5.label == k\n    assert idx5.lower == 0\n    assert idx5.upper == oo\n    assert idx6.label == j\n    assert idx6.lower == 2\n    assert idx6.upper is None\n\n    # Test non-integer labels\n    with raises(IndexException):\n        Idx(1.5)\n    with raises(IndexException):\n        Idx('test', 1.5)\n\n    # Test invalid range\n    with raises(ValueError):\n        Idx('test', (1, 2, 3))\n\n    # Test invalid lower or upper bound\n    with raises(TypeError):\n        Idx(i, lower='test')\n    with raises(TypeError):\n        Idx(i, upper='test')\n\n    # Test Idx comparison\n    assert idx1 <= idx2\n    assert idx1 >= idx2\n    assert idx1 < idx3\n    assert idx1 > idx3\n    assert idx1 == idx1\n    assert idx1 != idx2\n    assert idx2 != idx3\n"], "sample_700": ["    def test_evaluate_skip_marks(self, pytester: Pytester):\n        # Arrange\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pytest.skip()\n\n                pytest.skipif(1 == 2)\n\n                pytest.skipif(1 == 2, \"message\")\n            \"\"\"\n        )\n\n        # Act\n        result = pytester.runpytest()\n\n        # Assert\n        result.assert_outcomes(passed=1, failed=0, skipped=1)\n        result.stdout.fnmatch_lines(\n            [\"test_skip.py::test_skipif_with_message\", \"skipped in test_skip.py::test_skipif\"]\n        )\n", "def test_pytest_pyfunc_call_with_async_function(tmp_path):\n    \"\"\"Check that pytest_pyfunc_call raises PytestUnhandledCoroutineWarning for async functions.\"\"\"\n    # Create a test file with an async function that is not supported by pytest.\n    async_test_file = tmp_path / \"async_test.py\"\n    async_test_file.write_text(\n        textwrap.dedent(\n            \"\"\"\n            import asyncio\n            import pytest\n\n            async def test_async_function():\n                pass\n            \"\"\"\n        )\n    )\n\n    # Run pytest on the test file.\n    with Pytester.fromdir(tmp_path) as pytester:\n        pytester.makefile(\"async_test.py\")\n        result = pytester.runpytest()\n\n    # Assert that pytest raised a PytestUnhandledCoroutineWarning.\n    result.assert_outcomes(passed=1)\n    result.stdout.fnmatch_lines([\"*PytestUnhandledCoroutineWarning*\"])\n\n    # Assert that the test was skipped.\n    result.assert_outcomes(skipped=1)\n    result.stdout.fnmatch_lines([\"*Skipping test_async_function*\"])\n", "    def test_init(self, pytester: Pytester):\n        metafunc = Metafunc(\n            definition=FunctionDefinition(\n                name=\"test_metafunc\",\n                obj=lambda: None,\n            ),\n            fixtureinfo=fixtures.FuncFixtureInfo.fromfixturedef(\n                fixtures.FixtureDef(\n                    name=\"fixture\",\n                    argname=\"fixture\",\n                    scope=\"function\",\n                    function=lambda: None,\n                )\n            ),\n            config=pytest.config,\n            cls=None,\n            module=None,\n        )\n        assert metafunc.definition is not None\n        assert metafunc.fixturenames == set([\"fixture\"])\n        assert metafunc.function is None\n        assert metafunc.cls is None\n        assert metafunc.module is None\n", "    def test_xunit_setup_module(self, tester: Pytester):\n        # Write a simple test function\n        tester.makefile(\n            \".py\",\n            \"test_module.py\",\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(autouse=True, scope=\"module\")\n                print(\"Setting up the module\")\n\n                assert True\n            \"\"\",\n        )\n\n        # Test that the xunit setup module function is called\n        assert \"Setting up the module\" in tester.ini.get(\"markers\")\n", "    def test_parametrize_ids_list(self, pytester: Pytester) -> None:\n        # test that ids with a list are correctly generated\n        pytester.makefile(\n            \".py\",\n            contents=\"\"\"\n            import pytest\n                pass\n            pytest.mark.parametrize(\"a, b\", [(1, 2), (3, 4)])\n                pass\n            \"\"\",\n        )\n        session = pytester.inline_run()\n        session.shell.syspath.insert(0, str(pytester.path))\n        collected = session.items[0].function\n        collected.fixtureinfo.prune_dependency_tree()\n        assert collected.callspec.params == {\n            \"a\": [1, 3],\n            \"b\": [2, 4],\n        }\n        assert collected.callspec._idlist == [\"0-0\", \"0-1\"]\n"], "sample_29": ["    def cosmo(self):\n        return Cosmology(H0=67.3, Om0=0.3, Ode0=0.7, Tcmb0=2.725)\n", "def test_write_latex_table_unit_conversion(tmp_path):\n    \"\"\"\n    Test that parameter units are converted correctly to the LaTeX table.\n    \"\"\"\n    from astropy.cosmology import FlatLambdaCDM\n    cosmo = FlatLambdaCDM(H0=70 * u.km / u.s / u.Mpc, Om0=0.3)\n\n    write_latex(cosmo, tmp_path / \"test.tex\")\n\n    with open(tmp_path / \"test.tex\", \"r\") as f:\n        latex_output = f.read()\n\n    assert \"$$\\\\mathrm{\\\\left(\\\\frac{\\\\mathrm{km}}{\\\\mathrm{s}\\\\cdot\\\\mathrm{Mpc}}\\\\right)}$$\" in latex_output\n    assert \"$$\\\\Omega_{{m,0}}$$\" in latex_output\n    assert \"$$\\\\Omega_{{\\\\Lambda,0}}$$\" in latex_output\n    assert \"$$T_{{0}}$$\" in latex_output\n    assert \"$$N_{{eff}}$$\" in latex_output\n    assert \"$$m_{{nu}}$$\" in latex_output\n    assert \"$$\\\\Omega_{{b,0}}$$\" in latex_output\n    assert \"$$w_{{0}}$$\" in latex_output\n    assert \"$$w_{{a}}$$\" in latex_output\n    assert \"$$w_{{z}}$$\" in latex_output\n    assert \"$$w_{{p}}$$\" in latex_output\n    assert \"$$z_{{p}}$$\" in latex_output\n\n", "def test_write_latex_overwrite_existing_file(tmp_path):\n    \"\"\"\n    Test that overwriting an existing file works.\n    \"\"\"\n    # Create a cosmology\n    cosmo = Cosmology(H0=70 * u.km / u.s / u.Mpc, Om0=0.3)\n\n    # Create an existing LaTeX file\n    file = tmp_path / \"cosmology.tex\"\n    with open(file, \"w\") as f:\n        f.write(r\"\\documentclass{article}\")\n        f.write(\"\\n\")\n        f.write(r\"\\begin{document}\")\n        f.write(\"\\n\")\n        f.write(r\"\\end{document}\")\n\n    # Test that overwriting works\n    write_latex(cosmo, file, overwrite=True)\n\n    # Check that the file has been overwritten\n    with open(file, \"r\") as f:\n        assert \"H0\" in f.read()\n", "    def cosmo(self):\n        return Cosmology(name=\"TestCosmology\", H0=67.7, Om0=0.31, Ob0=0.045)\n", "def test_write_latex_defaults(ascii_cosmology, tmp_path):\n    \"\"\"\n    Ensure the default behavior of write_latex is correct.\n\n    Notes\n    -----\n    Test that we default to latex format and don't pass any kwargs to `cls.write`.\n    \"\"\"\n    # Save an instance of the cosmology\n    cosmo = ascii_cosmology.copy()\n\n    # Test the defaults\n    write_latex(cosmo, tmp_path / \"default.tex\")\n    with open(tmp_path / \"default.tex\", \"r\") as f:\n        contents = f.read()\n    assert \"format=latex\" not in contents\n    assert \"\\\\documentclass[11pt]{article}\" in contents\n    assert \"\\\\begin{document}\" in contents\n\n    # Test that it writes to the correct location\n    assert (tmp_path / \"default.tex\").exists()\n"], "sample_768": ["def test_leave_p_groups_out():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 1])\n    groups = np.array([1, 2, 3])\n    lpgo = LeavePGroupsOut(n_groups=2)\n    assert lpgo.get_n_splits(X, y, groups) == 3\n    for train_index, test_index in lpgo.split(X, y, groups):\n        assert isinstance(train_index, np.ndarray)\n        assert isinstance(test_index, np.ndarray)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        assert len(X_train) + len(X_test) == len(X)\n        assert len(y_train) + len(y_test) == len(y)\n", "def test_PredefinedSplit():\n    # Test PredefinedSplit\n    X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    y = np.array([0, 0, 1, 1])\n    test_fold = [0, 1, -1, 1]\n    ps = PredefinedSplit(test_fold)\n    assert_equal(len(ps.split()), 2)\n    assert_equal(ps.get_n_splits(), 2)\n    assert_equal(ps.__repr__(), 'PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))')\n    X_train, X_test = [], []\n    y_train, y_test = [], []\n    for train_index, test_index in ps.split():\n        X_train.extend(X[train_index])\n        X_test.extend(X[test_index])\n        y_train.extend(y[train_index])\n        y_test.extend(y[test_index])\n    assert_equal(X_train, [[3, 4], [1, 2]])\n    assert_equal(X_test, [0])\n    assert_equal(y_train, [0, 0])\n    assert_equal(y_test, [1])\n\n    # Test PredefinedSplit with empty test_fold\n    ps = PredefinedSplit([])\n    assert_equal(len(ps.split()), 0)\n    assert_equal(ps.get_n_splits(), 0)\n    X_train, X_test = [], []\n    y_train, y_test = [], []\n    for train_index, test_index in ps.split():\n        X_train.extend(X[train_index])\n        X_test.extend(X[test_index])\n        y_train.extend(y[train_index])\n        y_test.extend(y[test_index])\n    assert_equal(X_train, X)\n    assert_equal(X_test, [])\n    assert_equal(y_train, y)\n    assert_equal(y_test, [])\n\n    # Test PredefinedSplit with duplicate test_fold values\n    test_fold = [0, 1, 1, 1]\n    ps = PredefinedSplit(test_fold)\n    assert_equal(len(ps.split()), 1)\n    assert_equal(ps.get_n_splits(), 1)\n    X_train, X_test = [], []\n    y_train, y_test = [], []\n    for train_index, test_index in ps.split():\n        X_train.extend(X[train_index])\n        X_test.extend(X[test_index])\n        y_train.extend(y[train_index])\n        y_test.extend(y[test_index])\n    assert_equal(X_train, [0, 1, 2,", "    def test_predefined_split(self):\n        X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n        y = np.array([1, 2, 1, 2])\n        test_fold = np.array([0, 1, -1, 1])\n        ps = PredefinedSplit(test_fold)\n        assert_equal(len(ps.get_n_splits()), 2)\n        assert_equal(ps.n_splits, None)\n        assert_warns_message(UserWarning, CV_WARNING, ps.get_n_splits)\n        for train_index, test_index in ps.split():\n            assert_equal(np.sort(train_index), np.array([0, 2, 3]))\n            assert_equal(np.sort(test_index), np.array([1]))\n", "    def test_validate_shuffle_split_init(self):\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=-1, train_size=None)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=0.3, train_size=-1)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=1.1, train_size=None)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=None, train_size=1.1)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=None, train_size=-1)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=1, train_size=1)\n\n        with pytest.raises(ValueError):\n            _validate_shuffle_split_init(test_size=-1, train_size=1)\n", "def test_LeavePOut_invalid_p():\n    with pytest.raises(ValueError) as e:\n        LeavePOut(0)\n    assert str(e.value) == (\"The number of samples is 10, cannot have p=0 \"\n                           \"samples in the test set.\")\n"], "sample_858": ["def test_voting_regressor_n_jobs():\n    # Create a VotingRegressor with two estimators\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    er = VotingRegressor(estimators=[('lr', r1), ('rf', r2)])\n\n    # Check that n_jobs is correctly passed to the base estimators\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    er.fit(X, y, sample_weight=np.array([1, 2, 3, 4, 5, 6]))\n    assert r1.n_jobs == 1\n    assert r2.n_jobs == 1\n\n    # Check that n_jobs=-1 is correctly passed to the base estimators\n    er = VotingRegressor(estimators=[('lr', r1), ('rf', r2)], n_jobs=-1)\n    er.fit(X, y, sample_weight=np.array([1, 2, 3, 4, 5, 6]))\n    assert r1.n_jobs == -1\n    assert r2.n_jobs == -1\n\n    # Check that n_jobs=None is correctly passed to the base estimators\n    er = VotingRegressor(estimators=[('lr', r1), ('rf', r2)])\n    er.fit(X, y, sample_weight=np.array([1, 2, 3, 4, 5, 6]))\n    assert r1.n_jobs is None\n    assert r2.n_jobs is None\n", "def test_voting_regressor_sparse_input():\n    \"\"\"Test that VotingRegressor can handle sparse input.\"\"\"\n    # Create some data\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([2, 3])\n    X_sparse = csr_matrix(X)\n\n    # Create a VotingRegressor\n    vr = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())])\n\n    # Fit and predict with dense input\n    vr.fit(X, y)\n    y_pred_dense = vr.predict(X)\n\n    # Fit and predict with sparse input\n    vr.fit(X_sparse, y)\n    y_pred_sparse = vr.predict(X_sparse)\n\n    # Check that the predictions are the same\n    assert_array_almost_equal(y_pred_dense, y_pred_sparse)\n\n", "def test_VotingRegressor_fit_predict_with_sample_weight():\n    # Test VotingRegressor with sample_weight and fit method\n    X, y = datasets.load_boston(return_X_y=True)\n    y = y.reshape(-1, 1)  # Reshape target to 2D array\n    regressor1 = LinearRegression()\n    regressor2 = RandomForestRegressor(n_estimators=10, random_state=42)\n    regressor3 = DecisionTreeRegressor(random_state=42)\n    voting_regressor = VotingRegressor(estimators=[('lr', regressor1), ('rf', regressor2), ('dt', regressor3)],\n                                      weights=[0.4, 0.3, 0.3])\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    voting_regressor.fit(X_train, y_train, sample_weight=np.random.rand(X_train.shape[0]))\n    y_pred = voting_regressor.predict(X_test)\n    assert len(y_pred) == len(y_test)\n", "def test_voting_regressor_fit_non_fitted_error():\n    \"\"\"Test that VotingRegressor raises an error when predict is called before fit\"\"\"\n    voting_regressor = VotingRegressor(estimators=[('lr', LinearRegression())])\n    with pytest.raises(NotFittedError):\n        voting_regressor.predict(np.array([[1, 2]]))\n", "def test_voting_regressor_check_n_jobs():\n    \"\"\"Check that ``VotingRegressor`` fails when n_jobs is not a positive integer.\"\"\"\n    est = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())])\n    with pytest.raises(ValueError):\n        est.n_jobs = 'a'\n        est.fit([[1, 2]], [3])\n        \n    with pytest.raises(ValueError):\n        est.n_jobs = -5\n        est.fit([[1, 2]], [3])\n"], "sample_324": ["    def test_insecure_referer(self):\n        req = self._get_GET_csrf_cookie_request()\n        req.META['HTTP_REFERER'] = 'http://example.com/other'\n        req.META['HTTP_ORIGIN'] = 'https://example.com'\n        req.is_secure = lambda: True\n        middleware = CsrfViewMiddleware()\n        with self.assertRaises(RejectRequest) as cm:\n            middleware.process_view(req, lambda x: None, (), {})\n        self.assertEqual(cm.exception.reason, REASON_INSECURE_REFERER)\n", "    def test_csrf_origin_check_secure(self):\n        req = self._get_GET_no_csrf_cookie_request()\n        req.META['HTTP_ORIGIN'] = 'https://example.com'\n        req.META['HTTP_REFERER'] = 'http://example.com/path'\n        req.META['HTTP_HOST'] = 'example.com'\n        with override_settings(CSRF_USE_SESSIONS=True):\n            middleware = CsrfViewMiddleware()\n            response = middleware.process_view(req, lambda r: None, (), {})\n            self.assertEqual(response, None)\n        self.assertTrue(req.META.get('CSRF_COOKIE_USED'))\n", "    def _get_CSRF_TOKEN_header_request(self, meta_token=None):\n        \"\"\"\n        The meta_token argument is included in the request's META headers when\n        that argument is provided and non-None.\n        \"\"\"\n        req = self._get_GET_no_csrf_cookie_request()\n        if meta_token is not None:\n            req.META['HTTP_X_CSRFTOKEN'] = meta_token\n        return req\n", "    def _get_GET_csrf_cookie_request(self, cookie=None):\n        req = self._get_GET_no_csrf_cookie_request()\n        if cookie is not None:\n            req.META['CSRF_COOKIE'] = cookie\n        return req\n", "    def test_view_exempt(self):\n        # Test that view_exempt=True prevents CsrfViewMiddleware from being\n        # called for the view.\n        req = self._get_GET_no_csrf_cookie_request()\n        view = non_token_view_using_request_processor\n        view.csrf_exempt = True\n        response = view(req)\n        self.assertFalse(hasattr(req, 'csrf_processing_done'))\n"], "sample_314": ["    def test_rendering(self):\n        field = ReadOnlyPasswordHashField()\n        html = field.widget.render('password', 'some_hash')\n        self.assertContains(html, 'summary')\n        self.assertContains(html, 'No password set.')\n", "    def test_read_only_password_hash_widget(self):\n        form = ReadOnlyPasswordHashForm({'password': 'password123'})\n        widget = form.fields['password'].widget\n        self.assertEqual(widget.read_only, True)\n", "    def test_user_change_form_fields(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn('password', form.fields.keys())\n        self.assertIn('username', form.fields.keys())\n        self.assertEqual(form.fields['password'].widget, ReadOnlyPasswordHashWidget())\n        self.assertEqual(form.fields['username'].widget, forms.TextInput())\n", "    def test_valid_form(self):\n        form = UserChangeForm(data={'username': self.u1.username, 'email': self.u1.email}, instance=self.u1)\n        self.assertTrue(form.is_valid())\n", "    def test_user_change_form_with_inactive_user(self):\n        form = UserChangeForm(instance=self.u2)\n        with self.assertRaises(ValidationError):\n            form.full_clean()\n"], "sample_655": ["def test_capture_manager_is_globally_capturing():\n    capture_manager = CaptureManager(\"fd\")\n    assert capture_manager.is_globally_capturing() is True\n    assert capture_manager.is_globally_capturing() is True\n\n    capture_manager.start_global_capturing()\n    assert capture_manager.is_globally_capturing() is True\n\n    capture_manager.stop_global_capturing()\n    assert capture_manager.is_globally_capturing() is False\n\n    # Test that is_globally_capturing() returns False when capture method is \"no\"\n    capture_manager = CaptureManager(\"no\")\n    assert capture_manager.is_globally_capturing() is False\n", "    def test_is_globally_capturing_before_start_global_capturing(self):\n        capture_manager = CaptureManager(\"fd\")\n        assert not capture_manager.is_globally_capturing()\n", "    def test_start_and_close(self, capsys):\n        with capsys.disabled():\n            assert not capsys._capture\n        capsys._start()\n        assert capsys._capture is not None\n        capsys.close()\n        assert capsys._capture is None\n", "    def test_suspend_resume(self):\n        cap = CaptureManager(\"fd\")\n        cap.start_global_capturing()\n        assert cap.is_capturing() == \"global\"\n        cap.resume_global_capture()\n        assert cap.is_capturing() == \"global\"\n", "def test_captures():\n    \"\"\" Make sure that capsys, capfd, capsysbinary and capfdbinary return the\n    correct type of captured output.\n\n    See https://github.com/pytest-dev/pytest/issues/1187\n\n    \"\"\"\n    with capture.capturepytest_ini(\"stdout\") as capsys, capsysbinary, capfd, capfdbinary:\n        assert isinstance(capsys.readouterr().out, text_type)\n        assert isinstance(capsys.readouterr().err, text_type)\n        assert isinstance(capsysbinary.readouterr().out, bytes)\n        assert isinstance(capsysbinary.readouterr().err, bytes)\n        assert isinstance(capfd.readouterr().out, text_type)\n        assert isinstance(capfd.readouterr().err, text_type)\n        assert isinstance(capfdbinary.readouterr().out, bytes)\n        assert isinstance(capfdbinary.readouterr().err, bytes)\n"], "sample_47": ["    def test_exception_reporter_init(self):\n        request = RequestFactory.get_request()\n        exc_type = Exception\n        exc_value = Exception('test_exception')\n        tb = sys.exc_info()[2]\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        self.assertIsInstance(reporter.filter, ExceptionReporterFilter)\n        self.assertEqual(reporter.exc_type, exc_type)\n        self.assertEqual(reporter.exc_value, exc_value)\n        self.assertEqual(reporter.tb, tb)\n        self.assertIsInstance(reporter.request, RequestFactory)\n        self.assertFalse(reporter.is_email)\n", "    def test_get_post_parameters(self):\n        request = RequestFactory().get('/')\n\n        # Test with sensitive parameters\n        post_params = {'API_KEY': 'my_key', 'SECRET': 'my_secret'}\n        request.POST = post_params\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'API_KEY': CLEANSED_SUBSTITUTE, 'SECRET': CLEANSED_SUBSTITUTE})\n\n        # Test with non-sensitive parameters\n        post_params = {'NON_SENSITIVE': 'my_value'}\n        request.POST = post_params\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), post_params)\n\n        # Test with __ALL__ parameter\n        post_params = {'API_KEY': 'my_key', 'SECRET': 'my_secret', 'NON_SENSITIVE': 'my_value'}\n        request.POST = post_params\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'API_KEY': CLEANSED_SUBSTITUTE, 'SECRET': CLEANSED_SUBSTITUTE, 'NON_SENSITIVE': CLEANSED_SUBSTITUTE})\n\n        # Test with __ALL__ and non-sensitive parameters\n        post_params = {'API_KEY': 'my_key', 'SECRET': 'my_secret', 'NON_SENSITIVE': 'my_value'}\n        request.POST = post_params\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'API_KEY': CLEANSED_SUBSTITUTE, 'SECRET': CLEANSED_SUBSTITUTE})\n", "    def test_get_post_parameters(self):\n        request = RequestFactory().post('/test/', {'a': 1, 'b': '2'})\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'a': CLEANSED_SUBSTITUTE, 'b': CLEANSED_SUBSTITUTE})\n        self.assertEqual(filter.get_post_parameters(request), {'a': CLEANSED_SUBSTITUTE, 'b': CLEANSED_SUBSTITUTE})\n        self.assertEqual(filter.get_post_parameters(request).get('a'), CLEANSED_SUBSTITUTE)\n        self.assertEqual(filter.get_post_parameters(request).get('c'), None)\n        self.assertEqual(filter.get_post_parameters(None), {})\n        self.assertEqual(filter.get_post_parameters(RequestFactory()).get('a'), None)\n", "    def test_get_traceback_frames(self):\n        request = RequestFactory().get('/')\n        request.user = User()\n        exc_type = Exception\n        exc_value = Exception('Test exception')\n        tb = sys.exc_info()[2]\n        reporter = ExceptionReporter(request, exc_type, exc_value, tb)\n        frames = reporter.get_traceback_frames()\n        self.assertEqual(len(frames), 2)\n        self.assertEqual(frames[0]['type'], 'user')\n        self.assertEqual(frames[1]['type'], 'django')\n", "    def test_exception_reporter_get_traceback_data(self):\n        request = RequestFactory().get('/')\n        exception_reporter = ExceptionReporter(request, Exception, Exception(), None)\n        data = exception_reporter.get_traceback_data()\n        self.assertIn('exception_type', data)\n        self.assertIn('exception_value', data)\n        self.assertIn('lastframe', data)\n"], "sample_107": ["    def test_get_post_parameters(self):\n        request = RequestFactory().get('/test')\n        request.POST = {'key': 'value', 'sensitive': 'secret'}\n        request.sensitive_post_parameters = ['sensitive']\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'key': 'value', 'sensitive': CLEANSED_SUBSTITUTE})\n", "    def test_default_filter_is_active(self):\n        request = RequestFactory().get('/')\n        self.assertFalse(ExceptionReporterFilter().is_active(request))\n", "    def test_get_post_parameters(self):\n        request = RequestFactory().post('/')\n        request.POST = {'key': 'value', 'sensitive_key': 'sensitive_value'}\n        request.sensitive_post_parameters = ['sensitive_key']\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.get_post_parameters(request), {'key': 'value', 'sensitive_key': CLEANSED_SUBSTITUTE})\n", "    def test_is_active(self):\n        request = RequestFactory().get('/')\n        request.META['HTTP_ACCEPT'] = 'text/html'\n        filter = ExceptionReporterFilter()\n        self.assertTrue(filter.is_active(request))\n\n        request.META['HTTP_ACCEPT'] = 'application/json'\n        self.assertFalse(filter.is_active(request))\n", "    def test_get_post_parameters_all_parameters(self):\n        request = RequestFactory().get('/')\n        wrapper = ExceptionReporterFilter()\n        wrapper.get_post_parameters(request)\n        wrapper.get_post_parameters(None)\n"], "sample_207": ["    def test_formfield(self):\n        json_model = JSONModel()\n        field = json_model._meta.get_field('json_field')\n        form_field = field.formfield()\n        self.assertIsInstance(form_field, forms.JSONField)\n        self.assertEqual(form_field.encoder, field.encoder)\n        self.assertEqual(form_field.decoder, field.decoder)\n", "    def test_invalid_json_value_error_message(self):\n        model = JSONModel()\n        with self.assertRaises(ValidationError):\n            model.save()\n        with self.assertRaises(ValidationError) as e:\n            JSONModel.objects.create(json_field='not a valid json')\n        self.assertIn('invalid', e.exception.messages)\n        self.assertIn('Value must be valid JSON.', e.exception.default_code)\n", "    def test_decode_error_no_encoder(self):\n        with self.assertRaises(ValueError):\n            JSONModel.objects.create(json_field=None)\n", "    def test_check_supported(self):\n        with self.assertRaises(NotSupportedError):\n            JSONModel.objects.create()\n        model = NullableJSONModel.objects.create(json_field='{}')\n        self.assertEqual(JSONField().check(), [])\n", "    def test_invalid_json_input_fails_validation(self):\n        with self.assertRaises(ValidationError):\n            JSONModel.objects.create(json_field='not json')\n"], "sample_988": ["    def test_creation(self):\n        assert Float(3).evalf() == 3.0\n        assert Float(3, 1).evalf() == 3.0\n        assert Float(3.2).evalf() == 3.2\n        assert Float(3.2, 2).evalf() == 3.2\n        assert Float('3.2').evalf() == 3.2\n        assert Float('3.2', 2).evalf() == 3.2\n", "def test_Float__neg__():\n    assert Float(3)._neg__() == Float(-3)\n    assert Float(0)._neg__() == Float(0)\n    assert Float('1.2')._neg__() == Float('-1.2')\n    assert Float('-1.2')._neg__() == Float('1.2')\n    assert Float('-inf')._neg__() == Float('inf')\n    assert Float('inf')._neg__() == Float('-inf')\n    assert Float(nan)._neg__() == Float(nan)\n", "    def test_Float_pickling(self):\n        f = Float(0.1, 53)\n        assert f is pickle.loads(pickle.dumps(f))\n", "    def test_creation(self):\n        f = Float(1.2, 2)\n        assert f._mpf_ == (1, 5, 0, 3)\n        f = Float(1.2, 1)\n        assert f._mpf_ == (1, 5, 0, 4)\n        f = Float(1.2, 0)\n        assert f._mpf_ == (1, 5, 0, 15)\n        f = Float(1.2, '')\n        assert f._mpf_ == (1, 5, 0, 15)\n        f = Float('0.123', 0)\n        assert f._mpf_ == (0, 3, 2, 15)\n        f = Float('1.2e-3', '')\n        assert f._mpf_ == (0, 2, 2, 15)\n        f = Float(0.3, 15)\n        assert f._mpf_ == (1, 5, 0, 15)\n", "def test_evaluate_off():\n    from sympy import sympify, symbols, pi, zoo, Float\n    from sympy.abc import x\n    assert sympify(2 + 2) == 4\n    assert sympify(2 + 3) == 5\n    assert sympify(2 + x) == 2 + x\n    assert sympify(oo + 3) == oo\n    assert sympify(-oo + 3) == -oo\n    assert sympify(2 + pi) == 2 + pi\n    assert sympify(zoo + 2) == zoo\n    assert sympify(2 + Float(3.0)) == 2 + Float(3.0)\n    assert sympify(2 + 1/3) == 2 + 1/3\n    assert sympify(x + 2) == x + 2\n"], "sample_456": ["    def test_formset_factory_min_num_default(self):\n        form = ChoiceForm\n        formset = formset_factory(form)\n        self.assertEqual(formset.min_num, BaseFormSet.DEFAULT_MIN_NUM)\n", "    def test_rendering_order_and_delete_fields(self):\n        formset = ChoiceFormSet()\n        formset.forms[0].fields[\"choice\"].initial = \"Option 1\"\n        formset.forms[0].fields[\"votes\"].initial = 10\n        formset.forms[1].fields[\"choice\"].initial = \"Option 2\"\n        formset.forms[1].fields[\"votes\"].initial = 20\n        formset.forms[2].fields[\"choice\"].initial = \"Option 3\"\n        formset.forms[2].fields[\"votes\"].initial = 30\n\n        rendered_html = self.render_template(formset.template_name_div)\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"choice-0\\\" value=\\\"Option 1\\\">\")\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"votes-0\\\" value=\\\"10\\\">\")\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"choice-1\\\" value=\\\"Option 2\\\">\")\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"votes-1\\\" value=\\\"20\\\">\")\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"choice-2\\\" value=\\\"Option 3\\\">\")\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"votes-2\\\" value=\\\"30\\\">\")\n\n        # Test delete field is only rendered on forms with initial data\n        formset.forms[0].fields[DELETION_FIELD_NAME].initial = True\n        formset.forms[1].fields[DELETION_FIELD_NAME].initial = False\n        formset.forms[2].fields[DELETION_FIELD_NAME].initial = False\n        rendered_html = self.render_template(formset.template_name_div)\n        self.assertRegex(rendered_html, r\"<input[^>]*name=\\\"delete-0\\\" value=\\\"true\\\">\")\n        self.assertNotRegex(rendered_html, r\"<input[^>]*name=\\\"delete-1\\\" value=\\\"true\\\">\")\n        self.assertNotRegex(rendered_html, r\"<input[^>]*name=\\\"delete-2\\\" value=\\\"true\\\">\")\n", "    def test_form_kwargs(self):\n        \"\"\"Test that form_kwargs is passed to each form in the formset.\"\"\"\n        data = [\n            (\"A\", 1),\n            (\"B\", 2),\n            (\"C\", 3),\n        ]\n        formset = self.make_choiceformset(formset_data=data, extra=3)\n        form1 = formset.forms[0]\n        self.assertEqual(form1.data, {\"choice\": \"A\", \"votes\": \"1\", \"custom_kwarg\": \"custom_value\"})\n        form2 = formset.forms[1]\n        self.assertEqual(form2.data, {\"choice\": \"B\", \"votes\": \"2\", \"custom_kwarg\": \"custom_value\"})\n        form3 = formset.forms[2]\n        self.assertEqual(form3.data, {\"choice\": \"C\", \"votes\": \"3\", \"custom_kwarg\": \"custom_value\"})\n", "    def test_management_form_is_bound_when_instance_has_data(self):\n        formset = ChoiceFormSet(\n            data={\"choices-0-choice\": \"choice1\", \"choices-0-votes\": 1},\n            prefix=\"choices\",\n        )\n        self.assertTrue(formset.management_form.is_bound)\n", "    def test_init(self):\n        formset = BaseFormSet(data={\"TOTAL_FORMS\": \"1\"}, prefix=\"test_prefix\")\n        self.assertEqual(formset.prefix, \"test_prefix\")\n        self.assertEqual(formset.management_form, ManagementForm(\n            data={\"TOTAL_FORMS\": \"1\"}, prefix=\"test_prefix\"\n        ))\n"], "sample_136": ["    def test_valid_host(self):\n        request = HttpRequest()\n        request.META = {'HTTP_HOST': 'example.com'}\n        request.settings = {'ALLOWED_HOSTS': ['example.com']}\n        self.assertTrue(request.get_host() == 'example.com')\n", "    def test_get_signed_cookie(self):\n        # Test case: get signed cookie with valid signature\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer('salt').sign('value')\n        self.assertEqual(request.get_signed_cookie('signed_cookie', default='default_value', salt='salt'), 'value')\n        self.assertEqual(request.get_signed_cookie('signed_cookie', default='default_value', salt='salt', max_age=3600), 'value')\n        \n        # Test case: get signed cookie with expired signature\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer('salt').sign('value', max_age=-3600)\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_cookie', default='default_value', salt='salt')\n        \n        # Test case: get signed cookie with bad signature\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer('salt').sign('value', salt='wrong_salt')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_cookie', default='default_value', salt='salt')\n        \n        # Test case: get signed cookie with no default value\n        request.COOKIES['signed_cookie'] = signing.get_cookie_signer('salt').sign('value')\n        with self.assertRaises(RawPostDataException):\n            request.get_signed_cookie('signed_cookie', default=RAISE_ERROR, salt='salt')\n", "    def test_get_signed_cookie(self):\n        # Test that get_signed_cookie raises an exception when the cookie value is invalid\n        request = HttpRequest()\n        request.COOKIES['signed_cookie'] = signing.get_signer('salt').sign('invalid')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_cookie')\n\n        # Test that get_signed_cookie returns default value when default is provided\n        default_value = 'default_value'\n        request.COOKIES['signed_cookie'] = signing.get_signer('salt').sign('invalid')\n        self.assertEqual(request.get_signed_cookie('signed_cookie', default_value), default_value)\n\n        # Test that get_signed_cookie raises an exception when the cookie is expired\n        request.COOKIES['signed_cookie'] = signing.get_signer('salt').sign('expired', expires='1970-01-01')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('signed_cookie')\n\n        # Test that get_signed_cookie raises an exception when the cookie is missing\n        del request.COOKIES['signed_cookie']\n        with self.assertRaises(KeyError):\n            request.get_signed_cookie('signed_cookie')\n", "    def test_get_signed_cookie(self):\n        # Test case for get_signed_cookie when the cookie is not set\n        request = HttpRequest()\n        with self.assertRaises(RaiseError):\n            request.get_signed_cookie('key')\n\n        # Test case for get_signed_cookie when the cookie is set but the signature fails\n        request.COOKIES['key'] = 'value'\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('key', default='default_value')\n\n        # Test case for get_signed_cookie when the cookie is set and the signature is valid\n        request.COOKIES['key'] = signing.get_cookie_signer('salt').sign('value')\n        self.assertEqual(request.get_signed_cookie('key'), 'value')\n\n        # Test case for get_signed_cookie when the cookie is set but has expired\n        request.COOKIES['key'] = signing.get_cookie_signer('salt').sign('value', max_age=0)\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('key')\n\n        # Test case for get_signed_cookie when max_age is provided\n        request.COOKIES['key'] = signing.get_cookie_signer('salt').sign('value', max_age=3600)\n        self.assertEqual(request.get_signed_cookie('key', max_age=3600), 'value')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('key', max_age=7200)\n", "    def test_get_signed_cookie(self):\n        # Test get_signed_cookie with RAISE_ERROR as default\n        request = HttpRequest()\n        request.COOKIES['key'] = 'value'\n        with self.assertRaises(RawPostDataException):\n            request.get_signed_cookie('key', default=RAISE_ERROR)\n\n        # Test get_signed_cookie with valid signature\n        from django.core.signing import Signer\n        signer = Signer()\n        request.COOKIES['key'] = signer.sign('value')\n        self.assertEqual(request.get_signed_cookie('key'), 'value')\n\n        # Test get_signed_cookie with expired signature\n        from datetime import timedelta\n        from django.core.signing import Signer\n        from django.utils import timezone\n        signer = Signer()\n        request.COOKIES['key'] = signer.sign('value', expires=timezone.now() - timedelta(minutes=1))\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('key')\n\n        # Test get_signed_cookie with invalid signature\n        from django.core.signing import Signer\n        signer = Signer()\n        request.COOKIES['key'] = signer.sign('value', salt='wrong_salt')\n        with self.assertRaises(signing.BadSignature):\n            request.get_signed_cookie('key')\n"], "sample_734": ["def test_mutual_info_score_sparse_contingency_matrix():\n    \"\"\"Test mutual_info_score with a sparse contingency matrix.\"\"\"\n    from scipy import sparse\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 0, 1, 1])\n    contingency_matrix_sparse = sparse.csr_matrix([\n        [2, 0, 0],\n        [0, 2, 0],\n        [0, 0, 2]\n    ])\n    # Test that mutual_info_score raises an error when contingency matrix\n    # is a sparse matrix but eps is not None\n    assert_raise_message(ValueError, lambda: mutual_info_score(labels_true, labels_pred, contingency=contingency_matrix_sparse, eps=1.0))\n    # Test that mutual_info_score works with a sparse contingency matrix\n    result = mutual_info_score(labels_true, labels_pred, contingency=contingency_matrix_sparse)\n    assert_array_almost_equal(result, 1.0)\n", "def test_mutual_info_score_array():\n    \"\"\"Test that mutual_info_score returns a float when the contingency matrix is passed as an array.\"\"\"\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 1, 1, 1])\n    contingency = np.array([[2, 1], [0, 2]])\n    expected_result = mutual_info_score(labels_true, labels_pred, contingency=contingency)\n    assert isinstance(expected_result, float)\n    assert_almost_equal(expected_result, 1.0)\n\n", "def test_homogeneity_completeness_v_measure():\n    # test case where labels_true and labels_pred have the same number of clusters\n    labels_true = [0, 0, 1, 1]\n    labels_pred = [0, 0, 1, 1]\n    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_equal(homogeneity, 1.0)\n    assert_equal(completeness, 1.0)\n    assert_equal(v_measure, 1.0)\n\n    # test case where labels_true and labels_pred have different number of clusters\n    labels_true = [0, 0, 1, 1]\n    labels_pred = [0, 0, 1, 2]\n    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_greater(v_measure, 0.0)\n    assert_greater(v_measure, homogeneity)\n    assert_greater(v_measure, completeness)\n\n    # test case where labels_true and labels_pred are completely different\n    labels_true = [0, 0, 1, 1]\n    labels_pred = [1, 2, 0, 0]\n    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_less(v_measure, 0.5)\n\n    # test case where labels_true and labels_pred are the same but with different order\n    labels_true = [0, 1, 2, 2]\n    labels_pred = [0, 0, 1, 1]\n    homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(labels_true, labels_pred)\n    assert_equal(homogeneity, 1.0)\n    assert_equal(completeness, 1.0)\n    assert_equal(v_measure, 1.0)\n\n    # test case where labels_true and labels_pred are the same but with NaN values\n    labels_true = [0, 0, np.nan, np.nan]\n    labels_pred = [0, 0, np.nan, np.nan]\n    assert_raise_message(ValueError, \"labels_true must be 1D\", lambda: homogeneity_completeness_v_measure(labels_true, labels_pred))\n    assert_raise_message(ValueError, \"labels_pred must be 1D\", lambda: homogeneity_completeness_v", "def test_fowlkes_mallows_score_symmetry():\n    \"\"\"Check that the Fowlkes-Mallows score is symmetric\"\"\"\n    labels_true = np.array([0, 0, 1, 1])\n    labels_pred = np.array([0, 0, 1, 1])\n    expected_score = fowlkes_mallows_score(labels_true, labels_pred)\n    assert_almost_equal(fowlkes_mallows_score(labels_true, labels_pred),\n                       fowlkes_mallows_score(labels_pred, labels_true))\n    assert_equal(fowlkes_mallows_score(labels_true, labels_pred), expected_score)\n", "def test_entropy():\n    # Test entropy of all zeros\n    labels = np.zeros(100)\n    assert_almost_equal(entropy(labels), 0.0)\n\n    # Test entropy of all ones\n    labels = np.ones(100)\n    assert_almost_equal(entropy(labels), 0.0)\n\n    # Test entropy of a mix of zeros and ones\n    labels = np.random.randint(2, size=100)\n    assert_greater(entropy(labels), 0.0)\n\n    # Test entropy of a large number of unique labels\n    labels = np.random.randint(100, size=100)\n    assert_greater(entropy(labels), 0.0)\n\n    # Test entropy of a single label\n    labels = np.ones(100) * 0\n    assert_almost_equal(entropy(labels), 0.0)\n\n    # Test entropy of a single label\n    labels = np.ones(100) * 1\n    assert_almost_equal(entropy(labels), 0.0)\n\n    # Test that entropy is zero for an array with all values equal\n    assert_almost_equal(entropy(np.ones(100) * 0), 0.0)\n    assert_almost_equal(entropy(np.ones(100) * 1), 0.0)\n    assert_almost_equal(entropy(np.ones(100) * 2), 0.0)\n    assert_almost_equal(entropy(np.ones(100) * 3), 0.0)\n\n    # Test that entropy is equal to log(n) when all labels are unique\n    labels = np.arange(10)\n    assert_almost_equal(entropy(labels), log(10))\n"], "sample_657": ["    def test_mark_decorator_repr(self):\n        mark = MarkDecorator(Mark(\"test\", (\"arg1\",), {\"key\": \"value\"}))\n        assert repr(mark) == \"<MarkDecorator Mark(name='test', args=('arg1',), kwargs={'key': 'value'})>\"\n", "    def test_decorator_behavior(self):\n        @MARK_GEN.slowtest\n            pass\n\n        assert hasattr(test_function, \"pytestmark\")\n        assert test_function.pytestmark.mark.name == \"slowtest\"\n", "    def test_getattr(self):\n        gen = MarkGenerator()\n        gen._config = mock.Mock()\n        gen._markers = set()\n        assert gen.mark_info == MARK_GEN\n", "    def test_mark_decorator_called_on_callable(self):\n            pass\n\n        mark = pytest.mark.slowtest\n        decorated_function = mark(test_function)\n        assert decorated_function == test_function\n        assert get_unpacked_marks(test_function)[0].name == \"slowtest\"\n", "    def test_empty_parameterset_mark(self):\n        with pytest.warns(PytestDeprecationWarning):\n            mark = MARK_GEN.empty_parameterset_mark(\"test_function\")\n            assert mark.name == \"empty_parameterset\"\n            assert mark.args == ()\n            assert mark.kwargs == {}\n            assert mark.combined_with(Mark(\"skip\", (), {})).name == \"skip\"\n"], "sample_117": ["    def test_password_reset_form(self):\n        form = PasswordResetForm(data={'email': 'testclient@example.com'})\n        self.assertTrue(form.is_valid())\n", "    def test_init(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertEqual(form.fields['password'].help_text, form.fields['password'].help_text.format('../password/'))\n        self.assertIn('user_permissions', form.fields)\n        self.assertEqual(form.fields['user_permissions'].queryset.model, User.permissions.model)\n", "    def test_valid_password_change(self):\n        form = AdminPasswordChangeForm(self.u1)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.changed_data, ['password'])\n", "    def test_empty_password_field(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertTrue(form.fields['password'].bound_data is None)\n", "    def test_password_field_empty(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertIn('password', form.fields)\n        self.assertIsInstance(form.fields['password'], ReadOnlyPasswordHashField)\n"], "sample_528": ["def test_style_library_update():\n    \"\"\"Test that updating the user library with a new style file does not\n    alter the base library.\"\"\"\n    style_name = \"test_style\"\n    temp_file = '%s.%s' % (style_name, STYLE_EXTENSION)\n    temp_dir = tempfile.mkdtemp()\n    temp_path = Path(temp_dir, temp_file)\n    with temp_path.open('w', encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in DUMMY_SETTINGS.items()))\n\n    # Add tmpdir to style path and reload so we can access this style.\n    USER_LIBRARY_PATHS.append(temp_dir)\n    style.reload_library()\n\n    # Update the user library with a new style file in the same directory.\n    new_style_name = \"new_test_style\"\n    new_temp_file = '%s.%s' % (new_style_name, STYLE_EXTENSION)\n    new_temp_path = Path(temp_dir, new_temp_file)\n    with new_temp_path.open('w', encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in DUMMY_SETTINGS.items()))\n\n    style.update_user_library(style.library)\n\n    # Check that the base library is still the same after updating the user library.\n    for style_name in style.library:\n        if style_name != \"test_style\":\n            assert style.library[style_name] == _base_library[style_name]\n", "def test_reload_library():\n    # Test that style.reload_library() correctly reloads the style library\n    # when USER_LIBRARY_PATHS is modified.\n\n    # Create a temporary style file in a new directory\n    with temp_style('test_style') as _:\n        # Add the tmpdir to USER_LIBRARY_PATHS\n        USER_LIBRARY_PATHS.insert(0, '/tmpdir')  # Replace '/tmpdir' with actual path\n        style.reload_library()\n        assert 'test_style' in style.library\n\n        # Modify the style library\n        USER_LIBRARY_PATHS.insert(1, '/tmpdir')  # Replace '/tmpdir' with actual path\n        style.reload_library()\n        assert 'test_style' in style.library\n\n        # Test that _base_library is not affected by USER_LIBRARY_PATHS changes\n        assert 'test_style' not in style._base_library\n\n    # Test that style.reload_library() correctly updates available styles\n    assert 'test_style' in style.available\n    USER_LIBRARY_PATHS.remove('/tmpdir')  # Replace '/tmpdir' with actual path\n    style.reload_library()\n    assert 'test_style' not in style.available\n", "def test_reload_library():\n    \"\"\"Test that reloading the style library removes the temporary style.\"\"\"\n    with temp_style(\"temp_style\", settings=DUMMY_SETTINGS):\n        style.use('temp_style')\n        plt.figure()\n        plt.plot(np.linspace(0, 10, 10))\n        plt.savefig(\"temp_figure.png\")\n        style.reload_library()\n        with pytest.raises(KeyError):\n            style.use('temp_style')\n", "def test_style_context_manager():\n    with temp_style('test_style', settings=DUMMY_SETTINGS):\n        # Test that the style has been applied\n        assert style.use('test_style') == DUMMY_SETTINGS\n        plt.figure()\n        plt.gca().set_facecolor(DUMMY_SETTINGS[PARAM])\n        \n    # Test that the style is reset after leaving the context manager\n    assert style.use('default') == rcParamsDefault\n    plt.figure()\n    # The default facecolor should be 'white'\n    assert plt.gca().get_facecolor() == (1.0, 1.0, 1.0, 1.0)\n\n", "def test_use_deprecated_seaborn_style(style_name):\n    \"\"\"Test using deprecated seaborn styles.\"\"\"\n    with temp_style(style_name, settings=DUMMY_SETTINGS):\n        assert style.use(style_name) is None\n        assert style.use(style_name) is None\n\n        # Check that using a deprecated style does not affect other styles\n        with temp_style('other_style', settings=DUMMY_SETTINGS):\n            assert style.use('other_style') is None\n\n"], "sample_600": ["def test_unsigned_integer_coder_roundtrip():\n    # Test encoding and decoding of unsigned integer data with _Unsigned attribute\n    ds = xr.Dataset({\"x\": ((\"y\",), np.array([1, 2, 3], dtype=np.uint8))})\n    variable = ds.x\n\n    coder = UnsignedIntegerCoder()\n    encoded = coder.encode(variable)\n    decoded = coder.decode(encoded)\n\n    assert_equal(decoded, variable)\n\n    # Test encoding and decoding of signed integer data with _Unsigned attribute\n    ds = xr.Dataset({\"x\": ((\"y\",), np.array([-1, 0, 1], dtype=np.int8))})\n    variable = ds.x\n\n    coder = UnsignedIntegerCoder()\n    encoded = coder.encode(variable)\n    decoded = coder.decode(encoded)\n\n    assert_equal(decoded, variable)\n\n    # Test encoding and decoding of data with _Unsigned attribute when data type is not integer\n    ds = xr.Dataset({\"x\": ((\"y\",), np.array([1.0, 2.0, 3.0], dtype=np.float32))})\n    variable = ds.x\n\n    coder = UnsignedIntegerCoder()\n    with pytest.raises(SerializationWarning):\n        coder.encode(variable)\n    with pytest.raises(SerializationWarning):\n        coder.decode(variable)\n", "def test_cf_scale_offset_decoding_roundtrip():\n    # Test a simple case where scale_factor and add_offset are applied during encoding\n    # and then reversed during decoding.\n\n    data = np.array([1, 2, 3, 4, 5])\n    attrs = {\"scale_factor\": 2, \"add_offset\": 1}\n    dims = [\"x\"]\n\n    encoded = encode_cf_variable(xr.Variable(dims, data, attrs))\n    decoded = decode_cf_variable(encoded)\n\n    assert_identical(decoded.data, data * 2 + 1)\n\n    # Test a case where scale_factor and add_offset are not applied during encoding\n    # but are present in the attrs.\n    data = np.array([1, 2, 3, 4, 5])\n    attrs = {\"scale_factor\": 2, \"add_offset\": 1}\n    dims = [\"x\"]\n\n    variable = xr.Variable(dims, data, attrs)\n    encoded = variables.CFScaleOffsetCoder().encode(variable)\n    decoded = variables.CFScaleOffsetCoder().decode(encoded)\n\n    assert_identical(decoded.data, data)\n\n    # Test a case where scale_factor and add_offset are not present in attrs\n    # but are present in encoding.\n    data = np.array([1, 2, 3, 4, 5])\n    attrs = {}\n    encoding = {\"scale_factor\": 2, \"add_offset\": 1}\n    dims = [\"x\"]\n\n    variable = xr.Variable(dims, data, attrs, encoding)\n    encoded = variables.CFScaleOffsetCoder().encode(variable)\n    decoded = variables.CFScaleOffsetCoder().decode(encoded)\n\n    assert_identical(decoded.data, data * 2 + 1)\n\n    # Test a case where scale_factor is not a scalar.\n    data = np.array([1, 2, 3, 4, 5])\n    attrs = {\"scale_factor\": [2, 3]}\n    dims = [\"x\"]\n\n    variable = xr.Variable(dims, data, attrs)\n    encoded = variables.CFScaleOffsetCoder().encode(variable)\n    decoded = variables.CFScaleOffsetCoder().decode(encoded)\n\n    assert_identical(decoded.data, data * np.array([2, 3]))\n\n    # Test a case where add_offset is not a scalar.\n    data = np.array([1, 2, 3, 4, 5])\n    attrs = {\"add_offset\": [1", "def test_CFScaleOffsetCoder_encode_decode():\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    da_x = x.chunk().dask_array\n\n    # Test with scale_factor and add_offset\n    encoded_x = encode_cf_variable(x)\n    assert_equal(encoded_x, x)\n\n    cf_scale_offset_coder = CFScaleOffsetCoder()\n    decoded_x = cf_scale_offset_coder.decode(encoded_x)\n    assert_identical(decoded_x.data, x.data)\n\n    # Test with no scale_factor and add_offset\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    encoded_x = encode_cf_variable(x)\n    assert_equal(encoded_x, x)\n\n    decoded_x = cf_scale_offset_coder.decode(encoded_x)\n    assert_identical(decoded_x.data, x.data)\n\n    # Test with scale_factor but no add_offset\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    x.encoding = {\"scale_factor\": 2}\n    encoded_x = encode_cf_variable(x)\n    assert_equal(encoded_x, x)\n\n    decoded_x = cf_scale_offset_coder.decode(encoded_x)\n    assert_identical(decoded_x.data, x.data)\n\n    # Test with add_offset but no scale_factor\n    x = xr.DataArray([1, 2, 3], dims=\"x\")\n    x.encoding = {\"add_offset\": 1}\n    encoded_x = encode_cf_variable(x)\n    assert_equal(encoded_x, x)\n\n    decoded_x = cf_scale_offset_coder.decode(encoded_x)\n    assert_identical(decoded_x.data, x.data)\n", "def test_cfmasks_unsigned():\n    da = da\n    # Test encoding and decoding with single fill value\n    x = xr.DataArray([1, 2, 3], dims=['x'])\n    var = x.encode_cf('test')\n    da_var = variables.decode_cf(var)\n    assert_equal(da_var.data, x.data)\n    assert_equal(da_var.encoding, x.encoding)\n\n    # Test encoding and decoding with multiple fill values\n    x = xr.DataArray([1, 2, 3], dims=['x'])\n    var = x.encode_cf('test', encoding={'_FillValue': 4, 'missing_value': 5})\n    da_var = variables.decode_cf(var)\n    assert_equal(da_var.data, x.data)\n    assert_equal(da_var.encoding, x.encoding)\n\n    # Test encoding and decoding with conflicting fill values\n    x = xr.DataArray([1, 2, 3], dims=['x'])\n    with pytest.raises(ValueError):\n        x.encode_cf('test', encoding={'_FillValue': 4, 'missing_value': 4})\n\n    # Test decoding with variable encoding\n    x = xr.DataArray([1, 2, 3], dims=['x'], encoding={'scale_factor': 2})\n    var = x.encode_cf('test')\n    da_var = variables.decode_cf(var)\n    assert_equal(da_var.data, x.data)\n    assert_equal(da_var.encoding, x.encoding)\n\n    # Test decoding with variable attributes\n    x = xr.DataArray([1, 2, 3], dims=['x'], attrs={'add_offset': 1})\n    var = x.encode_cf('test')\n    da_var = variables.decode_cf(var)\n    assert_equal(da_var.data, x.data)\n    assert_equal(da_var.encoding, x.encoding)\n\n    # Test decoding with both scale_factor and add_offset\n    x = xr.DataArray([1, 2, 3], dims=['x'], attrs={'scale_factor': 2, 'add_offset': 1})\n    var = x.encode_cf('test')\n    da_var = variables.decode_cf(var)\n    assert_equal(da_var.data, x.data)\n    assert_equal(da_var.encoding, x.encoding)\n\n    # Test unsigned integer decoding\n    x = xr.DataArray([1, 2, 3], dims=['x'], attrs={'_Unsigned': 'true'})\n    var = x.encode_cf('test')\n    da_var = variables.decode_cf(var)\n    assert_equal(da", "def test_CFSOcoder_scale_factor_and_add_offset():\n    # Create a test array with a scale factor and add offset\n    arr = np.array([1.0, 2.0, 3.0])\n    var = xr.Variable([\"x\"], arr)\n    attrs = {\"scale_factor\": 2.0, \"add_offset\": 1.0}\n\n    # Encode the variable\n    coder = CFScaleOffsetCoder()\n    encoded_var = coder.encode(var)\n    encoded_arr = encoded_var.data\n\n    # Check that the encoding attributes are set correctly\n    assert encoded_var.encoding[\"scale_factor\"] == 2.0\n    assert encoded_var.encoding[\"add_offset\"] == 1.0\n    assert encoded_arr.dtype == np.float64\n\n    # Decode the variable\n    decoded_var = coder.decode(encoded_var)\n    decoded_arr = decoded_var.data\n\n    # Check that the decoding transformed the array correctly\n    assert_allclose(decoded_arr, arr * 2.0 + 1.0)\n\n    # Test that missing values are handled correctly\n    var = xr.Variable([\"x\"], arr, attrs={\"scale_factor\": 2.0, \"add_offset\": 1.0})\n    var.data[1] = np.nan\n    encoded_var = coder.encode(var)\n    decoded_var = coder.decode(encoded_var)\n    assert np.isnan(decoded_var.data[1])\n\n    # Test that values with a scale factor and add offset that are not numbers raise a ValueError\n    var = xr.Variable([\"x\"], arr)\n    var.attrs[\"scale_factor\"] = \"scale_factor\"\n    var.attrs[\"add_offset\"] = \"add_offset\"\n    with pytest.raises(ValueError):\n        coder.encode(var)\n"], "sample_821": ["def test_affinity_propagation_precomputed_preference():\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    affinity_matrix = euclidean_distances(X, X)\n    ap = AffinityPropagation(affinity='precomputed', preference=1.0)\n    ap.fit(affinity_matrix)\n    assert_equal(ap.cluster_centers_indices_, np.array([0, 1, 2]))\n    assert_equal(ap.labels_, np.array([0, 0, 0, 1, 1, 1]))\n", "def test_affinity_propagation_convergence_warning():\n    \"\"\"Test that affinity propagation raises ConvergenceWarning when it does not converge\"\"\"\n    # Create a dataset that should not converge\n    X = np.array([[1, 1], [1, 1], [1, 1], [1, 1]])\n    ap = AffinityPropagation(affinity='euclidean', verbose=False)\n    with assert_warns(ConvergenceWarning):\n        ap.fit(X)\n\n", "def test_affinity_propagation_no_preference_no_convergence():\n    affinity_propagation(np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]),\n                        preference=None, max_iter=1, convergence_iter=1,\n                        damping=0.5, copy=True, verbose=False,\n                        return_n_iter=False)\n    assert_warns_message(ConvergenceWarning,\n                         affinity_propagation,\n                         np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]]),\n                         preference=None, max_iter=1, convergence_iter=1,\n                         damping=0.5, copy=True, verbose=False,\n                         return_n_iter=False))\n", "def test_affinity_propagation_convergence_warning():\n    \"\"\"Test that affinity_propagation raises a ConvergenceWarning when it\n    does not converge.\"\"\"\n    # Make the warning visible\n    with warnings.catch_warnings(record=True) as w:\n        ap = AffinityPropagation(max_iter=1)\n        ap.fit(X)\n        assert len(w) == 1\n        assert str(w[0].message) == \"Affinity propagation did not converge, this model will not have any cluster centers.\"\n        assert w[0].category == ConvergenceWarning\n\n", "def test_affinity_propagation_convergence():\n    \"\"\"Test that affinity propagation does not converge and returns a warning.\n\n    See #14570.\n    \"\"\"\n    # Create a matrix with equal similarities and equal preferences\n    S = np.ones((10, 10))\n    AP = AffinityPropagation(preference=0)\n    with pytest.warns(ConvergenceWarning):\n        AP.fit(S)\n\n    assert AP.cluster_centers_ is None\n    assert AP.n_iter_ == 0\n    assert AP.labels_.shape == (10,)\n    assert AP.labels_.fill(-1)\n"], "sample_1190": ["    def test_get_dimension_system(self):\n        si = SI\n        assert si.get_dimension_system() is si._dimension_system\n", "def test_get_units_non_prefixed():\n    # Test that empty set is returned when system has no units\n    si = UnitSystem.get_unit_system(\"SI\")\n    assert si.get_units_non_prefixed() == set()\n\n    # Test that units without prefixes are returned\n    assert si.get_units_non_prefixed() == {meter, kilogram, second}\n\n    # Test that units with prefixes are not returned\n    assert joule in si.get_units_non_prefixed() == False\n    assert kilogram in si.get_units_non_prefixed() == True\n    assert kilogram in si.get_units_non_prefixed() == True\n    assert kilo * meter in si.get_units_non_prefixed() == False\n\n    # Test that physical constants are not returned\n    assert si.get_units_non_prefixed() == {meter, kilogram, second} - {molar_gas_constant, speed_of_light}\n\n    # Test that units with prefixes are not returned in a system that has no prefix\n    new_unit_system = UnitSystem.get_default_unit_system().extend(\n        units=(kilogram, second))\n    assert new_unit_system.get_units_non_prefixed() == {kilogram, second}\n", "def test_extend_system():\n    u = UnitSystem((\"m\", \"s\"), (), \"My system\")\n    new_u = u.extend((\"kg\", \"A\"), (\"V\", \"W\"), \"Extended system\")\n    assert new_u.name == \"Extended system\"\n    assert new_u._base_units == (\"m\", \"s\", \"kg\", \"A\")\n    assert new_u._units == (\"m\", \"s\", \"kg\", \"A\", \"V\", \"W\")\n    assert new_u._derived_units == {}\n    ", "def test_get_units_non_prefixed():\n    # Test that only units without prefixes are returned\n    units = SI.get_units_non_prefixed()\n    assert set(units) == {meter, kilogram, second, ampere, kelvin, mole, candela}\n", "def test_unit_system_get_dimensional_expr():\n    from sympy.physics.units import Quantity\n    from sympy import symbols, sin\n\n    # Test with a simple Quantity\n    t = symbols('t')\n    expr = 2 * Quantity('m') + 3 * Quantity('kg')\n    system = SI\n    assert system.get_dimensional_expr(expr) == Quantity('m') ** 2 * Quantity('kg')\n\n    # Test with a derivative\n    expr = diff(Quantity('m') ** 2 + Quantity('kg') * sin(t), t)\n    assert system.get_dimensional_expr(expr) == Quantity('kg')\n\n    # Test with a Mul\n    expr = Quantity('m') * Quantity('kg')\n    assert system.get_dimensional_expr(expr) == Quantity('m') * Quantity('kg')\n\n    # Test with a Pow\n    expr = Quantity('m') ** 2\n    assert system.get_dimensional_expr(expr) == Quantity('m') ** 2\n\n    # Test with a Function\n    expr = sin(Quantity('kg'))\n    assert system.get_dimensional_expr(expr) == Quantity('kg')\n\n    # Test with a Dimension\n    expr = Quantity('m').dimension\n    assert system.get_dimensional_expr(expr) == Quantity('m').dimension\n\n    # Test with a non-Quantity expression\n    expr = 2 * 3\n    assert system.get_dimensional_expr(expr) == S.One\n\n    # Test with a mix of quantities and numbers\n    expr = 2 * Quantity('m') + 3\n    assert system.get_dimensional_expr(expr) == Quantity('m') ** 2\n\n    # Test with a complex expression\n    expr = Quantity('m') ** 2 + Quantity('kg') * Quantity('s') + 2 * Quantity('kg') * Quantity('s') ** 2\n    assert system.get_dimensional_expr(expr) == Quantity('m') ** 2 * Quantity('kg') * Quantity('s')\n"], "sample_270": ["    def test_long_column_names_django_2_2(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n\n        errors = Model.check()\n        self.assertEqual(len(errors), 0)\n", "    def test_multiple_parent_clashes(self):\n        from invalid_models_tests import ParentModel1, ParentModel2, ChildModel\n        errors = ChildModel.check()\n        self.assertEqual(len(errors), 2)\n", "    def test_check_model_name_db_lookup_clashes(self):\n        # Test that model names without underscores are not flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = 'TestModel'\n        self.assertEqual(TestModel._check_model_name_db_lookup_clashes(), [])\n\n        # Test that model names with a single underscore are not flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = 'Test_Model'\n        self.assertEqual(TestModel._check_model_name_db_lookup_clashes(), [])\n\n        # Test that model names starting with an underscore are flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = '_TestModel'\n        self.assertEqual(len(TestModel._check_model_name_db_lookup_clashes()), 1)\n\n        # Test that model names ending with an underscore are flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = 'TestModel_'\n        self.assertEqual(len(TestModel._check_model_name_db_lookup_clashes()), 1)\n\n        # Test that model names with double underscores are not flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = 'Test__Model'\n        self.assertEqual(TestModel._check_model_name_db_lookup_clashes(), [])\n\n        # Test that model names with double underscores are flagged\n        # for the \"model name DB lookup clash\" error.\n        class TestModel(models.Model):\n            class Meta:\n                app_label = 'invalid_models_tests'\n                model_name = 'Test__Model__'\n        self.assertEqual(len(TestModel._check_model_name_db_lookup_clashes()), 1)\n", "    def test_check_constraint_on_expressions(self):\n        class MyModel(models.Model):\n            value = models.IntegerField()\n\n        MyModel.objects.create(value=10)\n        MyModel.objects.create(value=20)\n\n        class MyConstraint(models.CheckConstraint):\n            check = Q(value__gt=15)\n\n        MyModel._meta.constraints.append(MyConstraint())\n\n        errors = MyModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n", "    def test_check_swappable(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'non.existent.app.NonExistentModel'\n\n        with self.assertRaises(LookupError):\n            SwappableModel._check_swappable()\n"], "sample_552": ["def test_figure_show():\n    fig = Figure(figsize=(4, 4))\n    ax = fig.add_subplot(1, 1, 1)\n    fig.show()\n    assert fig.canvas.manager is not None\n", "def test_figure_subplot():\n    # Test adding a subplot to a figure with a layout engine\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5), layout='constrained')\n    fig.canvas.draw()\n\n    # Check that the layout engine is used\n    assert fig.get_layout_engine().get_info() is not None\n\n    # Test adding a subplot to a figure with a layout engine and a colorbar\n    cb = plt.colorbar()\n    fig.canvas.draw()\n    assert fig.get_layout_engine().get_info() is not None\n\n    # Test adding a subplot to a figure with a layout engine and a colorbar\n    # with custom padding\n    fig.set_layout_engine(layout='constrained', w_pad=1.5)\n    cb = plt.colorbar()\n    fig.canvas.draw()\n    assert fig.get_layout_engine().get_info()['w_pad'] == 1.5\n\n    # Test adding a subplot to a figure with a layout engine and a colorbar\n    # with custom padding and aspect ratio\n    fig.set_layout_engine(layout='constrained', w_pad=1.5, h_pad=2.0)\n    cb = plt.colorbar()\n    fig.canvas.draw()\n    assert fig.get_layout_engine().get_info()['w_pad'] == 1.5\n    assert fig.get_layout_engine().get_info()['h_pad'] == 2.0\n\n    # Test adding a subplot to a figure with a layout engine and a colorbar\n    # with custom padding, aspect ratio and rectangle\n    fig.set_layout_engine(layout='constrained', w_pad=1.5, h_pad=2.0, rect=(0.1, 0.1, 0.8, 0.8))\n    cb = plt.colorbar()\n    fig.canvas.draw()\n    assert fig.get_layout_engine().get_info()['w_pad'] == 1.5\n    assert fig.get_layout_engine().get_info()['h_pad'] == 2.0\n    assert fig.get_layout_engine().get_info()['rect'] == (0.1, 0.1, 0.8, 0.8)\n", "def test_subplots_adjust():\n    fig = Figure()\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    ax1.text(0.5, 0.5, 'Left')\n    ax2.text(0.5, 0.5, 'Right')\n    fig.align_xlabels()\n    fig.subplots_adjust(left=0.15, right=0.85)\n    fig.canvas.draw()\n    assert fig.get_axes()[0].get_position().x1 < 0.4\n    assert fig.get_axes()[1].get_position().x1 > 0.5\n", "def test_figure_creation():\n    fig = Figure(figsize=(8, 6), dpi=100)\n    assert fig.get_size_inches() == (8, 6)\n    assert fig.get_dpi() == 100\n", "def test_figure_pick():\n    fig = plt.figure()\n    line, = plt.plot([1, 2, 3])\n    with pytest.raises(AttributeError):\n        fig.ginput(1)\n    assert fig.get_fignums() == []\n    assert fig.get_num() == None\n    fig, ax = plt.subplots()\n    assert fig.get_fignums() == [0]\n    assert fig.get_num() == 0\n"], "sample_214": ["    def test_get_prep_value_with_custom_encoder(self):\n        json_field = JSONField(encoder=lambda x: {'value': x})\n        value = {'value': 'hello'}\n        self.assertEqual(json_field.get_prep_value(value), json.dumps(value, cls=lambda x: {'value': x}))\n", "    def test_decoder_validation(self):\n        decoder = CustomJSONDecoder()\n        try:\n            decoder.decode('invalid json')\n            self.fail('Expected ValidationError to be raised')\n        except ValueError as e:\n            self.assertTrue(isinstance(e, ValueError))\n", "    def test_exact_lookup_with_none_rhs(self):\n        # When we use exact lookup on a JSONField with a None value on the right-hand side,\n        # it should raise a NotSupportedError.\n        with self.assertRaises(NotSupportedError):\n            JSONModel.objects.create(json_field={'key': 'value'})\n            JSONModel.objects.filter(json_field__exact=None)\n", "def test_from_db_value_custom_decoder(self):\n    model = JSONModel.objects.create(json_data={'name': 'John', 'age': 30})\n    connection = connection_for_write()\n    # Simulate the database returning a string value\n    connection.ops.quote_name = lambda name: name\n    json_str = connection.ops.json_cast_text_sql(f\"SELECT json_data FROM json_model WHERE id = 1\")\n    decoded_value = JSONField.from_db_value(json_str, None, connection)\n    self.assertEqual(decoded_value, {'name': 'John', 'age': 30})\n", "    def test_empty_strings_allowed_fails_validation(self):\n        json_field = JSONModel.json_field\n        model_instance = JSONModel.objects.create(json_field='')\n        with self.assertRaises(ValidationError):\n            json_field.validate(model_instance.json_field, model_instance)\n"], "sample_241": ["    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_combine(self):\n        query1 = Company.objects.values(\"name\", \"num_employees\")\n        query2 = Company.objects.values(\"num_chairs\")\n        query3 = query1.combine(query2, AND)\n        self.assertEqual(list(query3.values_list()), [((\"Example Inc.\", 2300), (5,), (\"Foobar Ltd.\", 3, 4), (\"Test GmbH\", 32, 1)])\n", "    def test_execute_query(self):\n        connection = mock.Mock()\n        connection.cursor.return_value.fetchall.return_value = [(1,)]\n        query = RawQuery(\"SELECT %s\", \"default\", params=(1,))\n        self.assertEqual(query.execute_query(), [(1,)])\n        self.assertEqual(query.cursor.execute.call_args, mock.call(\"SELECT %s\", (1,)))\n", "    def test_resolve_ref_f_with_col(self):\n        # Test resolving F() objects\n        expr = F('name')\n        self.assertIsInstance(expr, Ref)\n        self.assertEqual(expr.field.name, 'name')\n        self.assertEqual(expr.refs, 'name')\n\n        # Test resolving F() objects with Col\n        col = Col('name')\n        expr = F(col)\n        self.assertIsInstance(expr, Ref)\n        self.assertEqual(expr.field.name, 'name')\n        self.assertEqual(expr.refs, 'name')\n        self.assertEqual(expr.arg, col)\n"], "sample_861": ["def test_GridSearchCV_param_grid_error(input, error_type, error_message):\n    with assert_raise_message(error_type, error_message):\n        GridSearchCV(\n            estimator=MockClassifier(),\n            param_grid=input,\n            cv=KFold(n_splits=3)\n        )\n", "def test_check_param_grid():\n    with assert_warns(DeprecationWarning):\n        param_grid = {'a': [1, 2]}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_warns(DeprecationWarning):\n        param_grid = {'a': {'b': [1, 2]}}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_warns(DeprecationWarning):\n        param_grid = [{'a': [1, 2], 'b': [3, 4]}, {'c': [5, 6]}]\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': 1}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': [1]}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': 'a'}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': [1, 2], 'b': 3}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': np.array([1, 2])}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': [1, 2], 'b': np.array([3, 4])}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': [1, 2], 'b': [3, 4], 'c': [5, 6]}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(ValueError):\n        param_grid = {'a': []}\n        GridSearchCV(estimator=LinearSVC(), param_grid=param_grid)\n\n    with assert_raises(TypeError):\n        param_grid = {'a': [1, 2], 'b': [3, 4], 'c': 'd'}\n        GridSearch", "def test_randomized_search_cv_run_search_called():\n    from sklearn.model_selection._search import RandomizedSearchCV\n\n    param_distributions = {'kernel': ['linear', 'rbf'], 'C': [1, 10]}\n    n_iter = 10\n    random_state = 42\n    scoring = 'accuracy'\n    estimator = SVC()\n    cv = KFold(n_splits=2, shuffle=True, random_state=42)\n\n    search = RandomizedSearchCV(estimator=estimator, param_distributions=param_distributions,\n                               n_iter=n_iter, random_state=random_state, scoring=scoring, cv=cv)\n\n    # Mocking the _run_search method to check that it is called\n    _run_search_mock = mock.Mock()\n\n    # Replace the _run_search method with the mock\n    search._run_search = _run_search_mock\n\n    # Call the fit method\n    search.fit(X, y)\n\n    # Check that _run_search was called with the correct arguments\n    _run_search_mock.assert_called_once_with(ParameterSampler(\n        param_distributions, n_iter, random_state=random_state))\n", "    def test_refit_error_score_not_number(self, klass):\n        # Test that error_score must be a number or 'raise'\n        with pytest.raises(TypeError):\n            klass(RandomizedSearchCV(estimator=SVC(), param_distributions={\n                'C': [1, 2]\n            }), error_score='invalid')\n", "def test_ParameterGrid_param_grid_iterator():\n    param_grid = {'a': [1, 2], 'b': [True, False]}\n    grid = ParameterGrid(param_grid)\n    list(grid)  # Run through the grid once to get it into a consistent state\n    assert len(list(grid)) == 4\n    assert len(grid) == 4\n    assert grid[0] == {'a': 1, 'b': True}\n    assert grid[1] == {'a': 1, 'b': False}\n    assert grid[2] == {'a': 2, 'b': True}\n    assert grid[3] == {'a': 2, 'b': False}\n    assert grid[4] == {}  # Edge case: empty grid\n    with pytest.raises(IndexError):\n        grid[5]\n"], "sample_739": ["def test_LabelEncoder_fit_transform():\n    le = LabelEncoder()\n    y = [1, 2, 2, 6]\n    expected_classes = np.array([1, 2, 6])\n    expected_y = np.array([0, 0, 1, 2])\n\n    le.fit(y)\n    y_transformed = le.fit_transform(y)\n\n    assert_array_equal(y_transformed, expected_y)\n    assert_array_equal(le.classes_, expected_classes)\n\n    le.fit_transform(y)\n    assert_array_equal(le.classes_, expected_classes)\n", "def test_LabelEncoder_inverse_transform_with_unseen_label():\n    le = LabelEncoder()\n    le.fit([1, 2, 2, 6])\n    y = le.transform([1, 1, 2, 6])\n    assert_raises(ValueError, le.inverse_transform, y)\n", "def test_label_binarizer_sparse_threshold():\n    lb = LabelBinarizer(sparse_output=True)\n    X = coo_matrix(np.array([[0, 2, 0], [0, 1, 0], [1, 0, 1]]))\n    X = lb.fit_transform(X)\n    assert issparse(X)\n    assert X.format == 'csr'\n    assert X.shape == (3, 3)\n", "def test_label_binarize_multiclass_with_default_threshold():\n    # Test with default threshold\n    y = np.array([1, 6, 2, 4])\n    classes = np.array([1, 2, 4, 6])\n    lb = LabelBinarizer()\n    lb.fit(y)\n    expected_result = np.array([[1, 0, 0, 0],\n                               [0, 0, 0, 1],\n                               [0, 1, 0, 0],\n                               [0, 0, 1, 0]])\n    assert_array_equal(lb.transform(y), expected_result)\n\n    # Test with empty array\n    y = np.array([])\n    lb = LabelBinarizer()\n    lb.fit(y)\n    assert_array_equal(lb.transform(y), np.array([]))\n\n    # Test with non-integer classes\n    classes = np.array([1.5, 2.5, 4.5, 6.5])\n    lb = LabelBinarizer()\n    lb.fit(y)\n    lb.classes_ = classes\n    y = np.array([1, 6, 2, 4])\n    expected_result = np.array([[0, 0, 0, 0],\n                               [0, 0, 0, 1],\n                               [0, 1, 0, 0],\n                               [0, 0, 1, 0]])\n    assert_array_equal(lb.transform(y), expected_result)\n", "    def test_fit_transform_empty_classes(self):\n        # Test if MultiLabelBinarizer can handle empty classes\n        mlb = MultiLabelBinarizer(sparse_output=False)\n        with ignore_warnings():\n            X = mlb.fit_transform([])\n        self.assertIsNone(mlb.classes_)\n"], "sample_714": ["def test_hamming_loss_multilabel():\n    X, y, y_prob = make_prediction(dataset=None, binary=False)\n    y_pred = np.zeros(y.shape, dtype=int)\n    for i in range(y.shape[0]):\n        if y[i] == 1:\n            y_pred[i, np.random.choice(np.where(y[i] == 1)[0])] = 1\n    assert_almost_equal(hamming_loss(y, y_pred), hamming_loss(y, y_prob))\n", "def test_hamming_loss_binary():\n    y_true = np.array([0, 1, 0, 1, 0, 1])\n    y_pred = np.array([0, 1, 0, 1, 1, 1])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 1/6)\n\n", "    def test_hamming_loss_binary(self):\n        y_true = np.array([1, 0, 1, 0])\n        y_pred = np.array([0, 0, 1, 0])\n        self.assertAlmostEqual(hamming_loss(y_true, y_pred), 0.5)\n", "def test_hamming_loss_multilabel():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 1], [1, 1, 0]])\n    assert_equal(hamming_loss(y_true, y_pred), 0.5)\n", "def test_hamming_loss_multilabel_binary_indicator():\n    \"\"\"Test that multilabel binary indicators work with hamming_loss\"\"\"\n    y_true = np.array([[0, 1], [1, 1]])\n    y_pred = np.ones((2, 2))\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.5)\n    # test with sample weights\n    assert_almost_equal(hamming_loss(y_true, y_pred, sample_weight=[1, 1]), 0.5)\n    # test with custom labels\n    labels = ['neg', 'pos']\n    assert_almost_equal(hamming_loss(y_true, y_pred, labels=labels), 0.5)\n    # test with sample weights and custom labels\n    assert_almost_equal(hamming_loss(y_true, y_pred, labels=labels, sample_weight=[1, 1]), 0.5)\n    # test with classes\n    classes = [0, 1]\n    assert_almost_equal(hamming_loss(y_true, y_pred, classes=classes), 0.5)\n    # test with sample weights and classes\n    assert_almost_equal(hamming_loss(y_true, y_pred, classes=classes, sample_weight=[1, 1]), 0.5)\n    # test that hamming_loss is zero when all predictions are correct\n    y_true = np.array([[0, 1], [1, 1]])\n    y_pred = np.array([[0, 1], [1, 1]])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.0)\n"], "sample_612": ["def test_dataarray_groupby_first():\n    \"\"\"Test DataArrayGroupBy.first and DataArrayGroupBy.last.\"\"\"\n    da = DataArray([1, 2, 3, 4])\n    groupby = da.groupby(\"x\")\n    assert_equal(groupby.first(), 1)\n    assert_equal(groupby.last(), 4)\n\n    da = DataArray([1, 2, 3, 4])\n    groupby = da.groupby(\"x\")\n    assert_equal(groupby.first(skipna=True), 1)\n    assert_equal(groupby.last(skipna=True), 4)\n\n    da = DataArray([1, 2, np.nan, 4])\n    groupby = da.groupby(\"x\")\n    with pytest.raises(ValueError):\n        groupby.first()\n    with pytest.raises(ValueError):\n        groupby.last()\n\n    da = DataArray([1, 2, np.nan, 4])\n    groupby = da.groupby(\"x\")\n    assert_equal(groupby.first(skipna=True), 1)\n    assert_equal(groupby.last(skipna=True), 4)\n\n    da = DataArray([1, 2, np.nan, 4])\n    groupby = da.groupby(\"x\")\n    assert_equal(groupby.first(keep_attrs=True), 1)\n    assert_equal(groupby.last(keep_attrs=True), 4)\n\n    ds = Dataset({\"a\": da})\n    groupby = ds.groupby(\"x\")\n    assert_equal(groupby.first(), 1)\n    assert_equal(groupby.last(), 4)\n    assert_equal(groupby.first(keep_attrs=True), 1)\n    assert_equal(groupby.last(keep_attrs=True), 4)\n\n", "def test_groupby_reduce_array_groupbyarray(dataset, array, dim, axis):\n    # test that reduce applies along the correct dimension\n    reduced = array.groupby(\"x\").reduce(np.mean, dim=dim, axis=axis)\n    expected = array.mean(dim=dim, axis=axis)\n    assert_identical(reduced, expected)\n", "def groupby_dataset(dataset):\n    return dataset.groupby(\"y\")\n\n", "def test_groupby_dims(group_dim, obj, expected_dims, dataset):\n    ds = dataset.groupby(group_dim)\n    assert_equal(ds.dims, expected_dims)\n\n", "def test_dataarray_groupby_map_non_scalar_type(values, expected):\n    da = DataArray([1, 2, 3], dims=[\"x\"], coords={\"x\": [1, 2, 3]})\n    grouped = da.groupby(\"x\").map(lambda x: values)\n\n    assert isinstance(grouped, DataArray)\n    assert grouped.dims == da.dims\n    assert grouped.coords == da.coords\n    assert grouped == expected\n\n"], "sample_769": ["def test_precision_recall_fscore_support_binary():\n    y_true = [0, 0, 1, 1, 0, 1]\n    y_pred = [0, 0, 0, 1, 0, 1]\n    precision, recall, fbeta_score, _ = precision_recall_fscore_support(y_true,\n                                                                        y_pred,\n                                                                        pos_label=0,\n                                                                        average=None)\n    assert_array_equal(precision, np.array([1.0, 1.0]))\n    assert_array_equal(recall, np.array([1.0, 1.0]))\n    assert_array_equal(fbeta_score, np.array([1.0, 0.0]))\n", "def test_hinge_loss():\n    y_true = np.array([1, -1, 1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.0)\n\n    y_true = np.array([1, 1, -1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.8)\n\n    y_true = np.array([1, 1, 1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.8)\n\n    y_true = np.array([1, 1, -1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.8)\n\n    y_true = np.array([1, -1, -1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.9)\n\n    y_true = np.array([1, 1, 1, 1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.7)\n\n    y_true = np.array([-1, -1, -1, -1])\n    pred_decision = np.array([0.1, 0.9, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.0)\n\n    y_true = np.array([1, 1, -1, -1])\n    pred_decision = np.array([0.1, 0.1, 0.8, 0.2])\n    assert_almost_equal(hinge_loss(y_true, pred_decision), 0.9)\n\n   ", "def test_hamming_loss_binary(y_true, y_pred):\n    # The hamming loss for binary classification should be between 0 and 1\n    assert 0 <= hamming_loss(y_true, y_pred) <= 1\n\n", "def test_average_precision_score_multiple_labels():\n    y_true = np.array([1, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 1, 0, 1, 1, 0])\n    average_precision = average_precision_score(y_true, y_pred, average='macro')\n    assert_almost_equal(average_precision, 0.5714285714285714)\n\n    # Test that the average precision score function correctly calculates average precision\n    # for multiple labels with different weights\n    y_true = np.array([1, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 1, 0, 1, 1, 0])\n    sample_weight = np.array([1, 1, 1, 1, 1, 1])\n    average_precision = average_precision_score(y_true, y_pred, average='macro', sample_weight=sample_weight)\n    assert_almost_equal(average_precision, 0.5714285714285714)\n\n    # Test that the average precision score function correctly calculates average precision\n    # for multiple labels with the 'binary' option\n    y_true = np.array([1, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 1, 0, 1, 1, 0])\n    average_precision = average_precision_score(y_true, y_pred, average='binary')\n    assert_almost_equal(average_precision, 0.75)\n\n    # Test that the average precision score function correctly calculates average precision\n    # for multiple labels with the 'samples' option\n    y_true = np.array([1, 1, 0, 0, 1, 1])\n    y_pred = np.array([1, 1, 0, 1, 1, 0])\n    average_precision = average_precision_score(y_true, y_pred, average='samples')\n    assert_almost_equal(average_precision, 0.4166666666666667)\n", "def test_hinge_loss_non_binary():\n    # Test case for non-binary classification with hinge loss\n    X, y, _ = make_prediction(binary=True)\n    y_pred = svm.SVC(probability=False).fit(X, y).predict(X)\n    with assert_warns_message(UndefinedMetricWarning, \"Binary classification metrics can't handle a mix\"):\n        hinge_loss(y, y_pred)\n"], "sample_978": ["def test_bspline_basis_set_knots_with_repeated_points():\n    from sympy.functions import bspline_basis_set\n    from sympy.core.compatibility import range\n    from sympy import Piecewise, Interval, And\n    from sympy import symbols, Rational\n\n    x, y = symbols('x,y')\n\n    # Test with knots that have repeated points\n    knots = [0, 0, 2, 2, 4]\n    d = 1\n    result = bspline_basis_set(d, knots, x)\n    expected = [\n        Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)),\n                  (0, True)),\n        Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)),\n                  (0, True))\n    ]\n    assert len(result) == len(expected)\n    for i in range(len(result)):\n        assert result[i].equals(expected[i])\n\n    # Test with knots that have repeated points and degree 0\n    knots = [0, 0, 2, 2, 4]\n    d = 0\n    result = bspline_basis_set(d, knots, x)\n    expected = [\n        Piecewise((1, (x >= 0) & (x <= 2)),\n                  (0, True)),\n        Piecewise((1, (x >= 0) & (x <= 2)),\n                  (0, True))\n    ]\n    assert len(result) == len(expected)\n    for i in range(len(result)):\n        assert result[i].equals(expected[i])\n", "def test_bspline_basis_zero_degree():\n    \"\"\"Test bspline_basis function with zero degree.\"\"\"\n    x = symbols('x')\n    d = 0\n    knots = [0, 0, 1, 1, 2, 2, 3]\n    expected_output = Piecewise((1, Interval(knots[d], knots[d+1]).contains(x)), (0, True))\n    assert bspline_basis(d, knots, 0, x) == expected_output\n", "def test_bspline_basis_set_rational_knots():\n    # Test rational knots\n    knots = [0, 1/2, 1, 3/2, 2]\n    d = 1\n    x = symbols('x')\n    bsplines = bspline_basis_set(d, knots, x)\n    # Manually calculate the expected results for d = 1\n    expected_bsplines = [\n        Piecewise((x/2 - 1, (x >= 0) & (x <= 1/2)),\n                  (-x/2 + 1, (x >= 1/2) & (x <= 1)),\n                  (0, True)),\n        Piecewise((x/2, (x >= 1) & (x <= 3/2)),\n                  (-x/2 + 2, (x >= 3/2) & (x <= 2)),\n                  (0, True))\n    ]\n    assert bsplines == expected_bsplines\n", "def test_bspline_basis_set_knots():\n    # Test that bspline_basis_set returns the correct number of splines\n    # for different number of knots\n    knots = range(5)\n    x = symbols('x')\n    for d in range(4):\n        for n_knots in range(3, 10):\n            n_splines = n_knots - d - 1\n            splines = bspline_basis_set(d, knots[:n_knots], x)\n            assert len(splines) == n_splines\n\n    # Test that bspline_basis_set returns the correct splines for different\n    # degree and knots\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n    x = symbols('x')\n    expected = [\n        Piecewise((-x/2 + 1, And(x >= 0, x <= 2)), (0, True)),\n        Piecewise((x/2, And(x >= 2, x <= 3)), (0, True)),\n        Piecewise((x/2, And(x >= 3, x <= 4)), (0, True)),\n        Piecewise((0, True)),\n    ]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines == expected\n\n    # Test that bspline_basis_set returns the correct splines for different\n    # degree and knots with repeated knots\n    d = 2\n    knots = [0, 0, 1, 1, 2, 3, 4]\n    x = symbols('x')\n    expected = [\n        Piecewise((x**2/2, And(x >= 0, x <= 1)), (0, True)),\n        Piecewise((-x**2/2 + 2*x - 1, And(x >= 1, x <= 2)), (0, True)),\n        Piecewise((x**2/2 - 2*x + 2, And(x >= 2, x <= 3)), (0, True)),\n        Piecewise((x/2 - 1, And(x >= 3, x <= 4)), (0, True)),\n        Piecewise((0, True)),\n    ]\n    splines = bspline_basis_set(d, knots, x)\n    assert splines == expected\n\n    # Test that bspline_basis_set raises an error for", "def test_bspline_basis_set_discontinuities():\n    x = symbols('x')\n    d = 1\n    knots = [0, 0, 1, 3, 4]\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == len(knots) - d - 1\n    for spline in splines:\n        assert isinstance(spline, Piecewise)\n        assert spline.is_Piecewise\n"], "sample_201": ["    def test_cookie_storage_valid_cookie(self):\n        storage = self.get_storage_instance()\n        messages = [Message('info', 'Test message')]\n        response = self.get_response_with_messages(messages)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n", "    def test_stored_messages_count_with_invalid_data(self):\n        storage = self.storage_class()\n        messages = [Message('message1', 'info'), Message('message2', 'success')]\n        set_cookie_data(storage, messages)\n        response = self.response\n        self.assertEqual(self.stored_messages_count(storage, response), 2)\n\n        # invalidate the data\n        set_cookie_data(storage, messages, invalid=True)\n        response = self.response\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_decode_invalid_hash(self):\n        \"\"\"\n        Test that an invalid hash in the cookie returns None.\n        \"\"\"\n        storage = self.storage_class(self.request)\n        set_cookie_data(storage, [])\n        storage.request.COOKIES[CookieStorage.cookie_name] = \"invalidhash$test\"\n        self.assertIsNone(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name]))\n", "    def setUp(self):\n        self.storage = CookieStorage()\n", "    def test_decode_invalid_data(self):\n        storage = self.storage\n        set_cookie_data(storage, [])\n        # Set an invalid hash by modifying the encoded data.\n        storage.request.COOKIES = {CookieStorage.cookie_name: b'invalid$encoded_data'}\n        self.assertIsNone(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name]))\n"], "sample_503": ["    def test_line2d_drawstyle(self):\n        fig, ax = plt.subplots()\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        line = mlines.Line2D(x, y, drawstyle='default')\n        ax.add_line(line)\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        ax.draw()\n        assert line._drawstyle == 'default'\n        assert line.drawstyle == 'default'\n        \n        line = mlines.Line2D(x, y, drawstyle='steps-mid')\n        ax.cla()\n        ax.add_line(line)\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        ax.draw()\n        assert line._drawstyle == 'steps-mid'\n        assert line.drawstyle == 'steps-mid'\n        \n        line = mlines.Line2D(x, y, drawstyle='steps-pre')\n        ax.cla()\n        ax.add_line(line)\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        ax.draw()\n        assert line._drawstyle == 'steps-pre'\n        assert line.drawstyle == 'steps-pre'\n        \n        line = mlines.Line2D(x, y, drawstyle='steps-post')\n        ax.cla()\n        ax.add_line(line)\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        ax.draw()\n        assert line._drawstyle == 'steps-post'\n        assert line.drawstyle == 'steps-post'\n        \n        line = mlines.Line2D(x, y, drawstyle='steps')\n        ax.cla()\n        ax.add_line(line)\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        ax.draw()\n        assert line._drawstyle == 'steps-pre'\n        assert line.drawstyle == 'steps'\n        \n        with pytest.raises(KeyError):\n            mlines.Line2D(x, y, drawstyle='invalid')\n        \n        with pytest.raises(KeyError):\n            mlines.Line2D(x, y, drawstyle='steps-invalid')\n        \n        with pytest.raises(KeyError):\n            mlines.Line2D(x, y, drawstyle=123)\n        \n        with pytest.raises(KeyError):\n            mlines.Line2D(x, y, drawstyle='steps-')\n", "def test_line2d_drawstyle():\n    # Create a plot with different drawstyles\n    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n\n    # Drawstyle 'default'\n    line = mlines.Line2D(x, y, drawstyle='default')\n    axs[0, 0].plot(x, y, line)\n    axs[0, 0].set_title('Drawstyle \"default\"')\n\n    # Drawstyle 'steps-mid'\n    line = mlines.Line2D(x, y, drawstyle='steps-mid')\n    axs[0, 1].plot(x, y, line)\n    axs[0, 1].set_title('Drawstyle \"steps-mid\"')\n\n    # Drawstyle 'steps-pre'\n    line = mlines.Line2D(x, y, drawstyle='steps-pre')\n    axs[1, 0].plot(x, y, line)\n    axs[1, 0].set_title('Drawstyle \"steps-pre\"')\n\n    # Drawstyle 'steps-post'\n    line = mlines.Line2D(x, y, drawstyle='steps-post')\n    axs[1, 1].plot(x, y, line)\n    axs[1, 1].set_title('Drawstyle \"steps-post\"')\n\n    # Check that all drawstyles are drawn correctly\n    for ax in axs.flat:\n        assert ax.get_legend_handles_labels() == [('Line2D',)]\n\n    # Check that all axes have the correct drawstyle\n    for ax, style in zip(axs.flat, ['default', 'steps-mid', 'steps-pre', 'steps-post']):\n        assert ax.get_lines()[0].get_drawstyle() == style\n\n    plt.show()\n", "def test_set_line_styles():\n    line = mlines.Line2D([0, 1], [0, 1])\n    line.set_linestyle('-')\n    assert line.get_linestyle() == '-'\n    line.set_linestyle('dashed')\n    assert line.get_linestyle() == 'dashed'\n    line.set_linestyle((0, [1, 1]))\n    assert line.get_linestyle() == (0, [1, 1])\n    line.set_linestyle('None')\n    assert line.get_linestyle() == 'None'\n    line.set_linestyle('')\n    assert line.get_linestyle() == ''\n", "def test_Line2D_update_from_line2D():\n    line1 = mlines.Line2D([0, 1], [0, 1])\n    line2 = mlines.Line2D([0, 1], [0, 1])\n\n    # Check that all attributes are copied correctly\n    line1.update_from(line2)\n    assert_array_equal(line1.get_xdata(), [0, 1])\n    assert_array_equal(line1.get_ydata(), [0, 1])\n    assert line1.get_linestyle() == line2.get_linestyle()\n    assert line1.get_linewidth() == line2.get_linewidth()\n    assert line1.get_color() == line2.get_color()\n    assert line1.get_drawstyle() == line2.get_drawstyle()\n    assert line1.get_marker() == line2.get_marker()\n    assert line1.get_markeredgecolor() == line2.get_markeredgecolor()\n    assert line1.get_markeredgewidth() == line2.get_markeredgewidth()\n    assert line1.get_markerfacecolor() == line2.get_markerfacecolor()\n    assert line1.get_markerfacecoloralt() == line2.get_markerfacecoloralt()\n    assert line1.get_markersize() == line2.get_markersize()\n\n    # Check that attributes that are None in the source are not changed in the copy\n    line2._linestyle = None\n    line2._color = None\n    line1.update_from(line2)\n    assert line1.get_linestyle() == line2.get_linestyle()\n    assert line1.get_linestyle() is not None\n    assert line1.get_color() == line2.get_color()\n    assert line1.get_color() is not None\n\n    # Check that attributes that are not present in the source are not added to the copy\n    line2._foo = 'bar'\n    line1.update_from(line2)\n    assert not hasattr(line1, 'foo')\n\n    # Check that a Line2D with markevery is copied correctly\n    line2 = mlines.Line2D([0, 1, 2], [0, 1, 2], markevery=(1, 2))\n    line1.update_from(line2)\n    assert_array_equal(line1.get_xdata(), [0, 1, 2])\n    assert_array_equal(line1.get_ydata(), [0, 1, 2])\n    assert line1.get_markevery() ==", "def test_mark_every():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    line, = ax.plot(x, y, picker=5, markevery=2)\n    selector = VertexSelector(line)\n    plt.show()\n\n        return selector.process_selected(selector.ind, line.get_data()[0][selector.ind], line.get_data()[1][selector.ind])\n\n    assert test_func(line) == []\n\n    # Test various markevery inputs\n    for every in [1, (0, 2), slice(1, 100, 2), [True, False, True], 0.1, (0.5, 0.1)]:\n        line, = ax.plot(x, y, picker=5, markevery=every)\n        assert len(selector.ind) == 0\n        selector.ind = set()\n        line.figure.canvas.draw_idle()\n        selector.onpick(line.findobj(event=matplotlib.backend_bases.MouseEvent(5, 5, 5, 5, 5, None, None, None, None, None, None, None, None, None, None))\n        assert len(selector.ind) == 0\n        assert len(test_func(line)) == 0\n\n    # Test that float arguments work as expected\n    line, = ax.plot(x, y, picker=5, markevery=0.1)\n    assert len(selector.ind) == 0\n    selector.ind = set()\n    line.figure.canvas.draw_idle()\n    selector.onpick(line.findobj(event=matplotlib.backend_bases.MouseEvent(5, 5, 5, 5, 5, None, None, None, None, None, None, None, None, None, None))\n    assert len(selector.ind) == 2\n    assert len(test_func(line)) == 2\n\n    # Test that float arguments with an offset work as expected\n    line, = ax.plot(x, y, picker=5, markevery=(0.5, 0.1))\n    assert len(selector.ind) == 0\n    selector.ind = set()\n    line.figure.canvas.draw_idle()\n    selector.onpick(line.findobj(event=matplotlib.backend_bases.MouseEvent(5, 5, 5, 5, 5, None, None, None, None, None, None, None, None, None, None))\n    assert"], "sample_798": ["def test_ridge_sparse_cg_nans_in_X_and_y(solver):\n    X = sp.csr_matrix([[1., 2., np.nan], [3., 4., np.nan]])\n    y = np.array([5., 6., np.nan])\n    alpha = 0.1\n\n    try:\n        ridge_regression(X, y, alpha, solver=solver)\n    except ValueError as e:\n        assert \"invalid value encountered in scalar multiplier\" in str(e)\n", "def test_ridge_solver_error_message(solver):\n    X = np.array([[1., 2.], [3., 4.]])\n    y = np.array([0, 1])\n    model = Ridge(solver=solver)\n    with pytest.raises(ValueError, match=\"Known solvers are 'sparse_cg', 'cholesky', 'svd'\"\n                                      \"'lsqr', 'sag' or 'saga'. Got %s.\" % solver):\n        model.fit(X, y)\n", "def test_ridge_sparse(X_diabetes, y_diabetes, solver):\n    X_sparse = sp.csr_matrix(X_diabetes)\n    model = Ridge(solver=solver, fit_intercept=False)\n    model.fit(X_sparse, y_diabetes)\n    y_pred = model.predict(X_sparse)\n    assert_greater(mean_squared_error(y_diabetes, y_pred), 0)\n", "def test_ridge_regressor_sparse_input():\n    from sklearn.linear_model import Ridge\n    X = np.random.rand(100, 10)\n    y = np.random.rand(100)\n    X_sparse = sp.csr_matrix(X)\n\n    estimator = Ridge()\n    estimator.fit(X_sparse, y)\n\n    assert hasattr(estimator, 'coef_')\n    assert hasattr(estimator, 'intercept_')\n\n    estimator_sparse = Ridge()\n    estimator_sparse.fit(X_sparse, y)\n    assert hasattr(estimator_sparse, 'coef_')\n    assert hasattr(estimator_sparse, 'intercept_')\n\n    # Check that the same solution is obtained with both the dense and sparse inputs\n    # for the fit method.\n    assert_array_almost_equal(estimator.coef_, estimator_sparse.coef_)\n    assert_almost_equal(estimator.intercept_, estimator_sparse.intercept_)\n\n    # Check that the coefficients are not identical when X is sparse and X is dense.\n    # This could happen if the sparse matrix is converted to dense internally.\n    assert not np.array_equal(estimator.coef_, estimator_sparse.coef_)\n", "def test_ridge_solver(solver):\n    \"\"\"Test for solver and output shape with multivariate y.\"\"\"\n    # Regression setup\n    X, y = make_regression(n_samples=20, n_features=3, n_targets=4, random_state=0)\n\n    # Solve for various solvers\n    for alpha in [0.1, 1.0, 10.0]:\n        model = Ridge(alpha=alpha, solver=solver)\n        model.fit(X, y)\n        assert model.coef_.shape == (4, 3)\n\n    # Test with svd solver and check that the coefficients are consistent\n    # with the ridge regression formula\n    model = Ridge(alpha=0.1, solver='svd')\n    model.fit(X, y)\n    U, s, Vt = linalg.svd(X, full_matrices=False)\n    for i in range(4):\n        expected_coef = np.dot(Vt.T, np.dot(s[i]**2 / (s[i]**2 + 0.1), U[:, i]))\n        assert_array_almost_equal(model.coef_[i], expected_coef)\n"], "sample_585": ["def test_groupby_first():\n    ds = xr.Dataset({'x': ('time', np.arange(10))})\n    ds['time'] = ('time', pd.date_range('2022-01-01', periods=10))\n    ds_group = ds.groupby('time').first()\n    assert_identical(ds_group, ds[['x']].isel(time=[0]))\n", "def test_unique_value_groups():\n    # test that unique_value_groups returns the correct values and indices\n    ar = np.array([1, 1, 2, 2, 3, 3, 3])\n    values, groups = unique_value_groups(ar)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[0, 1], [2, 3, 4], [5, 6]]\n\n    # test that unique_value_groups with sort=False returns the correct values and indices\n    ar = np.array([3, 2, 1, 1, 3, 2, 3])\n    values, groups = unique_value_groups(ar, sort=False)\n    assert np.array_equal(values, np.array([3, 2, 1]))\n    assert groups == [[5, 6], [0, 1, 4], [2, 3]]\n\n    # test that unique_value_groups with sort=True returns the correct values and indices\n    ar = np.array([3, 2, 1, 1, 3, 2, 3])\n    values, groups = unique_value_groups(ar, sort=True)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[2, 3], [0, 4], [1, 5, 6]]\n\n    # test that unique_value_groups with an array-like of length 0 returns the correct values and indices\n    ar = np.array([])\n    values, groups = unique_value_groups(ar)\n    assert np.array_equal(values, np.array([]))\n    assert groups == [[]]\n\n    # test that unique_value_groups with an array-like of length 1 returns the correct values and indices\n    ar = np.array([1])\n    values, groups = unique_value_groups(ar)\n    assert np.array_equal(values, np.array([1]))\n    assert groups == [[0]]\n", "def test_groupby_unique_value_groups():\n    # Test that GroupBy._unique_value_groups returns the correct unique values and indices\n    arr = np.array([1, 1, 2, 2, 3])\n    values, groups = GroupBy._unique_value_groups(arr, sort=True)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[0, 1], [2, 3], [4]]\n\n    arr = np.array([1, 1, 2, 2, 3])\n    values, groups = GroupBy._unique_value_groups(arr, sort=False)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[0, 1], [2, 3], [4]]\n\n    arr = np.array([1, np.nan, 2, 2, 3])\n    values, groups = GroupBy._unique_value_groups(arr, sort=True)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[0], [1], [2, 3, 4]]\n\n    arr = np.array([1, np.nan, 2, 2, 3])\n    values, groups = GroupBy._unique_value_groups(arr, sort=False)\n    assert np.array_equal(values, np.array([1, 2, 3]))\n    assert groups == [[0], [1], [2, 3, 4]]\n", "def test_groupby_bin():\n    \"\"\"Test GroupBy binning with different numbers of bins.\"\"\"\n    # Create a sample dataset\n    dates = pd.date_range('2020-01-01', '2020-01-31')\n    da = xr.DataArray(np.random.randn(len(dates)), dims=['time'], coords={'time': dates})\n\n    # Perform groupby and binning\n    binned = da.groupby('time').bin([10, 20])\n\n    # Check that the result has the correct shape and coordinates\n    assert binned.shape == (3,)\n    assert set(binned.coords.keys()) == {'time', 'time_bins'}\n\n    # Check that the result is a DataArray\n    assert isinstance(binned, xr.DataArray)\n", "def test_groupby_empty_dataset():\n    ds = xr.Dataset()\n    with pytest.raises(ValueError):\n        ds.groupby('not_a_dim')\n"], "sample_87": ["    def test_should_stop(self):\n        reloader = autoreload.StatReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def import_and_cleanup(self, name):\n        import_module(name)\n        self.addCleanup(lambda: sys.path_importer_cache.clear())\n        self.addCleanup(lambda: sys.modules.pop(name, None))\n", "    def setUp(self):\n        self.temp_dir = self.temporary_directory()\n        self.addCleanup(self.temp_dir.cleanup)\n", "    def test_tick_with_stop_event(self):\n        reloader = autoreload.StatReloader()\n        reloader._stop_condition.set()\n        ticker = reloader.tick()\n        self.assertFalse(next(ticker))\n        self.assertTrue(reloader.should_stop)\n", "    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.tempdir)\n"], "sample_794": ["def test_ridge_solver(solver, fit_intercept):\n    X, y = make_regression(n_samples=10, n_features=10, random_state=0)\n    ridge = Ridge(solver=solver, fit_intercept=fit_intercept)\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (X.shape[1],)\n    if fit_intercept:\n        assert ridge.intercept_.size == 1\n    else:\n        assert ridge.intercept_ is None\n\n    assert ridge.n_iter_ is None\n", "def test_ridge_solver_auto(solver):\n    \"\"\"Test solver='auto' with and without sparse data\"\"\"\n    rng = np.random.RandomState(42)\n    X = rng.rand(10, 10)\n    y = np.ones(10)\n    r = Ridge(solver=solver)\n    r.fit(X, y)\n    assert r.solver == 'cholesky'\n    \n    X_sparse = sp.csr_matrix(X)\n    r = Ridge(solver=solver)\n    r.fit(X_sparse, y)\n    assert r.solver == 'sparse_cg'\n", "def test_ridge_regression_max_iter():\n    X, y = make_regression(n_samples=100, n_features=10, n_informative=5)\n    ridge = Ridge(max_iter=5, solver='sag', tol=1e-3, random_state=0)\n    ridge.fit(X, y)\n    assert ridge.n_iter_ == [5] * y.shape[1]\n", "def test_ridge_ridge_regression_sparse(fit_intercept, solver):\n    rng = np.random.RandomState(0)\n    X = rng.rand(20, 10)\n    y = rng.rand(20)\n    alpha = 1.0\n    if solver == \"cholesky\":\n        assert_raises_regex(np.linalg.LinAlgError, \"Singular matrix\", ridge_regression,\n                           sp.csr_matrix(X), y, alpha, solver=solver)\n    else:\n        ridge_regression(X, y, alpha, solver=solver, fit_intercept=fit_intercept)\n\n", "def test_Ridge_cv_results_detailed(solver):\n    \"\"\"Test the details of the cross-validation results in RidgeCV\"\"\"\n    X, y = make_regression(n_samples=100, n_features=5, n_informative=5,\n                           random_state=42)\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    param_grid = {'alpha': [0.1, 1.0, 10.0]}\n    gcv_ridge = RidgeCV(solver=solver, cv=cv, scoring='neg_mean_squared_error')\n    gcv_ridge.fit(X, y)\n    expected_keys = ['mean_fit_time', 'mean_score_time', 'param_alpha',\n                     'param_solver', 'params', 'score_time', 'split0_test_score',\n                     'split1_test_score', 'split2_test_score',\n                     'split3_test_score', 'split4_test_score',\n                     'std_fit_time', 'std_score_time', 'train_score',\n                     'val_scores_time', 'val_times', 'params']\n    assert sorted(gcv_ridge.cv_results_.keys()) == sorted(expected_keys)\n    assert gcv_ridge.cv_results_['param_solver'] == [solver]\n\n    expected_result = ['neg_mean_squared_error']\n    assert sorted(gcv_ridge.scoring_func_names_) == sorted(expected_result)\n\n    expected_mean_scores = np.array([-0.0001, -0.0004, -0.0007])\n    assert_allclose(gcv_ridge.cv_results_['mean_test_score'], expected_mean_scores)\n    expected_std_scores = np.array([0.0002, 0.0005, 0.0008])\n    assert_allclose(gcv_ridge.cv_results_['std_test_score'], expected_std_scores)\n    expected_train_scores = np.array([-0.0001, -0.0004, -0.0007])\n    assert_allclose(gcv_ridge.cv_results_['mean_train_score'], expected_train_scores)\n"], "sample_1189": ["def test_lambdastr_with_tuple_and_args():\n    from sympy.abc import x, y, z\n    expr = x**2 + y**2 + z**2\n    args = (x, (y, z))\n    expected = 'lambda _0,_1: (lambda x,y,z: (x**2 + y**2 + z**2))(_0,_1[0],_1[1])'\n    assert lambdastr(args, expr) == expected\n    assert lambdastr(args, expr, printer=NumPyPrinter) == expected\n    assert lambdastr(args, expr, printer=LambdaPrinter) == expected\n", "def test_lambdify_list_of_strings():\n    from sympy.abc import x, y, z\n    expr = x + y + z\n    f = lambdify(['x', 'y', 'z'], expr)\n    assert f(1, 2, 3) == 6\n", "def test_lambdify_literal_expr():\n    from sympy import sin\n\n    expr = sin(1)\n    assert _imp_namespace(expr) == {}\n", "def test_lambdify_max():\n    from sympy import lambdify\n    from sympy.abc import x, y\n    from sympy import Max, S\n    Max = Max(x, y)\n    Max_np = lambdify(x, Max, 'numpy')\n    Max_tf = lambdify(x, Max, 'tensorflow')\n    assert isinstance(Max_np(1, 2), numpy.ndarray)\n    assert isinstance(Max_tf(1, 2), tensorflow.Tensor)\n", "def test_implemented_function_scalar_vector():\n    from sympy.abc import x\n    from sympy.utilities.lambdify import implemented_function\n    from sympy import lambdify, sin, cos\n\n    # Define a scalar function\n    f = implemented_function('f', lambda x: x+1)\n\n    # Test that it works with scalar input\n    func = lambdify(x, f(x))\n    assert func(4) == 5\n\n    # Define a vector function\n    f_vec = implemented_function('f', lambda v: v + [1])\n\n    # Test that it works with vector input\n    func_vec = lambdify(x, f_vec(x))\n    assert func_vec([1, 2, 3]) == [2, 3, 4]\n\n    # Test that it works with scalar and vector input\n    f_scal_vec = implemented_function('f', lambda v: v + [1] if isinstance(v, list) else v+1)\n    func_scal_vec = lambdify(x, f_scal_vec(x))\n    assert func_scal_vec([1, 2, 3]) == [2, 3, 4]\n    assert func_scal_vec(4) == 5\n"], "sample_951": ["def test_getglobals():\n        return x + y\n\n    obj = add\n    assert getglobals(obj) == {'x': None, 'y': None}\n", "def test_getglobals():\n    class TestClass:\n            self.__globals__ = {'key': 'value'}\n\n        pass\n\n    class TestClass2:\n            self.__globals__ = None\n\n    assert inspect.getglobals(TestClass()) == {'key': 'value'}\n    assert inspect.getglobals(test_function) is None\n    assert inspect.getglobals(TestClass2()) is None\n", "def test_type_alias_module():\n    \"\"\"Test TypeAliasModule.\"\"\"\n    mapping = {\n        'mod1': 'mod1',\n        'mod2.mod3': 'mod2.mod3',\n        'mod4.mod5.mod6': 'mod4.mod5.mod6',\n    }\n\n    mod = TypeAliasModule('mod1', mapping)\n\n    assert isinstance(mod, TypeAliasModule)\n    assert hasattr(mod, '__modname')\n    assert mod.__modname == 'mod1'\n    assert isinstance(mod.__mapping, dict)\n    assert mod.__mapping == mapping\n\n    assert mod.__getattr__('mod2') == TypeAliasModule('mod2', {\n        'mod3': 'mod3',\n    })\n\n    assert mod.__getattr__('mod4.mod5') == TypeAliasModule('mod4.mod5', {\n        'mod6': 'mod6',\n    })\n\n    with pytest.raises(KeyError):\n        mod.__getattr__('mod7')\n\n    assert mod.__getattr__('mod1') == TypeAliasForwardRef('mod1')\n\n    # Test with valid module\n    assert mod.__getattr__('mod1') == mod\n    assert mod.__getattr__('mod1') == mod.__getattr__('mod1')\n\n    # Test with invalid module\n    with pytest.raises(KeyError):\n        importlib.import_module('mod1')\n", "def test_getglobals():\n    class Foo:\n        pass\n\n    foo = Foo()\n\n    assert inspect.getglobals(foo) == {'__builtins__': __builtins__, '__name__': '__main__',\n                                      '__doc__': None, '__package__': None,\n                                      '__loader__': <_frozen_importlib_external.FrozenImporter object at 0x...>,\n                                      '__spec__': ModuleSpec(name='__main__', loader=<_frozen_importlib_external.FrozenImporter object at 0x...>,\n                                                            origin='/path/to/__main__.py', submodule_search_locations=[], has_location=True,\n                                                            is_package=False, is_namespace_package=False),\n                                      '__annotations__': {}, '__file__': '/path/to/__main__.py',\n                                      '__cached__': 'build/__pycache__/__main__.cpython-39.pyc',\n                                      '__package__': None, '__path__': None, '__name__': '__main__',\n                                      '__doc__': None, '__loader__': <_frozen_importlib_external.FrozenImporter object at 0x...>,\n                                      '__spec__': ModuleSpec(name='__main__', loader=<_frozen_importlib_external.FrozenImporter object at 0x...>,\n                                                            origin='/path/to/__main__.py', submodule_search_locations=[], has_location=True,\n                                                            is_package=False, is_namespace_package=False), '__path__': None,\n                                      '__builtins__': <module 'builtins' (built-in)>}\n", "def test_getmro():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert inspect.getmro(C) == (C, B, A, object)\n\n    class D:\n        pass\n\n    assert inspect.getmro(D) == (D, object)\n"], "sample_346": ["    def test_middleware_process_request(self):\n        \"\"\"Test the middleware process_request method.\"\"\"\n        class MyMiddleware:\n                return HttpResponse('<html><body>processed</body></html>')\n\n        middleware_decorator = decorator_from_middleware(MyMiddleware)\n        decorated_view = middleware_decorator(fully_decorated)\n        request = HttpRequest()\n        response = decorated_view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, '<html><body>processed</body></html>')\n", "    def test_decorator_from_middleware_with_args(self):\n        # Test that decorator_from_middleware_with_args correctly decorates a view\n        cache_page = decorator_from_middleware_with_args(CacheMiddleware)\n        @cache_page(3600)\n            return HttpResponse('Hello, world!')\n        self.assertEqual(my_view.__wrapped__.__name__, 'my_view')\n        self.assertEqual(my_view.__wrapped__.__doc__, my_view.__doc__)\n        self.assertEqual(my_view.__wrapped__.__defaults__, my_view.__defaults__)\n        self.assertEqual(my_view.__wrapped__.__annotations__, my_view.__annotations__)\n", "    def test_method_decorator_multiple_decorator_order(self):\n        class TestClass:\n            @method_decorator(decorator1, 'method')\n            @method_decorator(decorator2, 'method')\n                pass\n\n        TestClass.method = classonlymethod(TestClass.method)\n        self.assertTrue(decorator2(decorator1(TestClass.method)))\n", "def test_middleware_decorator(self):\n    class MyMiddleware:\n            return HttpResponse('Middleware processed request')\n\n            return HttpResponse('Middleware processed view')\n\n    @decorator_from_middleware_with_args(MyMiddleware)\n        return HttpResponse('View response')\n\n    response = my_view(HttpRequest())\n    self.assertEqual(response.content, b'Middleware processed requestMiddleware processed view')\n\n    # Test that middleware's process_response is called when response has a render method\n    class MyMiddlewareWithResponseProcessing:\n            return HttpResponse('Middleware processed response')\n\n    @decorator_from_middleware_with_args(MyMiddlewareWithResponseProcessing)\n        response = HttpResponse('<html><body>dummy</body></html>')\n        return response\n\n    response = my_view_with_rendered_response(HttpRequest())\n    self.assertEqual(response.content, b'Middleware processed response')\n\n    # Test that middleware's process_response is called when response is not rendered\n    class MyMiddlewareWithUnrenderedResponse:\n            return HttpResponse('Middleware processed response')\n\n    @decorator_from_middleware_with_args(MyMiddlewareWithUnrenderedResponse)\n        return HttpResponse()\n\n    response = my_view_with_unrendered_response(HttpRequest())\n    self.assertEqual(response.content, b'Middleware processed response')\n\n    # Test that decorator_from_middleware raises an error when middleware does not have process_request\n    with self.assertRaises(AttributeError):\n        @decorator_from_middleware_with_args(object)\n            return HttpResponse('View response')\n\n    # Test that decorator_from_middleware raises an error when middleware does not have process_view\n    with self.assertRaises(AttributeError):\n        @decorator_from_middleware_with_args(MyMiddleware)\n            return HttpResponse('View response')\n\n    # Test that decorator_from_middleware raises an error when middleware has both process_request and process_view\n    with self.assertRaises(ValueError):\n        class MyMiddlewareWithBothProcessRequestAndView:\n                return HttpResponse('Middleware processed request')\n\n                return HttpResponse('Middleware processed view')\n\n        @decorator_from_middleware_with_args(MyMiddlewareWithBothProcessRequestAndView", "    def test_decorator_from_middleware(self):\n        # Test decorator_from_middleware_with_args\n        class TestMiddleware:\n                self.view_func = view_func\n\n                return HttpResponse('Processed request')\n\n                return HttpResponse('Processed view')\n\n            middleware = TestMiddleware(view_func)\n            return self._make_middleware_decorator(middleware, 'process_request')\n\n        @middleware_decorator_with_args\n            return HttpResponse('Test view')\n\n        self.assertEqual(test_view(HttpRequest()).content.decode(), 'Processed request')\n\n        # Test decorator_from_middleware\n        @decorator_from_middleware(TestMiddleware)\n            return HttpResponse('Test view 2')\n\n        self.assertEqual(test_view_2(HttpRequest()).content.decode(), 'Processed request')\n"], "sample_209": ["    def test_save_with_force_update_and_force_insert(self):\n        with self.assertRaises(ValueError):\n            Model1.objects.create().save(force_insert=True, force_update=True)\n", "    def test_save_base_insert(self):\n        # Test that save_base() works correctly with force_insert=True\n        # and force_update=False.\n        model1 = Model1(a=1)\n        model1.save(force_insert=True, force_update=False)\n        self.assertEqual(model1.pk, 1)\n", "    def test_save_with_deferred_fields(self):\n        article = Article.objects.create(title='Test', published=False)\n        article.refresh_from_db()\n        self.assertFalse(article.published)\n\n        # Test that save doesn't set the deferred field to its default value\n        article.published = True\n        article.save()\n        article.refresh_from_db()\n        self.assertTrue(article.published)\n", "    def test_save_base_empty_fields(self):\n        \"\"\"Test that save_base handles empty fields correctly.\"\"\"\n        with self.assertRaises(DatabaseError):\n            Model().save_base(force_insert=True)\n", "    def test_save_base(self):\n        # Test that save_base doesn't attempt to save parent models\n        # when force_insert=True\n        article = Article.objects.create(title='Article 1')\n        self.assertEqual(article.pk, 1)\n\n        # Test that save_base sets the parent's PK value correctly\n        article.save_base(force_insert=True)\n        self.assertEqual(article.pk, 1)\n\n        # Test that save_base correctly handles partial saving\n        # when force_insert=False and force_update=False\n        article.save_base(force_insert=False, force_update=False)\n\n        # Test that save_base correctly handles a parent model that\n        # is swapped\n        with self.assertRaises(TypeError):\n            article.save_base(force_insert=True, force_update=False)\n"], "sample_463": ["    def test_renamed_fields_deconstructible_default(self):\n        from_state = self.make_project_state([\n            self.author_name_deconstructible_1.clone(),\n            self.author_name_deconstructible_2.clone(),\n            self.author_name_deconstructible_3.clone(),\n            self.author_name_deconstructible_4.clone(),\n            self.author_name_deconstructible_list_1.clone(),\n            self.author_name_deconstructible_list_2.clone(),\n            self.author_name_deconstructible_list_3.clone(),\n            self.author_name_deconstructible_tuple_1.clone(),\n            self.author_name_deconstructible_tuple_2.clone(),\n            self.author_name_deconstructible_tuple_3.clone(),\n            self.author_name_deconstructible_dict_1.clone(),\n            self.author_name_deconstructible_dict_2.clone(),\n            self.author_name_deconstructible_dict_3.clone(),\n            self.author_name_nested_deconstructible_1.clone(),\n            self.author_name_nested_deconstructible_2.clone(),\n            self.author_name_nested_deconstructible_changed_arg.clone(),\n            self.author_name_nested_deconstructible_extra_arg.clone(),\n            self.author_name_nested_deconstructible_changed_kwarg.clone(),\n            self.author_name_nested_deconstructible_extra_kwarg.clone(),\n        ])\n        to_state = self.make_project_state([\n            self.author_name.clone(),\n        ])\n        changes = self.get_changes(from_state, to_state)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n", "    def test_renamed_field_with_db_column_change(self):\n        before_states = [\n            self.author_custom_pk,\n            self.author_renamed_with_book,\n            self.book_with_field_and_author_renamed,\n        ]\n        after_states = [\n            self.author_custom_pk,\n            self.author_renamed_with_book,\n            self.book_with_writer_and_title,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 3)\n        self.assertNumberMigrations(changes, \"otherapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [(\"testapp\", \"0001_initial\")])\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [(\"otherapp\", \"0001_initial\")])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 1, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"testapp\", 2, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 1, [\"RenameField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"id\", new_name=\"pk_field\"\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 0, old_name=\"author\", new_name=\"writer\"\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 1, 0, old_name=\"title\", new_name=\"title\"\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, db_column=\"pk_field\"\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"otherapp\", 0, 0, db_column=\"writer\"\n        )\n", "    def test_rename_field_index_together(self):\n        # Tests that an index together is properly renamed when the fields in it change\n        from_state = self.make_project_state([\n            self.author_name,\n            self.book,\n        ])\n        to_state = self.make_project_state([\n            self.author_name_longer,\n            self.book,\n        ])\n        changes = self.get_changes(from_state, to_state)\n\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")])\n        self.assertMigrationDependencies(changes, \"otherapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")])\n\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RenameIndex\"])\n\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"name\", new_name=\"name\")\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book_title_author_idx\")\n\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, name=\"name\", type=\"TextField\", max_length=400)\n        self.assertOperationFieldAttributes(changes, \"otherapp\", 0, 0, name=\"book_title_author_idx\", fields=[\"author\", \"title\"])\n", "    def test_autodetector_default_field_alteration(self):\n        changes = self.get_changes(\n            [self.author_empty, self.author_empty],\n            [self.author_name_default_before, self.author_name_default],\n            questioner=MigrationQuestioner(self.author_empty.models),\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"Author\",\n            field__default=\"Ada Lovelace\",\n            field__max_length=200,\n        )\n", "    def test_autodetector_handles_model_dependency_on_foreign_key(self):\n        self.make_project_state(\n            [\n                self.author_with_book,\n                self.book,\n                self.contract,\n                self.contract_renamed,\n                self.publisher_with_author,\n                self.publisher_with_aardvark_author,\n                self.publisher_with_book,\n            ]\n        )\n        changes = self.get_changes(\n            [\n                self.author_with_book,\n                self.book,\n            ],\n            [\n                self.author_with_book_order_wrt,\n                self.book,\n                self.contract_renamed,\n                self.publisher_with_author,\n                self.publisher_with_book,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\", \"RenameModel\"])\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\", \"RenameModel\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"Author\", db_table=\"author_two\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"Author\", db_table=\"author_two\"\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 0, name=\"Book\", db_table=\"book_two\"\n        )\n        self.assertOperationAttributes(\n            changes, \"otherapp\", 0, 1, name=\"Book\", db_table=\"book_two\"\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            1,\n            0,\n            name=\"Deal\",\n            db_table=\"publisher_two\",\n        )\n        self.assertOperationAttributes(\n            changes,\n            \"otherapp\",\n            1,\n            1,\n            name=\"Deal\",\n            db_table=\"publisher_two\",\n        )\n        self.assertMigrationDependencies(\n            changes, \"testapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\")]\n        )\n        self.assertMigrationDependencies(changes, \"testapp\", 0, [(\"testapp\", \"AuthorProxy\")])\n        self.assertMigrationDependencies(changes, \"testapp\", 1, [(\"testapp\", \"AuthorProxy\")])\n"], "sample_801": ["    def test_bicluster_mixin(self):\n        class BiclusterEstimator(BaseEstimator, BiclusterMixin):\n                self.rows_ = np.array([0, 1])\n                self.columns_ = np.array([0, 1])\n                return self\n\n        estimator = BiclusterEstimator()\n        bicluster = estimator.get_biclusters_()\n        self.assertEqual(bicluster, (np.array([0, 1]), np.array([0, 1])))\n", "def test_ClusterMixin_fit_predict():\n    # Test that ClusterMixin's fit_predict method raises a NotImplementedError\n    from sklearn.cluster import KMeans\n    estimator = KMeans(n_clusters=2)\n    X = np.array([[1, 2], [1, 4], [1, 5], [10, 2], [10, 4], [10, 5]])\n    with pytest.raises(NotImplementedError):\n        estimator.fit_predict(X)\n", "    def test_clone(self):\n        est = LogisticRegression()\n        cloned_est = clone(est)\n        self.assertNotEqual(id(est), id(cloned_est))\n\n        # Check that cloned estimator has the same parameters\n        params = est.get_params()\n        cloned_params = cloned_est.get_params()\n        self.assertEqual(params, cloned_params)\n\n        # Check that cloned estimator has different objects as parameters\n        self.assertNotEqual(id(params['penalty']), id(cloned_params['penalty']))\n", "def test_ClusterMixin_fit_predict():\n    # Test that fit_predict works with ClusterMixin\n    from sklearn.cluster import KMeans\n    estimator = KMeans(n_clusters=2, random_state=0)\n    X = np.array([[1, 2], [1, 4], [1, 5], [10, 2], [10, 4], [10, 5]])\n    labels = estimator.fit_predict(X)\n    assert labels.size == 6\n    assert set(labels) == {0, 1}\n", "    def __init__(self, threshold=0.0):\n        self.threshold = threshold\n"], "sample_1091": ["def test_Relational__negated():\n    from sympy import Symbol, sin, cos\n    x = Symbol('x')\n    eq = Eq(sin(x), cos(x))\n    assert eq.negated == Ne(sin(x), cos(x))\n    assert (eq.reversedsign).negated == Ne(cos(x), -sin(x))\n    assert (eq.negated).reversedsign == Eq(cos(x), -sin(x))\n", "def test_relational_negated(self):\n    x, y, z = symbols('x y z')\n    a = Equality(x + 1, x)\n    b = a.negated\n    assert b == x + 1 > x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x, x + 1)\n    b = a.negated\n    assert b == x < x + 1\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x, x)\n    b = a.negated\n    assert b == x != x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x, x + 1)\n    b = a.reversedsign\n    assert b == -x + 1 >= -x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x + 1, x)\n    b = a.reversedsign\n    assert b == -x + 1 <= -x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x, x + 1)\n    b = a.reversed\n    assert b == x + 1 >= x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x, x + 1)\n    b = a.reversed\n    assert b == x + 1 >= x\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x + 1, x)\n    b = a.reversed\n    assert b == x <= x + 1\n    assert b.lhs.is_number and b.rhs.is_number\n    assert rel_check(b.lhs, b.rhs)\n    a = Equality(x + 1, x)\n    b = a.reversed\n    assert b == x <= x + 1\n    assert b.lhs.is_number and b.rhs", "def test_Equality_and_Relational_interface():\n    from sympy.core.relational import Relational\n    from sympy.abc import x, y\n\n    # Test that Equality can be used with Relational interface\n    eq = Equality(x, y)\n    assert eq.is_Equality\n    assert eq.is_Relational\n    assert eq == Eq(x, y)\n\n    # Test that Relational can be used with Relational interface\n    rel = Rel(x, y)\n    assert rel.is_Relational\n    assert rel == Rel(x, y)\n\n    # Test that Equality and Relational have the same interface\n    # except for their is_Equality and is_Relational properties\n    assert Relational(x, y).lhs == eq.lhs\n    assert Relational(x, y).rhs == eq.rhs\n    assert Relational(x, y).reversed == eq.reversed\n    assert Relational(x, y).reversedsign == eq.reversedsign\n    assert Relational(x, y).negated == eq.negated\n    assert Relational(x, y)._eval_evalf(3) == eq._eval_evalf(3)\n    assert Relational(x, y).canonical == eq.canonical\n    assert Relational(x, y).equals(eq, failing_expression=False)\n    assert Relational(x, y)._eval_simplify() == eq._eval_simplify()\n    assert Relational(x, y)._eval_trigsimp() == eq._eval_trigsimp()\n    assert Relational(x, y).expand() == eq.expand()\n    assert Relational(x, y).__nonzero__() == eq.__nonzero__()\n    assert Relational(x, y)._eval_as_set() == eq._eval_as_set()\n    assert Relational(x, y).binary_symbols == eq.binary_symbols\n\n    # Test that Equality and Relational can be used as objects with the same interface\n    assert Relational(x, y).lhs == eq.lhs\n    assert Relational(x, y).rhs == eq.rhs\n    assert Relational(x, y).reversed == eq.reversed\n    assert Relational(x, y).reversedsign == eq.reversedsign\n    assert Relational(x, y).negated == eq.negated\n    assert Relational(x, y)._eval_evalf(3) == eq._eval_evalf(3)\n    assert Relational(x, y).canonical == eq.canonical\n    assert Rel", "def test_Relational_unary_operations():\n    from sympy import symbols, sin, cos\n\n    x = symbols('x')\n    y = symbols('y')\n\n    assert not (x < y).is_number\n    assert (x < y).is_Relational\n\n    assert (x + y < 2*x + y).is_Relational\n    assert (x < y + x).is_Relational\n\n    assert (x + sin(x) < y).is_Relational\n    assert (cos(x) < sin(x + y)).is_Relational\n\n    assert (x**2 + 1 < x**2 + y**2).is_Relational\n    assert (y**2 + 1 < x**2 + y**2).is_Relational\n\n    assert (sin(x) > cos(x)).is_Relational\n    assert (sin(x) < cos(x)).is_Relational\n    assert (sin(x) <= cos(x)).is_Relational\n    assert (sin(x) >= cos(x)).is_Relational\n    assert (sin(x) == cos(x)).is_Relational\n    assert (sin(x) != cos(x)).is_Relational\n\n    # Test the negation property of Relational objects\n    assert not (x < y).negated.is_Equality\n    assert (x < y).negated.is_Unequality\n    assert (x > y).negated.is_Unequality\n    assert (x <= y).negated.is_Unequality\n    assert (x >= y).negated.is_Unequality\n    assert (x == y).negated.is_Unequality\n    assert (x != y).negated.is_Equality\n\n    # Test the reversed property of Relational objects\n    assert (x < y).reversed.is_Relational\n    assert (x > y).reversed.is_Relational\n    assert (x <= y).reversed.is_Relational\n    assert (x >= y).reversed.is_Relational\n    assert (x == y).reversed.is_Relational\n    assert (x != y).reversed.is_Relational\n\n    # Test the reversedsign property of Relational objects\n    assert (x < y).reversedsign.is_Relational\n    assert (x > y).reversedsign.is_Relational\n    assert (x <= y).reversedsign.is_Relational\n    assert (", "def test_relational_operations():\n    x = Symbol('x')\n    assert Gt(x, 2) == Gt(x, 2)\n    assert Gt(x, 2) != Gt(x, 1)\n    assert Gt(x, 2) != Lt(x, 1)\n    assert Le(x, 2) != Lt(x, 2)\n    assert Eq(x, 2) == Eq(x, 2)\n    assert Eq(x, 2) != Eq(x, 1)\n    assert Eq(x, 2) != Lt(x, 1)\n    assert Le(x, 2) != Gt(x, 2)\n    assert Gt(2, x) != Gt(x, 2)\n    assert Le(2, x) != Le(x, 2)\n    assert Eq(2, x) != Eq(x, 2)\n    assert Gt(x, 2) != Gt(x + 1, 3)\n    assert Le(x, 2) == Le(x + 1, 3)\n"], "sample_823": ["def test_pairwise_distances_chunked_generator():\n    X = np.random.rand(10, 10)\n\n        return D_chunk\n\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    D_chunks = []\n    for D in gen:\n        D_chunks.append(D)\n\n    D = pairwise_distances(X)\n    assert_array_almost_equal(D, np.concatenate(D_chunks))\n", "def test_pairwise_distances_dtype(dtype):\n    X = np.array([[1, 2, 3]], dtype=dtype)\n    Y = np.array([[4, 5, 6]], dtype=dtype)\n    distances = pairwise_distances(X, Y, metric='euclidean')\n    assert distances.dtype == dtype\n", "def test_check_pairwise_arrays_precomputed():\n    X = [[1, 2], [3, 4]]\n    Y = None\n    X, Y = check_pairwise_arrays(X, Y, precomputed=True)\n    assert issparse(X)\n    assert Y is None\n", "def test_pairwise_distances_dtype(dtype):\n    X = np.random.rand(5, 10).astype(dtype)\n    Y = np.random.rand(5, 10).astype(dtype)\n    D = pairwise_distances(X, Y)\n    assert D.dtype == dtype\n\n", "def test_pairwise_distances_chunked():\n    # Test that the generator yields the correct number of chunks\n    X = np.random.rand(100, 5)\n    gen = pairwise_distances_chunked(X)\n    assert_equal(len(list(gen)), 10)\n\n    # Test that the generator yields the correct number of rows per chunk\n    gen = pairwise_distances_chunked(X, working_memory=0)\n    assert_equal(len(next(gen)), 1)\n\n    # Test that the generator yields the correct number of rows per chunk\n    # when working_memory is not set\n    gen = pairwise_distances_chunked(X)\n    assert_equal(len(next(gen)), 10)\n\n    # Test that the generator yields the correct distance matrix\n    X = np.array([[0, 1], [1, 1]])\n    gen = pairwise_distances_chunked(X)\n    distance_matrix = np.array([[0, 1], [1, 0]])\n    assert_array_almost_equal(next(gen), distance_matrix)\n\n    # Test that the generator yields the correct distance matrix with a custom metric\n    gen = pairwise_distances_chunked(X, metric='cosine')\n    distance_matrix = np.array([[1, 0], [0, 1]])\n    assert_array_almost_equal(next(gen), distance_matrix)\n\n    # Test that the generator yields the correct distance matrix with a custom metric and a custom working memory\n    gen = pairwise_distances_chunked(X, working_memory=0)\n    distance_matrix = np.array([[1, 0], [0, 1]])\n    assert_array_almost_equal(next(gen), distance_matrix)\n\n    # Test that the generator yields the correct distance matrix when Y is not None\n    X = np.array([[0, 1], [1, 1]])\n    Y = np.array([[1, 1], [0, 1]])\n    gen = pairwise_distances_chunked(X, Y)\n    distance_matrix = np.array([[0, 1], [1, 0]])\n    assert_array_almost_equal(next(gen), distance_matrix)\n\n    # Test that the generator yields the correct distance matrix when Y is not None and a custom metric is used\n    gen = pairwise_distances_chunked(X, Y, metric='cosine')\n    distance_matrix = np.array([[0, 1], [1, 0]])\n    assert_array_almost_equal(next(gen), distance_matrix)\n\n    # Test that the generator yields the correct distance matrix when Y is not None, a custom metric"], "sample_127": ["    def test_empty_queryset_order_by(self):\n        qs = Country.objects.none()\n        with self.assertRaises(TypeError):\n            qs.order_by('name')\n", "    def test_prefetch_related_objects_with_prefetch(self):\n        country = Country(name='USA', iso_two_letter='US')\n        country.save()\n        restaurant = Restaurant(name='Pizzeria', country=country)\n        restaurant.save()\n        restaurant_prefetched = Restaurant.objects.select_related('country').prefetch_related('menu_items').get(id=restaurant.id)\n        self.assertEqual(restaurant_prefetched.country.name, 'USA')\n        self.assertEqual(restaurant_prefetched.menu_items.count(), 0)\n        self.assertEqual(restaurant_prefetched._prefetched_objects_cache, {'country': [country], 'menu_items': []})\n\n        # Add menu items and re-fetch\n        menu_item = restaurant.menu_items.create(name='Margherita')\n        menu_item2 = restaurant.menu_items.create(name='Hawaiian')\n        restaurant_prefetched = Restaurant.objects.select_related('country').prefetch_related('menu_items').get(id=restaurant.id)\n        self.assertEqual(restaurant_prefetched.country.name, 'USA')\n        self.assertEqual(restaurant_prefetched.menu_items.count(), 2)\n        self.assertEqual(restaurant_prefetched._prefetched_objects_cache, {'country': [country], 'menu_items': [menu_item, menu_item2]})\n", "    def test_filter_ordered(self):\n        pizzeria = Restaurant(name=\"Pizza Hut\", address=\"123 Main St\")\n        pizzeria.save()\n        country = Country(name=\"USA\", iso_two_letter=\"US\")\n        country.save()\n        restaurant = Restaurant(name=\"Another Pizza Hut\", country=country)\n        restaurant.save()\n        qs = Restaurant.objects.all().order_by('-name')\n        self.assertEqual(len(qs), 2)\n        self.assertEqual(qs[0].name, \"Another Pizza Hut\")\n        self.assertEqual(qs[1].name, \"Pizza Hut\")\n", "    def test_bulk_create_returned_columns(self):\n        \"\"\"\n        Test bulk_create() with can_return_rows_from_bulk_insert=True returns\n        the correct number of columns.\n        \"\"\"\n        with self.settings(DATABASES['default']['ATOMIC_REQUESTS'] = False,\n                          DATABASES['default']['returning_from_bulk_insert'] = True):\n            with self.assertRaises(NotSupportedError):\n                Restaurant.objects.bulk_create(\n                    [Restaurant(name='pizzeria'), Restaurant(name='pizzeria')],\n                    return_inserted_id=False\n                )\n\n        self.assertEqual(\n            Restaurant.objects.bulk_create(\n                [Restaurant(name='pizzeria'), Restaurant(name='pizzeria')],\n                return_inserted_id=False\n            ).count(),\n            2\n        )\n\n        self.assertEqual(\n            Restaurant.objects.bulk_create(\n                [Restaurant(name='pizzeria'), Restaurant(name='pizzeria')],\n                return_inserted_id=True\n            ).count(),\n            2\n        )\n\n        self.assertEqual(\n            Restaurant.objects.bulk_create(\n                [Restaurant(name='pizzeria'), Restaurant(name='pizzeria')],\n                return_inserted_id=True\n            ).values_list('id', flat=True).count(),\n            2\n        )\n", "    def setUp(self):\n        self.data = [\n            Pizzeria(name=\"Pizza Palace\", country=Country.objects.create(name=\"USA\", iso_two_letter=\"US\")),\n            Pizzeria(name=\"Pizza Hut\", country=Country.objects.create(name=\"Canada\", iso_two_letter=\"CA\")),\n            Pizzeria(name=\"Dominos\", country=Country.objects.create(name=\"Mexico\", iso_two_letter=\"MX\")),\n            Pizzeria(name=\"Papa John's\", country=Country.objects.create(name=\"Brazil\", iso_two_letter=\"BR\"))\n        ]\n"], "sample_911": ["def test_id_operator_v2():\n    ast = parse('operator', 'operator+')\n    assert ast.declaration.get_id(version=2) == 'pl'\n    ast = parse('operator', 'operator-')\n    assert ast.declaration.get_id(version=2) == 'mi'\n    ast = parse('operator', 'operator*')\n    assert ast.declaration.get_id(version=2) == 'ml'\n    ast = parse('operator', 'operator/')\n    assert ast.declaration.get_id(version=2) == 'dv'\n    ast = parse('operator', 'operator%')\n    assert ast.declaration.get_id(version=2) == 'rm'\n    ast = parse('operator', 'operator&')\n    assert ast.declaration.get_id(version=2) == 'an'\n    ast = parse('operator', 'operator^')\n    assert ast.declaration.get_id(version=2) == 'eo'\n    ast = parse('operator', 'operator==')\n    assert ast.declaration.get_id(version=2) == 'eq'\n    ast = parse('operator', 'operator!=')\n    assert ast.declaration.get_id(version=2) == 'ne'\n    ast = parse('operator', 'operator<')\n    assert ast.declaration.get_id(version=2) == 'lt'\n    ast = parse('operator', 'operator>')\n    assert ast.declaration.get_id(version=2) == 'gt'\n    ast = parse('operator', 'operator<=')\n    assert ast.declaration.get_id(version=2) == 'le'\n    ast = parse('operator', 'operator>=')\n    assert ast.declaration.get_id(version=2) == 'ge'\n    ast = parse('operator', 'operator!')\n    assert ast.declaration.get_id(version=2) == 'nt'\n    ast = parse('operator', 'operator&&')\n    assert ast.declaration.get_id(version=2) == 'aa'\n    ast = parse('operator', 'operator||')\n    assert ast.declaration.get_id(version=2) == 'oo'\n    ast = parse('operator', 'operator++')\n    assert ast.declaration.get_id(version=2) == 'pp'\n    ast = parse('operator', 'operator--')\n    assert ast.declaration.get_id(version=2) == 'mm'\n    ast = parse('operator', 'operator,')\n    assert ast.declaration.get_id(version=2) == 'cm'\n    ast = parse('operator', 'operator", "def test_parse_literal():\n    ast = parse(\"enum\", \"1\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"E\"\n    ast = parse(\"enum\", \"0\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"E\"\n    ast = parse(\"enum\", \"-1\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"NE\"\n    ast = parse(\"enum\", \"+1\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"PE\"\n    ast = parse(\"enum\", \"12\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"0x12\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"0X12\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"012\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"0X12l\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DEl\"\n    ast = parse(\"enum\", \"0X12L\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DEl\"\n    ast = parse(\"enum\", \"0b1010\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"0B1010\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DE\"\n    ast = parse(\"enum\", \"0b1010l\")\n    assert isinstance(ast.declaration, ASTEnum)\n    assert ast.declaration.name.name.get_id(1) == \"DEl\"\n    ast = parse(\"enum\",", "def test_simple_fundamental_type():\n    idDict = {1: \"i\", 2: \"i\", 3: \"j\", 4: \"j\"}\n    check('type', 'int', idDict, 'i')\n", "def test_function_type_id():\n    ast = parse('function', \"int foo();\")\n    check('function', \"int foo();\", {'1': 'i', '2': 'Ffi', '3': 'Ff0Ffi'})\n", "def test_parse_template_parameter_list_with_uneven_number_of_template_args_and_params():\n    # Test that the template parameter list can parse a case with an uneven number of template argument lists and parameter lists.\n    input = \"template <int, float> void f();\"\n    expected_output = \"template <int, float> void f();\"\n    check(\"function\", input, {1: \"f\"}, expected_output)\n"], "sample_809": ["def test_estimate_mi_sparse():\n    rng = check_random_state(0)\n    X = csr_matrix(np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]), shape=(3, 3))\n    y = np.array([0, 1, 2])\n    mi = _estimate_mi(X, y, discrete_features=False)\n    expected_mi = np.array([0., 0., 0.])\n    assert_array_almost_equal(mi, expected_mi)\n", "def test_estimate_mi_array_like_input():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([0, 1])\n    discrete_features = np.array([False, True, True])\n    n_neighbors = 3\n    copy = True\n    random_state = check_random_state(0)\n    mi = _estimate_mi(X, y, discrete_features, False, n_neighbors, copy, random_state)\n    assert_array_equal(mi.shape, (3,))\n", "def test_mutual_info_classif_sparse():\n    X = csr_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    y = np.array([0, 0, 1])\n    mi = mutual_info_classif(X, y)\n    assert_array_equal(mi, np.array([1.0, 1.0, 1.0]))\n", "def test_mutual_info_regression_sparse(self):\n    X = csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n    y = np.array([1, 2, 3])\n    assert_raises(ValueError, mutual_info_regression, X, y, discrete_features=True)\n", "def test_mutual_info_regression_sparse_input():\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([1, 2])\n    assert_raises(ValueError, mutual_info_regression, X, y, copy=False)\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([1, 2])\n    mi = mutual_info_regression(X, y, copy=True)\n    assert_equal(mi.shape, (2,))\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    y = np.array([1, 2])\n    mi = mutual_info_regression(X, y, copy=False)\n    assert_equal(mi.shape, (2,))\n"], "sample_292": ["    def test_view_with_csrf_exempt(self):\n        view = non_token_view_using_request_processor\n        request = self._get_GET_no_csrf_cookie_request()\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n", "    def test_csrf_token_for_https_request(self):\n        with override_settings(CSRF_COOKIE_SECURE=True):\n            req = self._get_GET_csrf_cookie_request()\n            req.META['HTTP_X_FORWARDED_PROTO'] = 'https'\n            response = ensure_csrf_cookie_view(req)\n            self._check_token_present(response)\n", "    def test_insecure_origin(self):\n        req = TestingHttpRequest()\n        req.META['HTTP_ORIGIN'] = 'http://www.cross-sitescript.org'\n        req.META['HTTP_REFERER'] = 'http://www.cross-sitescript.org/script'\n        req.is_secure = lambda: False\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(req, 'test_view', (), {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n", "    def test_csrf_cookie_not_set_and_csrf_protected_view(self):\n        # Make sure we get a 403 when a CSRF cookie isn't set and a view requires\n        # a CSRF cookie\n        req = self._get_POST_no_csrf_cookie_request()\n        req.META['HTTP_REFERER'] = 'https://example.com'\n        response = ensure_csrf_cookie_view(req)\n        self.assertContains(response, 'Forbidden (CSRF cookie not set.)')\n        self._check_token_present(response)\n", "    def test_csrf_set_on_response(self):\n        \"\"\"\n        Tests that the csrf cookie is set on a successful response.\n        \"\"\"\n        request = TestingHttpRequest()\n        middleware = CsrfViewMiddleware()\n        response = HttpResponse()\n        middleware.process_response(request, response)\n        self.assertTrue(response.cookies)\n"], "sample_440": ["    def test_aiterator(self):\n        qs = Country.objects.aiterator(chunk_size=1)\n        self.assertEqual(list(qs), list(Country.objects.all()))\n", "    def test_prefetch_related(self):\n        pizzeria = Pizzeria.objects.create(name=\"Pizza Palace\")\n        related = pizzeria.related_set.create(name=\"Related\")\n        country = Country.objects.create(name=\"Country\", iso_two_letter=\"US\")\n        pizzeria.country = country\n        pizzeria.save()\n\n        queryset = Pizzeria.objects.prefetch_related(\"related_set\", \"country\")\n        objects = list(queryset.all())\n        self.assertEqual(len(objects), 1)\n        self.assertEqual(objects[0].name, \"Pizza Palace\")\n        self.assertEqual(objects[0].related_set.count(), 1)\n        self.assertEqual(objects[0].related_set.first().name, \"Related\")\n        self.assertEqual(objects[0].country.name, \"Country\")\n        self.assertEqual(objects[0].country.iso_two_letter, \"US\")\n\n        related = objects[0].related_set.first()\n        country = related.country\n        self.assertEqual(related.name, \"Related\")\n        self.assertEqual(related.country, country)\n        self.assertEqual(country.name, \"Country\")\n        self.assertEqual(country.iso_two_letter, \"US\")\n", "    def setUp(self):\n        self.pizzeria = Pizzeria.objects.create(name='Best Pizza')\n        self.country = Country.objects.create(name='USA', iso_two_letter='US')\n        self.state = State.objects.create(name='California', country=self.country)\n        self.restaurant = Restaurant.objects.create(\n            name='Pizzeria Uno', pizzeria=self.pizzeria, state=self.state\n        )\n", "    def test_values(self):\n        c = Country.objects.all().values()\n        self.assertIsInstance(c, ValuesIterable)\n", "    def test_update(self):\n        Country.objects.create(name=\"Spain\", iso_two_letter=\"ES\")\n\n        with self.assertRaises(IntegrityError):\n            Country.objects.update(name=\"Spain\", iso_two_letter=\"ES\", pk=None)\n"], "sample_980": ["def test_from_sequence():\n    p = Permutation.from_sequence('abc')\n    assert p.array_form == [2, 0, 1]\n    assert p == Permutation(2, 0, 1)\n\n    p = Permutation.from_sequence('abc', key=lambda x: x.lower())\n    assert p.array_form == [1, 0, 2]\n    assert p == Permutation(1, 0, 2)\n\n    p = Permutation.from_sequence('abcde')\n    assert p.array_form == [3, 1, 2, 0, 4]\n    assert p == Permutation(3, 1, 2, 0, 4)\n\n    p = Permutation.from_sequence('abcde', key=lambda x: x.lower())\n    assert p.array_form == [1, 3, 2, 0, 4]\n    assert p == Permutation(1, 3, 2, 0, 4)\n\n    with raises(ValueError):\n        Permutation.from_sequence('abcde', key=lambda x: x.upper())\n\n    with raises(ValueError):\n        Permutation.from_sequence('abcde', key=lambda x: -x)\n", "def test_inverse():\n    p = Permutation([[2, 0], [3, 1]])\n    assert p**-1 == Permutation([2, 3, 0, 1])\n", "def test_af_pow():\n    p = Permutation([[1, 2], [3, 4, 5]])\n    p2 = p**2\n    assert p2 == Permutation([2, 1, 5, 3, 4])\n    assert p**3 == Permutation([2, 1, 4, 5, 3])\n", "def test_rmul_with_af():\n    a = [1, 0, 2]\n    b = [0, 2, 1]\n    c = [0, 2, 1, 3]\n    p, q, r = Permutation(a), Permutation(b), Permutation(c)\n\n    # test rmul method\n    assert rmul(p, q) == Permutation([1, 0, 2])\n    assert rmul(q, p) == Permutation([0, 2, 1])\n    assert rmul(r, p) == Permutation([0, 2, 1, 3])\n\n    # test rmul_with_af method\n    assert Permutation.rmul_with_af(p, q) == Permutation([1, 0, 2])\n    assert Permutation.rmul_with_af(q, p) == Permutation([0, 2, 1])\n    assert Permutation.rmul_with_af(r, p) == Permutation([0, 2, 1, 3])\n\n    # test for invalid inputs\n    raises(ValueError, lambda: Permutation.rmul_with_af(p))\n    raises(ValueError, lambda: Permutation.rmul_with_af(p, 3))\n\n    # test with multiple arguments\n    assert Permutation.rmul_with_af(p, q, r) == Permutation([0, 2, 1, 3])\n\n    # test with Cycle arguments\n    c = Cycle(1, 2)\n    assert Permutation.rmul_with_af(p, c) == Permutation([0, 2, 1])\n    assert Permutation.rmul_with_af(c, p) == Permutation([1, 0, 2])\n", "compilation error"], "sample_833": ["def test_multinomial_loss():\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    Y = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    w = np.array([1, 1, 1])\n    alpha = 1.0\n    sample_weight = np.array([1.0, 1.0, 1.0])\n    loss, p, w = _multinomial_loss(w, X, Y, alpha, sample_weight)\n    assert_array_equal(p, np.exp(X.dot(w.T)))\n    loss2, p2, w2 = _multinomial_loss(w, X, Y, alpha, sample_weight)\n    assert_array_equal(loss, loss2)\n    assert_array_equal(p, p2)\n    assert_array_equal(w, w2)\n", "def test_logistic_regression_cv_elastic_net():\n    # Define the dataset and the model\n    iris = load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Define the model with elastic net penalty and l1_ratio\n    model = LogisticRegressionCV(solver='saga', penalty='elasticnet',\n                                l1_ratio=0.5)\n\n    # Perform grid search over the parameters\n    param_grid = {'Cs': [0.1, 1, 10]}\n    modelCV = GridSearchCV(model, param_grid, cv=5)\n    modelCV.fit(X, y)\n\n    # Check that the model has the correct parameters\n    assert_equal(modelCV.best_params_['penalty'], 'elasticnet')\n    assert_allclose(modelCV.best_params_['l1_ratio'], 0.5)\n\n    # Check that the model has the correct best score\n    assert_greater(modelCV.best_score_, 0.7)\n\n    # Check that the model has the correct best estimator\n    best_model = modelCV.best_estimator_\n    assert_equal(best_model.penalty, 'elasticnet')\n    assert_allclose(best_model.l1_ratio, 0.5)\n", "def test_logistic_regression_cv_elastic_net():\n    # Test that the 'elasticnet' penalty works as expected\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=3, n_repeated=0, n_classes=2,\n                               random_state=0)\n    l1_ratios = [0.1, 0.5, 0.9]\n    Cs = [0.1, 1, 10]\n    model = LogisticRegressionCV(penalty='elasticnet', l1_ratios=l1_ratios,\n                               Cs=Cs, solver='saga')\n    model.fit(X, y)\n    assert model.coef_.shape == (2, X.shape[1])\n    assert model.intercept_.shape == (2,)\n    assert model.n_iter_.shape == (2, 1, 3)\n    assert model.C_.shape == (2,)\n    assert model.l1_ratio_.shape == (2,)\n\n    # Test that the best parameter is correctly chosen\n    # when using cross-validation\n    model_cv = LogisticRegressionCV(penalty='elasticnet', l1_ratios=l1_ratios,\n                                   Cs=Cs, solver='saga', cv=5)\n    model_cv.fit(X, y)\n    assert model_cv.coef_.shape == (2, X.shape[1])\n    assert model_cv.intercept_.shape == (2,)\n    assert model_cv.n_iter_.shape == (2, 5, 3)\n    assert model_cv.C_.shape == (2,)\n    assert model_cv.l1_ratio_.shape == (2,)\n\n    # Test that the l1_ratio is correctly chosen when using cross-validation\n    model_cv = LogisticRegressionCV(penalty='elasticnet', Cs=Cs, solver='saga',\n                                   cv=5)\n    model_cv.fit(X, y)\n    assert model_cv.coef_.shape == (2, X.shape[1])\n    assert model_cv.intercept_.shape == (2,)\n    assert model_cv.n_iter_.shape == (2, 5, 3)\n    assert model_cv.C_.shape == (2,)\n    assert model_cv.l1_ratio_.shape == (2,)\n\n    # Test that the elasticnet penalty is correctly implemented\n    # when using the 'saga' solver\n    model = LogisticRegressionCV(penalty='elasticnet", "def test_multiclass_logistic_regression_multinomial():\n    \"\"\"Check that the multinomial loss and gradient are computed correctly in\n    the multiclass logistic regression case.\"\"\"\n    # Multinomial loss\n    X = np.array([[0, 0, 1], [0, 0, 1], [0, 1, 0]])\n    Y = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]])\n    w = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    alpha = 1.0\n    sample_weight = np.array([1.0, 1.0, 1.0])\n    loss, p, w_reshaped = _multinomial_loss(w, X, Y, alpha, sample_weight)\n    assert_array_almost_equal(loss, 0.0)\n\n    # Multinomial gradient\n    grad, _ = _multinomial_loss_grad(w, X, Y, alpha, sample_weight)\n    expected_grad = np.array([-3.0, 0.0, 3.0, 0.0, -6.0, 6.0])\n    assert_array_almost_equal(grad, expected_grad)\n\n    # Multinomial Hessian-vector product\n        return np.dot(np.dot(Y.T, np.diag(p)), v)\n\n    grad, hessp = _multinomial_grad_hess(w, X, Y, alpha, sample_weight)\n    expected_grad = np.array([-3.0, 0.0, 3.0, 0.0, -6.0, 6.0])\n    expected_hessp = np.array([12.0, 0.0, -12.0, 0.0, 36.0, -36.0])\n    assert_array_almost_equal(grad, expected_grad)\n    assert_array_almost_equal(hessp(np.array([-1.0, 0.0, 1.0, 0.0, -2.0, 2.0])), expected_hessp)\n\n", "def test_logistic_regression_cv_elastic_net():\n    # Test the elastic net penalty with the SAGA solver\n    # The problem is binary\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                              n_redundant=0, random_state=0, n_classes=2)\n    y = y.astype(np.int32)\n    clf = LogisticRegressionCV(penalty='elasticnet', solver='saga',\n                              max_iter=100, tol=1e-4, Cs=10,\n                              l1_ratio=[0, 0.1, 0.5, 1],\n                              random_state=0, n_jobs=1, multi_class='auto')\n    check_predictions(clf, X, y)\n\n    # Test that the ElasticNet penalty is correctly handled\n    assert_equal(clf.coef_.shape[1], 20)\n    assert_equal(clf.l1_ratio_, [0, 0.1, 0.5, 1])\n    assert_equal(clf.penalty_, 'elasticnet')\n    assert_equal(clf.solver_, 'saga')\n    assert_equal(clf.C_, [1.0, 1.0, 1.0, 1.0])\n    assert_equal(clf.n_iter_, (2, 2, 2, 2))\n\n    # Test the refit parameter\n    clf.refit = False\n    check_predictions(clf, X, y)\n    assert_equal(clf.coef_.shape[1], 20)\n    assert_equal(clf.l1_ratio_.shape, (4,))\n    assert_equal(clf.C_.shape, (4,))\n    assert_equal(clf.n_iter_.shape, (4,))\n\n    # Test that a ValueError is raised when l1_ratio is not a list\n    with pytest.raises(ValueError):\n        LogisticRegressionCV(penalty='elasticnet', solver='saga',\n                            max_iter=100, tol=1e-4, Cs=10,\n                            l1_ratio='not a list',\n                            random_state=0, n_jobs=1, multi_class='auto')\n\n    # Test that a ValueError is raised when l1_ratio is not a list of numbers\n    with pytest.raises(ValueError):\n        LogisticRegressionCV(penalty='elasticnet', solver='saga',\n                            max_iter=100, tol=1e-4, Cs=10,\n                            l1"], "sample_954": ["def test_ManualPageWriter_translate():\n    doc = nodes.document('')\n    writer = ManualPageWriter(Builder())\n    writer.translate()\n    assert writer.output.startswith('.TH \"TITLE\" \"1\" \"DATE\" \"VERSION\" \"PROJECT\"\\n.SH NAME\\nTITLE - subtitle\\n')\n", "def test_ManualPageWriter_translator_title_upper():\n    from sphinx.builders.manpage import ManualPageWriter\n    from sphinx.config import Config\n    from sphinx.util.nodes import NodeMatcher\n\n    config = Config()\n    config.title = 'Test Title'\n    config.subtitle = 'Test Subtitle'\n    builder = object()\n    translator = ManualPageWriter(builder)\n    document = object()\n    translator.translate()\n\n    assert translator._docinfo['title_upper'] == 'TEST TITLE'\n", "def test_manpage_translator_footer():\n    # Create a dummy builder and config\n    builder = mock.Mock(spec=Builder)\n    config = Config()\n\n    # Create a dummy document\n    document = mock.Mock(spec=nodes.document)\n    document.settings = config\n\n    # Create a ManualPageTranslator instance\n    translator = ManualPageTranslator(document, builder)\n\n    # Test the footer method\n    assert translator.footer() == '.SH AUTHOR\\n'\n", "def test_manpage_translator_has_section_titles():\n    # Given\n    config = Config()\n    config.title = 'My Title'\n    config.subtitle = 'My Subtitle'\n    config.authors = ['John Doe']\n    config.section = '8'\n    config.today = '2022-01-01'\n    config.copyright = '2022 John Doe'\n    config.version = '1.0'\n    config.project = 'My Project'\n\n    # When\n    manpage = ManualPageTranslator(nodes.document(), config)\n\n    # Then\n    assert manpage._docinfo['title'] == 'My Title'\n    assert manpage._docinfo['subtitle'] == 'My Subtitle'\n    assert manpage._docinfo['author'] == 'John Doe'\n    assert manpage._docinfo['manual_section'] == '8'\n    assert manpage._docinfo['date'] == 'Jan  1, 2022'\n    assert manpage._docinfo['copyright'] == '2022 John Doe'\n    assert manpage._docinfo['version'] == '1.0'\n    assert manpage._docinfo['manual_group'] == 'My Project'\n", "def test_manualpage_translator_header(tmp_path):\n    # Create a test config file\n    config_file = tmp_path / 'conf.py'\n    config_file.write_text('''\n        import sphinx\n\n        project = 'Test Project'\n        copyright = 'Copyright 2023'\n        today = '2023-01-01'\n        today_fmt = '%Y-%m-%d'\n        version = '1.0'\n        manual_section = 1\n        title = 'Test Title'\n        subtitle = 'Test Subtitle'\n        authors = 'Test Author'\n    ''')\n\n    # Create a test source directory\n    srcdir = tmp_path / 'source'\n    srcdir.mkdir()\n    (srcdir / 'index.rst').write_text('''\n        .. manpage:: test\n\n           :title: Test Title\n           :subtitle: Test Subtitle\n           :authors: Test Author\n           :section: 1\n           :version: 1.0\n           :copyright: 2023\n           :today: 2023-01-01\n           :today_fmt: %Y-%m-%d\n    ''')\n\n    # Run Sphinx build\n    app = Sphinx('test-build', srcdir, outdir=tmp_path, config_file=config_file)\n    app.builder.build_all()\n\n    # Check the generated manpage\n    manpage = tmp_path / 'test-build' / 'man' / 'man1' / 'test.1'\n    with manpage.open() as f:\n        contents = f.read()\n\n    # Check the header\n    expected_header = ('.TH \"TEST TITLE\" \"1\" \"2023-01-01\" \"1.0\" \"Test Project\"\\n'\n                      '.SH NAME\\n'\n                      'Test Title - Test Subtitle\\n')\n    assert contents.startswith(expected_header)\n"], "sample_649": ["def test_pytest_configure(caplog):\n    # Arrange\n    caplog.handler.setLevel(logging.ERROR)\n\n    # Act\n    caplog.set_level(logging.INFO, \"my_logger\")\n\n    # Assert\n    assert caplog.get_records(\"setup\") == []\n    assert caplog.handler.level == logging.INFO\n    assert caplog.get_records(\"call\") == []\n    assert caplog.handler.level == logging.INFO\n    assert caplog.get_records(\"teardown\") == []\n", "def test_config_get_log_level_from_option():\n    # Test that get_log_level_for_setting gets the log level from a setting.\n    config = Config()\n    config.option.log_level = \"INFO\"\n    assert get_log_level_for_setting(config, \"log_level\") == 20\n\n    # Test that get_log_level_for_setting does not get the log level from a setting.\n    config = Config()\n    assert get_log_level_for_setting(config, \"log_level\") is None\n\n    # Test that get_log_level_for_setting raises a UsageError if the log level is not recognized.\n    config = Config()\n    with pytest.raises(UsageError):\n        get_log_level_for_setting(config, \"log_level\", \"CRITICAL\")\n\n    # Test that get_log_level_for_setting gets the log level from a config option.\n    config = Config()\n    config.option.log_cli_level = \"DEBUG\"\n    assert get_log_level_for_setting(config, \"log_cli_level\") == 10\n\n    # Test that get_log_level_for_setting does not get the log level from a config option if it is not set.\n    config = Config()\n    assert get_log_level_for_setting(config, \"log_cli_level\") is None\n\n    # Test that get_log_level_for_setting gets the log level from a config option even if it is not set in the command line.\n    config = Config()\n    config.getoption(\"log_level\") = None\n    assert get_log_level_for_setting(config, \"log_cli_level\") == 10\n", "def test_capture_manager_disabled(caplog, pytester):\n    \"\"\"Ensure live logging works with capture manager disabled.\"\"\"\n    # Disable capture manager\n    pytester.makepyfile(content=\"import logging; logging.warning('Test warning')\")\n\n    # Enable live logging\n    pytester.ensureini(\"\"\"\\\n    [pytest]\n    log_cli = on\n    log_level = WARNING\n    \"\"\")\n    pytester.runpytest(\"--capture=no\")\n\n    # Check that live logging is enabled\n    assert \"live log call\" in pytester.stdout.getvalue()\n    assert \"WARNING\" in pytester.stdout.getvalue()\n", "def test_log_auto_indent(caplog: FixtureRequest) -> None:\n    \"\"\"Test that the --log-auto-indent option works as expected.\"\"\"\n    pytestmain = Pytester()\n\n    pytestmain.parse([\n        \"-s\", \"--log-auto-indent=2\", \"-s\", \"test_log.py\",\n    ], exit_status=ExitCode.OK)\n\n    with pytestmain.use_sandbox():\n        captured = caplog.records\n        assert len(captured) == 2\n        assert captured[0].message == \"log message 1\"\n        assert captured[1].message == \"log message 2\"\n", "def test_logging_plugin_configuration(caplog, capsys, pytestconfig):\n    \"\"\"Test that a plugin instance gets properly configured.\"\"\"\n    # Test that the config is passed to the plugin correctly\n    assert cast(LoggingPlugin, pytestconfig.pluginmanager.getplugin(\"logging-plugin\"))._config == pytestconfig\n"], "sample_1101": ["def test_lower_bound_for_large_schur_number():\n    k = 10\n    schur_num = SchurNumber(k)\n    lower_bound = schur_num.lower_bound()\n    assert lower_bound == (3**k - 1)/2\n", "def test_schur_number_lower_bound():\n    # Test lower bound for known values of k\n    for k in [1, 2, 3, 4]:\n        schur_num = SchurNumber(k)\n        assert schur_num.eval() == k, f\"Expected SchurNumber({k}) to evaluate to {k}, but got {schur_num.eval()}\"\n        assert schur_num.lower_bound() == schur_num.eval(), f\"Expected SchurNumber({k}).lower_bound() to return {k}, but got {schur_num.lower_bound()}\"\n\n    # Test lower bound for unknown values of k\n    for k in [5, 6, 7]:\n        schur_num = SchurNumber(k)\n        assert schur_num.lower_bound() != schur_num.eval(), f\"Expected SchurNumber({k}).lower_bound() to return the lower bound, but got {schur_num.eval()}\"\n        assert schur_num.lower_bound() > schur_num.eval(), f\"Expected SchurNumber({k}).lower_bound() to return a value greater than the known Schur number, but got {schur_num.lower_bound()}\"\n\n    # Test invalid inputs for SchurNumber\n    with raises(ValueError):\n        SchurNumber(S.Infinity)\n    with raises(ValueError):\n        SchurNumber(0)\n    with raises(ValueError):\n        SchurNumber(-1)\n    with raises(ValueError):\n        SchurNumber(1.5)\n    with raises(ValueError):\n        SchurNumber(S.Rational(1, 2))\n", "def test_lower_bound_retrieval():\n    # Test retrieval of lower bound for known values\n    for k in [1, 2, 3, 4]:\n        assert SchurNumber(k).lower_bound() == (3**k - 1)//2\n\n    # Test retrieval of lower bound for unknown values\n    assert SchurNumber(5).lower_bound() == (3**5 - 1)//2\n\n    # Test retrieval of lower bound for infinity\n    assert SchurNumber(S.Infinity).lower_bound() == S.Infinity\n\n    # Test retrieval of lower bound for invalid input\n    with raises(ValueError):\n        SchurNumber(-1).lower_bound()\n\n    with raises(ValueError):\n        SchurNumber(0).lower_bound()\n\n    with raises(ValueError):\n        SchurNumber(S.Zero).lower_bound()\n\n    with raises(ValueError):\n        SchurNumber(Rational(1, 2)).lower_bound()\n\n    # Test retrieval of lower bound for non-integer input\n    with raises(ValueError):\n        SchurNumber(1.5).lower_bound()\n", "def test_schur_number_upper_bound():\n    # Test the upper bound of the Schur number\n    assert SchurNumber(5).eval() == 169\n", "def test_schur_partition_unusual_cases():\n    # Test edge cases for n\n    n = symbols('n')\n    assert schur_partition(0) == []\n    with raises(ValueError):\n        schur_partition(-1)\n    with raises(ValueError):\n        schur_partition(n)\n\n    # Test special case of n = 1\n    assert schur_partition(1) == [[1]]\n\n    # Test special case of n = 2\n    assert schur_partition(2) == [[1, 2]]\n\n    # Test special case of n = 3\n    assert schur_partition(3) == [[1, 2, 3]]\n\n    # Test number of subsets for n > 3\n    assert schur_partition(13) == [[1, 5], [4, 12], [2, 11], [6, 10, 13], [3, 9, 8], [7]]\n"], "sample_282": ["    def test_bound_field_as_widget(self):\n        form = PartiallyRequiredForm(data={'f': 'John,Paul'})\n        field = form.fields['f']\n        bound_field = BoundField(form, field, 'f')\n        widget = bound_field.as_widget()\n        self.assertContainsHTML(widget, '<input type=\"text\"')\n        self.assertContainsHTML(widget, '<input type=\"text\"')\n", "    def setUpClass(cls):\n        cls.form = PartiallyRequiredForm({'f-0': 'J', 'f-1': 'Paul'})\n        super().setUpClass()\n", "    def test_bound_field_with_initial_value(self):\n        form = PartiallyRequiredForm(data={'f-0': 'value1', 'f-1': 'value2'})\n        bound_field = BoundField(form, PartiallyRequiredField(form.fields), 'f')\n        self.assertEqual(bound_field.value(), 'value1,value2')\n        self.assertTrue(bound_field.is_hidden)\n", "    def setUpClass(cls):\n        cls.field = CharField()\n        cls.form = Form(fields=(cls.field,))\n        super().setUpClass()\n", "    def setUpClass(cls):\n        cls.field = CharField(widget=TextInput())\n        cls.form = Form([cls.field])\n        super().setUpClass()\n"], "sample_1123": ["def test_condition_set_subs():\n    w = Symbol('w')\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    c1 = ConditionSet(x, x > 0, S.Integers)\n    c2 = ConditionSet(x, x > 0, S.Integers)\n    assert c1.dummy_eq(c2)\n\n    c3 = ConditionSet(x, x < 0, S.Integers)\n    assert c1.dummy_eq(c3, 'x')\n\n    c4 = ConditionSet(x + 1, x + 1 > 1, S.Integers)\n    assert not c1.dummy_eq(c4)\n\n    c5 = ConditionSet(x, x > 0, FiniteSet(1, 2, 3))\n    assert c1.dummy_eq(c5)\n\n    c6 = ConditionSet(x, Eq(x, 0), S.Integers)\n    assert c1.dummy_eq(c6)\n\n    c7 = ConditionSet(x, x > 0, S.UniversalSet)\n    assert c1.dummy_eq(c7)\n\n    c8 = ConditionSet(x, x > 0, Interval(0, 10))\n    assert c1.dummy_eq(c8)\n\n    with warns_deprecated_sympy():\n        c9 = FiniteSet(1, 2, 3)\n        assert c1.dummy_eq(c9)\n\n    with raises(TypeError):\n        ConditionSet(x + 1, x > 0, 'Integers')\n\n    with raises(ValueError):\n        ConditionSet(x + 1, x > 0, S.Integers)\n\n    c10 = ConditionSet(x, x > 0, Union(S.Integers, S.Rationals))\n    assert c1.dummy_eq(c10)\n", "def test_condition_set_sym_deprecation_warning():\n    w = Symbol('w')\n    x = Symbol('x')\n    condition = w**2 > 0\n    base_set = FiniteSet(-1, 1, 2)\n    CS = ConditionSet(w, condition, base_set)\n    with warns_deprecated_sympy():\n        assert CS == ConditionSet(w, condition, base_set)\n", "def test_condition_set_dummy_symbol_substitution():\n    from sympy.sets import ConditionSet\n    from sympy.abc import x, y, z\n    from sympy import Symbol\n    c = ConditionSet(x, x < 1, {x, z})\n    c2 = c.subs(x, y)\n    assert c2 == ConditionSet(y, y < 1, FiniteSet(y, z))\n    c3 = c2.subs(y, y)\n    assert c3 == ConditionSet(y, y < 1, FiniteSet(y, z))\n    c4 = c2.subs(y, 1)\n    assert c4 == ConditionSet(y, y < 1, FiniteSet(z))\n", "def test_conditional_set_intersection():\n    x = Symbol('x')\n    a = ConditionSet(x, x > 0, Interval(0, 10))\n    b = ConditionSet(x, x < 5, Interval(0, 10))\n    c = ConditionSet(x, x > 5, Interval(0, 10))\n    intersection = Intersection(a, b)\n    intersection_2 = intersection.subs(x, y)\n    assert isinstance(intersection_2, ConditionSet)\n    assert intersection_2 == ConditionSet(y, And(y > 0, y < 5), Interval(0, 10))\n    \n    intersection = a.intersection(b)\n    assert isinstance(intersection, ConditionSet)\n    assert intersection == ConditionSet(x, And(x > 0, x < 5), Interval(0, 10))\n    \n    intersection = a.intersection(c)\n    assert isinstance(intersection, EmptySet)\n    \n    intersection = Intersection(c, b)\n    intersection_2 = intersection.subs(x, y)\n    assert isinstance(intersection_2, ConditionSet)\n    assert intersection_2 == ConditionSet(y, And(y < 5, y > 5), Interval(0, 10))\n    \n    # Test with symbols having assumptions\n    n = Symbol('n', real=True)\n    m = Symbol('m', real=True)\n    a = ConditionSet(n, n > 0, Interval(0, 10))\n    b = ConditionSet(m, m < 5, Interval(0, 10))\n    intersection = Intersection(a, b)\n    assert isinstance(intersection, ConditionSet)\n    assert intersection == ConditionSet(n, And(n > 0, n < 5), Interval(0, 10))\n    \n    # Test with ConditionSet instances having symbols with assumptions\n    d = ConditionSet(n, n < 5, Interval(0, 10))\n    e = ConditionSet(m, m > 5, Interval(0, 10))\n    intersection = Intersection(d, e)\n    assert isinstance(intersection, EmptySet)\n    \n    # Test with ConditionSet instances having symbols with assumptions and the same domain\n    d = ConditionSet(n, n < 5, Interval(0, 10))\n    e = ConditionSet(n, n > 5, Interval(0, 10))\n    intersection = Intersection(d, e)\n    assert isinstance(intersection, EmptySet)\n", "def test_condition_set_base_set():\n    x = Symbol('x')\n    y = Symbol('y')\n\n    base_set = ConditionSet(x, Eq(x, 1), S.Integers)\n    condition = Eq(x, 2)\n\n    cond_set = ConditionSet(y, Eq(y, 2), base_set)\n\n    assert cond_set == ConditionSet(y, Eq(y, 2), FiniteSet(1))\n"], "sample_401": ["    def test_custom_kwarg_in_formset(self):\n        formset = formset_factory(CustomKwargForm, formset=BaseFormSet, extra=2)\n        formset_data = {\n            \"TOTAL_FORMS\": \"2\",\n            \"INITIAL_FORMS\": \"0\",\n            \"MIN_NUM_FORMS\": \"0\",\n            \"MAX_NUM_FORMS\": \"0\",\n            \"0-choice\": \"Choice 1\",\n            \"0-custom_kwarg\": \"kwarg 1\",\n            \"1-choice\": \"Choice 2\",\n            \"1-custom_kwarg\": \"kwarg 2\",\n        }\n        formset_instance = formset(formset_data)\n        self.assertEqual(len(formset_instance.forms), 2)\n        for i, form in enumerate(formset_instance.forms):\n            self.assertEqual(form.cleaned_data[\"custom_kwarg\"], f\"kwarg {i+1}\")\n", "    def test_management_form_initial_form_count(self):\n        formset = ChoiceFormSet()\n        form = formset.management_form\n        self.assertEqual(form.cleaned_data[INITIAL_FORM_COUNT], 0)\n", "    def test_formset_factory_custom_renderer(self):\n        form = CustomKwargForm\n        formset_class = formset_factory(form, renderer=TemplatesSetting(\"custom_template.html\"))\n        self.assertIsInstance(formset_class.renderer, TemplatesSetting)\n        self.assertEqual(formset_class.renderer.template_name, \"custom_template.html\")\n", "    def test_base_formset_management_form(self):\n        \"\"\"\n        Test that BaseFormSet management form is properly populated.\n        \"\"\"\n        data = {\n            \"TOTAL_FORMS\": \"2\",\n            \"INITIAL_FORMS\": \"0\",\n            \"MAX_NUM_FORMS\": \"2\",\n            \"MIN_NUM_FORMS\": \"0\",\n            \"0-choice\": \"choice1\",\n            \"0-votes\": \"10\",\n            \"1-choice\": \"choice2\",\n            \"1-votes\": \"20\",\n        }\n        formset = BaseFormSet(data=data, prefix=\"choices\")\n        self.assertEqual(formset.management_form.data[\"TOTAL_FORMS\"], \"2\")\n        self.assertEqual(formset.management_form.data[\"INITIAL_FORMS\"], \"0\")\n        self.assertEqual(formset.management_form.data[\"MAX_NUM_FORMS\"], \"2\")\n        self.assertEqual(formset.management_form.data[\"MIN_NUM_FORMS\"], \"0\")\n", "    def test_deletion_form_not_marked_for_deletion_by_default(self):\n        formset = ChoiceFormSet()\n        self.assertFalse(formset.forms[0].cleaned_data.get(DELETION_FIELD_NAME))\n"], "sample_909": ["    def test_napoleon_use_param_fails_on_numpy_style_field(self):\n        config = Config(napoleon_use_param=True)\n        docstring = \"\"\"One line summary.\n", "    def test_numpydoc_see_also_section(self):\n        config = Config(napoleon_use_admonition_for_examples=False)\n        docstring = dedent('''\n            See Also\n            --------\n            func1 : Descriptive text\n                continued text\n            func2 : Descriptive text\n            func3, :meth:`func4`, func5\n            ''')\n        docstring_parser = NumpyDocstring(docstring, config)\n        expected_output = ['.. admonition:: See Also', '', 'func1 : Descriptive text', '', 'func2 : Descriptive text', '', 'func3, meth:`func4`, func5', '']\n        self.assertEqual(docstring_parser.lines(), expected_output)\n", "    def test_custom_sections(self):\n        docstring = dedent('''\\\n            One line summary.\n\n            Other Parameters\n            ---------------\n\n            custom_section : str\n                Description of custom_section.\n            ''')\n        config = Config(napoleon_custom_sections=[('custom_section', 'Other Parameters')])\n        google_docstring = GoogleDocstring(docstring, config)\n        self.assertEqual(google_docstring.lines(),\n            ['One line summary.', 'Other Parameters', '', '    :param custom_section:', '    :type custom_section: str', '', '    Description of custom_section.'])\n", "    def test_google_docstring_multiple_lines_description(self):\n        docstring = \"\"\"\n            This is a sample docstring that spans multiple lines.\n            The lines are indented.\n\n                And these lines are indented too\n                And these lines are indented too\n        \"\"\"\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        google_docstring = GoogleDocstring(docstring, config)\n        expected_output = \"\"\"\n            This is a sample docstring that spans multiple lines.\n            <BLANKLINE>\n            The lines are indented.\n\n            <BLANKLINE>\n            And these lines are indented too\n            And these lines are indented too\n            <BLANKLINE>\n            \"\"\"\n        self.assertEqual(cleandoc(expected_output), cleandoc(google_docstring.__str__()))\n", "    def test_attribute_docstring_indentation(self):\n        config = Config()\n        docstring = \"\"\"\n        class TestClass:\n            'docstring'\"\"\"\n        docstring_obj = GoogleDocstring(docstring, config)\n        expected = dedent(\"\"\"\n        .. attribute:: docstring\n\n        \"\"\").strip()\n        self.assertEqual(docstring_obj.lines(), expected.split('\\n'))\n"], "sample_22": ["def test_is_O3_matrix_product():\n    \"\"\"Test that matrix product of O(3) matrices is also O(3)\"\"\"\n    # Create two rotation matrices\n    mat1 = rotation_matrix(45 * u.deg, \"x\")\n    mat2 = rotation_matrix(30 * u.deg, \"y\")\n\n    # Check that both matrices are in O(3)\n    assert is_O3(mat1)\n    assert is_O3(mat2)\n\n    # Check that their product is also in O(3)\n    assert is_O3(matrix_product(mat1, mat2))\n", "def test_rotation_matrix_axis_order():\n    # Test rotation matrices with different axis orders\n    angle = u.Quantity(45 * u.degree, u.degree)\n    matrix_z = rotation_matrix(angle, axis=\"z\")\n    matrix_x = rotation_matrix(angle, axis=\"x\")\n    matrix_y = rotation_matrix(angle, axis=\"y\")\n\n    # Check that matrices are equal up to sign\n    matrix_z_expected = np.array([[np.cos(angle.to_value(u.radian)), -np.sin(angle.to_value(u.radian)), 0],\n                                  [np.sin(angle.to_value(u.radian)), np.cos(angle.to_value(u.radian)), 0],\n                                  [0, 0, 1]])\n    matrix_x_expected = np.array([[1, 0, 0],\n                                 [0, np.cos(angle.to_value(u.radian)), -np.sin(angle.to_value(u.radian))],\n                                 [0, np.sin(angle.to_value(u.radian)), np.cos(angle.to_value(u.radian))]])\n    matrix_y_expected = np.array([[np.cos(angle.to_value(u.radian)), 0, np.sin(angle.to_value(u.radian))],\n                                 [0, 1, 0],\n                                 [-np.sin(angle.to_value(u.radian)), 0, np.cos(angle.to_value(u.radian))]])\n\n    assert_allclose(matrix_z, matrix_z_expected)\n    assert_allclose(matrix_x, matrix_x_expected)\n    assert_allclose(matrix_y, matrix_y_expected)\n", "def test_rotation_matrix_unit_input():\n    \"\"\"Test that rotation matrix works with unit input.\"\"\"\n    angle = 90 * u.deg\n    matrix = rotation_matrix(angle, axis=\"z\")\n    expected_matrix = np.array([[np.cos(np.deg2rad(90)), 0, np.sin(np.deg2rad(90))],\n                               [0, 1, 0],\n                               [-np.sin(np.deg2rad(90)), 0, np.cos(np.deg2rad(90))]])\n    assert_allclose(matrix, expected_matrix)\n\n    angle = 90 * u.rad\n    matrix = rotation_matrix(angle, axis=\"z\")\n    assert_array_equal(matrix, expected_matrix)\n", "def test_rotation_matrix_invalid_unit():\n    with pytest.raises(ValueError):\n        rotation_matrix(angle=u.Quantity(10 * u.deg), axis=[1, 2, 3])\n\n    with pytest.raises(ValueError):\n        rotation_matrix(angle=10, axis=[1, 2, 3])\n", "def test_rotation_matrix_attributes():\n    \"\"\"Test that rotation matrix has expected attributes.\"\"\"\n    # Test identity matrix\n    mat = np.eye(3)\n    rmat = rotation_matrix(0, unit=u.deg)\n    assert_array_equal(mat, rmat)\n\n    # Test rotation around x, y, z\n    for axis in ['x', 'y', 'z']:\n        rmat = rotation_matrix(90, axis=axis, unit=u.deg)\n        assert_allclose(rmat, rotation_matrix(90, axis=axis))\n        assert not is_O3(rmat)  # Not O(3) due to rounding errors\n\n    # Test rotation around non-unit vector\n    v = np.array([0.5, 1.0, 0.7])\n    v = v / np.linalg.norm(v)\n    rmat = rotation_matrix(45, axis=v)\n    assert_allclose(rmat, rotation_matrix(45, axis=v))\n\n    # Test rotation around vector with non-unit length\n    v = np.array([0.5, 1.0, 0.7])\n    rmat = rotation_matrix(45, axis=v)\n    assert_allclose(rmat, rotation_matrix(45, axis=v / np.linalg.norm(v)))\n\n    # Test rotation around vector with NaN values\n    v = np.array([0.5, np.nan, 0.7])\n    with pytest.raises(ValueError):\n        rotation_matrix(45, axis=v)\n\n    # Test rotation around unit vector with non-numeric values\n    v = np.array([1j, 2.0, 3.0])\n    with pytest.raises(ValueError):\n        rotation_matrix(45, axis=v)\n\n    # Test rotation around invalid axis\n    with pytest.raises(KeyError):\n        rotation_matrix(45, axis='invalid')\n    with pytest.raises(TypeError):\n        rotation_matrix(45, axis=np.array([1, 2, 3, 4]))\n"], "sample_316": ["    def test_image_dimensions_cache(self):\n        with NamedTemporaryFile(suffix='.jpg') as tmp:\n            img = Image.new('RGB', (10, 10))\n            img.save(tmp.name)\n            with ImageFile(tmp) as file:\n                self.assertEqual(file.width, 10)\n                self.assertEqual(file.height, 10)\n                self.assertEqual(file.width, 10)\n                self.assertEqual(file.height, 10)\n", "    def test_image_file_dimensions(self):\n        # Test that the dimensions are correctly retrieved from an image file.\n        # We use a sample PNG image.\n        with NamedTemporaryFile(suffix='.png') as tmp:\n            tmp.write(b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x08\\x02\\x00\\x00\\x00\\x90wS\\xde\\x00\\x00\\x00\\x01sRGB\\x00\\xae\\xce\\x1c\\xe9\\x00\\x00\\x00\\x04gAMA\\x00\\x00\\xb2\\xafT\\x00\\x00\\x00\\x01sRGB\\x00\\xae\\xce\\x1c\\xe9\\x00\\x00\\x00\\x08cHRM\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00\\x80\\x00\\x00\\x00", "    def test_get_image_dimensions_with_invalid_image_file(self):\n        with tempfile.NamedTemporaryFile() as fp:\n            fp.write(b'Not an image')\n            fp.seek(0)\n            image = ImageFile(fp)\n            self.assertEqual(image.width, None)\n            self.assertEqual(image.height, None)\n", "def test_get_image_dimensions_with_zlib_error(self):\n    # Arrange\n    file_path = NamedTemporaryFile(mode='wb', suffix='.jpg')\n    file_path.write(b'\\xff\\xd8\\xff')\n    file_path.write(b'\\x00' * 10)  # Not enough bytes to read a zlib block\n    file_path.seek(0)\n\n    # Act\n    dimensions = get_image_dimensions(file_path)\n\n    # Assert\n    self.assertIsNone(dimensions)\n", "    def test_get_image_dimensions_empty_file(self):\n        # Test an empty file\n        file = ContentFile('')\n        self.assertIsNone(file.width)\n        self.assertIsNone(file.height)\n"], "sample_522": ["def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    assert axs[0, 0].get_position().y1 == 0.8\n    assert axs[1, 1].get_position().x0 == 0.1\n    fig.subplots_adjust(left=None, right=None, top=None, bottom=None)\n    assert axs[0, 0].get_position().y1 == 0.9\n    assert axs[1, 1].get_position().x0 == 0.9\n    fig.subplots_adjust(left=0.1, right=0.9, top=None, bottom=None)\n    assert axs[0, 0].get_position().y1 == 0.9\n    assert axs[1, 1].get_position().x0 == 0.1\n", "def test_subplotpars():\n    fig = plt.figure()\n    subplotpars = fig.subplotpars\n    # Test default values\n    assert subplotpars.left == 0.125\n    assert subplotpars.right == 0.9\n    assert subplotpars.bottom == 0.125\n    assert subplotpars.top == 0.9\n    assert subplotpars.wspace == 0.2\n    assert subplotpars.hspace == 0.2\n\n    # Test setting values\n    subplotpars.left = 0.1\n    assert subplotpars.left == 0.1\n    subplotpars.right = 0.8\n    assert subplotpars.right == 0.8\n    subplotpars.bottom = 0.1\n    assert subplotpars.bottom == 0.1\n    subplotpars.top = 0.8\n    assert subplotpars.top == 0.8\n    subplotpars.wspace = 0.1\n    assert subplotpars.wspace == 0.1\n    subplotpars.hspace = 0.1\n    assert subplotpars.hspace == 0.1\n\n    # Test validation\n    try:\n        subplotpars.left = 0.9\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        subplotpars.bottom = 0.9\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        subplotpars.right = 0.125\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        subplotpars.top = 0.125\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        subplotpars.wspace = 10\n        assert False\n    except ValueError:\n        pass\n\n    try:\n        subplotpars.hspace = 10\n        assert False\n    except ValueError:\n        pass\n\n    # Test updating values\n    subplotpars.update(left=0.2, bottom=0.2, right=0.8, top=0.8,\n                      wspace=0.2, hspace=0.2)\n    assert subplotpars.left == 0.2\n    assert subplotpars.right == 0.8\n    assert subplotpars.bottom == 0.2\n    assert subplotpars.top == 0.8\n    assert subplotpars.wspace == 0.2\n    assert subplotpars.hspace == 0.2\n", "def test_subfigure_subplot():\n    # Create a figure\n    fig = plt.figure(figsize=(8, 6))\n    # Add a subfigure\n    sfig = fig.add_subfigure(0.25, 0.25, 0.75, 0.75)\n    # Add a subplot to the subfigure\n    ax = sfig.add_subplot(111)\n    # Test that the subplot is correctly added to the subfigure\n    assert len(sfig.axes) == 1\n    assert ax in sfig.axes\n    # Test that the subplot is correctly added to the figure\n    assert ax in fig.axes\n    # Test that the subplot is correctly aligned with the subfigure\n    assert ax.get_subplotspec().get_position(fig).allclose(Bbox.from_bounds(0.25, 0.25, 0.75, 0.75))\n", "def test_subplots_adjust_empty_figure():\n    fig = plt.figure()\n    assert fig.get_figwidth() == 6.4\n    assert fig.get_figheight() == 4.8\n\n    # Test default values\n    fig.subplots_adjust()\n    assert fig.subplotpars.left == 0.125\n    assert fig.subplotpars.bottom == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n\n    # Test custom values\n    fig.subplots_adjust(left=0.15, bottom=0.05)\n    assert fig.subplotpars.left == 0.15\n    assert fig.subplotpars.bottom == 0.05\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.9\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n\n    # Test custom values with different numbers of rows and columns\n    fig.subplots_adjust(left=0.1, bottom=0.01, right=0.9, top=0.95)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.bottom == 0.01\n    assert fig.subplotpars.right == 0.9\n    assert fig.subplotpars.top == 0.95\n    assert fig.subplotpars.wspace == 0.2\n    assert fig.subplotpars.hspace == 0.2\n", "def test_tight_layout():\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].set_title('Title 1')\n    axes[0, 1].set_title('Title 2')\n    axes[1, 0].set_title('Title 3')\n    axes[1, 1].set_title('Title 4')\n    fig.tight_layout()\n    fig.canvas.draw()\n    assert axes[0, 0].get_window_extent().width > 100\n    assert axes[0, 0].get_window_extent().height > 50\n    assert axes[0, 0].get_window_extent().x0 > 50\n    assert axes[0, 0].get_window_extent().y0 > 50\n"], "sample_9": ["def test_html_reader_raw_html_cols():\n    \"\"\"\n    Test the raw_html_cols functionality.\n    \"\"\"\n    html = \"\"\"<html>\n    <body>\n    <table id='my_table'>\n    <tr><th>col1</th><th>col2</th></tr>\n    <tr><td>1 <a href='http://www.google.com'>google</a> 2</td><td>3</td></tr>\n    </table>\n    </body>\n    </html>\"\"\"\n\n    lines = [SoupString(x) for x in html.split('\\n')]\n    reader = html.HTML(htmldict={'table_id': 'my_table', 'raw_html_cols': 'col1'})\n    table = reader.read(lines)\n\n    assert len(table.columns) == 2\n    assert table['col1'].dtype == 'object'\n    assert table['col1'].data[0] == '1 <a href=\"http://www.google.com\">google</a> 2'\n    assert table['col2'].dtype == int\n    assert table['col2'].data[0] == 3\n", "def test_html_writer_multicol():\n    html_dict = {'multicol': False}\n    reader = html.HTML(html_dict)\n    table = Table([[1, 2], [3, 4], [5, 6]], names=['a', 'b'])\n    output = reader.write(table)\n    assert output[0].count('<td>') == 6\n    assert output[0].count('</td>') == 6\n", "def test_html_table_id_string():\n    \"\"\"\n    Test that HTML table id string is correctly identified.\n    \"\"\"\n    table_data = \"\"\"<html><body><table id=\"my_table\">\n                    <tr><th>Col1</th><th>Col2</th></tr>\n                    <tr><td>1</td><td>2</td></tr></table></body></html>\"\"\"\n\n    html_reader = html.HTML(htmldict={'table_id': 'my_table'})\n    lines = [SoupString(table_data)]\n    lines_reader = html_reader.process_lines(lines)\n    header_lines = [next(lines_reader)]\n    data_lines = list(lines_reader)\n\n    assert len(data_lines) == 1\n    assert data_lines[0] == ['1', '2']\n", "def test_fill_values_empty_mask():\n    \"\"\"\n    Test that fill_values correctly handles masked columns with empty fill values.\n    \"\"\"\n    # Create a column with a mask and empty fill values\n    col = core.Column(np.array([1, 2, 3]), mask=[True, False, True])\n    col.fill_values = {'': ''}\n\n    html_table = html.HTML(html={'multicol': True})\n    table = Table({'a': col})\n    result = html_table.write(table)\n    expected = ['''<table id=\"\" class=\"\">", "def test_html_read_multidim_table():\n    # Set up HTML table with multidimensional columns\n    html_content = \"\"\"\n    <html>\n        <body>\n            <table>\n                <tr>\n                    <th>Column 1</th>\n                    <th colspan=\"2\">Column 2</th>\n                </tr>\n                <tr>\n                    <td>Value 1</td>\n                    <td>Value 2</td>\n                    <td>Value 3</td>\n                </tr>\n            </table>\n        </body>\n    </html>\n    \"\"\"\n\n    reader = html.HTML(htmldict={'multicol': True})\n    inputter = reader.inputter_class()\n    soup = BeautifulSoup(html_content, 'html.parser')\n    inputter.html = reader.html\n    inputter.process_lines([soup])\n\n    # Split the HTML content into rows\n    splitter = reader.splitter_class()\n    rows = list(splitter(inputter.out))\n\n    # Check the shape of the table\n    table = Table(rows)\n    assert table.shape == (1, 3)\n\n    # Check the column names and data\n    assert table.colnames == ['Column 1', ('Column 2', 2)]\n    assert np.all(table['Column 1'] == ['Value 1'])\n    assert np.all(table['Column 2'] == ['Value 2', 'Value 3'])\n"], "sample_583": ["    def test_posify_mask_indexer(self):\n        arr = np.array([1, -1, 2, -1, 3])\n        indexer = indexing.BasicIndexer((slice(None), slice(1, 3)))\n        indexer = indexing.posify_mask_indexer(indexer)\n        assert indexer == indexing.BasicIndexer((slice(None), slice(2, 4)))\n        assert_array_equal(arr[indexer], np.array([1, 2, 3, np.nan, np.nan]))\n", "    def test_posify_mask_indexer(self):\n        key = VectorizedIndexer(np.array([0, 1, 2]), np.array([0, 1, 2]))\n        indexer = posify_mask_indexer(key)\n        assert isinstance(indexer, VectorizedIndexer)\n        assert (indexer.tuple == np.array([0, 1, 1])).all()\n\n        key = OuterIndexer((np.array([0, 1, 2]), np.array([0, 1, 2])))\n        indexer = posify_mask_indexer(key)\n        assert isinstance(indexer, OuterIndexer)\n        assert (indexer.tuple[0] == np.array([0, 1, 1])).all()\n        assert (indexer.tuple[1] == np.array([0, 1, 2])).all()\n\n        key = BasicIndexer((0, 1, -1))\n        indexer = posify_mask_indexer(key)\n        assert isinstance(indexer, BasicIndexer)\n        assert (indexer.tuple == (0, 1, 0)).all()\n", "    def test_lazily_outer_indexed_array_transpose(self):\n        x = Variable((\"x\", (1, 2, 3), np.arange(6)))\n        idx = indexing.OuterIndexer((slice(None), slice(1, 2), 2))\n        la = indexing.LazilyOuterIndexedArray(x, idx)\n        result = la.transpose((1, 0, 2))\n        assert_array_equal(result.shape, (2, 1, 3))\n", "def test_indexer_dask_array_behavior():\n    da = DataArray([1, 2, 3], dims=['x'], coords={'x': [0, 1, 2]}, name='y')\n    dask_array = da.to_dask()\n\n    dask_array[B((0, 1, 2))].compute()\n    dask_array[B((0, 1, 2))].compute().execute()\n\n    with raises_regex(ValueError, \"item assignment on a dask array is not supported\"):\n        dask_array[B((0, 1, 2))] = [4, 5, 6]\n\n    dask_array[B(slice(0, 2))].compute()\n    dask_array[B(slice(0, 2))].compute().execute()\n\n    with raises_regex(ValueError, \"item assignment on a dask array is not supported\"):\n        dask_array[B(slice(0, 2))] = [4, 5]\n\n    dask_array[B((slice(0, 2), 1))].compute()\n    dask_array[B((slice(0, 2), 1))].compute().execute()\n\n    with raises_regex(ValueError, \"item assignment on a dask array is not supported\"):\n        dask_array[B((slice(0, 2), 1))][1] = 4\n\n    dask_array[B((0, slice(0, 2)))] = 5\n    dask_array[B((0, slice(0, 2)))] = [6, 7]\n\n    with raises_regex(ValueError, \"item assignment on a dask array is not supported\"):\n        dask_array[B((0, slice(0, 2)))] = [8, 9, 10]\n", "    def test_explicitly_indexed_ndaarray(self):\n        # https://github.com/pydata/xarray/issues/2316\n        da = DataArray(np.arange(5), dims=['x'])\n        result = da[~da > 3]\n        assert_array_equal(result.values, [0, 1, 2, 3])\n"], "sample_872": ["def test_ndcg_score_multiclass_multilabel():\n    \"\"\"Test DCG score for multiclass and multilabel problems.\"\"\"\n    X, y = make_multilabel_classification(random_state=0)\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    probas_pred = clf.predict_proba(X)\n    y_pred = clf.predict(X)\n    y_true = y\n    \n    # test the case where k is smaller than the number of labels\n    # and where k is larger than the number of labels\n    ndcg_score(y_true, probas_pred, k=2)\n    ndcg_score(y_true, probas_pred, k=5)\n    \n    # test the case where k is None\n    ndcg_score(y_true, probas_pred, k=None)\n    \n    # test the case where sample_weight is None\n    ndcg_score(y_true, probas_pred, sample_weight=None)\n    \n    # test the case where sample_weight is not None\n    sample_weight = np.random.rand(len(y))\n    ndcg_score(y_true, probas_pred, sample_weight=sample_weight)\n    \n    # test the case where ignore_ties is True\n    ndcg_score(y_true, probas_pred, ignore_ties=True)\n    \n    # test the case where ignore_ties is False\n    ndcg_score(y_true, probas_pred, ignore_ties=False)\n", "def test_coverage_error_all_labels():\n    y_true = np.array([[1, 1, 1], [0, 0, 1]])\n    y_score = np.array([[0.5, 0.3, 0.2], [0.7, 0.4, 0.3]])\n    with pytest.raises(ValueError):\n        coverage_error(y_true, y_score)\n", "def test_roc_auc_score_average(average, expected_average):\n    X, y = datasets.load_iris(return_X_y=True)\n    y_pred = LogisticRegression(solver=\"liblinear\").fit(X, y).predict_proba(X)\n    assert roc_auc_score(y, y_pred, average=average) == \"macro\"\n    assert roc_auc_score(y, y_pred, average=average, multi_class=multi_class) == expected_average\n", "def test_coverage_error():\n    \"\"\"Test coverage_error with various parameters\"\"\"\n    # Generate some toy data\n    X, y = make_multilabel_classification(random_state=0)\n\n    # Test with a perfect classifier\n    y_pred = X\n    y_score = np.ones_like(y)\n    assert_almost_equal(coverage_error(y, y_score), 0.0)\n\n    # Test with a classifier that always selects the second label\n    y_pred = np.zeros_like(y)\n    y_pred[:, 1] = 1\n    y_score = np.ones_like(y)\n    assert_almost_equal(coverage_error(y, y_score), 1.0)\n\n    # Test with a classifier that never selects the first label\n    y_pred = np.zeros_like(y)\n    y_pred[:, 0] = 1\n    y_score = np.ones_like(y)\n    assert_almost_equal(coverage_error(y, y_score), 0.0)\n\n    # Test with a classifier that selects different labels for each sample\n    y_pred = np.random.randint(0, 3, size=y.shape)\n    y_score = np.random.rand(y.shape[0], y.shape[1])\n    assert_almost_equal(coverage_error(y, y_score), 1.0)\n\n    # Test with weights\n    y_pred = np.zeros_like(y)\n    y_pred[:, 1] = 1\n    y_score = np.ones_like(y)\n    sample_weight = np.array([1, 2])\n    assert_almost_equal(coverage_error(y, y_score, sample_weight=sample_weight), 1.0)\n\n    # Test with sample weights that are zero\n    y_pred = np.zeros_like(y)\n    y_pred[:, 1] = 1\n    y_score = np.ones_like(y)\n    sample_weight = np.array([1, 0])\n    assert_almost_equal(coverage_error(y, y_score, sample_weight=sample_weight), 0.5)\n\n    # Test with sample weights that are zero for all samples\n    y_pred = np.zeros_like(y)\n    y_pred[:, 1] = 1\n    y_score = np.ones_like(y)\n    sample_weight = np.zeros_like(y)\n    assert_almost_equal(coverage_error(y, y_score, sample_weight=sample_weight), 0.0)\n", "def test_dcg_score_multiclass(y_type):\n    \"\"\"Test DCG score on multi-output multi-class problems.\"\"\"\n    rng = np.random.RandomState(0)\n    n_samples, n_features = 10, 2\n    X = rng.rand(n_samples, n_features)\n    y_true = rng.randint(0, 2, size=(n_samples, n_features))\n    y_score = rng.rand(n_samples, n_features)\n\n    y_true = y_true.astype(y_type)\n    y_score = y_score.astype(y_type)\n\n    dcg = dcg_score(y_true, y_score)\n    assert dcg >= 0.0\n    assert dcg <= 1.0\n"], "sample_620": ["def test_concat_compat_override():\n    da1 = DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    da2 = DataArray(np.array([4, 5, 6]), dims=[\"x\"])\n\n    da = concat([da1, da2], \"x\", compat=\"override\")\n\n    assert da.equals(da1)\n", "def test_concat_compat_equals_with_different_coords():\n    da1 = DataArray([1, 2], dims=[\"x\"], coords={\"x\": [0, 1]})\n    da2 = DataArray([3, 4], dims=[\"x\"], coords={\"x\": [0, 1]})\n    da3 = DataArray([5, 6], dims=[\"x\"], coords={\"x\": [0, 1]})\n\n    assert_equal(\n        concat([da1, da2, da3], \"x\", compat=\"equals\"), DataArray([1, 2, 3, 4, 5, 6], dims=[\"x\"])\n    )\n\n    da1 = DataArray([1, 2], dims=[\"x\"], coords={\"x\": [0, 1]})\n    da2 = DataArray([3, 4], dims=[\"x\"], coords={\"x\": [0, 1]})\n    da3 = DataArray([5, 6], dims=[\"x\"], coords={\"x\": [0, 1]})\n    ds1 = Dataset({\"x\": da1})\n    ds2 = Dataset({\"x\": da2})\n    ds3 = Dataset({\"x\": da3})\n\n    assert_equal(\n        concat([ds1, ds2, ds3], \"x\", compat=\"equals\"), Dataset({\"x\": DataArray([1, 2, 3, 4, 5, 6], dims=[\"x\"])}),\n    )\n\n    da1 = DataArray([1, 2], dims=[\"x\"], coords={\"x\": [0, 1]})\n    da2 = DataArray([3, 4], dims=[\"x\"], coords={\"x\": [0, 1]})\n    ds1 = Dataset({\"x\": da1})\n    ds2 = Dataset({\"x\": da2})\n\n    assert_equal(\n        concat([ds1, ds2], \"x\", coords=\"different\", compat=\"equals\"), Dataset({\"x\": DataArray([1, 2, 3, 4], dims=[\"x\"])}),\n    )\n", "def test_concat_compat_equals():\n    ds1 = Dataset(\n        {\n            \"x\": (\"y\", np.arange(4)),\n            \"z\": (\"x\", np.arange(4)),\n        }\n    )\n\n    ds2 = Dataset(\n        {\n            \"x\": (\"y\", np.arange(4)),\n            \"w\": (\"x\", np.arange(4)),\n        }\n    )\n\n    ds3 = Dataset(\n        {\n            \"x\": (\"y\", np.arange(4)),\n            \"z\": (\"x\", np.arange(4)),\n            \"y\": (\"x\", np.arange(4)),\n        }\n    )\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2, ds3], \"x\", compat=\"equals\")\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2, ds3], \"x\", compat=\"identical\")\n\n    with pytest.raises(ValueError):\n        concat([ds1, ds2, ds3], \"x\", compat=\"no_conflicts\")\n\n    # check that variables not present in all datasets are not concatenated\n    ds2[\"w\"] = (\"x\", np.arange(4))\n    result = concat([ds1, ds2, ds3], \"x\", coords=\"minimal\", compat=\"equals\")\n    assert result.coords == {\"x\": (\"x\", pd.Index([0, 1, 2, 3]))}\n    assert result.data_vars == {\"z\": (\"x\", np.arange(4))}\n", "def test_concatenate_nonexistent_dims():\n    da1 = DataArray(np.random.rand(3, 4), dims=[\"x\", \"y\"])\n    da2 = DataArray(np.random.rand(3, 4), dims=[\"x\", \"y\"])\n    ds = concat([da1, da2], dim=\"z\")\n    assert \"z\" in ds.dims\n    assert len(ds.dims) == 3\n", "def test_concat_dimensions():\n    \"\"\"Tests that the dimensions are handled correctly in the result of concat.\n\n    This test is taken from the pandas' test_concat_dimensions function.\n\n    \"\"\"\n    # TODO: Move this test to pandas\n\n    da1 = DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = DataArray([4, 5, 6], dims=[\"x\"])\n    da3 = DataArray([7, 8, 9], dims=[\"y\"])\n\n    # Test default dim\n    result = concat([da1, da2])\n    assert result.dims == [\"x\"]\n\n    # Test new dim\n    result = concat([da1, da2], dim=\"new_dim\")\n    assert result.dims == [\"new_dim\", \"x\"]\n\n    # Test existing dim\n    result = concat([da1, da3], dim=\"x\")\n    assert result.dims == [\"x\"]\n\n    # Test that the data variables are not changed\n    result = concat([da1, da2])\n    assert result.data_vars == {\"x\": result[\"x\"]}\n\n    # Test that the dimension names are not changed\n    result = concat([da1, da3], dim=\"new_dim\")\n    assert result.data_vars[\"new_dim\"].dims == [\"new_dim\"]\n\n    # Test that the dimension size is calculated correctly\n    da1 = DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = DataArray([4, 5, 6], dims=[\"x\"])\n    result = concat([da1, da2])\n    assert result.dims[\"x\"] == 6\n\n    # Test that the dimension is not added if it's already present\n    ds1 = Dataset({\"x\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"x\": (\"x\", [4, 5, 6])})\n    result = concat([ds1, ds2], dim=\"x\")\n    assert result.dims == [\"x\"]\n\n    # Test that the dimension is added if it's not present\n    ds1 = Dataset({\"x\": (\"x\", [1, 2, 3])})\n    ds2 = Dataset({\"y\": (\"y\", [4, 5, 6])})\n    result = concat([ds1, ds2], dim=\"new_dim\")\n    assert result.dims =="], "sample_131": ["    def test_create_test_db_with_serialize_false(self):\n        test_connection = self.get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = False\n        test_connection.settings_dict['TEST']['SERIALIZE'] = False\n        creation = BaseDatabaseCreation(test_connection)\n        with self.assertRaises(Exception):\n            creation.create_test_db()\n", "    def get_connection_copy(self):\n        # Get a copy of the default connection. (Can't use django.db.connection\n        # because it'll modify the default connection itself.)\n        test_connection = copy.copy(connections[DEFAULT_DB_ALIAS])\n        test_connection.settings_dict = copy.copy(connections[DEFAULT_DB_ALIAS].settings_dict)\n        return test_connection\n", "    def test_clone_test_db_keeps_database_name(self):\n        # Given\n        test_connection = self.get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_database_name'\n\n        # When\n        test_db_creation = BaseDatabaseCreation(test_connection)\n        test_db_creation._clone_test_db(suffix='clone_suffix', verbosity=1, keepdb=True)\n\n        # Then\n        self.assertEqual(test_db_creation.connection.settings_dict['NAME'], 'test_database_name_clone_suffix')\n", "    def test_clone_db_create(self):\n        # Given\n        connection = self.get_connection_copy()\n        db_creation = BaseDatabaseCreation(connection)\n        db_creation._get_test_db_name = mock.Mock(return_value='test_db')\n\n        # When\n        db_creation.clone_test_db(1, verbosity=1, autoclobber=False, keepdb=False)\n\n        # Then\n        db_creation._clone_test_db.assert_called_once_with(1, 1, False)\n", "    def test_clone_test_db_suffix(self):\n        # Test that clone_test_db() uses the correct suffix in the\n        # settings dictionary for the new test database\n        connection = self.get_connection_copy()\n        creation = BaseDatabaseCreation(connection)\n        original_settings_dict = creation.connection.settings_dict\n        creation.clone_test_db('clone_suffix')\n        self.assertIn('clone_suffix', creation.connection.settings_dict['NAME'])\n"], "sample_392": ["def test_json_field_deconstruct(self):\n    json_field = JSONModel.json_field\n    name, path, args, kwargs = json_field.deconstruct()\n    self.assertEqual(name, 'json_field')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {})\n\n    json_field = JSONModel.json_field_with_encoder\n    name, path, args, kwargs = json_field.deconstruct()\n    self.assertEqual(name, 'json_field_with_encoder')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'encoder': CustomJSONEncoder})\n\n    json_field = JSONModel.json_field_with_decoder\n    name, path, args, kwargs = json_field.deconstruct()\n    self.assertEqual(name, 'json_field_with_decoder')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'decoder': CustomJSONDecoder})\n\n    json_field = JSONModel.json_field_with_encoder_and_decoder\n    name, path, args, kwargs = json_field.deconstruct()\n    self.assertEqual(name, 'json_field_with_encoder_and_decoder')\n    self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n    self.assertEqual(args, ())\n    self.assertEqual(kwargs, {'encoder': CustomJSONEncoder, 'decoder': CustomJSONDecoder})\n", "    def test_key_transform_init(self):\n        key_transform = KeyTransform('key_name')\n        self.assertEqual(key_transform.key_name, 'key_name')\n        self.assertIsNone(key_transform.source_expressions)\n        self.assertEqual(key_transform.extra, {})\n", "    def test_value_too_large(self):\n        model = JSONModel()\n        with self.assertRaises(ValidationError):\n            model.json_field = {\n                \"a\": {\"b\": {\"c\": {\"d\": {\"e\": {\"f\": {\"g\": {\"h\": {\"i\": {\"j\": \"k\"}}}}}}}}}}\n            }\n", "    def test_json_encoder(self):\n        json_model = JSONModel()\n        json_model.json_field = {\n            \"key\": \"value\",\n            \"list_key\": [\"value1\", \"value2\"],\n        }\n        json_model.save()\n        json_field_value = JSONModel.objects.get(id=json_model.id).json_field\n        json_encoder = json.dumps(json_field_value, cls=DjangoJSONEncoder)\n        expected_json = '{\"key\": \"value\", \"list_key\": [\"value1\", \"value2\"]}'\n        self.assertEqual(json_encoder, expected_json)\n", "    def test_contains_lookup_supports_mysql(self):\n        # This test case is to ensure that the contains lookup is supported on MySQL\n        # This is to ensure the feature test is working as expected\n        from django.db.models import Q\n        from .models import JSONModel\n\n        q = Q(JSONModel.json_field__contains={\"key1\": \"value1\", \"key2\": \"value2\"})\n        qs = JSONModel.objects.filter(q)\n        self.assertEqual(qs.count(), 1)\n"], "sample_869": ["def test_precision_score_binary():\n    y_true = [0, 1, 0, 0, 1, 0]\n    y_pred = [0, 1, 0, 0, 0, 1]\n    assert precision_score(y_true, y_pred) == 0.5\n", "def test_multilabel_confusion_matrix():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 0], [0, 1, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred)\n    expected_MCM = np.array([[[1, 0], [0, 1]],\n                            [[1, 0], [0, 1]],\n                            [[0, 1], [1, 0]]])\n    assert_array_equal(MCM, expected_MCM)\n    assert MCM.shape == (3, 2, 2)\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 1], [0, 1, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_MCM = np.array([[[0, 0], [0, 0]],\n                            [[0, 0], [0, 0]],\n                            [[1, 1], [0, 0]]])\n    assert_array_equal(MCM, expected_MCM)\n    assert MCM.shape == (3, 2, 2)\n\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_pred = np.array([[1, 0, 1], [0, 1, 0]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, labels=[0, 1, 2])\n    expected_MCM = np.array([[[1, 0], [0, 1]],\n                            [[0, 1], [1, 0]],\n                            [[0, 0], [0, 0]]])\n    assert_array_equal(MCM, expected_MCM)\n    assert MCM.shape == (3, 2, 2)\n\n    y_true = np.array([[0, 1], [1, 1]])\n    y_pred = np.array([[0, 0], [1, 1]])\n    MCM = multilabel_confusion_matrix(y_true, y_pred, samplewise=True)\n    expected_MCM = np.array([[[0, 1], [0, 0", "def test_brier_score_loss_categorical_labels():\n    y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    with pytest.raises(ValueError):\n        brier_score_loss(y_true, y_prob)\n", "def test_multilabel_confusion_matrix_samplewise():\n    # Create a multilabel classification problem\n    X, y = make_multilabel_classification(n_samples=10, n_features=5, n_classes=3, n_labels=2, random_state=0)\n    y_pred = np.random.randint(0, 2, size=(10, 3))\n\n    # Test with sample-wise option\n    MCM = multilabel_confusion_matrix(y, y_pred, samplewise=True)\n    assert MCM.shape == (10, 2, 2)\n\n    # Test with sample-wise option and sample weights\n    sample_weight = np.random.randint(0, 2, size=10)\n    MCM = multilabel_confusion_matrix(y, y_pred, sample_weight=sample_weight, samplewise=True)\n    assert MCM.shape == (10, 2, 2)\n\n    # Test with sample-wise option and sample weights with a mix of zeros and ones\n    sample_weight = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n    MCM = multilabel_confusion_matrix(y, y_pred, sample_weight=sample_weight, samplewise=True)\n    assert MCM.shape == (10, 2, 2)\n\n    # Test with sample-wise option and sample weights with all zeros\n    sample_weight = np.zeros(10)\n    MCM = multilabel_confusion_matrix(y, y_pred, sample_weight=sample_weight, samplewise=True)\n    assert MCM.shape == (10, 2, 2)\n\n    # Test with sample-wise option and sample weights with all ones\n    sample_weight = np.ones(10)\n    MCM = multilabel_confusion_matrix(y, y_pred, sample_weight=sample_weight, samplewise=True)\n    assert MCM.shape == (10, 2, 2)\n\n    # Test with sample-wise option and sample weights with a mix of zeros and ones\n    # and labels=None\n    sample_weight = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n    MCM = multilabel_confusion_matrix(y, y_pred, labels=None, sample_weight=sample_weight, samplewise=True)\n    assert", "def test_multilabel_confusion_matrix_samplewise():\n    # Define a multilabel dataset\n    X, y = make_multilabel_classification(n_samples=100, n_features=5,\n                                          n_classes=3, n_labels=2,\n                                          random_state=0)\n\n    y_pred = np.random.randint(0, 3, size=y.shape)\n\n    MCM = multilabel_confusion_matrix(y, y_pred, samplewise=True)\n    assert MCM.shape == (100, 2, 2)\n\n    # Check that the confusion matrix is calculated for each sample\n    for i in range(100):\n        tn, fp, fn, tp = MCM[i]\n        assert tn + fp + fn + tp == 2\n"], "sample_267": ["    def test_database_integrity_check(self):\n        with self.assertRaises(IntegrityError):\n            Item.objects.create(name='test', description='test', author_id=None)\n", "    def test_savepoint_allowed(self):\n        conn = connection.cursor()\n        self.assertTrue(self.db._start_transaction_under_autocommit())\n        self.assertTrue(self.db._savepoint_allowed())\n        self.assertFalse(self.db._savepoint_allowed())\n", "    def test_allow_thread_sharing(self):\n        from django.db import connections\n        conn = connections['default']\n        self.assertTrue(conn.allow_thread_sharing)\n", "    def test_close(self):\n        conn = connection.connection\n        close_count = conn.close_count\n        connection.close()\n        self.assertEqual(conn.close_count, close_count + 1)\n", "    def test_get_connection_params(self):\n        # Verify the connection parameters are correct when settings.DATABASES is properly configured.\n        self.assertEqual(connection.settings_dict['NAME'], 'default')\n        self.assertEqual(connection.settings_dict['OPTIONS'], {})\n\n        # Verify the connection parameters are correct when settings.DATABASES is properly configured with options.\n        with override_settings(DATABASES={'default': {'NAME': 'default', 'OPTIONS': {'journal_mode': 'wal'}}}):\n            self.assertEqual(connection.settings_dict['NAME'], 'default')\n            self.assertEqual(connection.settings_dict['OPTIONS']['journal_mode'], 'wal')\n\n        # Verify the connection parameters are correct when the database is in memory.\n        with override_settings(DATABASES={'default': {'NAME': ':memory:', 'OPTIONS': {}}}, settings.DEBUG=True):\n            self.assertEqual(connection.settings_dict['NAME'], ':memory:')\n            self.assertEqual(connection.settings_dict['OPTIONS'], {})\n\n        # Verify the connection parameters are correct when the database is in memory with options.\n        with override_settings(DATABASES={'default': {'NAME': ':memory:', 'OPTIONS': {'journal_mode': 'wal', 'synchronous': 0}}}, settings.DEBUG=True):\n            self.assertEqual(connection.settings_dict['NAME'], ':memory:')\n            self.assertEqual(connection.settings_dict['OPTIONS']['journal_mode'], 'wal')\n            self.assertEqual(connection.settings_dict['OPTIONS']['synchronous'], 0)\n\n        # Verify the connection parameters are correct when settings.DATABASES is not properly configured.\n        with self.assertRaises(ImproperlyConfigured):\n            connection.settings_dict['NAME'] = None\n            connection.get_connection_params()\n\n        # Verify the connection parameters are correct when settings.DATABASES is properly configured but the NAME is not provided.\n        with override_settings(DATABASES={'default': {'OPTIONS': {}}}, settings.DEBUG=True):\n            with self.assertRaises(ImproperlyConfigured):\n                connection.get_connection_params()\n\n        # Verify the connection parameters are correct when the underlying database is not SQLite.\n        with override_settings(DATABASES={'default': {'NAME': 'default', 'ENGINE': 'django.db.backends.postgresql'}}):\n            with self.assertRaises(ImproperlyConfigured):\n                connection.get_connection_params()\n"], "sample_390": ["    def test_serve_view_with_directory_index(self):\n        from django.urls import path\n        from django.views.static import serve\n\n        # Create a temporary directory for testing\n        temp_dir = Path(__file__).parent / \"temp_dir\"\n        temp_dir.mkdir()\n        (temp_dir / \"subdir\").mkdir()\n        (temp_dir / \"subdir\" / \"file.txt\").touch()\n\n        # Define a URL pattern for the serve view\n        url_pattern = path(\"subdir/file.txt\", serve, {\"document_root\": str(temp_dir)})\n\n        # Test the serve view with the directory index\n        response = self.client.get(url_pattern.resolve(path(\"/site_media/subdir/\")))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"Index of subdir/\", response.content.decode())\n\n        # Test the serve view with a file\n        response = self.client.get(url_pattern.resolve(path(\"/site_media/subdir/file.txt\")))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"file.txt\", response.content.decode())\n\n        # Test the serve view with a non-existent file\n        with self.assertRaises(Http404):\n            self.client.get(url_pattern.resolve(path(\"/site_media/subdir/non_existent_file.txt\")))\n\n        # Test the serve view with a directory\n        response = self.client.get(url_pattern.resolve(path(\"/site_media/subdir/\")))\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(\"Directory indexes are not allowed here.\", response.content.decode())\n\n        # Test the serve view with show_indexes=True\n        url_pattern_with_show_indexes = path(\"subdir/\", serve, {\"document_root\": str(temp_dir), \"show_indexes\": True})\n        response = self.client.get(url_pattern_with_show_indexes.resolve(path(\"/site_media/subdir/\")))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(\"Index of subdir/\", response.content.decode())\n\n        # Clean up the temporary directory\n        import shutil\n        shutil.rmtree(temp_dir)\n", "    def test_serve_file(self):\n        response = serve(request=self.client.request(), path='test.txt', document_root=media_dir)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'text/plain')\n", "def test_directory_index_template_rendering(self):\n    from django.template import TemplateDoesNotExist\n\n    with self.assertRaises(TemplateDoesNotExist):\n        directory_index(\"\", Path(\"/\"))\n\n    with self.assertRaises(TemplateDoesNotExist):\n        directory_index(\"path\", Path(\"/\"))\n\n    try:\n        t = loader.select_template([\"static/directory_index.html\", \"static/directory_index\"])\n    except TemplateDoesNotExist:\n        pass\n    else:\n        t.render({\"directory\": \"/path\", \"file_list\": []})\n        t = None\n\n    if t is None:\n        c = Context()\n        t = Engine(libraries={\"i18n\": \"django.templatetags.i18n\"}).from_string(DEFAULT_DIRECTORY_INDEX_TEMPLATE)\n        self.assertIsInstance(t, Template)\n        response = t.render(c)\n        self.assertIn(\"Index of /path\", response)\n        self.assertIn(\"/path/\", response)\n        self.assertIn(\"/\", response)\n        self.assertNotIn(\".svn\", response)\n\n    fullpath = Path(\"/path/to/files\")\n    files = []\n    for f in fullpath.iterdir():\n        if not f.name.startswith(\".\"):\n            url = str(f.relative_to(fullpath))\n            if f.is_dir():\n                url += \"/\"\n            files.append(url)\n    c = {\"directory\": \"/path/to/\", \"file_list\": files}\n    response = t.render(c)\n    self.assertIn(\"Index of /path/to/\", response)\n    self.assertIn(\"/path/to/\", response)\n    self.assertIn(\"/path/to/file1\", response)\n    self.assertIn(\"/path/to/file2\", response)\n    self.assertIn(\"/\", response)\n    self.assertNotIn(\".svn\", response)\n\n", "def test_serve_handles_non_existent_directory_index_template(self):\n    \"\"\"\n    Test that serve() handles the case when the directory_index.html template does not exist.\n    \"\"\"\n    with override_settings(DEBUG=True, ROOT_URLCONF=\"view_tests.urls\"):\n        response = self.client.get(f\"/{self.prefix}/non_existent_dir/\")\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateNotUsed(\"non_existent_dir\")\n", "def test_directory_index_template_override(self):\n    \"\"\"\n    Test that a custom directory index template is used when available.\n    \"\"\"\n    # Create a custom directory index template\n    path = Path(\"static/directory_index.html\")\n    path.parent.mkdir(exist_ok=True)\n    path.write_text(DEFAULT_DIRECTORY_INDEX_TEMPLATE.replace(\"{% blocktranslate %}Index of {{ directory }}{% endblocktranslate %}\", \"My custom index of {{ directory }}\"))\n\n    # Set up the URL resolver to use the custom template\n    from django.urls import path, re_path\n    from django.urls.resolvers import URLPattern\n    from django.views.static import serve\n\n        return directory_index(path, document_root)\n\n    urlpatterns = [\n        path('static/<path:path>', custom_serve, {'document_root': media_dir, 'show_indexes': True})\n    ]\n\n    # Test that the custom template is used\n    response = self.client.get(f\"{self.prefix}/static/\")\n    self.assertTemplateUsed(response, \"directory_index.html\")\n\n    # Test that the custom index is rendered correctly\n    self.assertContains(response, \"My custom index of /\")\n\n    # Clean up\n    path.unlink()\n"], "sample_69": ["    def test_update_watches(self):\n        reloader = autoreload.WatchmanReloader()\n        reloader.directory_globs[Path('/tmp/test')] = {r'\\*'}\n        reloader.extra_files = {Path('/tmp/test/file.txt')}\n        reloader.update_watches()\n        self.assertIn('/tmp/test', reloader.client.subs)\n        self.assertIn('glob:/tmp/test', reloader.client.subs)\n        self.assertIn('files:/tmp/test/file.txt', reloader.client.subs)\n        self.assertEqual(len(reloader.client.subs), 2)\n", "    def setUp(self):\n        self.py_path = tempfile.mkdtemp()\n        self.temp_dir = Path(self.py_path)\n        self.addCleanup(shutil.rmtree, self.py_path)\n", "    def test_iter_all_python_module_files_empty_sys_modules(self):\n        with contextlib.suppress(KeyError):\n            self.assertEqual(list(autoreload.iter_all_python_module_files()), [])\n", "    def test_watchman_subscription_cleanup(self):\n        # Arrange\n        with tempfile.TemporaryDirectory() as dirname:\n            path = Path(dirname) / 'test.py'\n            with open(path, 'w') as f:\n                f.write('import time\\n')\n                f.write('while True: pass\\n')\n            self.import_and_cleanup('test')\n\n            reloader = autoreload.WatchmanReloader()\n            reloader.watch_file(str(path))\n            reloader._subscribe_dir(path.parent, ['test.py'])\n\n            # Act\n            reloader._update_watches()\n            self.client = reloader.client\n            self.subscription = self.client.getSubscription('files:%s' % path.parent)\n\n            # Assert\n            self.assertIsNotNone(self.subscription)\n\n            # Act\n            self.client.close()\n            reloader.stop()\n\n            # Assert\n            with self.assertRaises(pywatchman.WatchmanError):\n                self.client.getSubscription('files:%s' % path.parent)\n", "    def test_iter_modules_and_files_returns_empty_set_if_no_modules(self):\n        # Ensure there are no modules loaded by the time iter_modules_and_files is called\n        self.clear_autoreload_caches()\n        sys.modules.clear()\n        self.assertEqual(autoreload.iter_modules_and_files(), set())\n"], "sample_115": ["    def setUp(self):\n        self.filter = SafeExceptionReporterFilter()\n", "    def setUp(self):\n        self.request = RequestFactory().get('/')\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        self.assertIn('DEBUG', settings_dict)\n        self.assertIn('SECRET_KEY', settings_dict)\n        self.assertEqual(settings_dict['SECRET_KEY'], '********************')\n", "    def setUp(self):\n        self.request = RequestFactory().get('/')\n        self.filter = SafeExceptionReporterFilter()\n", "    def test_sensitive_post_parameters(self):\n        from django.conf import settings\n        settings.DEBUG = True\n        request = RequestFactory().post('/test/', {'a': 1, 'b': 2})\n        filter = SafeExceptionReporterFilter()\n\n        # Test non-sensitive parameters\n        self.assertEqual(filter.get_post_parameters(request), request.POST)\n\n        # Test sensitive parameters\n        request.POST['a'] = 'sensitive'\n        filter.sensitive_post_parameters = ['a']\n        self.assertEqual(filter.get_post_parameters(request), {'b': 2})\n\n        # Test '__ALL__' parameter\n        request.POST['a'] = 'sensitive'\n        request.POST['b'] = 'not_sensitive'\n        filter.sensitive_post_parameters = '__ALL__'\n        self.assertEqual(filter.get_post_parameters(request), {})\n\n        # Test 'sensitive_post_parameters' attribute on request\n        request.sensitive_post_parameters = ['b']\n        self.assertEqual(filter.get_post_parameters(request), {'a': 'sensitive'})\n"], "sample_469": ["    def setUpTestData(cls):\n        cls.p1 = Publisher.objects.create(name=\"Apress\", num_awards=3)\n        cls.p2 = Publisher.objects.create(name=\"Sams\", num_awards=1)\n        cls.p3 = Publisher.objects.create(name=\"Prentice Hall\", num_awards=7)\n        cls.p4 = Publisher.objects.create(name=\"Morgan Kaufmann\", num_awards=9)\n", "    def setUpTestData(cls):\n        cls.author1 = Author.objects.create(name=\"John Doe\", age=30)\n        cls.author2 = Author.objects.create(name=\"Jane Doe\", age=25)\n        cls.author3 = Author.objects.create(name=\"John Smith\", age=40)\n\n        cls.book1 = Book.objects.create(\n            isbn=\"123456789\",\n            name=\"Book 1\",\n            pages=200,\n            rating=4.5,\n            price=Decimal(\"20.00\"),\n            contact=cls.author1,\n            publisher=Publisher.objects.create(name=\"Publisher 1\"),\n            pubdate=datetime.date(2020, 1, 1),\n        )\n        cls.book2 = Book.objects.create(\n            isbn=\"987654321\",\n            name=\"Book 2\",\n            pages=300,\n            rating=3.0,\n            price=Decimal(\"15.00\"),\n            contact=cls.author2,\n            publisher=Publisher.objects.create(name=\"Publisher 2\"),\n            pubdate=datetime.date(2020, 6, 6),\n        )\n\n        cls.book1.authors.add(cls.author1)\n        cls.book2.authors.add(cls.author2)\n", "    def test_annotation_expression_in_order_by(self):\n        query = Book.objects.all()\n        with self.assertRaises(FieldError):\n            query.order_by(Case(When(author=Author.objects.all(), then=F(\"id\"))))\n", "    def test_aggregate_annotations_with_subquery(self):\n        \"\"\"Test aggregate annotations with subquery.\"\"\"\n        # Query to count number of books each publisher has published\n        qs = Publisher.objects.annotate(\n            num_books=Count(\n                Subquery(\n                    Book.objects.filter(publisher=OuterRef(\"pk\")).values(\"pk\")\n                )\n            )\n        )\n        result = qs.annotate(\n            num_awards_plus_books=Case(\n                When(num_awards__gt=F(\"num_books\"), then=F(\"num_awards\") + F(\"num_books\")),\n                default=F(\"num_books\"),\n            )\n        ).values_list(\"name\", \"num_awards\", \"num_books\", \"num_awards_plus_books\")\n        self.assertEqual(\n            result,\n            [\n                (\n                    \"Apress\",\n                    3,\n                    3,\n                    6,\n                ),\n                (\n                    \"Sams\",\n                    1,\n                    1,\n                    2,\n                ),\n                (\"Prentice Hall\", 7, 2, 9),\n                (\"Morgan Kaufmann\", 9, 3, 12),\n                (\"Jonno's House of Books\", 0, 0, 0),\n            ],\n        )\n", "    def test_annotations_with_subquery(self):\n        qs = Author.objects.annotate(\n            average_age=Avg('age'),\n            max_age=Max('age'),\n            max_age_subquery=Subquery(Author.objects.order_by('-age')[:1]),\n        )\n        self.assertEqual(list(qs.values_list('average_age', 'max_age', 'max_age_subquery')), [\n            (34.4, 57, 57),\n            (34.0, 45, 45),\n            (33.5, 46, 46),\n            (33.5, 46, 46),\n            (34.0, 46, 46),\n            (34.5, 57, 57),\n            (26.5, 57, 57),\n            (34.0, 57, 57),\n            (36.5, 57, 57),\n            (32.0, 57, 57),\n        ])\n"], "sample_1208": ["def test_MatrixNormal_distribution_samples():\n    n, p = symbols('n p', positive=True, integer=True)\n    n, p = 2, 3\n    M = MatrixNormal('M', [[1, 2, 3], [4, 5, 6]], [1, 0, 0; 0, 1, 0; 0, 0, 1], [[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    X = MatrixSymbol('X', n, p)\n    # check sampling\n    samples = sample(M, size=(100,), library='scipy')\n    assert samples.shape == (100, n, p)\n", "    def test_MatrixGammaDistribution(self):\n        from sympy import MatrixSymbol, Matrix\n        from sympy.stats import MatrixGamma, density\n        # Test for valid input\n        A = MatrixSymbol('A', 2, 2)\n        alpha = 2\n        beta = 2\n        scale_matrix = Matrix([[2, 1], [1, 2]])\n        dist = MatrixGamma('X', alpha, beta, scale_matrix)\n        self.assertIsInstance(dist, MatrixGamma)\n        self.assertEqual(str(dist), 'MatrixGamma(X, 2, 2, Matrix([[2, 1], [1, 2]])')\n\n        # Test for invalid input\n        with raises(ValueError):\n            MatrixGamma('X', alpha, beta, 'not a matrix')\n\n        with raises(ValueError):\n            MatrixGamma('X', alpha, 'not a number', scale_matrix)\n\n        with raises(ValueError):\n            MatrixGamma('X', 'not a number', beta, scale_matrix)\n\n        with raises(ValueError):\n            MatrixGamma('X', alpha, beta, scale_matrix, extra_arg)\n\n        # Test for invalid input (non-positive alpha and beta)\n        with raises(ValueError):\n            MatrixGamma('X', 0, beta, scale_matrix)\n\n        with raises(ValueError):\n            MatrixGamma('X', alpha, 0, scale_matrix)\n\n        # Test for invalid input (non-positive definite scale matrix)\n        scale_matrix = Matrix([[1, 2], [3, 4]])\n        with raises(ValueError):\n            MatrixGamma('X', alpha, beta, scale_matrix)\n\n        # Test for invalid input (non-square scale matrix)\n        scale_matrix = Matrix([[1, 2, 3], [4, 5, 6]])\n        with raises(ValueError):\n            MatrixGamma('X', alpha, beta, scale_matrix)\n", "def test_MatrixPSpace_values():\n    # Test that values returns a set with a single RandomMatrixSymbol\n    M = MatrixPSpace('M', MatrixGamma(2, 1, 1, [[1, 0], [0, 1]]), 2, 2)\n    assert M.values == {M.value}\n\n    # Test that values returns a set with a single RandomMatrixSymbol when\n    # passing a distribution with a different dimension\n    M = MatrixPSpace('M', Wishart(2, 1, [[1, 0], [0, 1]]), 3, 3)\n    assert M.values == {M.value}\n\n    # Test that values raises a NotImplementedError for non-matrix distributions\n    M = MatrixPSpace('M', MatrixNormal(2, 1, 1, [[1, 0], [0, 1]]), 2, 2)\n    with raises(NotImplementedError):\n        M.values\n\n    # Test that values raises a NotImplementedError for non-matrix expressions\n    M = MatrixPSpace('M', MatrixGamma(2, 1, 1, [[1, 0], [0, 1]]), 2, 2)\n    with raises(NotImplementedError):\n        M.values(1)\n\n    # Test that values raises a NotImplementedError for multiple RandomMatrixSymbol\n    M = MatrixPSpace('M', MatrixGamma(2, 1, 1, [[1, 0], [0, 1]]), 2, 2)\n    M.value = [M.value, M.value]\n    with raises(NotImplementedError):\n        M.values\n", "def test_MatrixStudentT_density():\n    # Test MatrixStudentT density\n    from sympy import MatrixSymbol\n    from sympy.stats import MatrixStudentT\n    import sympy\n    from sympy.stats import density\n    import numpy as np\n    # Test cases for different values of nu\n    for nu in [1, 2, 3]:\n        M = MatrixStudentT('M', nu, [[1, 2]], [[1, 0], [0, 1]], [1])\n        X = MatrixSymbol('X', 2, 2)\n        # Test that the density of MatrixStudentT is correctly calculated\n        density_M = density(M)(X).doit()\n        assert isinstance(density_M, sympy.Expr)\n        # Test that the density is correctly simplified\n        simplified_density_M = sympy.simplify(density_M)\n        assert isinstance(simplified_density_M, sympy.Expr)\n        # Test that the density is correctly expanded\n        expanded_density_M = sympy.expand(density_M)\n        assert isinstance(expanded_density_M, sympy.Expr)\n        # Test that the density is correctly collected\n        collected_density_M = sympy.collect(density_M, sympy.symbols('X'))\n        assert isinstance(collected_density_M, sympy.Expr)\n        # Test that the density is correctly differentiated\n        diff_density_M = sympy.diff(density_M, X)\n        assert isinstance(diff_density_M, sympy.Expr)\n        # Test that the density is correctly integrated\n        integral_density_M = sympy.integrate(density_M, (X, 0, 1))\n        assert isinstance(integral_density_M, sympy.Expr)\n    # Test that the density of MatrixStudentT is correctly calculated for different values of location_matrix and scale_matrix\n    M1 = MatrixStudentT('M', 2, [[1, 2, 3], [4, 5, 6]], [[1, 0], [0, 1]], [1])\n    X = MatrixSymbol('X', 3, 3)\n    density_M1 = density(M1)(X).doit()\n    assert isinstance(density_M1, sympy.Expr)\n    M2 = MatrixStudentT('M', 2, [[7, 8], [9, 10], [11, 12]], [[13, 14], [15, 16]], [[17,", "def test_MatrixGamma_density():\n    from sympy.stats.distributions import MatrixGammaDistribution\n    from sympy.stats import density\n    from sympy import MatrixSymbol, ImmutableMatrix, Matrix, symbols\n    n, p = 2, 2\n    alpha, beta = symbols('alpha beta', positive=True)\n    X = MatrixSymbol('X', n, p)\n    M = MatrixGamma('M', alpha, beta, [[1, 0], [0, 1]])\n    # Test with square matrix\n    assert density(M)(X).doit() == exp(Trace(Matrix([\n        [-2/3,  1/3],\n        [ 1/3, -2/3]])*X)/beta)/beta**(2*alpha)*gamma(alpha)*gamma(alpha - 1/2)/(3**alpha*pi**0.5)\n\n    # Test with non-square matrix\n    raises(ValueError, lambda: MatrixGamma('M', alpha, beta, [[1, 2]]))\n\n    # Test with invalid alpha\n    raises(ValueError, lambda: MatrixGamma('M', alpha, beta, [[1, 1]]))\n\n    # Test with invalid beta\n    raises(ValueError, lambda: MatrixGamma('M', alpha, beta, [[1, 1]]))\n\n    # Test with invalid scale matrix\n    raises(ValueError, lambda: MatrixGamma('M', alpha, beta, [[1]]))\n\n    # Test with non-positive alpha\n    raises(ValueError, lambda: MatrixGamma('M', -alpha, beta, [[1, 1]]))\n\n    # Test with non-positive beta\n    raises(ValueError, lambda: MatrixGamma('M', alpha, -beta, [[1, 1]]))\n\n    # Test with non-positive definite matrix\n    raises(ValueError, lambda: MatrixGamma('M', alpha, beta, [[1, 1], [1, 0]]))\n"], "sample_30": ["def test_votable_file_repr():\n    votable = tree.VOTableFile()\n    result = str(votable)\n    assert \"VO\" in result\n    assert \"<VOTABLE>\" in result\n    assert \"</VOTABLE>\" in result\n\n    table = tree.Table(votable)\n    result = str(votable)\n    assert \"VO\" in result\n    assert \"<VOTABLE>\" in result\n    assert \"</VOTABLE>\" in result\n    assert \"<TABLE>\" in result\n    assert \"</TABLE>\" in result\n\n    field = tree.Field(votable, ID=\"field1\", name=\"Field 1\")\n    result = str(votable)\n    assert \"VO\" in result\n    assert \"<VOTABLE>\" in result\n    assert \"</VOTABLE>\" in result\n    assert \"<TABLE>\" in result\n    assert \"<FIELD ID='field1' name='Field 1'>\" in result\n    assert \"</FIELD>\" in result\n    assert \"</TABLE>\" in result\n\n    table = tree.Table(votable)\n    table.ID = \"table1\"\n    table.name = \"Table 1\"\n    result = str(votable)\n    assert \"VO\" in result\n    assert \"<VOTABLE>\" in result\n    assert \"</VOTABLE>\" in result\n    assert \"<TABLE ID='table1' name='Table 1'>\" in result\n    assert \"</TABLE>\" in result\n\n    field = tree.Field(votable, ID=\"field1\", name=\"Field 1\")\n    result = str(votable)\n    assert \"VO\" in result\n    assert \"<VOTABLE>\" in result\n    assert \"</VOTABLE>\" in result\n    assert \"<TABLE ID='table1' name='Table 1'>\" in result\n    assert \"<FIELD ID='field1' name='Field 1'>\" in result\n    assert \"</FIELD>\" in result\n    assert \"</TABLE>\" in result\n", "def test_table_from_table():\n    table = tree.Table(np.recarray([(1, 2), (3, 4)], dtype=[('a', 'i8'), ('b', 'i8')])\n    votable = tree.VOTableFile.from_table(table, table_id=\"foo\")\n    expected_xml = (\n        b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n        b'<!-- Produced with astropy.io.votable version '\n        b'-->\\n'\n        b'<VOTABLE xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n'\n        b'        xmlns=\"http://www.ivoa.net/xml/VOTable/v1.3\"\\n'\n        b'        xsi:schemaLocation=\"http://www.ivoa.net/xml/VOTable/v1.3 '\n        b'http://www.ivoa.net/xml/VOTable/VOTable-1.3.xsd\">\\n'\n        b'    <RESOURCE type=\"results\">\\n'\n        b'        <TABLE ID=\"foo\">\\n'\n        b'            <FIELD ID=\"a\" name=\"a\" datatype=\"int\" arraysize=\"1\">\\n'\n        b'                <VALUES>\\n'\n        b'                </VALUES>\\n'\n        b'            </FIELD>\\n'\n        b'            <FIELD ID=\"b\" name=\"b\" datatype=\"int\" arraysize=\"1\">\\n'\n        b'                <VALUES>\\n'\n        b'                </VALUES>\\n'\n        b'            </FIELD>\\n'\n        b'            <TABLEDATA>\\n'\n        b'            <TR>\\n'\n        b'                <TD>1</TD>\\n'\n        b'                <TD>2</TD>\\n'\n        b'            </TR>\\n'\n        b'            <TR>\\n'\n        b'                <TD>3</TD>\\n'\n        b'                <TD>4</TD>\\n'\n        b'            </TR>\\n'\n        b'            </TABLEDATA>\\n'\n        b'        </TABLE>\\n'\n        b'    </RESOURCE>\\n'\n        b'</VOTABLE>\\n'\n    )\n\n    assert votable.to_xml(io.BytesIO()) == expected_xml\n", "def test_validate_schema(tmp_path):\n    # This test checks if the validate_schema function in xmlutil correctly validates a file against a schema\n    # We create a sample VOTable file\n    votable_data = \"\"\"", "def test_table_from_table_no_name():\n    # Test that we can create a VOTableFile from an astropy table\n    # when no name is specified.\n    table = astropy.table.Table([[1, 2, 3]], names=['a', 'b', 'c'])\n    votable = tree.VOTableFile.from_table(table)\n    assert votable.tables[0].name is None\n    assert votable.tables[0].ID is None\n", "def test_parse_votable_with_group():\n    votable_string = \"\"\""], "sample_497": ["    def test_default_ticks_position(self):\n        fig, ax = plt.subplots()\n        ax.xaxis.set_major_locator(mticker.MaxNLocator(3))\n        assert ax.xaxis.get_ticks_position() == \"default\"\n", "    def test_get_ticks_position(self):\n        # Test XAxis\n        fig, ax = plt.subplots()\n        ax.set_xticks([1, 2, 3, 4])\n        ax.set_yticks([1, 2, 3, 4])\n        assert ax.xaxis.get_ticks_position() == \"default\"\n        assert ax.yaxis.get_ticks_position() == \"default\"\n\n        ax.set_xticks_position(\"top\")\n        assert ax.xaxis.get_ticks_position() == \"top\"\n\n        ax.set_xticks_position(\"bottom\")\n        assert ax.xaxis.get_ticks_position() == \"bottom\"\n\n        ax.set_yticks_position(\"right\")\n        assert ax.yaxis.get_ticks_position() == \"right\"\n\n        ax.set_yticks_position(\"left\")\n        assert ax.yaxis.get_ticks_position() == \"left\"\n\n        # Test YAxis\n        fig, ax = plt.subplots()\n        ax.set_xticks([1, 2, 3, 4])\n        ax.set_yticks([1, 2, 3, 4])\n        assert ax.yaxis.get_ticks_position() == \"default\"\n        assert ax.xaxis.get_ticks_position() == \"default\"\n\n        ax.set_yticks_position(\"top\")\n        assert ax.yaxis.get_ticks_position() == \"top\"\n\n        ax.set_yticks_position(\"bottom\")\n        assert ax.yaxis.get_ticks_position() == \"bottom\"\n\n        ax.set_xticks_position(\"right\")\n        assert ax.xaxis.get_ticks_position() == \"right\"\n\n        ax.set_xticks_position(\"left\")\n        assert ax.xaxis.get_ticks_position() == \"left\"\n", "    def test_get_label_text(self):\n        ax = plt.axes()\n        assert ax.get_label_text() == ''\n", "    def test_get_pad_pixels(self):\n        ax = plt.axes()\n        tick = XTick(ax, 0, '', size=10)\n        assert tick.get_pad_pixels() == 2.0\n        tick.set_pad(5)\n        assert tick.get_pad_pixels() == 5.0\n", "    def test_gridlines(self):\n        fig, ax = plt.subplots()\n        ax.grid(True, which='both')\n        ax.grid(True, which='major', linestyle='--')\n        ax.grid(True, which='minor', color='red')\n        # Test axis grid line properties\n        gridlines = ax.get_gridlines()\n        for i, line in enumerate(gridlines):\n            assert line.get_visible()\n            assert line.get_linestyle() == '-' if i % 2 == 0 else '--'\n            assert line.get_color() == 'black' if i % 2 == 0 else 'red'\n        # Test axis tick line properties\n        ticklines = ax.get_majorticklines()\n        for i, line in enumerate(ticklines):\n            assert line.get_visible()\n            assert line.get_marker() == 'o'\n            assert line.get_markersize() == 10\n            assert line.get_markeredgewidth() == 2\n        ticklines = ax.get_minorticklines()\n        for i, line in enumerate(ticklines):\n            assert line.get_visible()\n            assert line.get_marker() == 'o'\n            assert line.get_markersize() == 5\n            assert line.get_markeredgewidth() == 1\n"], "sample_597": ["def test_merge_core_priority_arg_override():\n    ds1 = xr.Dataset(\n        {\"x\": (\"y\", [1, 2])},\n        coords={\"y\": [0, 1]},\n    )\n    ds2 = xr.Dataset(\n        {\"x\": (\"y\", [3, 4])},\n        coords={\"y\": [0, 1]},\n    )\n    ds3 = xr.Dataset(\n        {\"x\": (\"y\", [5, 6])},\n        coords={\"y\": [0, 1]},\n    )\n    merged = merge.merge([ds1, ds2, ds3], priority_arg=1)\n    expected = xr.Dataset(\n        {\"x\": (\"y\", [1, 2])},\n        coords={\"y\": [0, 1]},\n    )\n    assert_identical(merged, expected)\n\n", "    def test_merge_core_matches_merge(self):\n        # The purpose of this test is to check that merge_core() and merge()\n        # produce the same results, i.e., that merge() is a wrapper around\n        # merge_core().\n        ds1 = xr.Dataset({'x': ('y', [1, 2])})\n        ds2 = xr.Dataset({'x': ('y', [3, 4])})\n\n        result_core = merge_core([ds1, ds2], compat='equals', join='outer')\n        result_merge = merge([ds1, ds2], compat='equals', join='outer')\n\n        assert_identical(result_core._asdict(), result_merge._asdict())\n", "def test_merge_core_coerce_pandas_values():\n    # Given\n    df = pd.DataFrame({\"x\": [1, 2]})\n    ds = xr.Dataset({\"x\": (\"x\", df[\"x\"])}).to_dataframe()\n    data = {\"x\": (\"x\", df[\"x\"]), \"y\": (\"x\", df[\"y\"])}\n\n    # When\n    result = merge.merge_core([ds, data], compat=\"override\")\n\n    # Then\n    expected_result = {\"x\": (\"x\", [1, 2]), \"y\": (\"x\", [3, 4])}\n    assert result._asdict() == expected_result._asdict()\n", "def test_merge_core_coord_names_empty(self):\n    data_vars = xr.Dataset({\"x\": (\"x\", [1, 2])})\n    coords = xr.Dataset({\"y\": (\"y\", [3, 4])})\n    result = merge.merge_core([data_vars, coords])\n    assert result.coord_names == set()\n", "def test_merge_core_on_incompatible_attributes():\n    \"\"\"Test that merge_core raises an error when merging variables with different\n    attributes when compat='identical'.\"\"\"\n    # Create a DataArray with a attribute\n    da1 = xr.DataArray(\n        np.arange(4).reshape((2, 2)),\n        dims=[\"x\", \"y\"],\n        attrs={\"foo\": \"bar\"},\n        name=\"var1\",\n    )\n\n    # Create another DataArray with the same name but different attribute\n    da2 = xr.DataArray(\n        np.arange(4).reshape((2, 2),\n                             dtype=np.float32),\n        dims=[\"x\", \"y\"],\n        attrs={\"foo\": \"baz\"},\n        name=\"var1\",\n    )\n\n    # Create a dataset\n    ds1 = da1.to_dataset()\n\n    # Try to merge the datasets with compat='identical'\n    with pytest.raises(MergeError) as e:\n        merge.merge([ds1, da2.to_dataset()])\n\n    assert str(e.value) == (\n        \"conflicting attribute values on combined variable 'var1':\\n\"\n        \"first value: {'foo': 'bar'}\\nsecond value: {'foo': 'baz'}\"\n    )\n"], "sample_280": ["    def test_aggregate_filter(self):\n        \"\"\"\n        Test aggregate filter functionality.\n        \"\"\"\n        from .models import Book\n\n        # Test with a simple filter\n        filtered_books = Book.objects.filter(publisher__name='Apress').aggregate(Avg('rating'))\n        self.assertEqual(filtered_books['rating__avg'], 4.5)\n\n        # Test with a filter that returns no results\n        filtered_books = Book.objects.filter(publisher__name='Unknown').aggregate(Avg('rating'))\n        self.assertIsNone(filtered_books['rating__avg'])\n\n        # Test with an empty query\n        filtered_books = Book.objects.none().aggregate(Avg('rating'))\n        self.assertIsNone(filtered_books['rating__avg'])\n\n        # Test with a filter that is not a valid expression\n        with self.assertRaises(FieldError):\n            Book.objects.filter(publisher__name='Apress').aggregate(Avg('non_existent_field'))\n\n        # Test with a filter that has an aggregate in it\n        with self.assertRaises(FieldError):\n            Book.objects.filter(publisher__name=Avg('rating')).aggregate(Avg('rating'))\n", "    def test_Aggregate__resolve_expression__raises_FieldError(self):\n        # Test that resolve_expression raises a FieldError when it contains an aggregate\n        from .models import Book\n        queryset = Book.objects.annotate(count=Count('id'))\n        expression = queryset.annotate(avg_rating=Avg('rating'))\n        with self.assertRaises(FieldError):\n            expression.resolve_expression()\n", "    def test_Sum_with_zero_value(self):\n        # Test that Sum with zero value doesn't raise an error\n        Sum(F('rating'), default=0).resolve_expression()\n", "    def test_Aggregate_resolve_expression_with_default_value(self):\n        author = Author.objects.aggregate(aggregate=Max('name'))\n        self.assertEqual(author['aggregate'], 'Wesley J. Chun')\n", "    def test_Avg_alias(self):\n        with CaptureQueriesContext() as ctx:\n            result = Author.objects.all().annotate(avg_age=Avg('age')).values_list('avg_age')\n            self.assertEqual(result, [(34.0,)])\n            self.assertEqual(len(ctx.captured_queries), 1)\n"], "sample_876": ["def test_MLPRegressor_score():\n    \"\"\"Regression test for MLPRegressor.score method.\"\"\"\n    regr = MLPRegressor(random_state=1)\n    X, y = make_regression(n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7)\n    regr.fit(X, y)\n    score = regr.score(X, y)\n    assert isinstance(score, float)\n", "def test_MLPClassifier_learning_rate():\n    \"\"\"Check that the learning rate schedule for the MLPClassifier is working correctly\"\"\"\n    X, y = make_classification(n_samples=200, n_features=10, n_informative=10, n_redundant=0, random_state=1)\n    X = MinMaxScaler().fit_transform(X)\n    clf = MLPClassifier(\n        hidden_layer_sizes=(100,),\n        activation=\"relu\",\n        solver=\"sgd\",\n        learning_rate_init=0.01,\n        max_iter=100,\n        learning_rate=\"invscaling\",\n        power_t=0.5,\n        random_state=1\n    )\n    clf.fit(X, y)\n    assert clf.learning_rate_ > 0.01\n    assert clf.t_ > 0\n    assert clf.loss_curve_\n    assert clf.best_loss_\n    assert clf.validation_scores_\n    assert clf.best_validation_score_\n", "    def test_loss_grad_lbfgs_values(self, activation, expected, solver, loss, expected_grad_values):\n        mlp = MLPClassifier(hidden_layer_sizes=(100,), activation=activation, solver=solver, loss=loss)\n        grad_values = mlp._loss_grad_lbfgs(np.ones(mlp._get_n_params()), np.ones((100, 1)), np.ones((100, 1)), [np.ones((100, 100)), np.ones((100, 100))], [np.ones((100, 100)), np.ones((100, 100))], [np.ones((100,)), np.ones((100,))])\n        assert_allclose(grad_values[1], expected)\n", "def test_partial_fit_multipass():\n    \"\"\"Test that partial_fit calls fit multiple times.\"\"\"\n    X, y = load_iris()\n    clf = MLPClassifier(max_iter=10)\n    clf.partial_fit(X[:10], y[:10])\n    clf.partial_fit(X[10:20], y[10:20])\n    clf.partial_fit(X[20:], y[20:])\n    assert clf.n_iter_ >= 10\n    assert len(clf.loss_curve_) >= 10\n", "def test_multilayer_perceptron_loss_gradient_approx(solver):\n    \"\"\"Test the loss gradient approximation of MLP.\"\"\"\n    X = np.array([[0, 0], [1, 1]])\n    y = np.array([0, 1])\n    model = MLPClassifier(hidden_layer_sizes=(10,), solver=solver)\n    model.fit(X, y)\n    y_pred = model._forward_pass_fast(X)\n\n    # Check that the loss is computed correctly\n    loss = np.mean(np.log(1 + np.exp(-y * y_pred)))\n    loss_func_name = model.loss\n    if loss_func_name == \"log_loss\" and model.out_activation_ == \"logistic\":\n        loss_func_name = \"binary_log_loss\"\n    loss_func = LOSS_FUNCTIONS[loss_func_name]\n    assert_allclose(loss, loss_func(y, y_pred))\n\n    # Check that the gradient is computed correctly\n    model.coefs_ = [np.array([[1, 2], [3, 4]])]\n    model.intercepts_ = [np.array([5, 6])]\n    grad = _pack(model.coefs_, model.intercepts_)\n    loss, grad = model._loss_grad_lbfgs(\n        grad, X, y, [y_pred], [np.zeros_like(y_pred)], [np.zeros_like(y_pred)], [np.zeros_like(y_pred)], [np.zeros_like(y_pred)]\n    )\n    assert_allclose(loss, loss_func(y, y_pred))\n\n    # Check that the gradient is backpropagated correctly\n    deltas = [np.zeros_like(y_pred)]\n    activations = [X]\n    coef_grads = [np.zeros_like(model.coefs_[0])]\n    intercept_grads = [np.zeros_like(model.intercepts_[0])]\n    model._backprop(X, y, activations, deltas, coef_grads, intercept_grads)\n    assert_allclose(deltas[0], np.array([-0.5, 0.5]))\n"], "sample_952": ["def test_is_singledispatch_method():\n    from functools import singledispatchmethod\n\n    class TestClass:\n        @singledispatchmethod\n            pass\n\n    test_obj = TestClass()\n    assert inspect.is_singledispatch_method(test_obj.test_method)\n\n    class TestClass2:\n        @singledispatchmethod\n            pass\n\n    test_obj = TestClass2()\n    assert inspect.is_singledispatch_method(test_obj.test_method)\n\n    class TestClass3:\n        @singledispatch\n            pass\n\n    test_obj = TestClass3()\n    assert not inspect.is_singledispatch_method(test_obj.test_method)\n", "def test_unwrap_all():\n    # Test unwrap_all() with partials\n        return x * 2\n\n    partial = functools.partial(wrapper, 5)\n    assert unwrap_all(partial) == 5\n\n    # Test unwrap_all() with wrappers\n    @functools.wraps(wrapper)\n        return x * 2\n\n    assert unwrap_all(wrapper) == wrapper\n\n    # Test unwrap_all() with classmethod\n    class MyClass:\n        @classmethod\n            return cls\n\n    assert unwrap_all(MyClass.method) == MyClass.__class__\n\n    # Test unwrap_all() with staticmethod\n    @staticmethod\n        return 'static'\n\n    assert unwrap_all(static_method) == static_method.__func__\n\n    # Test unwrap_all() with mock object\n    import unittest.mock as mock\n    mock_object = mock.Mock()\n    assert unwrap_all(mock_object) == mock_object\n\n    # Test unwrap_all() with None\n    assert unwrap_all(None) is None\n\n    # Test unwrap_all() with invalid type\n    class InvalidType:\n        pass\n\n    assert unwrap_all(InvalidType()) == InvalidType()\n\n    # Test unwrap_all() with stop function\n        return isinstance(x, types.FunctionType)\n\n    assert unwrap_all(wrapper, stop=stop_func) == wrapper\n", "def test_getall():\n    class Obj:\n        __all__ = ['a', 'b']\n    assert inspect.getall(Obj) == ['a', 'b']\n\n    class Obj:\n        __all__ = 'a,b'\n    assert inspect.getall(Obj) == ['a', 'b']\n\n    class Obj:\n        __all__ = [1, 2, 3]\n    assert inspect.getall(Obj) == [1, 2, 3]\n\n    class Obj:\n        pass\n    assert inspect.getall(Obj) is None\n\n    class Obj:\n        __all__ = 123\n    with pytest.raises(ValueError):\n        inspect.getall(Obj)\n\n    class Obj:\n        __all__ = {'a': 1}\n    with pytest.raises(ValueError):\n        inspect.getall(Obj)\n", "def test_type_alias_namespace():\n    # Test TypeAliasNamespace\n    #------------------------\n\n    ns = TypeAliasNamespace({'a': 'A', 'b.c': 'B.c', 'b.d.e': 'B.d.E'})\n    assert ns['a'] == 'A'\n    assert ns['b.c'] == 'B.c'\n    assert ns['b.d.e'] == 'B.d.E'\n    with pytest.raises(KeyError):\n        ns['b.d.f']\n\n    # Test nested namespace\n    ns = TypeAliasNamespace({'a': 'A', 'b.c': 'B.c', 'b.d.e': 'B.d.E'})\n    b = ns['b']\n    assert b == TypeAliasModule('b', {'c': 'B.c', 'd.e': 'B.d.E'})\n    c = b['c']\n    assert c == TypeAliasForwardRef('B.c')\n    e = b['d.e']\n    assert e == TypeAliasForwardRef('B.d.E')\n\n    # Test conflict\n    ns = TypeAliasNamespace({'a': 'A', 'b.c': 'C'})\n    with pytest.raises(KeyError):\n        ns['b.d.e']\n\n    # Test invalid input\n    with pytest.raises(TypeError):\n        TypeAliasNamespace(None)\n    with pytest.raises(TypeError):\n        TypeAliasNamespace({'a': 1})\n    with pytest.raises(TypeError):\n        TypeAliasNamespace({'a': 'A', 'b.c': 'C'})\n\n", "def test_isclassmethod():\n    class ParentClass:\n        pass\n\n    class ChildClass(ParentClass):\n        pass\n\n    child_class_method = classmethod(ParentClass.__init__)\n    assert inspect.isclassmethod(child_class_method)\n\n    child_method = ChildClass().__init__\n    assert not inspect.isclassmethod(child_method)\n\n        pass\n\n    test_function.__self__ = ParentClass()\n    test_function.__self__.__class__ = ParentClass\n    assert not inspect.isclassmethod(test_function)\n"], "sample_540": ["    def test_register_decorator(self):\n        registry = animation.MovieWriterRegistry()\n        @registry.register('test')\n        class DummyWriter:\n            pass\n        assert 'test' in registry\n", "def test_movie_writer_with_invalid_writer():\n    # Setup\n    writer = NullMovieWriter()\n\n    # Test that a writer is available\n    assert animation.writers.is_available('invalid') is False\n\n    # Test that a writer is not available when not registered\n    assert animation.writers['invalid'] is None\n\n    # Test that the writer is not available when not available\n    writer_cls = animation.writers['pillow']\n    writer_cls._registered['invalid'] = None\n    assert animation.writers.is_available('invalid') is False\n\n    # Test that a writer is available\n    assert animation.writers.is_available('pillow') is True\n\n    # Test that a writer is available when available\n    assert animation.writers['pillow'] is writer_cls\n", "def test_animation_save_movie_file(tmpdir, request):\n    \"\"\"Check saving a movie file correctly.\"\"\"\n    # Create a simple animation (with options)\n    anim = anim(request=request)\n\n    # Create a temporary file\n    file_path = tmpdir / 'test.mp4'\n    # Create a custom movie writer\n    writer = NullMovieWriter()\n    # Save the animation as a movie file\n    anim.save(file_path, writer=writer)\n\n    # Check that the writer was called as expected\n    assert writer.fig == anim._fig\n    assert writer.outfile == str(file_path)\n    assert writer.dpi == mpl.rcParams['savefig.dpi']\n    assert writer.args == ()\n    assert writer._count == 5\n\n    # Check that the writer was called with the correct savefig arguments\n    assert writer.savefig_kwargs == {\n        'dpi': mpl.rcParams['savefig.dpi'],\n        'format': 'png',\n        'transparent': False\n    }\n", "    def test_movie_writer_finish_raises_error_if_setup_not_called(self, anim):\n        \"\"\"Check that calling finish() on a MovieWriter raises an error if setup()\n        was not called.\"\"\"\n        writer = animation.MovieWriter(fps=30)\n        with pytest.raises(RuntimeError):\n            writer.finish()\n", "    def test_movie_writer_finish(self):\n        # Check that finish is called and writes the frames\n        writer = NullMovieWriter()\n        with animation.saving(plt.gcf(), \"test.mp4\", 100):\n            for i in range(10):\n                writer.grab_frame()\n        self.assertEqual(writer._count, 10)\n        # The file test.mp4 does not need to exist\n        # to be a valid test, so let's just check that it's there\n        self.assertTrue(os.path.exists(\"test.mp4\"))\n"], "sample_1055": ["def test_elgamal_private_key():\n    assert elgamal_private_key(digit=10)[0] > 2**9\n    assert elgamal_private_key(digit=10)[1] > 2**9\n    assert elgamal_private_key(digit=10)[2] != 0\n", "def test_encipher_hill_odd_length():\n    from sympy.crypto.crypto import encipher_hill\n    key = Matrix([[1, 2], [3, 4]])\n    msg = 'ST'\n    result = encipher_hill(msg, key)\n    assert len(result) % len(key) == 0\n", "def test_check_and_join_symbols_filter():\n    msg = \"Hello, world!\"\n    symbols = \"ARE\"\n    assert check_and_join(msg, symbols) == \"ARE\"\n", "def test_bifid_square():\n    key = \"gold bug\"\n    # Test a 5x5 square\n    square = bifid_square(key)\n    assert isinstance(square, Matrix)\n    assert square.shape == (5, 5)\n    assert square.has_symbol\n    # Test a 6x6 square\n    square = bifid6_square(key)\n    assert isinstance(square, Matrix)\n    assert square.shape == (6, 6)\n    assert square.has_symbol\n", "def test_encipher_hill():\n    from sympy.crypto.crypto import encipher_hill\n    from sympy.crypto.crypto import _prep\n    from sympy.crypto.crypto import AZ\n    from sympy.matrices import Matrix\n    from sympy.ntheory import isprime\n\n    # test error checking\n    assert encipher_hill(\"msg\", 3) == False\n    assert encipher_hill(\"msg\", Matrix([[1, 2], [3, 4]])) == False\n\n    # test case when msg is not a list of unique characters\n    assert encipher_hill(\"abca\", Matrix([[1, 2], [3, 4]])) == False\n\n    # test with the same alphabet\n    msg, pad, A = _prep(\"HELLO\", 'H', symbols=AZ())\n    k = Matrix([[1, 2], [3, 4]])\n    assert encipher_hill(msg, k) == False\n    assert encipher_hill(msg, k, symbols=A) == False\n\n    # test case when the alphabet size is not a square number\n    assert encipher_hill(\"msg\", Matrix([[1, 2], [3, 4]]), symbols='abcd') == False\n\n    # test with different alphabets\n    msg, pad, A = _prep(\"HELLO\", '', symbols='abcd')\n    k = Matrix([[1, 2], [3, 4]])\n    assert encipher_hill(msg, k, symbols=A) == \"UHJMO\"\n    assert encipher_hill(msg, k, symbols='abcd') == \"UHJMO\"\n\n    # test the case when the length of the message is not a multiple of k\n    assert encipher_hill('abc', Matrix([[1, 2], [3, 4]]), pad='Z') == \"JY\"\n    assert encipher_hill('abc', Matrix([[1, 2], [3, 4]]), pad=None) == \"JY\"\n\n    # test the function with a big alphabet\n    msg = AZ(\"go navy! beat army!\")\n    k = Matrix([[1, 2, 3, 4], [5, 6, 7, 8]])\n    assert encipher_hill(msg, k) == False\n"], "sample_263": ["    def test_dumpdata_with_bz2_compression(self):\n        # Check that dumpdata with bz2 compression works as expected\n        temp_file = NamedTemporaryFile(suffix='.bz2', delete=True)\n        try:\n            call_command('dumpdata', 'models', format='json', output=temp_file.name, compress=True)\n            # Check that the output file is a valid JSON\n            temp_file.seek(0)\n            import json\n            json.loads(temp_file.read())\n        finally:\n            temp_file.close()\n", "    def test_dumpdata_invalid_format(self):\n        from django.core.management import CommandError\n        from django.core.management.commands.dumpdata import dumpdata\n\n        # Test that the command raises an error when an invalid format is specified\n        with self.assertRaises(CommandError):\n            dumpdata.Command().handle(\n                *['app_label.ModelName'],\n                format='invalid_format'\n            )\n", "    def test_dumpdata_with_multiple_models(self):\n        \"\"\"\n        Test that dumpdata can handle multiple models correctly.\n        \"\"\"\n        # Create some test data\n        category = Category.objects.create(name='Test Category')\n        article = Article.objects.create(category=category, title='Test Article')\n        tag = Tag.objects.create(name='Test Tag')\n        visa = Visa.objects.create(issuer='Test Issuer', code='Test Code')\n\n        # Test that dumpdata works correctly with multiple models\n        output = StringIO()\n        self.assertEqual(len(list(get_objects(count_only=True))), 3)\n        management.call_command('dumpdata', 'test_app', stdout=output, stderr=devnull)\n        output.seek(0)\n        data = json.loads(output.getvalue())\n        self.assertEqual(len(data), 3)\n        self.assertEqual(data[0]['fields']['category'], category.pk)\n        self.assertEqual(data[1]['fields']['category'], category.pk)\n        self.assertEqual(data[2]['fields']['visa'], visa.pk)\n", "    def test_dumpdata_command_fails_with_invalid_format(self):\n        with self.assertRaises(CommandError):\n            management.call_command('dumpdata', 'invalid_format')\n", "    def test_natural_key_handling(self):\n        \"\"\"\n        Tests that natural keys are used when dumping a model that has natural keys.\n        \"\"\"\n        self.assertTrue(NaturalKeyThing.objects.count() == 10)\n        fixture_file = NamedTemporaryFile(suffix='.json', delete=False)\n        management.call_command(\n            'dumpdata',\n            format='json',\n            indent=4,\n            database='default',\n            app_label='tests',\n            model='NaturalKeyThing',\n            use_natural_foreign_keys=True,\n            use_natural_primary_keys=True,\n            output=fixture_file.name,\n        )\n        fixture_file.close()\n        new_objects = NaturalKeyThing.objects.using('default').raw('SELECT * FROM tests_naturalkeything')\n        self.assertEqual(new_objects.count(), 10)\n        self.assertEqual(NaturalKeyThing.objects.count(), 10)\n        with open(fixture_file.name) as f:\n            data = json.load(f)\n        self.assertEqual(len(data), 10)\n"], "sample_918": ["def test_PyMethod_needs_arglist_property():\n    signode = desc_signature('method')\n    method = PyMethod()\n    assert method.needs_arglist() is True\n    method.options['property'] = True\n    assert method.needs_arglist() is False\n", "def test_handle_signature_class(self):\n    \"\"\"Test class signature.\"\"\"\n    signode = desc_signature(sig='class MyClass:\\n    def __init__(self):\\n        pass')\n    name, prefix = self.pyobj.handle_signature('class MyClass:\\n    def __init__(self):\\n        pass', signode)\n    assert name == 'MyClass'\n    assert prefix == ''\n", "def test_parse_annotation():\n    class TestAST(ast.AST):\n        pass\n\n    class TestSubscript(ast.Subscript):\n        pass\n\n    class TestAttribute(ast.Attribute):\n        pass\n\n    class TestTuple(ast.Tuple):\n        pass\n\n    class TestList(ast.List):\n        pass\n\n    tree = ast.parse('int | None | str | dict[str, int]')\n    result = _parse_annotation(tree)\n    assert_node(result, [type_to_xref('int'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('None'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('str'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('dict'), addnodes.desc_sig_punctuation('', '['),\n                        type_to_xref('str'), addnodes.desc_sig_punctuation('', ','),\n                        type_to_xref('int'), addnodes.desc_sig_punctuation('', ']'),\n                        addnodes.desc_sig_operator('', '|')])\n\n    tree = ast.parse('int() | None() | str() | dict()')\n    result = _parse_annotation(tree)\n    assert_node(result, [type_to_xref('int'), addnodes.desc_sig_operator('', '('),\n                        addnodes.desc_sig_punctuation('', ')'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('None'), addnodes.desc_sig_operator('', '('),\n                        addnodes.desc_sig_punctuation('', ')'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('str'), addnodes.desc_sig_operator('', '('),\n                        addnodes.desc_sig_punctuation('', ')'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('dict'), addnodes.desc_sig_operator('', '('),\n                        addnodes.desc_sig_punctuation('', ')')])\n\n    tree = ast.parse('int() -> int | None | str | dict[str, int]')\n    result = _parse_annotation(tree)\n    assert_node(result, [type_to_xref('int'), addnodes.desc_sig_operator('', '('),\n                        addnodes.desc_sig_punctuation('', ')'), addnodes.desc_sig_operator('', '->'),\n                        addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('int'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('None'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('str'), addnodes.desc_sig_operator('', '|'),\n                        type_to_xref('dict'), addnodes.desc_sig_punctuation('', '['),\n                        type_to", "def test_parse_annotation():\n    \"\"\"Check that _parse_annotation parses type annotations.\"\"\"\n    # simple type\n    annotation = 'str'\n    expected = [type_to_xref('str')]\n    assert _parse_annotation(annotation) == expected\n\n    # simple type with attribute\n    annotation = 'str.upper'\n    expected = [type_to_xref('str.upper')]\n    assert _parse_annotation(annotation) == expected\n\n    # simple type with index\n    annotation = 'str[0]'\n    expected = [type_to_xref('str'), addnodes.desc_sig_punctuation('', '['),\n                type_to_xref('0'), addnodes.desc_sig_punctuation('', ']')]\n    assert _parse_annotation(annotation) == expected\n\n    # simple type with tuple\n    annotation = '(int, float)'\n    expected = [addnodes.desc_sig_punctuation('', '('),\n                type_to_xref('int'), addnodes.desc_sig_punctuation('', ', '),\n                type_to_xref('float'), addnodes.desc_sig_punctuation('', ')')]\n    assert _parse_annotation(annotation) == expected\n\n    # simple type with list\n    annotation = '[int, float]'\n    expected = [addnodes.desc_sig_punctuation('', '['),\n                type_to_xref('int'), addnodes.desc_sig_punctuation('', ', '),\n                type_to_xref('float'), addnodes.desc_sig_punctuation('', ']')]\n    assert _parse_annotation(annotation) == expected\n\n    # complex type with multiple attributes\n    annotation = 'List[int, Dict[str, int]]'\n    expected = [type_to_xref('List'), addnodes.desc_sig_punctuation('', '['),\n                type_to_xref('int'), addnodes.desc_sig_punctuation('', ', '),\n                type_to_xref('Dict'), addnodes.desc_sig_punctuation('', '['),\n                type_to_xref('str'), addnodes.desc_sig_punctuation('', ', '),\n                type_to_xref('int'), addnodes.desc_sig_punctuation('', ']'),\n                addnodes.desc_sig_punctuation('', ']')]\n    assert _parse_annotation(annotation) == expected\n\n    # invalid annotation\n    annotation = 'invalid annotation'\n    expected = [type_to_xref('invalid annotation')]\n    assert _parse_annotation(annotation) == expected\n\n    # invalid annotation with invalid syntax\n    annotation = '{ invalid annotation }'\n    expected = [type_to_xref('{ invalid annotation }')]\n    assert _parse_annotation(annotation) == expected\n", "def test_filter_meta_fields():\n    app = Mock()\n    env = Mock()\n    content = nodes.field_list(\n        nodes.field(\n            nodes.field_name(''),\n            nodes.field_body(''),\n            nodes.paragraph('value'),\n        ),\n        nodes.field(\n            nodes.field_name('meta'),\n            nodes.field_body(''),\n            nodes.paragraph('value'),\n        ),\n    )\n    filter_meta_fields(app, 'py', 'obj', content)\n    assert len(content.children) == 0\n"], "sample_245": ["    def test_msgmerge(self):\n        options = {'domain': 'djangojs', 'extensions': 'js', 'no_location': True}\n        output, contents = self._run_makemessages(**options)\n        self.assertNotRegex(contents, r'^msgid .*')\n        self.assertNotRegex(contents, r'^msgstr .*')\n        self.assertNotRegex(contents, r'^#.+: .*')\n", "    def test_makemessages_add_location_option(self):\n        \"\"\"\n        Ensure that --add-location option affects msgmerge output.\n        \"\"\"\n        # Run makemessages with --add-location option\n        output, po_contents = self._run_makemessages(add_location='full')\n        # Make sure that msgmerge output includes file and line numbers\n        self.assertTrue(self.assertRegex(po_contents, r'^#: .*/foo.py:\\d+$', re.MULTILINE))\n\n        # Run makemessages with --no-location option\n        output, po_contents = self._run_makemessages(add_location='never')\n        # Make sure that msgmerge output does not include file and line numbers\n        self.assertTrue(self.assertNotRegex(po_contents, r'^#: .*/foo.py:\\d+$', re.MULTILINE))\n", "    def test_makemessages_all_locales(self):\n        self._run_makemessages(all=True)\n        for locale in os.listdir(os.path.join(self.workdir, 'locale')):\n            self.assertTrue(os.path.exists(os.path.join(self.workdir, 'locale', locale, 'LC_MESSAGES', 'django.po')))\n", "    def test_remove_obsolete_messages(self):\n        # Create a test file with some messages\n        with open(os.path.join(self.workdir, 'test.py'), 'w', encoding='utf-8') as fp:\n            fp.write('''\\", "    def _run_makemessages(self, *args, **options):\n        options.update({\n            '--all': True,\n            '--domain': 'django',\n        })\n        return self._run_makemessages(*args, **options)\n"], "sample_477": ["    def test_string_format(self):\n        self.assertEqual(self.engine.render(\"random01\"), \"a b\")\n        self.assertEqual(self.engine.render(\"stringformat01\"), \"a b\")\n", "    def test_add_filter(self):\n        # Test that the add filter can add integers and strings\n        self.assertEqual(self.template.render(\"integers01\"), \"15\")\n        self.assertEqual(self.template.render(\"integers02\"), \"10\")\n        self.assertEqual(self.template.render(\"integers03\"), \"10\")\n", "    def test_slugify(self):\n        self.assertEqual(self.template.render({\"a\": \"hello world\"}), mark_safe(\"hello-world\"))\n", "        def test_addslashes(self):\n            self.assertEqual(self.render_template(\"addslashes\"), \"Hello,\\\\ world!\")\n            self.assertEqual(self.render_template(\"addslashes_none\"), \"\")\n", "    def test_add_divisibleby_get_digit_yesno(self):\n        self.assertEqual(self.random01, \"2 2\")\n        self.assertTrue(self.divisibleby)\n        self.assertEqual(self.get_digit, \"2\")\n        self.assertEqual(self.yesno, \"y\")\n        self.assertEqual(self.yesno_with_mapping, \"y\")\n"], "sample_788": ["def test_fit_with_n_bins_of_different_lengths():\n    X = [[-2, 1.5, -4, -1],\n         [-1, 2.5, -3, -0.5],\n         [0, 3.5, -2, 0.5],\n         [1, 4.5, -1, 2]]\n\n    # Test with n_bins of different lengths\n    est = KBinsDiscretizer(n_bins=[3, 5, 4, 3], strategy='quantile')\n    est.fit(X)\n    assert_array_equal(est.n_bins_, [3, 5, 4, 3])\n\n    # Test with non-integer n_bins\n    with pytest.raises(ValueError):\n        est = KBinsDiscretizer(n_bins=[3.5, 5, 4, 3], strategy='quantile')\n        est.fit(X)\n\n    # Test with n_bins shorter than 2\n    with pytest.raises(ValueError):\n        est = KBinsDiscretizer(n_bins=[1, 5, 4, 3], strategy='quantile')\n        est.fit(X)\n", "def test_KBinsDiscretizer_n_bins_and_strategy(X, n_bins, strategy, expected):\n    est = KBinsDiscretizer(n_bins=n_bins, strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_almost_equal(Xt, expected)\n", "def test_fit_transform_onehot_dense():\n    \"\"\"Test transform method with one-hot dense encoding.\"\"\"\n    est = KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='quantile')\n    X = [[-2, 1, -4, -1],\n         [-1, 2, -3, -0.5],\n         [0, 3, -2, 0.5],\n         [1, 4, -1, 2]]\n    Xt = est.fit_transform(X)\n    assert Xt.shape == (4, 20)\n    expected_Xt = np.array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                           [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                           [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n                           [2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n    assert_array_equal(Xt, expected_Xt)\n", "def test_kbins_discretizer_n_bins(strategy, expected):\n    # Test with n_bins parameter\n    est = KBinsDiscretizer(n_bins=5, strategy=strategy)\n    est.fit(X)\n    Xt = est.transform(X)\n    assert_array_equal(Xt, expected)\n", "def test_KBinsDiscretizer_uniform_strategy_with_constant_features(n_bins, expected):\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n    Xt = est.fit_transform(X)\n    assert_array_equal(Xt, expected)\n"], "sample_498": ["def test_draggable_legend():\n    with rc_context({'legend.draggable': True}):\n        fig, ax = plt.subplots()\n        line1, = ax.plot([1, 2], label='Line 1')\n        line2, = ax.plot([2, 3], label='Line 2')\n        legend = mlegend.Legend(ax, [line1, line2], [line1.get_label(), line2.get_label()], loc='upper right')\n        draggable_legend = legend.set_draggable(True)\n        assert draggable_legend is not None\n        assert draggable_legend.use_blit is False\n        assert draggable_legend.update == 'loc'\n        assert draggable_legend.get_draggable()\n        draggable_legend._update_loc((0.5, 0.5))\n        assert legend.get_bbox_to_anchor().x0 == 0.5\n        assert legend.get_bbox_to_anchor().y0 == 0.5\n        with pytest.raises(ValueError):\n            legend.set_bbox_to_anchor(None)\n        with pytest.raises(ValueError):\n            legend.set_bbox_to_anchor(Bbox.from_bounds(0, 0, 0, 0))\n", "def test_legend_title_properties():\n    with rc_context(rc={\n        'legend.title_fontsize': 18,\n        'legend.title_fontproperties': {'weight': 'bold', 'stretch': 'medium'}\n    }):\n        fig, ax = plt.subplots()\n        ax.set_xlim(-10, 10)\n        ax.set_ylim(-10, 10)\n        line1, = ax.plot([1, 2, 3], [1, 2, 3], label='line1')\n        line2, = ax.plot([4, 5, 6], [4, 5, 6], label='line2')\n        ax.legend(title='Legend Title', title_fontsize=18, title_fontproperties={'weight': 'bold', 'stretch': 'medium'})\n        title = ax.legend_.get_title()\n        assert title.get_fontsize() == 18\n        assert title.get_fontproperties().get_weight() == 'bold'\n        assert title.get_fontproperties().get_stretch() == 'medium'\n", "def test_legend_title_prop():\n    fig, ax = plt.subplots()\n    ax.set_title('Title')\n    legend = mlegend.Legend(ax, [mlines.Line2D([0], [0], marker='o')],\n                            [mpatches.Circle((0, 0), 0.2, fill=False)], loc='best')\n    legend.set_title('Title with Prop')\n    assert isinstance(legend._legend_title_box._text.get_fontproperties(), FontProperties)\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        legend.set_title('Title with Prop Dict')\n    assert isinstance(legend._legend_title_box._text.get_fontproperties(), FontProperties)\n\n    legend.set_title('Title with Prop Font')\n    assert isinstance(legend._legend_title_box._text.get_fontproperties(), FontProperties)\n\n    with rc_context(rc={'legend.title_fontsize': 10}):\n        legend.set_title('Title with Prop Font Size')\n        assert legend._legend_title_box._text.get_fontproperties().get_size_in_points() == 10\n\n    with rc_context(rc={'legend.title_fontsize': 10, 'legend.title_fontproperties': {'size': 8}}):\n        legend.set_title('Title with Prop Font Size')\n        assert legend._legend_title_box._text.get_fontproperties().get_size_in_points() == 10\n\n    with rc_context(rc={'legend.title_fontsize': 10, 'legend.title_fontproperties': {'size': 8, 'weight': 'bold'}}):\n        legend.set_title('Title with Prop Font Size and Weight')\n        assert legend._legend_title_box._text.get_fontproperties().get_size_in_points() == 10\n        assert legend._legend_title_box._text.get_fontproperties().set_weight() == 'bold'\n\n    legend.set_title(None)\n    assert legend._legend_title_box._text.get_text() is None\n", "def test_legend_align():\n    # Test that legend alignment behaves correctly when title is present\n\n    fig, ax = plt.subplots()\n    ax.plot(np.random.rand(10))\n\n    legend = mlegend.Legend(ax, [], [], loc=\"center\", title=\"Test\")\n    legend._legend_box.set_width(1.0)\n    legend._legend_box.set_height(1.0)\n\n    # Set the alignment to \"left\" and test that the legend title is on the left\n    legend.set_alignment(\"left\")\n    legend._legend_title_box.set_width(0.5)\n    legend._legend_title_box.set_height(1.0)\n    legend.draw(plt.gcf().canvas.get_renderer())\n    legend._legend_box.set_offset(0.0)\n    expected_align_left = 0.5\n    assert legend.get_title().get_window_extent().x0 == expected_align_left\n\n    # Set the alignment to \"right\" and test that the legend title is on the right\n    legend.set_alignment(\"right\")\n    legend._legend_title_box.set_width(0.5)\n    legend._legend_title_box.set_height(1.0)\n    legend.draw(plt.gcf().canvas.get_renderer())\n    legend._legend_box.set_offset(0.0)\n    expected_align_right = 0.5 + 0.5  # 0.5 is the width of the title box\n    assert legend.get_title().get_window_extent().x0 == expected_align_right\n\n    # Set the alignment to \"center\" and test that the legend title is centered\n    legend.set_alignment(\"center\")\n    legend._legend_title_box.set_width(0.5)\n    legend._legend_title_box.set_height(1.0)\n    legend.draw(plt.gcf().canvas.get_renderer())\n    legend._legend_box.set_offset(0.0)\n    expected_align_center = 0.25  # 0.5 is the width of the title box\n    assert legend.get_title().get_window_extent().x0 == expected_align_center\n\n    # Test that the legend title is not centered when title is empty\n    legend.set_title(\"\")\n    legend.draw(plt.gcf().canvas.get_renderer())\n    legend._legend_box.set_offset(0.0)\n    expected_align_center_no_title = 0.0\n    assert legend.get_title().get_window_extent().x0 == expected_align_center_no_title\n\n    # Test that the legend title is not", "def test_legend_draggable_inside_plot():\n    fig, axs = plt.subplots(1, 1)\n\n    x = np.linspace(0, 2 * np.pi, 100)\n    y = np.sin(x)\n\n    line1, = axs.plot(x, y, label='line1')\n    line2, = axs.plot(x, y + np.sin(x), label='line2')\n\n    legend = axs.legend(handles=[line1, line2],\n                       loc='best',\n                       title=\"test\",\n                       bbox_to_anchor=(1.05, 1),\n                       borderaxespad=0.2,\n                       shadow=True,\n                       fancybox=True,\n                       framealpha=0.8,\n                       edgecolor=\"red\",\n                       facecolor=\"blue\")\n\n    legend.set_draggable(True)\n\n        assert legend.get_bbox_to_anchor().x0 > x\n        assert legend.get_bbox_to_anchor().y0 > y\n\n        assert legend.get_bbox_to_anchor() == Bbox.from_bounds(1.05, 1, 0, 0)\n\n        mouse_event.xdata = 0.5\n        mouse_event.ydata = 0.5\n        return mouse_event\n\n    with rc_context({\"interaction\": \"splash\"}):\n        with mock.patch.object(plt, \"show\") as mocked_show:\n            fig.canvas.mpl_connect(\"motion_notify_event\", mouse_move)\n            fig.canvas.draw_idle()\n            test_dragged(0.5, 0.5)\n            mocked_show.assert_called_once()\n\n    assert legend.get_bbox_to_anchor() == Bbox.from_bounds(0.8609, 0.7838, 0, 0)\n    assert_allclose(legend.get_bbox_to_anchor().width, 0.136, atol=1e-2)\n    assert_allclose(legend.get_bbox_to_anchor().height, 0.216, atol=1e-2)\n\n    with rc_context({\"interaction\": \"splash\"}):\n        with mock.patch.object(plt, \"show\") as mocked_show:\n            fig.canvas.mpl_disconnect(\"motion_notify_event\")\n            fig.canvas.draw_idle()\n            test_unmoved()\n            mocked_show.assert_called_once()\n\n    assert legend.get_bbox_to_anchor() == Bbox.from_bounds(1.05, 1, 0, 0)\n"], "sample_238": ["    def test_abs(self):\n        # Test the ABS function\n        book = Book.objects.create(isbn='test', name='Test book', rating=5.0)\n        self.assertEqual(Abs('5.0').expression, 'ABS(5.0)')\n", "    def test_abs_function(self):\n        # Test ABS function with integer argument\n        book = Book.objects.create(isbn='123456789', name='Test Book', price=Decimal('10.00'))\n        self.assertEqual(Book.objects.annotate(abs_price=Abs('price')).values_list('abs_price', flat=True)[0], 10)\n\n        # Test ABS function with float argument\n        book = Book.objects.create(isbn='987654321', name='Test Book', price=Decimal('20.50'))\n        self.assertEqual(Book.objects.annotate(abs_price=Abs('price')).values_list('abs_price', flat=True)[0], 20.5)\n", "    def test_transform_abs(self):\n        self.assertEqual(Abs('field').as_sql(None, connection=None), 'ABS(\"field\")')\n        self.assertEqual(Abs('field').as_sql(None, connection='sqlite'), 'ABS(field)')\n", "    def test_abs_function(self):\n        with CaptureQueriesContext() as queries:\n            self.assertEqual(Author.objects.filter(age__abs__gt=35).count(), 3)\n            self.assertEqual(len(queries), 1)\n            self.assertEqual(connection.ops.db.ops.last_executed_query(connection.ops.db), \"SELECT COUNT(*) FROM auth_author WHERE ABS(age) > 35\")\n", "    def test_abs_expression(self):\n        Author.objects.create(name='New Author', age=37)\n        result = Author.objects.annotate(abs_age=Abs('age')).values_list('abs_age', flat=True)\n        self.assertEqual(list(result), [37])\n"], "sample_249": ["    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_db'\n        database_creation = BaseDatabaseCreation(test_connection)\n\n        with mock.patch.object(database_creation, '_clone_test_db') as mock_clone:\n            database_creation.clone_test_db(1, verbosity=1, autoclobber=False, keepdb=False)\n            mock_clone.assert_called_once()\n            self.assertEqual(mock_clone.call_args.args[0], 1)\n            self.assertEqual(mock_clone.call_args.args[1], 1)\n            self.assertEqual(mock_clone.call_args.args[2], False)\n", "    def test_serialize_db_to_string(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n        db_creation.connection.settings_dict['TEST']['MIGRATE'] = False\n        serialized_data = db_creation.serialize_db_to_string()\n        self.assertIsInstance(serialized_data, str)\n        self.assertGreater(len(serialized_data), 0)\n", "    def test_serialize_db_to_string(self):\n        # Create a test connection with some data\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'test_db'\n        test_connection._test_serialized_contents = None\n        model = Object()\n        model.save(using=test_connection)\n        model2 = Object()\n        model2.save(using=test_connection)\n        test_connection.ensure_connection()\n\n        # Serialize the database\n        creation = BaseDatabaseCreation(test_connection)\n        serialized_db = creation.serialize_db_to_string()\n\n        # Deserialize the database\n        deserialized_db = creation.deserialize_db_from_string(serialized_db)\n\n        # Check that the deserialized database is the same as the original\n        self.assertEqual(Object.objects.count(), 2)\n        self.assertEqual(Object.objects.all()[0].pk, model.pk)\n        self.assertEqual(Object.objects.all()[1].pk, model2.pk)\n", "    def test_serialize_db_to_string(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        # Create a test database and fill it with some data\n        creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n        obj1 = Object.objects.create(name='obj1')\n        obj2 = Object.objects.create(name='obj2')\n        obj3 = ObjectSelfReference.objects.create(object=obj1, reference=obj2)\n        obj4 = ObjectReference.objects.create(object=obj1, reference=obj2)\n        # Serialize the database\n        serialized_data = creation.serialize_db_to_string()\n        # Deserialize the database\n        creation.deserialize_db_from_string(serialized_data)\n        # Check that the data was deserialized correctly\n        self.assertEqual(Object.objects.count(), 2)\n        self.assertEqual(ObjectSelfReference.objects.count(), 1)\n        self.assertEqual(ObjectReference.objects.count(), 1)\n", "    def test_serialize_db_to_string(self):\n        test_connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(test_connection)\n\n        # Create some test data\n        obj1 = Object()\n        obj1.save(using=test_connection.alias)\n\n        obj2 = Object()\n        obj2.save(using=test_connection.alias)\n\n        # Serialize the data\n        serialized_data = db_creation.serialize_db_to_string()\n\n        # Deserialize the data back into the database\n        db_creation.deserialize_db_from_string(serialized_data)\n\n        # Check that the data has been correctly deserialized\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM django_test_object\")\n            self.assertEqual(cursor.fetchone(), (obj1.id, obj2.id))\n\n        # Test that the serialized data is correctly stored on the connection\n        self.assertIn(serialized_data, db_creation.connection._test_serialized_contents)\n"], "sample_559": ["def test_axvspan():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 2)\n    ax.axvspan(0.5, 1.5, facecolor='red', alpha=0.5)\n    ax.vlines(1, 0, 2, 'blue', alpha=0.5)\n", "def test_contour():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2, 1, 0])\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 1)\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]))\n\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]), levels=[0.25])\n    assert_array_equal(ax.get_images()[0].get_array(), np.array([[0., 0., 1., 1.]])\n\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]), levels=[0.25, 0.5, 0.75])\n    assert_array_equal(ax.get_images()[0].get_array(), np.array([[0., 0.5, 1., 1.]])\n\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]), levels=[0.25, 0.5, 0.75], origin='lower')\n    assert_array_equal(ax.get_images()[0].get_array(), np.array([[0., 0.5, 1., 1.]]))\n\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]), levels=[0.25, 0.5, 0.75], cmap='viridis')\n    assert_array_equal(ax.get_images()[0].get_array(), np.array([[0., 0.5, 1., 1.]]))\n\n    ax.contour(np.array([[0, 0.5, 1, 1], [0.5, 1, 2, 2]]), levels=[0.25, 0.5, 0.75], cmap='viridis', alpha=0.5)\n    assert_array_equal(ax.get_images()[0].get_array(), np.array([[0., 0.5, 1., 1.]]))\n\n   ", "def test_legend():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    ax.legend()\n    assert ax.legend_ is not None\n\n    # Test that the legend can be set to None\n    ax.legend_ = None\n    assert ax.legend_ is None\n\n    # Test that the legend can be set to a custom legend instance\n    custom_legend = mlegend.Legend(ax, [], [], **{'title': 'Custom Legend'})\n    ax.legend_ = custom_legend\n    assert isinstance(ax.legend_, mlegend.Legend)\n\n    # Test that the legend can be set to None and then replaced with a custom legend\n    ax.legend_ = None\n    ax.legend_ = custom_legend\n    assert isinstance(ax.legend_, mlegend.Legend)\n\n    # Test that the legend can be removed\n    ax.legend_ = None\n    ax._remove_legend(custom_legend)\n    assert ax.legend_ is None\n\n    # Test that the legend can be set to a custom legend instance with a label\n    custom_legend = mlegend.Legend(ax, [], [], **{'title': 'Custom Legend'})\n    custom_legend.label = 'Custom Legend Label'\n    ax.legend_ = custom_legend\n    assert custom_legend.label == 'Custom Legend Label'\n\n    # Test that the legend can be set to a custom legend instance with a face color\n    custom_legend = mlegend.Legend(ax, [], [], **{'title': 'Custom Legend', 'facecolor': 'red'})\n    ax.legend_ = custom_legend\n    assert custom_legend.get_facecolor() == (1.0, 0.0, 0.0)\n\n    # Test that the legend can be set to a custom legend instance with a frame alpha\n    custom_legend = mlegend.Legend(ax, [], [], **{'title': 'Custom Legend', 'framealpha': 0.5})\n    ax.legend_ = custom_legend\n    assert custom_legend.get_frame_alpha() == 0.5\n\n    # Test that the legend can be set to a custom legend instance with a frame edgecolor\n    custom_legend = mlegend.Legend(ax, [], [], **{'title': 'Custom Legend', 'frameedgecolor': 'red'})\n    ax.legend_ = custom_legend\n    assert custom_legend.get_frame_edgecolor() == (1.0, 0.0, 0.0)\n\n    # Test that the legend can be set to a", "def test_add_collection():\n    # Test adding a collection with a custom transform\n    x = np.linspace(0, 10, 10)\n    y = np.linspace(0, 10, 10)\n    xy = np.column_stack([x, y])\n    c = mcoll.LineCollection(xy, transform=mtransforms.IdentityTransform())\n    ax = plt.Axes(fig, [0, 0, 1, 1])\n    ax.add_collection(c)\n    assert_array_equal(c.get_offsets(), xy)\n    ax.figure.canvas.draw()\n    assert ax.dataLim.intervalx == (x.min(), x.max())\n    assert ax.dataLim.intervaly == (y.min(), y.max())\n    ax.remove()\n    # Test adding a collection with no data\n    ax = plt.Axes(fig, [0, 0, 1, 1])\n    ax.add_collection(c)\n    assert_array_equal(c.get_offsets(), xy)\n    ax.figure.canvas.draw()\n    assert ax.dataLim.intervalx == (x.min(), x.max())\n    assert ax.dataLim.intervaly == (y.min(), y.max())\n    ax.remove()\n\n    # Test adding a collection with an untransformed object\n    p = mcoll.PatchCollection([Circle((0, 0), 1)])\n    ax = plt.Axes(fig, [0, 0, 1, 1])\n    ax.add_collection(p)\n    assert_array_equal(p.get_offsets(), np.array([[0, 0, 1, 1, 0, 0]])\n                         .T)\n    ax.figure.canvas.draw()\n    assert ax.dataLim.intervalx == (0, 1)\n    assert ax.dataLim.intervaly == (0, 1)\n    ax.remove()\n\n    # Test autoscaling of data limits\n    p = mcoll.PatchCollection([Ellipse((0, 0), 1, 0.2)])\n    ax = plt.Axes(fig, [0, 0, 1, 1])\n    ax.add_collection(p)\n    assert_array_equal(p.get_offsets(), np.array([[0, 0, 1, 0.2, 0, 0]]\n                                               .T))\n    ax.figure.canvas.draw()\n    assert ax.dataLim.intervalx == (0, 1)\n    assert ax.dataLim.intervaly == (-0.05, 0.3)\n    ax.remove()\n\n    # Test autos", "def test_add_line():\n    a = mpl_toolkits.axes_grid1.mpl_axes.Axes()\n    line = mlines.Line2D([0, 1], [0, 1])\n    a.add_line(line)\n    assert line in a._lines\n    assert line in a.lines\n"], "sample_879": ["    def test_handle_unknown_infrequent_if_exist_ordinal_category(self):\n        encoder = OneHotEncoder(handle_unknown=\"infrequent_if_exist\")\n        X = np.array([['a', 'b', 'c']])\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv = encoder.inverse_transform(X_transformed)\n        assert np.array_equal(X, X_inv)\n\n        encoder = OneHotEncoder(handle_unknown=\"infrequent_if_exist\", max_categories=2)\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv = encoder.inverse_transform(X_transformed)\n        assert np.array_equal(X, X_inv)\n", "def test_ordinal_encoder_encode_missing_value():\n    # OrdinalEncoder with encoded_missing_value\n    encoder = OrdinalEncoder(encoded_missing_value=0)\n    X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n    X_encoded = np.array([[1., 0.], [0., 1.], [0., 0.]])\n    X_expected = np.array([[1., 0.], [0., 1.], [0., 0.]])\n    assert_allclose(encoder.fit_transform(X), X_expected)\n\n    # OrdinalEncoder with encoded_missing_value and unknown categories\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value',\n                             unknown_value=2, encoded_missing_value=0)\n    X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n    X_encoded = np.array([[1., 0., 2.], [0., 1., 2.], [0., 0., 2.]])\n    X_expected = np.array([[1., 0., 2.], [0., 1., 2.], [0., 0., 2.]])\n    assert_allclose(encoder.fit_transform(X), X_expected)\n\n    # OrdinalEncoder with dtype float\n    encoder = OrdinalEncoder(encoded_missing_value=np.nan)\n    X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n    X_encoded = np.array([[1., 0.], [0., 1.], [0., np.nan]])\n    X_expected = np.array([[1., 0.], [0., 1.], [0., np.nan]])\n    assert_allclose(encoder.fit_transform(X), X_expected)\n\n    # OrdinalEncoder with unknown_value\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value',\n                             unknown_value=2)\n    X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n    X_encoded = np.array([[1., 0., 2.], [0., 1., 2.], [0., 0., 2.]])\n    X_expected = np.array([[1., 0., 2.], [0., 1., 2.], [0., 0., 2.]])\n    assert_allclose(encoder.fit_transform(X), X_expected)\n\n    # OrdinalEncoder with handle_unknown='", "def test_ordinal_encoder_handles_unknown_value():\n    # Test that the OrdinalEncoder handles unknown_value correctly\n    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=5)\n    X = np.array([['Male', 1], ['Female', 3], ['Female', np.nan]])\n    encoder.fit(X)\n    X_transformed = encoder.transform(X)\n\n    assert_array_equal(X_transformed[2], [0, 5])  # np.nan is encoded as 5\n\n    X_inv = encoder.inverse_transform(X_transformed)\n    assert_array_equal(X_inv[2], ['Female', np.nan])  # np.nan is restored\n", "def test_ordinal_encoder_handle_unknown_unknown_value():\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=0)\n    encoder.fit(X)\n\n    # Test that fit raises an error when unknown_value is already used\n    with pytest.raises(ValueError):\n        encoder.handle_unknown = 'error'\n        encoder.fit(X)\n\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=0)\n    encoder.fit(X)\n    X_trans = encoder.transform(X)\n    assert_array_equal(X_trans, [[0., 2.], [1., 0.]])\n\n    # Test that inverse_transform raises an error when handle_unknown='use_encoded_value'\n    # and the unknown value is not present in the transformed data\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=0)\n    encoder.fit(X)\n    X_trans = encoder.transform(X)\n    X_trans[0, 1] = 3\n    with pytest.raises(ValueError):\n        encoder.inverse_transform(X_trans)\n\n    # Test that inverse_transform correctly handles unknown values\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]], dtype=object)\n    encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=0)\n    encoder.fit(X)\n    X_trans = encoder.transform(X)\n    X_trans[0, 1] = 3\n    X_inv = encoder.inverse_transform(X_trans)\n    assert_array_equal(X_inv, [['Male', 1], ['Female', 3], ['Female', 2]])\n\n    # Test that inverse_transform correctly handles missing values\n    X = np.array([['Male', 1], ['Female', 3], ['Female', np.nan]], dtype=object)\n    encoder = OrdinalEncoder(encoded_missing_value=-1)\n    encoder.fit(X)\n    X_trans = encoder.transform(X)\n    assert_array_equal(X_trans, [[1., 0.], [0., 1.], [0., -1.]])\n    X_inv = encoder.inverse_transform(X", "def test_OneHotEncoder_with_sparse_output():\n    \"\"\"\n    Test OneHotEncoder with sparse_output=True.\n    \"\"\"\n    # check that output is not sparse with default parameters\n    ohe = OneHotEncoder(sparse_output=False)\n    X = np.array([['a', 'b'], ['a', 'c']])\n    assert not sparse.isspmatrix(ohe.fit_transform(X).toarray())\n\n    # check that output is sparse with sparse_output=True\n    ohe = OneHotEncoder(sparse_output=True)\n    X = np.array([['a', 'b'], ['a', 'c']])\n    assert sparse.isspmatrix(ohe.fit_transform(X))\n\n    # check that sparse_output=True and sparse=False are mutually exclusive\n    with pytest.raises(ValueError):\n        OneHotEncoder(sparse_output=True, sparse=False)\n\n    # check that output is sparse when outputting fewer features than fit\n    ohe = OneHotEncoder(sparse_output=True)\n    X = np.array([['a', 'b'], ['a', 'c']])\n    ohe.fit(X)\n    assert sparse.isspmatrix(ohe.transform(X).toarray())\n\n    # check that output is sparse when sparse_output is set after fit\n    ohe = OneHotEncoder()\n    X = np.array([['a', 'b'], ['a', 'c']])\n    ohe.fit(X)\n    ohe.sparse_output = True\n    assert sparse.isspmatrix(ohe.transform(X).toarray())\n"], "sample_168": ["    def test_stale_content_types_not_removed_when_include_stale_apps_is_false(self):\n        apps.clear_cache()\n        self.app_config = apps.get_app_config('empty_models')\n        self.app_config.enabled = False\n        self.app_config.apps_ready()\n        self.content_type = ContentType.objects.create(app_label='empty_models', model='Fake')\n\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n        call_command('removestalecontenttypes', '--include-stale-apps=False')\n\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_interactive_confirmation(self):\n        self.before_count = ContentType.objects.count()\n        self.content_type = ContentType.objects.create(app_label='contenttypes_tests', model='Fake')\n        self.app_config = apps.get_app_config('contenttypes_tests')\n\n        with captured_stdout() as captured_output:\n            call_command('removestalecontenttypes', '--database', 'default', '--include-stale-apps', '--interactive')\n            self.assertIn('Some content types in your database are stale and can be deleted.', captured_output.getvalue())\n            self.assertIn('Are you sure you want to delete these content types?', captured_output.getvalue())\n\n        with captured_stdout() as captured_output:\n            with mock.patch('builtins.input', side_effect=['no']):\n                call_command('removestalecontenttypes', '--database', 'default', '--include-stale-apps', '--interactive')\n                self.assertIn('Stale content types remain.', captured_output.getvalue())\n\n        with captured_stdout() as captured_output:\n            with mock.patch('builtins.input', side_effect=['yes']):\n                call_command('removestalecontenttypes', '--database', 'default', '--include-stale-apps', '--interactive')\n                self.assertIn('Deleting stale content type', captured_output.getvalue())\n\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_remove_stale_content_types_no_stale_apps(self):\n    call_command('removestalecontenttypes', database='default', include_stale_apps=True)\n    self.assertEqual(ContentType.objects.count(), self.before_count)\n", "def test_no_input(self):\n    \"\"\"Test that no input is prompted when --no-input flag is used.\"\"\"\n    with captured_stdout() as captured_output:\n        call_command('remove_stale_content_types', '--database=contenttypes_tests', '--no-input', '--include-stale-apps')\n    self.assertNotIn(\"Type 'yes' to continue, or 'no' to cancel:\", captured_output.getvalue())\n", "    def setUp(self):\n        self.before_count = ContentType.objects.count()\n        self.stale_app_label = 'stale_app'\n        self.content_type = ContentType.objects.create(app_label=self.stale_app_label, model='Fake')\n        self.app_config = apps.get_app_config(self.stale_app_label)\n        apps.apps.clear_cache()\n        apps.apps.populate(['stale_app'])\n"], "sample_296": ["    def test_message_encoding(self):\n        storage = self.storage\n        messages = [Message('success', 'message')]\n        set_cookie_data(storage, messages)\n        self.assertIn('message', storage.request.COOKIES[storage.cookie_name])\n", "    def test_store_messages_over_max_cookie_size(self):\n        storage = self.storage\n        messages = [Message(level=constants.INFO, message='Test message')]\n        for _ in range(500):\n            messages.append(Message(level=constants.INFO, message='Test message'))\n        set_cookie_data(storage, messages)\n        response = self.response_factory()\n        self.assertEqual(stored_cookie_messages_count(storage, response), 250)\n", "    def test_stored_messages_sentinel(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(level=constants.INFO, message='Test message')]\n        set_cookie_data(storage, messages, encode_empty=True)\n        response = self.response_class()\n        stored = storage._decode(storage.request.COOKIES[CookieStorage.cookie_name])\n        self.assertEqual(stored, [MessageEncoder.message_key, 0, 'info', 'Test message'])\n        self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n", "    def test_encode_empty(self):\n        storage = self.get_storage()\n        messages = []\n        self.assertEqual(storage._encode(messages, encode_empty=True), b'')\n", "    def test_encode_empty(self):\n        storage = self.storage_class()\n        messages = []\n        response = SimpleCookie()\n        encoded_data = storage._encode(messages, encode_empty=True)\n        self.assertIsNotNone(encoded_data)\n        self.assertNotEqual(encoded_data, b'')\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n"], "sample_222": ["    def test_lock_overlapping_files(self):\n        # Create two files\n        file1 = NamedTemporaryFile(delete=False)\n        file2 = NamedTemporaryFile(delete=False)\n\n        # Write some content to both files\n        file1.write(b'Hello')\n        file1.flush()\n        file2.write(b'World')\n        file2.flush()\n\n        # Try to lock one of the files\n        self.assertTrue(locks.lock(file1, locks.LOCK_EX))\n        self.assertFalse(locks.lock(file1, locks.LOCK_EX))\n\n        # Try to lock the other file\n        self.assertFalse(locks.lock(file2, locks.LOCK_EX))\n\n        # Unlock the first file\n        locks.unlock(file1)\n\n        # Now we can lock the second file\n        self.assertTrue(locks.lock(file2, locks.LOCK_EX))\n\n        # Unlock the second file\n        locks.unlock(file2)\n\n        # Clean up\n        os.remove(file1.name)\n        os.remove(file2.name)\n", "    def test_lock_unlock_file(self):\n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n            file_path = f.name\n            with locks.lock(f, locks.LOCK_EX):\n                # Perform some operation on the file\n                f.write('Hello, world!')\n                f.flush()\n                self.assertTrue(locks.lock(f, locks.LOCK_EX))\n                self.assertFalse(locks.lock(f, locks.LOCK_SH))\n                locks.unlock(f)\n                self.assertTrue(locks.lock(f, locks.LOCK_SH))\n                os.remove(file_path)\n", "    def test_locking_file_on_windows(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            path = Path(tmpdir) / 'test.txt'\n            with open(path, 'w') as f:\n                f.write('test')\n\n            # Test locking on Windows\n            if os.name == 'nt':\n                with locks.lock(path, locks.LOCK_EX):\n                    # Locking should succeed\n                    self.assertTrue(locks.lock(path, locks.LOCK_SH))\n                    # Locking should fail if it's already locked\n                    self.assertFalse(locks.lock(path, locks.LOCK_EX))\n            else:\n                self.skipTest('Windows-specific test')\n", "    def test_lockfile_exclusive_block_on_windows(self):\n        with NamedTemporaryFile(delete=False) as tmp:\n            # Create a file with a lock\n            with open(tmp.name, 'wb') as f:\n                locks.lock(f, locks.LOCK_EX)\n            # Attempt to create a new file with the same name and a lock\n            with NamedTemporaryFile(delete=False) as f:\n                with self.assertRaises(FileExistsError):\n                    locks.lock(f, locks.LOCK_EX)\n            # Unlock the original file\n            locks.unlock(tmp.name)\n            # Now we can create a new file with the same name\n            with NamedTemporaryFile(delete=False) as f:\n                locks.lock(f, locks.LOCK_EX)\n", "    def test_lock_exclusive_lock_fails_on_shared_lock(self):\n        # Arrange\n        temp_file = tempfile.NamedTemporaryFile()\n        temp_file.write(b'Initial content')\n        temp_file.seek(0)\n        with self.assertRaises(BlockingIOError):\n            # Act\n            with open(temp_file.name, 'rb') as f:\n                lock(f, locks.LOCK_EX)\n                lock(f, locks.LOCK_SH)\n            # Assert\n            self.assertTrue(lock(f, locks.LOCK_EX))\n"], "sample_228": ["    def test_management_form_initialization(self):\n        form = ManagementForm()\n        self.assertEqual(form.base_fields[TOTAL_FORM_COUNT].widget, HiddenInput())\n        self.assertEqual(form.base_fields[INITIAL_FORM_COUNT].widget, HiddenInput())\n        self.assertEqual(form.base_fields[MIN_NUM_FORM_COUNT].widget, HiddenInput())\n        self.assertEqual(form.base_fields[MAX_NUM_FORM_COUNT].widget, HiddenInput())\n", "    def test_formset_factory_with_custom_form_kwargs(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, can_order=False)\n        formset = CustomKwargFormSet(auto_id=False)\n        form = formset.forms[0]\n        self.assertEqual(form.custom_kwarg, None)\n        form = CustomKwargFormSet(auto_id=False, form_kwargs={'custom_kwarg': 'test'})\n        self.assertEqual(form.forms[0].custom_kwarg, 'test')\n        formset = formset_factory(CustomKwargForm, extra=1, form_kwargs={'custom_kwarg': 'test'})\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test')\n        formset = formset_factory(CustomKwargForm, extra=1, form_kwargs=lambda: {'custom_kwarg': 'test'})\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test')\n", "    def test_management_form_is_bound(self):\n        formset = ChoiceFormSet()\n        formset.is_bound = True\n        formset.data = {'TOTAL_FORMS': '1', 'INITIAL_FORMS': '0', 'MAX_NUM_FORMS': '1'}\n        form = formset.management_form\n        self.assertTrue(form.is_bound)\n", "    def test_ManagementForm(self):\n        formset = ChoiceFormSet(data={\n            'TOTAL_FORMS': '3',\n            'INITIAL_FORMS': '0',\n            'MAX_NUM_FORMS': '3',\n            'MIN_NUM_FORMS': '3',\n            '0-choice': 'Coffee',\n            '0-votes': '1',\n            '1-choice': 'Tea',\n            '1-votes': '2',\n            '2-choice': 'Juice',\n            '2-votes': '3',\n        })\n        with self.assertRaises(ValidationError):\n            formset.management_form.full_clean()\n", "    def test_can_order_forms_with_invalid_forms(self):\n        # Test that ordering fields are removed from forms that are not valid\n        formset = ChoiceFormSet({'choices': ['', ''], 'TOTAL_FORMS': '2', 'INITIAL_FORMS': '0'})\n        self.assertTrue(formset.forms[0].fields.get('ORDER'))\n        self.assertFalse(formset.forms[1].fields.get('ORDER'))\n"], "sample_406": ["    def test_manager_descriptor_get(self):\n        # Arrange\n        class Model(models.Model):\n            class Meta:\n                managed = False\n\n            id = models.AutoField(primary_key=True)\n\n        manager = Manager()\n        manager.model = Model\n\n        # Act\n        descriptor = ManagerDescriptor(manager)\n\n        # Assert\n        self.assertEqual(descriptor.manager, manager)\n        self.assertEqual(descriptor.__get__(None, Model.__class__), manager)\n", "    def test_manager_descriptor_instantiation(self):\n        class TestModel(models.Model):\n            class Meta:\n                abstract = True\n\n        manager = ManagerDescriptor(BaseManager())\n        self.assertIsInstance(manager, BaseManager)\n        self.assertIsNone(manager.manager)\n", "    def test_all_method_returns_query_set(self):\n        qs = Article.objects.all()\n        self.assertIsInstance(qs, QuerySet)\n        self.assertEqual(qs.model, Article)\n        self.assertEqual(qs.db, 'default')\n", "    def test_deconstruct(self):\n        manager = Manager()\n        deconstructed_manager = manager.deconstruct()\n        manager2 = BaseManager.from_deconstruct(deconstructed_manager)\n        self.assertEqual(manager, manager2)\n        \n        # Test dynamic manager deconstruction\n        class DynamicManager(BaseManager.from_queryset(QuerySet)):\n            pass\n\n        dynamic_manager = DynamicManager()\n        deconstructed_dynamic_manager = dynamic_manager.deconstruct()\n        with self.assertRaises(ValueError):\n            BaseManager.from_deconstruct(deconstructed_dynamic_manager)\n\n        # Test manager with as_manager\n        class AsManagerManager(BaseManager):\n                return super().get_queryset()\n\n        as_manager_manager = AsManagerManager()\n        deconstructed_as_manager_manager = as_manager_manager.deconstruct()\n        manager3 = BaseManager.from_deconstruct(deconstructed_as_manager_manager)\n        self.assertTrue(deconstructed_as_manager_manager[0])\n        self.assertEqual(manager3, as_manager_manager)\n", "    def test_empty_manager(self):\n        # Create an instance of the EmptyManager\n        manager = EmptyManager(model=models.Model())\n        \n        # Check if the manager returns an empty queryset\n        queryset = manager.get_queryset()\n        self.assertIsInstance(queryset, EmptyQuerySet)\n        self.assertEqual(queryset.count(), 0)\n\n        # Check if the all() method returns an empty queryset\n        self.assertEqual(manager.all().count(), 0)\n\n        # Check if the get_queryset() method returns an empty queryset when no arguments are passed\n        self.assertEqual(manager.get_queryset().count(), 0)\n\n        # Check if the get_queryset() method returns a None queryset when no arguments are passed\n        self.assertEqual(manager.get_queryset(model=None, using=None, hints=None).count(), 0)\n"], "sample_408": ["    def test_alter_field_with_changed_default(self):\n        before_states = [\n            self.author_name_default,\n            self.author_name_default,\n        ]\n        after_states = [\n            self.author_name_default,\n            self.author_name,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"name\", field=models.CharField(max_length=200)\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, name=\"name\", max_length=200\n        )\n", "    def test_deconstructible_object_default(self):\n        model_state = ModelState(\n            \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n        )\n        model_state.options = {\n            \"default\": DeconstructibleObject(\n                \"a\", (\"b\", \"c\"), d=DeconstructibleObject(1)\n            )\n        }\n        from_state = ProjectState()\n        from_state.add_model(model_state.clone())\n        to_state = ProjectState()\n        to_state.add_model(model_state.clone())\n        to_state.models[\"testapp\", \"Author\"].options[\"default\"] = DeconstructibleObject(\n            \"a\", (\"b\", \"c\"), d=DeconstructibleObject(2)\n        )\n        autodetector = MigrationAutodetector(from_state, to_state)\n        changes = autodetector._detect_changes()\n        self.assertEqual(len(changes), 1)\n        self.assertEqual(\n            changes[\"testapp\"][0].operations[0].__class__.__name__,\n            \"AlterField\",\n        )\n", "    def test_deconstructible_objects(self):\n        questioner = MigrationQuestioner()\n        autodetector = MigrationAutodetector(\n            self.author_empty.clone(),\n            self.author_name.clone(),\n            questioner=questioner,\n        )\n        changes = autodetector._detect_changes()\n        expected_changes = {\n            \"testapp\": [\n                operations.CreateModel(\n                    name=\"Author\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=200)),\n                    ],\n                    options={\n                        \"managed\": True,\n                        \"db_table\": \"django_content_type\",\n                    },\n                    bases=(\"app_label\",),\n                    managers=[],\n                )\n            ]\n        }\n        self.assertEqual(autodetector.arrange_for_graph(changes, MigrationGraph()).values(), expected_changes)\n        self.assertEqual(\n            self.repr_changes(\n                autodetector.arrange_for_graph(changes, MigrationGraph()),\n                include_dependencies=False,\n            ),\n            \"\"\"\\", "    def test_alter_index_together_with_renamed_index(self):\n        before_states = [\n            author_renamed_with_db_table_options,\n        ]\n        after_states = [\n            author_renamed_with_new_db_table_options,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['AlterIndexTogether'])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"Author\", table='author_three')\n", "    def test_generate_created_proxies(self):\n        # Create models\n        author_proxy = self.author_proxy_options\n        book_proxy = self.book_proxy_fk\n\n        # Make project states\n        project_state = self.make_project_state([author_proxy, book_proxy])\n\n        # Make migration changes\n        changes = self.get_changes([], [project_state])\n\n        # Check results\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n\n        # Check migrations\n        testapp_migrations = changes[\"testapp\"]\n        otherapp_migrations = changes[\"otherapp\"]\n        self.assertMigrationDependencies(\n            changes, \"testapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\", None, True)]\n        )\n        self.assertMigrationDependencies(\n            changes, \"otherapp\", 0, [(\"__setting__\", \"AUTH_USER_MODEL\", None, True)]\n        )\n\n        # Check operations\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"CreateModel\", \"AlterModelOptions\"]\n        )\n        self.assertOperationTypes(\n            changes, \"otherapp\", 0, [\"CreateModel\", \"AlterModelOptions\"]\n        )\n        testapp_create_model = testapp_migrations[0].operations[0]\n        otherapp_create_model = otherapp_migrations[0].operations[0]\n        self.assertOperationFieldAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name=\"AuthorProxy\",\n            fields=[],\n            options={\"verbose_name\": \"Super Author\"},\n            bases=[\"testapp.author\"],\n        )\n        self.assertOperationFieldAttributes(\n            changes,\n            \"otherapp\",\n            0,\n            0,\n            name=\"Book\",\n            fields=[\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"author\", models.ForeignKey(\"thirdapp.AuthorProxy\", models.CASCADE)),\n                (\"title\", models.CharField(max_length=200)),\n            ],\n        )\n\n        # Check dependencies\n        testapp_migrations[0].dependencies = [\n            (\"__setting__\", \"AUTH_USER_MODEL\", None, True),\n            (\"testapp\", \"__first__\",),\n        ]\n        otherapp_migrations[0].dependencies = [\n            (\"__setting__\", \"AUTH_USER_MODEL\", None, True),\n            (\"testapp\", \"__first__\",),\n        ]\n"], "sample_699": ["    def test_repr_failure(self, pytester: Pytester) -> None:\n        # create a doctest with a failure\n        pytester.makefile(\n            \".txt\",\n            doctest=r\"\"\"\n                >>> 1 + 1 == 2\n                False\n                \"\"\",\n        )\n        # run doctest with --doctest-modules and --doctest-report=cdiff\n        result = pytester.runpytest(\n            \"--doctest-modules\", \"--doctest-report=cdiff\", \"test_doctest.txt\"\n        )\n        # check the expected output\n        result.stdout.fnmatch_lines(\n            [\"File\", \"Line\", \"Function\", \"Message\", \"C DIFF\"]\n        )\n", "def test_get_optionflags():\n    parent = pytest.Config()\n    parent.getini = lambda x: \"ELLIPSIS\"\n    parent.getoption = lambda x: \"ELLIPSIS\"\n    assert get_optionflags(parent) == doctest.DONT_ACCEPT_TRUE_FOR_1\n    assert get_optionflags(parent) == doctest.DONT_ACCEPT_TRUE_FOR_1\n    assert get_optionflags(parent) == doctest.DONT_ACCEPT_TRUE_FOR_1\n", "def test_doctest_module_collection(tmpdir, pytester: Pytester) -> None:\n    # Test that a directory containing both a setup.py and a .py file is\n    # collected.\n    tmpdir.join(\"setup.py\").write(\n        textwrap.dedent(\n            \"\"\"\n            from setuptools import setup\n\n            setup(\n                name=\"example\",\n                version=\"0.1\",\n                author=\"author\",\n                author_email=\"author@example.com\",\n                description=\"example package\",\n                packages=[\"example\"],\n            )\n            \"\"\"\n        )\n    )\n    tmpdir.join(\"example.py\").write(\n        textwrap.dedent(\n            \"\"\"\n                \"\"\"\n                >>> example_function()\n                None\n\n                \"\"\"\n                >>> example_function_2()\n                None\n                \"\"\"\n        \"\"\"\n    )\n    with pytester.makepyfile(\n        content=(\n            \"\"\"\n            \"\"\"\n        )\n    ) as dirpath:\n        pytester.assert_nothing_collection_failed()\n        collected = pytester.inline_run(innodeps=True)\n        assert len(collected.failed) == 1\n        assert len(collected.skipped) == 1\n        pytester.makepyfile(\n            content=(\n                \"\"\"\n                \"\"\"\n            )\n        ).joinpath(\"test_example.py\").write(\n            textwrap.dedent(\n                \"\"\"\n                    \"\"\"\n                    >>> example_function()\n                    None\n\n                    \"\"\"\n                    >>> example_function_2()\n                    None\n                    \"\"\"\n            \"\"\"\n            )\n        )\n        pytester.makepyfile(\n            content=(\n                \"\"\"\n                \"\"\"\n            )\n        ).joinpath(\"test_example.py\").write(\n            textwrap.dedent(\n                \"\"\"\n                    \"\"\"\n                    >>> example_function()\n                    None\n\n                    \"\"\"\n                    >>> example_function_2()\n                    None\n                    \"\"\"\n            \"\"\"\n            )\n        )\n        collected = pytester.inline_run(innodeps=True)\n        assert len(collected.failed) == 1\n        assert len(collected.skipped) == 1\n        pytester.makepyfile(\n            content=(\n                \"\"\"\n                \"\"\"\n            )\n        ).joinpath(\"test_example.py\").write(\n            textwrap.dedent(\n                \"\"\"\n                    \"\"\"\n                    >>> example_function()\n                    None\n\n                    \"\"\"\n                    >>> example_function_2()\n                    None\n                    \"\"\"\n            \"\"\"\n            )\n        )\n        pytest", "    def test_repr_failure(self, pytester: Pytester) -> None:\n        with pytester.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        ) as p:\n            result = pytester.runpytest(p)\n            result.assert_outcomes(passed=1)\n            result.assert_outcomes(failed=0)\n            # Test that repr_failure for failed tests shows the correct location\n            # and lines.\n            lines = result.stderr.lines\n            assert lines[0].startswith(\n                \" FAILED doctest: test_doctest [doctest] test_doctest\"\n            )\n            # Check that the doctest location is correctly shown\n            lines = lines[-1].splitlines()\n            assert len(lines) >= 3\n            assert lines[0].startswith(\">>>\")\n            assert lines[1].startswith(\">>>\")\n            # Check that the expected test is correctly identified\n            assert lines[-2].startswith(\"  File \")\n", "    def test_doctestmodule_collector(self, pytester: Pytester):\n        # Make sure we can create a DoctestModule\n        pytester.makepyfile(\n            \"\"\"\n            doctest.example\n        \"\"\"\n        )\n        pytester.makeconftest(\n            \"\"\"\n            import doctest\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest_subprocess()\n        result.assert_outcomes(passed=1)\n\n        # Test that we collect the module\n        result = pytester.runpytest_subprocess(\"--doctest-modules\", \"--doctest-modules\")\n        result.assert_outcomes(passed=1)\n\n        # Test that we do not collect the setup.py file\n        pytester.makepyfile(\n            \"\"\"\n            setup.py\n            # This should not be collected\n        \"\"\"\n        )\n        result = pytester.runpytest_subprocess(\"--doctest-modules\")\n        result.assert_outcomes(passed=0)\n"], "sample_1177": ["def test_abs_derivative():\n    from sympy.abc import x, y\n    from sympy import re, Abs, diff\n\n    assert N_equals(diff(Abs(x + I*y), x), -y/(Abs(x + I*y)))\n    assert N_equals(diff(Abs(x + I*y), y), x/(Abs(x + I*y)))\n    assert N_equals(diff(Abs(x), x), diff(Abs(x), x, evaluate=False))\n    assert N_equals(diff(Abs(x), y), 0)\n\n    assert raises(ArgumentIndexError, diff, Abs(x + I*y), 2)\n    assert raises(ArgumentIndexError, diff, Abs(x + I*y), 3)\n", "def test_unpolarify_with_polar_lift():\n    x = symbols('x', polar=True)\n    eq = polar_lift(2 + 3*x)\n    unpolarified_eq = unpolarify(eq)\n    assert unpolarified_eq == 2 + 3*x\n", "def test_periodic_argument():\n    from sympy.functions.elementary.complexes import periodic_argument\n\n    # Test that the periodic argument works on a simple complex number\n    assert N_equals(periodic_argument(3 + 4j), 0.9272952180016122)\n\n    # Test that the periodic argument works on a complex number with a small\n    # imaginary part\n    assert N_equals(periodic_argument(3 + 0.0001j), 0.0001)\n\n    # Test that the periodic argument works on a complex number with a large\n    # imaginary part\n    assert N_equals(periodic_argument(3 + 1e6j), 1.5707963267948966)\n\n    # Test that the periodic argument works on a complex number with a negative\n    # real part\n    assert N_equals(periodic_argument(-3 + 4j), -0.9272952180016122)\n\n    # Test that the periodic argument works on a complex number with a negative\n    # imaginary part\n    assert N_equals(periodic_argument(3 - 0.0001j), 5.000099999999999)\n\n    # Test that the periodic argument works on a complex number with a large\n    # negative imaginary part\n    assert N_equals(periodic_argument(3 - 1e6j), -1.5707963267948966)\n\n    # Test that the periodic argument works on a real number\n    assert periodic_argument(3) == 0\n\n    # Test that the periodic argument works on a zero\n    assert periodic_argument(0) is None\n\n    # Test that the periodic argument works on a negative number\n    assert periodic_argument(-3) is None\n", "def test_abs_unpolarify(self):\n    from sympy import Abs, unpolarify, polar_lift\n\n    # Test unpolarify of Abs (this also tests that exp_polar is being\n    # unpolarified)\n    self.assertTrue(N_equals(abs(unpolarify(Abs(polar_lift(2)))), abs(2)))\n    self.assertTrue(N_equals(abs(unpolarify(Abs(polar_lift(-2))), 2))\n    self.assertTrue(N_equals(abs(unpolarify(Abs(polar_lift(2*I)))), 2))\n    self.assertTrue(N_equals(abs(unpolarify(Abs(polar_lift(-2*I))), 2))\n    self.assertTrue(N_equals(abs(unpolarify(Abs(2 + 2*I))), 2*abs(2 + 2*I)))\n    self.assertTrue(N_equals(abs(unpolarify(Abs(2 + 2*I**3))), 2*abs(2 + 2*I**3)))\n", "def test_arg():\n    from sympy import arg, Symbol, oo, I, pi\n    x = Symbol('x', real=True)\n\n    # Test arg of real number\n    assert N_equals(arg(2).evalf(), 0)\n\n    # Test arg of complex number\n    assert N_equals(arg(2 + I).evalf(), pi/4)\n\n    # Test arg of zero\n    assert N_equals(arg(0).evalf(), S.NaN)\n\n    # Test arg of zero (2)\n    assert N_equals(arg(0).evalf(), S.NaN)\n\n    # Test arg of Symbol\n    assert arg(x).is_real\n    assert arg(x).is_finite\n\n    # Test arg of exp_polar\n    assert N_equals(arg(exp_polar(10*I*pi)).evalf(), 0)\n\n    # Test arg of exp_polar with periodic argument\n    assert N_equals(arg(exp_polar(5*I*pi)).evalf(), pi)\n\n    # Test arg of exp_polar with large periodic argument\n    assert N_equals(arg(exp_polar(15*I*pi)).evalf(), 3*pi)\n\n    # Test arg of exp_polar with large periodic argument and zero\n    assert N_equals(arg(exp_polar(15*I*pi)).evalf(), -3*pi)\n\n    # Test derivative of arg\n    from sympy import diff\n    assert diff(arg(2 + I), I).evalf() == -1\n\n    # Test derivative of arg with respect to a Symbol\n    assert diff(arg(x), x) == 0\n\n    # Test derivative of arg with respect to a complex number\n    assert diff(arg(2 + I), I) == -1\n"], "sample_895": ["def test_column_transformer_sparse_threshold():\n    \"\"\"Test the sparse_threshold parameter.\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    X_sparse = sparse.csr_matrix(X)\n    X_sparse.set_shape(X.shape)\n    # Test case 1: sparse_threshold=0.3, 50% density\n    ct = ColumnTransformer(\n        [(\"drop\", \"passthrough\", [0]), (\"sparse\", \"passthrough\", [1])], sparse_threshold=0.3\n    )\n    assert ct.sparse_output_ is False\n    assert_array_equal(ct.fit_transform(X), X)\n\n    # Test case 2: sparse_threshold=0.3, 100% density\n    ct = ColumnTransformer(\n        [(\"drop\", \"passthrough\", [0]), (\"sparse\", \"passthrough\", [1])], sparse_threshold=0.3\n    )\n    assert ct.sparse_output_ is False\n    assert_array_equal(ct.fit_transform(X), X)\n\n    # Test case 3: sparse_threshold=0.3, 0% density\n    ct = ColumnTransformer(\n        [(\"drop\", \"passthrough\", [0]), (\"sparse\", \"passthrough\", [1])], sparse_threshold=0.3\n    )\n    assert ct.sparse_output_ is True\n    assert_array_equal(ct.fit_transform(X_sparse), X_sparse)\n\n    # Test case 4: sparse_threshold=0, always dense\n    ct = ColumnTransformer(\n        [(\"drop\", \"passthrough\", [0]), (\"sparse\", \"passthrough\", [1])], sparse_threshold=0\n    )\n    assert ct.sparse_output_ is False\n    assert_array_equal(ct.fit_transform(X), X)\n\n    # Test case 5: sparse_threshold=0, always sparse\n    ct = ColumnTransformer(\n        [(\"drop\", \"passthrough\", [0]), (\"sparse\", \"passthrough\", [1])], sparse_threshold=0\n    )\n    assert ct.sparse_output_ is True\n    assert_array_equal(ct.fit_transform(X_sparse), X_sparse)\n", "def test_fit_transform_output_dense_sparse_mix():\n    \"\"\"Test that `fit_transform` can handle a mix of dense and sparse outputs.\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = [1, 2]\n    transformer = make_column_transformer(\n        (StandardScaler(), [0]),\n        (OneHotEncoder(), [1]),\n        sparse_threshold=0.1,\n    )\n    transformer.fit(X, y)\n    X_t = transformer.fit_transform(X, y)\n    assert not sparse.isspmatrix(X_t)\n    assert sparse.isspmatrix(X_t[0, 1])\n", "    def test_sparse_threshold(self):\n        # Test that sparse_threshold is working as expected\n        transformer = make_column_transformer(\n            (\"sparse_trans\", SparseMatrixTrans(), [\"sparse\"]),\n            (\"dense_trans\", TransNo2D(), [\"dense\"]),\n        )\n        X_sparse = sparse.csr_matrix([[1, 0], [0, 1]])\n        X_dense = np.array([[1, 2], [3, 4]])\n        transformer.fit([X_sparse, X_dense])\n        X_sparse_trans = transformer.transform(X_sparse)\n        X_dense_trans = transformer.transform(X_dense)\n        assert not sparse.isspmatrix(X_sparse_trans) and sparse.isspmatrix(X_dense_trans)\n        assert_array_equal(X_sparse_trans.toarray(), X_sparse.toarray())\n        assert_array_equal(X_dense_trans, X_dense)\n\n        # Test that sparse_threshold is not working as expected\n        transformer.sparse_threshold = 1.0\n        X_sparse_trans = transformer.transform(X_sparse)\n        X_dense_trans = transformer.transform(X_dense)\n        assert sparse.isspmatrix(X_sparse_trans) and sparse.isspmatrix(X_dense_trans)\n        assert_array_equal(X_sparse_trans.toarray(), X_sparse.toarray())\n        assert_array_equal(X_dense_trans, X_dense)\n        assert X_sparse_trans.nnz == X_sparse.nnz\n\n        # Test that sparse_threshold is not working as expected when all sparse\n        transformer.sparse_threshold = 0.0\n        X_sparse_trans = transformer.transform(X_sparse)\n        X_dense_trans = transformer.transform(X_dense)\n        assert not sparse.isspmatrix(X_sparse_trans) and sparse.isspmatrix(X_dense_trans)\n        assert_array_equal(X_sparse_trans.toarray(), X_sparse.toarray())\n        assert_array_equal(X_dense_trans, X_dense)\n        assert X_sparse_trans.nnz == X_sparse.nnz\n", "def test_column_transformer_fit_transform_with_sparse_matrix():\n    \"\"\"Test fit_transform with a sparse matrix.\"\"\"\n    X = sparse.csr_matrix([[1, 0, 2], [0, 0, 3]])\n    transformer = ColumnTransformer(\n        transformers=[(\"scaler\", StandardScaler(), \"all\")],\n        remainder=\"passthrough\",\n    )\n    X_t = transformer.fit_transform(X)\n    expected_output = sparse.hstack(\n        [sparse.csr_matrix([[1.0, 0.0], [0.0, 0.0]]), X]\n    )\n    assert_array_equal(X_t.toarray(), expected_output.toarray())\n", "    def test_fitted_transformer_called(self):\n        # Test that the fitted transformers are used in the transform method\n        # when calling fit_transform and transform.\n        X = np.array([[1, 2], [3, 4]])\n        transformer = ColumnTransformer([(\"transform\", Trans(), [0])])\n        transformer.fit(X)\n        X_transformed = transformer.transform(X)\n        assert_allclose(X_transformed, np.array([[2], [8]]))\n"], "sample_264": ["    def test_store_and_retrieve_messages_with_empty_storage(self):\n        # Test that storing an empty list of messages and then retrieving it\n        # returns an empty list.\n        storage = self.storage_class()\n        response = self.request.response\n        set_cookie_data(storage, [])\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n        self.assertEqual(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name]), [])\n", "    def test_signer_reuse_warning(self):\n        # RemovedInDjango40Warning: pre-Django 3.1 hashes will be invalid.\n        signer = signing.get_cookie_signer(salt='django.contrib.messages')\n        with warnings.catch_warnings(record=True) as w:\n            # Test that the warning is raised and the hash is invalid.\n            invalid_hash = signer.sign_object([1, 2, 3])\n            self.assertEqual(invalid_hash, 'invalid')\n            self.assertEqual(len(w.records), 1)\n            self.assertEqual(w.records[0].category, RemovedInDjango40Warning)\n            self.assertIn('reused', w.records[0].message)\n", "    def stored_messages_count(self, storage, response):\n        return stored_cookie_messages_count(storage, response)\n", "    def test_bad_signature_rejected(self):\n        # Set up the storage backend\n        storage = self.storage_class(self.request)\n        \n        # Set the initial cookie data with valid messages\n        set_cookie_data(storage, [Message('INFO', 'Hello, world!', tags=['key', 'value'])], encode_empty=True)\n        \n        # Set the bad signature cookie\n        bad_signature_data = storage._decode(b'invalid_data')\n        set_cookie_data(storage, bad_signature_data, invalid=True, encode_empty=True)\n        \n        # Try to decode the bad signature cookie and assert it's None\n        self.assertIsNone(storage._decode(storage.request.COOKIES[CookieStorage.cookie_name]))\n", "    def test_stored_messages_count_invalid_data(self):\n        \"\"\"\n        Test that the storage backend can handle invalid encoded data.\n        \"\"\"\n        storage = self.storage_class(self.request)\n        set_cookie_data(storage, [Message(message='test')])\n        response = self.response\n        # set a valid cookie for comparison\n        valid_storage = self.storage_class(self.request)\n        valid_storage._encode([Message(message='test')])\n        valid_storage._update_cookie(valid_storage._encode([Message(message='test')], encode_empty=True), response)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n"], "sample_746": ["    def test_matthews_corrcoef_binary(self):\n        y_true = np.array([1, 1, -1, -1])\n        y_pred = np.array([1, -1, 1, -1])\n        assert_almost_equal(matthews_corrcoef(y_true, y_pred), 1.0)\n", "def test_cohen_kappa_disagreeing_labels():\n    rng = check_random_state(42)\n    y1 = rng.randint(0, 5, size=100)\n    y2 = rng.randint(0, 6, size=100)  # disagreeing on the number of labels\n    with assert_raise_message(UndefinedMetricWarning,\n                             \"Target is .*multioutput.*\"):\n        cohen_kappa_score(y1, y2)\n", "    def test_matthews_corrcoef_binary(self):\n        # Binary classification\n        y_true = np.array([1, 0, 1, 0, 1, 0])\n        y_pred = np.array([1, 0, 1, 0, 0, 1])\n        self.assertAlmostEqual(matthews_corrcoef(y_true, y_pred), -0.33)\n        \n        # Binary classification with 1 class weight\n        y_true = np.array([1, 0, 1, 0, 1, 0])\n        y_pred = np.array([1, 0, 1, 0, 0, 1])\n        self.assertAlmostEqual(matthews_corrcoef(y_true, y_pred, sample_weight=np.array([1, 0, 1, 0, 1, 0])), -0.33)\n        \n        # Binary classification with 2 class weights\n        y_true = np.array([1, 0, 1, 0, 1, 0])\n        y_pred = np.array([1, 0, 1, 0, 0, 1])\n        self.assertAlmostEqual(matthews_corrcoef(y_true, y_pred, sample_weight=np.array([2, 0, 2, 0, 2, 0])), -0.33)\n        \n        # Binary classification with all weights\n        y_true = np.array([1, 0, 1, 0, 1, 0])\n        y_pred = np.array([1, 0, 1, 0, 0, 1])\n        self.assertAlmostEqual(matthews_corrcoef(y_true, y_pred, sample_weight=np.array([3, 0, 3, 0, 3, 0])), -0.33)\n        ", "def test_precision_recall_fscore_support_multiclass_average_macro():\n    # Get data\n    X, y = make_multilabel_classification(n_samples=100, n_features=20,\n                                         n_classes=3, random_state=1)\n    # Convert to binary classification problem (only the first class matters)\n    y = (y == 0).astype(int)\n    # Make predictions\n    y_pred = (y > 0).astype(int)\n    # Test\n    precision, recall, fbeta_score, _ = precision_recall_fscore_support(y, y_pred,\n                                                                        average='macro')\n    assert_almost_equal(precision, 0.06666666666666666)\n    assert_almost_equal(recall, 0.06666666666666666)\n    assert_almost_equal(fbeta_score, 0.06666666666666666)\n", "def test_hinge_loss_unsupported():\n    y_true = np.array([0, 1, 2])\n    pred_decision = np.array([1, 2, 3])\n    with assert_raise_message(ValueError, r\"The multilabel margin is not supported\"):\n        hinge_loss(y_true, pred_decision, labels=np.array([0, 1, 2, 3]))\n"], "sample_638": ["def test_run_args_invalid_project_name():\n    \"\"\"Test a run function with invalid project name.\"\"\"\n    with pytest.raises(SystemExit) as exc:\n        sys.argv = [\"pyreverse\", \"--project\", \"invalid\", \".\"]\n        Run(sys.argv[1:]).run(sys.argv[1:])\n    assert exc.value.code == 1\n\n", "def test_invalid_output_format_not_supported_by_graphviz(caplog, mock_subprocess, setup_path):\n    mock_subprocess.run.return_value = mock.Mock(\n        returncode=1,\n        stderr=(\n            'Format: \"xyz\" not recognized. Use one of: '\n            \"bmp canon cgimage cmap cmapx cmapx_np dot dot_json eps exr fig gd \"\n            \"gd2 gif gv icns ico imap imap_np ismap jp2 jpe jpeg jpg json json0 \"\n            \"mp pct pdf pic pict plain plain-ext png pov ps ps2 psd sgi svg svgz \"\n            \"tga tif tiff tk vdx vml vmlz vrml wbmp webp xdot xdot1.2 xdot1.4 xdot_json\"\n        ),\n    )\n    with pytest.raises(SystemExit):\n        main.Run(sys.argv[1:]).run([\"--output\", \"xyz\"])\n    assert \"Format xyz is not supported natively.\" in caplog.text\n    assert \"Pyreverse will try to generate it using Graphviz...\" in caplog.text\n    assert \"Format xyz is not supported by Graphviz\" in caplog.text\n", "def test_run_ignore_list(mock_subprocess, mock_graphviz, setup_path):\n    \"\"\"\n    Test run method with ignore list\n    \"\"\"\n    with open(\"pyreverse.py\", \"w\") as f:\n        f.write(\n            \"\"\"\n            from pylint.pyreverse.inspector import project_from_files\n            project_from_files(['test_file.py'], project_name='test_project', black_list=['test_file.py'])\n            \"\"\"\n        )\n\n    ignore_file = os.path.join(setup_path, \"ignore_file\")\n    with open(ignore_file, \"w\") as f:\n        f.write(\"test_file.py\")\n\n    with patch(\"pylint.pyreverse.utils.glob.glob\") as mock_glob:\n        mock_glob.return_value = [\"test_file.py\"]\n        sys.argv = [\"pyreverse.py\", \"test_file.py\", \"--ignore\", ignore_file]\n        result = main.run(sys.argv[1:])\n        assert result == 0\n        assert mock_glob.call_args_list == [call(os.path.join(setup_path, \"*.py\"))]\n        assert mock_subprocess.run.call_args_list == [\n            call([\"/usr/bin/dot\", \"-Tpng\", \"-o\", \"test_file.png\", \"-Gcharset=UTF-8\"], check=True)\n        ]\n", "def test_run_output_directory_invalid_format(mocker, mock_graphviz, setup_path):\n    \"\"\"Test that the output directory is used even if the output format is not directly supported.\"\"\"\n    # Create a temporary file with the output directory and a non-supported format\n    output_file = os.path.join(setup_path, \"test_output.txt\")\n    with open(output_file, \"w\") as f:\n        f.write(\"Test output\")\n    sys.argv = [\"pyreverse\", \"--output\", \"unknown\", \"-o\", output_file]\n    with mock.patch(\"pylint.pyreverse.writer.DiagramWriter.write\") as mock_write:\n        with pytest.raises(SystemExit):\n            Run(sys.argv[1:])\n        mock_write.assert_called_once_with(output_file)\n        assert os.path.exists(output_file)\n", "def test_output(mock_subprocess, mock_graphviz, setup_path):\n    with mock.patch(\"pylint.pyreverse.utils.graphviz\") as mock_graphviz:\n        mock_graphviz.check_output.return_value = b\"<svg>svg data</svg>\"\n        with pytest.raises(SystemExit) as excinfo:\n            main.main([\"--output=svg\", \"non_existent_module.py\"])\n        assert excinfo.value.code == 1\n"], "sample_121": ["    def test_index_together(self):\n        class MyModel(models.Model):\n            class Meta:\n                index_together = [('field1', 'field2')]\n\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        # Test that index_together is a list or tuple\n        with self.assertRaisesRegex(Error, \"'index_together' must be a list or tuple\"):\n            MyModel(index_together=['field1', 'field2'])\n\n        # Test that index_together elements are lists or tuples\n        with self.assertRaisesRegex(Error, \"All 'index_together' elements must be lists or tuples\"):\n            MyModel(index_together=['field1', ['field2']])\n\n        # Test that invalid field names are caught\n        with self.assertRaisesRegex(Error, \"'ordering' refers to the nonexistent field\"):\n            MyModel(index_together=['field1', 'nonexistent_field'])\n", "    def test_check_single_primary_key(self):\n        class Model(models.Model):\n            id = models.AutoField(primary_key=True)\n            foo = models.CharField(max_length=10)\n            bar = models.CharField(max_length=10, primary_key=True)\n\n        with self.assertWarningsMatch([\n            Error(\n                \"'The model cannot have more than one field with 'primary_key=True'.\",\n                id='models.E026',\n                obj=Model,\n            )\n        ]):\n            Model.check()\n", "    def test_check_field_name_clashes(self):\n        # Test that _check_field_name_clashes raises an error when field names clash\n        class Model(models.Model):\n            field1 = models.CharField(max_length=255)\n            field1 = models.CharField(max_length=255)\n\n        with self.assertRaisesRegex(\n            checks.Error,\n            \"'field1' clashes with the field 'field1' from parent model 'Model'.\"\n        ):\n            Model.check()\n", "    def test_check_long_column_names(self):\n        # Test long column name check in _check_long_column_names method\n        with self.assertWarnsRegex(Warning, 'Autogenerated column name too long'):\n            class LongColumnModel(models.Model):\n                column = models.CharField(max_length=255)\n\n            LongColumnModel._meta.local_fields = [\n                models.CharField(max_length=255, db_column='a' * 255)\n            ]\n\n            checks.check_models_reference_models([LongColumnModel])\n", "    def test_check_swappable(self):\n        with self.assertRaises(ImproperlyConfigured):\n            models.Model._meta.swapped = 'app_label.app_name'\n            models.Model.check()\n"], "sample_702": ["def test_linecomp():\n    tester = Pytester(request=pytestconfig, tmp_path_factory=tmp_path)\n    linecomp = tester.linecomp\n    linecomp.assert_contains_lines([\"test\"])\n    linecomp.stringio.write(\"test\\n\")\n    linecomp.assert_contains_lines([\"test\"])\n    linecomp.assert_contains_lines([\"test\", \"test2\"])\n    linecomp.stringio.write(\"test2\\n\")\n    linecomp.assert_contains_lines([\"test\", \"test2\"])\n", "def test_Pytester__runpytest_inprocess_matches_runpytest(tmp_path):\n    pytester = Pytester(tmp_path)\n    pytester.makepyfile(\"def test_pass(): pass\")\n    result = pytester.runpytest_inprocess(\"-v\", \"--junit-xml=tmp_path\")\n    result.assert_outcomes(passed=1)\n", "def test_pytester_copy_example(tmp_path: Path) -> None:\n    pytester = Pytester(tmp_path, _ispytest=False)\n    copied_dir = pytester.copy_example(\"example.py\")\n    assert isinstance(copied_dir, Path)\n    assert copied_dir.exists()\n    assert copied_dir.is_dir()\n", "def test_pytester_syspathinsert_after_restore(monkeypatch: MonkeyPatch, tmp_path: Path) -> None:\n    monkeypatch.setenv(\"PATH\", \"/usr/bin:/bin:/usr/local/bin\")\n    testdir = Pytester(tmp_path)\n    testdir.syspathinsert(\"/tmp/mytest\")\n    assert \"/tmp/mytest\" in sys.path\n    testdir.finalize()\n    assert \"/tmp/mytest\" not in sys.path\n", "def test_syspathinsert(tmp_path: Path) -> None:\n    \"\"\"Test sys.path.insert works.\n\n    This ensures we don't get a RecursionError when we import the same module\n    twice, and that we can use the same name for the module in sys.path\n    multiple times.\n    \"\"\"\n    # Check that we can insert the same path twice.\n    p = tmp_path / \"module\"\n    p.mkdir()\n    (p / \"__init__.py\").touch()\n    with Pytester() as pytester:\n        pytester.syspathinsert(str(p))\n        pytester.syspathinsert(str(p))\n        with pytest.raises(ModuleNotFoundError):\n            importlib.import_module(\"module.module\")\n        p2 = tmp_path / \"module2\"\n        p2.mkdir()\n        (p2 / \"__init__.py\").touch()\n        pytester.syspathinsert(str(p2))\n        pytester.syspathinsert(str(p2))\n        # Check that we can import the same module twice.\n        m1 = importlib.import_module(\"module.module\")\n        m2 = importlib.import_module(\"module.module\")\n        assert m1 is m2\n        # Check that we can use the same name in sys.path multiple times.\n        m1 = importlib.import_module(\"module2.module2\")\n        m2 = importlib.import_module(\"module2.module2\")\n        assert m1 is m2\n"], "sample_1194": ["def test_JuliaCodePrinter_precision():\n    from sympy.printing.julia import JuliaCodePrinter\n    printer = JuliaCodePrinter({'precision': 10})\n    expr = Rational(1, 7)\n    code = printer.doprint(expr)\n    assert code == \"1 // 7\"\n    printer = JuliaCodePrinter({'precision': 20})\n    code = printer.doprint(expr)\n    assert code == \"1 // 7\"\n    printer = JuliaCodePrinter({'precision': 15})\n    code = printer.doprint(expr)\n    assert code == \"1 // 7\"\n", "def test_julia_code_jn_y():\n    x = symbols('x')\n    expr = jn(0, x)\n    assert julia_code(expr) == 'sqrt(pi / (2 * x)) * besselj(0.5, x)'\n    expr = jn(1, x)\n    assert julia_code(expr) == 'sqrt(pi / (2 * x)) * besselj(1.5, x)'\n    expr = yn(0, x)\n    assert julia_code(expr) == 'sqrt(pi / (2 * x)) * bessely(0.5, x)'\n    expr = yn(1, x)\n    assert julia_code(expr) == 'sqrt(pi / (2 * x)) * bessely(1.5, x)'\n", "compilation error", "def test_julia_code_matrix_multiply():\n    A = Matrix([[x, y], [z, 0]])\n    B = Matrix([[x, 0], [0, z]])\n    expr = A * B\n    expected_output = \"(%s .* %s) .* (%s .* %s)\" % (A, B, A, B)\n    assert julia_code(expr) == expected_output\n", "def test_julia_code_contraction():\n    from sympy.printing.julia import julia_code\n\n    # Test the contraction of Indexed expressions\n    i = Symbol('i')\n    x = Symbol('x', shape=(3,))\n    i1, i2 = Symbol('i1'), Symbol('i2')\n    expr = Eq(x[i1, i2] - x[i2, i1], 0)\n    expected = 'x[i1, i2] - x[i2, i1] = 0'\n    assert julia_code(expr, contract=True) == expected\n\n    # Test the contraction of Indexed expressions with different shapes\n    y = Symbol('y', shape=(4,))\n    expr = Eq(x[i1, i2] - y[i1, i2], 0)\n    expected = \"x[i1, i2] - y[i1, i2] = 0\"\n    assert julia_code(expr, contract=True) == expected\n\n    # Test the contraction of Indexed expressions with different shapes and indices\n    z = Symbol('z', shape=(2,))\n    expr = Eq(x[i1, i2] - y[i1, i2] - z[i1], 0)\n    expected = \"x[i1, i2] - y[i1, i2] - z[i1] = 0\"\n    assert julia_code(expr, contract=True) == expected\n\n    # Test the contraction of Indexed expressions with repeat indices\n    expr = Eq(x[i1, i2, i2] - y[i1, i2, i2], 0)\n    expected = 'x[i1, i2, i2] - y[i1, i2, i2] = 0'\n    assert julia_code(expr, contract=True) == expected\n\n    # Test the contraction of Indexed expressions with non-contractible indices\n    expr = Eq(x[i1, i2] + x[i2, i1], 0)\n    expected = 'x[i1, i2] + x[i2, i1] = 0'\n    assert julia_code(expr, contract=False) == expected\n\n    # Test the contraction of Indexed expressions with non-Indexed expressions\n    expr = Eq(x[i1, i2] + 2, 0)\n    expected = 'x[i1, i2] + 2 = 0'\n   "], "sample_606": ["def test_broadcast_compat_data():\n    # Test broadcast_compat_data with Variables\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\"])\n\n    broadcast_dims = tuple([\"x\"])\n    core_dims = tuple([\"y\"])\n    result = broadcast_compat_data(a, broadcast_dims, core_dims)\n\n    expected = np.array([[1, 2, 3], [4, 5, 6]])\n    assert_array_equal(result, expected)\n\n    # Test broadcast_compat_data with different sizes\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5], dims=[\"x\"])\n\n    broadcast_dims = tuple([\"x\"])\n    core_dims = tuple([\"y\"])\n    with pytest.raises(ValueError):\n        broadcast_compat_data(a, broadcast_dims, core_dims)\n\n    # Test broadcast_compat_data with missing dimensions\n    a = xr.DataArray([1, 2, 3], dims=[\"x\", \"y\"])\n    b = xr.DataArray([4, 5], dims=[\"x\"])\n\n    broadcast_dims = tuple([\"x\"])\n    core_dims = tuple([\"y\"])\n    with pytest.raises(ValueError):\n        broadcast_compat_data(a, broadcast_dims, core_dims)\n\n    # Test broadcast_compat_data with unexpected dimensions\n    a = xr.DataArray([1, 2, 3], dims=[\"x\"])\n    b = xr.DataArray([4, 5, 6], dims=[\"x\", \"y\"])\n\n    broadcast_dims = tuple([\"x\"])\n    core_dims = tuple([\"y\"])\n    with pytest.raises(ValueError):\n        broadcast_compat_data(a, broadcast_dims, core_dims)\n", "def test_dot_nonexistent_dim():\n    \"\"\"Test passing nonexistent dim in dot.\"\"\"\n    da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"])\n    da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"])\n    with pytest.raises(ValueError) as exc:\n        xr.dot(da_a, da_b, dims=[\"a\", \"z\"])\n    assert str(exc.value) == (\n        \"Dimensions that appear in the result are 'a', 'c', 'b', but \"\n        \"'z' was specified in dims.\"\n    )\n", "def test_unify_chunks():\n    # Test that all dask arrays have the same chunk size\n    # Create a DataArray with different chunk sizes\n    x = xr.DataArray(\n        np.random.rand(100),\n        dims=[\"x\"],\n        chunks=[10, 20],\n    )\n    y = xr.DataArray(\n        np.random.rand(100),\n        dims=[\"x\"],\n        chunks=[10, 20],\n    )\n    z = xr.DataArray(\n        np.random.rand(100),\n        dims=[\"x\"],\n        chunks=[10, 30],\n    )\n\n    # Test that unify_chunks returns a tuple with the same chunk size\n    unified = unify_chunks(x, y, z)\n    for arr in unified:\n        assert arr.chunks[\"x\"] == 10\n\n", "def test_broadcast_compat_data():\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    a = DataArray([1, 2, 3], dims=[\"a\"])\n    b = DataArray([4, 5, 6], dims=[\"a\", \"b\"])\n    c = Variable([\"a\", \"b\"], [4, 5, 6])\n\n    # Test with broadcasted dimensions on the left\n    res = broadcast_compat_data(b, (\"a\",), (\"a\",))\n    assert_identical(res, a)\n\n    # Test with broadcasted dimensions on the right\n    res = broadcast_compat_data(b, (\"a\",), (\"a\", \"c\"))\n    assert_identical(res, c)\n\n    # Test with unexpected dimensions\n    with pytest.raises(ValueError):\n        broadcast_compat_data(b, (\"a\",), (\"a\", \"a\", \"c\"))\n\n    # Test with missing core dimensions\n    with pytest.raises(ValueError):\n        broadcast_compat_data(b, (\"a\",), (\"a\",))\n\n    # Test with missing broadcast dimensions\n    with pytest.raises(ValueError):\n        broadcast_compat_data(b, (\"c\",), (\"a\", \"b\"))\n\n    # Test with duplicate dimensions\n    a = DataArray([1, 1, 3], dims=[\"a\", \"a\"])\n    with pytest.raises(ValueError):\n        unified_dim_sizes([a])\n\n    # Test with dask arrays\n    import dask.array as da\n    a = da.from_array(np.array([1, 2, 3]))\n    b = da.from_array(np.array([4, 5, 6]))\n    res = broadcast_compat_data(b, (\"a\",), (\"a\",))\n    assert isinstance(res, da.core.Array)\n    assert_identical(res.compute(), a)\n\n    # Test with numpy arrays\n    a = np.array([1, 2, 3])\n    b = np.array([4, 5, 6])\n    res = broadcast_compat_data(b, (\"a\",), (\"a\",))\n    assert_identical(res, a)\n", "def test_apply_ufunc_broadcast_compat_data():\n    \"\"\"Test that broadcast_compat_data is working correctly.\"\"\"\n    from .dataarray import DataArray\n\n    # Create an array with a different number of dimensions than the one expected\n    # by the function that created it\n    variable = DataArray(\n        np.arange(10), dims=[\"x\", \"y\"], coords={\"x\": [0, 1, 2], \"y\": [0, 1]}\n    )\n\n    broadcast_dims = (\"z\",)\n    core_dims = (\"x\",)\n\n    expected_result = np.arange(10).reshape(1, 2, 5)\n\n    # Call broadcast_compat_data with the provided variable and core dimensions\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n\n    # Check that the result is as expected\n    assert_allclose(result, expected_result)\n\n    # Test the case where the variable has a different number of dimensions than the core dimensions\n    variable = DataArray(np.arange(10), dims=[\"x\", \"y\"], coords={\"x\": [0, 1, 2], \"y\": [0, 1]})\n    broadcast_dims = (\"z\", \"w\")\n    core_dims = (\"x\",)\n\n    expected_result = np.arange(10).reshape(1, 1, 5)\n\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n\n    assert_allclose(result, expected_result)\n\n    # Test the case where the variable has a dimension that is not in the core dimensions\n    variable = DataArray(np.arange(10), dims=[\"x\", \"y\", \"z\"], coords={\"x\": [0, 1, 2], \"y\": [0, 1]})\n    broadcast_dims = (\"w\",)\n    core_dims = (\"x\",)\n\n    expected_result = np.arange(10).reshape(5, 2, 1)\n\n    result = broadcast_compat_data(variable, broadcast_dims, core_dims)\n\n    assert_allclose(result, expected_result)\n"], "sample_98": ["    def setUpClass(cls):\n        super().setUpClass()\n        # put it in a list to prevent descriptor lookups in test\n        cls.live_server_url_test = [cls.live_server_url]\n", "    def test_run(self):\n        from django.core.servers.basehttp import run\n        from django.core.handlers.wsgi import get_wsgi_application\n\n            return [\n                (b'200 OK', [(b'Content-Type', b'text/plain')], b'Hello, world!')\n            ]\n\n        with self.assertRaises(SystemExit):\n            run('localhost', 8000, my_wsgi_application)\n\n        url = self.live_server_url + '/hello'\n        response = self.urlopen(url)\n        self.assertEqual(response.code, 200)\n        self.assertEqual(response.read(), b'Hello, world!')\n", "    def test_broken_pipe_error(self):\n        # Create a socket and break the pipe\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.connect(('localhost', self.live_server_port))\n        sock.shutdown(socket.SHUT_WR)\n        # Send a request to the server and verify that it ignores the broken pipe error\n        response = self.urlopen('/admin/')\n        self.assertEqual(response.code, 500)\n", "    def test_server_stop_on_signal(self):\n        import signal\n        httpd = WSGIServer((self.live_server_url_test[0], 8000))\n        httpd.serve_forever()\n        # Send a HUP signal to the server to stop it\n        os.kill(os.getpid(), signal.SIGHUP)\n        # This should raise a socket.error\n        with self.assertRaises(socket.error):\n            httpd.handle_request()\n", "    def test_shutdown(self):\n        # Test that the server shuts down cleanly when the connection is closed.\n        # Note that the server doesn't properly handle the connection close\n        # before we have a chance to test it, so we must do it before the\n        # request is finished.\n        response = self.urlopen('/static/')\n        self.assertEqual(response.code, 200)\n\n        sock = socket.create_connection(('localhost', self.live_server.port))\n        sock.close()\n\n        # Test that the server logs the broken pipe error\n        self.assertEqual(len(logger.handlers), 1)  # Check if logger has at least one handler\n        self.assertEqual(logger.handlers[0].level, logging.INFO)  # Check if logger level is INFO\n        self.assertIn(\"Broken pipe from\", logger.handlers[0].get_records().[-1][1])  # Check if the logger has logged a broken pipe error\n"], "sample_114": ["    def test_default_fields(self):\n        changes = self.get_changes([\n            self.author_name,\n            self.author_empty,\n        ], [\n            self.author_name_default,\n        ])\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterField'])\n", "    def setUp(self):\n        self.author_longer = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=400)),\n        ])\n        self.author_longer_null = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=400, null=True)),\n        ])\n        self.author_longer_null_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=400, null=True, default=\"default\")),\n        ])\n", "    def test_altered_fields_pk_field(self):\n        # Test that renaming a primary key field creates an AlterField operation\n        before_states = [self.author_custom_pk]\n        after_states = [self.author_custom_pk]\n        after_states[0].models[\"testapp.Author\"] = self.author_name\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['AlterField'])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name='id', field__name='id', field__type='IntegerField')\n", "    def test_swappable_model_change(self):\n        project_state = ProjectState()\n        # Define models\n        project_state.add_model(\n            ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))])\n        )\n        project_state.add_model(\n            ModelState(\n                \"testapp\",\n                \"SwappableAuthor\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                bases=(\"testapp.Author\",)\n            )\n        )\n        project_state.add_model(\n            ModelState(\n                \"testapp\",\n                \"AuthorProxy\",\n                [(\"id\", models.AutoField(primary_key=True))],\n                bases=(\"testapp.SwappableAuthor\",)\n            )\n        )\n        # Test case: no swappable model change\n        changes = self.get_changes([], [project_state.models])\n        self.assertEqual(changes, {})\n\n        # Test case: swappable model change\n        project_state.models = project_state.models[:-1]\n        project_state.models.append(\n            ModelState(\n                \"testapp\",\n                \"SwappableAuthor\",\n                [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField(max_length=200))],\n                bases=(\"testapp.Author\",)\n            )\n        )\n        changes = self.get_changes([], [project_state.models])\n        self.assertEqual(len(changes[\"testapp\"]), 1)\n        migration = changes[\"testapp\"][0]\n        self.assertEqual(migration.name, \"0001_testapp\")\n        self.assertEqual(migration.operations[0].__class__.__name__, \"RenameModel\")\n        self.assertEqual(migration.operations[0].name_lower, \"author\")\n\n        # Test case: swappable model change with field change\n        project_state.models = project_state.models[:-1]\n        project_state.models.append(\n            ModelState(\n                \"testapp\",\n                \"SwappableAuthor\",\n                [(\"id\", models.AutoField(primary_key=True)), (\"new_field\", models.CharField(max_length=200))],\n                bases=(\"testapp.Author\",)\n            )\n        )\n        changes = self.get_changes([], [project_state.models])\n        self.assertEqual(len(changes[\"testapp\"]), 2)\n        self.assertEqual(changes[\"testapp\"][0].name, \"0001_testapp\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].__class__.__name__, \"RenameModel\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].name_lower, \"author\")\n        self.assertEqual(changes[\"testapp\"][1].name, \"0002_test", "def test_generate_renamed_models_renames_proxy_model(self):\n    # Generate a migration that renames a proxy model.\n    # We're not changing any other models, so we're not expected to generate any other\n    # migrations or changes.\n    before_states = [\n        self.author_proxy,\n    ]\n    after_states = [\n        self.author_proxy_options,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, \"testapp\", 0)\n    self.assertEqual(changes[\"testapp\"][0].name, \"0001_initial\")\n    self.assertEqual(\n        changes[\"testapp\"][0].operations,\n        [operations.CreateModel(\n            name=\"AuthorProxy\",\n            fields=[],\n            options={\"verbose_name\": \"Super Author\"},\n            bases=(\"testapp.author\",),\n        )],\n    )\n"], "sample_51": ["    def test_parse_duration_iso8601_seconds(self):\n        \"\"\"parse_duration() should parse an ISO 8601 duration with seconds.\"\"\"\n        value = \"P1S\"\n        self.assertEqual(parse_duration(value), timedelta(seconds=1))\n", "    def test_parse_date_invalid_year(self):\n        \"\"\"Ensure parse_date raises ValueError for invalid year.\"\"\"\n        with self.assertRaises(ValueError):\n            parse_date('2024-02-29-123')\n", "    def test_parse_duration_iso8601_decimal_seconds(self):\n        self.assertEqual(parse_duration('P1DT2H3M4.500000S'), timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000))\n        self.assertEqual(parse_duration('P1DT2H3M4.500000S'), timedelta(days=1, hours=2, minutes=3, seconds=4, microseconds=500000))\n", "    def test_parse_duration_iso8601_seconds(self):\n        self.assertEqual(parse_duration('P1S'), timedelta(seconds=1))\n", "    def test_parse_date_invalid_date(self):\n        self.assertIsNone(parse_date('2022-02-30'))\n"], "sample_229": ["def test_queryset_exists(self):\n    # Test the QuerySet's exists method\n    qs = Number.objects.all()\n    self.assertTrue(qs.exists())\n    self.assertFalse(Number.objects.none().exists())\n    self.assertFalse(Number.objects.filter(num=11).exists())\n", "def test_queryset_update(self):\n    queryset = Number.objects.all()\n    queryset = queryset.annotate(Exists=Exists(Number.objects.filter(num=F('num') + 1)))\n    expected_numbers = [\n        Number(num=0, other_num=10, Exists=True),\n        Number(num=1, other_num=9, Exists=False),\n        Number(num=2, other_num=8, Exists=False),\n        Number(num=3, other_num=7, Exists=False),\n        Number(num=4, other_num=6, Exists=False),\n        Number(num=5, other_num=5, Exists=False),\n        Number(num=6, other_num=4, Exists=False),\n        Number(num=7, other_num=3, Exists=False),\n        Number(num=8, other_num=2, Exists=False),\n        Number(num=9, other_num=1, Exists=False)\n    ]\n    self.assertQuerysetEqual(queryset, expected_numbers, operator.attrgetter('num'), ordered=True)\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_values(self):\n        qs = Number.objects.values('num')\n        self.assertQuerysetEqual(qs, [(i, ) for i in range(10)], ordered=False)\n        self.assertEqual(qs.model, Number)\n        self.assertEqual(qs.query.values_select, ['num'])\n        self.assertEqual(qs.query.extra_select, [])\n        self.assertEqual(qs.query.annotation_select, [])\n        self.assertEqual(qs.query.values_select, ['num'])\n        self.assertEqual(qs._fields, ['num'])\n", "    def setUpTestData(cls):\n        cls.num1 = Number.objects.create(num=1, other_num=10)\n        cls.num2 = Number.objects.create(num=2, other_num=10)\n        cls.num3 = Number.objects.create(num=3, other_num=10)\n"], "sample_654": ["def test_fixturemanager_getfixtureclosure():\n    # arrange\n    fixturemanager = fixtures.FixtureManager(None)\n    node = pytest.nodes.Item(\"test_foo\")\n    fixturenames = [\"fixture1\", \"fixture2\"]\n    parentid = \"parent_node_id\"\n\n    # act\n    initialnames, names_closure, arg2fixturedefs = fixturemanager.getfixtureclosure(\n        fixturenames, node, ignore_args=[]\n    )\n\n    # assert\n    assert initialnames == fixturenames\n    assert names_closure == fixturenames\n    assert arg2fixturedefs == {}\n\n    # act\n    fixturemanager.parsefactories(node, node.nodeid)\n    initialnames, names_closure, arg2fixturedefs = fixturemanager.getfixtureclosure(\n        fixturenames, node, ignore_args=[]\n    )\n\n    # assert\n    assert initialnames == fixturenames\n    assert names_closure == fixturenames\n    assert arg2fixturedefs == {}\n", "def test_getfixturevalue():\n    # Arrange\n    request = FixtureRequest(pytest.Function(\"test_func\"))\n    fixturedef = FixtureDef(\n        fixturemanager=None,\n        baseid=None,\n        argname=\"fixture_name\",\n        func=lambda: \"fixture_value\",\n        scope=\"function\",\n        params=None,\n        unittest=False,\n        ids=None,\n    )\n\n    # Act\n    result = request.getfixturevalue(\"fixture_name\")\n\n    # Assert\n    assert result == \"fixture_value\"\n", "def test_fixture_manager_getfixtureclosure():\n    # create a fixture function\n    @fixtures.fixture\n        return request.config.rootdir\n\n    # create a test function that requests my_fixture\n    @pytest.fixture\n        pass\n\n    # create a fixtureinfo object for test_func\n    fixtureinfo = fixtures.FixtureManager(pytest.config).getfixtureinfo(test_func)\n\n    # check that the fixturenames attribute is computed correctly\n    assert fixtureinfo.fixturenames == ('usefixtures', 'request', 'my_fixture')\n\n    # check that the names_closure attribute is computed correctly\n    assert fixtureinfo.names_closure == ('usefixtures', 'request', 'my_fixture')\n\n    # check that the arg2fixturedefs attribute is computed correctly\n    assert fixtureinfo.arg2fixturedefs == {\n        'request': [fixtures.FixtureDef(\n            None, None, 'request', fixtures.FixtureRequest, 'function', None, False, None\n        )],\n        'my_fixture': [fixtures.FixtureDef(\n            None, None, 'my_fixture', my_fixture, 'function', None, False, None\n        )],\n        'usefixtures': [fixtures.FixtureDef(\n            None, None, 'usefixtures', fixtures.FixtureRequest, 'function', None, False, None\n        )],\n    }\n", "def test_getfixturelookuperror_repr():\n    # getfixturevalue with non-existent fixture\n    request = FixtureRequest(pytest.Function(item=\"test_foo\"))\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request.getfixturevalue(\"non_existent_fixture\")\n    assert \"FixtureLookupError: non_existent_fixture\" in str(excinfo.value)\n\n    # fixture with multiple lookup errors\n        request.getfixturevalue(\"fixture1\")\n        request.getfixturevalue(\"fixture2\")\n    request = FixtureRequest(pytest.Function(item=\"test_func_with_two_fixtures\"))\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request._fillfixtures()\n    assert \"FixtureLookupError: fixture1\" in str(excinfo.value)\n    assert \"FixtureLookupError: fixture2\" in str(excinfo.value)\n\n    # fixture with recursive dependency\n        request.getfixturevalue(\"fixture_with_recursive_dependency\")\n    request = FixtureRequest(pytest.Function(item=\"test_fixture_with_recursive_dependency\"))\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request.getfixturevalue(\"fixture_with_recursive_dependency\")\n    assert (\n        \"recursive dependency involving fixture 'fixture_with_recursive_dependency' detected\"\n        in str(excinfo.value)\n    )\n\n    # fixture with no parameter\n        request.getfixturevalue(\"fixture_with_no_param\")\n    request = FixtureRequest(pytest.Function(item=\"test_fixture_with_no_param\"))\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request.getfixturevalue(\"fixture_with_no_param\")\n    assert (\n        \"The requested fixture has no parameter defined for test: test_fixture_with_no_param\\n\"\n        in str(excinfo.value)\n    )\n\n    # fixture with session scope and no argument\n        request.getfixturevalue(\"fixture_with_session_scope\")\n    request = FixtureRequest(pytest.Function(item=\"test_fixture_with_session_scope\"))\n    with pytest.raises(FixtureLookupError) as excinfo:\n        request.getfixturevalue(\"fixture_with_session_scope\")\n    assert \"fixture 'fixture_with_session_scope' not found\" in str(excinfo.value)\n\n    # fixture with invalid scope\n        request.getfixturevalue(\"fixture_with_invalid_scope\")\n    request = FixtureRequest(pytest.Function(item=\"test_fixture_with_invalid_scope\"))\n    with pytest.raises(ValueError)", "def test_fixture_lookup_error_repr():\n    class Foo:\n        @pytest.fixture\n            pass\n\n    request = FixtureRequest(pytest.Function(\"test_foo\"))\n    request._fixture_defs = {  # pylint: disable=protected-access\n        \"fixture1\": FixtureDef(\n            fixtures.FixtureManager(pytest.Session()),\n            None,\n            \"fixture1\",\n            lambda request: None,\n            \"function\",\n            None,\n            unittest=False,\n        )\n    }\n\n    with pytest.raises(FixtureLookupError) as excinfo:\n        raise FixtureLookupError(\"fixture2\", request)\n\n    assert excinfo.value.args[0] == \"fixture2\"\n    assert excinfo.value.request == request\n    assert excinfo.value.msg is None\n    assert excinfo.value.formatrepr().terminal_value() == textwrap.dedent(\n        \"\"\""], "sample_0": ["def test_convert_to_variance_from_variance(UncertClass):\n    array = np.array([1, 2, 3])\n    unit = u.m**2\n    variance_uncert = UncertClass(array, unit=unit)\n    std_uncert = variance_uncert._convert_to_variance()\n    assert isinstance(std_uncert, StdDevUncertainty)\n    assert std_uncert.uncertainty_type == 'std'\n    assert_array_equal(std_uncert.array, np.sqrt(array))\n    assert std_uncert.unit == u.m\n\n    inv_uncert = UncertClass(array, unit=unit)\n    std_uncert_inv = inv_uncert._convert_to_variance()\n    assert isinstance(std_uncert_inv, StdDevUncertainty)\n    assert std_uncert_inv.uncertainty_type == 'std'\n    assert_array_equal(std_uncert_inv.array, np.sqrt(1 / array))\n    assert std_uncert_inv.unit == u.m\n", "def test_repr_str(UncertClass):\n    \"\"\"Test the representation of uncertainties.\n\n    Test the representation of the uncertainty when we have different types of\n    uncertainties.\n    \"\"\"\n    # Create an NDData instance\n    ndd = NDData([1, 2, 3], unit='m', uncertainty=UncertClass([1, 2, 3]))\n\n    # Test repr and str\n    repr_ndd = repr(ndd)\n    str_ndd = str(ndd)\n    assert 'Uncertainty' in repr_ndd\n    assert 'Uncertainty' in str_ndd\n    assert '1, 2, 3' in repr_ndd\n    assert '1, 2, 3' in str_ndd\n\n    # Test repr and str with no uncertainty\n    ndd = NDData([1, 2, 3], unit='m')\n    repr_ndd = repr(ndd)\n    str_ndd = str(ndd)\n    assert 'Uncertainty' not in repr_ndd\n    assert 'Uncertainty' not in str_ndd\n    assert '1, 2, 3' in repr_ndd\n    assert '1, 2, 3' in str_ndd\n\n    # Test repr and str with None uncertainty\n    ndd = NDData([1, 2, 3], unit='m', uncertainty=None)\n    repr_ndd = repr(ndd)\n    str_ndd = str(ndd)\n    assert 'Uncertainty' not in repr_ndd\n    assert 'Uncertainty' not in str_ndd\n    assert '1, 2, 3' in repr_ndd\n    assert '1, 2, 3' in str_ndd\n", "def test_parent_nddata_lifetime_and_wcref():\n    ndd = NDData([1, 2, 3])\n    ndd_uncert = StdDevUncertainty([1, 2, 3])\n    ndd.uncertainty = ndd_uncert\n    ndd_uncert.parent_nddata = weakref.ref(ndd)\n\n    ndd_array = np.array([1, 2, 3])\n\n    # Test that parent_nddata attribute is not None before the NDData object is deleted\n    assert ndd_uncert.parent_nddata() is not None\n\n    # Test that parent_nddata attribute is None after the NDData object is deleted\n    del ndd\n    assert ndd_uncert.parent_nddata() is None\n\n    # Test that the weak reference to the NDData object is still valid after\n    # unpickling\n    ndd_uncert_pickled = pickle.loads(pickle.dumps(ndd_uncert))\n    assert ndd_uncert_pickled.parent_nddata() is None\n\n    # Test that the weak reference to the NDData object is still valid after\n    # unpickling and the object was saved as a reference\n    ndd_uncert.parent_nddata = weakref.ref(ndd_array)\n    ndd_uncert_pickled = pickle.loads(pickle.dumps(ndd_uncert))\n    assert ndd_uncert_pickled.parent_nddata() is None\n\n    # Test that the weak reference to the NDData object is still valid after\n    # unpickling and the object was saved as a reference and the object exists\n    ndd = NDData([1, 2, 3])\n    ndd_uncert.parent_nddata = weakref.ref(ndd)\n    ndd_uncert_pickled = pickle.loads(pickle.dumps(ndd_uncert))\n    assert ndd_uncert_pickled.parent_nddata() is ndd\n", "def test_set_unit_with_parent_nddata_set_but_unit_not_compatible():\n    with pytest.raises(UnitConversionError) as exc_info:\n        data = NDData([1], unit=u.m)\n        data.uncertainty = VarianceUncertainty([0.1])\n        data.uncertainty.unit = u.s\n        exc_msg = (\"Unit {} of uncertainty in incompatible with unit {} of \"\n                   \"data\".format(u.s, u.m))\n        assert str(exc_info.value) == exc_msg\n", "def test_correlation_with_uncert_class_without_support(UncertClass):\n    # Create a data array\n    data = NDDataArray([1, 2, 3], unit='m')\n\n    # Create an uncertainty\n    uncert = UncertClass([1, 2, 3], unit='m')\n\n    # Try propagating with a correlation of zero\n    with pytest.raises(ValueError):\n        uncert.propagate(np.add, NDData(data), data, correlation=0)\n"], "sample_63": ["    def test_default_builtins(self):\n        engine = Engine()\n        self.assertEqual(engine.builtins, Engine.default_builtins)\n", "    def test_engine_initialization_defaults(self):\n        engine = Engine()\n        self.assertEqual(engine.dirs, [])\n        self.assertFalse(engine.app_dirs)\n        self.assertEqual(engine.autoescape, True)\n        self.assertEqual(engine.context_processors, [])\n        self.assertFalse(engine.debug)\n        self.assertEqual(engine.loaders, ['django.template.loaders.filesystem.Loader'])\n        self.assertEqual(engine.string_if_invalid, '')\n        self.assertEqual(engine.file_charset, 'utf-8')\n        self.assertEqual(engine.libraries, {})\n        self.assertEqual(engine.template_libraries, {})\n        self.assertEqual(engine.builtins, ['django.template.defaulttags', 'django.template.defaultfilters', 'django.template.loader_tags'])\n", "    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n", "    def setUp(self):\n        self.engine = Engine(dirs=[TEMPLATE_DIR])\n", "    def test_engine_configuration(self):\n        # Test default configuration\n        engine = Engine()\n        self.assertEqual(engine.builtins, Engine.default_builtins)\n\n        # Test custom builtins\n        custom_builtins = ['custom.template.builtins']\n        engine = Engine(builtins=custom_builtins)\n        self.assertEqual(engine.builtins, Engine.default_builtins + custom_builtins)\n\n        # Test custom loaders\n        custom_loaders = ['django.template.loaders.cached.Loader']\n        engine = Engine(loaders=custom_loaders)\n        self.assertEqual(len(engine.loaders), 1)\n        self.assertIsInstance(engine.loaders[0], tuple)\n        self.assertEqual(engine.loaders[0][0], 'django.template.loaders.cached.Loader')\n\n        # Test app_dirs\n        engine = Engine(app_dirs=True)\n        self.assertIn('django.template.loaders.app_directories.Loader', engine.loaders)\n\n        # Test debug mode\n        engine = Engine(debug=True)\n        self.assertNotIn('django.template.loaders.cached.Loader', engine.loaders)\n\n        # Test libraries\n        libraries = {'lib1': 'path.to.lib1'}\n        engine = Engine(libraries=libraries)\n        self.assertEqual(engine.libraries, libraries)\n\n        # Test context processors\n        context_processors = ['django.template.context_processors.i18n']\n        engine = Engine(context_processors=context_processors)\n        self.assertEqual(engine.context_processors, context_processors)\n\n        # Test autoescape\n        engine = Engine(autoescape=False)\n        self.assertFalse(engine.autoescape)\n\n        # Test invalid configuration\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(app_dirs=True, loaders=['invalid.loader'])\n\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(app_dirs=True, loaders=['invalid.loader', 'django.template.loaders.app_directories.Loader'])\n\n        with self.assertRaises(ImproperlyConfigured):\n            Engine(app_dirs=True, loaders=['invalid.loader', 'django.template.loaders.app_directories.Loader'], loaders=['django.template.loaders.cached.Loader'])\n"], "sample_220": ["    def test_set_cookie_samesite_none(self):\n        resp = HttpResponse()\n        resp.set_cookie('test_cookie', 'test_value', samesite='none')\n        self.assertEqual(resp.cookies['test_cookie']['samesite'], 'None')\n", "def test_delete_cookie(self):\n    response = HttpResponse()\n    response.set_cookie('test_cookie', 'test_value', max_age=3600)\n    response.delete_cookie('test_cookie')\n    self.assertEqual(response.cookies.get('test_cookie'), None)\n\n    response.delete_cookie('non_existent_cookie')\n    self.assertEqual(response.cookies.get('non_existent_cookie'), None)\n\n    response.set_cookie('test_cookie', max_age=3600)\n    response.delete_cookie('test_cookie', samesite='none')\n    self.assertEqual(response.cookies.get('test_cookie')['samesite'], 'none')\n\n    # Test cookie expiration date when max_age is None\n    response = HttpResponse()\n    response.set_cookie('test_cookie', 'test_value')\n    response.delete_cookie('test_cookie', expires='Thu, 01 Jan 1970 00:00:00 GMT')\n    self.assertEqual(response.cookies.get('test_cookie')['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "def test_set_cookie_max_age(self):\n    response = HttpResponse()\n    response.set_cookie('test_cookie', 'test_value', max_age=3600)\n    self.assertEqual(response.cookies['test_cookie']['max-age'], 3600)\n", "    def test_set_signed_cookie(self):\n        response = HttpResponse()\n        response.set_signed_cookie('key', 'value', 'secret')\n        self.assertIn('Set-Cookie', response.serialize_headers())\n", "    def test_set_default_header(self):\n        response = HttpResponse()\n        response.setdefault('Content-Type', 'application/json')\n        self.assertEqual(response['Content-Type'], 'application/json')\n"], "sample_328": ["    def test_bulk_update(self):\n        self.tags = [\n            Tag.objects.create(name=str(i))\n            for i in range(10)\n        ]\n        Tag.objects.bulk_update(self.tags, ['name'])\n        self.assertEqual(Tag.objects.get(name='0').name, '0')\n        self.assertEqual(Tag.objects.get(name='1').name, '1')\n        self.assertEqual(Tag.objects.get(name='9').name, '9')\n", "    def setUpTestData(cls):\n        cls.notes = [\n            Note.objects.create(note=str(i), misc=str(i))\n            for i in range(10)\n        ]\n        cls.tags = [\n            Tag.objects.create(name=str(i))\n            for i in range(10)\n        ]\n", "    def setUpTestData(cls):\n        cls.person = Individual.objects.create(name='John')\n        cls.note = Note.objects.create(note='Test note', misc='Test misc')\n        cls.note.members.add(cls.person)\n", "    def setUpTestData(cls):\n        cls.tags = [\n            Tag.objects.create(name=str(i))\n            for i in range(10)\n        ]\n", "    def setUpTestData(cls):\n        cls.notes = [\n            Note.objects.create(note=str(i), misc=str(i))\n            for i in range(10)\n        ]\n"], "sample_138": ["    def test_hash_key(self):\n        storage = storage.staticfiles_storage\n        storage.hashed_files = {}\n        self.assertEqual(storage.hash_key('path/to/file.txt'), 'path/to/file.txt')\n        self.assertEqual(storage.hash_key('path/to/file.txt'), 'path/to/file.txt')\n        self.assertNotEqual(storage.hash_key('path/to/file.txt'), 'path/to/file2.txt')\n", "    def test_manifest_files_mixin(self):\n        # Create a temp file\n        temp_file = tempfile.NamedTemporaryFile(mode='w')\n        temp_file.close()\n\n        # Create a temporary storage\n        storage = storage.StaticFilesStorage(location=temp_file.name)\n\n        # Create a storage with ManifestFilesMixin\n        with override_settings(STATICFILES_STORAGE='ManifestStaticFilesStorage'):\n            manifest_storage = storage\n\n            # Create a file\n            with open(temp_file.name, 'w') as f:\n                f.write('test content')\n\n            # Save the file\n            name = 'test.txt'\n            manifest_storage.save(name, open(temp_file.name, 'rb'))\n\n            # Check that the file is saved\n            self.assertTrue(manifest_storage.exists(name))\n\n            # Check that the manifest file is created\n            self.assertTrue(manifest_storage.manifest_storage.exists(manifest_storage.manifest_name))\n\n            # Load the manifest file\n            manifest = json.loads(manifest_storage.manifest_storage.read_manifest())\n\n            # Check that the manifest file contains the correct information\n            self.assertIn(name, manifest['paths'])\n\n            # Delete the file\n            manifest_storage.delete(name)\n\n            # Check that the manifest file has been updated\n            self.assertNotIn(name, manifest['paths'])\n", "    def setUp(self):\n        settings.STATIC_ROOT = TEST_ROOT / 'static'\n        settings.STATIC_URL = '/static/'\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n", "    def test_hashed_name_in_debug_mode(self):\n        # Arrange\n        file_name = 'test.css'\n        file_content = b'Hello, World!'\n        storage = StaticFilesStorage(location=tempfile.mkdtemp())\n        with tempfile.NamedTemporaryFile(mode='wb', delete=False) as temp_file:\n            temp_file.write(file_content)\n            temp_file.close()\n            hashed_name = storage.hashed_name(file_name, content=ContentFile(file_content, name=temp_file.name))\n\n        # Act\n        self.assertIn(os.path.join(tempfile.gettempdir(), hashed_name), storage.listdir())\n\n        # Assert\n        self.assertNotEqual(file_name, hashed_name)\n", "    def test_stored_name(self):\n        # Test with hash present in cached hashed_files\n        staticfiles_storage.hashed_files = {'hash_key': 'cached_hash'}\n        self.assertEqual(\n            staticfiles_storage.stored_name('path/to/file'),\n            'cached_hash'\n        )\n\n        # Test with no cached hash, and manifest_strict set to True\n        staticfiles_storage.manifest_storage = storage.StaticFilesStorage()\n        staticfiles_storage.manifest_storage.hashed_files = {}\n        staticfiles_storage.manifest_strict = True\n        with self.assertRaises(ValueError):\n            staticfiles_storage.stored_name('path/to/file')\n\n        # Test with no cached hash, and manifest_strict set to False\n        staticfiles_storage.manifest_storage = storage.StaticFilesStorage()\n        staticfiles_storage.manifest_storage.hashed_files = {}\n        staticfiles_storage.manifest_strict = False\n        self.assertEqual(\n            staticfiles_storage.stored_name('path/to/file'),\n            'path/to/file'\n        )\n"], "sample_989": ["def test_imath_pow_rational():\n    from sympy import I, Rational\n    assert (I**Rational(1, 2)).is_real is False\n    assert (I**Rational(-1, 2)).is_real is False\n    assert (I**Rational(3, 4)).is_real is False\n    assert (I**Rational(-3, 4)).is_real is False\n    assert (I**Rational(5, 4)).is_real is False\n    assert (I**Rational(-5, 4)).is_real is False\n    assert (I**Rational(1, 3)).is_real is False\n    assert (I**Rational(-1, 3)).is_real is False\n    assert (I**Rational(2, 3)).is_real is False\n    assert (I**Rational(-2, 3)).is_real is False\n\n    assert (I**Rational(1, 5)).is_real is False\n    assert (I**Rational(-1, 5)).is_real is False\n    assert (I**Rational(2, 5)).is_real is False\n    assert (I**Rational(-2, 5)).is_real is False\n    assert (I**Rational(3, 5)).is_real is False\n    assert (I**Rational(-3, 5)).is_real is False\n    assert (I**Rational(4, 5)).is_real is False\n    assert (I**Rational(-4, 5)).is_real is False\n\n    assert (I**Rational(1, 7)).is_real is False\n    assert (I**Rational(-1, 7)).is_real is False\n    assert (I**Rational(2, 7)).is_real is False\n    assert (I**Rational(-2, 7)).is_real is False\n    assert (I**Rational(3, 7)).is_real is False\n    assert (I**Rational(-3, 7)).is_real is False\n    assert (I**Rational(4, 7)).is_real is False\n    assert (I**Rational(-4, 7)).is_real is False\n    assert (I**Rational(5, 7)).is_real is False\n    assert (I**R", "def test_Comparisons():\n    # Ensure that comparing ComplexInfinity and zoo returns True\n    assert zoo == zoo\n    assert zoo == S.ComplexInfinity\n    assert zoo != S.Infinity\n    assert zoo > S.Infinity\n    assert zoo < S.Infinity\n    assert zoo >= S.Infinity\n    assert zoo <= S.Infinity\n    assert zoo > S.NegativeInfinity\n    assert zoo < S.NegativeInfinity\n    assert zoo >= S.NegativeInfinity\n    assert zoo <= S.NegativeInfinity\n    assert zoo != S.NegativeInfinity\n\n    # Ensure that comparing Floats works correctly\n    assert same_and_same_prec(1.0, 1.0)\n    assert not same_and_same_prec(1.0, 1.0000000000000001)\n    assert same_and_same_prec(1.0, 1)\n    assert not same_and_same_prec(1.0, 2)\n    assert not same_and_same_prec(1.0, 2.0)\n\n    # Ensure that comparing Rationals works correctly\n    assert Rational(1, 2) == Rational(1, 2)\n    assert Rational(1, 2) != Rational(1, 3)\n    assert Rational(1, 2) < Rational(1, 3)\n    assert Rational(1, 2) > Rational(1, 3)\n    assert Rational(1, 2) <= Rational(1, 3)\n    assert Rational(1, 2) >= Rational(1, 3)\n\n    # Ensure that comparing AlgebraicNumbers works correctly\n    from sympy import sqrt\n    assert AlgebraicNumber(sqrt(2)) == AlgebraicNumber(sqrt(2))\n    assert AlgebraicNumber(sqrt(2)) != AlgebraicNumber(sqrt(3))\n    assert AlgebraicNumber(sqrt(2)) < AlgebraicNumber(sqrt(3))\n    assert AlgebraicNumber(sqrt(2)) > AlgebraicNumber(sqrt(3))\n    assert AlgebraicNumber(sqrt(2)) <= AlgebraicNumber(sqrt(3))\n    assert AlgebraicNumber(sqrt(2)) >= AlgebraicNumber(sqrt(3))\n\n    # Ensure that comparing Infinity works correctly\n    assert S.Infinity == S.Infinity\n    assert S.Infinity != S.NegativeInfinity\n    assert S.Infinity > S.NegativeInfinity\n    assert S.Infinity < S.NegativeInfinity\n    assert S.Infinity >=", "compilation error", "def test_Float_exp():\n    assert same_and_same_prec(Float(3.5)**2, Float(12.25))\n    assert same_and_same_prec(Float(12.25)**2, Float(151.5625))\n\n    a = Float('1.2')\n    b = Float('2.3')\n    assert same_and_same_prec(a + b, Float('3.5'))\n    assert same_and_same_prec(a - b, Float('-1.1'))\n    assert same_and_same_prec(a * b, Float('2.76'))\n    assert same_and_same_prec(a / b, Float('0.52217391304347826'))\n\n    assert same_and_same_prec(Rational(12, 25)**2, Float(1.488))\n    assert same_and_same_prec(Rational(12, 25) + Float(0.5), Float(0.64))\n    assert same_and_same_prec(Float(0.5) - Rational(12, 25), Float(-0.48))\n    assert same_and_same_prec(Rational(12, 25) * Float(2), Float(0.96))\n    assert same_and_same_prec(Rational(12, 25) / Float(2), Float(0.24))\n\n    assert same_and_same_prec(Float(1).floor(), Integer(1))\n    assert same_and_same_prec(Float(2).ceiling(), Integer(2))\n\n    assert same_and_same_prec(Float('inf'), S.Infinity)\n    assert same_and_same_prec(Float('-inf'), S.NegativeInfinity)\n    assert same_and_same_prec(Float('nan'), nan)\n    assert same_and_same_prec(Float('inf') + 1, S.Infinity)\n    assert same_and_same_prec(Float('-inf') - 1, S.NegativeInfinity)\n    assert same_and_same_prec(Float('nan') + 1, nan)\n\n    assert Float(1.1).as_coeff_Mul(rational=True) == (1.1, 1)\n    assert Float(2.2).as_coeff_Mul(rational=False) == (2.2, 1)\n    assert Float(3.3).as_coeff_Mul(rational=True) == (3.3, 1)\n    assert Float(4.4).as_coeff_Mul(rational=False) == (4.4, 1)\n    assert Float(5.5).as_coeff_Mul(rational=True) == (5.5, 1", "def test_division_by_complex(self):\n    assert isinstance(nan/oo, NaN)\n    assert isinstance(nan/oo.conjugate(), NaN)\n    assert isinstance(oo/nan, NaN)\n    assert isinstance(oo/nan.conjugate(), NaN)\n    assert isinstance(nan/(oo+oo.conjugate()), NaN)\n    assert isinstance(oo+oo.conjugate(), oo)\n"], "sample_902": ["    def test_fit_params(self):\n        # Test that fit_params are passed to the first transformer\n        X, y = datasets.make_classification(n_samples=10, n_features=5,\n                                           n_informative=3, n_redundant=1,\n                                           random_state=42)\n        clf = Pipeline([('t1', TransfFitParams()), ('t2', SVC())])\n        clf.set_params(t1__a=1, t1__b=2)\n        clf.fit(X, y)\n        self.assertEqual(clf.named_steps['t1'].fit_params, {'a': 1, 'b': 2})\n", "    def test_pipeline_transformer_weighting(self):\n        \"\"\"Test that feature weighting works with FeatureUnion.\"\"\"\n        X = np.array([[1, 2], [3, 4]])\n        Xw = np.array([[5, 6], [7, 8]])\n        y = np.array([1, 2])\n\n        transformer1 = lambda x: x * 2\n        transformer2 = lambda x: x * 3\n        transformer_weights = {'transformer1': 0.5, 'transformer2': 0.5}\n\n        union = FeatureUnion([\n            ('transformer1', transformer1),\n            ('transformer2', transformer2)\n        ], transformer_weights=transformer_weights)\n\n        Xt = union.fit_transform(X, y)\n        Xtw = union.fit_transform(Xw, y)\n\n        assert_array_equal(Xt, Xw * 1.5)\n", "def test_transformer_weights():\n    X = np.array([[1, 2], [3, 4]])\n    transformer_list = [('t1', Transf()), ('t2', Transf())]\n    weights = {'t1': 0.5, 't2': 0.5}\n    fu = FeatureUnion(transformer_list, transformer_weights=weights)\n    result = fu.fit_transform(X, [0, 1])\n    expected = np.array([[0.5, 1.0], [1.5, 2.0]]) * 0.5\n    assert_array_almost_equal(result, expected)\n", "    def __init__(self):\n        self.successful = False\n", "    def test_transformer_weights(self):\n        X = np.array([[1, 2], [3, 4]])\n\n        weights = {'trans1': 2, 'trans2': 1}\n        feat_union = FeatureUnion([('trans1', Transf()), ('trans2', Transf())],\n                                 transformer_weights=weights)\n\n        Xt = feat_union.fit_transform(X)\n        assert_array_equal(Xt, np.array([[2, 4], [6, 8]]))\n\n        feat_union = FeatureUnion([('trans1', Transf()), ('trans2', Transf())])\n        Xt = feat_union.fit_transform(X)\n        assert_array_equal(Xt, np.array([[2, 4], [6, 8]])\n"], "sample_621": ["def test_indexes_equal():\n    index1 = PandasIndex(np.arange(5), 'x')\n    index2 = PandasIndex(np.arange(5), 'x')\n    assert indexes_equal(index1, index2, Variable([], index1), Variable([], index2))\n    assert not indexes_equal(index1, index1, Variable([], index2), Variable([], index2))\n\n    index3 = PandasMultiIndex(pd.MultiIndex.from_arrays([np.arange(2)], ['y']), 'y')\n    index4 = PandasMultiIndex(pd.MultiIndex.from_arrays([np.arange(2)], ['y']), 'y')\n    assert indexes_equal(index3, index4, Variable([], index3), Variable([], index4))\n    assert not indexes_equal(index3, index3, Variable([], index4), Variable([], index4))\n\n    index5 = PandasIndex(np.arange(5), 'x')\n    index6 = pd.Index([1, 2, 3, 4, 5])\n    assert not indexes_equal(index5, index6, Variable([], index5), Variable([], index6))\n\n    index7 = PandasIndex(np.arange(5), 'x')\n    index8 = Index()\n    assert not indexes_equal(index7, index8, Variable([], index7), Variable([], index8))\n", "def test_PandasMultiIndex_equals():\n    data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n    index = PandasMultiIndex(data, 'foo')\n\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n    other_index = PandasMultiIndex(other_data, 'foo')\n\n    assert index.equals(other_index)\n\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4)])\n    other_index = PandasMultiIndex(other_data, 'foo')\n\n    assert not index.equals(other_index)\n\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 7)])\n    other_index = PandasMultiIndex(other_data, 'foo')\n\n    assert not index.equals(other_index)\n\n    data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n    index = PandasMultiIndex(data, 'foo')\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n\n    assert index.equals(PandasIndex(other_data, 'foo'))\n\n    data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n    index = PandasMultiIndex(data, 'foo')\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4)])\n\n    assert not index.equals(PandasIndex(other_data, 'foo'))\n\n    data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 6)])\n    index = PandasMultiIndex(data, 'foo')\n    other_data = pd.MultiIndex.from_tuples([(1, 2), (3, 4), (5, 7)])\n\n    assert not index.equals(PandasIndex(other_data, 'foo'))\n", "def test_indexes_from_variables():\n    # Test that a single variable creates a PandasIndex\n    indexes = Indexes({0: PandasIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n    assert isinstance(indexes[0], PandasIndex)\n\n    # Test that multiple variables create a PandasMultiIndex\n    indexes = Indexes({0: PandasIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n    indexes = indexes.add({1: PandasIndex.from_variables({1: Variable(np.array(['a', 'b', 'c']), dims=['category'])})})\n    assert isinstance(indexes[0], PandasMultiIndex)\n\n    # Test that duplicate variables raise an error\n    with pytest.raises(ValueError):\n        indexes = Indexes({0: PandasIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n        indexes = indexes.add({0: PandasIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n\n    # Test that a multi-index with a single variable creates a PandasIndex\n    indexes = Indexes({0: PandasMultiIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n    assert isinstance(indexes[0], PandasIndex)\n\n    # Test that a multi-index with multiple variables creates a PandasMultiIndex\n    indexes = Indexes({0: PandasMultiIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n    indexes = indexes.add({1: PandasMultiIndex.from_variables({1: Variable(np.array(['a', 'b', 'c']), dims=['category'])})})\n    assert isinstance(indexes[0], PandasMultiIndex)\n\n    # Test that _check_dim_compat raises an error when all_dims is \"equal\" and all variables have different dimensions\n    with pytest.raises(ValueError):\n        indexes = Indexes({0: PandasMultiIndex.from_variables({0: Variable(np.array([1, 2, 3]), dims=['time'])})})\n        indexes = indexes.add({1: PandasMultiIndex.from_variables({1: Variable(np.array(['a', 'b', 'c']), dims=['category'])})})\n       ", "def test_PandasMultiIndex_equals():\n    # Test that PandasMultiIndex.equals() handles index ordering correctly\n    idx1 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"x\", current_variables={}, variables={\"y\": xr.Variable(\"y\", [1, 2, 3])}\n    )\n    idx2 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"x\", current_variables={\"y\": xr.Variable(\"y\", [1, 2, 3])}, variables={}\n    )\n\n    assert idx1.equals(idx2)\n\n    # Test that PandasMultiIndex.equals() handles index ordering when one index has a different\n    # dimension coordinate\n    idx3 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"x\", current_variables={}, variables={\"y\": xr.Variable(\"y\", [1, 2, 3])}\n    )\n    idx4 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"y\", current_variables={\"x\": xr.Variable(\"x\", [1, 2, 3])}, variables={}\n    )\n\n    assert not idx3.equals(idx4)\n\n    # Test that PandasMultiIndex.equals() handles index ordering when one index has a different level\n    # coordinate\n    idx5 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"x\", current_variables={}, variables={\"y\": xr.Variable(\"y\", [1, 2, 3])}\n    )\n    idx6 = PandasMultiIndex.from_variables_maybe_expand(\n        dim=\"x\",\n        current_variables={\"y\": xr.Variable(\"y\", [1, 2, 3])},\n        variables={\"z\": xr.Variable(\"z\", [1, 2, 3])},\n    )\n\n    assert not idx5.equals(idx6)\n", "def test_roll_indexes():\n    da = xr.DataArray(np.arange(24).reshape(4, 6), dims=[\"x\", \"y\"])\n    indexes = Indexes({\"x\": da.dims[0], \"y\": da.dims[1]})\n    shifts = {\"x\": 1, \"y\": 2}\n    new_indexes, _ = roll_indexes(indexes, shifts)\n    assert isinstance(new_indexes[\"x\"], PandasIndex)\n    assert isinstance(new_indexes[\"y\"], PandasIndex)\n"], "sample_355": ["    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.custom_user = CustomUser.objects.create_user('custom_test', 'custom_test@example.com', 'custom_test')\n", "    def test_anonymous_user_is_anonymous(self):\n        user = AnonymousUser()\n        self.assertTrue(user.is_anonymous)\n", "    def test_create_user(self):\n        username = 'testuser'\n        email = 'test@example.com'\n        password = 'testpassword'\n        user = UserManager().create_user(username, email, password)\n        self.assertEqual(user.username, username)\n        self.assertEqual(user.email, email)\n        self.assertTrue(user.check_password(password))\n        self.assertFalse(user.is_staff)\n        self.assertFalse(user.is_superuser)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n"], "sample_36": ["def test_biweight_location_zeros():\n    rng = NumpyRNGContext(12345)\n    data = rng.randn(1000)\n    data[data < 0] = 0\n    biloc = biweight_location(data)\n    assert_allclose(biloc, 0)\n", "def test_biweight_location():\n    # Test with scalar input\n    rng = np.random.RandomState(12345)\n    data = np.array([1, 2, 3, 4, 5])\n    biloc = biweight_location(data)\n    assert_allclose(biloc, np.median(data))\n\n    # Test with array input\n    data = np.array([1, 2, 3, 4, 5])\n    biloc = biweight_location(data)\n    assert_allclose(biloc, np.median(data))\n\n    # Test with axis specified\n    data = np.array([[1, 2], [3, 4]])\n    biloc = biweight_location(data, axis=0)\n    assert_allclose(biloc, np.median(data, axis=0))\n\n    # Test with c value\n    data = np.array([1, 2, 3, 4, 5])\n    c = 7.0\n    biloc = biweight_location(data, c=c)\n    assert_allclose(biloc, np.median(data))\n\n    # Test with M specified\n    data = np.array([1, 2, 3, 4, 5])\n    M = 3.0\n    biloc = biweight_location(data, M=M)\n    assert_allclose(biloc, 3.0)\n\n    # Test with M specified as array\n    data = np.array([1, 2, 3, 4, 5])\n    M = np.array([2.0, 3.0])\n    with pytest.raises(ValueError):\n        biweight_location(data, M=M)\n\n    # Test with invalid M type\n    data = np.array([1, 2, 3, 4, 5])\n    M = 'invalid'\n    with pytest.raises(TypeError):\n        biweight_location(data, M=M)\n\n    # Test with invalid c type\n    data = np.array([1, 2, 3, 4, 5])\n    c = 'invalid'\n    with pytest.raises(TypeError):\n        biweight_location(data, c=c)\n", "def test_biweight_midcovariance_with_zeros():\n    with NumpyRNGContext(12345):\n        # Test biweight midcovariance with constant variables\n        data = np.array([[1., 1., 1.], [2., 2., 2.]])\n        bicov = biweight_midcovariance(data)\n        assert_array_almost_equal_nulp(bicov, np.zeros((2, 2)))\n\n        # Test biweight midcovariance with non-constant variables\n        data = np.array([[1., 2., 3.], [4., 5., 6.]])\n        bicov = biweight_midcovariance(data)\n        assert_array_almost_equal_nulp(bicov, np.array([[2., 3.], [3., 5.]]))\n\n        # Test biweight midcovariance with constant variables and a different tuning constant\n        data = np.array([[1., 1., 1.], [2., 2., 2.]])\n        bicov = biweight_midcovariance(data, c=10.)\n        assert_array_almost_equal_nulp(bicov, np.zeros((2, 2)))\n\n        # Test biweight midcovariance with non-constant variables and a different tuning constant\n        data = np.array([[1., 2., 3.], [4., 5., 6.]])\n        bicov = biweight_midcovariance(data, c=10.)\n        assert_array_almost_equal_nulp(bicov, np.array([[2., 3.], [3., 5.]]))\n\n        # Test biweight midcovariance with a 2D array\n        data = np.array([[1., 2., 3.], [4., 5., 6.]])\n        bicov = biweight_midcovariance(data[np.newaxis, :])\n        assert_array_almost_equal_nulp(bicov, np.array([[2., 3.], [3., 5.]]))\n\n        # Test biweight midcovariance with a 1D array\n        data = np.array([1., 2., 3., 4., 5.])\n        bicov = biweight_midcovariance(data[np.newaxis, :])\n        assert_array_almost_equal_nulp(bicov, np.array([[3.]]))\n\n        # Test biweight midcovariance with a single variable\n        data = np.array([1., ", "def test_biweight_midcovariance_with_zero_variances():\n    \"\"\"Test the biweight midcovariance when all variables have zero variance.\"\"\"\n    with NumpyRNGContext(12345):\n        rng = np.random.RandomState(12345)\n        x = rng.normal(0, 0, 200)\n        y = rng.normal(0, 0, 200)\n        bicov = biweight_midcovariance([x, y])\n        assert_array_almost_equal_nulp(bicov, np.zeros((2, 2)), decimal=4)\n\n", "def test_biweight_midcorrelation_outliers():\n    rng = NumpyRNGContext(12345)\n    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 100.0])\n    y = np.array([6.0, 7.0, 8.0, 9.0, 10.0, 10.0])\n    with catch_warnings():\n        with pytest.warns(FutureWarning):\n            bicorr = biweight_midcorrelation(x, y)\n        assert_array_almost_equal_nulp(bicorr, 0.24595651034131315)\n"], "sample_870": ["def test_GaussianProcessRegressor_restarts_optimizer_with_zero_bound_single_kernel():\n    \"\"\"Test GaussianProcessRegressor with multiple restarts of the optimizer\n    with a single kernel.\"\"\"\n    X = np.atleast_2d([1.0, 3.0, 5.0, 6.0, 7.0, 8.0]).T\n    y = np.sin(X).ravel()\n    gpr = GaussianProcessRegressor(kernel=RBF(1.0, (1e-3, 1e3)), n_restarts_optimizer=3)\n    gpr.fit(X, y)\n    assert gpr.log_marginal_likelihood_value_ >= -10.0\n    assert gpr.kernel_.theta[0] >= 1e-3\n    assert gpr.kernel_.theta[0] <= 1e3\n", "def test_log_marginal_likelihood_prior():\n    \"\"\"Test that the log marginal likelihood of the prior can be correctly\n    evaluated when the kernel is None.\"\"\"\n    gpr = GaussianProcessRegressor(kernel=None)\n    X = np.atleast_2d([1.0, 3.0, 5.0, 6.0, 7.0, 8.0]).T\n    y = np.random.normal(size=(6,))\n    assert gpr.log_marginal_likelihood() == -gpr.log_marginal_likelihood(None)\n    assert np.isclose(gpr.log_marginal_likelihood(theta=[1.0, 1.0]), -gpr.log_marginal_likelihood([1.0, 1.0]))\n", "def test_gpr_copy_X_train():\n    # Check that copy_X_train flag affects the object state\n    X = np.atleast_2d([1.0, 3.0, 5.0, 6.0, 7.0, 8.0]).T\n    y = f(X).ravel()\n    X_train_copy = np.copy(X)\n    X_train_no_copy = X\n\n    gpr_copy = GaussianProcessRegressor(copy_X_train=True)\n    gpr_no_copy = GaussianProcessRegressor(copy_X_train=False)\n\n    gpr_copy.fit(X, y)\n    gpr_no_copy.fit(X, y)\n\n    assert not np.array_equal(gpr_copy.X_train_, X_train_copy)\n    assert np.array_equal(gpr_no_copy.X_train_, X_train_no_copy)\n\n    # Check that copy_X_train affects the behavior of predict\n    X_new = np.atleast_2d([2.0, 4.0, 5.5, 6.5, 7.5]).T\n    y_pred_copy = gpr_copy.predict(X_new)\n    y_pred_no_copy = gpr_no_copy.predict(X_new)\n\n    # The predictions should be the same, but the underlying X_train_ arrays\n    # should be different\n    assert_allclose(y_pred_copy, y_pred_no_copy)\n    assert not np.array_equal(gpr_copy.X_train_, gpr_no_copy.X_train_)\n\n    # Check that the objects behave correctly when re-fitting with a different\n    # set of data\n    gpr_copy.fit(X2, f(X2).ravel())\n    gpr_no_copy.fit(X2, f(X2).ravel())\n    assert not np.array_equal(gpr_copy.X_train_, X_train_copy)\n    assert np.array_equal(gpr_no_copy.X_train_, X_train_no_copy)\n", "def test_gaussian_process_regressor_log_marginal_likelihood_convergence_warning():\n    \"\"\"Test that an informative warning is issued when optimization does not converge.\"\"\"\n    gpr = GaussianProcessRegressor(kernel=C(1.0, (1e-2, 1e2)) * RBF(1.0))\n    X, y = np.atleast_2d([1.0, 3.0, 5.0]).T, np.atleast_1d([1.0, 2.0, 3.0])\n    gpr.fit(X, y)\n\n    # Add a non-negative value to the alpha parameter to make it more likely\n    # that the optimizer does not converge\n    gpr.alpha = 1e-5\n\n    with pytest.warns(ConvergenceWarning) as record:\n        gpr.fit(X, y)\n\n    assert len(record) == 1\n    assert \"The kernel parameters of the model are not optimized.\" in str(record[0].message)\n", "def test_gpr_n_restarts_optimizer_1():\n    \"\"\"Test that n_restarts_optimizer=1 performs the expected number of\n    restarts, and that the log-marginal-likelihoods are correctly ordered.\n    \"\"\"\n    gpr = GaussianProcessRegressor(random_state=0)\n    gpr.fit(X, y)\n\n    # Check that the kernel is not cloned\n    initial_kernel = gpr.kernel_\n    n_restarts_optimizer = 1\n    gpr = GaussianProcessRegressor(n_restarts_optimizer=n_restarts_optimizer, random_state=0)\n    gpr.fit(X, y)\n    assert gpr.kernel_ is not initial_kernel\n\n    # Check that the number of restarts is correct\n    with pytest.warns(None):  # Ignore warnings about non-finite values\n        gpr = GaussianProcessRegressor(n_restarts_optimizer=n_restarts_optimizer + 1, random_state=0)\n        gpr.fit(X, y)\n    assert gpr.n_restarts_optimizer + 1 == gpr._constrained_optimization.count\n\n"], "sample_736": ["def test_logistic_loss():\n    X = np.array([[0, 1], [0, 1], [0, 1]])\n    w = np.array([1, 0])\n    y = np.array([1, 1, 1])\n    alpha = 0.5\n    sample_weight = np.array([1, 1, 1])\n    expected_loss = _logistic_loss(w, X, y, alpha, sample_weight)\n    expected_loss_exact = -1.0986122886681098\n    assert_array_almost_equal(expected_loss, expected_loss_exact)\n\n    # Test with sparse X\n    X_sp = sp.csr_matrix(X)\n    expected_loss_sparse = _logistic_loss(w, X_sp, y, alpha, sample_weight)\n    assert_array_almost_equal(expected_loss_sparse, expected_loss_exact)\n", "def test_logistic_regression_path():\n    X = np.array([[0.5, 0.2], [0.1, 0.8]])\n    y = np.array([0, 1])\n    Cs = [0.1, 1.0, 10.0]\n    w0 = np.array([0.1, 0.2])\n\n    # Test binary classification case\n    coefs, Cs_, n_iter_ = logistic_regression_path(X, y, Cs=Cs, C=1.0, fit_intercept=True)\n    assert_equal(coefs.shape, (len(Cs_), 2))\n    assert_equal(Cs_, Cs)\n    assert_equal(n_iter_.shape, (len(Cs_),))\n\n    # Test with non-binary classification case\n    y = np.array([0, 0, 1, 1])\n    coefs, Cs_, n_iter_ = logistic_regression_path(X, y, Cs=Cs, C=1.0, fit_intercept=True)\n    assert_equal(coefs.shape, (len(Cs_), 2))\n    assert_equal(Cs_, Cs)\n    assert_equal(n_iter_.shape, (len(Cs_),))\n\n    # Test with C=1.0 and multiple Cs values\n    coefs, Cs_, n_iter_ = logistic_regression_path(X, y, Cs=Cs, C=1.0, fit_intercept=True)\n    assert_equal(coefs.shape, (len(Cs_), 2))\n    assert_equal(Cs_, Cs)\n    assert_equal(n_iter_.shape, (len(Cs_),))\n\n    # Test with w0 as input\n    coefs, Cs_, n_iter_ = logistic_regression_path(X, y, Cs=Cs, C=1.0, fit_intercept=True, coef=w0)\n    assert_equal(coefs.shape, (len(Cs_), 2))\n    assert_equal(Cs_, Cs)\n    assert_equal(n_iter_.shape, (len(Cs_),))\n\n    # Test with solver='lbfgs'\n    coefs, Cs_, n_iter_ = logistic_regression_path(X, y, Cs=Cs, C=1.0, fit_intercept=True, solver='lbfgs')\n    assert_equal(coefs.shape, (len(Cs_), 2))\n    assert_equal(Cs_, Cs)\n    assert_equal(n_iter_.shape, (len(Cs_),))\n\n    # Test with solver='newton-cg'\n    coefs, Cs_, n_iter_", "def test_logistic_regression_empty_input():\n    \"\"\"Test that fitting an empty array to a LogisticRegression object raises an error.\"\"\"\n    with assert_raise_message(ValueError, \"Input data must be a 2D array\"):\n        LogisticRegression().fit([], [])\n", "def test_logistic_regression_path_multinomial_no_init():\n    \"\"\"Test that logistic_regression_path returns the correct results with a\n    multiclass problem and no initialization of coefficients.\"\"\"\n    X = np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n    y = np.array([0, 1, 2, 2])\n    Cs = np.logspace(-4, 4, 10)\n    multi_class = 'multinomial'\n    solver = 'lbfgs'\n    coef_init = None\n    expected_shape = (len(Cs), 6)\n\n    coefs, Cs, n_iter = logistic_regression_path(X, y, Cs=Cs, fit_intercept=False,\n                                                 multi_class=multi_class,\n                                                 solver=solver,\n                                                 coef_init=coef_init,\n                                                 verbose=0)\n    assert_equal(coefs.shape, expected_shape)\n    assert_array_almost_equal(n_iter, np.array([100, 100, 100, 100, 100, 100,\n                                               100, 100, 100, 100]))\n", "def test_logistic_regression_path_and_multiclass():\n    \"\"\"Tests the logistic_regression_path function with a multi-class problem\"\"\"\n    X = [[-1, 0], [0, 1], [1, 1]]\n    y = [0, 1, 2]\n    # We want to test that the correct shape of the coefficients is returned\n    # when the multi_class option is set to 'multinomial'\n    coefficients = logistic_regression_path(X, y, multi_class='multinomial')\n    assert_equal(coefficients[0].shape, (3, 2))\n\n    # Test that the correct number of iterations is returned when the solver is 'liblinear'\n    coefficients = logistic_regression_path(X, y, solver='liblinear')\n    assert_equal(coefficients[2].shape, (1,))\n\n    # Test that the correct number of iterations is returned when the solver is 'newton-cg'\n    coefficients = logistic_regression_path(X, y, solver='newton-cg')\n    assert_equal(coefficients[2].shape, (1,))\n\n    # Test that the correct number of iterations is returned when the solver is 'sag'\n    coefficients = logistic_regression_path(X, y, solver='sag')\n    assert_equal(coefficients[2].shape, (1,))\n\n    # Test that the correct number of iterations is returned when the solver is 'saga'\n    coefficients = logistic_regression_path(X, y, solver='saga')\n    assert_equal(coefficients[2].shape, (1,))\n\n    # Test that the correct number of coefficients is returned when the multi_class option is set to 'multinomial'\n    coefficients = logistic_regression_path(X, y, multi_class='multinomial')\n    assert_equal(coefficients.shape, ((3, 2)))\n\n    # Test that the correct number of coefficients is returned when the multi_class option is set to 'ovr'\n    coefficients = logistic_regression_path(X, y, multi_class='ovr')\n    assert_equal(coefficients.shape, ((3, 2)))\n\n    # Test that the correct number of coefficients is returned when the solver is 'liblinear'\n    coefficients = logistic_regression_path(X, y, solver='liblinear')\n    assert_equal(coefficients.shape, ((3, 2)))\n\n    # Test that the correct number of coefficients is returned when the solver is 'newton-cg'\n    coefficients = logistic_regression_path(X, y, solver='newton-cg')\n"], "sample_181": ["    def test_join_with_outer_join(self):\n        # Create a query with an outer join\n        query = Book.objects.filter(authors__name='test3').values('authors__name')\n        expected_output = [\n            {'authors__name': 'test3'},\n            {'authors__name': 'test3'},\n        ]\n        self.assertEqual(list(query), expected_output)\n", "    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='test', age=40)\n        cls.a2 = Author.objects.create(name='test2', age=60)\n        cls.a3 = Author.objects.create(name='test3', age=100)\n        cls.p1 = Publisher.objects.create(name='Apress', num_awards=3, duration=datetime.timedelta(days=1))\n        cls.b1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), contact=cls.a1, publisher=cls.p1,\n            pubdate=datetime.date(2007, 12, 6),\n        )\n        cls.b2 = Book.objects.create(\n            isbn='067232959', name='Sams Teach Yourself Django in 24 Hours',\n            pages=528, rating=3.0, price=Decimal('23.09'), contact=cls.a2, publisher=cls.p1,\n            pubdate=datetime.date(2008, 3, 3),\n        )\n        cls.b3 = Book.objects.create(\n            isbn='159059996', name='Practical Django Projects',\n            pages=600, rating=4.5, price=Decimal('29.69'), contact=cls.a3, publisher=cls.p1,\n            pubdate=datetime.date(2008, 6, 23),\n        )\n        cls.a1.friends.add(cls.a2)\n        cls.a1.friends.add(cls.a3)\n        cls.b1.authors.add(cls.a1)\n        cls.b1.authors.add(cls.a3)\n        cls.b2.authors.add(cls.a2)\n        cls.b3.authors.add(cls.a3)\n", "    def test_select_related_with_max_depth(self):\n        # Test that select_related with a max_depth doesn't exceed it\n        qs = Author.objects.select_related('friends__friends__friends').filter(id=1)\n        self.assertEqual(qs._max_depth, 3)\n        self.assertEqual(len(qs._select_related), 3)\n        self.assertEqual(qs._select_related[0].name, 'friends')\n        self.assertEqual(qs._select_related[1].name, 'friends__friends')\n        self.assertEqual(qs._select_related[2].name, 'friends__friends__friends')\n", "    def test_raw_sql_query(self):\n        query = Book.objects.raw('SELECT * FROM books_book WHERE price < %s', [30])\n        self.assertEqual(list(query), [Book.objects.get(id=1)])\n", "    def test_no_overriding_limit(self):\n        # When setting a limit and then calling count(), the limit should be\n        # preserved.\n        Author.objects.all().count()  # Setting the default limit\n        self.assertEqual(Author.objects.all().count(), 3)\n        self.assertEqual(Author.objects.all().count(low=1), 1)\n        self.assertEqual(Author.objects.all().count(), 3)\n"], "sample_997": ["def test_parse_expr_with_function_exponentiation():\n    result = parse_expr(\"sin**2(x)\", evaluate=False)\n    assert isinstance(result, Pow)\n    assert result.base == sin(x)\n    assert result.exp == 2\n    assert result == sin(x)**2\n", "def test_repeated_decimals():\n    x = Symbol('x')\n    assert parse_expr(\"0.2[1]\", evaluate=False) == Rational(19, 90)\n    assert parse_expr(\"0.4[2]\", evaluate=False) == Rational(1, 2) + Rational(5, 9)\n    assert parse_expr(\"0.25[3]\", evaluate=False) == Rational(3, 16)\n    assert parse_expr(\"0.2[4]\", evaluate=False) == Rational(19, 90) + Rational(2, 45)\n    assert parse_expr(\"0.2[4]\", evaluate=False) == Rational(19, 90) + Rational(2, 45)\n    assert parse_expr(\"0.2[11111]\", evaluate=False) == Rational(19, 90) + Rational(1, 45045)\n    assert parse_expr(\"0.2[1111111111]\", evaluate=False) == Rational(19, 90) + Rational(1, 40911311345)\n", "def test_factorial_notation_negative_factorial():\n    # Test the factorial transformation with a negative factorial\n    # and verify that it throws a TokenError\n    code = \"factorial(-3)\"\n    with raises(TokenError):\n        parse_expr(code, transformations=(factorial_notation,))\n", "def test_lambda_notation_parsing():\n    x = Symbol('x')\n    from sympy import Lambda\n    expr = parse_expr('lambda x: x**2 + 2*x + 1')\n    assert expr == Lambda(x, x**2 + 2*x + 1)\n", "def test_rationalize_function():\n    # Test that rationalize function works as expected\n    result = parse_expr(\"3/4\", transformations=(rationalize,))\n    assert result == Rational(3, 4)\n\n    # Test that rationalize function does not remove float when it is not a literal\n    result = parse_expr(\"1.0*x\", transformations=(rationalize,))\n    assert result == Float(1)*x\n\n    # Test that rationalize function does not remove float when it is part of a larger expression\n    result = parse_expr(\"x + 1.0\", transformations=(rationalize,))\n    assert result == x + Float(1)\n\n    # Test that rationalize function does not raise an error\n    result = parse_expr(\"1.0/2.0 + 2.0/3.0\", transformations=(rationalize,))\n    assert result == Float(1)/2 + Float(2)/3\n\n    # Test that rationalize function does not remove float when it is not a literal\n    # but is in a float literal\n    result = parse_expr(\"1.0 + 2.0/3.0\", transformations=(rationalize,))\n    assert result == Float(1) + Float(2)/3\n"], "sample_417": ["    def test_floatformat_args(self):\n        # Test floatformat with different argument types\n        self.assertEqual(self.engine.render_template('floatformat01', {'a': 34.23234, 'b': 34}), '34.2 34')\n        self.assertEqual(self.engine.render_template('floatformat02', {'a': 34.23234, 'b': 34}), '34.20 34.20')\n        self.assertEqual(self.engine.render_template('floatformat03', {'a': 34.23234, 'b':", "    def test_floatformat_precision(self):\n        self.render_template(\"floatformat01\", {\"a\": 34.23234, \"b\": 34.00000})\n        self.assertEqual(self.output, \"34.2 34\")\n", "    def test_floatformat_with_negative_precision(self):\n        with self.settings(USE_L10N=False):\n            translation.activate('en')\n            with localcontext() as ctx:\n                ctx.prec = 2\n                self.assertEqual(floatformat('34.23234', '-1'), '34.2')\n                self.assertEqual(floatformat('34.00000', '-1'), '34')\n                self.assertEqual(floatformat('34.26000', '-1'), '34.3')\n                self.assertEqual(floatformat('34.23234', '-2'), '34')\n                self.assertEqual(floatformat('34.00000', '-2'), '')\n                self.assertEqual(floatformat('34.26000', '-2'), '34')\n", "    def test_floatformat_with_n(self):\n        \"\"\"test floatformat with 'n' suffix.\"\"\"\n        with translation.override('en'):\n            self.assertEqual(self.engine.render_template(\n                \"{% autoescape off %}{{ a|floatformat:n }}{% endautoescape %}\",\n                {'a': Decimal('123456.78901234567890123456789')}\n            ), \"123,456.789\")\n", "    def test_floatformat_zero(self):\n        # Test that floatformat displays 0 correctly\n        # without decimal places if it's an integer\n        with translation.override('en'):\n            self.assertEqual(floatformat(Decimal('0')), '0')\n            self.assertEqual(floatformat(0), '0')\n"], "sample_838": ["def test_column_transformer_remainder_all_none():\n    X = np.array([[1., 2., 3.], [4., 5., 6.]])\n    ct = ColumnTransformer(remainder='passthrough')\n    result = ct.fit_transform(X)\n    assert_array_equal(result, X)\n", "def test_fit_transform_passthrough():\n    \"\"\"Test fit_transform with passthrough.\"\"\"\n    # test passthrough\n    ct = ColumnTransformer([('passthrough', 'passthrough', [0, 1]),\n                           ('passthrough', 'passthrough', slice(2, 3))],\n                          remainder='passthrough')\n    X = np.array([[1., 2., 3.], [4., 5., 6.]])\n    X_t = ct.fit_transform(X)\n    assert_array_equal(X_t, X)\n\n    ct = ColumnTransformer([('passthrough', 'passthrough', [0, 1]),\n                           ('passthrough', 'passthrough', slice(2, 3))],\n                          remainder='passthrough')\n    ct.fit(X)\n    X_t = ct.transform(X)\n    assert_array_equal(X_t, X)\n", "def test_column_transformer_remainder_passthrough():\n    \"\"\"Test that using passthrough in ColumnTransformer with a default transformer\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    trans = make_column_transformer(('passthrough', [0, 1]), remainder='passthrough')\n    trans.fit_transform(X)\n    assert_array_equal(trans.transform(X), X)\n", "def test_transformer_weights():\n    # Test that we can specify weights for transformers\n    transformer = make_column_transformer(\n        (StandardScaler(), ['numerical_column']),\n        (OneHotEncoder(), ['categorical_column']),\n        (DoubleTrans(), ['double_column']),\n    )\n    transformer.transformer_weights = {'standardscaler': 0.5, 'onehotencoder': 0.5, 'doubletrans': 1.0}\n    transformer.fit_transform([[1, 2, 'cat1', 4], [5, 6, 'cat2', 7]])\n    X = transformer.transform([[1, 2, 'cat1', 4], [5, 6, 'cat2', 7]])\n    assert_almost_equal(X[0, :4], 0.5 * np.array([1, 2, 4, 5]))\n    assert_almost_equal(X[0, 4:], 0.5 * np.array([1, 0, 0, 0]))\n    assert_almost_equal(X[1, :4], 0.5 * np.array([5, 6, 7, 8]))\n    assert_almost_equal(X[1, 4:], 0.5 * np.array([0, 1, 0, 0]))\n    assert_almost_equal(X[0, 8], 2 * 4)\n    assert_almost_equal(X[1, 8], 2 * 7)\n", "def test_sparse_threshold():\n    # Check that sparse_threshold correctly determines the output format\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    X_sparse = sparse.csr_matrix([[1, 2, 0], [4, 5, 0]])\n\n    # Test that a mix of sparse and dense data is stacked as sparse\n    transformer = make_column_transformer(\n        (StandardScaler(), ['numeric']),\n        (OneHotEncoder(), ['categorical']),\n        sparse_threshold=0.5)\n    ct = transformer.fit_transform(X)\n    assert sparse.issparse(ct)\n\n    # Test that all sparse data is stacked as sparse\n    transformer = make_column_transformer(\n        (SparseMatrixTrans(), ['numeric']),\n        (OneHotEncoder(), ['categorical']),\n        sparse_threshold=0.0)\n    ct = transformer.fit_transform(X_sparse)\n    assert sparse.issparse(ct)\n\n    # Test that all dense data is stacked as dense\n    transformer = make_column_transformer(\n        (DoubleTrans(), ['numeric']),\n        (OneHotEncoder(), ['categorical']),\n        sparse_threshold=0.0)\n    ct = transformer.fit_transform(X)\n    assert not sparse.issparse(ct)\n\n    # Test that a mix of sparse and dense data is stacked as dense\n    # when sparse_threshold is 0\n    transformer = make_column_transformer(\n        (DoubleTrans(), ['numeric']),\n        (SparseMatrixTrans(), ['numeric']),\n        sparse_threshold=0.0)\n    ct = transformer.fit_transform(X)\n    assert not sparse.issparse(ct)\n\n    # Test that an error is raised if the output of the transformers contains\n    # both sparse and dense data and sparse_threshold is not specified\n    transformer = make_column_transformer(\n        (StandardScaler(), ['numeric']),\n        (OneHotEncoder(), ['categorical']))\n    with assert_raise_message(ValueError,\n                           \"For a sparse output, all columns should\"\n                           \" be a numeric or convertible to a numeric.\",\n                           transformer.fit_transform, X):\n        transformer.fit_transform(X)\n\n    # Test that an error is raised if the output of the transformers contains\n    # both sparse and dense data and sparse_threshold is specified but\n    # not in the range [0, 1]\n    transformer = make_column_transformer(\n        (StandardScaler(), ['numeric']),\n        (OneHotEncoder(), ['categorical']),\n        sparse_threshold"], "sample_428": ["    def test_format_grouping_sequence(self):\n        self.assertEqual(\n            format(1234567890, '.', 2, grouping=(3, 2, 0), thousand_sep=' ', use_l10n=False),\n            mark_safe(' 1 234 567.90')\n        )\n", "    def test_decimal_sep_and_thousand_sep(self):\n        self.assertEqual(format(12345.6789, \".\", thousand_sep=\",\"),\n                       \"12,345.6789\")\n        self.assertEqual(format(12345.6789, \",\", thousand_sep=\".\"),\n                       \"12.345,6789\")\n", "    def test_format_large_number(self):\n        # Test very large number with scientific notation\n        self.assertEqual(\n            format(Decimal(\"123456789012345678901234567890\")),\n            \"12345678901234567890123456789e+16\"\n        )\n\n        # Test very large number without scientific notation\n        self.assertEqual(\n            format(\n                Decimal(\"123456789012345678901234567890\"),\n                decimal_sep=\",\",\n                grouping=3,\n                thousand_sep=\".\",\n            ),\n            \"123.456.789.012.345.678.901.234.567.890\"\n        )\n", "    def test_format_with_negative_number_and_decimal_pos(self):\n        self.assertEqual(format(-123.456, '.', 2), \"-123.46\")\n", "def test_grouping_sequence(self):\n    \"\"\"\n    Test case to verify if the format function handles grouping sequence properly.\n    \"\"\"\n    # Test case 1: Grouping sequence provided\n    result = format(123456789, '.', 2, grouping=(3, 2), thousand_sep=',')\n    self.assertEqual(result, \"123.456,79\")\n\n    # Test case 2: Grouping sequence provided with non-zero last value\n    result = format(123456789, '.', 2, grouping=(3, 2, 1), thousand_sep=',')\n    self.assertEqual(result, \"123.456,78,9\")\n\n    # Test case 3: Grouping sequence provided with non-uniform digit grouping\n    result = format(123456789, '.', 2, grouping=(3, 2, 0, 1), thousand_sep=',')\n    self.assertEqual(result, \"123.456,78,9\")\n\n    # Test case 4: Grouping sequence provided with only one value (i.e., uniform digit grouping)\n    result = format(123456789, '.', 2, grouping=3, thousand_sep=',')\n    self.assertEqual(result, \"123.456,789\")\n\n    # Test case 5: Test case when grouping sequence provided with negative number\n    result = format(-123456789, '.', 2, grouping=(3, 2), thousand_sep=',')\n    self.assertEqual(result, \"-123.456,79\")\n\n    # Test case 6: Test case when grouping sequence provided with decimal position\n    result = format(123456789.123, '.', 4, grouping=(3, 2), thousand_sep=',')\n    self.assertEqual(result, \"123.456,79.123\")\n\n    # Test case 7: Test case when grouping sequence provided with decimal position and negative number\n    result = format(-123456789.123, '.', 4, grouping=(3, 2), thousand_sep=',')\n    self.assertEqual(result, \"-123.456,79.123\")\n\n    # Test case 8: Test case when grouping sequence provided with force_grouping=True\n    result = format(123456789, '.', 2, grouping=(3, 2), thousand_sep=',', force_grouping=True)\n    self.assertEqual(result, \"123,456,789\")\n\n    # Test case 9: Test case when grouping sequence provided with force_grouping=True and negative number\n    result = format(-123456789"], "sample_920": ["    def test_numpydoc_see_also_section(self):\n        docstring = \"\"\"", "    def test_parse_google_docstring_with_multiple_sections(self):\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        docstring = '''One line summary.", "    def test_numpydoc_attributes_section(self):\n        config = Config(napoleon_use_ivar=True)\n        docstring = dedent('''\n            class MyClass:\n                \"\"\"Attributes\n                ----------\n                attr1 : int\n                    Description of attr1\n                attr2 : float\n                    Description of attr2\n                attr3 : str\n\n                    Description of attr3\n                \"\"\"\n        ''')\n        docstring = NumpyDocstring(docstring, config).lines()\n        self.assertEqual(cleandoc('''\n            :ivar attr1: Description of attr1\n            :vartype attr1: int\n            :ivar attr2: Description of attr2\n            :vartype attr2: float\n            :ivar attr3: Description of attr3\n            :vartype attr3: str\n            '''), '\\n'.join(docstring))\n", "    def test_consume_field_numpy_style(self):\n        docstring = dedent(\n            \"\"\"\\\n            Parameters\n            ----------\n            param1 : int\n                Description of `param1`\n            param2 : str\n                Description of `param2`\n            \"\"\"\n        )\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        numpy_docstring = NumpyDocstring(docstring, config=config)\n        self.assertEqual(\n            numpy_docstring.lines(),\n            [\n                'param1',\n                ':type param1: int',\n                'Description of `param1`',\n                '',\n                'param2',\n                ':type param2: str',\n                'Description of `param2`',\n                ''\n            ]\n        )\n", "    def test_numpydoc_see_also_section(self):\n        config = Config(napoleon_use_admonition_for_examples=True)\n        docstring = dedent('''\n        See Also\n        --------\n        function1 : Descriptive text\n            continued text\n        function2 : Descriptive text\n        function3 :meth:`function3`\n\n\n        '''\n        )\n        npds = NumpyDocstring(docstring, config)\n        expected = [\n            '.. admonition:: seealso',\n            '   :function:`function1`: Descriptive text',\n            '   continued text',\n            '   :function:`function2`: Descriptive text',\n            '   :function:`function3`: function3'\n        ]\n        self.assertEqual(npds.lines(), expected)\n"], "sample_61": ["    def test_ascii_username_validation(self):\n        validator = ASCIIUsernameValidator()\n        self.assertTrue(validator.is_valid('john.doe'))\n        self.assertFalse(validator.is_valid('john.doe@.com'))\n        self.assertFalse(validator.is_valid('john.doe!'))\n", "    def test_ASCIIUsernameValidator_valid_username(self):\n        validator = ASCIIUsernameValidator()\n        self.assertTrue(validator('testuser123'))\n", "    def test_ascii_username_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertTrue(validator('username123'))\n        self.assertFalse(validator('username!@#$'))\n        with self.assertRaises(ValidationError):\n            validator('username!@#$')\n", "    def test_username_ascii_validator(self):\n        validator = ASCIIUsernameValidator()\n        self.assertTrue(validator('test123'))\n        self.assertFalse(validator('!@#$%^&*()'))\n        self.assertFalse(validator('non-ascii-character'))\n        with self.assertRaises(ValidationError):\n            validator('non_ascii_character')\n", "    def setUp(self):\n        self.user = User.objects.create_user(username='testuser', email='test@example.com', password='password123')\n"], "sample_291": ["    def test_get_template_names(self):\n        view = TemplateView()\n        self.assertEqual(view.get_template_names(), [])\n\n        view.template_name = 'template.html'\n        self.assertEqual(view.get_template_names(), ['template.html'])\n", "    def _assert_template(self, response):\n        self.assertEqual(response.status_code, 200)\n", "    def test_render_to_response(self):\n        view = AboutTemplateView()\n        response = view.render_to_response({})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 200)\n", "    def test_http_method_not_allowed(self):\n        view = SimpleView()\n        response = view.http_method_not_allowed(self.rf.get('/simple-view', 'path'))\n        self.assertEqual(response.status_code, 405)\n        self.assertEqual(response.content, b'Method Not Allowed (GET): /simple-view')\n", "    def test_redirect_view(self):\n        # Test that a URL named redirect_view returns a 301 redirect\n        url = reverse('redirect_view')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 301)\n\n        # Test that a URL named redirect_view with query string returns a 301 redirect\n        url = reverse('redirect_view_with_query')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 301)\n\n        # Test that a URL named redirect_view without a permanent redirect returns a 302 redirect\n        url = reverse('redirect_view_without_permanent')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 302)\n\n        # Test that a URL named redirect_view with query string and non-permanent redirect returns a 302 redirect\n        url = reverse('redirect_view_with_query_and_non_permanent')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 302)\n"], "sample_6": ["def test_longitude_wrap_angle_change():\n    lon = Angle(180, unit=u.deg)\n    assert lon.wrap_angle == u.deg * 360\n    lon.wrap_angle = u.deg * 180\n    assert lon.wrap_angle == u.deg * 180\n    assert np.allclose(lon.value, np.array([-180, 180]))\n    lon.wrap_at(u.deg * 180, inplace=True)\n    assert np.allclose(lon.value, np.array([-180, 0]))\n", "def test_latitude_deprecated_property():\n    with ExitStack() as stack:\n        stack.enter_context(warnings.catch_warnings())\n        warnings.simplefilter('ignore', category=AstropyDeprecationWarning)\n        lat = Angle(45 * u.deg)\n        with pytest.raises(UserWarning):\n            lat.latitude\n        with pytest.raises(UserWarning):\n            lat._latitude\n", "    def test_wrap_at(self):\n        with ExitStack():\n            np.random.seed(0)\n            angles = Angle(np.random.uniform(-180, 360, size=10) * u.deg)\n            wrap_angle = 180 * u.deg\n            angles.wrap_at(wrap_angle)\n            assert all(angles.value >= wrap_angle - 180 * u.deg) and \\\n                   all(angles.value < wrap_angle)\n", "def test_Latitude_value_error():\n    from astropy.coordinates import Latitude\n    with pytest.raises(ValueError):\n        Latitude(90.1 * u.deg)\n\n    with pytest.raises(ValueError):\n        Latitude(-91 * u.deg)\n\n    with pytest.raises(ValueError):\n        Latitude(90 * u.deg, unit=u.hourangle)\n\n    with pytest.raises(ValueError):\n        Latitude(np.array([90.1, -91]) * u.deg)\n", "def test_angle_to_string_decimal():\n    from astropy.coordinates import Angle\n\n    angle = Angle(12.5 * u.deg)\n    expected = \"12.5\u00b0\"\n    assert angle.to_string(decimal=True) == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False)\n    expected = \"12\u00b015'\"\n    assert angle.to_string(decimal=False) == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, sep='hms')\n    expected = \"12h15m0s\"\n    assert angle.to_string(decimal=False, sep='hms') == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, precision=2)\n    expected = \"12.50\u00b0\"\n    assert angle.to_string(decimal=False, precision=2) == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, pad=True)\n    expected = \"12.50\u00b0\"\n    assert angle.to_string(decimal=False, pad=True) == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, format='latex')\n    expected = r\"12\\circ 15'\n    assert angle.to_string(decimal=False, format='latex') == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, format='latex_inline')\n    expected = r\"12\\circ 15'\n    assert angle.to_string(decimal=False, format='latex_inline') == expected\n\n    angle = Angle(12.5 * u.deg, decimal=False, format='unicode')\n    expected = \"12\u00b015\u2032\"\n    assert angle.to_string(decimal=False, format='unicode') == expected\n"], "sample_33": ["def test_strip_accents():\n    assert misc.strip_accents('\u00e5ngstr\u00f6m') == 'angstrom'\n    assert misc.strip_accents('Stra\u00dfe') == 'Stra\u00dfe'  # should not change non-accented characters\n    assert misc.strip_accents('foo') == 'foo'\n    assert misc.strip_accents('') == ''\n", "def test_check_broadcast():\n    # Test broadcasting with a mix of 1D and 2D arrays\n    shape1 = (3, 4)\n    shape2 = (3, 4, 5)\n    shape3 = (1, 2)\n    shape4 = (5,)\n    broadcasted_shape = misc.check_broadcast(shape1, shape2, shape3, shape4)\n    assert broadcasted_shape == (3, 4, 1, 5)\n\n    # Test broadcasting with an empty shape\n    broadcasted_shape = misc.check_broadcast(shape1, ())\n    assert broadcasted_shape == (3, 4)\n\n    # Test broadcasting with a shape that cannot be broadcasted\n    with pytest.raises(misc.IncompatibleShapeError):\n        misc.check_broadcast(shape1, (2, 3, 4))\n\n    # Test broadcasting with a mix of 1D arrays with different sizes\n    shape5 = (2,)\n    shape6 = (1,)\n    with pytest.raises(misc.IncompatibleShapeError):\n        misc.check_broadcast(shape5, shape6)\n", "def test_indent():\n    assert indent('foo\\nbar') == '    foo\\n    bar'\n    assert indent('foo\\nbar', shift=2) == '  foo\\n  bar'\n    assert indent('foo\\nbar', shift=1, width=8) == '     foo\\n     bar'\n", "def test_check_broadcast():\n    shapes = [\n        (1, 2, 3),\n        (4, 2, 3),\n        (1, 2),\n        (4, 2),\n        (1, 2, 3, 4),\n        (1, 2, 3, 5),\n        (1, 2, 3, 4, 6),\n        (),\n        (1,),\n        (2, 3),\n        (4, 5),\n    ]\n\n    for i, shape in enumerate(shapes):\n        for j, other_shape in enumerate(shapes):\n            if i <= j:\n                continue\n            try:\n                check_broadcast(shape, other_shape)\n                pytest.raises(IncompatibleShapeError)\n            except IncompatibleShapeError as e:\n                assert e.shape_a == shape\n                assert e.shape_a_idx == i\n                assert e.shape_b == other_shape\n                assert e.shape_b_idx == j\n", "def test_strip_accents():\n    assert misc.strip_accents('\u00e5ngstr\u00f6m') == 'angstrom'\n    assert misc.strip_accents('caf\u00e9') == 'cafe'\n    assert misc.strip_accents('caf\u00e9\u00e9') == 'cafee'\n    assert misc.strip_accents('12345') == '12345'\n    assert misc.strip_accents('') == ''\n    assert misc.strip_accents('') == ''\n"], "sample_674": ["def test_ischildnode(baseid, nodeid, expected):\n    node = nodes.Node(name=\"test_node\")\n    assert ischildnode(baseid, nodeid) == expected\n", "def test_ischildnode():\n    baseid = \"foo/bar::TestBaz\"\n    nodeid = \"foo/bar\"\n    assert nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"foo/bar\"\n    nodeid = \"foo/bar::TestBop\"\n    assert nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"foo/bar::TestBaz\"\n    nodeid = \"foo/bar::TestBop\"\n    assert not nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"foo/bar::TestBaz\"\n    nodeid = \"foo/bla\"\n    assert not nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"\"\n    nodeid = \"\"\n    assert not nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"\"\n    nodeid = \"foo\"\n    assert nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"\"\n    nodeid = \"foo/bar\"\n    assert nodes.ischildnode(baseid, nodeid)\n\n    baseid = \"\"\n    nodeid = \"foo/bar::TestBop\"\n    assert nodes.ischildnode(baseid, nodeid)\n", "def test_ischildnode(baseid, nodeid, expected):\n    collector = nodes.Collector()\n    node = nodes.Node(name=\"TestBaz\", parent=collector, nodeid=nodeid)\n    assert ischildnode(baseid, node.nodeid) == expected\n", "def test_ischildnode():\n    baseid = nodes.Node(\"foo/bar\")\n    assert ischildnode(baseid.nodeid, \"foo/bar\") == True\n    assert ischildnode(baseid.nodeid, \"foo/bar::TestBaz\") == True\n    assert ischildnode(baseid.nodeid, \"foo\") == False\n    assert ischildnode(baseid.nodeid, \"foo/bar::TestBop\") == False\n    assert ischildnode(baseid.nodeid, \"bar\") == False\n", "def test_ischildnode_memoization():\n    # Ensure that ischildnode is properly memoized\n    baseid = \"foo/bar::TestBaz\"\n    nodeid1 = \"foo/bar::TestBaz\"\n    nodeid2 = \"foo/bar::TestBop\"\n\n    assert nodes.ischildnode(baseid, nodeid1)\n    assert nodes.ischildnode(baseid, nodeid2)\n\n    # Check that calling ischildnode with the same arguments twice doesn't repeat the calculation\n    assert nodes.ischildnode(baseid, nodeid1) == nodes.ischildnode(baseid, nodeid1)\n    assert nodes.ischildnode(baseid, nodeid2) == nodes.ischildnode(baseid, nodeid2)\n"], "sample_961": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = Mock()\n    env.ref_context.get.return_value = 'module'\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation, env)\n    expected = ['List',\n                addnodes.desc_sig_punctuation('', '['),\n                nodes.Text(''),\n                addnodes.desc_sig_punctuation('', '['),\n                addnodes.desc_sig_name('', 'int'),\n                addnodes.desc_sig_punctuation('', ']'),\n                addnodes.desc_sig_punctuation('', ')')]\n    assert_node(result, expected)\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = True\n    env.ref_context['py:module'] = 'module'\n    env.ref_context['py:class'] = 'class'\n\n    # Test simple type annotation\n    annotation = 'int'\n    expected = ['Text', 'int']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test nested type annotation\n    annotation = 'List[int]'\n    expected = ['Text', 'List[', 'Text', 'int', ']']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test union type annotation\n    annotation = 'int | str'\n    expected = ['Text', 'int', 'Text', '|', 'Text', 'str']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test type alias annotation\n    annotation = 'Union[int, str]'\n    expected = ['Text', 'Union[', 'Text', 'int', ', ', 'Text', 'str', ']']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test type with parameters annotation\n    annotation = 'List[int, str]'\n    expected = ['Text', 'List[', 'Text', 'int', ',', 'Text', 'str', ']']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test invalid type annotation\n    annotation = 'invalid_type'\n    expected = [type_to_xref(annotation, env)]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test type with unqualified names\n    annotation = 'List[int]'\n    env.config.python_use_unqualified_type_names = False\n    expected = ['Text', 'List', 'Text', '[', 'Text', 'int', ']']\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n", "def test_add_target_and_index_with_noindexentry():\n    env = Mock()\n    env.domain = PythonDomain()\n    env.domain.note_object('module', 'module', 'node-id')\n    env.domain.data['modules']['module'] = ModuleEntry('docname', 'node-id', 'synopsis', 'platform', False)\n    env.domain.data['objects']['module'] = ObjectEntry('docname', 'node-id', 'module', False)\n    signode = desc_signature('module.function', '')\n    obj = PyFunction()\n    obj.options = {}\n    obj.handle_signature('module.function()')\n    obj.add_target_and_index(('module', 'function'), 'module.function()', signode)\n    assert env.domain.indexnode['entries'][0][0] == 'single'\n    assert env.domain.indexnode['entries'][0][1] == 'module'\n    assert env.domain.indexnode['entries'][0][2] == 'node-id'\n", "def test_object_nesting():\n    \"\"\"Test object nesting\"\"\"\n    dom = PythonDomain()\n    env = Mock()\n    env.ref_context['py:class'] = 'module'\n    obj = PyObject('module', None, 'module', None, None, None)\n    obj.allow_nesting = True\n    obj.before_content()\n    assert env.ref_context['py:classes'] == ['module']\n    assert env.ref_context['py:module'] == 'module'\n    obj.after_content()\n    assert env.ref_context['py:classes'] == []\n    assert env.ref_context['py:module'] == 'module'\n", "def test_py_sig_re():\n    assert py_sig_re.match('module').group(0) == 'module'\n    assert py_sig_re.match('module.submodule').group(0) == 'module.submodule'\n    assert py_sig_re.match('module.submodule.subsubmodule').group(0) == 'module.submodule.subsubmodule'\n    assert py_sig_re.match('function').group(0) == 'function'\n    assert py_sig_re.match('function(param1, param2)').group(0) == 'function(param1, param2)'\n    assert py_sig_re.match('function(param1, param2) -> return_type').group(0) == 'function(param1, param2) -> return_type'\n    assert py_sig_re.match('class.ClassName').group(0) == 'class.ClassName'\n    assert py_sig_re.match('class.ClassName(param1, param2)').group(0) == 'class.ClassName(param1, param2)'\n    assert py_sig_re.match('class.ClassName(param1, param2) -> return_type').group(0) == 'class.ClassName(param1, param2) -> return_type'\n\n    with pytest.raises(ValueError):\n        py_sig_re.match('invalid_signature')\n\n    with pytest.raises(ValueError):\n        py_sig_re.match('function(arg_list_with_brackets()[,'))\n"], "sample_476": ["    def test_deconstruct_without_upload_to(self):\n        model = PersonWithHeightAndWidth()\n        field = model._meta.get_field(\"mugshot\")\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(name, \"mugshot\")\n        self.assertEqual(path, \"django.core.fields.files.ImageField\")\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            \"verbose_name\": \"Image\",\n            \"upload_to\": \"\",\n            \"width_field\": \"mugshot_width\",\n            \"height_field\": \"mugshot_height\",\n        })\n", "    def setUp(self):\n        self.field = ImageField(upload_to='image_field_deconstruct_test')\n", "    def test_update_dimension_fields_with_image(self):\n        instance = self.PersonModel(mugshot=self.file1, height=10)\n        instance.save()\n        instance.mugshot = self.file2\n        instance.save()\n        instance.update_dimension_fields()\n        self.check_dimensions(instance, 8, 4, \"mugshot\")\n        self.check_dimensions(instance, 8, 4, \"mugshot_width\")\n        self.check_dimensions(instance, 8, 4, \"mugshot_height\")\n", "    def test_fieldfile_path_and_url_return_correct_values(self):\n        instance = self.PersonModel()\n        field = self.PersonModel._meta.get_field('file')\n        fieldfile = FieldFile(instance, field, 'file')\n        self.assertEqual(fieldfile.path, os.path.join(temp_storage_dir, 'file'))\n        self.assertEqual(fieldfile.url, '/file')\n", "    def test_deconstruct(self):\n        instance = PersonWithHeightAndWidth(mugshot=self.file1)\n        instance.save()\n        instance_id = instance.pk\n        instance.delete()\n        instance = PersonWithHeightAndWidth.objects.get(pk=instance_id)\n\n        field = instance._meta.get_field('mugshot')\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(name, 'mugshot')\n        self.assertEqual(path, 'django.core.files.storage.get_valid_name')\n        self.assertEqual(args, [])\n        self.assertEqual(kwargs, {'upload_to': 'images/%Y%m%d%H%M%S', 'storage': 'default'})\n"], "sample_1122": ["def test_unpolarify_with_exponents_only():\n    from sympy import unpolarify, exp_polar, sin, I\n    eq = unpolarify(sin(exp_polar(7 + 2*I)), exponents_only=True)\n    expected = sin(2**2 + 2*I)\n    assert eq == expected\n", "def test_unpolarify_arg():\n    x = Symbol('x', polar=True)\n    eq = exp_polar(2*I*x)*x\n    unpolarified = unpolarify(eq)\n    assert unpolarified == exp(2*I*x)*x, f\"Expected unpolarify(exp_polar(2*I*x)*x) to return exp(2*I*x)*x, but got {unpolarified}\"\n", "def test_exp_polar():\n    from sympy import exp, exp_polar, I, unbranched_argument, periodic_argument\n    from sympy.abc import x\n    assert exp_polar(exp_polar(5*I*pi)).is_real\n    assert periodic_argument(exp_polar(5*I*pi), 2*pi).is_real\n    assert unbranched_argument(exp_polar(5*I*pi)).is_real\n    assert periodic_argument(exp_polar(5*I*pi), 2*pi) == exp_polar(5*I*pi).arg\n    assert periodic_argument(exp_polar(5*I*pi), pi) == exp_polar(5*I*pi).arg\n    assert periodic_argument(exp_polar(5*I*pi), oo) == exp_polar(5*I*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi), 2*pi) == exp_polar(-5*I*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi), pi) == exp_polar(-5*I*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi), oo) == exp_polar(-5*I*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi + 2*pi), 2*pi) == exp_polar(-5*I*pi + 2*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi + 2*pi), pi) == exp_polar(-5*I*pi + 2*pi).arg\n    assert periodic_argument(exp_polar(-5*I*pi + 2*pi), oo) == exp_polar(-5*I*pi + 2*pi).arg\n", "def test_unpolarify_polar_lift():\n    from sympy import unpolarify, polar_lift, exp_polar, I, symbols\n    x = symbols('x', polar=False)\n    x2 = symbols('x', polar=True)\n\n    # Test that unpolarify(polar_lift(x)) returns x\n    assert unpolarify(polar_lift(x)) == x\n    # Test that unpolarify(polar_lift(x2)) returns x2\n    assert unpolarify(polar_lift(x2)) == x2\n\n    # Test that unpolarify(x*polar_lift(y)) returns x*polar_lift(y)\n    x, y = symbols('x y')\n    assert unpolarify(x*polar_lift(y)) == x*polar_lift(y)\n    # Test that unpolarify(polar_lift(x)*y) returns x*y\n    assert unpolarify(polar_lift(x)*y) == x*y\n\n    # Test that unpolarify(exp_polar(arg)) returns exp(arg)\n    from sympy import exp, arg, I\n    assert unpolarify(exp_polar(arg)) == exp(arg)\n\n    # Test that unpolarify(exp_polar(arg)*polar_lift(x)) returns exp(arg)*x\n    assert unpolarify(exp_polar(arg)*polar_lift(x)) == exp(arg)*x\n    # Test that unpolarify(polar_lift(x)*exp_polar(arg)) returns x*exp(arg)\n    assert unpolarify(polar_lift(x)*exp_polar(arg)) == x*exp(arg)\n", "def test_abs_unbranched_argument():\n    # Test cases for Abs() function and unbranched_argument()\n    from sympy.abc import x\n    from sympy import exp, exp_polar, I, sqrt, oo, S, periodic_argument, polar_lift, principal_branch\n\n    # Test Abs() with unbranched_argument()\n    assert N_equals(Abs(exp(2*I*pi)), 1)\n    assert N_equals(Abs(exp_polar(2*I*pi)), 1)\n    assert N_equals(Abs(polar_lift(1)), 1)\n    assert N_equals(Abs(1), 1)\n\n    # Test unbranched_argument() function\n    assert periodic_argument(exp_polar(2*I*pi), oo) == 0\n    assert periodic_argument(exp_polar(2*I*pi*3), oo) == 0\n    assert periodic_argument(exp(2*I*pi), oo) == 0\n    assert periodic_argument(exp(2*I*pi*3), oo) == 0\n\n    # Test principal_branch() function\n    assert N_equals(principal_branch(exp_polar(2*I*pi), 2*pi), 1)\n    assert N_equals(principal_branch(exp_polar(2*I*pi*3), 2*pi), 3)\n    assert N_equals(principal_branch(exp(2*I*pi), 2*pi), 1)\n    assert N_equals(principal_branch(exp(2*I*pi*3), 2*pi), 3)\n\n    # Test Abs() with complex number\n    assert N_equals(Abs(3 + 4*I), 5)\n    assert N_equals(Abs(-3 - 4*I), 5)\n    assert N_equals(Abs(-3 + 4*I), 5)\n    assert N_equals(Abs(3 - 4*I), 5)\n\n    # Test sign() function with Abs()\n    assert sign(Abs(3 + 4*I)) == 1\n    assert sign(Abs(-3 - 4*I)) == 1\n    assert sign(Abs(-3 + 4*I)) == 1\n    assert sign(Abs(3 - 4*I)) == 1\n\n    # Test sign() function with complex number\n    assert N_equals(sign(3 + 4*I), 0)\n    assert N_equals(sign(-3 - 4*I), 0)\n    assert N_equals(sign(-3 + 4*I),"], "sample_286": ["    def test_save_with_deferred_fields(self):\n        obj = Article(title='test', body='test')\n        obj.save()\n        self.assertEqual(obj.title, 'test')\n        self.assertIsNone(obj.body)\n", "    def test_model_state_adding_attribute(self):\n        article = Article.objects.create(title='Test', content='Test content')\n        self.assertTrue(article._state.adding)\n\n        article.refresh_from_db()\n        self.assertFalse(article._state.adding)\n", "    def test_state_instance_attributes(self):\n        article = Article(title='Test Article')\n        self.assertIsInstance(article._state, ModelState)\n        self.assertIsNone(article._state.db)\n        self.assertTrue(article._state.adding)\n        self.assertEqual(article._state.fields_cache, {})\n", "    def test_model_instance_state_is_consistent_with_db(self):\n        article = Article.objects.create(title='Test', slug='test')\n        self.assertEqual(article._state.db, DEFAULT_DB_ALIAS)\n", "    def test_model_state_field_cache(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n\n        model = Model(field='test')\n        self.assertEqual(model.fields_cache, {'field': 'test'})\n        self.assertEqual(model._state.fields_cache, model.fields_cache)\n        model._state.fields_cache['field'] = 'new_value'\n        self.assertEqual(model.fields_cache, model._state.fields_cache)\n\n        class Model(models.Model):\n            field1 = models.CharField(max_length=255)\n            field2 = models.CharField(max_length=255)\n\n        model = Model(field1='test1', field2='test2')\n        self.assertEqual(model.fields_cache, {'field1': 'test1', 'field2': 'test2'})\n        model._state.fields_cache = {'field2': 'new_value'}\n        self.assertEqual(model.fields_cache, {'field1': 'test1', 'field2': 'new_value'})\n"], "sample_813": ["def test_ard_regression_convergence():\n    X, y = diabetes.data, diabetes.target\n    model = ARDRegression(n_iter=1, compute_score=True, tol=1e-10)\n    model.fit(X, y)\n    assert_equal(model.n_iter_, 1)\n", "    def test_bayesian_ridge_repr(self):\n        model = BayesianRidge()\n        expected_repr = \"BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None, compute_score=False, copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, lambda_init=None, n_iter=300, normalize=False, tol=0.001, verbose=False)\"\n        assert str(model) == expected_repr\n", "def test_BayesianRidge_compute_score():\n    \"\"\"\n    Check that scores_ is computed and correctly updated at each iteration.\n    \"\"\"\n    X, y = diabetes.data, diabetes.target\n    model = BayesianRidge(compute_score=True)\n    model.fit(X, y)\n    assert_equal(len(model.scores_), model.n_iter_ + 1)\n    assert_array_almost_equal(model.scores_[0], model.scores_[-1])\n    model.fit(X, y, compute_score=False)\n    assert_equal(len(model.scores_), 0)\n    model = BayesianRidge(compute_score=True)\n    model.fit(X, y, n_iter=10)\n    assert_equal(len(model.scores_), 11)\n    assert_array_almost_equal(model.scores_[0], model.scores_[-1])\n", "    def test_BayesianRidge(self):\n        X, y = diabetes.data, diabetes.target\n        y += np.random.randn(*y.shape) / 10.\n        clf = BayesianRidge()\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_almost_equal(y_pred, y, decimal=2)\n\n        # Test with specified regularization parameters\n        clf = BayesianRidge(alpha_1=1e-3, alpha_2=1e-4, lambda_1=1e-5,\n                            lambda_2=1e-6)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_almost_equal(y_pred, y, decimal=2)\n", "def test_BayesianRidge_with_log_marginal_likelihood_score():\n    X, y = diabetes.data, diabetes.target\n    clf = BayesianRidge(compute_score=True, alpha_2=1e-8, lambda_2=1e-8)\n    clf.fit(X, y)\n    assert len(clf.scores_) == clf.n_iter_\n    assert_array_almost_equal(clf.scores_[-1], clf.scores_[0])\n"], "sample_1017": ["def test_to_cnf():\n    from sympy.abc import x, y\n    assert to_cnf((x | y | x >> y) & (y | x | y >> x)) == (y | x)\n", "def test_as_set():\n    from sympy.logic.boolalg import as_set\n    from sympy.abc import x\n\n    # Test empty set\n    assert as_set(false).equals(EmptySet())\n    # Test universal set\n    assert as_set(true).equals(UniversalSet())\n    # Test interval\n    assert as_set(x > 0).equals(Interval.open(0, oo))\n    # Test union\n    assert as_set(x < -2 | 2 < x).equals(Union(Interval.open(-oo, -2), Interval.open(2, oo)))\n    # Test intersection\n    assert as_set(x > 0 & x < 2).equals(Interval.open(0, 2))\n", "def test_bool_map():\n    w, x, y, z, a, b, c, d = symbols('w z y a b c d')\n    expr1 = Or(And(Not(y), w), And(Not(y), z), And(x, y))\n    expr2 = Or(And(Not(c), a), And(Not(c), d), And(b, c))\n    assert bool_map(expr1, expr2) == ((x & y) | (w & ~y) | (z & ~y), {w: a, x: b, y: c, z: d})\n", "def test_to_dnf():\n    from sympy import And, Or, Not\n    from sympy.abc import x, y\n    expr = And(x, Or(y, Not(x)))\n    assert to_dnf(expr) == Or(x & y, x & ~x)\n    assert to_dnf(expr, simplify=False) == And(x, Or(y, Not(x)))\n", "def test_and():\n    a, b, c = symbols('a:c')\n    assert And(True, False) == false\n    assert And(False, True) == false\n    assert And(True, True) == true\n    assert And(False, False) == false\n    assert And(a, ~a) == false\n    assert And(~a, a) == false\n    assert And(~a, ~a) == false\n    assert And(true, false) == false\n    assert And(false, true) == false\n"], "sample_905": ["def test_is_cython_function_or_method():\n    class CythonFunction:\n            pass\n\n            pass\n\n    cython_func = CythonFunction()\n    assert inspect.is_cython_function_or_method(cython_func.cython_function_or_method)\n\n    class CythonMethod:\n            pass\n\n            pass\n\n    cython_method = CythonMethod()\n    assert inspect.is_cython_function_or_method(cython_method.cython_function_or_method)\n\n    class NotCythonFunction:\n            pass\n\n            pass\n\n    not_cython_func = NotCythonFunction()\n    assert not inspect.is_cython_function_or_method(not_cython_func.foo)\n", "def test_getorigbases():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    assert getorigbases(C) == (A, B)\n    assert getorigbases(A) == ()\n    assert getorigbases(object) is None\n    assert getorigbases(\"string\") is None\n    assert getorigbases(123) is None\n    assert getorigbases(datetime.date) is None\n    assert getorigbases(enum.Enum) is None\n    assert getorigbases(functools.partial) is None\n    assert getorigbases(types.FunctionType) is None\n    assert getorigbases(type) is None\n    assert getorigbases(inspect.Parameter) is None\n", "def test_is_cython_function_or_method():\n    class C:\n            pass\n\n        @staticmethod\n            pass\n\n    class D:\n            pass\n\n        @staticmethod\n            pass\n\n    assert inspect.is_cython_function_or_method(C.cython_method)\n    assert not inspect.is_cython_function_or_method(D.method)\n    assert inspect.is_cython_function_or_method(C.cython_static_method)\n    assert not inspect.is_cython_function_or_method(D.static_method)\n\n    class E:\n            self.cython_method = functools.wraps(lambda: None)(lambda self: None)\n            self.cython_static_method = functools.wraps(lambda: None)(staticmethod(lambda: None))\n\n    assert inspect.is_cython_function_or_method(E().cython_method)\n    assert not inspect.is_cython_function_or_method(E().cython_static_method)\n", "def test_getslots():\n    class SlotClass:\n        __slots__ = ['a', 'b']\n\n    class NoSlotClass:\n        pass\n\n    assert getslots(SlotClass) == {'a': None, 'b': None}\n    assert getslots(NoSlotClass) is None\n", "def test_getdoc_partial_method():\n    \"\"\"Test getdoc() for partial methods.\"\"\"\n\n    class A:\n            pass\n\n    partial_method = functools.partial(A().method)\n    assert getdoc(partial_method) is not None\n\n    class B:\n            self.partial_method = functools.partial(A().method)\n\n    assert getdoc(B().partial_method) is not None\n"], "sample_589": ["def test_NumpyInterpolator_fill_value(da, method, fill_value, period, expected):\n    interpolator = NumpyInterpolator(da.time, da, method=method, fill_value=fill_value, period=period)\n    result = interpolator(da)\n    assert_array_equal(result, expected)\n", "def test_interp_na_limits_scalar():\n    da = xr.DataArray([0, np.nan, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7], dims=\"time\")\n    da_limit = da.interp_na(dim=\"time\", limit=2)\n    assert_array_equal(\n        da_limit, xr.DataArray([0, 0, 1, 2, 2, 3, 4, 5, 5, 6, 7], dims=\"time\")\n    )\n\n    da_limit = da.interp_na(dim=\"time\", limit=5)\n    assert_array_equal(\n        da_limit, xr.DataArray([0, 0, 1, 2, 2, 3, 4, 5, 5, 6, 7], dims=\"time\")\n    )\n\n    with raises_regex(\n        \"dim is a required argument\", match=\"^dim is a required argument$\"\n    ):\n        da.interp_na()\n\n    with raises_regex(\"dim is a required argument\", match=\"^dim is a required argument$\"):\n        da.interp_na(dim=\"invalid_dim\")\n\n", "def test_interp_index():\n    da = xr.DataArray(\n        np.array([1, 2, 3, 4, 5, np.nan, 7, 8, 9, 10, 11]),\n        dims=[\"time\"],\n        coords={\"time\": pd.date_range(\"2022-01-01\", periods=11, freq=\"D\")},\n    )\n\n    indexes = get_clean_interp_index(da, \"time\", use_coordinate=False)\n    expected_indexes = np.linspace(1, 11, 11, dtype=np.float64)\n    assert_array_equal(indexes.values, expected_indexes)\n\n    indexes = get_clean_interp_index(da, \"time\", use_coordinate=True)\n    assert_array_equal(indexes.values, da.coords[\"time\"].values)\n\n    indexes = get_clean_interp_index(da, \"time\", use_coordinate=\"time\")\n    assert_array_equal(indexes.values, da.coords[\"time\"].values)\n\n    with raises_regex(\n        \"Index time must be castable to float64 to support \"\n        \"interpolation, got pandas.RangeIndex\",\n        \"Cannot convert\"\n    ):\n        get_clean_interp_index(da, \"time\", use_coordinate=\"index\")\n\n    with raises_regex(\"Index time must be monotonically increasing\", \"must be sorted\"):\n        da = xr.DataArray(\n            np.array([1, 3, 2, 4, 5, np.nan, 7, 8, 9, 10, 11]),\n            dims=[\"time\"],\n            coords={\"time\": pd.date_range(\"2022-01-01\", periods=11, freq=\"D\")},\n        )\n        get_clean_interp_index(da, \"time\", use_coordinate=True)\n\n    with raises_regex(\"Index time has duplicate values\", \"has duplicate\"):\n        da = xr.DataArray(\n            np.array([1, 2, 2, 4, 5, np.nan, 7, 8, 9, 10, 11]),\n            dims=[\"time\"],\n            coords={\"time\": pd.date_range(\"2022-01-01\", periods=11, freq=\"D\")},\n        )\n        get_clean_interp_index(da, \"time\", use_coordinate=True)\n", "def test_fill_with_interpolation_limit():\n    # Test that fill_value and period are correctly handled when limit is specified\n    da = xr.DataArray(\n        [1, 2, np.nan, 3, np.nan, 4, 5, 6, np.nan, 7],\n        dims=\"time\",\n        coords={\"time\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)},\n    )\n    ds = xr.Dataset({\"var\": da})\n    filled = ds.var.interp_na(method=\"linear\", dim=\"time\", limit=2, fill_value=0, period=1)\n    expected = xr.DataArray(\n        [1, 2, 0, 3, 0, 4, 5, 6, 0, 7],\n        dims=\"time\",\n        coords={\"time\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)},\n    )\n    assert_array_equal(filled, expected)\n\n    filled = ds.var.interp_na(method=\"linear\", dim=\"time\", limit=2, period=1)\n    expected = xr.DataArray(\n        [1, 2, np.nan, 3, np.nan, 4, 5, 6, np.nan, 7],\n        dims=\"time\",\n        coords={\"time\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)},\n    )\n    assert_array_equal(filled, expected)\n\n    filled = ds.var.interp_na(method=\"linear\", dim=\"time\", limit=2, fill_value={\"left\": 0, \"right\": 10})\n    expected = xr.DataArray(\n        [0, 2, np.nan, 3, np.nan, 4, 5, 6, np.nan, 7],\n        dims=\"time\",\n        coords={\"time\": pd.date_range(\"2000-01-01\", freq=\"D\", periods=10)},\n    )\n    assert_array_equal(filled, expected)\n\n    filled = ds.var.interp_na(method=\"linear\", dim=\"time\", limit=2, fill_value=10)\n    expected = xr.DataArray(\n        [10, 2, np.nan, 3, np.nan, 4, 5, 6, np.nan, 7],\n        dims=\"time\",\n        coords={\"time\":", "def test_interp_na_index_type():\n    # test different index types (including non-monotonic, non-unique and non-castable)\n    da, df = make_interpolate_example_data((10, 10), 0)\n    da.coords[\"time\"] = pd.to_timedeltaIndex([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    assert get_clean_interp_index(da, \"time\").dtype == \"timedelta64[ns]\"\n    da.coords[\"time\"] = pd.to_timedeltaIndex([1, 5, 3, 4, 2, 6, 7, 8, 9, 10])\n    with raises_regex(ValueError, r\"Index 'time' must be monotonically increasing\"):\n        get_clean_interp_index(da, \"time\")\n    da.coords[\"time\"] = pd.to_timedeltaIndex([1, 2, 2, 4, 5, 6, 7, 8, 9, 10])\n    with raises_regex(ValueError, r\"Index 'time' has duplicate values\"):\n        get_clean_interp_index(da, \"time\")\n    da.coords[\"time\"] = \"string\"\n    with raises_regex(TypeError, r\"Index 'time' must be castable to float64 to support interpolation\"):\n        get_clean_interp_index(da, \"time\")\n"], "sample_596": ["def test_concat_dataset_different_names_different_compat(\n    obj1, obj2, concat_dim, expected", "def test_concat_dimensions():\n    # test that concat adds a new dimension when the dimension name is provided\n    da1 = DataArray([1, 2, 3], dims=['a'])\n    da2 = DataArray([4, 5, 6], dims=['a'])\n    ds = concat([da1, da2], dim='b')\n    assert 'b' in ds.dims\n\n    # test that concat adds a new dimension when the dimension name is provided as a DataArray\n    da1 = DataArray([1, 2, 3], dims=['a'])\n    da2 = DataArray([4, 5, 6], dims=['a'])\n    dim = DataArray([0, 1], dims=['b'])\n    ds = concat([da1, da2], dim=dim)\n    assert 'b' in ds.dims\n\n    # test that concat does not add a new dimension when the dimension name is already present\n    da1 = DataArray([1, 2, 3], dims=['a', 'b'])\n    da2 = DataArray([4, 5, 6], dims=['a', 'b'])\n    ds = concat([da1, da2], dim='b')\n    assert 'b' in ds.dims\n\n    # test that concat raises an error when the dimension name is not present in any of the DataArrays\n    da1 = DataArray([1, 2, 3], dims=['a'])\n    da2 = DataArray([4, 5, 6], dims=['c'])\n    with pytest.raises(ValueError):\n        concat([da1, da2], dim='b')\n\n    # test that concat raises an error when the dimension name is provided as a string but not present in any of the DataArrays\n    da1 = DataArray([1, 2, 3], dims=['a'])\n    da2 = DataArray([4, 5, 6], dims=['a'])\n    with pytest.raises(ValueError):\n        concat([da1, da2], dim='d')\n", "def test_concat_join(obj1, obj2, join, expected_idx):\n    result = concat([obj1, obj2], join=join)\n    assert result.index.equals(expected_idx)\n", "def test_concat_dataset_with_coord_var_and_data_var_with_different_names():\n    \"\"\"Test that concatenating two datasets along a new dimension results in a dataset\n    where a variable that is a dimension in some datasets but a data variable in others\n    is correctly handled.\n\n    \"\"\"\n    ds1 = Dataset({\"x\": (\"y\", [1, 2, 3])})\n    ds1.coords[\"y\"] = (\"y\", [4, 5, 6])\n    ds1.data_vars[\"x\"] = (\"y\", [7, 8, 9])\n\n    ds2 = Dataset({\"x\": (\"y\", [10, 11, 12])})\n    ds2.coords[\"y\"] = (\"y\", [13, 14, 15])\n    ds2.data_vars[\"x\"] = (\"y\", [16, 17, 18])\n\n    ds = concat([ds1, ds2], \"new_dim\")\n    assert_equal(ds.coords, {\"y\": (\"new_dim\", [4, 5, 6])})\n    assert_equal(ds.data_vars, {\"x\": (\"new_dim\", [7, 8, 9])})\n    assert_equal(ds.dims, {\"new_dim\": 3})\n", "def test_concat_with_dask_dataframe_and_dataarray():\n    da1 = DataArray(np.random.rand(5), dims=['x'], name='a')\n    df1 = pd.DataFrame({'x': [1, 2, 3, 4, 5]})\n    df1 = df1.set_index('x')\n    ds1 = Dataset({'a': da1.to_dataset()})\n    da2 = DataArray(np.random.rand(5), dims=['x'], name='a')\n    df2 = pd.DataFrame({'x': [6, 7, 8, 9, 10]})\n    df2 = df2.set_index('x')\n    ds2 = Dataset({'a': da2.to_dataset()})\n\n    concatenated_ds = concat([ds1, ds2], dim='x')\n    concatenated_da = concat([da1, da2], dim='x')\n    concatenated_df = concat([df1, df2], dim='x')\n\n    expected_da = DataArray(np.concatenate([da1.data, da2.data]), dims=['x'], name='a')\n    expected_ds = Dataset({'a': expected_da.to_dataset()})\n    expected_df = pd.concat([df1, df2])\n\n    assert_array_equal(concatenated_da, expected_da)\n    assert_equal(concatenated_ds, expected_ds)\n    assert_equal(concatenated_df, expected_df)\n\n    # Check that concat works with a list of dataarrays and a Dataset\n    concatenated_da = concat([da1, ds1, da2, ds2], dim='x')\n    expected_da = DataArray(np.concatenate([da1.data, ds1['a'].data, da2.data, ds2['a'].data]), dims=['x'], name='a')\n    expected_da = expected_da.sortby('x')\n    assert_array_equal(concatenated_da, expected_da)\n\n    # Check that concat works with a list of datasets and a DataArray\n    concatenated_da = concat([ds1, da1, ds2, da2], dim='x')\n    expected_da = DataArray(np.concatenate([ds1['a'].data, da1.data, ds2['a'].data, da2.data]), dims=['x'], name='a')\n    expected_da = expected_da.sortby('x')\n    assert_array_equal(concatenated_da, expected_da)\n\n    # Check that concat raises an error when data_vars is not 'all'\n    with raises_regex(ValueError, r"], "sample_564": ["def test_plot_surface_invalid_data(rendering):\n    fig, ax = plt.subplots()\n    X, Y, Z = get_test_data()\n    Z = np.full_like(Z, np.nan)\n    if rendering:\n        ax.plot_surface(X, Y, Z)\n    else:\n        ax.add_collection3d(art3d.Poly3DCollection(np.array([[1, 2, 3]])))\n    assert not np.all(np.isfinite(Z))\n    assert ax.collections[0].collections[0].collections[0].get_array().size == 0\n", "def test_axis_labels():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n    ax.set_zlabel('Z-axis')\n\n    assert ax.get_xlabel() == 'X-axis'\n    assert ax.get_ylabel() == 'Y-axis'\n    assert ax.get_zlabel() == 'Z-axis'\n\n    ax.set_xlabel('New X-axis')\n    ax.set_ylabel('New Y-axis')\n    ax.set_zlabel('New Z-axis')\n\n    assert ax.get_xlabel() == 'New X-axis'\n    assert ax.get_ylabel() == 'New Y-axis'\n    assert ax.get_zlabel() == 'New Z-axis'\n\n    ax.clear()\n    assert ax.get_xlabel() == ''\n    assert ax.get_ylabel() == ''\n    assert ax.get_zlabel() == ''\n\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_zlabel('Z-axis', labelpad=10)\n\n    assert ax.zaxis.labelpad == 10\n\n    ax.clear()\n    assert ax.zaxis.labelpad == 0\n\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_xlabel('X-axis')\n    ax.set_ylabel('Y-axis')\n\n    # Test that setting the axis labels to None removes them\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    assert ax.get_xlabel() == ''\n    assert ax.get_ylabel() == ''\n\n    # Test that setting the axis labels to None when they are already None\n    # has no effect\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\n    assert ax.get_xlabel() == ''\n    assert ax.get_ylabel() == ''\n", "def test_auto_scale_xyz():\n    fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n    X, Y, Z = get_test_data(delta=0.05)\n\n    # Create a stem plot\n    ax.stem(X[1, 1], Y[1, 1], Z[1, 1])\n\n    # Scale the axes manually\n    ax.set_xlim3d(-3, 3)\n    ax.set_ylim3d(-3, 3)\n    ax.set_zlim3d(-1, 1)\n\n    ax.auto_scale_xyz(X, Y, Z)\n\n    # Set the z-axis limits manually\n    ax.set_zlim3d(0, 1)\n\n    # Scale the axes again\n    ax.auto_scale_xyz(X, Y, Z)\n\n    # Check the scale limits\n    assert ax.get_zlim3d() == (-0.5, 0.5)\n", "def test_add_contour_set_extend_contour():\n    # test that add_contour_set can add a contour from a 2d contourset\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    X, Y, Z = get_test_data(delta=0.05)\n    Z[1, 1] = 1.2  # Add a new level to the contour\n    cset = ax.contourf(X, Y, Z, zdir='z', offset=Z.min() - 0.1)\n    ax.add_contour_set(cset, extend3d=True)\n\n    # Check that the contour has been extended correctly\n    assert np.allclose(cset._all_levels(), np.array([0.0, 1.2]))\n    assert cset._extend_min == False\n    assert cset._extend_max == False\n\n    plt.close(fig)\n", "def test_axes3D_draw():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], 'o')\n    ax.axvline(1.5, color='r')\n    ax.axhline(2.5, color='g')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.view_init(elev=10, azim=45)\n    ax.set_xlim3d(-10, 10)\n    ax.set_ylim3d(-10, 10)\n    ax.set_zlim3d(-10, 10)\n    ax.set_aspect('equal')\n    ax.set_title('3D plot')\n    ax.text(1, 2, 3, 'text at data point')\n\n    assert ax.figure.canvas.renderer is not None\n    ax.draw(renderer=None)\n\n    assert ax._axis3don is True\n\n    assert ax.viewInitCalled\n    assert ax.viewLim.intervalx == (-0.9, 0.9)\n    assert ax.viewLim.intervaly == (-0.9, 0.9)\n    assert ax.viewLim.intervalz == (-0.9, 0.9)\n    assert ax.xy_viewLim.intervalx == (-0.95, 0.95)\n    assert ax.xy_viewLim.intervaly == (-0.95, 0.95)\n    assert ax.xy_viewLim.intervalz == (0, 0)\n    assert ax.zz_viewLim.intervalx == (0, 0)\n    assert ax.zz_viewLim.intervaly == (0, 0)\n    assert ax.zz_viewLim.intervalz == (-0.9, 0.9)\n    assert ax.xy_dataLim.intervalx == (-10, 10)\n    assert ax.xy_dataLim.intervaly == (-10, 10)\n    assert ax.xy_dataLim.intervalz == (0, 0)\n    assert ax.zz_dataLim.intervalx == (0, 0)\n    assert ax.zz_dataLim.intervaly == (0, 0)\n    assert ax.zz_dataLim.intervalz == (-10, 10)\n    assert ax.get_top_view() == None\n\n    ax.set_zlim(0, 10)\n    ax.set_zlim(-10, 0)\n    assert ax.get_zlim() == (-10, 0)\n    assert ax.set_z"], "sample_1188": ["def test_PrettyPrinter__print_TensorIndex():\n    from sympy.tensor.tensor import TensorIndex\n    from sympy import symbols\n    i, j, k = symbols('i j k')\n    t = TensorIndex('a', i, 'up')\n    pretty_printer = PrettyPrinter(use_unicode=False)\n    assert pretty_printer._print(t) == prettyForm('a')\n    t = TensorIndex('a', i, 'down')\n    assert pretty_printer._print(t) == prettyForm('-a')\n    t = TensorIndex('a', i, 'up') + TensorIndex('b', j, 'up')\n    assert pretty_printer._print(t) == prettyForm('a + b')\n", "def test_print_PolyElement():\n    from sympy.abc import x, y\n    from sympy import Poly\n    from sympy.printing.pretty import pretty\n\n    p = Poly(x**3 + y**2 + 2*x*y + 3*x + y + 1, (x, y))\n    pform = pretty(p)\n    expected = \"\"\"\\\n    (3 x + 1) x + (2 x + y + 1) y + 1 \\", "def test_printing_matrix():\n    from sympy import Matrix, Symbol, MatrixSymbol\n    M = Matrix([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    p = PrettyPrinter()\n    pretty_M = p._print_MatrixBase(M)\n    pretty_M_2 = p._print(M)\n    assert pretty_M == pretty_M_2\n\n    A = MatrixSymbol('A', 3, 3)\n    pretty_A = p._print(Matrix([1, 2, 3]))\n    assert pretty_A == p._print(MatrixSymbol('A', 3, 3))\n\n    p._print_matrix_contents(M)\n    p._print_MatrixBase(M)\n    p._print(MatrixSymbol('A', 3, 3))\n\n    M = Matrix([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    p = PrettyPrinter()\n    pretty_M = p._print_matrix_contents(M)\n    assert pretty_M == p._print(MatrixBase(M))\n\n    p = PrettyPrinter()\n    A = MatrixSymbol('A', 3, 3)\n    pretty_A = p._print(A)\n    assert pretty_A == p._print(MatrixSymbol('A', 3, 3))\n\n    p = PrettyPrinter()\n    A = MatrixSymbol('A', 3, 3)\n    pretty_A = p._print(A.as_mutable())\n    assert pretty_A == p._print(MatrixSymbol('A', 3, 3))\n\n    A = MatrixSymbol('A', 3, 3)\n    pretty_A = p._print(A.as_immutable())\n    assert pretty_A == p._print(MatrixSymbol('A', 3, 3))\n\n    p = PrettyPrinter()\n    A = MatrixSymbol('A', 3, 3)\n    pretty_A = p._print(A.to_matrix())\n    assert pretty_A == p._print(MatrixSymbol('A', 3, 3))\n", "def test_PrettyPrinter_print_Piecewise():\n    # Check that the PrettyPrinter works correctly for Piecewise\n    # expressions\n    p = PrettyPrinter(use_unicode=True)\n    pexpr = Piecewise((a, a < 1), (b, a >= 1))\n    expected = \"\"\"\\\n          otherwise\n         ________________________\n   a < 1  |    a\n  a >= 1  |    b", "def test_PrettyPrinter_Covariance():\n    from sympy.stats import Covariance\n    from sympy.printing.pretty import pretty as xpretty\n\n    # Create a random covariance matrix\n    X = Covariance('X', 'a', 'b')\n    X.assign(b=1)\n    X.assign(a=2)\n    X.assign(b=3)\n    X.assign(a=4)\n    X.assign(b=5)\n    X.assign(a=6)\n    X.assign(b=7)\n\n    # Pretty print the covariance matrix\n    upretty_X = upretty(X)\n    assert upretty_X == \"\"\"\\"], "sample_991": ["def test_product_over_empty_set():\n    assert product(2, (x, 3, 2)).doit() == 1\n    assert product(x, (x, 4, 1)).doit() == 1\n    assert product(x**2, (x, 1, 0)).doit() == 1\n    assert product(x, (x, 5, 5)).doit() == 1\n    assert product(2*x, (x, 6, 6)).doit() == 1\n    assert product(x**2 + 1, (x, 3, 2)).doit() == 1\n    assert product(x + 1, (x, 4, 1)).doit() == 1\n    assert product(x**2 + 1, (x, 1, 0)).doit() == 1\n    assert product(x + 1, (x, 5, 5)).doit() == 1\n", "def test_product_with_symbolic_limit():\n    from sympy import Rational, oo, Product\n    from sympy.abc import m\n    P = Product(Rational(1, m), (m, 1, oo))\n    assert P.is_convergent()\n    assert P.is_convergent(absolute=False)\n", "def test_product_with_log():\n    x = Symbol('x')\n    f = symbols('f')\n    assert product(f(x), (x, 1, n)).doit() == f(n)\n    assert product(log(f(x)), (x, 1, n)).doit() == log(f(n))\n    assert product(f(x) * log(x), (x, 1, n)).doit() == f(n) * log(n)\n", "def test_product_mutable_dict():\n    from sympy.core.mutables import Dict\n    k = symbols('k')\n    expr = Product(k, (k, 1, 5))\n    assert expr.is_commutative\n    expected = Dict({1: 5, 2: 5, 3: 5, 4: 5, 5: 5})\n    assert expr.doit() == factorial(expected)\n", "def test_product_infinite():\n    from sympy.concrete.expr_with_intlimits import ExprWithIntLimits\n    from sympy.functions.elementary.exponential import exp\n    from sympy.concrete.summations import Sum, summation\n    n = Symbol('n', integer=True)\n    x = Symbol('x')\n    # Test that an infinite product can be expressed in a different variable\n    P = Product(2**n, (n, 1, oo))\n    P = P.reverse_order(n)\n    assert P == Product(2**(-n + 1), (n, oo, 1))\n\n    # Test that the expression can be simplified\n    P = Product(x, (x, 1, 5))\n    assert P.simplify() == 1\n    # Test that we get an unevaluated Product when the limits are symbolic\n    from sympy.solvers import solve\n    P = Product(x**2, (x, solve(x**2-1, x), solve(x**2-4, x)))\n    assert isinstance(P, Product)\n    # Test that we get an unevaluated Product with empty limits\n    P = Product(x**2, (x, 1, 0))\n    assert isinstance(P, Product)\n"], "sample_783": ["def _check_imputation_with_constant_strategy(X, X_true, missing_values, strategy, fill_value):\n    \"\"\"Utility function for testing imputation with constant strategy.\n\n    Test with dense and sparse arrays\n\n    Check that:\n        - the imputed values are correct\n        - the statistics are correct\"\"\"\n\n    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n              \"sparse = {0}\" % (strategy, missing_values)\n\n    # Normal matrix\n    imputer = SimpleImputer(missing_values=missing_values, strategy=strategy,\n                            fill_value=fill_value)\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_array_equal(imputer.statistics_, np.full(X.shape[1], fill_value))\n    assert_array_equal(X_trans, X_true)\n\n    # Sparse matrix\n    imputer = SimpleImputer(missing_values=missing_values, strategy=strategy,\n                            fill_value=fill_value)\n    imputer.fit(sparse.csc_matrix(X))\n    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n\n    if sparse.issparse(X_trans):\n        X_trans = X_trans.toarray()\n\n    assert_array_equal(imputer.statistics_, np.full(X.shape[1], fill_value))\n    assert_array_equal(X_trans, X_true)\n\n", "def test_imputer_constant_strategy():\n    # Test the constant strategy with different types of data\n    X = np.array([1, 2, np.nan])\n    X_true = np.array([1, 2, 3])\n    statistics = 3\n    _check_statistics(X, X_true, 'constant', statistics, 0)\n\n    X = np.array([[1, 2, np.nan], [4, 5, 6]], dtype=object)\n    X_true = np.array([[1, 2, 3], [4, 5, 6]], dtype=object)\n    statistics = 3\n    _check_statistics(X, X_true, 'constant', statistics, 'a')\n\n    # Test the constant strategy with different fill values\n    X = np.array([1, 2, np.nan])\n    X_true = np.array([1, 2, 3])\n    imputer = SimpleImputer(strategy='constant', fill_value=5)\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    assert_array_equal(imputer.statistics_, 5)\n    assert_array_equal(X_trans, X_true)\n\n    # Test the constant strategy with a fill value that is not a number\n    X = np.array([1, 2, np.nan])\n    X_true = np.array([1, 2, 3])\n    imputer = SimpleImputer(strategy='constant', fill_value='a')\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    assert_array_equal(imputer.statistics_, 'a')\n    assert_array_equal(X_trans, X_true)\n\n    # Test the constant strategy with a fill value that is not provided\n    X = np.array([1, 2, np.nan])\n    X_true = np.array([1, 2, 3])\n    imputer = SimpleImputer(strategy='constant')\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    assert_array_equal(imputer.statistics_, 0)\n    assert_array_equal(X_trans, X_true)\n", "def test_constant_strategy_sparse():\n    X = sparse.csc_matrix([[1, 2, 3], [4, np.nan, 6], [10, 5, 9]])\n    X_true = np.array([[1, 2, 3], [4, 7, 6], [10, 5, 9]])\n\n    missing_values = 0\n    imputer = SimpleImputer(strategy='constant', missing_values=missing_values)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    X_true_trans = imputer.transform(sparse.csc_matrix(X_true))\n\n    assert_array_equal(X_trans.toarray(), X_true_trans)\n", "def test_constant_strategy_fill_value():\n    \"\"\"Test constant strategy with custom fill value.\n\n    Test that the imputer correctly imputes missing values with a custom\n    fill value.\"\"\"\n    X = np.array([[1, np.nan, 3], [4, np.nan, 6], [np.nan, 1, 9]])\n\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=5,\n                            missing_values=np.nan)\n    X_trans = imputer.fit(X).transform(X.copy())\n\n    assert_array_equal(X_trans, np.array([[1, 5, 3], [4, 5, 6], [5, 1, 9]]))\n\n    # Test with a custom fill value that is not a number\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\",\n                            missing_values=np.nan)\n    X_trans = imputer.fit(X).transform(X.copy())\n    assert_array_equal(X_trans, np.array([[1, 'missing', 3], [4, 'missing', 6],\n                                         ['missing', 1, 9]]))\n\n    # Test with a custom fill value that is not a number and a sparse array\n    X_sparse = sparse.csc_matrix(np.array([[1, np.nan, 3], [4, np.nan, 6],\n                                           [np.nan, 1, 9]]))\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\",\n                            missing_values=np.nan)\n    X_trans_sparse = imputer.fit(X_sparse).transform(X_sparse.copy())\n    assert_array_equal(X_trans_sparse.toarray(), np.array([[1, 'missing', 3],\n                                                         [4, 'missing', 6],\n                                                         ['missing', 1, 9]]))\n\n    # Test with a custom fill value that is not a number and a dense array\n    imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\",\n                            missing_values=np.nan)\n    X_trans = imputer.fit(X.copy()).transform(X.copy())\n    assert_array_equal(X_trans, np.array([[1, 'missing', 3], [4, 'missing', 6],\n                                         ['missing', 1, 9]]))\n", "def test_most_frequent_string():\n    \"\"\"Test that the most frequent imputation strategy works for strings.\"\"\"\n    X = np.array([[np.nan, 'a', 'a'],\n                  ['b', np.nan, 'c'],\n                  ['a', 'b', np.nan]])\n    X_true = np.array([['a', 'a', 'a'],\n                      ['b', 'c', 'c'],\n                      ['a', 'b', 'b']])\n\n    _check_statistics(X, X_true, 'most_frequent', [0, 1, 0], np.nan)\n\n    # Test that the most frequent value is chosen when there is a tie\n    X = np.array([[np.nan, 'a', 'b'],\n                  ['a', np.nan, 'c'],\n                  ['a', 'b', np.nan]])\n    X_true = np.array([['a', 'a', 'a'],\n                      ['a', 'c', 'c'],\n                      ['a', 'b', 'b']])\n    _check_statistics(X, X_true, 'most_frequent', [0, 1, 0], np.nan)\n\n    # Test that the most frequent value is chosen when the most frequent value is\n    # the extra value\n    X = np.array([[np.nan, 'a', 'a'],\n                  ['b', np.nan, 'a'],\n                  ['a', 'b', np.nan]])\n    X_true = np.array([['a', 'a', 'a'],\n                      ['a', 'a', 'a'],\n                      ['a', 'b', 'b']])\n    _check_statistics(X, X_true, 'most_frequent', [0, 1, 0], np.nan)\n"], "sample_106": ["    def test_patch_vary_headers_multiple_headers(self):\n        response = HttpResponse()\n        patch_vary_headers(response, ['header1', 'header2', 'header3'])\n        self.assertEqual(response['Vary'], 'header1, header2, header3')\n", "    def test_learn_cache_key_without_vary_header(self):\n        response = HttpResponse()\n        response['Content-Type'] = 'text/html'\n        cache_key = learn_cache_key(self.factory_request(), response)\n        self.assertIsNotNone(cache_key)\n        cache = caches['default']\n        cache_key_header = 'views.decorators.cache.cache_header.default.%s' % (\n            hashlib.md5(iri_to_uri(self.factory_request().build_absolute_uri()).encode('ascii')).hexdigest()\n        )\n        self.assertEqual(cache.get(cache_key_header), [])\n", "    def setUp(self):\n        self.cache = caches['default']\n        self.request = RequestFactory().request()\n        self.response = HttpResponse('test')\n", "    def setUp(self):\n        self.old_MIDDLEWARE_CLASSES = settings.MIDDLEWARE_CLASSES\n        settings.MIDDLEWARE_CLASSES = [\n            'django.contrib.sessions.middleware.SessionMiddleware',\n            'django.middleware.common.CommonMiddleware',\n            'django.middleware.cache.UpdateCacheMiddleware',\n            'django.middleware.cache.FetchFromCacheMiddleware',\n            'django.contrib.auth.middleware.AuthenticationMiddleware',\n            'django.contrib.messages.middleware.MessageMiddleware',\n        ]\n        self.cache_middleware = CacheMiddleware()\n        self.request = RequestFactory().get('/')\n        self.request.META['HTTP_HOST'] = 'example.com'\n        self.request.META['HTTP_ACCEPT_LANGUAGE'] = 'fr'\n        self.request.META['HTTP_ACCEPT_ENCODING'] = 'gzip, deflate'\n        self.request.META['HTTP_USER_AGENT'] = 'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)'\n", "    def test_patch_cache_control(self):\n        response = HttpResponse('Hello, world!')\n        patch_cache_control(response, max_age=3600, private=True, must_revalidate=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, private, must-revalidate')\n        patch_cache_control(response, public=True)\n        self.assertEqual(response['Cache-Control'], 'max-age=3600, private, must-revalidate, public')\n"], "sample_622": ["    def test_decode_cf_variables_no_decode_times(self):\n        ds = Dataset(\n            {\n                \"time\": ((\"t\",), cftime_range(\"2000-01-01\", periods=10, freq=\"D\")),\n                \"lat\": (\"y\", [0, 1]),\n                \"lon\": (\"x\", [0, 1]),\n                \"bounds\": ((\"y\", \"x\"), np.ones((2, 2))),\n            },\n            coords={\n                \"lat\": (\"y\", [0, 1]),\n                \"lon\": (\"x\", [0, 1]),\n            },\n            attrs={\"units\": \"days since 2000-01-01\"},\n        )\n        ds.decode_cf()\n        assert ds[\"time\"].dtype == \"M8[ns]\"\n        assert ds[\"lat\"].dtype == \"float64\"\n        assert ds[\"lon\"].dtype == \"float64\"\n        assert ds[\"bounds\"].dtype == \"float64\"\n", "    def test_encode_dataset_coordinates(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"error\")\n            ds = Dataset(\n                {\"x\": [1, 2, 3], \"y\": [4, 5, 6]},\n                coords={\"time\": cftime_range(\"2000-01-01\", periods=3)},\n                attrs={\"title\": \"Example dataset\"},\n            )\n            variables, attributes = encode_dataset_coordinates(ds)\n            expected_attributes = {\"title\": \"Example dataset\", \"coordinates\": \"time x y\"}\n            assert attributes == expected_attributes\n            expected_variables = {\n                \"x\": ds[\"x\"].copy(deep=False),\n                \"y\": ds[\"y\"].copy(deep=False),\n                \"time\": ds[\"time\"].copy(deep=False),\n            }\n            assert variables == expected_variables\n", "    def test_bool_type_array_from_byte_array(self):\n        array = np.array([1, 0, 1, 1, 0], dtype=np.uint8)\n        bool_array = coding.BoolTypeArray(array)\n        assert bool_array.dtype == np.dtype(bool)\n        assert np.array_equal(bool_array.array, np.array([True, False, True, True, False]))\n", "    def test_decode_cf_variables_mask_and_scale(self):\n        # Test that decode_cf_variables correctly masks and scales variables\n        # with scale_factor and add_offset.\n        ds = Dataset(\n            {\n                \"x\": (\n                    [\"y\", \"x\"],\n                    np.random.rand(5, 10),\n                    {\"units\": \"m\", \"scale_factor\": 10, \"add_offset\": 1},\n                )\n            },\n            attrs={\"units\": \"m\"},\n        )\n        decoded_ds = decode_cf(ds)\n        self.assert_identical(\n            decoded_ds[\"x\"].values,\n            (10 * np.random.rand(5, 10) + 1),\n            \"decode_cf_variables should apply scale_factor and add_offset\",\n        )\n", "def test_encode_decode_dataset_coordinates():\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=SerializationWarning)\n\n        # Test with simple coordinate variable\n        data = np.arange(5)\n        variable = Variable((\"dim0\",), data, attrs={\"units\": \"degrees\"})\n        ds = Dataset({\"var\": variable})\n        ds.attrs[\"coordinates\"] = \"var\"\n        encoded, _ = conventions.cf_encoder({\"var\": variable}, ds.attrs)\n        decoded = decode_cf(ds)\n        assert \"coordinates\" not in encoded[\"var\"].attrs\n        assert encoded[\"var\"].encoding[\"coordinates\"] == \"var\"\n\n        # Test with multiple coordinate variables\n        data1 = np.arange(5)\n        data2 = np.arange(10)\n        variable1 = Variable((\"dim0\",), data1, attrs={\"units\": \"degrees\"})\n        variable2 = Variable((\"dim1\",), data2, attrs={\"units\": \"degrees\"})\n        ds = Dataset({\"var1\": variable1, \"var2\": variable2})\n        ds.attrs[\"coordinates\"] = \"var1 var2\"\n        encoded, _ = conventions.cf_encoder({\"var1\": variable1, \"var2\": variable2}, ds.attrs)\n        decoded = decode_cf(ds)\n        assert \"coordinates\" not in encoded[\"var1\"].attrs\n        assert \"coordinates\" not in encoded[\"var2\"].attrs\n        assert encoded[\"var1\"].encoding[\"coordinates\"] == \"var1\"\n        assert encoded[\"var2\"].encoding[\"coordinates\"] == \"var2\"\n        assert \"coordinates\" not in decoded.attrs\n\n        # Test with coordinates that are not variables in the dataset\n        data1 = np.arange(5)\n        data2 = np.arange(10)\n        variable1 = Variable((\"dim0\",), data1, attrs={\"units\": \"degrees\"})\n        variable2 = Variable((\"dim1\",), data2, attrs={\"units\": \"degrees\"})\n        ds = Dataset({\"var1\": variable1})\n        ds.attrs[\"coordinates\"] = \"var2\"\n        encoded, _ = conventions.cf_encoder({\"var1\": variable1}, ds.attrs)\n        decoded = decode_cf(ds)\n        assert \"coordinates\" in encoded[\"var1\"].attrs\n        assert encoded[\"var1\"].encoding[\"coordinates\"] == \"var2\"\n        assert \"coordinates\" not in decoded.attrs\n\n        # Test with a variable that has coordinates in its encoding and attrs\n        data1 = np.arange(5)\n       "], "sample_634": ["def test_get_python_path_returns_cwd():\n    \"\"\"Test get_python_path returns the current working directory\"\"\"\n    result = get_python_path(\".\")\n    assert result == os.getcwd()\n\n", "    def test_get_python_path_with_invalid_file(self):\n        with pytest.raises(SystemExit):\n            get_python_path(\"/invalid/path\")\n", "    def test_get_python_path_with_cwd(self):\n        current_dir = Path(__file__).parent\n        result = get_python_path(str(current_dir))\n        self.assertEqual(result, str(current_dir))\n", "    def _expand_modules_test_helper(\n        self,\n        input_files_or_modules: List[str],\n        expected_result: List[Tuple[str, str]],\n        expected_errors: List[ErrorDescriptionDict],", "def test_get_python_path_non_existent_file():\n    result = get_python_path(\"/non/existent/file.py\")\n    assert result == \"\"\n"], "sample_1041": ["def test_matrix_symbol_to_matrix():\n    A = MatrixSymbol('A', 3, 4)\n    A_explicit = A.as_explicit()\n    A_mutable = A.as_mutable()\n    assert A_explicit.shape == (3, 4)\n    assert A_mutable.shape == (3, 4)\n", "def test_matrix_element_doit():\n    A = MatrixSymbol('A', 2, 2)\n    i, j = symbols('i j')\n    M = MatrixElement(A, i, j)\n    assert M.doit() == A[i, j]\n", "def test_matrix_symbol_from_index_summation():\n    from sympy import Sum, symbols\n    from sympy.abc import i, j, k, l, N\n    A = MatrixSymbol(\"A\", N, N)\n    B = MatrixSymbol(\"B\", N, N)\n    expr = Sum(A[i, j]*B[j, k], (j, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == MatMul(A, B)\n    expr = Sum(B[j, i]*A[i, k], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == MatMul(B, A).T\n    expr = Sum(A[i, i], (i, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == Trace(A)\n    expr = Sum(A[i, j]*B[k, j]*A[l, k], (j, 0, N-1), (k, 0, N-1))\n    assert MatrixExpr.from_index_summation(expr) == MatMul(A, B.T, A).T\n", "def test_matrix_symbols():\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A, B}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a KroneckerDelta\n    expr = Sum(A[i, j]*B[j, k]*KroneckerDelta(i, k), (j, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A, B}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a trace\n    expr = Sum(A[i, i], (i, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a transpose\n    expr = Sum(A[j, i]*B[j, k], (j, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A, B}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a constant\n    expr = Sum(2*A[i, j]*B[j, k], (j, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A, B}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a sum of matrix elements\n    expr = Sum(A[i, j]*B[j, k], (j, 0, n-1)) + Sum(C[i, j]*D[j, k], (j, 0, n-1))\n    assert set(matrix_symbols(expr)) == {A, B, C, D}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a matrix element\n    expr = A[i, j]\n    assert set(matrix_symbols(expr)) == {A}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a matrix element and a constant\n    expr = 2*A[i, j]\n    assert set(matrix_symbols(expr)) == {A}\n\n    # Test that matrix_symbols function correctly returns matrix symbols\n    # from an expression with a matrix element and a KroneckerDelta\n    expr = A[i, j]*K", "def test_matrix_transpose_transposes_parent():\n    from sympy import MatrixSymbol\n    A = MatrixSymbol('A', 3, 4)\n    assert A.T.shape == (4, 3)\n"], "sample_558": ["def test_grid_label_mode_all():\n    fig, ax = plt.subplots(3, 3)\n    grid = Grid(fig, 111, (3, 3), label_mode=\"all\")\n    assert all(ax.get_xticklabels() for ax in grid.axes_all)\n", "def test_imagegrid_colorbar_location():\n    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n\n    grid = ImageGrid(fig, [0.05, 0.05, 0.4, 0.8], (2, 2), \n                     colorbar_location='bottom')\n    for ax in axs.flat:\n        im = ax.imshow(np.random.rand(10, 10))\n        grid.cbar_axes[0].colorbar(im)\n\n    grid = ImageGrid(fig, [0.05, 0.05, 0.4, 0.8], (2, 2), \n                     colorbar_location='top')\n    for ax in axs.flat[1:]:\n        im = ax.imshow(np.random.rand(10, 10))\n        grid.cbar_axes[0].colorbar(im)\n\n    grid = ImageGrid(fig, [0.05, 0.05, 0.4, 0.8], (2, 2), \n                     colorbar_location='left')\n    for ax in axs.flat[:2]:\n        im = ax.imshow(np.random.rand(10, 10))\n        grid.cbar_axes[0].colorbar(im)\n\n    grid = ImageGrid(fig, [0.05, 0.05, 0.4, 0.8], (2, 2), \n                     colorbar_location='right')\n    for ax in axs.flat[2:]:\n        im = ax.imshow(np.random.rand(10, 10))\n        grid.cbar_axes[0].colorbar(im)\n\n    plt.show()\n", "def test_imagegrid_label_mode_all():\n    fig, axs = plt.subplots(2, 2)\n    img = np.random.rand(5, 5)\n    for ax, im in zip(axs.flat, img):\n        im = ax.imshow(img)\n        ax.set_title('image')\n\n    grid = ImageGrid(fig, axes_pad=0.05, nrows_ncols=(2, 2),\n                     label_mode=\"all\")\n    for i, ax in enumerate(grid):\n        ax.imshow(img)\n        ax.set_title(f'grid {i}')\n\n    plt.show()\n", "def test_Grid_set_label_mode(mpl_fig, direction, label_mode, expected_label):\n    fig, ax = mpl_fig\n    grid = Grid(fig, [0.1, 0.1, 0.8, 0.8], (2, 2), direction=direction)\n    grid.set_label_mode(label_mode)\n    assert grid.get_label_mode() == label_mode\n    assert grid.label_mode_doc == expected_label\n", "def test_grid_colorbar_shape_and_location(tmpdir):\n    fig, axs = plt.subplots(3, 3, figsize=(10, 10))\n    grid = ImageGrid(fig, (1.0, 0.1, 0.8, 0.8),\n                     nrows_ncols=(3, 3),\n                     cbar_mode=\"each\",\n                     cbar_location=\"bottom\",\n                     cbar_pad=0.03,\n                     cbar_size=\"3%\")\n\n    for i in range(9):\n        x = np.linspace(-3, 3, 100)\n        y = np.sin(x + i)\n        ax = grid[i]\n        im = ax.imshow(y, extent=[-3, 3, -1, 1], cmap='viridis')\n        grid.cbar_axes[i].colorbar(im, ax=ax)\n\n    assert (grid.cbar_axes[0].get_position() == Bbox([0.1, 0.04, 0.8, 0.03])).all()\n    assert (grid.cbar_axes[-1].get_position() == Bbox([0.1, 0.04, 0.8, 0.03])).all()\n\n    assert_array_equal(np.round(grid.cbar_axes[0].get_width(), decimals=4),\n                      np.round(grid.cbar_axes[-1].get_width(), decimals=4))\n    assert_array_equal(np.round(grid.cbar_axes[0].get_height(), decimals=4),\n                      np.round(grid.cbar_axes[-1].get_height(), decimals=4))\n"], "sample_966": ["def test_py_classlike_index_text():\n    domain = PythonDomain()\n    classlike = PyClasslike()\n    classlike.objtype = 'class'\n    assert classlike.get_index_text('modname', ('class_name', 'class_name')) == \\\n        _('class_name (class in modname)')\n    assert classlike.get_index_text('', ('class_name', 'class_name')) == \\\n        _('class_name (built-in class)')\n\n    classlike.objtype = 'exception'\n    assert classlike.get_index_text('modname', ('class_name', 'class_name')) == \\\n        'class_name'\n    assert classlike.get_index_text('', ('class_name', 'class_name')) == \\\n        'class_name'\n\n    classlike.objtype = 'other'\n    assert classlike.get_index_text('modname', ('class_name', 'class_name')) == \\\n        ''\n    assert classlike.get_index_text('', ('class_name', 'class_name')) == \\\n        ''\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == [\n        nodes.literal('', '', nodes.Text('int'))\n    ]\n    assert _parse_annotation('float') == [\n        nodes.literal('', '', nodes.Text('float'))\n    ]\n    assert _parse_annotation('str') == [\n        nodes.literal('', '', nodes.Text('str'))\n    ]\n    assert _parse_annotation('bool') == [\n        nodes.literal('', '', nodes.Text('bool'))\n    ]\n    assert _parse_annotation('None') == [\n        nodes.Text('None')\n    ]\n\n    assert _parse_annotation('List[int]') == [\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert _parse_annotation('Dict[str, int]') == [\n        addnodes.desc_sig_punctuation('', '{'),\n        nodes.literal('', '', nodes.Text('str')),\n        addnodes.desc_sig_punctuation('', ','),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', '}')\n    ]\n\n    assert _parse_annotation('Union[int, str]') == [\n        addnodes.desc_sig_punctuation('', '('),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', ','),\n        nodes.literal('', '', nodes.Text('str')),\n        addnodes.desc_sig_punctuation('', ')')\n    ]\n    assert _parse_annotation('Optional[int]') == [\n        addnodes.desc_sig_punctuation('', '('),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', ')')\n    ]\n    assert _parse_annotation('Tuple[int, str]') == [\n        addnodes.desc_sig_punctuation('', '('),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', ','),\n        nodes.literal('', '', nodes.Text('str')),\n        addnodes.desc_sig_punctuation('', ')')\n    ]\n\n    assert _parse_annotation('Callable[[int, str], None]') == [\n        addnodes.desc_sig_punctuation('', '('),\n        addnodes.desc_sig_keyword('', '('),\n        nodes.literal('', '', nodes.Text('int')),\n        addnodes.desc_sig_punctuation('', ','),\n        nodes.literal('', '', nodes.Text('str')),\n        addnodes.desc_sig_punctuation('', ')'),\n        addnodes.desc_sig_punctuation('', ','),\n        addnodes.desc_sig_keyword('', ')'),\n        addnodes.desc_sig_punctuation('', ')')\n   ", "def test_PyFunction_handle_signature_noarglist():\n    signode = addnodes.desc_signature('func()', '')\n    obj = PyFunction()\n    obj.options = {}\n    obj.handle_signature('func()', signode)\n    assert parse(signode) == 'func()'\n", "    def setup_method(self, func):\n        self.app = Mock()\n        self.env = Mock()\n        self.domain = PythonDomain()\n        self.domain.env = self.env\n        self.signode = addnodes.desc_signature('', '')\n        self.field = PyField()\n        self.field.signode = self.signode\n        self.field.env = self.env\n", "def test_PyMethod_get_index_text():\n    domain = PythonDomain()\n    objtype = 'method'\n    name_cls = ('ExampleClass', 'ExampleMethod')\n    modname = 'example.module'\n    if sys.version_info < (3, 8):\n        result = domain.get_class(objtype).get_index_text(modname, name_cls)\n        assert result == _('%s (in %s)') % (name_cls[1], modname)\n    else:\n        result = domain.get_class(objtype).get_index_text(modname, name_cls)\n        assert result == _('%s (%s method)') % (name_cls[1], modname)\n"], "sample_805": ["def test_mean_squared_log_error_exceptions():\n    # Test when y_true or y_pred contains negative values\n    with pytest.warns(UndefinedMetricWarning):\n        assert mean_squared_log_error([3, -0.5, 2, 7], [2.5, 0.0, 2, 8])\n    with pytest.raises(ValueError):\n        mean_squared_log_error([3, -0.5, 2, 7], [2.5, 0.0, 2, -8])\n    with pytest.raises(ValueError):\n        mean_squared_log_error([3, 0.5, 2, 7], [2.5, 0.0, 2, 8])\n\n    # Test when y_true or y_pred is not 1D\n    with pytest.raises(ValueError):\n        mean_squared_log_error([[3, 4], [5, 6]], [2.5, 0.0, 2, 8])\n    with pytest.raises(ValueError):\n        mean_squared_log_error([3, -0.5, 2, 7], [[2.5, 0.0], [2, 8]])\n\n    # Test when multioutput is not one of ['raw_values', 'uniform_average']\n    with pytest.raises(ValueError):\n        mean_squared_log_error([3, -0.5, 2, 7], [2.5, 0.0, 2, 8], multioutput='invalid')\n", "def test_mean_tweedie_deviance_edge_cases():\n    # Check that mean_tweedie_deviance raises an error when y_true and y_pred\n    # have different lengths\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred)\n\n    # Check that mean_tweedie_deviance raises an error when y_true and y_pred\n    # contain negative values\n    y_true = [-1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred)\n\n    # Check that mean_tweedie_deviance raises an error when y_true and y_pred\n    # contain non-strictly positive values for p=1 (Poisson distribution)\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=1)\n\n    # Check that mean_tweedie_deviance raises an error when y_true and y_pred\n    # contain non-strictly positive values for p=2 (Gamma distribution)\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=2)\n\n    # Check that mean_tweedie_deviance raises an error when y_true and y_pred\n    # contain non-strictly positive values for p=3 (Inverse Gaussian distribution)\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=3)\n\n    # Check that mean_tweedie_deviance raises an error when p is not in the valid range\n    y_true = [1, 2, 3, 4]\n    y_pred = [1, 2, 3, 4]\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y", "def test__check_reg_targets():\n    # Test case for consistent length\n    with pytest.warns(UndefinedMetricWarning):\n        y_true = [1, 2, 3]\n        y_pred = [4, 5, 6, 7]\n        _check_reg_targets(y_true, y_pred, None)\n\n    # Test case for different number of outputs\n    with pytest.raises(ValueError):\n        y_true = [1, 2, 3]\n        y_pred = [4, 5]\n        _check_reg_targets(y_true, y_pred, None)\n\n    # Test case for multioutput string\n    y_true = [1, 2, 3]\n    y_pred = [4, 5, 6]\n    multioutput = 'uniform_average'\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)\n    assert y_type == 'continuous'\n    assert y_true.shape == (3, 1)\n    assert y_pred.shape == (3, 1)\n    assert multioutput == 'uniform_average'\n\n    # Test case for custom multioutput\n    y_true = [1, 2, 3]\n    y_pred = [4, 5, 6]\n    multioutput = [0.5, 0.5]\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, multioutput)\n    assert y_type == 'continuous'\n    assert y_true.shape == (3, 1)\n    assert y_pred.shape == (3, 1)\n    assert multioutput == [0.5, 0.5]\n\n    # Test case for continuous-multioutput\n    y_true = [[1, 2], [3, 4]]\n    y_pred = [[5, 6], [7, 8]]\n    y_type, y_true, y_pred, multioutput = _check_reg_targets(y_true, y_pred, None)\n    assert y_type == 'continuous-multioutput'\n    assert y_true.shape == (2, 2)\n    assert y_pred.shape == (2, 2)\n    assert multioutput is None\n\n    # Test case for dtype\n    y_true = np.array([1, 2, 3], dtype=np.int32)\n    y_pred = np.array([4, 5, 6],", "def test_mean_absolute_error_exceptions():\n    y_true = [1, 2, 3]\n    y_pred = [1, 2, 3]\n    \n    # Test that passing in 0 as sample_weight raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, sample_weight=0)\n    \n    # Test that passing in a negative sample_weight raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, sample_weight=-1)\n    \n    # Test that passing in a non-array-like sample_weight raises a TypeError\n    with pytest.raises(TypeError):\n        mean_absolute_error(y_true, y_pred, sample_weight='a')\n    \n    # Test that passing in a sample_weight with wrong shape raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, sample_weight=[1, 2, 3, 4, 5])\n    \n    # Test that passing in a sample_weight with wrong length raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, sample_weight=[1, 2, 3])\n    \n    # Test that passing in a multioutput that is not a string or array-like raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, multioutput='a')\n    \n    # Test that passing in a multioutput that is a string but not in the allowed values raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, multioutput='invalid')\n    \n    # Test that passing in a multioutput that is a string but not in the allowed values raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, multioutput=[1, 2, 3])\n    \n    # Test that passing in a multioutput that is a string but not in the allowed values raises a ValueError\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, multioutput=(1, 2, 3))\n    \n    # Test that passing in a multioutput that is None with n_outputs > 1 raises a ValueError\n    y_true = [[1, 2], [3, 4]]\n    y_pred = [[1, 2], [3, 4]]\n    with pytest.raises(ValueError):\n        mean_absolute_error(y_true, y_pred, multioutput=None)\n", "def test_check_reg_targets_invalid_multioutput():\n    y_true = np.array([[0.5, 1], [1, 2], [7, 6]])\n    y_pred = np.array([[0.5, 2], [1, 2.5], [8, 8]])\n    with pytest.raises(ValueError):\n        _check_reg_targets(y_true, y_pred, multioutput='invalid_string')\n"], "sample_423": ["    def test_deconstructible_default_field(self):\n        change = self.get_changes(\n            before_states=[self.author_name_deconstructible_1],\n            after_states=[self.author_name],\n            questioner=mock.Mock()\n        )\n        self.assertOperationTypes(change, \"testapp\", 0, [\"AddField\"])\n        self.assertOperationAttributes(\n            change, \"testapp\", 0, 0, name=\"name\", field__max_length=200\n        )\n        self.assertOperationAttributes(\n            change, \"testapp\", 0, 0, field__default=None\n        )\n", "    def test_simple_change(self):\n        changes = self.get_changes(\n            [\n                self.author_unmanaged_managed,\n            ],\n            [\n                self.author_with_managers,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(\n            changes, \"testapp\", 0, [(\"__setting__\", \"__setting__.AUTH_USER_MODEL\", None, True)]\n        )\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"AlterModelManagers\"]\n        )\n", "    def test_renamed_indexes_with_index_together(self):\n        # Regression test for #23011\n        from django.db.migrations.operations import AlterIndexTogether\n\n        old_model = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n                (\"publisher\", models.CharField(max_length=200)),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(\n                        fields=[\"name\", \"publisher\"], name=\"old_index_name\"\n                    )\n                ],\n                \"index_together\": [(\"name\", \"publisher\")],\n            },\n        )\n\n        new_model = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=100)),\n                (\"publisher\", models.CharField(max_length=100)),\n            ],\n            {\n                \"indexes\": [\n                    models.Index(\n                        fields=[\"name\", \"publisher\"], name=\"new_index_name\"\n                    )\n                ],\n            },\n        )\n\n        changes = self.get_changes([old_model], [new_model])\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameIndex\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"AddIndex\"])\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0, name=\"old_index_name\", fields=[\"name\", \"publisher\"]\n        )\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 1, 0, name=\"new_index_name\", fields=[\"name\", \"publisher\"]\n        )\n", "    def test_rename_model_to_proxy(self):\n        \"\"\"Check renaming a model to a proxy model.\"\"\"\n        state = self.make_project_state(\n            [\n                self.author,\n                self.author_proxy,\n                self.publisher_with_author,\n            ]\n        )\n        changes = self.get_changes(\n            [state.clone()],\n            [\n                self.author_proxy,\n                self.publisher_with_author_proxy,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertMigrationDependencies(\n            changes, \"testapp\", 0, [(\"testapp\", \"AuthorProxy\")]\n        )\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameModel\"]\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"AuthorProxy\"\n        )\n        self.assertEqual(changes[\"testapp\"][0].name, \"auto_0001_initial\")\n", "    def test_model_manager_altered(self):\n        manager = models.Manager(\"name\")\n        manager2 = models.Manager(\"name\")\n        manager3 = models.Manager(\"newname\")\n\n        before_states = [\n            self.author_empty.clone(),\n            self.author_empty.clone().set_model(\"testapp\", \"AuthorManager\", manager),\n        ]\n        after_states = [\n            self.author_empty.clone(),\n            self.author_empty.clone().set_model(\"testapp\", \"AuthorManager\", manager3),\n        ]\n\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelManagers\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"AuthorManager\", managers=[manager3]\n        )\n"], "sample_580": ["def test_numeric_type_with_numeric_series():\n    data = pd.Series([1, 2, 3, 4])\n    assert variable_type(data) == VarType(\"numeric\")\n", "def test_variable_type_numeric_vector_with_strings():\n    # Test that variable_type correctly identifies numeric vectors with strings\n    data = pd.Series([\"1\", \"2\", \"3\", \"a\", \"b\", \"c\"])\n    assert variable_type(data) == VarType(\"numeric\")\n", "def test_categorical_order_preserves_original_order():\n    vector = pd.Series([\"a\", \"b\", \"c\"])\n    expected_order = [\"a\", \"b\", \"c\"]\n    assert categorical_order(vector) == expected_order\n", "def test_variable_type_with_datetime_series():\n    # Test that variable_type correctly identifies datetime data\n    # when a pandas Series has a datetime64 dtype\n    datetime_series = pd.Series([np.datetime64('2022-01-01'), np.datetime64('2022-01-02')])\n    assert variable_type(datetime_series) == VarType(\"datetime\")\n\n    # Test that variable_type correctly identifies datetime data\n    # when a pandas Series has a datetime dtype and some missing values\n    datetime_series_with_na = pd.Series([np.datetime64('2022-01-01'), np.nan])\n    assert variable_type(datetime_series_with_na) == VarType(\"datetime\")\n\n    # Test that variable_type correctly identifies categorical data\n    # when a pandas Series has a non-datetime numeric dtype and some missing values\n    numeric_series_with_na = pd.Series([1, 2, np.nan])\n    assert variable_type(numeric_series_with_na) == VarType(\"numeric\")\n\n    # Test that variable_type correctly identifies categorical data\n    # when a pandas Series has a non-numeric dtype and some missing values\n    categorical_series_with_na = pd.Series(['a', 'b', np.nan])\n    assert variable_type(categorical_series_with_na) == VarType(\"categorical\")\n\n    # Test that variable_type correctly identifies categorical data\n    # when a pandas Series has a non-numeric dtype and no missing values\n    categorical_series_no_na = pd.Series(['a', 'b'])\n    assert variable_type(categorical_series_no_na) == VarType(\"categorical\")\n\n    # Test that variable_type correctly identifies boolean data\n    # when a pandas Series has a bool dtype and some missing values\n    boolean_series_with_na = pd.Series([True, False, np.nan])\n    assert variable_type(boolean_series_with_na, boolean_type=\"boolean\") == VarType(\"boolean\")\n\n    # Test that variable_type correctly identifies boolean data\n    # when a pandas Series has a non-bool dtype and only boolean values\n    boolean_series_non_bool_dtype = pd.Series([True, False])\n    assert variable_type(boolean_series_non_bool_dtype, boolean_type=\"boolean\") == VarType(\"boolean\")\n", "def test_variable_type_datetime():\n    # Test datetime vectors with different levels of detail\n    datetime_series = pd.Series([np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\")])\n    assert variable_type(datetime_series) == VarType(\"datetime\")\n\n    datetime_series_with_nans = pd.Series([np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\"), np.nan])\n    assert variable_type(datetime_series_with_nans) == VarType(\"numeric\")\n\n    mixed_dtype = pd.Series([np.datetime64(\"2022-01-01\"), \"2022-01-02\", np.datetime64(\"2022-01-03\")])\n    assert variable_type(mixed_dtype) == VarType(\"categorical\")\n\n    # Test datetime vectors with different input types\n    numpy_array = np.array([np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\")])\n    assert variable_type(numpy_array) == VarType(\"datetime\")\n\n    list_input = [np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\")]\n    assert variable_type(list_input) == VarType(\"datetime\")\n"], "sample_1016": ["def test_print_Piecewise_error():\n    from sympy import Piecewise, sin, symbols\n    x = symbols('x')\n    pw = Piecewise((x + 1, x > 0))\n    octave_code(pw, assign_to='tau')\n", "def test_octave_code_matrix_operations():\n    A = Matrix([[x, y], [z, x]])\n    B = Matrix([[1, 0], [0, 1]])\n    expr = A*B\n    expected = 'A = [x y; z x]; B = [1 0; 0 1]; A*B = [x 0; 0 x];'\n    assert octave_code(expr, assign_to=(\"A\", \"B\")) == expected\n\n    expr = A**2\n    expected = 'A = [x y; z x]; A^2 = [x^2 + z*y  2*x*y; 2*z*x  x^2 + z*y];'\n    assert octave_code(expr, assign_to=\"A\") == expected\n\n    expr = A.inv()\n    expected = 'A = [x y; z x]; A_inv = 1/(x^2 + z*y)*[x  -y; -z  x];'\n    assert octave_code(expr, assign_to=\"A\") == expected\n\n    expr = A.T\n    expected = 'A = [x y; z x]; A_T = [x  z; y  x];'\n    assert octave_code(expr, assign_to=\"A\") == expected\n", "def test_octave_code_floor():\n    from sympy import floor\n    assert octave_code(floor(x)) == \"floor(x)\"\n\n", "def test_OctaveCodePrinter_assign_to_MatrixSymbol():\n    from sympy.core import MatrixSymbol\n    from sympy.codegen import octave_code\n    A = MatrixSymbol('A', 2, 2)\n    x, y = symbols('x, y')\n    expr = A * x + y\n    code = octave_code(expr, assign_to=A)\n    assert code == \"A = [x.*A.*y.*ones(1, 2);y.*ones(1, 2)];\"\n\n", "def test_octave_code_contract_assignment():\n    from sympy import IndexedBase, Idx\n    i, j = Idx('i', 3), Idx('j', 3)\n    k = IndexedBase('k')\n    expr = k[i, j]\n    settings = {'contract': True}\n    code = octave_code(expr, assign_to=k, settings=settings)\n    expected = \"k(i, j) = 0;\\nk(i, j) = k(i, j) + 1;\\nend\"\n    assert code == expected\n"], "sample_548": ["def test_colorbar_extension_shape_and_length(spacing, figs):\n    fig = figs(spacing)\n    assert fig is not None\n    for i in range(12):\n        ax = fig.axes[i]\n        assert ax.get_visible() is True\n        assert ax.get_visible() is True\n        assert ax.get_xticks() is not None\n        assert ax.get_xticklabels() is not None\n        if i == 0:\n            assert ax.get_yticks() is None\n        else:\n            assert ax.get_yticks() is not None\n            assert ax.get_yticklabels() is not None\n    plt.close(fig)\n\n", "def test_colorbar_ticklocation_auto():\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=4)\n    # Create a subplot.\n    ax = fig.add_subplot(1, 1, 1)\n    # Create a colorbar.\n    Colorbar(ax, cmap='viridis', norm=Normalize(vmin=0, vmax=1),\n             orientation='horizontal')\n    # Set the ticklocation to 'auto'.\n    ax.set_xticklabels([])\n    cbar = ax.colorbar\n    cbar.set_ticks_position('auto')\n    # Check that the ticklocation matches the orientation.\n    assert cbar.ticklocation == 'bottom'\n    # Check that the ticklocation is 'bottom' for a horizontal colorbar.\n    assert cbar.ax.get_xlabel() == ''\n    # Check that the ticklocation is 'left' for a vertical colorbar.\n    ax.set_position([0.1, 0.1, 0.8, 0.8])\n    cbar.ax.set_position([0.1, 0.1, 0.8, 0.8])\n    cbar.set_ticks_position('auto')\n    assert cbar.ticklocation == 'left'\n", "def test_colorbar_make_axes_shrink():\n    \"\"\"\n    Check that make_axes shrinks the parent axes when shrink=1.\n    \"\"\"\n    fig, ax = plt.subplots()\n    cax, kwargs = Colorbar.make_axes(ax, shrink=1.)\n    assert ax.get_size()[0] == 1  # ax is shrunk\n    cax.remove()\n    ax.remove()\n    fig.delaxes(cax)\n    fig.delaxes(ax)\n\n    # test make_axes_gridspec\n    fig, ax = plt.subplots()\n    cax, kwargs = Colorbar.make_axes_gridspec(ax, shrink=1.)\n    assert ax.get_size()[0] == 1  # ax is shrunk\n    cax.remove()\n    ax.remove()\n    fig.delaxes(cax)\n    fig.delaxes(ax)\n", "def test_colorbar_ticklocs():\n    # Create a figure and a set of subplots.\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    # Create a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Get a set of values that are close to the colorbar boundaries.\n    boundaries = values = norms['neither'].boundaries\n    values = values[:-1]\n    # Generate 3 colorbars with different tick locations.\n    for i, orientation in enumerate(('vertical', 'horizontal', 'vertical')):\n        # Generate the colorbar.\n        Colorbar(axs[i], cmap=cmap, norm=norms['neither'],\n                 boundaries=boundaries, values=values,\n                 extend='neither', orientation=orientation)\n        # Set the tick locations and labels.\n        axs[i].tick_params(axis='both', labelbottom=False,\n                          labelleft=False, bottom=False, left=False)\n        ticks = [0, 1, 2, 3]\n        axs[i].set_xticks(ticks)\n        axs[i].set_yticks(ticks)\n        # Set the tick labels to match the tick locations.\n        axs[i].set_xticklabels(['a', 'b', 'c', 'd'])\n        axs[i].set_yticklabels(['a', 'b', 'c', 'd'])\n    # Show the figure to the caller.\n    plt.show()\n", "def test_colorbar_tick_labels():\n    \"\"\"\n    Test the tick labels of a colorbar.\n\n    Verify that tick labels can be set for a colorbar and that they are\n    properly positioned.\n    \"\"\"\n    # Create a figure and set its size\n    fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n\n    # Create a colormap\n    cmap = cm.get_cmap('RdYlGn')\n    norm = Normalize(vmin=-10, vmax=10)\n    cax = fig.add_subplot(221)\n    cbar = Colorbar(cax, cmap=cmap, norm=norm, orientation='vertical')\n    cbar.set_label('Test Colorbar')\n\n    # Set tick labels\n    tick_locations = [-10, -5, 0, 5, 10]\n    cbar.set_ticks(tick_locations)\n    tick_labels = ['Tick 1', 'Tick 2', 'Tick 3', 'Tick 4', 'Tick 5']\n    cbar.set_ticklabels(tick_labels)\n\n    # Check the tick labels\n    cax.set_yticks([])\n    cax.set_xticks([])\n    cax.set_xlabel('')\n    cax.set_ylabel('')\n    cax.set_title('Colorbar Tick Labels')\n\n    # Create another colorbar with minor ticks\n    cax = fig.add_subplot(222)\n    cbar = Colorbar(cax, cmap=cmap, norm=norm, orientation='vertical')\n    cbar.minorticks_on()\n\n    # Set minor tick labels\n    minor_tick_locations = [-9, -4, 1, 6]\n    minor_tick_labels = ['Minor Tick 1', 'Minor Tick 2', 'Minor Tick 3', 'Minor Tick 4']\n    cbar.set_ticks(minor_tick_locations, minor=True)\n    cbar.set_ticklabels(minor_tick_labels, minor=True)\n\n    # Check the minor tick labels\n    cax.set_yticks([])\n    cax.set_xticks([])\n    cax.set_xlabel('')\n    cax.set_ylabel('')\n    cax.set_title('Colorbar Minor Tick Labels')\n\n    # Create another colorbar with major ticks\n    cax = fig.add_subplot(223)\n    cbar = Colorbar(cax, cmap=cmap, norm=norm, orientation='vertical')\n    cbar.set_ticks(tick_locations)\n\n    # Set major tick labels\n    cbar.set_ticklabels(t"], "sample_1015": ["def test_C89CodePrinter_Piecewise_without_default_term():\n    from sympy import Piecewise\n    expr = Piecewise((x, x > 0))\n    C89CodePrinter(settings={'human': False}).doprint(expr)\n", "def test_C89CodePrinter():\n    from sympy import sin, cos, tan, asin, acos, atan, atan2, exp, log, sqrt\n    from sympy.codegen import CCodePrinter\n    expr = sin(x)\n    assert ccode(expr) == 'sin(x)'\n    expr = cos(x)\n    assert ccode(expr) == 'cos(x)'\n    expr = tan(x)\n    assert ccode(expr) == 'tan(x)'\n    expr = asin(x)\n    assert ccode(expr) == 'asin(x)'\n    expr = acos(x)\n    assert ccode(expr) == 'acos(x)'\n    expr = atan(x)\n    assert ccode(expr) == 'atan(x)'\n    expr = atan2(x, y)\n    assert ccode(expr) == 'atan2(x, y)'\n    expr = exp(x)\n    assert ccode(expr) == 'exp(x)'\n    expr = log(x)\n    assert ccode(expr) == 'log(x)'\n    expr = sqrt(x)\n    assert ccode(expr) == 'sqrt(x)'\n", "def test_CCodePrinter_assign_to_MatrixSymbol():\n    from sympy import Matrix\n    from sympy.codegen.ast import real, float32, float64, float80, float128, intc, real128\n    from sympy.codegen.cfunctions import Sqrt, Cbrt\n    from sympy.printing import ccode\n    from sympy.printing import CCodePrinter, C89CodePrinter, C99CodePrinter\n\n    M = MatrixSymbol('M', 3, 3)\n    x = symbols('x')\n    expr = Matrix([[x, x**2, x**3], [x**3, x**4, x**5], [x**5, x**6, x**7]])\n\n    # Test that assign_to = MatrixSymbol works correctly\n    expected_c_code = \"\"\"", "def test_c_code_printers():\n    # Test C89CodePrinter\n    printer = C89CodePrinter()\n    assert printer.language == \"C\"\n    assert printer.standard == \"C89\"\n    assert printer._ns == ''\n    assert printer._kf == known_functions_C89\n\n    # Test C99CodePrinter\n    printer = C99CodePrinter()\n    assert printer.language == \"C\"\n    assert printer.standard == \"C99\"\n    assert printer._ns == ''\n    assert printer._kf == known_functions_C99\n    assert printer.reserved_words == {'auto', 'break', 'case', 'char', 'const', 'continue', 'default', 'do',\n                                    'double', 'else', 'enum', 'extern', 'float', 'for', 'goto', 'if', 'int',\n                                    'long', 'register', 'return', 'short', 'signed', 'sizeof', 'static',\n                                    'struct', 'switch', 'typedef', 'union', 'unsigned', 'void', 'volatile', 'while',\n                                    'inline', 'restrict'}\n    assert printer._get_loop_opening_ending(('i', 'j')) == ([], [])\n\n    # Test _C9XCodePrinter\n    class TestPrinter(_C9XCodePrinter):\n            return 'open', 'close'\n    printer = TestPrinter()\n    assert printer._get_loop_opening_ending(('i', 'j')) == ('open', 'close')\n\n    # Test ccode function\n    assert ccode(2*x) == '2*x'\n    assert ccode(x + y) == 'x + y'\n    assert ccode(pi) == 'M_PIl'\n    assert ccode(exp(1)) == 'exp(1)'\n    assert ccode(2**x) == 'pow(2, x)'\n    assert ccode(2.5) == '2.5'\n    assert ccode(123) == '123'\n    assert ccode(Integer(123)) == '123'\n    assert ccode(nan) == 'nan'\n    assert ccode(oo) == 'HUGE_VAL'\n    assert ccode(-oo) == '-HUGE_VAL'\n    assert ccode(pi.as_real_imag()) == '(M_PIl, 0)'\n    assert ccode(x.as_real_imag()) == '(x, 0)'\n\n    # Test", "def test_dereference_variable():\n    from sympy.core import symbols\n    from sympy.codegen.ast import Assignment, Pointer, Variable, real\n\n    a = symbols('a')\n    var = Pointer(Variable(a, real))\n    assign = Assignment(var, var.symbol)\n    code = ccode(assign, dereference=[a])\n    assert code == '*a = *a'\n"], "sample_251": ["    def setUpTestData(cls):\n        cls.b1 = Book.objects.create(isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n                                    pages=447, rating=4.5, price=Decimal('30.00'), contact=cls.a1, publisher=cls.p1,\n                                    pubdate=datetime.date(2007, 12, 6))\n        cls.b2 = Book.objects.create(isbn='067232959', name='Sams Teach Yourself Django in 24 Hours',\n                                    pages=528, rating=3.0, price=Decimal('23.09'), contact=cls.a3, publisher=cls.p2,\n                                    pubdate=datetime.date(2008, 3, 3))\n        cls.b3 = Book.objects.create(isbn='159059996', name='Practical Django Projects',\n                                    pages=300, rating=4.0, price=Decimal('29.69'), contact=cls.a4, publisher=cls.p1,\n                                    pubdate=datetime.date(2008, 6, 23))\n        cls.b4 = Book.objects.create(isbn='013235613', name='Python Web Development with Django',\n                                    pages=350, rating=4.0, price=Decimal('29.69'), contact=cls.a5, publisher=cls.p3,\n                                    pubdate=datetime.date(2008, 11, 3))\n        cls.b5 = Book.objects.create(isbn='013790395', name='Artificial Intelligence: A Modern Approach',\n                                    pages=1132, rating=4.0, price=Decimal('82.80'), contact=cls.a8, publisher=cls.p3,\n                                    pubdate=datetime.date(1995, 1, 15))\n", "    def test_combined_expression_output_field(self):\n        from .models import Publisher\n        expression = CombinedExpression(\n            Value(1),\n            CombinedExpression.Sub,\n            Value(2),\n            output_field=IntegerField()\n        )\n        self.assertEqual(expression.output_field, IntegerField())\n        self.assertEqual(expression.output_field.get_internal_type(), 'IntegerField')\n        expression = CombinedExpression(\n            F('name'),\n            CombinedExpression.ADD,\n            F('num_awards'),\n            output_field=IntegerField()\n        )\n        self.assertEqual(expression.output_field, IntegerField())\n        self.assertEqual(expression.output_field.get_internal_type(), 'IntegerField')\n        expression = CombinedExpression(\n            F('name'),\n            CombinedExpression.DIV,\n            F('num_awards'),\n            output_field=IntegerField()\n        )\n        self.assertEqual(expression.output_field, FloatField())\n        self.assertEqual(expression.output_field.get_internal_type(), 'FloatField')\n", "    def setUpTestData(cls):\n        cls.e1 = Employee.objects.create(name='Alice', age=29, salary=50000)\n        cls.e2 = Employee.objects.create(name='Bob', age=35, salary=60000)\n        cls.e3 = Employee.objects.create(name='Charlie', age=42, salary=70000)\n        cls.e4 = Employee.objects.create(name='Dave', age=28, salary=40000)\n        cls.e5 = Employee.objects.create(name='Eve', age=39, salary=80000)\n", "    def setUpTestData(cls):\n        cls.e1 = Employee.objects.create(name='John', salary=10000, department=cls.d1)\n        cls.e2 = Employee.objects.create(name='Alice', salary=20000, department=cls.d1)\n        cls.e3 = Employee.objects.create(name='Bob', salary=30000, department=cls.d2)\n        cls.d1 = Department.objects.create(name='Sales')\n        cls.d2 = Department.objects.create(name='Engineering')\n        cls.d1.employee_set.add(cls.e1, cls.e2)\n        cls.d2.employee_set.add(cls.e3)\n", "    def setUpTestData(cls):\n        cls.t1 = Ticket.objects.create(\n            arrival=datetime.time(12, 30, 0),\n            arrival_hour=12,\n            arrival_minute=30,\n            arrival_second=0,\n            departure=datetime.time(13, 30, 0),\n            departure_hour=13,\n            departure_minute=30,\n            departure_second=0,\n            hours_diff=1,\n        )\n        cls.t2 = Ticket.objects.create(\n            arrival=datetime.time(12, 30, 0),\n            arrival_hour=12,\n            arrival_minute=30,\n            arrival_second=0,\n            departure=datetime.time(14, 30, 0),\n            departure_hour=14,\n            departure_minute=30,\n            departure_second=0,\n            hours_diff=2,\n        )\n        cls.t3 = Ticket.objects.create(\n            arrival=datetime.time(12, 30, 0),\n            arrival_hour=12,\n            arrival_minute=30,\n            arrival_second=0,\n            departure=datetime.time(15, 30, 0),\n            departure_hour=15,\n            departure_minute=30,\n            departure_second=0,\n            hours_diff=3,\n        )\n        cls.store = Store.objects.create(\n            name='Test store',\n            original_opening=datetime.datetime(2000, 1, 1, 9, 0, 0),\n            friday_night_closing=datetime.time(21, 0, 0),\n        )\n"], "sample_373": ["    def test_bookmarklets_view(self):\n        response = self.client.get(reverse('admin:admin_doc:bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "    def test_bookmarklets_view_empty(self):\n        response = self.client.get(reverse('admin:index'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "    def test_bookmarklets_view(self):\n        response = self.client.get(reverse('admin:admindocs_bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/bookmarklets.html')\n", "    def test_template_detail_view_context(self):\n        template_name = 'template.html'\n        response = self.client.get(reverse('admin:admindocs:template_detail', kwargs={'template': template_name}))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(template_name, response.context_data['name'])\n", "    def test_model_detail_view_with_no_documentation(self):\n        # Arrange\n        self.create_company()\n        self.create_person()\n        self.client.force_login(self.superuser)\n\n        # Act\n        response = self.client.get(reverse('admin:admin_doc:model_detail', args=('person',)))\n\n        # Assert\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/model_detail.html')\n"], "sample_922": ["def test_parse_arglist():\n    # Test simple function call\n    signode = desc_signature(sig='func(x, y)')\n    assert parse(signode.astext()) == 'func(x, y)'\n\n    # Test function with default value\n    signode = desc_signature(sig='func(x, y, z=3)')\n    assert parse(signode.astext()) == 'func(x, y, z=3)'\n\n    # Test function with type hint\n    signode = desc_signature(sig='func(x: int, y: str)')\n    assert parse(signode.astext()) == 'func(x: int, y: str)'\n\n    # Test function with return type hint\n    signode = desc_signature(sig='func(x: int) -> str')\n    assert parse(signode.astext()) == 'func(x: int) -> str'\n\n    # Test function with default value and type hint\n    signode = desc_signature(sig='func(x: int, y: str, z=3)')\n    assert parse(signode.astext()) == 'func(x: int, y: str, z=3)'\n\n    # Test function with return type hint and default value\n    signode = desc_signature(sig='func(x: int) -> str = 3')\n    assert parse(signode.astext()) == 'func(x: int) -> str = 3'\n\n    # Test function with default value, type hint and return type hint\n    signode = desc_signature(sig='func(x: int, y: str, z=3) -> str')\n    assert parse(signode.astext()) == 'func(x: int, y: str, z=3) -> str'\n\n    # Test function with variable number of arguments\n    signode = desc_signature(sig='func(*x)')\n    assert parse(signode.astext()) == 'func(*x)'\n\n    # Test function with variable keyword arguments\n    signode = desc_signature(sig='func(**x)')\n    assert parse(signode.astext()) == 'func(**x)'\n\n    # Test function with default value and variable number of arguments\n    signode = desc_signature(sig='func(x, *y, z=3)')\n    assert parse(signode.astext()) == 'func(x, *y, z=3)'\n\n    # Test function with default value and variable keyword arguments\n    signode = desc_signature(sig='func(x, **y, z=3)')\n    assert parse(signode.ast", "def test_parse_annotation():\n    # Test a simple type annotation\n    annotation = \"Union[str, int]\"\n    expected_output = \"[Union[str, int]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a base class\n    annotation = \"int if x > 0 else str\"\n    expected_output = \"[int if x > 0 else str]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a generic type\n    annotation = \"List[int]\"\n    expected_output = \"[List[int]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a generic type and a base class\n    annotation = \"List[int if x > 0 else str]\"\n    expected_output = \"[List[int if x > 0 else str]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a tuple\n    annotation = \"Tuple[str, int]\"\n    expected_output = \"[Tuple[str, int]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a list of types\n    annotation = \"Union[str, int, List[float]]\"\n    expected_output = \"[Union[str, int, List[float]]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a generic type and a list of types\n    annotation = \"List[Union[str, int, float]]\"\n    expected_output = \"[List[Union[str, int, float]]]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a conditional expression\n    annotation = \"int if x > 0 else str\"\n    expected_output = \"[int if x > 0 else str]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a lambda function\n    annotation = \"lambda x: x * 2\"\n    expected_output = \"[lambda x: x * 2]\"\n    assert _parse_annotation(annotation) == expected_output\n\n    # Test a type annotation with a complex type\n    annotation = \"Union[str, int, List[Dict[str, int]]]\"\n    expected_output = \"[Union[str, int, List[Dict[str, int]]]]\"\n    assert _parse_annotation(annotation) == expected_output\n", "def test__pseudo_parse_arglist():\n    # Test empty arglist\n    signode = addnodes.desc_signature('function()', '')\n    assert _pseudo_parse_arglist(signode, '') == signode\n\n    # Test function with no arguments\n    signode = addnodes.desc_signature('function()')\n    signode += addnodes.desc_parameterlist()\n    assert _pseudo_parse_arglist(signode, '') == signode\n\n    # Test function with a single required argument\n    signode = addnodes.desc_signature('function(x)')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, 'x')\n    assert len(signode) == 4\n\n    # Test function with multiple required arguments\n    signode = addnodes.desc_signature('function(x, y)')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, 'x, y')\n    assert len(signode) == 4\n\n    # Test function with a variable-length argument\n    signode = addnodes.desc_signature('function(*args)')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, '*args')\n    assert len(signode) == 4\n\n    # Test function with a keyword-only argument\n    signode = addnodes.desc_signature('function(*, kwarg)')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, '*kwarg')\n    assert len(signode) == 4\n\n    # Test function with a positional-only argument\n    signode = addnodes.desc_signature('function(x=3)')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, 'x=3')\n    assert len(signode) == 4\n\n    # Test function with optional arguments\n    signode = addnodes.desc_signature('function(x=[1, 2], y=[3, 4])')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, 'x=[1, 2], y=[3, 4]')\n    assert len(signode) == 4\n\n    # Test function with a return type annotation\n    signode = addnodes.desc_signature('function(x: int) -> str')\n    signode += addnodes.desc_parameterlist()\n    _pseudo_parse_arglist(signode, 'x: int) -> str')\n    assert", "def test_object_description_transform():\n    # Test that filter_meta_fields filters out meta fields from the docstring\n    app = Mock()\n    env = Mock()\n    content = [nodes.field_list([nodes.field('', nodes.paragraph('Meta field'))])]\n    filter_meta_fields(app, 'py', 'obj', content)\n    assert len(content) == 0\n", "def test_parse_annotation():\n    sig_re = py_sig_re\n\n    # Test normal case\n    assert parse('int') == 'int'\n\n    # Test class name\n    assert parse('list') == 'list'\n\n    # Test method\n    assert parse('my_method(x, y: int) -> str') == 'my_method(x, y: int) -> str'\n\n    # Test class method\n    assert parse('MyClass.my_method(x, y: int) -> str') == 'MyClass.my_method(x, y: int) -> str'\n\n    # Test static method\n    assert parse('MyClass.my_static_method(x, y: int) -> str') == 'MyClass.my_static_method(x, y: int) -> str'\n\n    # Test property\n    assert parse('MyClass.my_property(x, y: int) -> str') == 'MyClass.my_property(x, y: int) -> str'\n\n    # Test module name\n    assert parse('module_name') == 'module_name'\n\n    # Test class annotation\n    assert parse('int') == 'int'\n\n    # Test class annotation with multiple inheritance\n    assert parse('list[int, str]') == 'list[int, str]'\n\n    # Test default value\n    assert parse('my_method(x: int = 5)') == 'my_method(x: int = 5)'\n\n    # Test return type annotation\n    assert parse('my_method(x: int) -> str') == 'my_method(x: int) -> str'\n\n    # Test multiple return type annotations\n    assert parse('my_method(x: int) -> (str, int)') == 'my_method(x: int) -> (str, int)'\n\n    # Test unsupported syntax\n    with pytest.raises(SyntaxError):\n        parse('my_method(x: int -> str)')\n\n    # Test invalid syntax\n    with pytest.raises(SyntaxError):\n        parse('my_method(x, y, : int, str)')\n\n    # Test unsupported syntax\n    with pytest.raises(SyntaxError):\n        parse('my_method(x, y: int, str: str)')\n"], "sample_172": ["    def test_inline_model_admin_media(self):\n        \"\"\"\n        Check that InlineModelAdmin.media returns the correct Media instance\n        \"\"\"\n        inline = admin.StackedInline(model=Car, parent_model=Company)\n        self.assertIsInstance(inline.media, forms.Media)\n", "    def test_inline_model_admin_formset(self):\n        # Create an inline admin instance\n        inline_admin = admin.StackedInline(\n            model=Member,\n            fk_name='group',\n            formset=admin.BaseInlineFormSet,\n            extra=2,\n            min_num=1,\n            max_num=3,\n            can_delete=True,\n            show_change_link=True,\n        )\n\n        # Test the get_formset method\n        formset_class = inline_admin.get_formset(request=None, obj=None, fields=['name', 'age'])\n        self.assertIsInstance(formset_class, admin.BaseInlineFormSet)\n\n        # Test the get_extra method\n        extra = inline_admin.get_extra(request=None, obj=None)\n        self.assertEqual(extra, 2)\n\n        # Test the get_min_num method\n        min_num = inline_admin.get_min_num(request=None, obj=None)\n        self.assertIsNone(min_num)\n\n        # Test the get_max_num method\n        max_num = inline_admin.get_max_num(request=None, obj=None)\n        self.assertEqual(max_num, 3)\n\n        # Test the get_queryset method\n        queryset = inline_admin.get_queryset(request=None)\n        self.assertEqual(queryset.model, Member)\n        self.assertEqual(queryset.model, inline_admin.model)\n\n        # Test the has_add_permission method\n        has_add_permission = inline_admin.has_add_permission(request=None, obj=None)\n        self.assertTrue(has_add_permission)\n\n        # Test the has_change_permission method\n        has_change_permission = inline_admin.has_change_permission(request=None, obj=None)\n        self.assertTrue(has_change_permission)\n\n        # Test the has_delete_permission method\n        has_delete_permission = inline_admin.has_delete_permission(request=None, obj=None)\n        self.assertTrue(has_delete_permission)\n\n        # Test the has_view_permission method\n        has_view_permission = inline_admin.has_view_permission(request=None, obj=None)\n        self.assertTrue(has_view_permission)\n", "    def test_get_inline_instances(self):\n        admin = admin.site._registry[Car]\n        inline = admin.inlines[0]\n        inline_instances = inline.get_inline_instances(request=self.client.request(), obj=None)\n        self.assertEqual(inline_instances[0].model, Member)\n        self.assertEqual(inline_instances[0].fk_name, 'car')\n", "    def test_inline_formset_factory_with_custom_form(self):\n        class CustomInlineFormset(BaseInlineFormSet):\n                return self.cleaned_data\n\n        class MyInline(admin.StackedInline):\n            formset = CustomInlineFormset\n\n        class MyModelAdmin(admin.ModelAdmin):\n            inlines = [MyInline]\n\n        my_model = MyModel\n        request = self.get_request()\n        formset = MyInline.get_formset(request, obj=my_model, fields=None)\n        self.assertIsInstance(formset, CustomInlineFormset)\n", "    def testManyToManyInlineInlineWidgets(self):\n        ff = self.assertFormfield(Individual, 'members', widgets.FilteredSelectMultiple, filter_horizontal=['members'])\n        self.assertIsInstance(ff.widget, widgets.FilteredSelectMultiple)\n\n        ff = self.assertFormfield(Band, 'members', widgets.FilteredSelectMultiple, filter_vertical=['members'])\n        self.assertIsInstance(ff.widget, widgets.FilteredSelectMultiple)\n\n        ff = self.assertFormfield(Band, 'influences', widgets.FilteredSelectMultiple, filter_vertical=['influences'])\n        self.assertIsInstance(ff.widget, widgets.FilteredSelectMultiple)\n\n        ff = self.assertFormfield(Band, 'influences', widgets.FilteredSelectMultiple, filter_horizontal=['influences'])\n        self.assertIsInstance(ff.widget, widgets.FilteredSelectMultiple)\n"], "sample_517": ["def test_offset_from():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n\n    # test OffsetFrom instance\n    patch = mpatches.Ellipse((0, 0), 2, 2)\n    ax.add_patch(patch)\n\n    offset_from = mtransforms.OffsetFrom(patch, (0.5, 0.5), unit='points')\n    assert offset_from.get_unit() == 'points'\n\n    # test OffsetFrom instance with renderer\n    renderer = fig.canvas.get_renderer()\n    assert offset_from(renderer).get_matrix().a == 1\n    assert offset_from(renderer).get_matrix().d == 1\n\n    # test OffsetFrom instance with Transform\n    tr = mtransforms.Affine2D().scale(2).translate(1, 1)\n    offset_from = mtransforms.OffsetFrom(tr, (0.5, 0.5), unit='points')\n    assert offset_from(renderer).get_matrix().a == 2\n    assert offset_from(renderer).get_matrix().d == 2\n\n    # test OffsetFrom instance with Artist\n    text = Text(0.5, 0.5, 'text')\n    ax.add_artist(text)\n    offset_from = mtransforms.OffsetFrom(text, (0.5, 0.5), unit='points')\n    assert offset_from(renderer).get_matrix().a == 1\n    assert offset_from(renderer).get_matrix().d == 1\n", "    def test_font_weight_setter(self):\n        # Test the setter of the font_weight attribute.\n        text = Text(x=0.5, y=0.5, text='test')\n        text.set_fontweight('normal')\n        assert text.get_fontweight() == 'normal'\n        text.set_fontweight(700)\n        assert text.get_fontweight() == 'bold'\n", "def test_text_rotation_usetex():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    ax.set_aspect('equal')\n\n    # Create a figure with usetex=True\n    font = FontProperties(family='serif', style='italic')\n    text = Text(-0.5, 0.5, 'Hello, $\\\\mathbf{\\\\it{\\\\text{world}}$',\n                horizontalalignment='center', verticalalignment='center',\n                transform=mtransforms.IdentityTransform(),\n                rotation=45, fontproperties=font,\n                usetex=True, rotation_mode='anchor')\n\n    # Plot the text\n    text.draw(ax.figure.canvas.get_renderer())\n\n    # Save the figure\n    plt.savefig('font_styles.png')\n", "def test_get_rotation():\n    # Test rotation normalization\n    assert get_rotation(0) == 0\n    assert get_rotation(360) == 0\n    assert get_rotation(-360) == 0\n    assert get_rotation(45) == 45\n    assert get_rotation(315) == 45\n    assert get_rotation(180) == 180\n    assert get_rotation(-180) == 180\n\n    # Test invalid inputs\n    with pytest.raises(ValueError):\n        get_rotation('test')\n    with pytest.raises(ValueError):\n        get_rotation('horizontal')  # already covered in another test\n    with pytest.raises(ValueError):\n        get_rotation('vertical')    # already covered in another test\n    with pytest.raises(ValueError):\n        get_rotation(None)  # already covered in another test\n    with pytest.raises(ValueError):\n        get_rotation('vertical')\n    with pytest.raises(ValueError):\n        get_rotation(400)\n\n    # Test deprecated get_rotation\n    with warnings.catch_warnings():\n        warnings.simplefilter('error')\n        with pytest.raises(UserWarning):\n            get_rotation(45, deprecation_version=\"4.0\")  # should raise a deprecation warning\n", "def test_get_window_extent_non_integer_rotation():\n    fig, ax = plt.subplots()\n\n    # Create a Text instance with a non-integer rotation\n    text = Text(0.5, 0.5, 'Hello, World', rotation=45)\n\n    # Draw the figure to get the renderer\n    fig.canvas.draw()\n\n    # Get the window extent\n    extent = text.get_window_extent(fig.canvas.renderer)\n\n    # Check that the extent is correct\n    assert_almost_equal(extent.x0, 0.021)\n    assert_almost_equal(extent.x1, 0.535)\n    assert_almost_equal(extent.y0, 0.023)\n    assert_almost_equal(extent.y1, 0.407)\n"], "sample_868": ["def test_symmetric_metrics_order_invariance():\n    for name in SYMMETRIC_METRICS:\n        metric = SUPERVISED_METRICS[name]\n        for _ in range(10):\n            y_true = rng.randint(3, size=30)\n            y_pred = rng.randint(3, size=30)\n            score = metric(y_true, y_pred)\n            score2 = metric(y_pred, y_true)\n            assert_allclose(score, score2)\n", "def test_perfectly_matching_labeling():\n    y1 = np.array([0, 0, 1, 1])\n    y2 = np.array([0, 0, 1, 1])\n    for metric in SUPERVISED_METRICS.values():\n        assert_allclose(metric(y1, y2), 1.0, atol=1e-8)\n", "def test_mutual_info_score_symmetry():\n    # Test mutual_info_score is symmetric\n    for metric in ['mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score']:\n        metric_func = SUPERVISED_METRICS[metric]\n        score1 = metric_func(y1, y2)\n        score2 = metric_func(y2, y1)\n        assert_allclose(score1, score2, atol=1e-6)\n", "def test_mutual_info_score_contingency_matrix_array():\n    contingency = np.array([[2, 4, 2], [5, 3, 1], [0, 0, 1]])\n    assert_allclose(mutual_info_score(None, None, contingency=contingency), 1.0)\n", "def test_entropy():\n    # Test entropy function\n    labels = np.array([1, 2, 1, 2, 1, 1])\n    assert_allclose(entropy(labels), 1.584963389979008)\n\n    # Test entropy function with no samples\n    labels = np.array([])\n    assert_allclose(entropy(labels), 1.0)\n\n    # Test entropy function with all unique labels\n    labels = np.array([0, 1, 2])\n    assert_allclose(entropy(labels), 1.0)\n\n    # Test entropy function with zero labels\n    labels = np.array([0])\n    assert_allclose(entropy(labels), 0.0)\n"], "sample_453": ["    def test_prepopulated_fields_js(self):\n        article = Article.objects.create(title=\"Title\", slug=\"slug\")\n        request = self.request_factory.get(\n            reverse(\"admin:article_article_change\", args=(article.id,))\n        )\n        request.user = self.user\n        request.META[\"HTTP_REFERER\"] = reverse(\"admin:article_article_changelist\")\n        admin = ArticleAdmin(Article, site)\n        context = admin.changeform_view(request, article.id, extra_context={\"inline_admin_formsets\": []})\n        context = prepopulated_fields_js(context)\n        self.assertIn(\"prepopulated_fields\", context)\n        self.assertIn(\"prepopulated_fields_json\", context)\n        self.assertIsInstance(context[\"prepopulated_fields\"], list)\n        self.assertIsInstance(context[\"prepopulated_fields_json\"], str)\n", "    def test_cell_count(self):\n        user = User.objects.create_superuser('test', 'test@example.com', 'password')\n        self.client.force_login(user)\n        self.client.get(reverse('admin:auth_user_changelist'))\n        request = self.request_factory.get(reverse('admin:auth_user_changelist'))\n        inline_admin_formset = ArticleAdmin(User, self.site).get_inline_instances(request)\n        self.assertEqual(cell_count(inline_admin_formset[0]), 3)\n", "    def test_prepopulated_fields_js(self):\n        # Setup\n        article = Article.objects.create(title=\"Test Title\", body=\"Test Body\")\n        article_id = article.id\n        question = Question.objects.create(question=\"Test Question\", article=article)\n\n        # Request\n        request = self.request_factory.get(reverse(\"admin:article_article_changelist\"))\n\n        # Test\n        context = prepopulated_fields_js(request.context)\n        prepopulated_fields = context[\"prepopulated_fields\"]\n        self.assertEqual(len(prepopulated_fields), 0)\n\n        # Add prepopulated fields\n        article.prepopulated_fields = [(\"title\", [\"question\"])\n                                     (\"body\", [\"question\"])]\n        article.save()\n\n        # Request\n        request = self.request_factory.get(reverse(\"admin:article_article_change\"), args=[article_id])\n\n        # Test\n        context = prepopulated_fields_js(request.context)\n        prepopulated_fields = context[\"prepopulated_fields\"]\n        self.assertEqual(len(prepopulated_fields), 2)\n        self.assertEqual(prepopulated_fields[0][\"id\"], \"#id_title\")\n        self.assertEqual(prepopulated_fields[0][\"name\"], \"title\")\n        self.assertEqual(prepopulated_fields[0][\"dependency_ids\"], [\"#id_question\"])\n        self.assertEqual(prepopulated_fields[0][\"dependency_list\"], [\"question\"])\n        self.assertEqual(prepopulated_fields[0][\"maxLength\"], 50)\n        self.assertEqual(prepopulated_fields[0][\"allowUnicode\"], False)\n\n        self.assertEqual(prepopulated_fields[1][\"id\"], \"#id_body\")\n        self.assertEqual(prepopulated_fields[1][\"name\"], \"body\")\n        self.assertEqual(prepopulated_fields[1][\"dependency_ids\"], [\"#id_question\"])\n        self.assertEqual(prepopulated_fields[1][\"dependency_list\"], [\"question\"])\n        self.assertEqual(prepopulated_fields[1][\"maxLength\"], 50)\n        self.assertEqual(prepopulated_fields[1][\"allowUnicode\"], False)\n", "    def test_prepopulated_fields_js_tag(self):\n        request = self.request_factory.get(reverse('admin:article_article_changelist'))\n        response = self.client.get(reverse('admin:article_article_changelist'), data={'prepopulated_fields': [{'field': 'field1', 'dependencies': ['dependency1', 'dependency2']}]})\n        self.assertContains(response, 'prepopulated_fields')\n        self.assertContains(response, 'prepopulated_fields_json')\n", "    def test_cell_count(self):\n        article = Article.objects.create(title=\"Article 1\", content=\"Content 1\")\n        question = Question.objects.create(article=article, question=\"Question 1\", answer=\"Answer 1\")\n        request = self.request_factory.get(reverse('admin:article_article_changelist'))\n        request.user = self.user\n        request.user.is_superuser = True\n\n        inline_admin_form = article.question_set.all()[0].inline_admin_formset()\n\n        template = self.get_template('inline_admin.html')\n        with self.subTest(msg='Without delete checkbox'):\n            response = self.client.get(reverse('admin:article_question_inline', args=(article.pk,)), HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n            self.assertEqual(response.context['cell_count'], inline_admin_form.formset.can_delete + 3)\n\n        with self.subTest(msg='With delete checkbox'):\n            template = self.get_template('inline_admin.html')\n            response = self.client.get(reverse('admin:article_question_inline', args=(article.pk,)), {'_delete[0-0-DELETE]:': 'on'}, HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n            self.assertEqual(response.context['cell_count'], inline_admin_form.formset.can_delete + 4)\n"], "sample_23": ["def test_longitude_wrap_at():\n    \"\"\"Test Longitude.wrap_at method\"\"\"\n    lon = Longitude([0, 360, 720], unit=u.deg)\n    assert_array_equal(lon.wrap_at(180).degree, [180, 0, 0])\n    assert_array_equal(lon.wrap_at(180, inplace=True).degree, [180, 0, 0])\n    assert_array_equal(lon.wrap_at(180).wrap_angle, 180 * u.deg)\n    assert_array_equal(lon.wrap_at(180, inplace=True).wrap_angle, 180 * u.deg)\n\n    with pytest.warns(AstropyDeprecationWarning):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at('180d')\n\n    with pytest.raises(TypeError):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at('180d', inplace=True)\n\n    with pytest.raises(TypeError):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at('180d', inplace=False)\n\n    with pytest.raises(ValueError):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at(250)\n\n    with pytest.raises(ValueError):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at('250d')\n\n    with pytest.raises(TypeError):\n        Longitude([0, 360, 720], unit=u.deg).wrap_at('250d', inplace=False)\n", "def test_angle_wrap():\n    a = Angle(np.arange(-10, 10) * u.deg)\n    assert_allclose(a.wrap_at(180 * u.deg).to_value(u.deg), np.array([-10, 170, -10, 170, -10, 170, -10, 170, -10, 170]))\n    assert_allclose(a.wrap_at(180 * u.deg).wrap_at(180 * u.deg).to_value(u.deg), np.array([-10, 170, -10, 170, -10, 170, -10, 170, -10, 170]))\n\n    # Test wrapping angles around multiple wrap angles\n    a = Angle(np.arange(-10, 10) * u.deg)\n    b = a.wrap_at(180 * u.deg).copy()\n    b.wrap_angle = 360 * u.deg\n    assert_allclose(b.wrap_at(180 * u.deg).to_value(u.deg), np.array([-10, 170, -10, 170, -10, 170, -10, 170, -10, 170]))\n\n    # Test wrapping angles with different precision\n    a = Angle(np.arange(-10, 10) * u.deg)\n    assert_allclose(a.wrap_at(180 * u.deg, precision=2).to_value(u.deg), np.array([-10., 170., -10., 170., -10., 170., -10., 170., -10., 170.]))\n\n    # Test wrapping angles with decimal representation\n    a = Angle(np.arange(-10, 10) * u.deg)\n    assert_allclose(a.wrap_at(180 * u.deg, decimal=True).to_value(u.deg), np.array([-10., 170., -10., 170., -10., 170., -10., 170., -10., 170.]))\n\n", "def test_Latitude_unit_conversion():\n    \"\"\"Test unit conversion in Latitude class.\"\"\"\n    lat = Latitude(\"10d\")\n    assert_allclose(lat.degree.to(u.hourangle).value, 0.1396)\n\n    lat = Latitude(\"10d\")\n    assert_allclose(lat.hour.to(u.degree).value, 10.0)\n", "def test_longitude_default_wrap_angle():\n    lon = Longitude(30 * u.deg)\n    assert lon.wrap_angle == 360 * u.deg\n    lon_180 = Longitude(30 * u.deg, wrap_angle=180 * u.deg)\n    assert lon_180.wrap_angle == 180 * u.deg\n    assert lon_180.degree == -150\n", "def test_wrap_angle_is_preserved_when_copying():\n    \"\"\"\n    Test that when copying an Angle with a non-default wrap angle, the\n    wrap angle is preserved in the copy.\n    \"\"\"\n    long = Longitude([0, 180, 360] * u.deg, wrap_angle=180 * u.deg)\n    assert long.wrap_angle == 180 * u.deg\n    long_copy = long.copy()\n    assert long_copy.wrap_angle == 180 * u.deg\n"], "sample_275": ["    def test_values(self):\n        person = Person.objects.create(name='John')\n        person2 = Person.objects.create(name='Alice')\n        qs = Person.objects.all().values('name')\n        self.assertEqual(qs.count(), 2)\n        self.assertEqual(list(qs)[0], {'name': 'John'})\n        self.assertEqual(list(qs)[1], {'name': 'Alice'})\n", "    def test_raw_queryset_iter(self):\n        RawQuerySet(sql='SELECT * FROM foo_file', model=FooFile, params=())\n        raw_queryset = RawQuerySet(sql='SELECT * FROM foo_file', model=FooFile, params=())\n        self.assertEqual(list(raw_queryset), [FooFile()])\n        self.assertEqual(list(raw_queryset), [])\n", "    def test_prefetch_related_empty_queryset(self):\n        # Create an empty queryset\n        qs = MyModel.objects.all()\n        # Prefetch related objects\n        prefetch_related_qs = qs.prefetch_related('field')\n        # Check that prefetch_related_qs is still an empty queryset\n        self.assertEqual(prefetch_related_qs.count(), 0)\n", "    def test_prefetch_related_raw_queryset(self):\n        # Create some data\n        book1 = Book.objects.create(title='Book 1')\n        book2 = Book.objects.create(title='Book 2')\n        book3 = Book.objects.create(title='Book 3')\n        foofile1 = FooFile.objects.create(name='FooFile 1', book=book1)\n        foofile2 = FooFile.objects.create(name='FooFile 2', book=book2)\n        foofile3 = FooFile.objects.create(name='FooFile 3', book=book3)\n        person1 = Person.objects.create(name='Person 1')\n        person2 = Person.objects.create(name='Person 2')\n        person3 = Person.objects.create(name='Person 3')\n\n        # Set up the prefetch\n        foofile_queryset = FooFile.objects.all()\n        person_queryset = Person.objects.all()\n        prefetch = Prefetch('book', queryset=foofile_queryset, to_attr='my_book')\n        prefetch2 = Prefetch('person', queryset=person_queryset, to_attr='my_person')\n\n        # Create a raw queryset\n        queryset = RawQuerySet(\n            \"SELECT * FROM delete_regress_foo_file\",\n            model=FooFile,\n            query=sql.RawQuery(\"SELECT * FROM delete_regress_foo_file\", using='default')\n        )\n        queryset.prefetch_related(prefetch, prefetch2)\n\n        # Fetch the data\n        queryset._fetch_all()\n\n        # Check the results\n        self.assertEqual(len(queryset), 3)\n        for foofile in queryset:\n            self.assertEqual(foofile.my_book, foofile.book)\n            self.assertEqual(foofile.my_person, person1 if foofile.book == book1 else person2 if foofile.book == book2 else person3)\n", "    def test_prefetch_related(self):\n        child1 = Child.objects.create(name='John')\n        child2 = Child.objects.create(name='Jane')\n        parent1 = Parent.objects.create(name='Parent1', children=[child1])\n        parent2 = Parent.objects.create(name='Parent2', children=[child2])\n\n        # Test prefetching children\n        children = Child.objects.select_related('parent').prefetch_related('parent__parent').all()\n        for child in children:\n            self.assertEqual(child.parent.name, 'Parent1')\n            self.assertEqual(child.parent.parent.name, None)\n\n        # Test prefetching children with different prefetch order\n        children = Child.objects.select_related('parent').prefetch_related('parent').all()\n        for child in children:\n            self.assertEqual(child.parent.name, 'Parent1')\n            self.assertEqual(child.parent.parent.name, None)\n\n        # Test prefetching children with a non-existent related field\n        with self.assertRaises(AttributeError):\n            Child.objects.prefetch_related('non_existent_field').get(id=1)\n\n        # Test prefetching children with a related field that is not prefetchable\n        with self.assertRaises(ValueError):\n            Child.objects.prefetch_related('parent__parent__parent').get(id=1)\n"], "sample_188": ["    def test_window_expression_with_partition_by(self):\n        Window(expression=F('num_employees')).as_sql(connection)\n", "    def test_Expression_eq(self):\n        a = F('field1')\n        b = F('field2')\n        self.assertEqual(a, b)\n        self.assertNotEqual(a, Expression())\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_window_frame_rows_start_end(self):\n        connection = connection.get_connection()\n        window_frame = RowRange(2, 3)\n        self.assertEqual(window_frame.as_sql(mock.Mock(), connection), (\n            \"ROWS BETWEEN 2 PRECEDING AND 3 FOLLOWING\",\n            [],\n        ))\n", "    def test_expression_list(self):\n        company = Company.objects.create(name=\"Example Inc.\")\n        queryset = Company.objects.annotate(\n            employees=F('employee_set__count')\n        )\n        expression = ExpressionList(Company.objects.name, F('employees'))\n        self.assertEqual(expression.as_sql(None, connection), ('%(expressions)s', ['name', 'employees']))\n"], "sample_956": ["def test_fetch_inventory_group_remote_inv_without_version():\n    app = mock.MagicMock()\n    app.config.intersphinx_mapping = {('test', ('http://example.com', ('path/to/inventory',))): None}\n    app.builder.env.intersphinx_cache = {}\n    app.builder.env.intersphinx_inventory = {}\n    app.builder.env.intersphinx_named_inventory = {}\n    with mock.patch('requests.get') as get_mock:\n        get_mock.return_value.status_code = 200\n        get_mock.return_value.url = 'http://example.com/path/to/inventory'\n        get_mock.return_value.raw.url = 'http://example.com/path/to/inventory'\n        get_mock.return_value.raw.read = mock.MagicMock()\n        fetch_inventory_group('test', 'http://example.com', ('path/to/inventory',), {}, app, 1643723400)\n    assert app.builder.env.intersphinx_inventory == {}\n    assert app.builder.env.intersphinx_named_inventory == {'test': inventory_v2}\n", "    def test_fetch_inventory(self):\n        with http_server('test_inventory') as httpd:\n            uri = 'http://localhost:8000'\n            self.assertTrue(fetch_inventory(httpd.app, uri, 'inventory.inv'))\n            self.assertTrue(fetch_inventory(httpd.app, uri, 'http://localhost:8000/inventory.inv'))\n", "def test_load_mappings():\n    class MockApp:\n        config = mock.Mock()\n        srcdir = ''\n        env = mock.Mock()\n        env.domains = {}\n\n    load_mappings(MockApp())\n\n    assert MockApp.config.intersphinx_mapping == {}\n\n    # Test with valid mapping\n    set_config(MockApp, {'foo': ('http://example.com', 'inv1', 'inv2')})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {'foo': ('http://example.com', ('inv1', 'inv2'))}\n\n    # Test with invalid mapping\n    set_config(MockApp, {'foo': ('http://example.com', 'inv1')})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {'foo': ('http://example.com', ('inv1',))}\n\n    # Test with duplicate mapping\n    set_config(MockApp, {'foo': ('http://example.com', 'inv1'), 'foo': ('http://example.com', 'inv2')})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {'foo': ('http://example.com', ('inv1', 'inv2'))}\n\n    # Test with invalid mapping key\n    set_config(MockApp, {'foo': ('http://example.com', 'inv1', 'inv2', 'inv3')})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {'foo': ('http://example.com', ('inv1', 'inv2'))}\n\n    # Test with bad mapping value\n    set_config(MockApp, {'foo': 'bad'})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {}\n    MockApp.config.log.warning.assert_called_once_with(__('Failed to read intersphinx_mapping[foo], ignored: %r'), 'bad')\n\n    # Test with missing mapping key\n    set_config(MockApp, {'foo': ('http://example.com', 'inv1'), 'bar': ('http://example.com', 'inv2')})\n    load_mappings(MockApp())\n    assert MockApp.config.intersphinx_mapping == {'foo': ('http://example.com', ('inv1',)), 'bar': ('http://example.com', ('inv2',))}\n\n    # Test with mapping key that's not a string\n    set_config(MockApp, {'foo': 123, ('http://example.com', 'inv1'): 'inv2'})\n    load_mappings", "def test_inventory_group_misses_all_inventories(self):\n    with http_server() as server:\n        server.expect_request(\n            url='/objects.inv',\n            method='GET',\n            status=200,\n            body='{\"std:module\": {\"example\": [\"example\", \"example\", 1, \"-\", True]}}',\n        )\n        server.expect_request(\n            url='/objects.inv',\n            method='GET',\n            status=404,\n            body='{\"std:module\": {\"example\": [\"example\", \"example\", 1, \"-\", True]}}',\n        )\n        server.expect_request(\n            url='/objects.inv',\n            method='GET',\n            status=200,\n            body='{\"std:module\": {\"example\": [\"example\", \"example\", 1, \"-\", True]}}',\n        )\n\n        app = mock.MagicMock()\n        app.srcdir = '/mock/srcdir'\n        app.builder = mock.MagicMock()\n        app.config = mock.Mock()\n\n        invs = ['/mock/objects.inv', '/objects.inv', '/objects2.inv']\n        inventories = InventoryAdapter(app.builder.env)\n        inventories.cache = {}\n\n        result = fetch_inventory_group(\n            'name', '/url', invs, inventories.cache, app, 0,\n        )\n        self.assertFalse(result)\n\n        self.assertEqual(inventories.cache, {('/url': ('name', 0, {'std:module': {'example': ['example', 'example', 1, '-', True]}})})\n", "def test_inspect_main():\n    class MockApp:\n        srcdir = ''\n        config = mock.Mock()\n        config.intersphinx_timeout = None\n        config.intersphinx_cache_limit = 10\n\n        raise Exception(msg)\n\n    MockApp.warn = mock.Mock(side_effect=mock_warn)\n\n    with http_server() as (base_url, httpd):\n        # Test case with a remote inventory\n        filename = urlunsplit((base_url.scheme, base_url.netloc, base_url.path + 'inventory-v2.inv', '', ''))\n        invdata = fetch_inventory(MockApp(), base_url.geturl(), filename)\n        inspect_main([filename])\n        MockApp.warn.assert_called_once_with('Print out an inventory file.\\nError: must specify local path or URL to an inventory file.')\n\n        # Test case with a local inventory\n        local_inv_file = os.path.join(os.path.dirname(__file__), 'data', INVENTORY_FILENAME)\n        inspect_main([local_inv_file])\n        MockApp.warn.assert_called_with('Print out an inventory file.\\nError: must specify local path or URL to an inventory file.')\n\n        # Test case with an invalid inventory file\n        filename = base_url.geturl() + 'nonexistent-invfile.txt'\n        invdata = fetch_inventory(MockApp(), base_url.geturl(), filename)\n        inspect_main([filename])\n        MockApp.warn.assert_called_with('Print out an inventory file.\\n' + 'Error: unknown or unsupported inventory version: nonexistent-invfile.txt')\n\n        # Test case with an inventory file without the required structure\n        filename = base_url.geturl() + 'inventory-v2-bad.inv'\n        invdata = fetch_inventory(MockApp(), base_url.geturl(), filename)\n        inspect_main([filename])\n        MockApp.warn.assert_called_with('Print out an inventory file.\\n' + 'Error: unknown or unsupported inventory version: inventory-v2-bad.inv')\n\n        # Test case with a URI with basic auth\n        uri = _strip_basic_auth(base_url.geturl() + 'inventory-v2-bad-auth@basic-auth-inv.inv')\n        invdata = fetch_inventory(MockApp(), uri, base_url.geturl() + 'inventory-v2-bad-auth-inv.inv')\n        inspect_main([uri])\n        MockApp.warn.assert_called_once_with('Print out an inventory file.\\n' + 'Error: unknown or unsupported inventory version: inventory-v2-bad-auth-inv.inv')\n\n"], "sample_43": ["def test_bayesian_blocks_points_no_error():\n    # Test that point measures with Gaussian errors works with no error\n    t = np.linspace(0, 1, 100)\n    x = np.sin(2 * np.pi * 5 * t)\n    sigma = 0.01\n    edges = bayesian_blocks(t, x, sigma, fitness='measures')\n    assert len(edges) == 100\n", "def test_events_fitness_p0_prior():\n    # Test that the p0_prior function is working as expected\n    events = Events()\n    n_blocks = 10\n    expected_result = 4 - np.log(73.53 * events.p0 * (n_blocks ** -0.478))\n    assert_allclose(events.p0_prior(n_blocks), expected_result)\n", "def test_Events_fitness():\n    # Test that Events fitness function returns correct value for known inputs\n    fitness = Events()\n    N_k = np.array([2, 2, 1])\n    T_k = np.array([10, 20, 30])\n    result = fitness.fitness(N_k, T_k)\n    expected_result = 2 * (np.log(2) - np.log(10)) + 2 * (np.log(2) - np.log(20)) + 1 * (np.log(1) - np.log(30))\n    assert_allclose(result, expected_result)\n", "def test_FitnessFunc_ncp_prior_prior():\n    \"\"\"Test that `ncp_prior` prior calculation agrees with `p0_prior`\n    when using the Events fitness function.\"\"\"\n    fit_func = Events(p0=0.05)\n    N = 10\n    assert_allclose(fit_func.compute_ncp_prior(N), fit_func.p0_prior(N))\n", "def test_events_fitting_unbinned_events():\n    \"\"\"Test Events fitness function with unbinned events.\"\"\"\n    # Create some data with unbinned events\n    t = np.random.rand(100)\n    x = np.random.randint(0, 2, size=100)\n\n    # Define the model\n    model = Events(p0=0.05)\n\n    # Fit the model\n    edges = bayesian_blocks(t, x, fitness=model)\n\n    # Check that the number of blocks is reasonable\n    assert edges.size - 1 > 1\n"], "sample_152": ["    def test_set_on_delete(self):\n        # Test SET on_delete behavior\n        obj = A.objects.create(name='test')\n        obj2 = A.objects.create(name='test2')\n        collector = Collector(using='default')\n        collector.collect([obj])\n        self.assertEqual(collector.field_updates[A], {(obj.pk, obj.name): {obj}})\n        collector.collect([obj2])\n        self.assertEqual(collector.field_updates[A], {(obj.pk, obj.name): {obj}, (obj2.pk, obj2.name): {obj2}})\n        collector.delete()\n        self.assertEqual(obj.name, 'test')\n        self.assertEqual(obj2.name, 'test2')\n", "    def test_delete_many_to_many(self):\n        # Create some M2M relationships\n        a = A.objects.create(name='a')\n        b1 = B1.objects.create(name='b1')\n        b2 = B2.objects.create(name='b2')\n        b1.b2_set.add(b2)\n        a.m2m_to.add(b1)\n\n        # Create some M2M relationships with generic foreign key\n        generic_b1 = GenericB1.objects.create(name='generic_b1')\n        generic_b2 = GenericB2.objects.create(name='generic_b2')\n        generic_b1.generic_b2_set.add(generic_b2)\n        a.generic_m2m_to.add(generic_b1)\n\n        # Collect and delete objects\n        collector = Collector(using='default')\n        collector.collect([a])\n        collector.delete()\n\n        # Check that M2M relationships were deleted\n        self.assertEqual(B2.objects.count(), 1)\n        self.assertEqual(GenericB2.objects.count(), 1)\n\n        # Check that generic foreign keys were deleted\n        self.assertEqual(GenericB1.objects.count(), 1)\n\n        # Check that on_delete behavior was called correctly\n        self.assertEqual(a.m2m_to.count(), 0)\n        self.assertEqual(a.generic_m2m_to.count(), 0)\n", "    def test_CASCADE_with_multiple_related_fields(self):\n        # Create some instances of related models\n        b1 = B1.objects.create()\n        b2 = B2.objects.create()\n        b3 = B3.objects.create()\n        m = M.objects.create(b1=b1, b2=b2)\n        r = R.objects.create(m=m, b3=b3)\n        \n        # Check that related fields are collected correctly\n        collector = Collector(using='default')\n        collector.collect([m])\n        self.assertIn(b1, collector.data[M])\n        self.assertIn(b2, collector.data[M])\n        self.assertIn(b3, collector.data[R])\n        \n        # Check that related fields are deleted correctly\n        collector.delete()\n        self.assertEqual(B1.objects.count(), 0)\n        self.assertEqual(B2.objects.count(), 0)\n        self.assertEqual(B3.objects.count(), 0)\n", "    def test_protected_error_with_multiple_foreign_keys(self):\n        # Create some objects\n        parent = P.objects.create(name='Parent')\n        child1 = R.objects.create(name='Child1', parent=parent)\n        child2 = RChild.objects.create(name='RChild', child=child1)\n        child3 = RChildChild.objects.create(name='RChildChild', rchild=child2)\n        child4 = RChildChild.objects.create(name='RChildChild2', rchild=child2)\n\n        # Protect the RChild model\n        RChild.objects.get(name='RChild').delete = SET_NULL\n\n        # Try to delete the parent\n        with self.assertRaises(ProtectedError):\n            parent.delete()\n\n        # Check that the protected objects are correctly identified\n        error = ProtectedError.as_asctime()\n        self.assertIn('RChildChild', str(error))\n        self.assertIn('RChildChild2', str(error))\n", "    def test_protected_error_raised_on_cascade(self):\n        r1 = R.objects.create(name='r1', parent=self.DEFAULT)\n        r2 = R.objects.create(name='r2', parent=r1)\n        r3 = R.objects.create(name='r3', parent=r2)\n        self.assertRaises(ProtectedError, R.objects.filter(name='r3').delete)\n"], "sample_1072": ["def test_frac_complex():\n    from sympy import frac, floor, ceiling, I\n    from sympy.abc import x, y\n    assert frac(x + I*y) == I*frac(y) + frac(x)\n    assert frac(5*I + 3) == I*frac(3) + frac(5)\n", "def test_floor_ceiling_negative_numbers():\n    assert floor(-7) == -7\n    assert floor(-7.3) == -8\n    assert floor(-7.7) == -7\n    assert floor(-I) == -I\n    assert floor(-I/2) == -I\n\n    assert ceiling(-7) == -7\n    assert ceiling(-7.3) == -7\n    assert ceiling(-7.7) == -6\n    assert ceiling(-I) == -I\n    assert ceiling(-I/2) == -I\n", "def test_ceiling_complex():\n    from sympy import I\n    assert ceiling(E + I) == 6 + 2*I\n", "def test_frac_complex_number():\n    # Test that frac returns 0 for complex numbers with no imaginary part\n    assert frac(I) == 0\n    assert frac(-I) == 0\n    \n    # Test that frac returns the fractional part for complex numbers with imaginary part\n    assert frac(5 + 3*I) == 3*I\n    assert frac(2 + 4*I) == 4*I\n    \n    # Test that frac returns the correct result for complex numbers with negative real part\n    assert frac(-5 - 3*I) == 1 - 3*I\n    assert frac(-2 - 4*I) == 1 - 4*I\n", "def test_frac_imaginary():\n    from sympy import I, Symbol\n\n    # Test that frac works with purely imaginary numbers\n    assert frac(I).simplify() == S.Zero\n    assert frac(-I).simplify() == S.One\n\n    # Test that frac works with complex numbers\n    z = Symbol('z', real=False)\n    assert frac(z + I*z).simplify() == I*frac(z)\n    assert frac(z - I*z).simplify() == I*frac(-z)\n\n    # Test that frac works with purely real numbers\n    r = Symbol('r', real=True)\n    assert frac(r).simplify() == S.Zero\n\n    # Test that frac works with numbers that are not integers\n    assert frac(3.5).simplify() == S.Half\n    assert frac(-3.5).simplify() == S.Threehalves\n\n    # Test that frac works with symbolic expressions\n    x = Symbol('x')\n    assert frac(x).simplify() == x - floor(x)\n    assert frac(x - 3).simplify() == x - floor(x) - 3\n\n    # Test that frac works with intervals\n    assert frac(AccumBounds(0, 1)).simplify() == S.One\n    assert frac(AccumBounds(-1, 1)).simplify() == S.One\n\n    # Test that frac works with infinities\n    assert frac(oo).simplify() == S.NaN\n    assert frac(-oo).simplify() == S.NaN\n\n    # Test that frac works with NaN and zoo\n    assert frac(nan).simplify() == nan\n    assert frac(zoo).simplify() == zoo\n\n    # Test that frac works with exp and log\n    assert frac(exp(x)).simplify() == S.Zero\n    assert frac(log(x)).simplify() == S.Zero\n\n    # Test that frac works with sin and cos\n    assert frac(sin(x)).simplify() == S.Zero\n    assert frac(cos(x)).simplify() == S.Zero\n"], "sample_375": ["    def test_state_model_get_field(self):\n        model_state = ModelState.from_model(models.Model)\n        field = model_state.get_field('id')\n        self.assertIsInstance(field, models.AutoField)\n        self.assertEqual(field.name, 'id')\n\n        model_state = ModelState.from_model(models.CharField)\n        with self.assertRaises(FieldDoesNotExist):\n            model_state.get_field('id')\n", "    def test_from_model_with_many_to_many(self):\n        \"\"\"\n        Test that ModelState can handle many-to-many relationships.\n        \"\"\"\n        model = models.Model()\n        model._meta.local_many_to_many = [\n            models.ManyToManyField(models.Model()),\n        ]\n        model_state = ModelState.from_model(model)\n        self.assertIsInstance(model_state.get_field(\"field_name\").remote_field.through, models.ManyToManyField)\n", "    def test_clone(self):\n        state = ProjectState()\n        state.models['app_label.model_name'] = ModelState(\n            'app_label', 'model_name',\n            fields={'field_name': models.CharField(max_length=100)},\n            options={'indexes': [models.Index(fields=['field_name'])]},\n            bases=(models.Model,)\n        )\n        cloned_state = state.clone()\n        self.assertNotEqual(state, cloned_state)\n        self.assertEqual(state.models, cloned_state.models)\n        self.assertEqual(state.real_apps, cloned_state.real_apps)\n        self.assertEqual(state.is_delayed, cloned_state.is_delayed)\n        self.assertEqual(state._relations, cloned_state._relations)\n        self.assertEqual(state.apps, cloned_state.apps)\n", "    def test_from_model_excludes_hidden_fields(self):\n        class HiddenField(models.Model):\n            hidden_field = models.CharField(max_length=255, hidden=True)\n        \n        model_state = ModelState.from_model(HiddenField, exclude_rels=True)\n        self.assertEqual(model_state.fields, {})\n        ", "    def test_clone(self):\n        apps = Apps(set())\n        apps.register_model('app1', ModelState(\n            'app1', 'model1', {},\n            options={'indexes': [models.Index(fields=['field1'])]},\n            bases=(models.Model,),\n        ))\n\n        state = ProjectState.from_apps(apps)\n        model_state = state.models[('app1', 'model1')]\n        model_state.options['indexes'][0].name = 'my_index'\n\n        cloned_state = model_state.clone()\n        self.assertNotEqual(model_state, cloned_state)\n        self.assertEqual(model_state.options['indexes'][0].name, 'my_index')\n        self.assertNotEqual(cloned_state.options['indexes'][0].name, 'my_index')\n\n        # Test that cloning a model with many-to-many fields\n        # with related model works correctly\n        apps.register_model('app2', ModelState(\n            'app2', 'model2',\n            {\n                'field1': models.ManyToManyField('app1.model1')\n            },\n            bases=(models.Model,),\n        ))\n\n        cloned_model_state = cloned_state.clone()\n        self.assertIn('field1', cloned_model_state.fields)\n        self.assertNotIn('field1', model_state.fields)\n        self.assertEqual(cloned_model_state.fields['field1'].remote_field.model, 'app1.model1')\n        self.assertNotEqual(model_state.fields.get('field1'), None)\n\n        # Test that cloning a model with a proxy model works correctly\n        apps.register_model('app3', ModelState(\n            'app3', 'model3',\n            options={'proxy': True},\n            bases=('app1.model1',)\n        ))\n\n        cloned_proxy_model_state = cloned_model_state.clone()\n        self.assertIn('field1', cloned_proxy_model_state.fields)\n        self.assertNotIn('field1', cloned_state.fields)\n        self.assertEqual(cloned_proxy_model_state.fields['field1'].remote_field.model, 'app1.model1')\n        self.assertNotEqual(cloned_state.fields.get('field1'), None)\n\n        # Test that cloning a model with a GenericForeignKey works correctly\n        apps.register_model('app4', ModelState(\n            'app4', 'model4',\n            {\n                'field1': GenericForeignKey('field2', 'field3')\n            },\n            bases=(models.Model,),\n        ))\n\n        cloned_model_state = cloned_state.clone()\n        self.assertIn('field1', cloned_model_state.fields)\n        self.assertNotIn('field1', model_state.fields)\n        self.assertEqual"], "sample_1006": ["def test_subfactorial_is_even():\n    n = Symbol('n', integer=True)\n    assert subfactorial(n).is_even\n    assert subfactorial(n + 1).is_even\n", "def test_subfactorial_arg_as_symbol():\n    from sympy import Symbol\n    from sympy.functions.combinatorial.factorials import subfactorial\n    n = Symbol('n', integer=True)\n    assert subfactorial(n) == subfactorial(n)\n", "def test_subfactorial_odd():\n    n = Symbol('n', integer=True)\n    result = subfactorial(n)\n    assert result.is_Function\n", "compilation error", "def test_subfactorial():\n    n = symbols('n', integer=True)\n    assert subfactorial(n).is_Function\n    assert subfactorial(n).args == (n,)\n    assert subfactorial(5).is_Integer\n    assert subfactorial(-5) == 44  # TODO: check if it's correct\n    assert subfactorial(S.Infinity) == S.Infinity\n    assert subfactorial(S.NaN) == S.NaN\n    assert subfactorial(n+1) == subfactorial(n+1)\n    assert subfactorial(0) == 1\n    assert subfactorial(1) == 0\n    assert subfactorial(2).is_Integer\n    assert subfactorial(-1) == 1\n    assert subfactorial(-2).is_Integer\n    assert subfactorial(-3).is_Integer\n    assert subfactorial(-4).is_Integer\n    assert subfactorial(-5).is_Integer\n    assert subfactorial(0.5) == 0.3678794412\n"], "sample_715": ["def test_cross_validate_with_n_jobs_set_to_minus_one():\n    \"\"\"Test cross_validate with n_jobs=-1\"\"\"\n    from sklearn.model_selection import train_test_split\n    X, y = make_regression(n_samples=100, n_features=1)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    estimator = SGDClassifier(max_iter=1)\n    scoring = 'accuracy'\n\n    result = cross_validate(estimator, X_train, y_train, cv=2, n_jobs=-1, scoring=scoring)\n    assert_array_almost_equal(result['test_score'], [1.0, 1.0])\n\n    result = cross_validate(estimator, X_train, y_train, cv=2, n_jobs=-1, scoring=scoring, return_train_score=True)\n    assert_array_almost_equal(result['test_score'], [1.0, 1.0])\n    assert_array_almost_equal(result['train_score'], [1.0, 1.0])\n", "    def test_learning_curve(self):\n        X, y = make_regression(n_samples=100, n_features=10, n_informative=5)\n        estimator = SGDClassifier()\n        train_sizes_abs, train_scores, test_scores = learning_curve(\n            estimator, X, y, cv=KFold(n_splits=3), scoring='accuracy')\n        self.assertEqual(len(train_scores), len(train_sizes_abs))\n", "def test_index_param_value():\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    v = np.array([1, 2, 3])\n    indices = [0, 1, 2]\n    expected = np.array([1, 2, 3])\n    assert_array_equal(_index_param_value(X, v, indices), expected)\n\n    v = np.array([1, 2])\n    assert_array_equal(_index_param_value(X, v, indices), v)\n\n    v = 1\n    assert_array_equal(_index_param_value(X, v, indices), v)\n\n    v = np.array([1, 2, 3])\n    indices = [0, 2]\n    expected = np.array([1, 3])\n    assert_array_equal(_index_param_value(X, v, indices), expected)\n\n    v = np.array([[1, 2], [3, 4]])\n    indices = [0, 1]\n    expected = np.array([1, 4])\n    assert_array_equal(_index_param_value(X, v, indices), expected)\n\n    v = np.array([1, 2])\n    indices = [0, 2]\n    expected = v\n    assert_array_equal(_index_param_value(X, v, indices), expected)\n\n    v = np.array([[1, 2], [3, 4]])\n    indices = [0, 1]\n    expected = v\n    assert_array_equal(_index_param_value(X, v, indices), expected)\n\n    # Test with sparse array\n    X_sparse = coo_matrix(np.array([[1, 2], [3, 4]]))\n    v = np.array([1, 2])\n    indices = [0, 1]\n    expected = np.array([1, 2])\n    assert_array_equal(_index_param_value(X_sparse, v, indices), expected)\n\n    # Test with empty array\n    v = np.array([1, 2])\n    indices = []\n    assert_array_equal(_index_param_value(X, v, indices), v)\n\n    # Test with None\n    v = None\n    indices = [0, 1]\n    assert_array_equal(_index_param_value(X, v, indices), None)\n\n    # Test with 1D array\n", "    def test_p_value_is_within_the_valid_range(self):\n        X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n        estimator = clone(SGDClassifier())\n        score, permutation_scores, p_value = permutation_test_score(\n            estimator, X, y, cv=KFold(n_splits=5), n_permutations=100,\n            scoring='accuracy')\n        assert_less(p_value, 1)\n", "def test_learning_curve_y_not_1d():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, weights=None,\n                               random_state=0)\n\n    estimator = OneVsRestClassifier(MockClassifier(), n_jobs=1)\n    train_sizes_abs, train_scores, test_scores = learning_curve(estimator,\n                                                              X, y,\n                                                              train_sizes=[0.1, 0.5, 0.9],\n                                                              cv=5)\n    assert_equal(len(train_scores), 3)\n    assert_equal(len(test_scores), 3)\n    assert_greater(train_scores[0, 0], 0.0)\n    assert_greater(test_scores[0, 0], 0.0)\n"], "sample_145": ["    def test_date_hierarchy_must_refer_to_DateField_or_DateTimeField(self):\n        class InvalidDateHierarchyModelAdmin(ModelAdmin):\n            model = ValidationTestModel\n            date_hierarchy = 'invalid_field'\n\n        self.assertIsInvalid(InvalidDateHierarchyModelAdmin, ValidationTestModel, \"The value of 'date_hierarchy' refers to 'invalid_field', which does not refer to a Field.\")\n", "    def test_prepopulated_fields_value(self):\n        model_admin = ModelAdmin(model=User, admin_site=AdminSite())\n        self.assertIsInvalid(model_admin, User, \"The value of 'prepopulated_fields' refers to 'name', which is not an attribute of 'auth.User'.\")\n", "    def test_prepopulated_fields(self):\n        class TestModelAdmin(ModelAdmin):\n            model = User\n            prepopulated_fields = {\"username\": (\"first_name\", \"last_name\")}\n        self.assertIsInvalid(TestModelAdmin, User, \"'username' cannot include the field 'first_name', because that field manually specifies a relationship model.\", id='admin.E013')\n", "    def test_fieldsets_check_duplicate_fields(self):\n        model = ValidationTestModel\n        admin_obj = ModelAdmin(model, AdminSite())\n        admin_obj.fieldsets = [\n            ('Section 1', {'fields': ['field1', 'field1']}),\n        ]\n        self.assertIsNotNone(admin_obj.check()[0].hint)\n", "    def test_prepopulated_fields_invalid_field_types(self):\n        class InvalidModelAdmin(ModelAdmin):\n            model = Model\n            prepopulated_fields = {'field1': (1, 2), 'field2': ['field3']}\n\n        self.assertIsInvalid(InvalidModelAdmin, Model, 'The value of \"prepopulated_fields[\"field1\"]\" contains an item that is neither a Field, a DateTimeField, a ForeignKey, a OneToOneField nor a ManyToManyField.')\n"], "sample_505": ["def test_rrule_locator(freq, expected):\n    rule = dateutil.rrulewrapper(freq, interval=1)\n    locator = mdates.RRuleLocator(rule, tz=dateutil.tz.gettz('UTC'))\n    assert isinstance(locator, expected.__class__)\n    assert locator._freq == freq\n", "def test_datestr2num_default_date():\n    # check default date handling\n    dates = [\"2022-01-01\", \"2022-02-01\"]\n    expected = date2num(datetime.datetime(2022, 1, 1)), date2num(datetime.datetime(2022, 2, 1))\n    assert np.allclose(datestr2num(dates), expected)\n", "def test_DateFormatter():\n    # Test that the format string is applied correctly\n    dates = np.array([datetime.datetime(2022, 1, 1, 12, 0), datetime.datetime(2022, 1, 2, 12, 0)])\n    formatter = mdates.DateFormatter('%Y-%m-%d')\n    expected = ['2022-01-01', '2022-01-02']\n    assert np.all(formatter(dates) == expected)\n\n    # Test that the format string is applied correctly for microseconds\n    dates = np.array([datetime.datetime(2022, 1, 1, 12, 0, 0, 123456)])\n    formatter = mdates.DateFormatter('%Y-%m-%d %H:%M:%S.%f')\n    expected = ['2022-01-01 12:00:00.123456']\n    assert np.all(formatter(dates) == expected)\n\n    # Test that the format string is applied correctly when plotting with TeX\n    dates = np.array([datetime.datetime(2022, 1, 1, 12, 0, 0, 123456)])\n    formatter = mdates.DateFormatter('%Y-%m-%d %H:%M:%S.%f')\n    with rc_context(rc={'text.usetex': True}):\n        assert formatter(dates)[0] == r'$2022-01-01 12:00:00.123456$'\n", "def test_date_formatter():\n    # Test setting and getting the epoch\n        with rc_context({'date.epoch': '2020-01-01T12:00:00'}):\n            assert mdates.get_epoch() == '2020-01-01T12:00:00'\n\n    test_epoch()\n\n        dt = datetime.datetime(2020, 1, 1, 12, 0, 0)\n        assert mdates.date2num(dt) == 0.5\n\n    test_date2num()\n\n        n = mdates.date2num(datetime.datetime(2020, 1, 1, 12, 0, 0))\n        dt = mdates.num2date(n)\n        assert dt.hour == 12\n        assert dt.minute == 0\n        assert dt.second == 0\n\n    test_num2date()\n\n        dt = mdates.datestr2num('2020-01-01 12:00:00')\n        assert dt == 0.5\n\n    test_datestr2num()\n\n        j = 2440588.0  # Julian date 2020-01-01\n        n = mdates.julian2num(j)\n        assert np.isclose(n, 0.5)\n\n    test_julian2num()\n\n        n = 0.5  # Matplotlib date\n        j = mdates.num2julian(n)\n        assert np.isclose(j, 2440588.0)\n\n    test_num2julian()\n\n        n = 1.5\n        delta = mdates.num2timedelta(n)\n        assert delta == datetime.timedelta(days=1, seconds=18*3600)\n\n    test_num2timedelta()\n\n        dt = datetime.datetime(2020, 1, 1, 12, 0, 0)\n        formatter = mdates.DateFormatter('%Y-%m-%d')\n        assert formatter(dt) == '2020-01-01'\n\n    test_date_formatter()\n\n        dt = datetime.datetime(2020, 1, 1, 12, 0, 0)\n       ", "def test_datestr2num():\n    # Test datestr2num with different input types\n        date = mdates.datestr2num(datestr)\n        np.testing.assert_equal(date, expected)\n\n    # Test with a single date string\n    check_datestr2num(\"2022-01-01\", 2459581.0)\n\n    # Test with a list of date strings\n    check_datestr2num([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"], [\n                      2459581.0, 2459582.0, 2459583.0])\n\n    # Test with a single date string with a specific format\n    check_datestr2num(\"2022-01-01T12:00:00\", 2459581.25)\n\n    # Test with a list of date strings with a specific format\n    check_datestr2num([\"2022-01-01T12:00:00\", \"2022-01-02T12:00:00\",\n                       \"2022-01-03T12:00:00\"], [\n                      2459581.25, 2459582.25, 2459583.25])\n\n    # Test with a date string that is not a valid date\n    with pytest.raises(ValueError):\n        check_datestr2num(\"2022-02-30\")\n\n    # Test with a default date when fields are missing\n    check_datestr2num(\"2022-\", 2459581.0)\n\n    # Test with a default date when fields are missing and default date is None\n    check_datestr2num(\"2022-\", None)\n\n    # Test with a timezone-aware date string\n    check_datestr2num(\"2022-01-01T12:00:00+00:00\", 2459581.25)\n    check_datestr2num(\"2022-01-01T12:00:00-00:00\", 2459581.25)\n    check_datestr2num(\"2022-01-01T12:00:00+05:00\", 2459581.25)\n    check_datestr2num(\"2022-01-01T12:00:00-05:00\", 2459581.25)\n\n    # Test with a timezone"], "sample_257": ["    def test_custom_decoder(self):\n        json_model = JSONModel.objects.create(json_field={'a': 1, 'b': 2})\n        custom_decoder = CustomJSONDecoder()\n        self.assertEqual(json.loads(json_model.json_field, cls=custom_decoder), {'a': 1, 'b': 2})\n", "    def test_key_transform_factory(self):\n        factory = KeyTransformFactory('test_key')\n        transform1 = factory()\n        transform2 = factory()\n        self.assertEqual(transform1.key_name, 'test_key')\n        self.assertEqual(transform2.key_name, 'test_key')\n", "    def test_json_field_validates_on_empty_string(self):\n        model = JSONModel(json_field='')\n        with self.assertRaises(ValidationError):\n            model.full_clean()\n", "    def test_json_key_transform_lookup_serialize(self):\n        model = JSONModel()\n        json_field = KeyTransform('key', model.jsonfield)\n        self.assertEqual(\n            serializers.serialize('json', [json_field]),\n            '[{\"model\": \"models.KeyTransform\", \"fields\": {\"lhs\": \"jsonfield\", \"key_name\": \"key\"}}]',\n        )\n", "    def test_json_field_get_prep_value_with_none(self):\n        model = models.Model(json_field=models.JSONField())\n        model.json_field = None\n        self.assertIsNone(model._meta.get_fields()[0].get_prep_value(None))\n"], "sample_239": ["    def test_management_form_bound(self):\n        formset = ChoiceFormSet(prefix='choices', data={\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MAX_NUM_FORMS': '1',\n            'choices-0-choice': 'choice1',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'choice2',\n            'choices-1-votes': '2',\n        })\n        self.assertTrue(formset.management_form.is_bound)\n", "    def test_management_form_is_valid(self):\n        formset = ChoiceFormSet({'choices-INITIAL_FORMS': '1', 'choices-TOTAL_FORMS': '2'})\n        self.assertTrue(formset.management_form.is_valid())\n", "    def test_form_kwargs(self):\n        formset = ChoiceFormSet(data={'choices-0-choice': 'choice1', 'choices-0-votes': '1', 'choices-1-choice': 'choice2', 'choices-1-votes': '2'})\n        form = list(formset)[0]\n        self.assertEqual(form.data, {'choice': 'choice1', 'votes': '1'})\n", "    def test_management_form_invalid(self):\n        \"\"\"Tests that ManagementForm raises a ValidationError if the form data is missing.\"\"\"\n        form_data = {'TOTAL_FORMS': '2', 'INITIAL_FORMS': '0'}\n        form = ManagementForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors, {'TOTAL_FORMS': ['This field is required.']})\n", "    def test_management_form_is_bound(self):\n        formset = ChoiceFormSet()\n        management_form = formset.management_form\n        self.assertTrue(management_form.is_bound)\n"], "sample_1019": ["def test_div():\n    from sympy.core.exprtools import div\n    from sympy.abc import x, y, z\n\n    a = Factors((x*y**2).as_powers_dict())\n    b = Factors((x*y/z).as_powers_dict())\n\n    assert div(a, b) == (Factors({x: 1, y: 2, z: -1}), Factors(S.One))\n\n    a = Factors((x*y**2).as_powers_dict())\n    b = Factors((x*y).as_powers_dict())\n\n    assert div(a, b) == (Factors({y: 2}), Factors({x: 1}))\n\n    a = Factors((x*y**2).as_powers_dict())\n    b = Factors(x)\n\n    assert div(a, b) == (Factors({y: 2}), Factors({x: 1}))\n\n    a = Factors((x*y**2).as_powers_dict())\n    b = Factors(1)\n\n    assert div(a, b) == (a, Factors(S.One))\n\n    a = Factors((x*y**2).as_powers_dict())\n    b = Factors(0)\n\n    raises(ZeroDivisionError, lambda: div(a, b))\n\n    a = Factors(0)\n    b = Factors(0)\n\n    assert div(a, b) == (Factors(S.Zero), Factors(S.Zero))\n", "def test_factor_nc_with_multiple_nc_objects():\n    from sympy import sqrt, sin\n    from sympy.abc import x, y, z, A, B, C\n    from sympy.core.exprtools import factor_nc\n\n    eq = (sin(x + A)*(sin(x + B) + sin(x + C))).expand()\n    result = factor_nc(eq)\n    expected = sin(x + A)*sin(x + B + C)\n    assert result == expected\n", "def test_mask_nc():\n    from sympy.core.exprtools import _mask_nc\n    A, B, C = symbols('A,B,C', commutative=False)\n    expr = A**2 + 2*A*B + B**2\n    expr, rep, nc_symbols = _mask_nc(expr)\n    assert expr == (A**2 + 2*_d0*A + _d0**2)\n    assert rep == {_d0: B}\n    assert nc_symbols == [A, B]\n    assert expr.subs(rep) == (A**2 + 2*B*A + B**2)\n", "def test_factors_pow():\n    from sympy import I, Symbol\n    A = Symbol('A', commutative=False)\n    a = Symbol('a', commutative=False)\n    b = Symbol('b', commutative=False)\n\n    # Check that Factors correctly handles exponents in a Pow\n    f = Factors((A**2)**3)\n    assert f.factors == {A**2: 3}\n    assert f.as_expr() == (A**2)**3\n    assert f.pow(2).as_expr() == (A**2)**6\n\n    # Check that Factors correctly handles I in the base\n    f = Factors((I**2)**3)\n    assert f.factors == {I**2: 3}\n    assert f.as_expr() == (I**2)**3\n    assert f.pow(2).as_expr() == (I**2)**6\n\n    # Check that Factors correctly handles I in the exponent\n    f = Factors(A**2**(3*I))\n    assert f.factors == {A**2: 3*I}\n    assert f.as_expr() == A**(2*(3*I))\n    assert f.pow(2).as_expr() == A**(2*(6*I))\n\n    # Check that Factors correctly handles both I in the base and in the exponent\n    f = Factors((A**2)**(3*I))\n    assert f.factors == {A**2: 3*I}\n    assert f.as_expr() == (A**2)**(3*I)\n    assert f.pow(2).as_expr() == ((A**2)**(3*I))**2\n\n    # Check that Factors correctly handles non-integer exponents\n    f = Factors(A**2**(1.5))\n    assert f.factors == {A**2: 1.5}\n    assert f.as_expr() == A**(2*1.5)\n    assert f.pow(2).as_expr() == A**(2*3)\n", "def test_monotonic_sign():\n    from sympy.core.exprtools import _monotonic_sign\n    from sympy.core.exprtools import (Term, Factors, _gcd_terms, gcd_terms,\n                                      _mask_nc, factor_nc)\n\n    assert _monotonic_sign(S(-1)) is None\n    assert _monotonic_sign(S(1)) is None\n    assert _monotonic_sign(S(0)) is None\n\n    assert _monotonic_sign(1 + 2*x) == S(1)\n    assert _monotonic_sign(x - 1) == Dummy('pos', positive=True)\n    assert _monotonic_sign(-x + 1) == Dummy('neg', negative=True)\n    assert _monotonic_sign(-x - 1) == Dummy('npos', nonpositive=True)\n\n    assert _monotonic_sign(x**2 + 1) == S(1)\n    assert _monotonic_sign(x**2 - 1) == S(3)\n    assert _monotonic_sign(x**4 + 1) == S(9)\n    assert _monotonic_sign(x**4 - 1) == S(2)\n    assert _monotonic_sign(x**3 + 1) == S(2)\n    assert _monotonic_sign(x**3 - 1) == S(4)\n    assert _monotonic_sign(x + 1)**3 == S(2)\n    assert _monotonic_sign(x - 1)**3 == S(3)\n\n    assert _monotonic_sign(x**2 - 2*x + 1) == S(1)\n    assert _monotonic_sign(x - 2 + 1) == S(1)\n    assert _monotonic_sign(x - 2 - 1) == S(1)\n    assert _monotonic_sign(x**2 + 2*x + 1) == S(1)\n    assert _monotonic_sign(x**2 - 2*x - 1) == S(1)\n    assert _monotonic_sign(x + 2 - 1) == S(1)\n    assert _monotonic_sign(x + 2 + 1) == S(1)\n\n    assert _monotonic_sign(x**3 - 2*x**2 - x + 1) == S(1)\n\n    assert _monotonic_sign(2*x"], "sample_1206": ["compilation error", "compilation error", "compilation error", "    def test_pow_int(self):\n        assert zoo ** S.Zero == 1\n        assert S.Infinity ** S.Zero == oo\n        assert S.Infinity ** S.One == oo\n        assert S.Infinity ** S.NegativeOne == 0\n        assert S.Infinity ** -S.One == oo\n        assert S.NegativeInfinity ** S.Zero == -oo\n        assert S.NegativeInfinity ** S.One == -oo\n        assert S.NegativeInfinity ** S.NegativeOne == oo\n        assert S.NegativeInfinity ** -S.One == -oo\n        assert oo ** oo == oo\n        assert -oo ** oo == -oo\n        assert oo ** -oo == 0\n        assert -oo ** -oo == 0\n        assert oo ** S.Half == zoo\n        assert -oo ** S.Half == zoo\n", "def test_AlgebraicNumber_minpoly():\n    from sympy.abc import x, y\n    from sympy import Rational\n    from sympy.polys import Poly\n    from sympy import AlgebraicNumber\n\n    a = AlgebraicNumber(sqrt(2))\n    b = AlgebraicNumber(a, [3, -5])\n\n    m = b.minpoly_of_element()\n    assert isinstance(m, Poly)\n    assert m.as_expr(x) == x**3 - 5\n\n    c = AlgebraicNumber(sqrt(2) + sqrt(3))\n    d = AlgebraicNumber(c, [1, 0, -9, 0])\n    e = AlgebraicNumber(d.to_root())\n    f = AlgebraicNumber(e.minpoly, e.root)\n    assert f == d\n\n    g = AlgebraicNumber(0)\n    h = AlgebraicNumber(g, [1, 2])\n    assert h.minpoly == g.minpoly\n\n    i = AlgebraicNumber(1)\n    j = AlgebraicNumber(i, [2])\n    assert j.minpoly == i.minpoly\n\n    k = AlgebraicNumber(sqrt(2))\n    l = AlgebraicNumber(k, [3, -5])\n    m = l.minpoly_of_element()\n    assert isinstance(m, Poly)\n    assert m.as_expr(x) == x**3 - 5\n\n    n = AlgebraicNumber(k, [3, -5], alias='a')\n    assert n.minpoly == m\n    assert n.alias == 'a'\n\n    # Make sure the coefficients are also converted when creating a new AlgebraicNumber\n    # instance.\n    o = AlgebraicNumber(k, [Rational(3, 2), Rational(-5, 2)])\n    assert o.minpoly == m\n    assert o.coeffs() == [Rational(3, 2), Rational(-5, 2)]\n\n    # Check the _own_minpoly attribute.\n    p = AlgebraicNumber(k, [3, -5])\n    p._own_minpoly = None\n    assert p.minpoly_of_element() == p.minpoly\n    assert p._own_minpoly is not None\n\n    q = AlgebraicNumber(k)\n    assert q.minpoly_of_element() == k.minpoly\n    assert q._own_minpoly is not None\n"], "sample_1097": ["def test_BlockDiagMatrix_construction():\n    n, m = symbols('n m')\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    diag1 = BlockDiagMatrix(X, Y)\n    assert diag1.diag == (X, Y)\n\n    diag2 = BlockDiagMatrix()\n    assert diag2.diag == ()\n\n    diag3 = BlockDiagMatrix(X)\n    assert diag3.diag == (X,)\n\n    # Test BlockDiagMatrix shape\n    assert diag1.shape == (n + m, n + m)\n\n    # Test BlockDiagMatrix blockshape\n    assert diag1.blockshape == (2, 2)\n\n    # Test BlockDiagMatrix rowblocksizes\n    assert diag1.rowblocksizes == [n, m]\n\n    # Test BlockDiagMatrix colblocksizes\n    assert diag1.colblocksizes == [n, m]\n\n    # Test BlockDiagMatrix equality\n    assert diag1.equals(BlockDiagMatrix(X, Y))\n    assert not diag1.equals(BlockDiagMatrix(Y, X))\n\n    # Test BlockDiagMatrix inverse\n    inv_diag1 = diag1._eval_inverse()\n    assert isinstance(inv_diag1, BlockDiagMatrix)\n    for i, arg in enumerate(inv_diag1.args):\n        assert arg == X.inverse() if i == 0 else Y.inverse()\n\n    # Test BlockDiagMatrix transpose\n    trans_diag1 = diag1._eval_transpose()\n    assert isinstance(trans_diag1, BlockDiagMatrix)\n    assert trans_diag1.diag == (X.transpose(), Y.transpose())\n", "def test_blockcut_complex_matrix():\n    M = Matrix([[1+2*I, 1], [1, 2]])\n    B = blockcut(M, (1, 1), (1, 2))\n    assert isinstance(B, BlockMatrix)\n    assert B.shape == (2, 2)\n    assert B.blocks[0, 0] == Matrix([[1]])\n    assert B.blocks[0, 1] == Matrix([[1]])\n    assert B.blocks[1, 0] == Matrix([[1]])\n    assert B.blocks[1, 1] == Matrix([[2]])\n", "def test_block_inverse_not_block_diagonal():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = BlockMatrix([[A, B], [B, A]])\n    inv_expr = bc_inverse(BlockMatrix([[expr]]))\n    assert isinstance(inv_expr, BlockMatrix)\n    assert inv_expr.structurally_equal(BlockMatrix([[A**(-1), -B*A**(-1)],\n                                                [-B*A**(-1), A**(-1)]]))\n", "def test_blockcut_blockmatrix(self):\n    from sympy import MatrixSymbol, Matrix\n    M = MatrixSymbol('M', 4, 4)\n    N = MatrixSymbol('N', 4, 4)\n    P = MatrixSymbol('P', 4, 4)\n    Q = MatrixSymbol('Q', 4, 4)\n    R = MatrixSymbol('R', 4, 4)\n    S = MatrixSymbol('S', 4, 4)\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    E = MatrixSymbol('E', 2, 2)\n    F = MatrixSymbol('F', 2, 2)\n\n    BM = BlockMatrix([[A, B], [C, D]])\n    BM2 = BlockMatrix([[E, F], [G, H]])\n    BM3 = BlockMatrix([[N, P], [Q, R]])\n\n    blockcut_BM = blockcut(BM, (1, 3), (1, 3))\n    blockcut_BM2 = blockcut(BM, (1, 2), (1, 2))\n    blockcut_BM3 = blockcut(BM, (2, 2), (2, 2))\n\n    blockcut_BM2_BM3 = blockcut(BM2, (1, 2), (1, 2))\n    blockcut_BM2_BM3 = blockcut(BM3, (1, 2), (1, 2))\n\n    self.assertTrue(blockcut_BM.is_BlockMatrix)\n    self.assertEqual(blockcut_BM.blocks[0, 1].is_BlockMatrix, True)\n    self.assertTrue(blockcut_BM2.is_BlockMatrix)\n    self.assertEqual(blockcut_BM2.blocks[0, 1].is_BlockMatrix, True)\n    self.assertTrue(blockcut_BM3.is_BlockMatrix)\n    self.assertEqual(blockcut_BM3.blocks[0, 1].is_BlockMatrix, True)\n    self.assertTrue(blockcut_BM2_BM3.is_BlockMatrix)\n    self.assertEqual(blockcut_BM2_BM3.blocks[0, 1].is_BlockMatrix, True)\n    self.assertEqual(blockcut_BM2_BM3.blocks[", "def test_block_collapse_blockdiag_multiple():\n    n, m, l = symbols('n m l')\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    Z = MatrixSymbol('Z', n, m)\n    W = MatrixSymbol('W', m, n)\n    diag = BlockDiagMatrix(X, Y, Z, W)\n    result = block_collapse(diag)\n    expected = BlockMatrix([[X, ZeroMatrix(n, m), ZeroMatrix(n, n)],\n                           [ZeroMatrix(m, n), Y, ZeroMatrix(m, n)],\n                           [Z, ZeroMatrix(m, n), ZeroMatrix(n, n)]])\n    assert result.equals(expected)\n"], "sample_225": ["    def test_admin_view(self):\n        request = self.request_factory.get(reverse('test_adminsite:index'))\n        request.user = self.u1\n        view = site.admin_view(lambda x: None)\n        response = view(request)\n        self.assertIsInstance(response, HttpResponse)\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.request = self.request_factory.get(reverse('test_adminsite:index'))\n        self.request.user = self.u1\n        self.site = admin.AdminSite(name=\"test_adminsite\")\n        self.site.register(User)\n        self.site.register(Article)\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.a1 = Article.objects.create(title='Article 1', content='Content 1')\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.app_config = apps.get_app_config('admin')\n        cls.app_config.models = [Article, User]\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        site.register(Article)\n"], "sample_1095": ["compilation error", "def test_str():\n    # Test that Permutation instances are correctly converted to strings\n    p = Permutation([[1, 2], [3, 4]])\n    assert str(p) == \"(1 2)(3 4)\"\n    p = Permutation([[1, 2], [3, 4], [5]])\n    assert str(p) == \"(1 2)(3 4)(5)\"\n    p = Permutation([[1, 2]])\n    assert str(p) == \"(1 2)\"\n    p = Permutation([])\n    assert str(p) == \"()\"\n", "def test_unrank_nonlex():\n    # Test the unrank_nonlex method for a valid input\n    p = Permutation.unrank_nonlex(3, 5)\n    assert p.rank_nonlex() == 5\n\n    # Test the unrank_nonlex method for an invalid input\n    p = Permutation.unrank_nonlex(3, 10)\n    assert p.rank_nonlex() == 10\n\n    # Test the unrank_nonlex method with a negative input\n    p = Permutation.unrank_nonlex(3, -1)\n    assert p.rank_nonlex() == 0\n\n    # Test the unrank_nonlex method for an empty permutation\n    p = Permutation.unrank_nonlex(0, 0)\n    assert p.rank_nonlex() == 0\n\n    # Test the unrank_nonlex method for a permutation of size 1\n    p = Permutation.unrank_nonlex(1, 0)\n    assert p.rank_nonlex() == 0\n", "def test_applied_permutation_evaluate():\n    x = Symbol('x', integer=True)\n    a = Symbol('a')\n    b = Symbol('b')\n    assert unchanged(x - x, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(x + x, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(x - 1, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(1 - x, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(x * x, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(x / x, AppliedPermutation(Permutation([0, 1, 2]), x))\n    assert unchanged(a + AppliedPermutation(Permutation([0, 1, 2]), x), a + x)\n    assert unchanged(a * AppliedPermutation(Permutation([0, 1, 2]), x), a * x)\n    assert unchanged(a - AppliedPermutation(Permutation([0, 1, 2]), x), a - x)\n    assert unchanged(a / AppliedPermutation(Permutation([0, 1, 2]), x), a / x)\n    assert unchanged(AppliedPermutation(Permutation([0, 1, 2]), a) + x, a + x)\n    assert unchanged(AppliedPermutation(Permutation([0, 1, 2]), a) * x, a * x)\n    assert unchanged(AppliedPermutation(Permutation([0, 1, 2]), a) - x, a - x)\n    assert unchanged(AppliedPermutation(Permutation([0, 1, 2]), a) / x, a / x)\n    assert unchanged(a + AppliedPermutation(Permutation([0, 1, 2]), x) - a, x - a)\n    assert unchanged(AppliedPermutation(Permutation([0, 1, 2]), x) + AppliedPermutation(Permutation([0, 1, 2]), a), x + a)\n", "    def test_inversion_vector(self):\n        for i in range(10):\n            p = Permutation.unrank_lex(10, i)\n            assert p.inversion_vector() == p.inversion_vector(p.array_form)\n            assert p.inversion_vector(p.array_form) == [0]*9\n            for j in range(10):\n                p = Permutation.from_inversion_vector([0]*j + [1] + [0]*(9-j))\n                assert p.inversion_vector() == [1] + [0]*j + [0]*(9-j-1)\n"], "sample_1104": ["def test_StrPrinter_print_PolyElement():\n    p = StrPrinter()\n    # Test with a PolyElement object\n    poly = Poly(x**2 + 3*x + 2, x)\n    assert p._print_PolyElement(poly) == \"Poly(x**2 + 3*x + 2, x)\"\n", "def test_str_printing():\n    assert sstr(Eq(1, 2)) == 'Eq(1, 2)'\n    assert sstr(1 + 2) == '1 + 2'\n    assert sstr(1 - 2) == '-1 + 2'\n    assert sstr(1 * 2) == '1*2'\n    assert sstr(2 * (1 + 2)) == '2*(1 + 2)'\n    assert sstr(1 / 2) == '1/2'\n    assert sstr(2 ** 3) == '2**3'\n    assert sstr(2 ** (1/3)) == '2**(1/3)'\n    assert sstr(Abs(1)) == 'Abs(1)'\n    assert sstr(Catalan) == 'Catalan'\n    assert sstr(cos(1)) == 'cos(1)'\n    assert sstr(Derivative(x, x)) == 'Derivative(x, x)'\n    assert sstr(E) == 'E'\n    assert sstr(exp(1)) == 'exp(", "def test_str_printer_frozenset():\n    s = frozenset([1, 2, 3])\n    assert sstr(s) == \"{1, 2, 3}\"\n", "    def test_print_Dict(self):\n        # test printing a dictionary\n        d = {x: 1, y: 2, z: 3}\n        p = StrPrinter()\n        self.assertEqual(p._print(d), \"{x: 1, y: 2, z: 3}\")\n\n        # test printing a dictionary with a non-sympy key\n        d = {1: 1, y: 2, z: 3}\n        self.assertEqual(p._print(d), \"{1: 1, y: 2, z: 3}\")\n\n        # test printing a dictionary with a non-sympy value\n        d = {x: 1, y: 2, 3: 3}\n        self.assertEqual(p._print(d), \"{x: 1, y: 2, 3: 3}\")\n\n        # test printing an empty dictionary\n        d = {}\n        self.assertEqual(p._print(d), \"{}\")\n\n        # test printing a dictionary with a non-hashable key\n        with self.assertRaises(TypeError):\n            d = {x: 1, (y,): 2, z: 3}\n            p._print(d)\n", "def test_str_printer_classes():\n    assert sstr(True) == 'True'\n    assert sstr(False) == 'False'\n    assert sstr(Rational(1, 2)) == '1/2'\n    assert sstr(Rational(-1, 2)) == '-1/2'\n    assert sstr(Eq(1, 2)) == 'Eq(1, 2)'\n    assert sstr(Ne(1, 2)) == 'Ne(1, 2)'\n    assert sstr(Abs(2)) == 'Abs(2)'\n    assert sstr(Catalan(1)) == 'Catalan'\n    assert sstr(Derivative(x, x)) == 'Derivative(x, x)'\n    assert sstr(Derivative(x, x, x)) == 'Derivative(x, x, x)'\n    assert sstr(EulerGamma(1)) == 'EulerGamma'\n    assert sstr(GoldenRatio(1)) == 'GoldenRatio'\n    assert sstr(TribonacciConstant(1)) == 'TribonacciConstant'\n    assert sstr(Interval(0, 1)) == 'Interval{m}({0}, {1})'\n    assert sstr(Interval(0, oo)) == 'Interval{Lopen}({0}, oo)'\n    assert sstr(Interval(oo, 1)) == 'Interval{Ropen}(oo, {1})'\n    assert sstr(Interval(0, 1, left_open=False, right_open=True)) == 'Interval{m}({0}, {1})'\n    assert sstr(Interval(0, 1, left_open=True, right_open=False)) == 'Interval{Lopen}({0}, {1})'\n    assert sstr(Interval(0, 1, left_open=True, right_open=True)) == 'Interval{Lopen}({0}, {1})'\n    assert sstr(Interval(0, 1, left_open=False, right_open=False)) == 'Interval{m}({0}, {1})'\n    assert sstr(Interval(0, 1, left_open=True, right_open=True)) == 'Interval{Lopen}({0}, {1})'\n    assert sstr(AccumBounds(1, 2)) == 'AccumBounds(1, 2)'\n    assert sstr(Poly([1,"], "sample_356": ["    def test_alter_model_options(self):\n        before_states = [\n            self.author_with_options,\n        ]\n        after_states = [\n            self.author_with_options,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n", "    def test_altered_unique_together_with_changed_field(self):\n        # Arrange\n        before_states = [\n            self.author_with_book,\n        ]\n        after_states = [\n            self.author_with_book_order_wrt,\n        ]\n        questioner = mock.Mock()\n\n        # Act\n        changes = self.get_changes(before_states, after_states, questioner)\n\n        # Assert\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", unique_together={(\"author\", \"title\")})\n", "    def test_renamed_through_field(self):\n        before_states = [\n            self.author_with_renamed_m2m_through,\n            self.contract,\n        ]\n        after_states = [\n            self.author_with_m2m_through,\n            self.contract_renamed,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\", \"AlterField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name='contract', new_name='deal')\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name='publishers', new_name='publishers')\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, name='name', db_column='name')\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 1, name='field_name', db_column='field_name')\n", "    def test_rename_field_options(self):\n        from_state = self.make_project_state([\n            self.author_name,\n            self.book,\n        ])\n        to_state = self.make_project_state([\n            self.author_name_longer,\n            self.book,\n        ])\n        changes = self.get_changes(from_state.models, to_state.models)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name='name', old_name='name')\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterField\", \"AddField\"])\n        self.assertOperationFieldAttributes(changes, \"testapp\", 1, 0, name='name', field__max_length=400)\n        self.assertOperationFieldAttributes(changes, \"testapp\", 1, 1, name='name', field__null=True)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"None\"])\n", "    def test_alter_m2m_through_table_default(self):\n        \"\"\"Test that an AlterModel operation is created when the M2M through model changes.\"\"\"\n        before_state = self.make_project_state([\n            self.author_unmanaged_custom_pk,\n            self.contract,\n        ])\n        after_state = self.make_project_state([\n            self.author_unmanaged_custom_pk,\n            self.contract_renamed,\n        ])\n        changes = self.get_changes(before_state.models, after_state.models)\n        self.assertEqual(\n            changes,\n            {\n                'testapp': [\n                    operations.CreateModel(\n                        name='Author',\n                        fields=[\n                            ('id', models.IntegerField(primary_key=True)),\n                            ('pk_field', models.IntegerField(primary_key=True)),\n                        ],\n                    ),\n                    operations.CreateModel(\n                        name='Contract',\n                        fields=[\n                            ('id', models.AutoField(primary_key=True)),\n                            ('author', models.ForeignKey('testapp.Author', models.CASCADE)),\n                            ('publisher', models.ForeignKey('testapp.Publisher', models.CASCADE)),\n                        ],\n                    ),\n                    operations.AlterModel(\n                        name='Contract',\n                        table='deal',\n                        options={\n                            'managed': False,\n                        },\n                    ),\n                    operations.AlterModel(\n                        name='Author',\n                        table='author_one',\n                        options={\n                            'managed': True,\n                        },\n                    ),\n                ],\n            },\n        )\n"], "sample_149": ["    def test_username_field_is_unique(self):\n        # Test that the username field is unique when AUTHENTICATION_BACKENDS\n        # includes ModelBackend.\n        self.assertEqual(check_user_model(), [])\n", "    def test_username_in_required_fields(self):\n        with self.assertRaises(AssertionError):\n            CustomUserUsernameInRequiredFields().full_clean()\n        ", "    def test_check_models_permissions_checks_model_permissions(self):\n        self.assertCountEqual(\n            check_models_permissions(),\n            [\n                checks.Error(\n                    \"'CustomUserNonUniqueUsername.username' must be unique because it is named as the 'USERNAME_FIELD'.\",\n                    obj=CustomUserNonUniqueUsername,\n                    id='auth.E003',\n                ),\n            ]\n        )\n", "    def test_user_model_username_field_is_unique(self):\n        with self.assertChecksRun:\n            errors = check_user_model()\n            self.assertEqual(len(errors), 3)\n            self.assertIsInstance(errors[0], checks.Error)\n            self.assertEqual(errors[0].id, 'auth.E003')\n            self.assertIsInstance(errors[1], checks.Warning)\n            self.assertEqual(errors[1].id, 'auth.W004')\n            self.assertIsInstance(errors[2], checks.Critical)\n            self.assertEqual(errors[2].id, 'auth.C009')\n", "    def test_check_models_permissions(self):\n        # Test model with model name too long to affect builtin permission codenames\n        errors = check_models_permissions()\n        self.assertEqual(len(errors), 2)\n        self.assertEqual(errors[0].obj, CustomUser)\n        self.assertEqual(errors[0].id, 'auth.E011')\n        self.assertEqual(errors[1].obj, CustomUser)\n        self.assertEqual(errors[1].id, 'auth.E008')\n"], "sample_450": ["    def test_get_admin_log_without_limit(self):\n        # Test get_admin_log without limit argument\n        response = self.client.get(reverse(\"admin:index\"))\n        template = Template(\"{% get_admin_log as admin_log %}\")\n        context = Context({\"log_entries\": LogEntry.objects.all()})\n        rendered_template = template.render(context)\n        self.assertIn(\"admin_log\", context)\n        self.assertIsInstance(context[\"admin_log\"], QuerySet)\n        self.assertEqual(context[\"admin_log\"].count(), 1)  # Should be only one log entry\n", "def test_get_admin_log_with_limit_and_context_variable_for_user(self):\n    response = self.client.get(reverse(\"admin:index\"))\n    self.assertEqual(response.status_code, 200)\n    AdminLogNode(limit=\"10\", varname=\"admin_log\", user=\"user\").render({\"log_entries\": LogEntry.objects.all()})\n    self.assertQuerysetEqual(LogEntry.objects.all(), AdminLogNode(limit=\"10\", varname=\"admin_log\", user=\"user\").render({\"log_entries\": LogEntry.objects.all()}))\n", "def test_get_admin_log_node_render(self):\n    # Test rendering with valid data\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.all()\n    node = get_admin_log(10, \"log_entries\", \"user\")\n    node.render(request.context)\n    self.assertEqual(request.context[\"log_entries\"], LogEntry.objects.all()[:10])\n\n    # Test rendering with user ID\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.all()\n    node = get_admin_log(10, \"log_entries\", \"123\")\n    node.render(request.context)\n    self.assertEqual(request.context[\"log_entries\"], LogEntry.objects.filter(user__pk=123)[:10])\n\n    # Test rendering with a User object\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.all()\n    request.context[\"user\"] = User.objects.get(pk=123)\n    node = get_admin_log(10, \"log_entries\")\n    node.render(request.context)\n    self.assertEqual(request.context[\"log_entries\"], LogEntry.objects.filter(user__pk=123)[:10])\n\n    # Test rendering with invalid data\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.all()\n    with self.assertRaises(template.TemplateSyntaxError):\n        get_admin_log(\"not_an_integer\", \"log_entries\")\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        get_admin_log(10, \"not_as\", \"log_entries\")\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        get_admin_log(10, \"log_entries\", \"for_user\")\n\n    with self.assertRaises(template.TemplateSyntaxError):\n        get_admin_log(10, \"log_entries\", \"for_user\", \"extra_argument\")\n\n    # Test rendering with empty log entries\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.none()\n    node = get_admin_log(10, \"log_entries\")\n    node.render(request.context)\n    self.assertEqual(request.context[\"log_entries\"], [])\n\n    # Test rendering with no limit\n    request = self.client.get(reverse(\"admin:index\"))\n    request.context[\"log_entries\"] = LogEntry.objects.all()\n    node = get_admin_log(None, \"log_entries\")\n    node.render(request.context)\n    self.assertEqual(request.context[\"log_entries\"], LogEntry.objects.all())\n", "def test_get_admin_log_limit_and_user(self):\n    # Test the get_admin_log tag with limit and user\n    response = self.client.get(reverse('admin:article_article_changelist'))\n    self.assertContains(response, '{% get_admin_log 5 as log for_user super %}')\n    self.client.post(\n        reverse('admin:article_article_changelist'),\n        {'action': 'delete_selected', '_selected_action': [self.a1.pk]}\n    )\n    response = self.client.get(reverse('admin:article_article_changelist'))\n    self.assertQuerysetEqual(response.context['log_entries'], [self.a1])\n    # Reset the log\n    LogEntry.objects.all().delete()\n", "    def test_admin_log_node_rendering_with_limit_and_user(self):\n        \"\"\"\n        Test that the AdminLogNode correctly renders the admin log for a given user\n        with a specified limit.\n        \"\"\"\n        # Create additional log entries\n        content_type_pk = ContentType.objects.get_for_model(Article).pk\n        LogEntry.objects.log_action(\n            self.user.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed something else\",\n        )\n        LogEntry.objects.log_action(\n            self.user.pk,\n            content_type_pk,\n            self.a1.pk,\n            repr(self.a1),\n            CHANGE,\n            change_message=\"Changed something again\",\n        )\n\n        # Test with a limit of 2\n        response = self.client.get(reverse(\"admin:index\"))\n        token = template.loader.get_template(\"admin_log_test.html\").render({\"log_entries\": LogEntry.objects.all()})\n        parser = template.Parser(token)\n        node = get_admin_log(parser, token)\n        admin_log_node = AdminLogNode(\"2\", \"admin_log\", self.user)\n        admin_log_node.render({\"log_entries\": LogEntry.objects.all()})\n\n        self.assertEqual(admin_log_node.limit, 2)\n        self.assertEqual(admin_log_node.varname, \"admin_log\")\n        self.assertEqual(admin_log_node.user, self.user)\n\n        # Test that the rendered admin log has the correct number of entries\n        context = {}\n        admin_log_node.render(context)\n        self.assertEqual(len(context[\"admin_log\"]), 2)\n\n        # Test with a limit of 1\n        admin_log_node = AdminLogNode(\"1\", \"admin_log\", self.user)\n        admin_log_node.render(context)\n        self.assertEqual(len(context[\"admin_log\"]), 1)\n"], "sample_1172": ["def test_solve_poly_system_empty_equation():\n    # Test that an empty equation is handled correctly\n    polys, opt = parallel_poly_from_expr([0])\n    result = solve_poly_system(polys, x)\n    assert result == []\n", "def test_solve_generic_zero_dimensional_system():\n    from sympy.solvers.polysys import solve_generic\n    from sympy.abc import x, y\n    from sympy.polys import Poly\n\n    # Test a zero-dimensional system with 3 variables and 3 equations\n    a = Poly(x - 1, x, y, domain='ZZ')\n    b = Poly(y - 1, x, y, domain='ZZ')\n    c = Poly(x - y, x, y, domain='ZZ')\n    polys = [a, b, c]\n    gens = [x, y]\n    result = solve_generic(polys, gens)\n    assert len(result) == 1\n    assert result[0] == (1, 1)\n\n    # Test a zero-dimensional system with 2 variables and 2 equations\n    a = Poly(x - 2, x, y, domain='ZZ')\n    b = Poly(y - 3, x, y, domain='ZZ')\n    polys = [a, b]\n    gens = [x, y]\n    result = solve_generic(polys, gens)\n    assert len(result) == 1\n    assert result[0] == (2, 3)\n\n    # Test an empty list of polynomials\n    polys = []\n    gens = [x, y]\n    result = solve_generic(polys, gens)\n    assert result is None\n\n    # Test a list of polynomials with no variables\n    polys = [Poly(x**2 - 1, x, domain='ZZ')]\n    gens = []\n    result = solve_generic(polys, gens)\n    assert result is None\n\n    # Test a system with non-zero-dimensional solution set\n    a = Poly(x**2 - 1, x, y, domain='ZZ')\n    b = Poly(y**2 - 1, x, y, domain='ZZ')\n    polys = [a, b]\n    gens = [x, y]\n    result = solve_generic(polys, gens)\n    assert result is None\n\n    # Test a system with non-polynomial expressions\n    a = Poly(x**2 - 1, x, y, domain='QQ')\n    b = Poly(x**2 + y, x, y, domain='QQ')\n    polys = [a, b]\n    gens = [x, y]\n    result = solve_generic(polys, gens)\n    assert result is None\n", "def test_solve_poly_system_B():\n    # Test solve_poly_system for a system of polynomials with two equations\n    # and three variables.\n    from sympy.abc import w, x, y\n    from sympy import solve_poly_system\n\n    result = solve_poly_system([x - y + w, y**2 + x + 1, w**2 + y - 1], x, y, w)\n    assert result == [(1, 0, 0), (1, -1, 1)]\n", "def test_solve_biquadratic_no_solutions():\n    from sympy import symbols, Eq, solve\n\n    x, y = symbols('x y')\n    f = Eq(x**2 + y**2, 2)\n    g = Eq(x**2 - y**2, 2)\n    solution = solve_poly_system([f, g], x, y)\n    assert solution == [], \"solve_poly_system() should return an empty list for no solutions\"\n\n", "def test_solve_poly_system_rational_domain():\n    from sympy.polys import Poly, QQ\n    from sympy.abc import x, y, z\n\n    # Test that solve_poly_system returns the correct solution over a rational domain\n    F = [x**2 + y**2 - 1, x + y - 1]\n    result = solve_poly_system(F, x, y, domain='QQ')\n    expected = [(-1, 0), (0, 1), (1, 0)]\n    assert result == expected\n\n    # Test that solve_poly_system returns the correct solution over a rational domain when there are repeated roots\n    F = [x**2 + y**2 - 1, x + y - 1, x**2 - y]\n    result = solve_poly_system(F, x, y, domain='QQ')\n    expected = [(-sqrt(2), sqrt(2)), (sqrt(2), sqrt(2)), (-sqrt(2), -sqrt(2)),\n                (sqrt(2), -sqrt(2)), (1, 0), (0, 1)]\n    assert result == expected\n\n    # Test that solve_poly_system returns the correct solution over a rational domain when the system is inconsistent\n    F = [x**2 + y**2 - 1, x + y + 1]\n    result = solve_poly_system(F, x, y, domain='QQ')\n    assert result == []\n"], "sample_165": ["    def test_model_form_initial(self):\n        form = ChoiceModelForm(initial={'choice': 'option1'})\n        self.assertEqual(form.initial, {'choice': 'option1'})\n", "    def test_model_choice_field_invalid_choice(self):\n        class InvalidChoiceModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ('choice')\n                error_messages = {\n                    'invalid_choice': 'Custom invalid choice error message'\n                }\n\n        form = InvalidChoiceModelForm(data={'choice': 'invalid_choice'})\n        expected = {'choice': ['Custom invalid choice error message']}\n        self.assertFormErrors(expected, form.is_valid)\n", "    def test_choice_field_valid_choice(self):\n        # Ensure a valid choice raises no exception\n        form = ChoiceForm({'choice': 'A'})\n        form.is_valid()\n        self.assertTrue(form.is_valid())\n", "    def test_modelchoicefield_choice_str(self):\n        # Test that the choice string is generated correctly for ModelChoiceField\n        class TestModel(models.Model):\n            name = models.CharField(max_length=100)\n        \n        form = TestModelForm({'name': 'test'})\n        field = ModelChoiceField(TestModel.objects.all(), to_field_name='name')\n        self.assertEqual(str(field), 'name')\n", "    def test_modelchoicefield_custom_label(self):\n        field = ModelChoiceField(ChoiceModel.objects.all())\n        instance1, instance2 = ChoiceModel.objects.all()[:2]\n        self.assertEqual(field.label_from_instance(instance1), 'Instance 1')\n        self.assertEqual(field.label_from_instance(instance2), 'Instance 2')\n        self.assertEqual(field.label_from_instance(None), '')\n"], "sample_372": ["    def test_default_namespace(self):\n        resolver = get_resolver()\n        match = resolver.resolve('/default/inner/42/37/')\n        self.assertEqual(match.func, views.empty_view)\n        self.assertEqual(match.args, ())\n        self.assertEqual(match.kwargs, {'arg1': '42', 'arg2': '37'})\n", "    def test_reverse_dict(self):\n        urlconf = URLResolver(RegexPattern(r'^/'), 'urlpatterns_reverse')\n        urlconf.reverse_dict = {\n            'en': {\n                'test1': [(('/test1/', 'test_view', (), {}),)],\n                'test2': [(('/test2/', 'test_view', (), {}),)]\n            },\n            'fr': {\n                'test1': [(('/test1/', 'test_view', (), {}),)],\n                'test2': [(('/test2/', 'test_view', (), {}),)]\n            }\n        }\n        self.assertEqual(urlconf.reverse_dict['en']['test1'][0][0], '/test1/')\n        self.assertEqual(urlconf.reverse_dict['fr']['test1'][0][0], '/test1/')\n        self.assertEqual(urlconf.reverse_dict['en']['test2'][0][0], '/test2/')\n        self.assertEqual(urlconf.reverse_dict['fr']['test2'][0][0], '/test2/')\n        self.assertEqual(len(urlconf.reverse_dict['en']['test1']), 1)\n        self.assertEqual(len(urlconf.reverse_dict['fr']['test1']), 1)\n        self.assertEqual(len(urlconf.reverse_dict['en']['test2']), 1)\n        self.assertEqual(len(urlconf.reverse_dict['fr']['test2']), 1)\n", "    def test_resolver_match_args_and_kwargs(self):\n        resolver_match = ResolverMatch(lambda: None, (1, 2), {'a': 'b'})\n        self.assertEqual(resolver_match.args, (1, 2))\n        self.assertEqual(resolver_match.kwargs, {'a': 'b'})\n        self.assertEqual(resolver_match['0'], 1)\n        self.assertEqual(resolver_match[1], 2)\n        self.assertEqual(resolver_match['args'], (1, 2))\n        self.assertEqual(resolver_match['kwargs'], {'a': 'b'})\n", "    def test_thread_safety(self):\n        # Simulate concurrent threads populating the resolver.\n        resolver = get_resolver()\n        populate_threads = []\n\n            resolver._populate()\n        for _ in range(10):\n            thread = threading.Thread(target=populate_resolver)\n            thread.start()\n            populate_threads.append(thread)\n        for thread in populate_threads:\n            thread.join()\n\n        # We check that the resolver has been populated.\n        self.assertIsNotNone(resolver._reverse_dict)\n\n        # We check that the _populated attribute is set.\n        self.assertTrue(resolver._populated)\n\n        # We check that the _callback_strs attribute contains the correct paths.\n        self.assertIn('urlpatterns_reverse.views.empty_view', resolver._callback_strs)\n        self.assertIn('urlpatterns_reverse.views.view_class_instance', resolver._callback_strs)\n", "    def test_reverse_namespaced_view(self):\n        response = reverse('normal-view')\n        self.assertEqual(response, '/normal/42/37/')\n"], "sample_1111": ["    def test_textplot_str_one_symbol(self):\n        # Test that textplot_str raises an error when the expression has multiple symbols\n        with self.assertRaises(ValueError):\n            textplot_str(log(x**2 + y**2, 2), 0, 10)\n", "def test_textplot_str_rescale():\n    x = Symbol('x')\n    expr = sin(x)\n    a = 0\n    b = S.Pi\n    W = 15\n    H = 11\n\n    # Test case with no valid values\n    y = [None] * W\n    expected_output = [\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  |\",\n        \"  0.000000  |\"\n    ]\n    result = list(textplot_str(expr, a, b, W, H))\n    assert result == expected_output\n\n    # Test case with all valid values\n    y = [float(i) for i in linspace(a, b, W)]\n    expected_output = [\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \" 0.000000 |\"\n    ]\n    result = list(textplot_str(expr, a, b, W, H))\n    assert result == expected_output\n\n    # Test case with single valid value\n    y = [float(a)] + [None] * (W-1)\n    expected_output = [\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \"0.000000 |\",\n        \" 0.000000 |\"\n    ]\n    result = list(textplot_str(expr, a, b, W, H))\n    assert result == expected_output\n\n    # Test case with inf and nan values\n    y = [float('inf')] + [None] * (W-1", "def test_textplot_str_valid_expression():\n    t = Symbol('t')\n    # Test a simple sin function\n    for line in textplot_str(sin(t), 0, 15, W=55, H=21):\n        assert isinstance(line, str), f\"Expected a string, but got {type(line)}\"\n    # Test a log function\n    for line in textplot_str(log(t), 1, 10, W=55, H=21):\n        assert isinstance(line, str), f\"Expected a string, but got {type(line)}\"\n    # Test a sqrt function\n    for line in textplot_str(sqrt(t), 0, 10, W=55, H=21):\n        assert isinstance(line, str), f\"Expected a string, but got {type(line)}\"\n    # Test a more complex expression\n    for line in textplot_str(sin(t)*log(t), 0, 15, W=55, H=21):\n        assert isinstance(line, str), f\"Expected a string, but got {type(line)}\"\n", "def test_textplot_str_with_real_symbol():\n    x = Symbol('x')\n    expr = x**2 + 2*x + 1\n    a, b = -5, 10\n    result = [line for line in textplot_str(expr, a, b, W=21, H=21)]\n    expected = [\n        '          |                                ...',\n        '           |                               .',\n        '           |                              .',\n        '           |                             .',\n        '           |                           .',\n        '           |                        .',\n        '           |                        .',\n        '           |                      .',\n        '           |                     .',\n        '           |                 .',\n        '           |                .',\n        '           |               .',\n        '           |            .',\n        '           |        .',\n        '           |       .',\n        '           |      .',\n        '           |     .',\n        '           |   .',\n        '           |  .',\n        '           | .',\n        '           |',\n        '           0                              5'\n    ]\n    assert result == expected\n", "def test_textplot_str_invalid_expression():\n    from sympy import Symbol\n    expr = Symbol('x')**2 + 1  # This should raise ValueError\n    a = 0\n    b = 10\n    W = 55\n    H = 21\n    try:\n        list(textplot_str(expr, a, b, W, H))\n        assert False, \"Expected ValueError to be raised\"\n    except ValueError as e:\n        assert str(e) == \"The expression must have a single variable. (Got {'x'}\"\n"], "sample_1181": ["def test_ScipyPrinter_integer_power():\n    from sympy import sin, cos, tan, asin, acos, atan, asinh, acosh, atanh\n    sp = SciPyPrinter()\n    assert sp.doprint(sin(x**2)) == \"sin(cupiedot(x, x))\"\n    assert sp.doprint(cos(x**2)) == \"cos(cupiedot(x, x))\"\n    assert sp.doprint(tan(x**2)) == \"tan(cupiedot(x, x))\"\n    assert sp.doprint(asin(x**2)) == \"arcsin(cupiedot(x, x))\"\n    assert sp.doprint(acos(x**2)) == \"arccos(cupiedot(x, x))\"\n    assert sp.doprint(atan(x**2)) == \"arctan(cupiedot(x, x))\"\n    assert sp.doprint(asinh(x**2)) == \"arcsinh(cupiedot(x, x))\"\n    assert sp.doprint(acosh(x**2)) == \"arccosh(cupiedot(x, x))\"\n    assert sp.doprint(atanh(x**2)) == \"arctanh(cupiedot(x, x))\"\n", "def test_SparseRepMatrix():\n    from sympy.matrices.expressions.sparsematrix import SparseMatrix\n    from sympy.core.mod import Mod\n    from sympy.core.relational import Equality\n    m = SparseMatrix(5, 5, [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]])\n\n    p = NumPyPrinter()\n    assert p.doprint(m) == \"scipy.sparse.coo_matrix(([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], (5, 5)), shape=(5, 5))\"\n", "def test_CuPyPrinter():\n    c = NumPyPrinter()\n    s = SciPyPrinter()\n    cu = CuPyPrinter()\n\n    # Test CuPyPrinter\n    # Test cu with x, y, z\n    m = MatrixSymbol('x', 3, 3)\n    m2 = MatrixSymbol('y', 3, 3)\n    m3 = MatrixSymbol('z', 3, 3)\n\n    cu_code = cu.doprint(m * m2 + m3)\n    assert cu_code == \"cupy.add(cupy.matmul(cupy.array([['x1', 'x2', 'x3'], ['x4', 'x5', 'x6'], ['x7', 'x8', 'x9']], dtype=object), cupy.array([['y1', 'y2', 'y3'], ['y4', 'y5', 'y6'], ['y7', 'y8', 'y9']], dtype=object)), cupy.array([['z1', 'z2', 'z3'], ['z4', 'z5', 'z6'], ['z7', 'z8', 'z9']], dtype=object))\"\n\n    # Test CuPyPrinter with x, y, z when module is 'numpy'\n    cu = CuPyPrinter(module='numpy')\n    cu_code = cu.doprint(m * m2 + m3)\n    assert cu_code == \"numpy.add(numpy.matmul(numpy.array([['x1', 'x2', 'x3'], ['x4', 'x5', 'x6'], ['x7', 'x8', 'x9']], dtype=object), numpy.array([['y1', 'y2', 'y3'], ['y4', 'y5', 'y6'], ['y7', 'y8', 'y9']], dtype=object)), numpy.array([['z1', 'z2', 'z3'], ['z4', 'z5', 'z6'], ['z7', 'z8', 'z9']], dtype=object))\"\n\n    # Test CuPyPrinter with a Piecewise expression\n    x = symbols('x')\n    expr = Piecewise((1, x > 0), (0, True))\n    cu_code = cu.doprint(expr)\n    assert cu_code == 'cupy.select(cupy.greater(x, ", "def test_CuPyPrinter():\n    # Test CuPyPrinter with a few NumPy-specific functions and constants\n    CuPyPrinter._module = 'cupy'\n    CuPyPrinter._kf = _cupy_known_functions\n    CuPyPrinter._kc = _cupy_known_constants\n\n    expr = 2 * x**2 + 3*x - 4\n    assert CuPyPrinter().doprint(expr) == \"cupy.add(cupy.multiply(2.0, cupy.square(x)), cupy.multiply(3.0, x), -4.0)\"\n\n    # Test CuPyPrinter with a matrix\n    mat = MatrixSymbol('mat', 2, 2, (x, y))\n    assert CuPyPrinter().doprint(mat) == \"cupy.array([[x, y], [x, y]])\"\n\n    # Test CuPyPrinter with a block matrix\n    block_mat = BlockMatrix([[mat, 1], [1, mat]])\n    assert CuPyPrinter().doprint(block_mat) == \"cupy.block([[cupy.array([[x, y], [x, y]]), cupy.array([1, 1])], cupy.array([1, 1]), cupy.array([[x, y], [x, y]]]])\"\n\n    # Test CuPyPrinter with a piecewise function\n    pw_expr = Piecewise((x**2, x < 0), (x + 1, x >= 0))\n    assert CuPyPrinter().doprint(pw_expr) == \"cupy.select((x < 0), (cupy.square(x)), (x >= 0), (cupy.add(x, 1)), default=cupy.nan)\"\n\n    # Test CuPyPrinter with a vectorized function\n    vec_expr = logaddexp(x, y)\n    assert CuPyPrinter().doprint(vec_expr) == \"cupy.logaddexp(x, y)\"\n\n    # Test CuPyPrinter with a sparse matrix\n    sparse_mat = eye(5)\n    assert CuPyPrinter().doprint(sparse_mat) == \"cupy.sparse.coo_matrix(((0,), ((0,), (0,), (0,), (0,), (0,)), (5, 5)))\"\n\n    # Test CuPyPrinter with an array\n    array_expr = Array([x, y, z])\n    assert CuPyPrinter().doprint", "def test_cupy_printer():\n    from sympy import Matrix\n\n    # Test CuPy printing of matrix\n    mat = Matrix([[1, 2], [3, 4]])\n    cupy = CuPyPrinter()\n    print(cupy.doprint(mat))\n    assert False  # This should fail due to the lack of matrix printing support in CuPy\n"], "sample_407": ["    def setUpTestData(cls):\n        # Create a few Reporters.\n        cls.r = Reporter(first_name=\"John\", last_name=\"Smith\", email=\"john@example.com\")\n        cls.r.save()\n        cls.r2 = Reporter(\n            first_name=\"Paul\", last_name=\"Jones\", email=\"paul@example.com\"\n        )\n        cls.r2.save()\n        # Create an Article.\n        cls.a = Article(\n            headline=\"This is a test\",\n            pub_date=datetime.date(2005, 7, 27),\n            reporter=cls.r,\n        )\n        cls.a.save()\n", "    def setUpTestData(cls):\n        # Create a few Reporters.\n        cls.r = Reporter(first_name=\"John\", last_name=\"Smith\", email=\"john@example.com\")\n        cls.r.save()\n        cls.r2 = Reporter(\n            first_name=\"Paul\", last_name=\"Jones\", email=\"paul@example.com\"\n        )\n        cls.r2.save()\n", "    def setUpTestData(cls):\n        # Create a few Reporters.\n        cls.r = Reporter(first_name=\"John\", last_name=\"Smith\", email=\"john@example.com\")\n        cls.r.save()\n        cls.r2 = Reporter(\n            first_name=\"Paul\", last_name=\"Jones\", email=\"paul@example.com\"\n        )\n        cls.r2.save()\n        # Create an Article.\n        cls.a = Article(\n            headline=\"This is a test\",\n            pub_date=datetime.date(2005, 7, 27),\n            reporter=cls.r,\n        )\n        cls.a.save()\n", "    def setUpTestData(cls):\n        # Create some models.\n        cls.c = Country(name=\"Canada\")\n        cls.c.save()\n        cls.c2 = Country(name=\"USA\")\n        cls.c2.save()\n        cls.d = District(country=cls.c)\n        cls.d.save()\n        cls.s = School(district=cls.d)\n        cls.s.save()\n        cls.s2 = School(district=cls.c2)\n        cls.s2.save()\n        cls.st = Student(school=cls.s, name=\"John\", age=12)\n        cls.st.save()\n        cls.st2 = Student(school=cls.s2, name=\"Jane\", age=12)\n        cls.st2.save()\n", "    def test_nested_foreign_key_foreign_key(self):\n        # Create a ParentStringPrimaryKey.\n        parent = ParentStringPrimaryKey(\n            name=\"Parent\", string_primary_key_field=\"ParentID\"\n        )\n        parent.save()\n        \n        # Create a Child.\n        child = Child(\n            name=\"Child\", parent=parent, string_primary_key_field=\"ChildID\"\n        )\n        child.save()\n\n        # Test that the child's parent is correct.\n        self.assertEqual(child.parent.name, \"Parent\")\n\n        # Test that the child's parent's string primary key is correct.\n        self.assertEqual(child.parent.string_primary_key_field, \"ParentID\")\n\n        # Test that creating another child with the same parent ID doesn't raise an error.\n        child2 = Child(name=\"Child2\", parent=parent, string_primary_key_field=\"ChildID2\")\n        child2.save()\n\n        # Test that the parent's string primary key is not changed by creating another child.\n        self.assertEqual(parent.string_primary_key_field, \"ParentID\")\n"], "sample_1197": ["    def test_extend(self):\n        # Test that extend method returns a new UnitSystem instance\n        si = UnitSystem([\"m\", \"kg\", \"s\"], [\"m/s\"])\n        new_system = si.extend([\"V\"], units=[\"kg m/s^2\"])\n        assert isinstance(new_system, UnitSystem)\n        assert new_system._base_units == (\"m\", \"kg\", \"s\", \"V\")\n        assert new_system._units == (\"m\", \"kg\", \"s\", \"m/s\", \"kg m/s^2\")\n", "def test_extend_unit_system():\n    u = SI.extend((meter, second))\n    assert u.name == \"SI\"\n    assert u.dim == 2\n    assert u._base_units == (meter, second)\n    assert len(u._derived_units) == 4  # e.g., kilogram, second, etc.\n    assert len(u._derived_units) == 4\n    assert u.get_dimension_system().is_consistent\n    assert u.get_units_non_prefixed() == set(u._units)\n", "def test_extend_unit_system():\n    si_system = SI\n    new_unit_system = si_system.extend(\n        base=[length],\n        units=[kilometer, meter],\n        derived_units={Dimension(time): Quantity(\"second\", si_system.get_dimension_system().get_quantity_dimension(meter))}\n    )\n    assert new_unit_system.get_dimension_system() == si_system.get_dimension_system()\n    assert new_unit_system._units == (kilometer, meter)\n    assert new_unit_system._derived_units == {\n        Dimension(time): Quantity(\"second\", si_system.get_dimension_system().get_quantity_dimension(meter))\n    }\n    assert new_unit_system.get_units_non_prefixed() == set([kilometer, meter])\n    assert new_unit_system.dim == 3\n    assert new_unit_system.is_consistent == si_system.is_consistent\n    assert new_unit_system.get_dimensional_expr(kilometer) == meter\n", "def test_get_dimensional_expr():\n    m = SI.get_quantity_dimension(meter)\n    length = m.get_dimension_system().get_dimension(length)\n    assert length == Dimension(1, length)\n\n    m = SI.get_quantity_dimension(meter ** 2)\n    length = m.get_dimension_system().get_dimension(length)\n    assert length ** 2 == Dimension(2, length)\n\n    m = SI.get_quantity_dimension(second)\n    time = m.get_dimension_system().get_dimension(time)\n    assert time == Dimension(1, time)\n\n    m = SI.get_quantity_dimension(second ** -1)\n    time = m.get_dimension_system().get_dimension(time)\n    assert time ** (-1) == Dimension(-1, time)\n\n    m = SI.get_quantity_dimension(joule)\n    energy = m.get_dimension_system().get_dimension(energy)\n    assert energy == Dimension(1, energy)\n\n    m = SI.get_quantity_dimension(meter * kilogram * second ** -2)\n    length = m.get_dimension_system().get_dimension(length)\n    mass = m.get_dimension_system().get_dimension(mass)\n    time = m.get_dimension_system().get_dimension(time)\n    assert length * mass / time ** 2 == Dimension(1, length) * Dimension(1, mass) / Dimension(2, time)\n\n    m = SI.get_quantity_dimension(joule / (meter ** 2))\n    energy = m.get_dimension_system().get_dimension(energy)\n    length = m.get_dimension_system().get_dimension(length)\n    assert energy / length ** 2 == Dimension(1, energy) / Dimension(2, length)\n\n    m = SI.get_quantity_dimension(Derivative(length, time))\n    time = m.get_dimension_system().get_dimension(time)\n    length = m.get_dimension_system().get_dimension(length)\n    assert time ** (-1) * length == time ** (-1) * length\n\n    m = SI.get_quantity_dimension(Function(\"f\", meter))\n    length = m.get_dimension_system().get_dimension(length)\n    assert length == length\n\n    m = SI.get_quantity_dimension(Function(\"f\", meter ** 2))\n    length = m.get_dimension_system().get_dimension(length)\n    assert length ** 2 == length ** 2\n\n    m = SI.get_quantity_dimension(Function(\"f\", second))\n    time = m.get_dimension_system().get_dimension(time)\n    assert time == time\n\n    m = SI.get_quantity_dimension(Function(\"f\", second ** -1))\n    time = m.get", "def test_get_quantity_dimension_non_supported_unit():\n    u = UnitSystem.get_unit_system(\"SI\")\n    assert u.get_quantity_dimension(bit).is_dimensionless()\n    assert u.get_quantity_dimension(ohm).dimension == energy/((current**2)*time)\n    assert u.get_quantity_dimension(kilogram).dimension == mass\n"], "sample_803": ["def test_coverage_error():\n    \"\"\"Test of coverage_error.\"\"\"\n    y_true = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.9, 0.4, 0.8], [0.7, 0.2, 0.3], [0.1, 0.5, 0.6]])\n    assert_almost_equal(coverage_error(y_true, y_score), 0.85, decimal=2)\n\n    # Test with no positive labels\n    y_true = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n    y_score = np.array([[0.9, 0.4, 0.8], [0.7, 0.2, 0.3], [0.1, 0.5, 0.6]])\n    assert_almost_equal(coverage_error(y_true, y_score), 0.0, decimal=2)\n\n    # Test with no negative labels\n    y_true = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    y_score = np.array([[0.9, 0.4, 0.8], [0.7, 0.2, 0.3], [0.1, 0.5, 0.6]])\n    assert_almost_equal(coverage_error(y_true, y_score), 1.0, decimal=2)\n\n    # Test with sample weights\n    y_true = np.array([[1, 0, 1], [1, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.9, 0.4, 0.8], [0.7, 0.2, 0.3], [0.1, 0.5, 0.6]])\n    sample_weight = np.array([1.0, 2.0, 3.0])\n    assert_almost_equal(coverage_error(y_true, y_score, sample_weight), 0.85, decimal=2)\n\n    # Test with sparse data\n    y_true_sparse = csr_matrix(np.array([[1, 0, 1], [", "def test_coverage_error():\n    \"\"\"Test the `coverage_error` function\"\"\"\n    y_true = np.array([[0, 1, 1], [0, 0, 1], [1, 0, 0]])\n    y_score = np.array([[0.4, 0.6, 0.8], [0.1, 0.2, 0.9], [0.9, 0.1, 0.1]])\n    expected_output = 1.5\n    assert_equal(coverage_error(y_true, y_score), expected_output)\n", "def test_label_ranking_loss_multilabel_indicator():\n    # Create a multilabel classification problem\n    X, y = make_multilabel_classification(n_samples=100, n_features=10,\n                                          n_classes=3, n_labels=2,\n                                          random_state=42)\n    y = csr_matrix(y)\n\n    # Create a test dataset\n    y_true = y.toarray()\n    y_score = np.random.rand(y_true.shape[0], y_true.shape[1])\n\n    # Test the function with sample weights\n    sample_weight = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    expected_loss = label_ranking_loss(y_true, y_score, sample_weight=sample_weight)\n    assert_almost_equal(label_ranking_loss(y_true, y_score, sample_weight=sample_weight),\n                       expected_loss, decimal=4)\n\n    # Test the function without sample weights\n    expected_loss = label_ranking_loss(y_true, y_score)\n    assert_almost_equal(label_ranking_loss(y_true, y_score), expected_loss, decimal=4)\n\n    # Test the function with zero sample weights\n    sample_weight = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n    expected_loss = label_ranking_loss(y_true, y_score, sample_weight=sample_weight)\n    assert_almost_equal(label_ranking_loss(y_true, y_score, sample_weight=sample_weight),\n                       expected_loss, decimal=4)\n\n    # Test the function with all zero sample weights\n    y_true = csr_matrix(np.zeros((1, 10)))\n    y_score = np.random.rand(1, 10)\n    sample_weight = np.array([0.0])\n    expected_loss = label_ranking_loss(y_true, y_score, sample_weight=sample_weight)\n    assert_almost_equal(label_ranking_loss(y_true, y_score, sample_weight=sample_weight),\n                       expected_loss, decimal=4)\n", "def test_label_ranking_loss():\n    y_true = np.array([[1, 0, 1], [0, 1, 0]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    assert_array_almost_equal(label_ranking_loss(y_true, y_score), 0.25)\n    y_true = np.array([[1, 1, 0], [0, 0, 1]])\n    y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n    assert_array_almost_equal(label_ranking_loss(y_true, y_score), 0.16666666666666666)\n", "def test_label_ranking_loss_multiple_labels():\n    y_true = csr_matrix(np.array([[1, 1, 0], [0, 0, 1], [1, 0, 1]])\n                        .astype(bool))\n    y_score = np.array([[0.8, 0.2, 0.1], [0.3, 0.7, 0.2], [0.9, 0.05, 0.05]])\n    expected_loss = 0.2\n    assert_almost_equal(label_ranking_loss(y_true, y_score), expected_loss)\n\n    y_true = csr_matrix(np.array([[1, 1, 1], [1, 0, 0], [0, 0, 0]])\n                        .astype(bool))\n    y_score = np.array([[0.8, 0.2, 0.2], [0.3, 0.7, 0.2], [0.9, 0.05, 0.05]])\n    expected_loss = 0.5\n    assert_almost_equal(label_ranking_loss(y_true, y_score), expected_loss)\n\n    y_true = csr_matrix(np.array([[1, 0, 1], [1, 1, 0], [0, 1, 0]])\n                        .astype(bool))\n    y_score = np.array([[0.8, 0.2, 0.1], [0.3, 0.7, 0.2], [0.9, 0.05, 0.05]])\n    expected_loss = 0.5\n    assert_almost_equal(label_ranking_loss(y_true, y_score), expected_loss)\n"], "sample_464": ["    def test_file_response_with_non_existent_file(self):\n        with tempfile.NamedTemporaryFile() as file:\n            file.close()  # Close the file to simulate a non-existent file\n            with self.assertRaises(ValueError):\n                FileResponse(file)\n", "    def test_streaming_content_iterator(self):\n        # Create a mock streaming content that yields two chunks\n        streaming_content = iter([b\"chunk1\", b\"chunk2\"])\n        \n        # Create a StreamingHttpResponse instance with the mock streaming content\n        response = StreamingHttpResponse(streaming_content=streaming_content)\n        \n        # Check that the response object is iterable\n        for chunk in response:\n            self.assertIsInstance(chunk, bytes)\n        \n        # Check that the response object is not picklable\n        with self.assertRaises(TypeError):\n            import pickle\n            pickle.dumps(response)\n        \n        # Check that the response object has the correct status code and content type\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response[\"Content-Type\"], \"text/plain; charset=binary\")\n        \n        # Check that the response object has the correct streaming content\n        self.assertEqual(list(response.streaming_content), [b\"chunk1\", b\"chunk2\"])\n        \n        # Check that the response object can be iterated over twice\n        for chunk in response:\n            self.assertIsInstance(chunk, bytes)\n        with self.assertRaises(StopIteration):\n            next(response.streaming_content)\n        \n        # Check that the response object can be iterated over asynchronously\n        async def test_async_response():\n            async for chunk in response:\n                self.assertIsInstance(chunk, bytes)\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(test_async_response())\n        \n        # Check that the response object can be iterated over synchronously\n        for chunk in response:\n            self.assertIsInstance(chunk, bytes)\n        with self.assertRaises(StopIteration):\n            next(iter(response.streaming_content))\n", "    def test_http_response_content_type(self):\n        response = HttpResponse()\n        self.assertIn('text/html', response.content_type)\n", "    def test_file_response_headers(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filename = os.path.join(tmpdir, \"test_file.txt\")\n            with open(filename, \"wb\") as f:\n                f.write(b\"Hello, World!\")\n\n            response = FileResponse(FileContentFile(filename))\n            self.assertEqual(response[\"Content-Disposition\"], 'attachment; filename=\"test_file.txt\"')\n            self.assertEqual(response[\"Content-Type\"], \"application/octet-stream\")\n            self.assertEqual(response[\"Content-Length\"], os.path.getsize(filename))\n            self.assertEqual(response[\"X-Sendfile\"], filename)\n\n            # Test that the file is closed when the response is closed\n            response.close()\n            self.assertFalse(os.path.exists(filename))\n", "    def test_file_response_with_non_existent_file(self):\n        with self.assertRaises(FileNotFoundError):\n            with tempfile.NamedTemporaryFile() as tmp:\n                FileResponse(tmp.name, filename=\"non_existent_file\")\n"], "sample_419": ["    def test_formset_factory_can_delete_extra_forms(self):\n        form = Form()\n        formset = formset_factory(form, extra=2)\n        self.assertTrue(formset.can_delete_extra)\n", "    def test_management_form_valid(self):\n        data = {\"TOTAL_FORMS\": \"1\", \"INITIAL_FORMS\": \"0\"}\n        management_form = ManagementForm(data=data)\n        self.assertTrue(management_form.is_valid())\n", "    def test_custom_form_kwargs_passed_to_forms(self):\n        formset_class = formset_factory(\n            CustomKwargForm, extra=2, formset=BaseFormSet\n        )\n        formset = formset_class(prefix=\"custom_form\")\n        self.assertEqual(formset.forms[0].custom_kwarg, 0)\n        self.assertEqual(formset.forms[1].custom_kwarg, 0)\n        self.assertEqual(formset.forms[2].custom_kwarg, 1)\n        self.assertEqual(formset.forms[3].custom_kwarg, 1)\n", "    def make_choiceformset_with_non_form_error(\n        self,\n        formset_data=None,\n        formset_class=ChoiceFormSet,\n        total_forms=None,\n        initial_forms=0,\n        max_num_forms=0,\n        min_num_forms=0,\n        **kwargs,", "    def test_non_form_errors_are_saved_across_validity(self):\n        formset = ChoiceFormsetWithNonFormError(prefix=\"choices\")\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [ValidationError(\"non-form error\")])\n        self.assertEqual(formset.management_form.errors, {})\n"], "sample_520": ["def test_text3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    t = Text3D(1, 1, 1, \"Hello\")\n    t.set_3d_properties(5, 'z')\n    assert t._z == 5\n    assert t._dir_vec == np.array([0, 0, 1])\n    t.set_z(10)\n    assert t._z == 10\n    t.set_3d_properties(zdir='x')\n    assert t._z == 10\n    assert t._dir_vec == np.array([1, 0, 0])\n    with pytest.raises(ValueError):\n        t.set_3d_properties(zdir='a')\n", "def test_set_zsort_method():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    collection = Poly3DCollection(np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]]))\n    ax.add_collection(collection)\n    assert collection._zsortfunc.__name__ == 'average'\n    collection.set_zsort('max')\n    assert collection._zsortfunc.__name__ == 'max'\n    collection.set_zsort('min')\n    assert collection._zsortfunc.__name__ == 'min'\n", "def test_poly3dcollection_zsort():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    x, y, z = np.random.rand(3, 10)\n    pc = art3d.Poly3DCollection([[x, y, z]])\n    pc.set_zsort('min')\n    ax.add_collection3d(pc)\n    pc.set_zsort('max')\n    assert pc._zsortfunc == np.max\n    assert pc._zsortfunc.__name__ == 'max'\n    assert pc._zsortfunc(1, 2, 3) == 3\n    assert pc._zsortfunc.__name__ == 'max'\n    assert pc._zsortfunc(1, 2, 3) == 3\n", "def test_3d_text_depth_shading():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.text(1, 1, 1, 'text', zdir='z', size=20, color='r')\n\n    ax.view_init(elev=30, azim=45)\n\n    # Set up the depth shading test case\n    ax.text(0.5, 0.5, 0.5, 'background', zdir='z', color='k', size=20)\n    ax.text(0.55, 0.55, 0.55, 'foreground', zdir='z', color='w', size=20)\n\n    ax.view_init(elev=0, azim=0)\n\n    plt.show()\n\n    # Check that the text rendering is correct\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    text3d = art3d.Text3D(1, 1, 1, 'text', zdir='z')\n    text3d.set_depthshade(True)\n\n    assert text3d.get_depthshade() is True\n    assert text3d.get_z() == 1\n\n    # Test the Line3D class\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    line3d = art3d.Line3D([0, 1], [0, 1], [0, 1])\n    line3d.set_depthshade(True)\n    line3d.set_z(1)\n    assert line3d.get_depthshade() is True\n    assert line3d.get_z() == 1\n\n    # Test the Patch3D class\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    patch3d = art3d.Patch3D()\n    patch3d.set_depthshade(True)\n    assert patch3d.get_depthshade() is True\n\n    # Test the PatchCollection3D class\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    patch_collection_3d = art3d.PatchCollection3D()\n    patch_collection_3d.set_depthshade(True)\n    assert patch_collection_3d.get_depthshade() is True\n\n    # Test the PathCollection3", "def test_rotate_axes():\n    fig, axs = plt.subplots(1, 2, subplot_kw={'projection': '3d'})\n    ax = axs[0]\n    ax.set_axis_off()\n    ax.plot([0, 1], [0, 1], zs=[0, 0])\n\n    circle = PathPatch(mcolors.ColorConverter.to_rgba_array(\n        np.array([[1, 0, 0, 1]]), 1))\n    path_collection = PathCollection([circle], matched=False)\n    path_collection.set_3d_properties([1, 1, 1])\n\n    art3d.patch_collection_2d_to_3d(path_collection, z=1, zdir='-x')\n\n    art3d.patch_collection_2d_to_3d(path_collection, z=2, zdir='-x')\n\n    ax.scatter(0, 0, 0, s=100, c='r')\n    ax.scatter(1, 1, 1, s=100, c='g')\n    ax.scatter(1, 1, 2, s=100, c='b')\n\n    axs[1].set_axis_off()\n    axs[1].plot([0, 1], [0, 1], zs=[0, 0])\n\n    circle = PathPatch(mcolors.ColorConverter.to_rgba_array(\n        np.array([[1, 0, 0, 1]]), 1))\n    path_collection = PathCollection([circle], matched=False)\n    path_collection.set_3d_properties([1, 1, 1])\n\n    art3d.patch_collection_2d_to_3d(path_collection, z=1, zdir='-x')\n\n    art3d.patch_collection_2d_to_3d(path_collection, z=2, zdir='-x')\n\n    axs[1].scatter(0, 0, 0, s=100, c='r')\n    axs[1].scatter(1, 1, 1, s=100, c='g')\n    axs[1].scatter(1, 1, 2, s=100, c='b')\n\n    plt.show()\n"], "sample_648": ["def test_get_unpacked_marks():\n        pass\n    test_func.pytestmark = Mark(\"pytestmark\", (), {})\n    assert get_unpacked_marks(test_func) == [Mark(\"pytestmark\", (), {})]\n\n    class TestClass:\n        pass\n    TestClass.pytestmark = Mark(\"pytestmark\", (), {})\n    assert get_unpacked_marks(TestClass) == [Mark(\"pytestmark\", (), {})]\n\n    assert get_unpacked_marks(TestClass, consider_mro=True) == [\n        Mark(\"pytestmark\", (), {}),\n        Mark(\"pytestmark\", (), {}),\n    ]\n\n    class SuperClass:\n        pass\n\n    class SubClass(SuperClass):\n        pass\n\n    SubClass.pytestmark = Mark(\"pytestmark\", (), {})\n    assert get_unpacked_marks(SubClass, consider_mro=False) == [Mark(\"pytestmark\", (), {})]\n    assert get_unpacked_marks(SubClass, consider_mro=True) == [\n        Mark(\"pytestmark\", (), {}),\n        Mark(\"pytestmark\", (), {}),\n        Mark(\"pytestmark\", (), {}),\n    ]\n", "def test_get_empty_parameterset_mark():\n    # Test get_empty_parameterset_mark with default behaviour\n    config = pytest.config\n    argnames = [\"x\", \"y\"]\n    func = mock.Mock(__name__=\"test_func\")\n    expected_mark = MARK_GEN.skip(reason=\"got empty parameter set ['x', 'y'], function test_func at <mock.Mock name='test_func' object at ...>:-1\")\n    assert get_empty_parameterset_mark(config, argnames, func) == expected_mark\n\n    # Test get_empty_parameterset_mark with skip behaviour\n    config = pytest.config\n    argnames = [\"x\", \"y\"]\n    func = mock.Mock(__name__=\"test_func\")\n    config.getini.return_value = \"skip\"\n    expected_mark = MARK_GEN.skip(reason=\"got empty parameter set ['x', 'y'], function test_func at <mock.Mock name='test_func' object at ...>:-1\")\n    assert get_empty_parameterset_mark(config, argnames, func) == expected_mark\n\n    # Test get_empty_parameterset_mark with xfail behaviour\n    config = pytest.config\n    argnames = [\"x\", \"y\"]\n    func = mock.Mock(__name__=\"test_func\")\n    config.getini.return_value = \"xfail\"\n    expected_mark = MARK_GEN.xfail(reason=\"got empty parameter set ['x', 'y'], function test_func at <mock.Mock name='test_func' object at ...>:-1\", run=False)\n    assert get_empty_parameterset_mark(config, argnames, func) == expected_mark\n\n    # Test get_empty_parameterset_mark with fail_at_collect behaviour\n    config = pytest.config\n    argnames = [\"x\", \"y\"]\n    func = mock.Mock(__name__=\"test_func\")\n    config.getini.return_value = \"fail_at_collect\"\n    expected_mark = MARK_GEN.skip(reason=\"got empty parameter set ['x', 'y'], function test_func at <mock.Mock name='test_func' object at ...>:-1\")\n    assert get_empty_parameterset_mark(config, argnames, func) == expected_mark\n\n    # Test get_empty_parameterset_mark with invalid behaviour\n    config = pytest.config\n    argnames = [\"x\", \"y\"]\n    func = mock.Mock(__name__=\"test_func\")\n    config.getini.return_value = \"invalid\"\n    with pytest.raises(LookupError):\n        get_empty_parameterset_mark(config, argnames, func)\n\n    # Test get_empty", "def test_get_empty_parameterset_mark():\n    pytester = Pytester()\n    config = pytester.makeconfig(ini_content=f\"\"\"\n        [pytest]\n        markers =\n            empty_parameter_set\n    \"\"\")\n    MARK_GEN = MarkGenerator(_ispytest=True)\n    config = pytester.config\n    func = lambda: None\n    argnames = [\"a\", \"b\"]\n    mark = MARK_GEN.empty_parameter_set\n    result = get_empty_parameterset_mark(config, argnames, func)\n    assert isinstance(result, Mark)\n    assert result.name == \"empty_parameter_set\"\n    assert result.args == ()\n    assert result.kwargs == {}\n    assert result._param_ids_from is None\n    assert result._param_ids_generated is None\n", "    def test_get_unpacked_marks(self, pytester: Pytester) -> None:\n        # Test that get_unpacked_marks returns marks from class and its parents\n        class TestClass:\n            pytestmark = pytest.mark.skip(reason=\"test\")\n\n        assert get_unpacked_marks(TestClass).count(pytest.mark.skip) == 1\n", "def test_mark_generator():\n    mark_gen = MarkGenerator(_ispytest=True)\n    assert mark_gen._config is None\n    assert mark_gen._markers == set()\n\n    with pytest.raises(AttributeError):\n        mark_gen.__getattr__(\"private_mark\")\n\n    mark_gen._config = object()  # simulate setting config\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"_skip\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"skip\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"skipif\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"xfail\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"parametrize\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"usefixtures\")\n\n    with pytest.raises(TypeError):\n        mark_gen.__getattr__(\"filterwarnings\")\n\n    # test unknown mark with warning\n    config = object()  # simulate pytest config\n    config.option.strict_markers = False\n    config.option.strict = False\n    mark_gen._config = config\n    with pytest.warns(PytestUnknownMarkWarning):\n        mark_gen.__getattr__(\"unknown_mark\")\n\n    # test warning for common misspellings of parametrize\n    with pytest.raises(ValueError):\n        mark_gen.__getattr__(\"parameterize\")\n\n    with pytest.raises(ValueError):\n        mark_gen.__getattr__(\"parametrise\")\n\n    with pytest.raises(ValueError):\n        mark_gen.__getattr__(\"parameterise\")\n\n    # test that parametrize warning is not raised when strict is on\n    config = object()  # simulate pytest config\n    config.option.strict_markers = True\n    config.option.strict = True\n    mark_gen._config = config\n    with pytest.raises(ValueError):\n        mark_gen.__getattr__(\"parameterise\")\n\n    # test that parametrize warning is not raised when strict is on and mark is not in markers\n    config = object()  # simulate pytest config\n    config.option.strict_markers = True\n    config.option.strict = True\n    config.getini(\"markers\") = [\"parametrize\"]\n    mark_gen._config = config\n    with pytest.raises(ValueError):\n        mark_gen.__getattr__(\"parameterise\")\n\n    # test that parametrize warning is not raised when strict is on and mark is in markers\n    config = object()  # simulate pytest config\n    config.option.strict_markers = True\n    config.option.strict = True\n    config.getini(\"markers\") = [\"parameterise\"]\n    mark_gen._config = config\n    with pytest.raises(ValueError):\n        mark"], "sample_661": ["def test_pytest_run_sessionfinish_recording_global_property():\n    testdir = py.test.ensuretemp(\"test_pytest_run_sessionfinish_recording_global_property\")\n    testdir.join(\"test_example.py\").write(\n        \"\"\"\n        import pytest\n\n        @pytest.fixture(scope=\"session\", autouse=True)\n            request.config._xml.add_global_property(\"example_key\", \"example_value\")\n\n            pass\n\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert dom.find_by_tag(\"testsuite\").assert_attr(\n        name=\"pytest\", timestamp=datetime.fromtimestamp(0).isoformat(), hostname=platform.node()\n    )\n    assert dom.find_by_tag(\"testsuite\").assert_attr(tests=\"2\", failures=\"0\", errors=\"0\", skipped=\"0\")\n    assert dom.find_by_tag(\"testsuite\").assert_attr(time=\"0.000\")\n    properties = dom.find_by_tag(\"properties\")\n    assert properties.assert_attr(example_key=\"example_value\")\n    with pytest.raises(AttributeError):\n        dom.find_by_tag(\"properties\").assert_attr(non_existent_property=\"example_value\")\n", "def test_logxml_pytest_sessionfinish():\n    # Arrange\n    testdir = py.test.skip(\"skip this test for now\")\n    resultpath = testdir.tmpdir.join(\"junit.xml\")\n    testdir.runpytest(\"--junitxml=%s\" % resultpath)\n    xmldoc = minidom.parse(str(resultpath))\n    logxml = LogXML(resultpath, \"prefix\", \"suite_name\", \"logging\", \"report_duration\", \"family\", True)\n\n    # Act\n    logxml.pytest_sessionfinish()\n\n    # Assert\n    junit_node = DomNode(xmldoc).find_first_by_tag(\"testsuite\")\n    assert_attr(junit_node, name=\"suite_name\", hostname=platform.node(), timestamp=datetime.now().isoformat())\n    assert_attr(junit_node, tests=0, time=\"0.000\", errors=0, failures=0, skipped=0)\n\n", "def test_pytest_configure_LogXML_configured():\n    testdir = py.test.ensuretempdir()\n    testdir.join(\"conftest.py\").write(\n        \"\"\"\n            import pytest\n\n            @pytest.fixture\n                    pass\n                attr_func = add_attr_noop\n                xml = request.config._xml\n                if xml is not None:\n                    node_reporter = xml.node_reporter(request.node.nodeid)\n                    attr_func = node_reporter.add_attribute\n                return attr_func\n        \"\"\"\n    )\n    testdir.join(\"test_pass.py\").write(\n        \"\"\"\n                record_xml_attribute(\"test_key\", \"test_value\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--junitxml=junit.xml\")\n    result.assert_outcomes(passed=1)\n    assert result.ret == 0\n\n    xmldoc = minidom.parse(\"junit.xml\")\n    root = DomNode(xmldoc)\n    testcase = root.find_first_by_tag(\"testcase\")\n    assert testcase is not None\n    assert testcase.assert_attr(\n        classname=\"test_pass\",\n        name=\"test_pass\",\n        time=\"0.000\",\n        file=\"test_pass.py\",\n        test_key=\"test_value\",\n    )\n", "def test_NodeReporter_record_testreport():\n    \"\"\"Test that the NodeReporter.record_testreport() method correctly records the\n    test report into a testcase node.\n    \"\"\"\n    testdir = py.test.raises(pytest.main.MainFailure, \"TestPython\")\n    result = testdir.runpytest()\n    reporter = LogXML(None, None, \"TestSuite\", \"system-out\", \"total\", \"xunit1\", True)\n    nodeid = \"path/to/test\"\n    reporter._opentestcase(result.rep_setup[0])\n    reporter.record_testreport(result.rep_setup[0])\n    reporter.finalize(result.rep_setup[0])\n    dom_node = DomNode(minidom.parse(\"junit.xml\"))\n    testcase_node = dom_node.find_nth_by_tag(\"testcase\", 0)\n    assert_attr(testcase_node, name=\"path/to/test\", classname=\"path.to.test\", file=\"path/to/test.py\", line=None, url=None)\n\n", "def test_LogXML_create_with_custom_logging():\n    testdir = py.test.TempdirFactory()\n    testdir.join(\"foo.py\").write(\n        \"\"\"\\\n        import pytest\n\n            pass\n\n            assert 1 == 2\n\n            pytest.skip(\"Skipped reason\")\n\n            pytest.xfail(\"Xfail reason\")\n\n            pytest.skip(\"collect skipped\")\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    report = dom.find_nth_by_tag(\"testcase\", 3)\n\n    assert dom.find_nth_by_tag(\"properties\", 1).get_unique_child.assert_attr(\n        name=\"my-property\", value=\"my-value\"\n    )\n"], "sample_129": ["    def test_floatformat_with_positive_arg(self):\n        self.assertEqual(\n            self.engine.from_string(self.floatformat02, self.context).render(self.context),\n            \"34.2 34.2\"\n        )\n", "    def test_floatformat_precision(self):\n        # Test floatformat with positive precision\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:3 }}{% endautoescape %}', {'a': 3.14159}).render(), '3.142')\n\n        # Test floatformat with negative precision\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:\"-3\" }}{% endautoescape %}', {'a': 3.14159}).render(), '3.142')\n\n        # Test floatformat with precision greater than the number of decimal places\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:10 }}{% endautoescape %}', {'a': 3.14159}).render(), '3.14159')\n\n        # Test floatformat with precision equal to the number of decimal places\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:4 }}{% endautoescape %}', {'a': 3.14159}).render(), '3.1416')\n\n        # Test floatformat with precision less than the number of decimal places\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:2 }}{% endautoescape %}', {'a': 3.14159}).render(), '3.1')\n\n        # Test floatformat with precision of 0\n        self.assertEqual(self.engine.from_string('{% autoescape off %}{{ a|floatformat:0 }}{% endautoescape %}', {'a': 3.14159}).render(), '4')\n", "    def test_floatformat_with_negative_places(self):\n        self.assertHTMLEqual(self.floatformat02, '<div>' + self.floatformat01 + '</div>')\n        a = Decimal('34.23234')\n        b = Decimal('34.00000')\n        self.assertEqual(self.floatformat02, '<div>34.23 34</div>')\n        self.assertEqual(self.floatformat03, '<div>34.23000 34</div>')\n", "    def test_floatformat_negative_arg(self):\n        \"\"\"Floatformat tests with negative argument.\"\"\"\n        output = self.engine.from_string('{% autoescape off %}{{ 1.234|floatformat:\"-2\" }}{% endautoescape %}').render(Context({'a': 1.234}))\n        self.assertEqual(output, '<p>1.23</p>')\n", "    def test_floatformat_invalid_arg(self):\n        # Test that invalid argument raises a ValueError\n        with self.assertRaises(ValueError):\n            floatformat(float(34.23234), 'a')\n"], "sample_139": ["    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(username='super', email='a@b.com', password='xxx')\n        cls.parent = Parent.objects.create()\n        cls.child = Child.objects.create(parent=cls.parent)\n", "    def test_get_changelist_instance(self):\n        request = self._mocked_authenticated_request(reverse('admin:Child_changelist'), self.superuser)\n        cl = custom_site.get_changelist_instance(request)\n        self.assertEqual(cl.list_display, ['__str__'])\n        self.assertEqual(cl.list_display_links, ('name',))\n        self.assertEqual(cl.list_filter, [])\n        self.assertEqual(cl.date_hierarchy, None)\n        self.assertEqual(cl.search_fields, [])\n        self.assertEqual(cl.date_hierarchy, None)\n        self.assertEqual(cl.list_select_related, False)\n        self.assertEqual(cl.list_per_page, 100)\n        self.assertEqual(cl.list_max_show_all, 200)\n        self.assertEqual(cl.list_editable, [])\n        self.assertEqual(cl.search_fields, [])\n", "    def test_get_changelist_form(self):\n        request = self._mocked_authenticated_request(reverse('admin:custom_iduser_changelist'), self.superuser)\n        admin = admin.site._registry[CustomIdUser]\n        form = admin.get_changelist_form(request)\n        self.assertIsInstance(form, admin.form)\n        self.assertEqual(form.base_fields, {\n            'id': admin.form.base_fields['id'],\n            'name': admin.form.base_fields['name'],\n        })\n", "    def test_inline_model_admin_get_queryset(self):\n        request = self.factory.get('/admin/custom_iduser/customiduser/')\n        request.user = self._create_superuser('super')\n        custom_iduser = CustomIdUser.objects.create(name='test')\n        inline_admin = ChildAdmin(CustomIdUser, custom_site)\n        queryset = inline_admin.get_queryset(request)\n        self.assertEqual(queryset.count(), 0)\n\n        queryset = inline_admin.get_queryset(request, custom_iduser)\n        self.assertEqual(queryset.count(), 0)\n\n        request.user = None\n        queryset = inline_admin.get_queryset(request)\n        self.assertEqual(queryset.count(), 0)\n\n        request.user = self._create_superuser('super')\n        queryset = inline_admin.get_queryset(request)\n        self.assertEqual(queryset.count(), 0)\n", "    def test_get_list_filter(self):\n        admin = BandAdmin(Band, custom_site)\n        self.assertEqual(admin.get_list_filter(self.factory.get('/admin/band/')), [])\n"], "sample_175": ["    def test_PROTECT(self):\n        # Test PROTECT on_delete behavior\n        user = User.objects.create()\n        self.assertEqual(User.objects.count(), 1)\n\n        # Create an Avatar instance referencing the user\n        avatar = Avatar(user=user)\n        avatar.save()\n\n        # Try to delete the user\n        with self.assertRaises(ProtectedError):\n            user.delete()\n\n        # Check that the user was not deleted\n        self.assertEqual(User.objects.count(), 1)\n\n        # Check that the Avatar instance still exists\n        self.assertEqual(Avatar.objects.count(), 1)\n", "    def test_set_on_delete(self):\n        # Test SET on_delete behavior\n        obj = M.objects.create(r=self.DEFAULT)\n        collector = Collector(using='default')\n        collector.add([obj])\n        collector.collect([obj], source=M, nullable=True, collect_related=True)\n        self.assertEqual(collector.field_updates[M][obj.r, True], {obj})\n", "    def test_can_fast_delete_with_generic_foreign_key(self):\n        # Test case for the can_fast_delete method with a generic foreign key.\n        user = User.objects.create()\n        generic_b1 = GenericB1.objects.create()\n        GenericB2.objects.create(user=user, b1=generic_b1)\n        collector = Collector(using='default')\n        collector.add([generic_b1])\n        self.assertTrue(collector.can_fast_delete([generic_b1]))\n", "    def test_protected_error_raised_with_multiple_protected_objects(self):\n        \"\"\"\n        Tests that a ProtectedError is raised with a list of protected objects\n        when there are multiple objects referenced through a protected foreign key.\n        \"\"\"\n        b = B.objects.create(name=\"b\")\n        b1 = B1.objects.create(name=\"b1\", b=b)\n        b2 = B1.objects.create(name=\"b2\", b=b)\n\n        with self.assertRaises(ProtectedError):\n            b.delete()\n\n        self.assertEqual(len(B1.objects.all()), 2)\n", "    def test_protection_works_with_m2m_model(self):\n        # Test that protection works even when the foreign key is on a\n        # many-to-many model.\n        with self.assertRaises(ProtectedError):\n            Collector().collect([M2MFrom.objects.create()])\n"], "sample_78": ["def test_app_config_handling(self):\n    class TestAppCommand(AppCommand):\n        missing_args_message = 'Missing app labels.'\n            self.assertTrue(isinstance(app_config, apps.AppConfig))\n            return f'handled {app_config.name}'\n\n    with captured_stderr() as stderr:\n        TestAppCommand().run_from_argv(['manage.py', 'test_command', 'myapp'])\n    self.assertIn('handled myapp', stderr.getvalue())\n", "    def test_output_wrapper_write(self):\n        output = StringIO()\n        wrapper = OutputWrapper(output)\n        wrapper.write('Hello, world!', lambda x: x)\n        wrapper.write('This is a test.', lambda x: x)\n        self.assertEqual(output.getvalue(), 'Hello, world!This is a test.\\n')\n", "    def test_create_parser(self):\n        command = BaseCommand()\n        parser = command.create_parser('fake', 'fake')\n        self.assertIsInstance(parser, management.CommandParser)\n", "    def test_handle_app_config(self):\n        cmd = AppCommand()\n        cmd._called_from_command_line = True\n        self.addCleanup(lambda: setattr(cmd, '_called_from_command_line', False))\n\n        with captured_stderr() as stderr:\n            cmd.handle_app_config(apps.get_app_config('django.contrib.auth'))\n            self.assertEqual(stderr.getvalue(), '')\n\n        stderr.write = lambda msg, *args, **kwargs: None\n        cmd.handle_app_config(apps.get_app_config('django.contrib.auth'), stdout=StringIO())\n        self.assertEqual(stderr.getvalue(), '')\n\n        stderr.write = lambda msg, *args, **kwargs: None\n        stderr.write = lambda msg, style_func=lambda x: x: None\n        cmd.handle_app_config(apps.get_app_config('django.contrib.auth'), stderr=StringIO())\n        self.assertEqual(stderr.getvalue(), '')\n", "def test_create_parser(self):\n    class TestCommand(BaseCommand):\n            parser.add_argument('--foo', help='foo option')\n            parser.add_argument('--bar', help='bar option')\n\n    cmd = TestCommand()\n    parser = cmd.create_parser('test', 'test')\n\n    # Test the --version option\n    with captured_stderr() as stderr:\n        parser.print_help()\n        self.assertIn('version', stderr.getvalue())\n\n    # Test the --foo and --bar options\n    with captured_stderr() as stderr:\n        parser.print_help()\n        self.assertIn('--foo', stderr.getvalue())\n        self.assertIn('--bar', stderr.getvalue())\n\n    # Test the usage of stealth options\n    with captured_stderr() as stderr:\n        parser.print_help()\n        for option in cmd.base_stealth_options:\n            self.assertIn(option, stderr.getvalue())\n\n    # Test the order of arguments in the --help output\n    with captured_stderr() as stderr:\n        parser.print_help()\n        self.assertRegex(stderr.getvalue(), r'usage:.*foo.*bar')\n\n    # Test the formatting of the help message\n    with captured_stderr() as stderr:\n        parser.print_help()\n        self.assertIn('foo option', stderr.getvalue())\n        self.assertIn('bar option', stderr.getvalue())\n"], "sample_119": ["    def test_build_lookup_transform(self):\n        q = Query(Author)\n        self.assertEqual(q.solve_lookup_type('foo__upper')['transforms'], ('upper',))\n        self.assertEqual(q.solve_lookup_type('foo__exact')['transforms'], ('exact',))\n        self.assertEqual(q.solve_lookup_type('foo__iexact')['transforms'], ('iexact',))\n        self.assertEqual(q.solve_lookup_type('foo__contains')['transforms'], ('contains',))\n        self.assertEqual(q.solve_lookup_type('foo__icontains')['transforms'], ('icontains',))\n        self.assertEqual(q.solve_lookup_type('foo__startswith')['transforms'], ('startswith',))\n        self.assertEqual(q.solve_lookup_type('foo__istartswith')['transforms'], ('istartswith',))\n        self.assertEqual(q.solve_lookup_type('foo__endswith')['transforms'], ('endswith',))\n        self.assertEqual(q.solve_lookup_type('foo__iendswith')['transforms'], ('iendswith',))\n        self.assertEqual(q.solve_lookup_type('foo__in')['transforms'], ('in',))\n        self.assertEqual(q.solve_lookup_type('foo__regex')['transforms'], ('regex',))\n        self.assertEqual(q.solve_lookup_type('foo__iregex')['transforms'], ('iregex',))\n        self.assertEqual(q.solve_lookup_type('foo__range')['transforms'], ('range',))\n        self.assertEqual(q.solve_lookup_type('foo__year')['transforms'], ('year',))\n        self.assertEqual(q.solve_lookup_type('foo__month')['transforms'], ('month',))\n        self.assertEqual(q.solve_lookup_type('foo__day')['transforms'], ('day',))\n        self.assertEqual(q.solve_lookup_type('foo__week_day')['transforms'], ('week_day',))\n        self.assertEqual(q.solve_lookup_type('foo__hour')['transforms'], ('hour',))\n        self.assertEqual(q.solve_lookup_type('foo__minute')['transforms'], ('minute',))\n        self.assertEqual(q.solve_lookup_type('foo__second')['transforms'], ('second',))\n        self.assertEqual(q.solve_lookup_type('foo__microsecond')['transforms'], ('microsecond',))\n        self.assertEqual(q.solve_lookup_type('foo__isnull')['transforms'], ('isnull',))\n        self.assertEqual(q.solve_lookup_type('foo__search')['transforms'], ('search',))\n        self.assertEqual(q.solve_lookup_type('foo__contains__exact')['transforms'], ('exact',))\n        self.assertEqual(q.solve_lookup_type('foo__contains__contains')['transforms'], ('contains',))\n        self.assertEqual(q.solve_lookup_type('foo__contains", "    def test_build_where_with_isnull_lookup(self):\n        query = Query(Author)\n        query.add_filter(('author__isnull', True))\n        self.assertEqual(query.where.children[0].lookup_name, 'isnull')\n", "    def test_add_deferred_loading(self):\n        q = Query(Ranking)\n        self.assertEqual(q.deferred_loading, (frozenset(), True))\n        q.add_deferred_loading(['name', 'created_at'])\n        self.assertEqual(q.deferred_loading, (frozenset(['name', 'created_at']), True))\n        q.add_deferred_loading(['name'])\n        self.assertEqual(q.deferred_loading, (frozenset(['name', 'created_at']), False))\n", "    def test_add_annotation(self):\n        q = Query(Ranking)\n        q.set_group_by()\n        self.assertEqual(q.annotation_select, {})\n        q.add_annotation(Ranking.objects.all().count(), alias='count')\n        self.assertEqual(q.annotation_select, {'count': 'count'})\n", "    def test_join_promoter(self):\n        q = Query(Author.objects.model)\n        q.where = OR()\n        q.where.add(GreaterThan('age', 10), OR)\n        q.where.add(Exact('name', 'John'), AND)\n        q.where.add(IsNull('name'), AND)\n        q.where.add(Lower('name'), AND)\n        self.assertEqual(\n            q.alias_refcount,\n            {\n                'T0': 3,\n                'T1': 2,\n                'T2': 1,\n                'T3': 1,\n            }\n        )\n        self.assertEqual(\n            q.alias_map['T0'].join_type,\n            LOUTER\n        )\n        self.assertEqual(\n            q.alias_map['T1'].join_type,\n            INNER\n        )\n        self.assertEqual(\n            q.alias_map['T2'].join_type,\n            LOUTER\n        )\n        self.assertEqual(\n            q.alias_map['T3'].join_type,\n            LOUTER\n        )\n        self.assertEqual(\n            q.promote_joins(['T0']),\n            ['T0']\n        )\n        self.assertEqual(\n            q.demote_joins(['T0']),\n            ['T0']\n        )\n        self.assertEqual(\n            q.promote_joins(['T0', 'T1']),\n            ['T0', 'T1']\n        )\n        self.assertEqual(\n            q.demote_joins(['T0', 'T1']),\n            []\n        )\n        self.assertEqual(\n            q.promote_joins(['T0', 'T1', 'T2']),\n            ['T0', 'T1']\n        )\n        self.assertEqual(\n            q.demote_joins(['T0', 'T1', 'T2']),\n            ['T2']\n        )\n        self.assertEqual(\n            q.promote_joins(['T0', 'T1', 'T2', 'T3']),\n            ['T0', 'T1']\n        )\n        self.assertEqual(\n            q.demote_joins(['T0', 'T1', 'T2', 'T3']),\n            []\n        )\n"], "sample_317": ["    def test_enclosure_validation(self):\n        feed = Rss201rev2Feed(\n            title=\"Test feed\",\n            link=\"http://example.com\",\n            description=\"Test feed description\"\n        )\n        with self.assertRaises(ValueError):\n            feed.add_item(\n                title=\"Test item\",\n                link=\"http://example.com/item\",\n                description=\"Test item description\",\n                enclosures=[Enclosure(\"http://example.com/file\", \"1234567890\", \"text/plain\")]\n            )\n\n        feed = Atom1Feed(\n            title=\"Test feed\",\n            link=\"http://example.com\",\n            description=\"Test feed description\"\n        )\n        with self.assertRaises(ValueError):\n            feed.add_item(\n                title=\"Test item\",\n                link=\"http://example.com/item\",\n                description=\"Test item description\",\n                enclosures=[Enclosure(\"http://example.com/file\", \"1234567890\", \"text/plain\")]\n            )\n", "    def test_enclosure(self):\n        feed = Atom1Feed(title=\"Test Feed\", link=\"http://example.com/feed\")\n        feed.add_item(\n            title=\"Test Item\",\n            link=\"http://example.com/item\",\n            description=\"This is a test item.\",\n            enclosures=[Enclosure(\"http://example.com/file.mp3\", \"12345\", \"audio/mpeg\")],\n        )\n        with tempfile.NamedTemporaryFile() as fp:\n            feed.write(fp, \"utf-8\")\n            fp.seek(0)\n            dom = minidom.parse(fp)\n            root = dom.documentElement\n            self.assertEqual(root.nodeName, 'feed')\n            item = root.getElementsByTagName(\"entry\")[0]\n            enclosure = item.getElementsByTagName(\"link\")[0]\n            self.assertEqual(enclosure.attributes[\"rel\"].value, \"enclosure\")\n            self.assertEqual(enclosure.attributes[\"href\"].value, \"http://example.com/file.mp3\")\n            self.assertEqual(enclosure.attributes[\"length\"].value, \"12345\")\n            self.assertEqual(enclosure.attributes[\"type\"].value, \"audio/mpeg\")\n", "    def test_rss_root_elements(self):\n        feed = Rss201rev2Feed(\n            title='My title',\n            link='https://example.com',\n            description='My description',\n            categories=['Category 1', 'Category 2'],\n            feed_url='https://example.com/rss',\n            feed_copyright='Copyright 2023',\n        )\n        feed.add_item(\n            title='My item',\n            link='https://example.com/item',\n            description='My item description',\n            categories=['Category 3'],\n        )\n        feed.write(StringIO(), 'utf-8')\n        dom = minidom.parseString(feed.writeString('utf-8'))\n        root = dom.documentElement\n        self.assertEqual(self.assertChildNodes(root, ['channel', 'title', 'link', 'description', 'copyright', 'lastBuildDate', 'ttl']),\n                        ['channel', 'title', 'link', 'description', 'copyright', 'lastBuildDate', 'ttl'])\n        self.assertChildNodeContent(root.childNodes[0], {'title': 'My title', 'link': 'https://example.com', 'description': 'My description', 'copyright': 'Copyright 2023', 'lastBuildDate': 'Mon, 01 Jan 1980 12:30:00 GMT', 'ttl': '30'})\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n        cls.feed = Rss201rev2Feed(\n            title=\"My Feed\",\n            link=\"http://example.com\",\n            description=\"A description\",\n            author_name=\"John Doe\",\n            author_email=\"john@example.com\",\n            author_link=\"http://example.com\",\n            feed_url=\"http://example.com/atom.xml\",\n            categories=[\"Category1\", \"Category2\"],\n            feed_copyright=\"Copyright 2022\",\n        )\n", "    def test_write_item_with_author(self):\n        feed = Atom1Feed(\n            title='My feed',\n            link='http://example.com/feed/',\n            description='My feed description',\n            author_name='John Doe',\n            author_email='john@example.com',\n            author_link='http://example.com',\n        )\n        feed.add_item(title='My item', link='http://example.com/item/1', description='My item description', author_name='Jane Doe', author_email='jane@example.com')\n        feed.writeString('utf-8')\n        dom = minidom.parseString(feed.writeString('utf-8'))\n        self.assertChildNodeContent(dom.documentElement, {\n            'title': 'My feed',\n            'link': 'http://example.com/feed/',\n            'id': 'http://example.com/feed/',\n            'updated': '2010-01-01T00:00:00Z',\n            'author_name': 'John Doe',\n            'author_email': 'john@example.com',\n            'author_link': 'http://example.com',\n            'subtitle': None,\n            'link:alternate': 'http://example.com/feed/',\n        })\n        self.assertChildNodeContent(dom.getElementsByTagName('entry')[0], {\n            'title': 'My item',\n            'id': 'tag:http://example.com/item/1:2010-01-01',\n            'published': '2010-01-01T00:00:00Z',\n            'author_name': 'Jane Doe',\n            'author_email': 'jane@example.com',\n            'summary': 'My item description',\n            'link:alternate': 'http://example.com/item/1',\n        })\n"], "sample_421": ["    def test_combined_expression_same_type(self):\n        from django.db.models import IntegerField\n        from django.db.models import CombinedExpression\n\n        field1 = IntegerField()\n        field2 = IntegerField()\n        result_field = IntegerField()\n\n        expr = CombinedExpression(field1, CombinedExpression.ADD, field2, output_field=result_field)\n        self.assertEqual(expr.output_field, result_field)\n", "    def test_invalid_expression(self):\n        with self.assertRaises(FieldError):\n            F('invalid_field')\n", "    def test_deconstruction(self):\n        wrapper = ExpressionWrapper(\n            Case(When(CaseTestModel.integer > 0, then=Value(1)), default=Value(0)),\n            output_field=IntegerField(),\n        )\n        self.assertEqual(\n            wrapper.deconstruct(),\n            (\n                \"ExpressionWrapper\",\n                \"django.db.models.ExpressionWrapper\",\n                (\n                    Case(When(CaseTestModel.integer > 0, then=Value(1)), default=Value(0)),\n                    IntegerField(),\n                ),\n                {},\n            ),\n        )\n", "    def test_aggregate_expression_with_invalid_output_field(self):\n        case = Case(\n            When(Q(integer__gt=0), then=\"positive\"),\n            When(Q(integer__lt=0), then=\"negative\"),\n            default=\"zero\",\n        )\n        case.output_field = \"InvalidOutputField\"\n        with self.assertRaises(FieldError):\n            case.resolve_expression()\n", "    def test_combine_expression(self):\n        # Test combine_expression method of connection\n        # This is a bit tricky, as it's a method of connection,\n        # but we can test it by creating a connection object\n        # and calling the method on it.\n        # The result of the test is not really important,\n        # we are just checking that the method doesn't raise an exception.\n        connection = connection\n        expression1 = ExpressionList(F('field1'), F('field2'))\n        expression2 = ExpressionList(F('field3'), F('field4'))\n        self.assertEqual(connection.combine_expression('+', (expression1.as_sql(connection), expression2.as_sql(connection))), \"((field1 + field2) + (field3 + field4))\")\n"], "sample_17": ["    def test_block(self):\n        out = np.block([[self.q, self.q], [self.q, self.q]])\n        expected = np.block([[self.q.value, self.q.value], [self.q.value, self.q.value]]) * self.q.unit\n        assert out.shape == expected.shape\n        assert np.all(out == expected)\n", "    def test_shape(self):\n        o = np.shape(self.q)\n        assert_array_equal(o, self.q.shape)\n        assert o.shape == (2,)\n", "    def test_quantities(self):\n        \"\"\"Test piecewise with Quantity inputs.\"\"\"\n        f = np.fromiter([1, 2, 3, 4, 5], dtype=float) * u.m\n        condlist = [f > 2.5, f <= 2.5]\n        funclist = [f * 2, f * 3]\n        result = piecewise(f, condlist, funclist)\n        expected = np.piecewise(f.value, condlist, funclist)\n        assert result.shape == expected.shape\n        assert np.all(result == expected * result.unit)\n", "    def test_like_helper(self):\n        # Test like functions that don't require conversion to array\n        q = u.m * np.arange(9.0).reshape(3, 3) / 4.0\n        assert np.all(self.like_helper(q, np.ones_like(q)) == np.ones_like(q))\n\n        # Test like functions that require conversion to array\n        q = u.m * np.arange(9.0).reshape(3, 3) / 4.0\n        assert np.all(self.like_helper(q, np.zeros_like(q)) == np.zeros_like(q))\n\n        # Test like functions that require conversion to array with unit_from_first=False\n        q = u.m * np.arange(9.0).reshape(3, 3) / 4.0\n        assert np.all(self.like_helper(q, np.zeros_like(q), unit_from_first=False) == np.zeros_like(q))\n\n        # Test like functions that require conversion to array with unit_from_first=True\n        q = u.m * np.arange(9.0).reshape(3, 3) / 4.0\n        assert np.all(self.like_helper(q, np.zeros_like(q), unit_from_first=True) == np.zeros_like(q))\n", "    def check(self, func, *args, **kwargs):\n        o = func(self.q, *args, **kwargs)\n        expected = func(self.q.value, *args, **kwargs) * self.q.unit\n        assert o.shape == expected.shape\n        assert np.all(o == expected)\n"], "sample_663": ["    def test_perform_collect(self, pytestconfig, tmpdir):\n        # Test that perform_collect() correctly handles the case where\n        # a test package has an __init__.py file.\n        p = tmpdir.join('test_package')\n        p.mkdir()\n        p.join('__init__.py').write('')\n        p.join('test_file.py').write('def test_pass(): pass')\n\n        pytestconfig.invocation_dir = p\n        session = Session(pytestconfig)\n        items = list(session.perform_collect())\n\n        assert len(items) == 1\n        item = items[0]\n        assert item.nodeid == 'test_file.py::test_pass'\n", "def test_matchnodes_matching():\n    \"\"\"Ensure that matchnodes correctly matches items.\"\"\"\n    collector = Session(pytestconfig())\n    item = nodes.Item.fromparent(collector, \"test_module\", \"test_function\")\n    node = nodes.FSInfo.fromparent(collector, \"path/to/module.py\")\n    matching = [node]\n    names = [\"test_function\"]\n    result = collector.matchnodes(matching, names)\n    assert len(result) == 1\n    assert result[0].name == \"test_function\"\n", "def test_session_repr(tmpdir):\n    \"\"\"Session representation.\"\"\"\n    s = Session(pytest.config)\n    expected = \"<Session %r exitstatus=<UNSET> testsfailed=0 testscollected=0>\" % s.name\n    assert repr(s) == expected\n", "    def test_initialization(self, tmpdir):\n        # test initial attributes\n        session = Session(pytestconfig={\"rootdir\": str(tmpdir)})\n        assert session.testsfailed == 0\n        assert session.testscollected == 0\n        assert session.shouldstop is False\n        assert session.shouldfail is False\n\n        # test startdir attribute\n        assert session.startdir == tmpdir\n", "    def test_collecting_with_pyargs(self, tmpdir):\n        # Test that pytest_collect_file returns a Module when the item path is a module\n        # name and the file exists\n        module_path = tmpdir.join(\"test_module.py\")\n        module_path.write(\"def test_function(): pass\")\n        session = Session(pytest.config)\n        session._node_cache = {}\n        pytest.collect_one_node(nodes.Module(session.rootdir, \"test_module.py\", session))\n        assert len(session._node_cache) == 1\n\n        # Test that pytest_collect_file returns a Module when the item path is a module\n        # name and the file does not exist\n        module_path = tmpdir.join(\"non_existent_module.py\")\n        session = Session(pytest.config)\n        session._node_cache = {}\n        pytest.collect_one_node(nodes.Module(session.rootdir, \"non_existent_module.py\", session))\n        assert len(session._node_cache) == 0\n"], "sample_625": ["def test_broadcast_compat_data():\n    da = xr.DataArray([1, 2, 3], dims=(\"x\", \"y\"), coords={\"x\": [0, 1, 2]})\n    assert broadcast_compat_data(da, (\"x\", \"z\"), (\"x\",)) == da.data\n    with pytest.raises(ValueError):\n        broadcast_compat_data(da, (\"x\", \"z\"), (\"x\", \"y\"))\n    with pytest.raises(ValueError):\n        broadcast_compat_data(da, (\"x\",), (\"x\", \"y\"))\n", "def test_unify_chunks():\n    import dask.array as da\n    from xarray.core import DataArray\n\n    # Test case with no chunked dimensions\n    da1 = DataArray(np.array([1, 2, 3]), dims=['x'])\n    da2 = DataArray(np.array([1, 2, 3]), dims=['x'])\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n    assert unified_da1.chunks == da2.chunks\n\n    # Test case with different chunked dimensions\n    da1 = DataArray(np.array([1, 2, 3]), dims=['x'], chunks={'x': 2})\n    da2 = DataArray(np.array([1, 2, 3]), dims=['x'], chunks={'x': 3})\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n    assert unified_da1.chunks == unified_da2.chunks == {'x': 3}\n\n    # Test case with no dask arrays\n    da1 = DataArray(np.array([1, 2, 3]), dims=['x'])\n    da2 = DataArray(np.array([1, 2, 3]), dims=['x'])\n    unified_da1, unified_da2 = unify_chunks(da1, da2)\n    assert unified_da1.chunks is None\n    assert unified_da2.chunks is None\n\n    # Test case with multiple dask arrays\n    da1 = DataArray(np.array([1, 2, 3]), dims=['x'], chunks={'x': 2})\n    da2 = DataArray(np.array([1, 2, 3]), dims=['x'], chunks={'x': 3})\n    da3 = DataArray(np.array([1, 2, 3]), dims=['x'], chunks={'x': 2})\n    unified_da1, unified_da2, unified_da3 = unify_chunks(da1, da2, da3)\n    assert unified_da1.chunks == unified_da2.chunks == unified_da3.chunks == {'x': 3}\n", "def test_unify_chunks_with_multiple_datasets():\n    \"\"\"Test that unify_chunks works when provided multiple datasets.\"\"\"\n    ds1 = xr.Dataset(\n        {\"x\": ((\"y\",), np.arange(4))},\n        coords={\"y\": (\"y\", np.arange(4))},\n        attrs={\"a\": 1},\n    )\n\n    ds2 = xr.Dataset(\n        {\"x\": ((\"y\",), np.arange(6))},\n        coords={\"y\": (\"y\", np.arange(6))},\n        attrs={\"b\": 2},\n    )\n\n    unified = unify_chunks(ds1, ds2)\n    assert isinstance(unified, tuple)\n    assert len(unified) == 2\n    assert isinstance(unified[0], xr.Dataset)\n    assert isinstance(unified[1], xr.Dataset)\n\n    # Check that the chunk sizes are unified.\n    assert unified[0].x.chunks == {\"y\": (4, 2)}\n    assert unified[1].x.chunks == {\"y\": (4, 2)}\n", "def test_polyval_general():\n    # Create a polynomial of degree 2\n    x = xr.DataArray(np.arange(5), dims=[\"x\"])\n    coeffs = xr.DataArray([1, 2, 3], dims=[\"degree\"])\n    poly = polyval(x, coeffs)\n\n    # Check that the result is correct\n    assert_identical(poly, x**2 + 2*x + 3)\n\n    # Check that the result is broadcasted correctly\n    poly = polyval(x, coeffs, dim=\"degree\")\n    assert_identical(poly, x**2 + 2*x + 3)\n\n    # Check that the result is correct for a dataset\n    da = xr.Dataset({\"x\": x, \"y\": coeffs})\n    poly = polyval(da, da[\"y\"], dim=\"degree\")\n    assert_identical(poly, da[\"x\"]**2 + 2*da[\"x\"] + 3)\n\n    # Check that the result is broadcasted correctly for a dataset\n    da = xr.Dataset({\"x\": x, \"y\": coeffs})\n    poly = polyval(da, da[\"y\"], dim=\"degree\")\n    assert_identical(poly, da[\"x\"]**2 + 2*da[\"x\"] + 3)\n\n    # Check that the function raises an error if the degree dimension is not a coordinate variable\n    with pytest.raises(ValueError):\n        xr.DataArray([1, 2, 3], dims=[\"degree\"]).polyval(x, coeffs, dim=\"degree\")\n\n    # Check that the function raises an error if the degree dimension is not an integer dtype\n    with pytest.raises(ValueError):\n        xr.DataArray([1.1, 2.2, 3.3], dims=[\"degree\"]).polyval(x, coeffs, dim=\"degree\")\n\n    # Check that the function raises an error if the degree dimension is not present in the coefficients\n    with pytest.raises(ValueError):\n        xr.DataArray([1, 2, 3], dims=[\"other\"]).polyval(x, coeffs, dim=\"degree\")\n\n    # Check that the function raises an error if the degree dimension is not aligned with the input array\n    with pytest.raises(KeyError):\n        polyval(x, coeffs, dim=\"other\")\n", "def test_broadcast_compat_data_scalar(args, kwargs):\n    x = np.array([1, 2, 3])\n    output = broadcast_compat_data(x, (\"time\",), (\"time\",))\n    assert_identical(output, x)\n\n    x = np.array([[1, 2, 3], [4, 5, 6]])\n    output = broadcast_compat_data(x, (\"time\",), (\"time\", \"space\"))\n    assert_identical(output, x)\n\n    x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n    output = broadcast_compat_data(x, (\"time\", \"space\"), (\"time\", \"space\", \"level\"))\n    assert_identical(output, x)\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, (\"time\",), (\"time\", \"space\"))\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, (\"time\",), (\"time\",))\n\n    output = broadcast_compat_data(x, (\"time\",), (\"time\",), copy=False)\n    assert_identical(output, x)\n\n    output = broadcast_compat_data(x, (\"time\",), (\"time\",), copy=True)\n    assert_identical(output, x)\n\n        return x * y\n\n    output = broadcast_compat_data(x, (\"time\",), (\"time\",), copy=False)\n    assert_identical(output, x * x)\n\n    output = broadcast_compat_data(x, (\"time\",), (\"time\",), copy=True)\n    assert_identical(output, x * x)\n\n    x = xr.DataArray([1, 2, 3])\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, (\"time\",), (\"time\",), copy=True)\n\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, (\"time\",), (\"time\",), copy=True, reshape=True)\n\n    x = np.array([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError):\n        broadcast_compat_data(x, (\"time\",), (\"time\", \"space\"), copy=True, reshape=True)\n\n    x = np.array([[[1, 2, 3], [4, 5"], "sample_554": ["def test_font_styles_fontfamily(tmpdir):\n    # This test is for test of text rendering with font family\n    import matplotlib.font_manager as font_manager\n\n    fig, axs = plt.subplots(2, 2)\n    fonts = ['serif', 'sans-serif', 'cursive', 'fantasy', 'monospace']\n    for i, font in enumerate(fonts):\n        text = Text('Hello, world!', (0.5, 0.5), fontfamily=font, fontsize=24)\n        axs[i // 2, i % 2].add_artist(text)\n\n    axs[0, 0].set_title('serif')\n    axs[0, 1].set_title('sans-serif')\n    axs[1, 0].set_title('cursive')\n    axs[1, 1].set_title('fantasy')\n    axs[0, 0].set_xlabel('serif')\n    axs[0, 1].set_xlabel('sans-serif')\n    axs[1, 0].set_xlabel('cursive')\n    axs[1, 1].set_xlabel('fantasy')\n    axs[0, 0].set_ylabel('serif')\n    axs[0, 1].set_ylabel('sans-serif')\n    axs[1, 0].set_ylabel('cursive')\n    axs[1, 1].set_ylabel('fantasy')\n\n    for i in range(2):\n        for j in range(2):\n            axs[i, j].set_xlim(-1, 1)\n            axs[i, j].set_ylim(-1, 1)\n\n    for i in range(2):\n        axs[i, 0].set_aspect('equal')\n        axs[i, 1].set_aspect('equal')\n\n    plt.show()\n", "def test_draw_text_with_usetex():\n    import matplotlib.pyplot as plt\n    from matplotlib.text import Text\n\n    fig, ax = plt.subplots()\n\n    t = Text(0.5, 0.5, r'$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x, y)$',\n             rotation=45, usetex=True)\n    t.set_figure(fig)\n    ax.add_artist(t)\n\n    with pytest.raises(RuntimeError):\n        Text(0.5, 0.5, r'$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x, y)$',\n             rotation=45, usetex=False).get_window_extent()\n\n    fig.canvas.draw()\n\n    # Test get_window_extent with renderer\n    text = Text(0.5, 0.5, r'$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x, y)$',\n                rotation=45, usetex=True)\n    text.set_figure(fig)\n    ax.add_artist(text)\n    text.set_renderer(fig.canvas.renderer)\n    assert text.get_window_extent(fig.canvas.renderer) is not None\n\n    # Test get_window_extent with renderer=None\n    text = Text(0.5, 0.5, r'$\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x, y)$',\n                rotation=45, usetex=True)\n    text.set_figure(fig)\n    ax.add_artist(text)\n    assert text.get_window_extent() is not None\n\n    plt.close()\n", "def test_annotation_with_fancy_arrow():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 2)\n    ax.set_ylim(0, 2)\n    \n    text = Annotation(\"Test\", (1, 1), (0.5, 0.5), xycoords=\"data\",\n                     arrowprops={\"arrowstyle\": \"->\", \"color\": \"red\",\n                                 \"connectionstyle\": \"arc3,rad=0.2\"})\n    ax.add_patch(text)\n    \n    fig.canvas.draw_idle()\n    \n    # Check arrow position and style\n    assert text.arrow_patch.get_positions()[0] == (0.5, 0.5)\n    assert text.arrow_patch.get_arrowstyle() == \"->\"\n    assert text.arrow_patch.get_color() == \"red\"\n    assert text.arrow_patch.get_connectionstyle() == \"arc3,rad=0.2\"\n    \n    # Check text position\n    assert text.get_position() == (0.5, 0.5)\n    \n    # Check that the annotation is clipped when outside the axes\n    ax.set_xlim(3, 4)\n    text.set_xy((1, 1))\n    assert text.get_window_extent().width == 0\n    assert text.get_window_extent().height == 0\n    \n    # Check that the annotation is not clipped when inside the axes\n    ax.set_xlim(0, 2)\n    text.set_xy((1, 1))\n    assert text.get_window_extent().width > 0\n    assert text.get_window_extent().height > 0\n", "def test_text_wrap_figure(tst, image_compare):\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    text = Text(0.5, 0.5, \"Hello world\\nThis is a\\nwrapped text\", transform=ax.transAxes)\n    text.set_wrap(True)\n    text.set_linespacing(1.5)\n    ax.add_artist(text)\n    image_compare.rewind()\n    plt.show()\n", "def test_Annotation_draw_with_fancy_arrow():\n    fig, ax = plt.subplots()\n\n    ann = Annotation(\"Hello\", (1, 2),\n                    xytext=(1.1, 2.1), arrowprops=dict(arrowstyle='fancy',\n                                                         connectionstyle='arc3',\n                                                         relpos=(0.5, 0.5)),\n                    textcoords='data')\n\n    ax.add_patch(mpatches.Circle((1, 2), 1, edgecolor='black', facecolor='none'))\n    ann.set_transform(ax.transData)\n\n    # To avoid an edge case where the arrowhead is at the position of the text\n    # and the arrow is attached to the text box at the exact position of the\n    # arrowhead.\n    ann.arrow_patch.arrowstyle = \"fancy\"\n\n    with mpl.use('Agg') as mgr:\n        fig.canvas.draw()\n        fig.savefig(io.BytesIO(), bbox_inches='tight')\n        fig.savefig('reference.png')\n        # Disable this test for now since it's too brittle.\n        # assert check_figures_equal(fig, 'reference')\n"], "sample_789": ["def test_AdaBoostRegressor_decision_function():\n    \"\"\"Test the decision_function method of AdaBoostRegressor.\"\"\"\n    \n    # Create a simple dataset\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    \n    # Create an AdaBoostRegressor\n    abr = AdaBoostRegressor(n_estimators=10, random_state=42)\n    \n    # Fit the regressor\n    abr.fit(X, y)\n    \n    # Check that decision_function returns a 1D array\n    y_pred = abr.decision_function(X)\n    assert_equal(y_pred.ndim, 1)\n", "def test_AdaBoostRegressor_loss():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([2, 4, 5])\n    reg = AdaBoostRegressor(loss='square')\n    reg.fit(X, y)\n    assert reg.loss == 'square'\n\n    reg = AdaBoostRegressor(loss='exponential')\n    reg.fit(X, y)\n    assert reg.loss == 'exponential'\n\n    reg = AdaBoostRegressor(loss='linear')\n    reg.fit(X, y)\n    assert reg.loss == 'linear'\n\n    # Test that error is raised for invalid loss\n    with assert_raises_regexp(ValueError, \"loss must be 'linear', 'square', or 'exponential'\"):\n        AdaBoostRegressor(loss='invalid').fit(X, y)\n", "def test_ada_boost_regressor_predict():\n    \"\"\"Test the predict function of the AdaBoost regressor.\"\"\"\n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn.tree import DecisionTreeRegressor\n\n    # Create a decision tree regressor\n    tree = DecisionTreeRegressor(max_depth=3)\n\n    # Create an AdaBoost regressor with the tree as base estimator\n    regressor = AdaBoostRegressor(base_estimator=tree, n_estimators=5)\n\n    # Generate some random data\n    X = np.random.rand(10, 5)\n    y = np.random.rand(10)\n\n    # Fit the AdaBoost regressor to the data\n    regressor.fit(X, y)\n\n    # Test the predict function\n    assert regressor.predict(X).shape == (10,)\n\n    # Test the predict function with a new input\n    X_new = np.random.rand(5, 5)\n    assert regressor.predict(X_new).shape == (5,)\n\n    # Test the predict function with a new input and multiple samples\n    X_new = np.random.rand(20, 5)\n    assert regressor.predict(X_new).shape == (20,)\n", "def test_adaboost_regressor_median_prediction():\n    \"\"\"Check that the median prediction is used for AdaBoostRegressor.\n\n    This is a regression test for the bug #12345.\n    \"\"\"\n    # Create a decision tree regressor with a deep tree\n    dt = DecisionTreeRegressor(max_depth=10)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    dt.fit(X, y)\n    predictions = np.array([dt.predict(x) for x in X])\n\n    # Create an AdaBoost regressor with the decision tree as base estimator\n    abr = AdaBoostRegressor(base_estimator=dt, n_estimators=1)\n    abr.fit(X, y)\n    assert_array_almost_equal(abr.predict(X), predictions, decimal=4)\n\n    # Check that the median prediction is used when the ensemble size is 1\n    assert abr._get_median_predict(X, 1) == predictions\n", "def test_adaboost_regressor_base_estimator_cant_fit():\n    from sklearn.ensemble import AdaBoostRegressor\n    from sklearn.linear_model import LinearRegression\n    from sklearn.datasets import make_regression\n\n    X, y = make_regression(n_samples=100, n_features=10, n_informative=5,\n                           noise=0.0, random_state=0)\n    rng = np.random.RandomState(0)\n    X_test, y_test = make_regression(n_samples=10, n_features=10, n_informative=5,\n                                    noise=0.0, random_state=rng)\n\n    # A base estimator that is unable to fit the training data\n    class BaseEstimator(BaseEstimator):\n            return self\n\n            return np.random.rand(X.shape[0])\n\n    estimator = AdaBoostRegressor(base_estimator=BaseEstimator(), n_estimators=10)\n    with assert_raises_regexp(ValueError, \"Unable to compute regression error \"\n                                   \"since base_estimator does not have a \"\n                                   \"predict method\"):\n        estimator.fit(X, y)\n"], "sample_28": ["    def test_card_itersubcards(self):\n        card = fits.Card(\"CARD\", \"HISTORY    This is a test card.\")\n        cards = list(card._itersubcards())\n        self.assertEqual(len(cards), 1)\n        self.assertEqual(cards[0].keyword, \"CARD\")\n        self.assertEqual(cards[0].value, \"HISTORY    This is a test card.\")\n\n        card = fits.Card(\"CARD\", \"CONTINUE  This is a test card.\")\n        cards = list(card._itersubcards())\n        self.assertEqual(len(cards), 2)\n        self.assertEqual(cards[0].keyword, \"CARD\")\n        self.assertEqual(cards[0].value, \"CONTINUE  This\")\n        self.assertEqual(cards[1].keyword, \"CARD\")\n        self.assertEqual(cards[1].value, \"CONTINUE  is a test card.\")\n\n        card = fits.Card(\"CARD\", \"HISTORY    This is a test card.\\nThis is the second part.\")\n        cards = list(card._itersubcards())\n        self.assertEqual(len(cards), 1)\n        self.assertEqual(cards[0].keyword, \"CARD\")\n        self.assertEqual(cards[0].value, \"HISTORY    This is a test card.\\nThis is the second part.\")\n\n        card = fits.Card(\"CARD\", \"CONTINUE  This is a test card.\\nThis is the second part.\")\n        cards = list(card._itersubcards())\n        self.assertEqual(len(cards), 2)\n        self.assertEqual(cards[0].keyword, \"CARD\")\n        self.assertEqual(cards[0].value, \"CONTINUE  This is a test card.\")\n        self.assertEqual(cards[1].keyword, \"CARD\")\n        self.assertEqual(cards[1].value, \"CONTINUE  This is the second part.\")\n\n        card = fits.Card(\"CARD\", \"HISTORY    This is a test card.\\nThis is the second part.\\nThis is the third part.\")\n        cards = list(card._itersubcards())\n        self.assertEqual(len(cards), 1)\n        self.assertEqual(cards[0].keyword, \"CARD\")\n        self.assertEqual(cards[0].value, \"HISTORY    This is a test card.\\nThis is the second part.\\nThis is the third part.\")\n\n        card = fits.Card(\"CARD\", \"CONTINUE  This is a test card.\\nThis is the second part.\\nThis is the third part.\")\n        cards =", "    def test_card_fix_value(self):\n        card = Card(\"CARD1\", \"001.000e+00\")\n        card.verify(option=\"fix\")\n        assert card.value == 1.0\n        assert card.image == \"CARD1 =  1.000e+00\"\n", "def test_card_keyword_attribute_modification():\n    card = fits.Card(\"KEYWORD\")\n    assert card.keyword == \"KEYWORD\"\n    with pytest.raises(AttributeError):\n        card.keyword = \"NEWKEYWORD\"\n    assert card.keyword == \"KEYWORD\"\n    card._keyword = \"NEWKEYWORD\"\n    assert card.keyword == \"NEWKEYWORD\"\n", "def test_HIERARCH_card_keyword():\n    # Test HIERARCH card keyword\n    card = fits.Card.fromstring(\"HIERARCH  MJD-OBS     =  54412.4567\")\n    assert card.keyword == \"HIERARCH MJD-OBS\"\n    assert card.hierarch\n\n    card = fits.Card.fromstring(\"HIERARCH  MJD-OBS     54500.1234\")\n    assert card.keyword == \"MJD-OBS\"\n    assert card.hierarch\n\n    card = fits.Card.fromstring(\"MJD-OBS     54500.1234\")\n    assert card.keyword == \"MJD-OBS\"\n    assert not card.hierarch\n\n", "    def test_verify_keyword_uppercase(self):\n        card = Card(\"key\", \"value\")\n        card._image = \"    key  = value  \"\n        card._verified = False\n        result = card._verify(\"warn\")\n        self.assertIn(\n            dict(err_text=\"Card key is not upper case.\", fix_text=\"Fixed key card to meet the FITS standard.\"),\n            result._errdict),\n        card._verified = True\n"], "sample_676": ["def test_pytest_report_header(config, expected):\n    reporter = TerminalReporter(config, sys.stdout)\n    result = reporter.pytest_report_header(config)\n    assert \"\".join(result) == expected\n", "def test_getreportopt(input, expected):\n    plugins = [DistInfo(project_name=\"test\", version=1)]\n    config = pytest.Config(plugins=plugins)\n    reportopts = getreportopt(config)\n    assert reportopts == expected\n", "def test_getreportopt_reportchars_no_warnings(config):\n    config.option.disable_warnings = True\n    config.option.reportchars = \"P\"\n    assert getreportopt(config) == \"P\"\n", "def test_get_progress_information_message(config, expected):\n    reporter = TerminalReporter(config, sys.stdout)\n    reporter._show_progress_info = \"progress\"\n    reporter._session.testscollected = 10\n    reporter._progress_nodeids_reported = [1, 2, 3, 4, 5]\n    assert reporter._get_progress_information_message() == expected\n\n", "def test_get_summary_stats_line(config, expected):\n    reporter = TerminalReporter(pytest.config, None)\n    reporter.config.setini(\"console_output_style\", config[\"console_output_style\"])\n    result = build_summary_stats_line(reporter.stats)\n    assert result[0] == expected\n"], "sample_20": ["    def test_read_fits_table_masked_column(self):\n        mixin_cols = {\n            \"a\": Column(np.array([1, 2, 3, 4], dtype=int), name=\"a\", mask=np.array([True, False, True, False])),\n            \"b\": Column(np.array([\"a\", \"b\", \"c\", \"d\"], dtype=\"U1\"), name=\"b\", mask=np.array([False, False, False, True])),\n            \"c\": Column(np.array([2.3, 4.5, 6.7, np.nan], dtype=float), name=\"c\"),\n        }\n        input_table = Table(mixin_cols, masked=True)\n        expected_table = Table(\n            [\n                Column(np.array([1, 2, 3, 4], dtype=int), name=\"a\", mask=np.array([True, False, True, False])),\n                Column(np.array([\"a\", \"b\", \"c\", \"d\"], dtype=\"U1\"), name=\"b\", mask=np.array([False, False, False, True])),\n                Column(np.array([2.3, 4.5, 6.7, np.nan], dtype=float), name=\"c\"),\n            ],\n            masked=True,\n        )\n        output_table = read_table_fits(input_table, character_as_bytes=True)\n        assert equal_data(output_table, expected_table)\n", "    def setup_method(self):\n        self.table1 = Table(\n            data=np.array([(1, 2), (3, 4)], dtype=[(\"a\", int), (\"b\", int)]),\n            meta={\"comments\": [\"This is a comment\"]},\n        )\n        self.table2 = Table(\n            data=np.array([(1.2, 2.3), (3.4, 4.5)], dtype=[(\"c\", float), (\"d\", float)]),\n            meta={\"comments\": [\"This is another comment\"]},\n        )\n        self.hdulist = HDUList([self.table1, self.table2])\n", "    def test_read_table_fits_memmap(self):\n        with connect(\"table.fits\", mode=\"w\", overwrite=True) as hdu:\n            hdu.append(self.data)\n            hdu.flush()\n        with connect(\"table.fits\", mode=\"r\") as hdu:\n            table = read_table_fits(hdu)\n            assert equal_data(table, self.data)\n", "    def test_read_table_fits_astropy_native(self):\n        # Create a FITS file with a Time mixin column\n        with fits.open(\n            get_pkg_data_filename(\"io/mixin_columns/fits_time.fits\"), mode=\"update\"\n        ) as hdul:\n            time_col = hdul[1].data[\"TIME\"]\n            time_col.info.format = \"iso\"\n            time_col.info.meta = {\"description\": \"Test time column\"}\n            hdul.flush()\n\n        # Read the table in with astropy_native=True\n        table = Table.read(get_pkg_data_filename(\"io/mixin_columns/fits_time.fits\"), hdu=1)\n\n        # Check that the Time column is a mixin column\n        assert isinstance(table[\"TIME\"], Time)\n\n        # Check that the mixin column attributes are preserved\n        for attr in compare_attrs[\"TIME\"]:\n            assert getattr(table[\"TIME\"], attr) == mixin_cols[\"TIME\"][attr]\n\n        # Check that the serialized columns are preserved\n        assert table.meta[\"__serialized_columns__\"] == mixin_cols[\"TIME\"].info.meta\n\n        # Check that the mixin column's description is preserved\n        assert table[\"TIME\"].info.description == \"Test time column\"\n", "    def setup_class(self):\n        self.data = np.array(\n            list(zip([1, 2, 3, 4], [\"a\", \"b\", \"c\", \"d\"], [2.3, 4.5, 6.7, 8.9])),\n            dtype=[(\"a\", int), (\"b\", \"U1\"), (\"c\", float)],\n        )\n"], "sample_1135": ["def test_as_coefficients_dict():\n    a, b, c = symbols('a b c')\n    # Test simple cases\n    assert Mul(a, b).as_coefficients_dict() == {a*b: 1}\n    assert Mul(a, b, c).as_coefficients_dict() == {a*b*c: 1}\n    # Test with zero coefficients\n    assert Mul(a, b, a, b, c).as_coefficients_dict() == {a*b*c: 1}\n    # Test non-commutative case\n    A, B = symbols('A B', commutative=False)\n    assert Mul(A, B).as_coefficients_dict() == {A*B: 1}\n    assert Mul(A, A, B).as_coefficients_dict() == {A**2*B: 1}\n    # Test with coefficients\n    assert Mul(2, a, b).as_coefficients_dict() == {2*a*b: 1}\n    assert Mul(-3, a, b).as_coefficients_dict() == {-3*a*b: 1}\n    # Test with Rational coefficients\n    assert Mul(Rational(1, 2), a, b).as_coefficients_dict() == {(1/2)*a*b: 1}\n    assert Mul(Rational(-3, 2), a, b).as_coefficients_dict() == {(-3/2)*a*b: 1}\n", "def test_unevaluated_Mul(self):\n    from sympy import Mul, factorint\n    a = Mul(2, x, y)\n    b = Mul(2, x, y)\n    assert a.is_Mul\n    assert a == b\n    assert Mul(*a.args) == a\n    assert a.as_two_terms() == (2, x*y)\n    assert a.as_coefficients_dict() == {x*y: 2}\n    assert a.as_coefficients_dict()[a] == 2\n    assert a.as_coefficients_dict()[x*y] == 2\n    assert a.as_coefficients_dict()[x] == 0\n    assert a.as_coefficients_dict()[y] == 0\n    assert a.as_coeff_Mul() == (2, x*y)\n    assert a.as_coeff_Mul(rational=True) == (2, x*y)\n    assert a.as_coeff_Mul(rational=False) == (2, x*y)\n    assert a.as_coeff_Mul(rational=True, [x])[0] == 2\n    assert a.as_coeff_Mul(rational=True, [x])[1] == (y,)\n    assert a.as_coeff_Mul(rational=False, [x])[0] == 2\n    assert a.as_coeff_Mul(rational=False, [x])[1] == (y,)\n    assert a.as_real_imag() == (2, 0)\n    assert a.as_real_imag(True) == (2, 0)\n    assert a.as_real_imag(False) == (2, 0)\n    assert a.as_imag() == (0, 2)\n    assert a.as_imag(True) == (0, 2)\n    assert a.as_imag(False) == (0, 2)\n    assert a.as_real() == (2, 0)\n    assert a.as_real(True) == (2, 0)\n    assert a.as_real(False) == (2, 0)\n    assert a.as_coeff_add() == (2, x*y)\n    assert a.as_coeff_add(rational=True) == (2, x*y)\n    assert a.as_coeff_add(rational=False) == (2, x*y)\n    assert a.as_coeff_add(rational=True, [x])[0] == 2\n    assert a.as_coeff_add(rational=True, [x])[1] == (y,)\n    assert a.as", "def test_Mul_as_two_terms():\n    \"\"\"Check that as_two_terms returns the expected result.\n\n    \"\"\"\n    M = Mul(x**2, x, y, y, z)\n    assert M.as_two_terms() == (x**2 * x * y * y * z, S.One)\n\n    # Test case where the first argument is one\n    M = Mul(S.One, x, y, y, z)\n    assert M.as_two_terms() == (x * y * y * z, S.One)\n\n    # Test case where the arguments are not commutative\n    M = Mul(x, y, z)\n    assert M.as_two_terms() == (x, y * z)\n\n    # Test case where the arguments are commutative\n    M = Mul(x, y, z)\n    assert M.as_two_terms() == (x * y * z, S.One)\n\n    # Test case where the expression has a single argument\n    M = Mul(x**2)\n    assert M.as_two_terms() == (x**2, S.One)\n\n    # Test case where the expression has no arguments\n    M = Mul()\n    assert M.as_two_terms() == (S.One, S.One)\n", "def test_Mul_coefficients_dict():\n    a = x**2 + x\n    d = a.as_coefficients_dict()\n    assert d[x**2] == 1\n    assert d[x] == 1\n    assert d[S.One] == 1\n\n    a = x**2 - x\n    d = a.as_coefficients_dict()\n    assert d[x**2] == 1\n    assert d[x] == -1\n    assert d[S.One] == 1\n\n    a = x**2 + 2*x + 1\n    d = a.as_coefficients_dict()\n    assert d[x**2] == 1\n    assert d[x] == 2\n    assert d[S.One] == 1\n\n    a = x**2 + 2*x + 2\n    d = a.as_coefficients_dict()\n    assert d[x**2] == 1\n    assert d[x] == 2\n    assert d[S.One] == 2\n\n    a = x**2 + Rational(1, 2) * x + 1\n    d = a.as_coefficients_dict()\n    assert d[x**2] == 1\n    assert d[x] == Rational(1, 2)\n    assert d[S.One] == 1\n\n    # Test zero\n    a = 0\n    d = a.as_coefficients_dict()\n    assert d[x] == 0\n\n    # Test non-commutative\n    NC = sympify('NC')\n    a = NC**2 + NC\n    d = a.as_coefficients_dict()\n    assert d[NC**2] == 1\n    assert d[NC] == 1\n", "    def test_new_args(self):\n        from sympy.core.core import Expr\n        from sympy.core.expr import Expr\n        self.assertEqual(Expr._new_rawargs(1, 2, 3), Mul(1, 2, 3))\n        self.assertEqual(Expr._new_rawargs(1, 2, 3, 4), Mul(1, 2, 3, 4))\n        self.assertEqual(Expr._new_rawargs(1, 2, 3, 4, 5), Mul(1, 2, 3, 4, 5))\n        self.assertEqual(Expr._new_rawargs(1, 2), Mul(1, 2))\n        self.assertEqual(Expr._new_rawargs(), Mul())\n"], "sample_836": ["def test_unique_labels_multiclass_multioutput():\n    y1 = np.array([[1, 0], [0, 1]])\n    y2 = np.array([[1, 1], [0, 0]])\n    with pytest.raises(ValueError):\n        unique_labels(y1, y2)\n", "def test_type_of_target_empty_array():\n    # Test case for type_of_target when y is an empty array\n    assert type_of_target(np.array([])) == 'unknown'\n    assert type_of_target(np.array([], dtype=np.int8)) == 'unknown'\n    assert type_of_target(np.array([], dtype=np.float32)) == 'unknown'\n    assert type_of_target(np.array([], dtype=np.object)) == 'unknown'\n", "def test_unique_labels_multioutput():\n    \"\"\"Test the unique_labels function for multioutput targets.\"\"\"\n    # Test a 2d array with two columns\n    y = np.array([[1, 2], [2, 2], [3, 2]])\n    expected_labels = np.array([1, 2, 3])\n    assert_array_equal(unique_labels(y), expected_labels)\n\n    # Test a 2d array with more than 2 columns\n    y = np.array([[1, 2, 3], [2, 2, 3], [3, 2, 3]])\n    expected_labels = np.array([1, 2, 3])\n    assert_array_equal(unique_labels(y), expected_labels)\n\n    # Test a sparse matrix\n    y = csc_matrix([[1, 2], [2, 2], [3, 2]])\n    expected_labels = np.array([1, 2, 3])\n    assert_array_equal(unique_labels(y.toarray()), expected_labels)\n\n    # Test an empty list of targets\n    with pytest.raises(ValueError):\n        unique_labels([])\n\n    # Test a list with a mix of multioutput and single output targets\n    y = np.array([[1, 2], [2, 2], [3, 2], [4]])\n    with pytest.raises(ValueError):\n        unique_labels(y)\n", "def test_unique_labels_invalid_input():\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3], ['a', 'b', 'c'])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 'c'])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 4], [1, 2, 3])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 4])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], ['a', 'b', 'c'])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 4, 'a'])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 'a'])\n    with pytest.raises(ValueError):\n        unique_labels(['a', 'b', 'c'], [1, 2, 3])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 4, 5])\n    with pytest.raises(ValueError):\n        unique_labels([1, 2, 3], [1, 2, 3, 4, 5], [1, 2, 3])\n", "def test_unique_labels_empty_array():\n    # Empty array should raise a ValueError\n    with assert_raises(ValueError):\n        unique_labels([])\n\n    # Empty array with a specific type should raise a ValueError\n    with assert_raises(ValueError):\n        unique_labels([], type_of_target=np.array([1, 2]))\n\n    # Test that passing an empty array with different types raises a ValueError\n    with assert_raises(ValueError):\n        unique_labels([], [np.array([1, 2]), np.array([1, 2])])\n\n    # Test that passing an empty array with a mix of types raises a ValueError\n    with assert_raises(ValueError):\n        unique_labels([], [np.array([1, 2]), [1, 2]])\n"], "sample_353": ["    def test_invalid_password_not_bypassed(self):\n        # Test that we don't create a user with an invalid password\n        # without bypassing the validation.\n        inputs = {'password': 'weakpassword'}\n        with mock_inputs(inputs):\n            with self.assertRaises(CommandError):\n                call_command('createsuperuser', '--username', 'alice', '--password', inputs['password'], '--noinput')\n", "    def test_username_field_not_provided_in_options_and_env(self):\n        with self.assertRaises(CommandError) as cm:\n            call_command('createsuperuser', '--database', 'default')\n        self.assertIn('You must use --username with --noinput.', str(cm.exception))\n", "    def test_createsuperuser_noinput(self):\n        \"\"\"\n        Verifies that createsuperuser works correctly with all options\n        specified in the environment.\n        \"\"\"\n        with self.assertLogs(level='INFO') as cm:\n            call_command('createsuperuser', '--noinput', '--database', 'default',\n                         '--username', 'alice', '--email', 'alice@example.com',\n                         '--date_of_birth', '1990-01-01', '--first_name', 'Alice',\n                         '--password', 'password123')\n            self.assertRegex(cm.output[0], r'Superuser created successfully\\.')\n", "    def test_not_running_in_tty(self):\n        with mock.patch('sys.stdin', MockTTY()):\n            with self.assertRaises(createsuperuser.NotRunningInTTYException):\n                call_command('createsuperuser', 'alice', database='default')\n", "    def test_username_taken_error_message(self):\n        # Test that the username is taken error message is displayed when creating a superuser.\n        with self.assertRaises(CommandError) as cm:\n            with override_settings(LOADINGSITE_NOT_ACTIVE=True):\n                call_command('createsuperuser', username='alice', database='default')\n        self.assertIn(\"That username is already taken.\", str(cm.exception))\n"], "sample_26": ["    def test_scale(self, data, header, data_scaled, do_not_scale_image_data, uint, scale_back):\n        data_scaled.data = data\n        data_scaled.header = header\n\n        # Test normal scaling\n        data_scaled.scale()\n        assert_equal(data_scaled.data.dtype, np.float32)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 0)\n\n        # Test scaling to int16\n        data_scaled.scale(type=\"int16\")\n        assert_equal(data_scaled.data.dtype, np.int16)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 32768)\n\n        # Test scaling to uint8\n        data_scaled.scale(type=\"uint8\")\n        assert_equal(data_scaled.data.dtype, np.uint8)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 0)\n\n        # Test scaling to uint16\n        data_scaled.scale(type=\"uint16\")\n        assert_equal(data_scaled.data.dtype, np.uint16)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 1)\n\n        # Test scaling with custom BSCALE and BZERO\n        data_scaled.scale(bscale=2.0, bzero=3.0)\n        assert_equal(data_scaled.data.dtype, np.float32)\n        assert_equal(data_scaled.header[\"BSCALE\"], 2.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 3.0)\n\n        # Test scaling with BSCALE and BZERO not in header\n        data_scaled.header = fits.Header(cards=[(\"SIMPLE\", True)])\n        data_scaled.scale()\n        assert_equal(data_scaled.data.dtype, np.float32)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data_scaled.header[\"BZERO\"], 0)\n\n        # Test do_not_scale_image_data\n        data_scaled.scale(do_not_scale_image_data=True)\n        assert_equal(data_scaled.data.dtype, np.float32)\n        assert_equal(data_scaled.header[\"BSCALE\"], 1.0)\n        assert_equal(data", "    def test_section(self):\n        # Check that the shape of the section is correct\n        hdu = fits.ImageHDU(data=np.random.randint(0, 100, size=(10, 10)))\n        section = hdu.section\n        assert_equal(section.shape, (10, 10))\n        \n        # Check that slicing works correctly\n        sliced_section = section[5:7, 5:7]\n        assert_equal(sliced_section.shape, (2, 2))\n        assert_equal(sliced_section.dtype, hdu.data.dtype)\n        \n        # Check that a single index works correctly\n        single_section = section[5]\n        assert_equal(single_section.shape, (10,))\n        assert_equal(single_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with an ellipsis works correctly\n        ellipsis_section = section[..., 5]\n        assert_equal(ellipsis_section.shape, (10,))\n        assert_equal(ellipsis_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with a single integer and an ellipsis works correctly\n        int_ellipsis_section = section[5, ...]\n        assert_equal(int_ellipsis_section.shape, (10,))\n        assert_equal(int_ellipsis_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with two integers and an ellipsis works correctly\n        int_int_ellipsis_section = section[5, 5, ...]\n        assert_equal(int_int_ellipsis_section.shape, (1,))\n        assert_equal(int_int_ellipsis_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with a slice and an ellipsis works correctly\n        slice_ellipsis_section = section[5:7, ..., 5]\n        assert_equal(slice_ellipsis_section.shape, (2,))\n        assert_equal(slice_ellipsis_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with a slice, an integer, and an ellipsis works correctly\n        slice_int_ellipsis_section = section[5:7, 5, ...]\n        assert_equal(slice_int_ellipsis_section.shape, (2,))\n        assert_equal(slice_int_ellipsis_section.dtype, hdu.data.dtype)\n        \n        # Check that an index with a slice, an integer, and an integer, and an ellipsis works correctly\n        slice_int_int_ellipsis_section = section[5:7, 5, 5, ...]\n        assert", "    def test_section(self, indices):\n        # Create a simple 3D FITS image\n        hdu = fits.PrimaryHDU(np.random.rand(10, 10, 10))\n        hdu.verify(\"error\")\n\n        # Create a section of the image\n        section = hdu.section\n        section_data = section[indices]\n\n        # Check the shape of the section\n        assert_equal(section_data.shape, tuple(indices))\n\n        # Check that the section is a view of the original data\n        assert_equal(id(section_data), id(hdu.data[indices]))\n\n        # Check that the section has the same data type as the original data\n        assert_equal(section_data.dtype, hdu.data.dtype)\n\n        # Check that the section has the same BZERO and BSCALE values as the original data\n        assert_equal(section_data.data.dtype, hdu.data.data.dtype)\n\n        # Check that the section has the same header as the original HDU\n        assert_equal(section.header, hdu.header)\n\n        # Check that the section is not a copy of the original data\n        hdu.data[indices] = np.zeros_like(hdu.data[indices])\n        assert_equal(hdu.data[indices], section_data)\n", "    def test_uint_scaling(self):\n        with fits.open(get_pkg_data_filename(\"test_image.fits\")) as hdul:\n            hdu = hdul[0]\n            self.assertEqual(hdu.header[\"BZERO\"], 0)\n            self.assertEqual(hdu.header[\"BSCALE\"], 1)\n            data = hdu.data.astype(np.uint8)\n            hdu.data = data\n            self.assertEqual(hdu.header[\"BZERO\"], 0)\n            self.assertEqual(hdu.header[\"BSCALE\"], 1)\n\n        with fits.open(get_pkg_data_filename(\"test_image.fits\")) as hdul:\n            hdu = hdul[0]\n            self.assertEqual(hdu.header[\"BZERO\"], 0)\n            self.assertEqual(hdu.header[\"BSCALE\"], 1)\n            data = hdu.data.astype(np.int8)\n            hdu.data = data\n            self.assertEqual(hdu.header[\"BZERO\"], -128)\n            self.assertEqual(hdu.header[\"BSCALE\"], 1)\n", "    def test_scale(\n        self, bscale, bzero, blank, dtype, bitpix, expected_scaled_dtype"], "sample_399": ["    def test_combine_aggregates(self):\n        # Test combining aggregates on the same field\n        q1 = Author.objects.annotate(avg_age=Avg('age'))\n        q2 = Author.objects.annotate(max_age=Max('age'))\n        q = q1.combine(q2, 'OR')\n        self.assertEqual(list(q.values('avg_age', 'max_age')), [\n            {'avg_age': 34.0, 'max_age': 57}\n        ])\n", "    def test_add_annotation(self):\n        from datetime import date\n\n        qs = Author.objects.annotate(age_group=Case(\n            When(age__lte=30, then=Value('young')),\n            When(age__gte=30, then=Value('old')),\n            default=Value('unknown')\n        ))\n\n        self.assertEqual(list(qs.values_list('name', 'age_group')), [\n            ('Adrian Holovaty', 'young'),\n            ('Jacob Kaplan-Moss', 'young'),\n            ('Brad Dayley', 'old'),\n            ('James Bennett', 'young'),\n            ('Jeffrey Forcier', 'old'),\n            ('Paul Bissex', 'young'),\n            ('Wesley J. Chun', 'young'),\n            ('Peter Norvig', 'old'),\n            ('Stuart Russell', 'old'),\n        ])\n\n        # Test when field is not in model\n        qs = Author.objects.annotate(age_group=Case(\n            When(age__lte=30, then=Value('young')),\n            When(age__gte=30, then=Value('old')),\n            default=Value('unknown')\n        ))\n\n        with self.assertRaises(FieldError):\n            qs.values_list('name', 'age_group')\n\n        # Test with multiple annotations\n        qs = Author.objects.annotate(\n            age_group=Case(\n                When(age__lte=30, then=Value('young')),\n                When(age__gte=30, then=Value('old')),\n                default=Value('unknown')\n            ),\n            new_age=Case(\n                When(age__lte=30, then=Value(30)),\n                When(age__gte=30, then=Value(50)),\n                default=Value(0)\n            )\n        )\n\n        self.assertEqual(list(qs.values_list('name', 'age_group', 'new_age')), [\n            ('Adrian Holovaty', 'young', 30),\n            ('Jacob Kaplan-Moss', 'young', 30),\n            ('Brad Dayley', 'old', 50),\n            ('James Bennett', 'young', 30),\n            ('Jeffrey Forcier', 'old', 50),\n            ('Paul Bissex', 'young', 30),\n            ('Wesley J. Chun', 'young', 30),\n            ('Peter Norvig', 'old', 50),\n            ('Stuart Russell', 'old', 50),\n        ])\n\n        # Test with raw SQL annotation\n        qs = Author.objects.annotate(\n            age_group=Case(\n                When(age__lte=30, then=Value('young')),\n", "    def test_clone(self):\n        query = Book.objects.all()\n        clone = query._clone()\n        self.assertEqual(query.alias_refcount, clone.alias_refcount)\n        self.assertEqual(query.alias_map, clone.alias_map)\n        self.assertEqual(query.external_aliases, clone.external_aliases)\n        self.assertEqual(query.table_map, clone.table_map)\n        self.assertEqual(query.where, clone.where)\n        self.assertEqual(query.annotations, clone.annotations)\n        self.assertEqual(query.extra, clone.extra)\n        self.assertEqual(query.annotation_select, clone.annotation_select)\n        self.assertEqual(query.extra_select, clone.extra_select)\n        self.assertEqual(query.deferred_loading, clone.deferred_loading)\n        self.assertEqual(query.explain_info, clone.explain_info)\n        self.assertEqual(query.combine, clone.combine)\n        self.assertEqual(query.get_children_from_q, clone.get_children_from_q)\n        self.assertEqual(query.get_field_names_from_opts, clone.get_field_names_from_opts)\n        self.assertEqual(query.get_children_from_q, clone.get_children_from_q)\n        self.assertEqual(query.get_children_from_q, clone.get_children_from_q)\n", "    def test_aggregate_fails_with_multiple_aggregate_annotations(self):\n        with self.assertRaises(FieldError):\n            Author.objects.aggregate(Avg('id'), Max('id'))\n", "    def test_subquery_and_annotation(self):\n        # Test creating a subquery for aggregate() and annotating the outer query with the result\n        # of the subquery.\n        query = Author.objects.aggregate(\n            num_authors=Count(\"id\"),\n            avg_age=Avg(\"age\"),\n            max_age=Max(\"age\"),\n            min_age=Min(\"age\"),\n            sum_age=Sum(\"age\"),\n            std_dev_age=StdDev(\"age\"),\n            var_age=Variance(\"age\"),\n            greatest_age=Greatest(\"age\", \"age\", \"age\"),\n            coalesce_age=Coalesce(\"age\", 30),\n            cast_age=Cast(\"age\", IntegerField()),\n            trunc_date_age=TruncDate(\"age\"),\n            trunc_hour_age=TruncHour(\"age\"),\n        )\n        self.assertEqual(query[\"num_authors\"], 9)\n        self.assertAlmostEqual(query[\"avg_age\"], 36.22222222222222)\n        self.assertEqual(query[\"max_age\"], 57)\n        self.assertEqual(query[\"min_age\"], 25)\n        self.assertEqual(query[\"sum_age\"], 324)\n        self.assertAlmostEqual(query[\"std_dev_age\"], 9.79370050719617)\n        self.assertAlmostEqual(query[\"var_age\"], 96.18518518518518)\n        self.assertEqual(query[\"greatest_age\"], 57)\n        self.assertEqual(query[\"coalesce_age\"], 34)\n        self.assertEqual(query[\"cast_age\"], 34)\n        self.assertEqual(query[\"trunc_date_age\"], datetime.date(2008, 1, 1))\n        self.assertEqual(query[\"trunc_hour_age\"], datetime.time(0, 0))\n"], "sample_979": ["def test_matrix_element_diff():\n    A = MatrixSymbol('A', 2, 2)\n    i, j = symbols('i j', integer=True)\n    M = A[i, j]\n    assert diff(M, A) == KroneckerDelta(i, 0)*KroneckerDelta(j, 0)*A + \\\n        KroneckerDelta(i, 1)*KroneckerDelta(j, 1)*A\n    assert diff(M, i) == A[i, j]\n    assert diff(M, j) == A[i, j]\n    assert diff(M, x) == 0\n    assert diff(M, x, x) == 0\n    assert diff(M, x, x, x) == 0\n    assert diff(M, i, x) == 0\n    assert diff(M, j, x) == 0\n    assert diff(M, i, i) == A[i, j]\n    assert diff(M, j, j) == A[i, j]\n    assert diff(M, i, j) == A[i, j]\n    assert diff(M, j, i) == A[i, j]\n    assert diff(M, x, i) == 0\n    assert diff(M, x, j) == 0\n    assert diff(M, x, x, i) == 0\n    assert diff(M, x, x, j) == 0\n    assert diff(M, i, x, i) == A[i, j]\n    assert diff(M, i, x, j) == 0\n    assert diff(M, j, x, i) == 0\n    assert diff(M, j, x, j) == A[i, j]\n    assert diff(M, i, i, i) == A[i, j]\n    assert diff(M, i, i, j) == 0\n    assert diff(M, j, i, i) == 0\n    assert diff(M, j, i, j) == A[i, j]\n    assert diff(M, i, j, i) == A[i, j]\n    assert diff(M, i, j, j) == 0\n    assert diff(M, j, i, i) == 0\n    assert diff(M, j, i, j) == A[i, j]\n    assert diff(M, j, j, i) == 0\n    assert diff(M", "def test_MatrixSymbol_equals():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A.equals(B) == False\n    A[0, 0] = 1\n    assert A.equals(B) == False\n    B[0, 0] = 1\n    assert A.equals(B) == True\n", "def test_matrix_symbol_shape():\n    A = MatrixSymbol('A', 2, 3)\n    assert A.shape == (2, 3)\n", "def test_matrix_symbol_shape():\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    assert A.shape == (n, m)\n\n    # Test shape with symbolic dimensions\n    n, m = symbols('n m', integer=True)\n    A = MatrixSymbol('A', n, m)\n    assert A.shape == (n, m)\n\n    # Test shape with non-symbolic dimensions\n    A = MatrixSymbol('A', 3, 4)\n    assert A.shape == (3, 4)\n\n    # Test shape with zero dimensions\n    A = MatrixSymbol('A', 0, 0)\n    assert A.shape == (0, 0)\n\n    # Test shape with negative dimensions\n    with raises(ValueError):\n        MatrixSymbol('A', -1, 1)\n\n    # Test shape with non-integer dimensions\n    with raises(ValueError):\n        MatrixSymbol('A', 3.5, 4)\n\n    # Test shape with zero in the middle\n    A = MatrixSymbol('A', 0, 3)\n    assert A.shape == (0, 3)\n\n    # Test shape with zero at the end\n    A = MatrixSymbol('A', 3, 0)\n    assert A.shape == (3, 0)\n", "def test_equals_matrix_types():\n    from sympy import Identity, MatrixSymbol\n    A = Identity(2)\n    B = MatrixSymbol('B', 2, 2)\n    assert A.equals(B)\n"], "sample_65": ["    def test_get_context_data(self):\n        \"\"\"Test the JavaScriptCatalog.get_context_data method.\"\"\"\n        response = JavaScriptCatalog().get_response({})\n        self.assertIn('catalog', response.context)\n        self.assertIn('formats', response.context)\n        self.assertIn('plural', response.context)\n", "    def test_get(self):\n        request = RequestFactory().get('/')\n        request.LANGUAGE_CODE = 'en'\n        response = JavaScriptCatalog().get(request)\n        self.assertEqual(response.status_code, 200)\n", "    def _get_request(self, path=None, **kwargs):\n        if not path:\n            path = reverse('jsi18n')\n        return RequestFactory().get(path, **kwargs)\n", "    def test_get_catalog(self):\n        \"\"\"Test the get_catalog method.\"\"\"\n        request = RequestFactory().get('/test')\n        view = JavaScriptCatalog()\n        locale = get_language()\n        domain = 'test_domain'\n        packages = ['test_app']\n        view._set_language = lambda request: None\n        view._get_paths = lambda x: ['/path/to/locale']\n        view.translation = DjangoTranslation(locale, domain=domain, localedirs=['/path/to/locale'])\n        view._plural_string = 'nplurals=3; plural=n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2;'\n        view._num_plurals = 3\n        context = view.get_context_data()\n        catalog = view.get_catalog()\n        self.assertIsInstance(catalog, dict)\n        self.assertEqual(context['catalog'], catalog)\n        self.assertIn('plural', context)\n", "    def setUp(self):\n        self.locale_path = path.join(settings.LANGUAGE_CODE, 'locale')\n        self.locale_dir = locale_dir(self.locale_path)\n        self.url = reverse('javascript-catalog')\n        self.request_factory = RequestFactory()\n"], "sample_169": ["    def test_defused_xml_parser(self):\n        # Test that the DefusedExpatParser forbids DTDs\n        parser = DefusedExpatParser()\n        with self.assertRaises(DTDForbidden):\n            parser.start_doctype_decl('foo', 'http://example.com/foo.dtd', 'public foo.dtd', True)\n\n        # Test that the DefusedExpatParser forbids entities\n        parser = DefusedExpatParser()\n        with self.assertRaises(EntitiesForbidden):\n            parser.entity_decl('foo', False, 'bar', None, 'http://example.com/bar.ent', 'public bar.ent')\n\n        # Test that the DefusedExpatParser forbids external entity references\n        parser = DefusedExpatParser()\n        with self.assertRaises(ExternalReferenceForbidden):\n            parser.external_entity_ref_handler(None, 'bar', 'http://example.com/bar.ent', 'http://example.com/bar.ent')\n", "    def test_defused_xml_parser(self):\n        # Test that the parser forbids external entity references\n        with self.assertRaises(ExternalReferenceForbidden):\n            xml = '<!ENTITY % e SYSTEM \"http://example.com/external-entity.dtd\">'\n            parser = DefusedExpatParser()\n            parser.feed(xml)\n\n        # Test that the parser forbids entity declarations\n        with self.assertRaises(EntitiesForbidden):\n            xml = '<!ENTITY % e \"external-entity-value\">'\n            parser = DefusedExpatParser()\n            parser.feed(xml)\n\n        # Test that the parser forbids document type definitions\n        with self.assertRaises(DTDForbidden):\n            xml = '<!DOCTYPE doc SYSTEM \"http://example.com/dtd.dtd\">'\n            parser = DefusedExpatParser()\n            parser.feed(xml)\n\n        # Test that the parser allows well-formed XML\n        xml = '<root><child>hello</child></root>'\n        parser = DefusedExpatParser()\n        parser.feed(xml)\n        self.assertEqual(parser.getroot().tagName, 'root')\n\n        # Test that the parser allows entity references\n        xml = '<root><child>&e;</child>'\n        parser = DefusedExpatParser()\n        parser.feed(xml)\n        self.assertEqual(parser.getroot().getElementsByTagName('child')[0].childNodes[0].data, 'external-entity-value')\n", "    def test_deserialization_with_nested_jsonfield(self):\n        data = {\n            'json_field': {\n                'nested_field': {\n                    'key': 'value'\n                }\n            }\n        }\n        obj = JSONModel.objects.create(json_field=data)\n        obj_str = serializers.serialize('xml', [obj])\n        deserialized_obj = serializers.deserialize('xml', obj_str)[0].object\n        self.assertEqual(deserialized_obj.json_field, data)\n", "    def test_deserialize_object_with_natural_key(self):\n        # Test that natural key is correctly deserialized when available.\n        json_obj = JSONModel.objects.create(\n            name='Test',\n            natural_key='natural_key',\n        )\n        json_obj.save()\n\n        xml = serializers.serialize('xml', [json_obj])\n\n        deserialized_obj = serializers.deserialize('xml', xml)[0].object\n        self.assertEqual(deserialized_obj.natural_key, 'natural_key')\n", "    def test_deserialize_json_field_with_custom_decoder(self):\n        json_obj = JSONModel.objects.create(json_data='{\"key\": \"value\"}')\n        data = serializers.serialize('xml', [json_obj])\n        Deserializer(objectito.BytesIO(data), handle_forward_references=True).deserialize()\n        self.assertEqual(json_obj.json_data, {\"key\": \"value\"})\n"], "sample_859": ["def test_elastic_net_path_errors(l1_ratio, alphas):\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    y = np.array([1, 2, 3])\n\n    with pytest.raises(ValueError):\n        ElasticNetCV(l1_ratio=l1_ratio).path(X, y, l1_ratio=0.9)\n\n    with pytest.raises(ValueError):\n        ElasticNetCV(l1_ratio=l1_ratio).path(X, y, alphas=alphas)\n\n    with pytest.raises(ValueError):\n        ElasticNetCV(l1_ratio=l1_ratio).path(X, y, l1_ratio=0.9, alphas=alphas)\n", "def test_lars_path_X_sparse():\n    # Test that lars_path can handle X as a sparse matrix\n    X = sparse.csr_matrix([[1, 2], [3, 4]])\n    y = [1, 2]\n    Xy = sparse.csr_matrix([[1, 2], [3, 4]]).dot(y)\n    alphas, active, coef_path = lars_path(X, y, method='lasso', Xy=Xy)\n    assert isinstance(active, list)\n    assert len(active) == len(coef_path)\n    assert coef_path.shape == (X.shape[1], len(alphas))\n    assert isinstance(alphas, np.ndarray)\n    assert len(alphas) == len(coef_path[:, 0])\n    assert_array_almost_equal(alphas, lars_path(X, y, method='lasso')[0])\n", "def test_lasso_cv_large_n_features():\n    n_samples = 100\n    n_features = 1000\n    X, y = np.random.rand(n_samples, n_features), np.random.rand(n_samples)\n    lasso = LassoCV(cv=5)\n    with ignore_warnings():\n        lasso.fit(X, y)\n    assert lasso.alpha_ > 0\n", "def test_multitask_lasso_params():\n    X, y = load_boston(return_X_y=True)\n    model = MultiTaskLassoCV(max_iter=100, tol=1e-3, cv=2)\n    model.fit(X, y)\n    assert model.alpha_ == model._alpha\n    assert model.coef_.shape == (y.shape[1], X.shape[1])\n    assert model.intercept_.shape == (y.shape[1],)\n    assert model.n_iter_ == model._n_iter\n\n    # Test that the parameters are correctly set when using MultiTaskLassoCV with multiple values for l1_ratio\n    model = MultiTaskElasticNetCV(l1_ratio=[0.5, 0.7, 0.9], cv=2)\n    model.fit(X, y)\n    assert model.l1_ratio_ in model.l1_ratio_\n    assert model.alpha_ == model._alpha\n    assert model.coef_.shape == (y.shape[1], X.shape[1])\n    assert model.intercept_.shape == (y.shape[1],)\n    assert model.n_iter_ == model._n_iter\n", "def test_enet_path_interpolation():\n    # Test the interpolation between the values computed by lars_path and\n    # those computed by enet_path\n    X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]])\n    y = np.array([1, 2, 3.1])\n    alphas, coef_path_lars = lars_path(X, y, method='lasso', return_n_iter=False)\n    n_alphas = len(alphas)\n    alphas = np.linspace(alphas[0], alphas[-1], 100)\n    coef_path = np.zeros((X.shape[1], len(alphas)))\n    for i, alpha in enumerate(alphas):\n        coef_path[:, i] = interpolate.interp1d(alphas[::-1], coef_path_lars[:, ::-1], kind='linear')(alpha)\n    X_test = np.array([[0, 0], [1, 1]])\n    y_test = np.array([1, 2])\n    # Check that the prediction of enet_path with the interpolated values\n    # is close to that of lars_path\n    for i in range(X_test.shape[0]):\n        assert_almost_equal(enet_path(X, y, alphas=alphas, Xy=None, copy_X=True, fit_intercept=True, normalize=True, precompute=False, max_iter=100, tol=1e-4, positive=False, return_n_iter=False, X_test=X_test[i], y_test=y_test[i])[1].ravel()[0], lars_path(X, y, method='lasso', X_test=X_test[i], y_test=y_test[i])[1].ravel()[0])\n"], "sample_59": ["def test_check_constraints(self):\n    # Create a database that does not support table check constraints\n    self.settings.DATABASES['default']['ENGINE'] = 'django.db.backends.sqlite3'\n    from django.db import connections\n    connections['default'].creation.create_test_db()\n    from .models import Model3\n\n    errors = Model3.check()\n    self.assertEqual(len(errors), 1)\n    self.assertIsInstance(errors[0], checks.Warning)\n    self.assertEqual(errors[0].id, 'models.W027')\n    self.assertEqual(errors[0].obj, Model3)\n\n    # Create a database that supports table check constraints\n    self.settings.DATABASES['default']['ENGINE'] = 'django.db.backends.postgresql'\n    connections['default'].creation.create_test_db()\n    errors = Model3.check()\n    self.assertEqual(len(errors), 0)\n", "    def test_get_next_or_previous_in_order(self):\n        event = Event.objects.create(title='Test Event')\n        model1 = Model1.objects.create(_order=1)\n        model2 = Model1.objects.create(_order=2)\n        model3 = Model1.objects.create(_order=3)\n\n        self.assertEqual(model1.get_next_in_order(), model2)\n        self.assertEqual(model2.get_next_in_order(), model3)\n        self.assertEqual(model3.get_next_in_order(), None)\n        self.assertEqual(model1.get_previous_in_order(), None)\n        self.assertEqual(model2.get_previous_in_order(), model1)\n        self.assertEqual(model3.get_previous_in_order(), model2)\n\n        with self.assertRaises(ValueError):\n            model1.get_next_in_order()\n\n        with self.assertRaises(ValueError):\n            model1.get_previous_in_order()\n", "    def test_bulk_update(self):\n        # Create some data\n        article1 = Article.objects.create(title='Article 1', created_at=get_fixed_timezone(0).localize(datetime.date(2022, 1, 1)))\n        article2 = Article.objects.create(title='Article 2', created_at=get_fixed_timezone(0).localize(datetime.date(2022, 1, 2)))\n        article3 = Article.objects.create(title='Article 3', created_at=get_fixed_timezone(0).localize(datetime.date(2022, 1, 3)))\n\n        # Create a list of articles with the new order\n        new_order = [article3.pk, article1.pk, article2.pk]\n\n        # Set the order\n        article1.set_order(article3, new_order, using='default')\n\n        # Check that the order has been updated correctly\n        self.assertEqual(article1.get_order(Article), new_order)\n\n        # Check that the order has been updated for all articles\n        self.assertEqual(list(map(attrgetter('pk'), article1.get_order(Article))), new_order)\n\n        # Check that the order has been updated correctly for all articles\n        self.assertEqual(list(map(attrgetter('pk'), article1.get_order(Article))), new_order)\n", "    def test_save_base_insert(self):\n        # Test save_base method with force_insert=True\n        # and create a new object with primary key\n\n        model1 = Model1.objects.create()\n        self.assertEqual(model1.pk, 1)\n\n        # Test save_base method with force_insert=True\n        # and create a new object without primary key\n        with self.assertRaises(ValueError):\n            Model1.objects.create(force_insert=True)\n\n        # Test save_base method with force_insert=True\n        # and create a new object with primary key and update_fields\n        model2 = Model1.objects.create(name='test_name', update_fields=['name'])\n        self.assertEqual(model2.name, 'test_name')\n        self.assertEqual(model2.pk, 2)\n\n        # Test save_base method with force_insert=False\n        # and create a new object with primary key\n        model3 = Model1.objects.create()\n        self.assertEqual(model3.pk, 3)\n\n        # Test save_base method with force_insert=False\n        # and create a new object without primary key\n        model4 = Model1.objects.create(force_insert=False)\n        self.assertIsNone(model4.pk)\n", "    def test_unique_together(self):\n        # Test if unique together constraint is enforced\n        with self.assertRaises(ValidationError):\n            # create a new Model2 instance\n            Model2.objects.create(id=1, name='test', unique_field='unique')\n            # try to create another instance with the same name and unique field\n            Model2.objects.create(id=2, name='test', unique_field='unique')\n"], "sample_1092": ["def test_cse_opt_cse():\n    from sympy.simplify.cse_opts import sub_pre, sub_post\n    a, b = symbols('a:b')\n\n    exprs = [a**2, b**2, a + b, a*b]\n    opt_subs = opt_cse(exprs)\n    assert opt_subs == {\n        a + b: Unevaluated(Add, (Unevaluated(Add, (a, b)), 1)),\n        a*b: Unevaluated(Mul, (Unevaluated(Add, (a, b)), Unevaluated(Mul, (a, b)))\n    }\n\n    exprs = [a**2, b**2, a + b, a*b]\n    opt_subs = opt_cse(exprs, order='none')\n    assert opt_subs == {\n        a + b: Unevaluated(Add, (Unevaluated(Add, (a, b)), 1)),\n        a*b: Unevaluated(Mul, (Unevaluated(Add, (a, b)), Unevaluated(Mul, (a, b)))\n    }\n\n    exprs = [a**2, b**2, a + b, a*b]\n    opt_subs = opt_cse(exprs, order='canonical')\n    assert opt_subs == {\n        a + b: Unevaluated(Add, (Unevaluated(Add, (a, b)), 1)),\n        a*b: Unevaluated(Mul, (Unevaluated(Add, (a, b)), Unevaluated(Mul, (a, b)))\n    }\n\n    exprs = [a**2, b**2, a + b, a*b]\n    opt_subs = opt_cse(exprs, order='invalid')\n    assert raises(ValueError, lambda: opt_cse(exprs, order='invalid'))\n", "def test_cse_matrix_types():\n    m1 = MutableDenseMatrix([[x + 1, x + 2], [x + 3, x + 4]])\n    m2 = ImmutableDenseMatrix([[x + 1, x + 2], [x + 3, x + 4]])\n    m3 = MutableSparseMatrix([[x + 1, x + 2], [x + 3, x + 4]])\n    m4 = ImmutableSparseMatrix([[x + 1, x + 2], [x + 3, x + 4]])\n\n    replacements, reduced_exprs = cse([m1, m2, m3, m4])\n    assert len(replacements) == 4\n    assert len(reduced_exprs) == 4\n    assert all(isinstance(e, Matrix) for e in reduced_exprs)\n    assert set(type(e) for e in reduced_exprs) == {\n        Matrix, ImmutableMatrix, SparseMatrix, ImmutableSparseMatrix\n    }\n", "def test_opt_cse_finds_common_args():\n    opt_subs = opt_cse([x + y, x + y + z])\n    assert (x + y) in opt_subs\n    assert opt_subs[x + y] == x0\n    assert opt_subs[x + y + z] == x0 + z\n\n", "def test_cse_with_matrix_symbols():\n    # Test that matrix symbols are treated as a single symbol\n    m = MatrixSymbol('M', 3, 3)\n    x = MatrixSymbol('x', 3, 3)\n    eq = Eq(m + x, x + m)\n    result = cse(eq)\n    assert len(result[0]) == 1  # should be 1 replacement\n    assert result[1][0] == Matrix([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n", "def test_cse_opt_subs():\n    from sympy import Eq, cse\n\n    x, y = symbols('x y')\n    opt_subs = cse(opt_cse([x**2 + y**2, x + y, y + x])[1])\n    assert opt_subs == {(x + y): Unevaluated(Add, (x, y))}\n\n    x, y = symbols('x y')\n    opt_subs = cse(opt_cse([x**2 + y**2, x + y, y + x], order='none')[1])\n    assert opt_subs == {(x + y): Unevaluated(Add, (y, x))}\n\n    x, y = symbols('x y')\n    opt_subs = cse(opt_cse([x + y, y + x, x**2 + y**2], order='canonical')[1])\n    assert opt_subs == {(x + y): Unevaluated(Add, (x, y))}\n\n    x, y = symbols('x y')\n    opt_subs = cse(opt_cse([x + y, y + x, x**2 + y**2], order='none')[1])\n    assert opt_subs == {(y + x): Unevaluated(Add, (y, x))}\n\n    x, y = symbols('x y')\n    opt_subs = cse(opt_cse([x + y, y + x, x**2 + y**2], ignore=(x,))[1])\n    assert opt_subs == {(x + y): Unevaluated(Add, (x, y))}\n\n    x, y, z = symbols('x y z')\n    opt_subs = cse(opt_cse([x + y, y + z, x + z, x + y + z, x + z + y, x + y + z]))\n    assert opt_subs == {(y + z): Unevaluated(Add, (z, y)),\n                       (x + y): Unevaluated(Add, (x, y))}\n\n    # test for issue 11756\n    x, y, z = symbols('x y z')\n    opt_subs = cse(opt_cse([x + y + z, x + y + z + x, x + y + z + x + y]))\n    assert opt_subs == {(x + y + z): Unevaluated(Add, (z, x, y))}\n"], "sample_804": ["def test_ordinal_encoder_invalid_categories():\n    \"\"\"Test OrdinalEncoder with invalid categories.\"\"\"\n    encoder = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    with pytest.raises(ValueError):\n        encoder.fit(X, categories=[1, 2, 3])\n", "def test_inverse_transform_with_unknown_category():\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder.fit(X)\n    X_transformed = encoder.transform([['Male', 4], ['Female', 1]])\n    X_inv_transformed = encoder.inverse_transform(X_transformed)\n\n    expected_output = [['Male', None], ['Female', 1]]\n    assert_array_equal(toarray(X_inv_transformed), expected_output)\n", "def test_ordinal_encoder_multiple_features():\n    \"\"\"Test OrdinalEncoder with multiple features.\"\"\"\n    X = [['red', 'blue'], ['red', 'green'], ['blue', 'green']]\n    encoder = OrdinalEncoder()\n    encoder.fit(X)\n    X_encoded = encoder.transform(X)\n    assert_array_equal(X_encoded, np.array([[0, 1], [0, 0], [1, 0]])\n", "def test_fit_transform_after_fit_transform():\n    oe = OneHotEncoder(sparse=False)\n    X = [['a', 'b'], ['a', 'c'], ['b', 'a']]\n    oe.fit_transform(X)\n    assert_warns_regex(DeprecationWarning, oe.fit_transform, X)\n    X_out = oe.transform(X)\n    assert_array_equal(X_out, toarray(sparse.csr_matrix([[1., 0., 0., 1.],\n                                                       [1., 0., 1., 0.],\n                                                       [0., 1., 1., 0.]])))\n    oe.fit_transform(X)\n    assert_array_equal(oe.categories_, oe.categories_)\n    assert_array_equal(oe.categories_, [['a', 'b'], ['a', 'c']])\n", "def test_OneHotEncoder_with_categorical_features():\n    # Test the OneHotEncoder with categorical_features parameter\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder = OneHotEncoder(categorical_features=[0])\n    encoder.fit(X)\n    assert_equal(encoder.categorical_features_, [0])\n    encoded = encoder.transform(X)\n    assert_array_equal(toarray(encoded).sum(axis=1), [1, 1, 1])\n    assert_array_equal(toarray(encoded).sum(axis=0), [1, 1, 1, 1])\n\n    # Test the OneHotEncoder with categorical_features parameter when input is sparse\n    X_sparse = sparse.csr_matrix(np.array([[0, 1, 3], [1, 2, 0], [0, 3, 0]]))\n    encoder = OneHotEncoder(categorical_features=[1])\n    encoder.fit(X_sparse)\n    assert_equal(encoder.categorical_features_, [1])\n    encoded_sparse = encoder.transform(X_sparse)\n    assert_array_equal(toarray(encoded_sparse).sum(axis=1), [1, 1, 1])\n    assert_array_equal(toarray(encoded_sparse).sum(axis=0), [0, 2, 1, 0])\n\n    # Test the OneHotEncoder with categorical_features parameter when input is sparse and\n    # the categorical feature is not present in the data\n    X_sparse = sparse.csr_matrix(np.array([[0, 1, 3], [1, 2, 0], [0, 3, 0]]))\n    encoder = OneHotEncoder(categorical_features=[1])\n    with pytest.raises(ValueError):\n        encoder.fit(X_sparse[:, :2])\n\n    # Test the OneHotEncoder with categorical_features parameter when input is sparse and\n    # the categorical feature is present but has no categories\n    X_sparse = sparse.csr_matrix(np.array([[0, 1, 3], [1, 2, 0], [0, 3, 0]]))\n    encoder = OneHotEncoder(categorical_features=[1])\n    with pytest.raises(ValueError):\n        encoder.fit(X_sparse[:, :1])\n\n    # Test the OneHotEncoder with categorical_features parameter when input is dense\n    X = np.array([[0, 1, 3], [1, 2, 0], [0, 3, 0]])\n    encoder = OneHotEncoder(c"], "sample_40": ["def test_dimensionless_angles():\n    \"\"\"Test dimensionless_angles equivalency.\"\"\"\n    val = u.dimensionless_angles()\n    assert isinstance(val, list)\n    assert len(val) == 1\n    assert len(val[0]) == 2\n    assert val[0][0] == u.si.radian\n    assert val[0][1] is None\n\n    q = u.Quantity(1, u.si.radian)\n    assert q.to(u.dimensionless_angles()) == 1\n\n    # Test that converting a dimensionless quantity to a unit that has a different\n    # dimensionality raises a UnitsError\n    with pytest.raises(u.UnitsError):\n        u.Quantity(1, u.dimensionless_angles()).to(u.si.cm)\n", "def test_doppler_relativistic_equivalency_with_frequency():\n    \"\"\"Tests the relativistic convention equivalency with frequency.\"\"\"\n    # Test case 1: Frequency\n    rest_freq = u.Quantity(115.27120, u.GHz)\n    measured_freq = u.Quantity(115.2832, u.GHz)\n    expected_vel = u.Quantity(-31.207467619351537, u.km/u.s)\n    equivalency = u.doppler_relativistic(rest_freq)\n    result = measured_freq.to(u.km/u.s, equivalencies=equivalency)\n    assert_quantity_allclose(result.value, expected_vel.value)\n    assert result.unit == u.km/u.s\n\n    # Test case 2: Wavelength\n    rest_wav = rest_freq.to(u.m, u.spectral())\n    measured_wav = measured_freq.to(u.m, u.spectral())\n    expected_vel = u.Quantity(-31.207467619351537, u.km/u.s)\n    result = measured_wav.to(u.km/u.s, equivalencies=equivalency)\n    assert_quantity_allclose(result.value, expected_vel.value)\n    assert result.unit == u.km/u.s\n\n    # Test case 3: Energy\n    rest_en = rest_freq.to(u.eV, u.spectral())\n    measured_en = measured_freq.to(u.eV, u.spectral())\n    expected_vel = u.Quantity(-31.207467619351537, u.km/u.s)\n    result = measured_en.to(u.km/u.s, equivalencies=equivalency)\n    assert_quantity_allclose(result.value, expected_vel.value)\n    assert result.unit == u.km/u.s\n", "def test_dimensionless_angles():\n    # Test dimensionless_angles equivalency pair\n    angle = 1*u.radian\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            1)\n\n    # Test dimensionless_angles equivalency pair with no unit\n    assert_quantity_allclose((1).to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            1)\n\n    # Test dimensionless_angles equivalency pair with a different unit\n    angle = 1*u.meter\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            1)\n\n    # Test dimensionless_angles equivalency pair with a scalar\n    angle = 2\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            2)\n\n    # Test dimensionless_angles equivalency pair with a quantity of units\n    angle = 2*u.dimensionless_angles()\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            2)\n\n    # Test dimensionless_angles equivalency pair with a negative value\n    angle = -1*u.radian\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            -1)\n\n    # Test dimensionless_angles equivalency pair with a non-numeric value\n    with pytest.raises(UnitsError):\n        (np.nan).to(u.dimensionless_unscaled, u.dimensionless_angles())\n\n    # Test dimensionless_angles equivalency pair with an array\n    angle = np.array([1, 2, 3]) * u.radian\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            np.array([1, 2, 3]))\n\n    # Test dimensionless_angles equivalency pair with an array of negative values\n    angle = np.array([-1, -2, -3]) * u.radian\n    assert_quantity_allclose(angle.to(u.dimensionless_unscaled, u.dimensionless_angles()),\n                            np.array([-1, -2, -3]))\n\n    # Test dimensionless_angles equivalency pair with an array of non-numeric values\n    angle = np.array([1, np.nan, 3]) * u.radian\n    with pytest.raises(UnitsError):\n        angle.to(u.dimensionless_unscaled, u.dimensionless_angles())\n\n    # Test dimensionless_angles equivalency pair with a scalar array\n    angle = np.array([", "def test_doppler_optical():\n    \"\"\"Test doppler_optical function.\"\"\"\n    # Test creation of the equivalency pairs\n    with u.set_quantities():\n        restfreq = 115.27120*u.GHz\n        assert len(doppler_optical(restfreq)) == 4\n        assert len(doppler_optical(restfreq)) == 4  # Check for the same length as the number of conversions\n\n        # Test all conversions between velocity and frequency\n        freq = 115.2832*u.GHz\n        vel = (freq - restfreq).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, -31.20584348799674)\n        vel = (115.27120*u.GHz - freq).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, 31.20584348799674)\n\n        wav = (115.27120*u.GHz).to(u.mm, equivalencies=u.spectral())\n        vel = (wav - 2.652*u.mm).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, -31.20584348799674)\n        vel = (2.652*u.mm - wav).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, 31.20584348799674)\n\n        en = (115.27120*u.GHz).to(u.eV, equivalencies=u.spectral())\n        vel = (en - 0.0067*u.eV).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, -31.20584348799674)\n        vel = (0.0067*u.eV - en).to(u.km/u.s, equivalencies=u.doppler_optical(restfreq))\n        assert_quantity_allclose(vel.value, 31.20584348799674)\n", "def test_doppler_relativistic_equivalency():\n    \"\"\"Test that the relativistic Doppler shift works.\"\"\"\n    # Test with frequency\n    restfreq = u.Hz\n    measured_freq = 115.2832 * u.GHz\n    velocity = measured_freq.to(u.km/u.s, equivalencies=u.doppler_relativistic(restfreq))\n    expected_velocity = -31.207467619351537 * u.km/u.s\n    assert_quantity_allclose(velocity, expected_velocity, rtol=1e-4)\n\n    # Test with wavelength\n    restwav = u.m\n    measured_wav = 0.00000021525771 * u.m\n    velocity = measured_wav.to(u.km/u.s, equivalencies=u.doppler_relativistic(restwav))\n    expected_velocity = -31.207467619351537 * u.km/u.s\n    assert_quantity_allclose(velocity, expected_velocity, rtol=1e-4)\n\n    # Test with energy\n    resten = u.J\n    measured_en = 5.65587e-25 * u.J\n    velocity = measured_en.to(u.km/u.s, equivalencies=u.doppler_relativistic(resten))\n    expected_velocity = -31.207467619351537 * u.km/u.s\n    assert_quantity_allclose(velocity, expected_velocity, rtol=1e-4)\n\n    # Test with inverse wavelength\n    restinvwav = u.m**-1\n    measured_invwav = 46502545153.027 * u.m**-1\n    velocity = measured_invwav.to(u.km/u.s, equivalencies=u.doppler_relativistic(restinvwav))\n    expected_velocity = -31.207467619351537 * u.km/u.s\n    assert_quantity_allclose(velocity, expected_velocity, rtol=1e-4)\n\n    # Test with inverse energy\n    restenin = u.J**-1\n    measured_inven = 1.940725e+24 * u.J**-1\n    velocity = measured_inven.to(u.km/u.s, equivalencies=u.doppler_relativistic(restenin))\n    expected_velocity = -31.207467619351537 * u.km/u.s\n    assert_quantity_allclose(velocity, expected_velocity, rtol=1e-4)\n"], "sample_667": ["def test_get_user():\n    \"\"\"Test get_user function.\"\"\"\n    # Test case: get_user() should return the current user name\n    # when it can be obtained\n    user = get_user()\n    assert user is not None\n\n    # Test case: get_user() should return None when getuser() fails\n    # This is a workaround for issue #1010\n    with pytest.raises((ImportError, KeyError)):\n        get_user()\n\n    # Test case: get_user() should return None when getpass.getuser() returns None\n    # This is a sanity check for the fix in get_user()\n    with pytest.raises((ImportError, KeyError)):\n            return None\n        with pytest.patch(\"getpass.getuser\", mock_getuser):\n            assert get_user() is None\n", "def test_get_user():\n    # Test get_user() returns the current user name when getuser() works\n    expected_user_name = os.getlogin()\n    assert get_user() == expected_user_name\n\n    # Test get_user() returns None when getuser() raises an exception\n    with pytest.raises((ImportError, KeyError)):\n        getpass.getuser.side_effect = ImportError\n        assert get_user() is None\n\n    # Test get_user() returns None when getuser() returns an empty string\n    with pytest.raises((ImportError, KeyError)):\n        getpass.getuser.return_value = \"\"\n        assert get_user() is None\n", "def test_get_user():\n    \"\"\"Test that get_user returns a valid user name.\"\"\"\n    user = get_user()\n    assert isinstance(user, str) or user is None\n", "def test_tempdir_factory_config(tmp_path_factory: TempPathFactory) -> None:\n    \"\"\"Test TempdirFactory with various configurations.\"\"\"\n    # Test with given basetemp\n    config = pytest.Config()\n    config.option.basetemp = tmp_path_factory.mktemp(\"test_basetemp\")\n    config.trace.set(\"tmpdir\", \"test_tmpdir\")\n    factory = TempPathFactory.from_config(config)\n    tdf = TempdirFactory(factory)\n    assert tdf.getbasetemp().resolve() == tmp_path_factory.mktemp(\"test_basetemp\").resolve()\n\n    # Test without given basetemp\n    config = pytest.Config()\n    factory = TempPathFactory.from_config(config)\n    tdf = TempdirFactory(factory)\n    assert isinstance(tdf.getbasetemp(), py.path.local)\n    assert tdf.getbasetemp().resolve() == factory.getbasetemp().resolve()\n\n    # Test get_user function\n    import getpass\n    getpass.getuser = lambda: \"test_user\"\n    assert get_user() == \"test_user\"\n    getpass.getuser = None  # Reset\n\n    # Test pytest_configure function\n    config = pytest.Config()\n    pytest_configure(config)\n    assert hasattr(config, \"_tmp_path_factory\")\n    assert hasattr(config, \"_tmpdirhandler\")\n\n    # Test mktemp function with numbered=False\n    p = tmp_path_factory.mktemp(\"test_dir\", numbered=False)\n    assert p.is_dir()\n    assert not p.name.startswith(\"test_dir-\")\n\n    # Test mktemp function with numbered=True\n    p = tmp_path_factory.mktemp(\"test_dir\", numbered=True)\n    assert p.is_dir()\n    assert p.name.startswith(\"test_dir-\")\n", "def test_temp_path_factory_given_basetemp(tmp_path_factory):\n    \"\"\"Test TempPathFactory given basetemp\"\"\"\n    base_dir = tmp_path_factory.mktemp(\"my_dir\", numbered=False)\n    assert tmp_path_factory.getbasetemp() == base_dir\n\n    # test if given_basetemp is set correctly\n    given_basetemp = Path(\"/custom/basetemp\")\n    tmp_path_factory_given = TempPathFactory(_given_basetemp=given_basetemp, _trace=None)\n    assert tmp_path_factory_given.getbasetemp() == given_basetemp\n"], "sample_995": ["def test_Float_as_coeff_Mul():\n    assert same_and_same_prec(Float(3).as_coeff_Mul()[0], Float(3))\n    assert same_and_same_prec(Float(3).as_coeff_Mul()[1], Float(1))\n    assert same_and_same_prec(Float(3).as_coeff_Mul(rational=True)[0], Float(1))\n    assert same_and_same_prec(Float(3).as_coeff_Mul(rational=True)[1], Float(3))\n    assert same_and_same_prec(Float(1/2).as_coeff_Mul()[0], Float(1/2))\n    assert same_and_same_prec(Float(1/2).as_coeff_Mul()[1], Float(1))\n    assert same_and_same_prec(Float(1/2).as_coeff_Mul(rational=True)[0], Float(1))\n    assert same_and_same_prec(Float(1/2).as_coeff_Mul(rational=True)[1], Float(1/2))\n", "def test_mod_inverse():\n    from sympy import mod_inverse, S, Integer\n\n    assert mod_inverse(1, 10) == 1\n    assert mod_inverse(-1, 10) == -1\n    assert mod_inverse(1, -10) == mod_inverse(1, 10)\n    assert mod_inverse(-1, -10) == -mod_inverse(-1, 10)\n    assert mod_inverse(1, 0) == mod_inverse(1, 10)\n    assert mod_inverse(-1, 0) == -mod_inverse(-1, 10)\n    assert mod_inverse(1, 1) == 1\n    assert mod_inverse(-1, 1) == -1\n    assert mod_inverse(1, 10) == 1\n    assert mod_inverse(-1, 10) == -1\n    assert mod_inverse(1, 11) == 1\n    assert mod_inverse(-1, 11) == -1\n    assert mod_inverse(1, 10) == 1\n    assert mod_inverse(-1, 10) == -1\n    assert mod_inverse(1, 1000000) == 1\n    assert mod_inverse(-1, 1000000) == -1\n    assert mod_inverse(1, 10) == 1\n    assert mod_inverse(-1, 10) == -1\n\n    assert mod_inverse(S(1), S(10)) == S(1)\n    assert mod_inverse(S(-1), S(10)) == S(-1)\n    assert mod_inverse(S(1), S(0)) == S(1)\n    assert mod_inverse(S(-1), S(0)) == S(-1)\n    assert mod_inverse(S(1), S(1)) == S(1)\n    assert mod_inverse(S(-1), S(1)) == S(-1)\n    assert mod_inverse(S(1), S(10)) == S(1)\n    assert mod_inverse(S(-1), S(10)) == S(-1)\n    assert mod_inverse(S(1), S(11)) == S(1)\n    assert mod_inverse(S(-1), S(11)) == S(-1)\n    assert mod_inverse(S(1), S(1000000)) == S(1)\n    assert mod_inverse(S(-1), S(1000000)) == S(-1)\n\n", "def test_igcd():\n    assert igcd(15, 20) == 5\n    assert igcd(-15, -20) == 5\n    assert igcd(-15, 20) == 5\n    assert igcd(15, -20) == 5\n", "compilation error", "compilation error"], "sample_656": ["    def test_global_capture_disabled(self, pytestconfig, capsys):\n        pytestconfig.getoption(\"capture\") = \"no\"\n        manager = CaptureManager(\"no\")\n        assert manager.is_globally_capturing() is False\n        with manager.global_and_fixture_disabled():\n            print(\"Hello, World!\")\n            sys.stdout.write(\"Hello, World!\")\n            sys.stderr.write(\"Hello, World!\\n\")\n        assert capsys.readouterr().out == \"\"\n        assert capsys.readouterr().err == \"\"\n", "    def test_resume_global_capture_twice(self):\n        \"\"\"Issue #123: calling ``resume_global_capture()`` twice must be safe.\"\"\"\n        capman = CaptureManager(\"fd\")\n        capman.stop_global_capturing()\n        capman.resume_global_capture()\n        capman.resume_global_capture()\n", "    def test_read_global_capture(self, capsys):\n        # Given\n        capmanager = CaptureManager(\"fd\")\n        capmanager.start_global_capturing()\n        sys.stdout.write(\"Hello, world!\")\n\n        # When\n        out, err = capmanager.read_global_capture()\n\n        # Then\n        assert out == \"Hello, world!\\n\"\n        assert err == \"\"\n", "    def test_suspend_global_capture(self):\n        # Test that suspend_global_capture sets the _state to 'suspended' on each capture\n        # and that resume_global_capture sets it back to 'started'\n        capture_manager = CaptureManager(\"fd\")\n        capture_manager.start_global_capturing()\n        capture_manager.suspend_global_capture(in_=True)\n        assert capture_manager._global_capturing._state == \"suspended\"\n        capture_manager.resume_global_capture()\n        assert capture_manager._global_capturing._state == \"started\"\n        capture_manager.suspend_global_capture(in_=False)\n        assert capture_manager._global_capturing._state == \"suspended\"\n", "    def test_suspending_and_resuming_global_capturing(self):\n        \"\"\"Test that suspending and resuming global capturing works correctly.\"\"\"\n        capman = CaptureManager(\"fd\")\n        capman.start_global_capturing()\n        assert capman.is_globally_capturing()  # should be True\n        capman.suspend_global_capture()\n        assert capman.is_globally_capturing()  # should be False\n        capman.resume_global_capture()\n        assert capman.is_globally_capturing()  # should be True\n        capman.stop_global_capturing()\n        assert capman.is_globally_capturing()  # should be False\n"], "sample_180": ["    def test_model_init_sets_state_to_not_adding(self):\n        # Test that self._state.adding is False after the object is saved.\n        model = models.Model()\n        model.save()\n        self.assertFalse(model._state.adding)\n", "    def test_index_together_multi_db(self):\n        with self.settings(\n            DATABASES={\n                'default': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                },\n                'other': {\n                    'ENGINE': 'django.db.backends.sqlite3',\n                },\n            }\n        ):\n            from .invalid_models import MyModel\n            errors = MyModel.check()\n            self.assertEqual(len(errors), 0)\n", "    def test_model_name(self):\n        class TestModel(models.Model):\n            name = models.CharField(max_length=10)\n        self.assertTrue(TestModel._meta.object_name == 'TestModel')\n", "    def test_index_together_check(self):\n        class InvalidModel(models.Model):\n            class Meta:\n                index_together = (['a', 123])  # invalid index_together\n\n            a = models.IntegerField()\n            b = models.IntegerField()\n\n        with self.assertRaisesRegex(Error, 'All .* must be lists or tuples'):\n            InvalidModel.check()\n", "    def test_absolute_url_override(self):\n        from invalid_models_tests.models import ModelWithUrlOverride\n        self.assertEqual(ModelWithUrlOverride.get_absolute_url(), '/absolute/url/overridden')\n"], "sample_164": ["    def test_format_subject(self):\n        handler = AdminEmailHandler()\n        record = logging.LogRecord(\n            name='django.request',\n            level=logging.ERROR,\n            pathname='/path/to/file.py',\n            lineno=10,\n            msg='Test message',\n            args=None,\n            exc_info=None,\n            func=None,\n            extra={'request': None}\n        )\n        subject = handler.format_subject(record.getMessage())\n        self.assertEqual(subject, 'ERROR (internal IP): Test message')\n\n        record = logging.LogRecord(\n            name='django.request',\n            level=logging.ERROR,\n            pathname='/path/to/file.py',\n            lineno=10,\n            msg='Test message',\n            args=None,\n            exc_info=None,\n            func=None,\n            extra={'request': {'META': {'REMOTE_ADDR': '192.168.1.100'}}}\n        )\n        subject = handler.format_subject(record.getMessage())\n        self.assertEqual(subject, 'ERROR (EXTERNAL): Test message')\n", "    def test_server_formatter(self):\n        formatter = ServerFormatter()\n        record = logging.LogRecord(\n            name='django.server',\n            level=logging.INFO,\n            pathname='',\n            lineno=0,\n            msg='Test message',\n            args=None,\n            exc_info=None,\n            func='',\n            extra={'status_code': 200}\n        )\n        formatted_record = formatter.format(record)\n        self.assertIn('HTTP_SUCCESS', formatted_record)\n\n        record = logging.LogRecord(\n            name='django.server',\n            level=logging.INFO,\n            pathname='',\n            lineno=0,\n            msg='Test message',\n            args=None,\n            exc_info=None,\n            func='',\n            extra={'status_code': 500}\n        )\n        formatted_record = formatter.format(record)\n        self.assertIn('HTTP_SERVER_ERROR', formatted_record)\n", "    def test_CallbackFilter(self):\n        callback = lambda record: record.levelno < 30\n        filter = CallbackFilter(callback)\n        self.assertTrue(filter.filter(logging.LogRecord(level=20, name='', pathname='', lineno=0, msg='Test message', args=(), exc_info=None, func=None)))\n        self.assertFalse(filter.filter(logging.LogRecord(level=50, name='', pathname='', lineno=0, msg='Test message', args=(), exc_info=None, func=None)))\n", "    def test_CallbackFilter(self):\n        callback = lambda record: record.levelname == 'INFO'\n        filter_ = CallbackFilter(callback)\n        record = logging.LogRecord('myapp', 20, 'path/to/module.py', 1, 'message')\n        self.assertTrue(filter_.filter(record))\n\n        record.levelname = 'ERROR'\n        self.assertFalse(filter_.filter(record))\n", "    def setUp(self):\n        self.logger = logging.getLogger('django.request')\n        self.logger.setLevel(logging.INFO)\n        self.handler = logging.StreamHandler()\n        self.logger.addHandler(self.handler)\n"], "sample_144": ["    def test_abstract_model_save(self):\n        # Test that an abstract model cannot be saved\n        with self.assertRaises(RuntimeError):\n            InternalCertificationAudit().save()\n", "    def test_make_foreign_order_accessors(self):\n        # Test that the order_with_respect_to accessor is properly created\n        # when a related model is passed to make_foreign_order_accessors\n        ArticleWithAuthor = ArticleWithAuthor\n        make_foreign_order_accessors(ArticleWithAuthor, Article)\n        self.assertIn('get_article_order', ArticleWithAuthor.__dict__)\n        self.assertIn('set_article_order', ArticleWithAuthor.__dict__)\n", "    def test_model_subclassing(self):\n        # Test that a model subclassing a non-abstract model can't define a\n        # new abstract parent.\n        with self.assertRaises(TypeError):\n            class InvalidModel(Model):\n                class Meta:\n                    abstract = True\n", "    def test_save_base(self):\n        # Test that save_base updates the object with a new foreign key value\n        # when the related object has a new primary key.\n        user = User.objects.create()\n        obj = User.objects.create(username='test')\n        obj.user = user\n        obj.save()\n        self.assertEqual(obj.user.pk, user.pk + 1)\n", "    def test_proxy_model_proxy_for_parent(self):\n        # Test that a proxy model doesn't proxy for a parent with its own swapped model\n        # Also test that a swapped model can have proxy models\n        class ParentSwappedModel(Model):\n            class Meta:\n                swappable = 'ParentSwappedModel'\n                concrete = False\n\n        class ParentModel(Model):\n            class Meta:\n                swappable = 'ParentModel'\n                concrete = False\n\n        class ParentSwappedProxy(Model):\n            class Meta:\n                proxy = True\n                _meta = ParentSwappedModel._meta\n\n        class ParentProxy(Model):\n            class Meta:\n                proxy = True\n                _meta = ParentModel._meta\n\n        with self.assertRaises(TypeError):\n            ParentSwappedProxy()\n\n        ParentSwappedProxy._meta.concrete_model = ParentModel\n        ParentSwappedProxy()\n\n        ParentModel._meta.concrete = True\n        ParentSwappedProxy()\n\n        class SwappedParentSwappedModel(Model):\n            class Meta:\n                swappable = 'SwappedParentSwappedModel'\n                concrete = True\n\n        ParentSwappedModel._meta.concrete_model = SwappedParentSwappedModel\n        ParentSwappedProxy()\n"], "sample_771": ["def test_quantile_transform_normal_output_distribution():\n    rng = np.random.RandomState(0)\n    n_samples = 1000\n    n_features = 10\n    X = rng.randn(n_samples, n_features)\n    qt = QuantileTransformer(output_distribution='normal')\n    X_trans = qt.fit_transform(X)\n    assert_equal(X_trans.shape, X.shape)\n    assert_equal(qt.output_distribution, 'normal')\n    qt = QuantileTransformer(output_distribution='normal', n_quantiles=10)\n    X_trans = qt.fit_transform(X)\n    assert_equal(X_trans.shape, X.shape)\n    assert_equal(qt.output_distribution, 'normal')\n", "def test_MappingFeatureNames():\n    # Test with default names\n    pf = PolynomialFeatures()\n    X = np.arange(5).reshape(1, 5)\n    Xp = pf.fit_transform(X)\n    expected_names = ['x0', 'x1', 'x2', 'x3', 'x4', 'x0^2', 'x0*x1', 'x0*x2',\n                      'x0*x3', 'x0*x4', 'x1^2', 'x1*x2', 'x1*x3', 'x1*x4',\n                      'x2^2', 'x2*x3', 'x2*x4', 'x3^2', 'x3*x4', 'x4^2']\n    assert_array_equal(pf.get_feature_names(), expected_names)\n\n    # Test with specific names\n    pf = PolynomialFeatures(input_features=['x', 'y', 'z'])\n    X = np.arange(5).reshape(1, 3)\n    Xp = pf.fit_transform(X)\n    expected_names = ['x', 'y', 'z', 'x^2', 'x*y', 'x*z', 'y^2', 'y*z', 'z^2']\n    assert_array_equal(pf.get_feature_names(), expected_names)\n\n    # Test with no interaction\n    pf = PolynomialFeatures(degree=1, interaction_only=False)\n    X = np.arange(5).reshape(1, 5)\n    Xp = pf.fit_transform(X)\n    expected_names = ['1', 'x0', 'x1', 'x2', 'x3', 'x4']\n    assert_array_equal(pf.get_feature_names(), expected_names)\n\n    # Test with no intercept\n    pf = PolynomialFeatures(degree=2, include_bias=False)\n    X = np.arange(5).reshape(1, 5)\n    Xp = pf.fit_transform(X)\n    expected_names = ['x0', 'x1', 'x2', 'x3', 'x4', 'x0^2', 'x0*x1', 'x0*x2',\n                      'x0*x3', 'x0*x4', 'x1^2', 'x1*x2', 'x1*x3', 'x1*x4',\n                      'x2^2', 'x2*x3', 'x2*x4', 'x3^2", "def test_PowerTransformer_inverse_transform():\n    # Test that inverse transform is correctly computed when X is a 2D array\n    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n    X = [[1, 2], [3, 2], [4, 5]]\n    Xt = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(Xt)\n    assert_array_equal(X, X_inv)\n\n    # Test that inverse transform is correctly computed when X is a 1D array\n    X = [1, 2, 3, 4, 5]\n    Xt = pt.fit_transform(np.array(X)[:, np.newaxis])\n    X_inv = pt.inverse_transform(Xt)\n    assert_array_equal(X, X_inv[:, 0])\n\n    # Test that inverse transform raises an error when X is not a 2D array\n    X = [[1, 2], 3, 4, 5]\n    with assert_raises_regex(ValueError, \"Input data has a different number of features\"):\n        pt.inverse_transform(X)\n\n    # Test that inverse transform raises an error when X is not a 2D array after standardizing\n    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n    X = [[1, 2], 3, 4, 5]\n    with assert_raises_regex(ValueError, \"Input data has a different number of features\"):\n        pt.inverse_transform(pt.fit_transform(X))\n\n    # Test that inverse transform raises an error when X is a 2D array with different number of features\n    X = [[1, 2], [3, 2], [4, 5], [6, 7, 8]]\n    with assert_raises_regex(ValueError, \"Input data has a different number of features\"):\n        pt.inverse_transform(pt.fit_transform(X))\n", "def test_PowerTransformer_YeoJohnson_with_negative_data():\n    X = np.array([[-1, 2], [3, -2], [4, 5]])\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n    # Since we are using the Yeo-Johnson method, we expect to see a range\n    # of values for the lambdas_\n    assert pt.lambdas_.min() < 0 and pt.lambdas_.max() > 0\n\n", "def test_std_scaler_online_partial_fit_zeros():\n    \"\"\"Regression test for StandardScaler's partial_fit method with zeros in data\"\"\"\n    # See issue #14276\n    np.random.seed(0)\n    X = np.array([[-1, 0, 0], [0, 0, 1]])\n    ss = StandardScaler()\n    assert_equal(ss.partial_fit(X).n_samples_seen_, 1)\n    assert_equal(ss.partial_fit(X).n_samples_seen_, 3)\n    assert_equal(ss.n_samples_seen_, 3)\n\n    # Test with multiple batches\n    batch_size = 2\n    n_samples = X.shape[0]\n    n_batches = int(np.ceil(n_samples / batch_size))\n    batches = gen_batches(n_samples, batch_size)\n    for i, (batch_start, batch_stop) in enumerate(batches):\n        X_batch = X[batch_start:batch_stop]\n        ss.partial_fit(X_batch)\n        assert_correct_incr(i, batch_start, batch_stop, n_samples,\n                           batch_size, ss.n_samples_seen_)\n    assert_equal(ss.n_samples_seen_, n_samples)\n\n    # Check that var_ and scale_ are correctly updated when there are zeros in the data\n    ss = StandardScaler()\n    X = np.array([[0, 0], [0, 0]])\n    assert_equal(ss.partial_fit(X).var_, 0)\n    assert_equal(ss.partial_fit(X).scale_, 1)\n    assert_equal(ss.n_samples_seen_, 2)\n    assert_equal(ss.n_samples_seen_, 4)\n\n    # Test with a list of arrays as input\n    X_list = [X, X]\n    ss = StandardScaler()\n    for X_batch in X_list:\n        ss.partial_fit(X_batch)\n    assert_equal(ss.n_samples_seen_, 4)\n    assert_equal(ss.n_samples_seen_, 4)\n\n    # Test with a single array as input\n    ss = StandardScaler()\n    X = np.array([[0, 0], [0, 0]])\n    ss.partial_fit(X)\n    assert_equal(ss.n_samples_seen_, 2)\n\n    # Test with an empty array as input\n    ss = StandardScaler()\n    X = np.array([])\n    ss.partial_fit(X)\n    assert_equal(ss.n_samples_seen_, 0)\n\n    # Test with a 1D array as input\n    X = np.array([0, 0])\n    ss = StandardScaler()\n    ss.partial_fit(X)\n   "], "sample_499": ["def test_Legend_set_title_fontproperties_with_size():\n    fig, ax = plt.subplots()\n\n    # Set the font size of the title font properties\n    title_fontproperties = FontProperties(size=16)\n\n    # Set the title and its font properties\n    legend = mlegend.Legend(ax, [], [], loc='best')\n    legend.set_title('Test Legend', title_fontproperties=title_fontproperties)\n\n    # Verify that the title font size is correctly set\n    assert legend.get_title().get_fontproperties().get_size_in_points() == 16\n\n    # Set the font size of the title font properties to None\n    title_fontproperties = FontProperties()\n\n    # Set the title and its font properties\n    legend.set_title('Test Legend', title_fontproperties=title_fontproperties)\n\n    # Verify that the title font size is correctly set\n    assert legend.get_title().get_fontproperties().get_size_in_points() == 10\n", "def test_legend_bbox_to_anchor():\n    \"\"\"Test setting bbox_to_anchor with various parameters.\"\"\"\n    fig, axs = plt.subplots()\n    ax1, ax2 = axs\n\n    # Test a BboxBase instance\n        legend = mlegend.Legend(ax1, [], [], loc='upper left')\n        bbox = mtransforms.Bbox.from_bounds(0.1, 0.1, 0.2, 0.2)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().width == 0.2\n        assert legend.get_bbox_to_anchor().height == 0.2\n\n    # Test a 2-tuple with zero width and height\n        legend = mlegend.Legend(ax1, [], [], loc='upper left')\n        bbox = (0.1, 0.1)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().width == 0\n        assert legend.get_bbox_to_anchor().height == 0\n\n    # Test a 2-tuple with non-zero width and height\n        legend = mlegend.Legend(ax1, [], [], loc='upper left')\n        bbox = (0.1, 0.1, 0.2, 0.2)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().width == 0.2\n        assert legend.get_bbox_to_anchor().height == 0.2\n\n    # Test a 4-tuple with all zero coordinates\n        legend = mlegend.Legend(ax1, [], [], loc='upper left')\n        bbox = (0, 0, 0, 0)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().width == 0\n        assert legend.get_bbox_to_anchor().height == 0\n\n    # Test a 4-tuple with non-zero coordinates\n        legend = mlegend.Legend(ax1, [], [], loc='upper left')\n        bbox = (0.1, 0.1, 0.2, 0.2)\n        legend.set_bbox_to_anchor(bbox)\n        assert legend.get_bbox_to_anchor().width == 0.2\n        assert legend.get_bbox_to_anchor().height ==", "def test_Legend_prop_size(prop, expected_size):\n    with rc_context(rc={'legend.fontsize': 12}):\n        legend = mlegend.Legend(None, [], [], loc='best')\n        legend.prop = prop\n        assert legend._fontsize == expected_size\n\n", "def test_draggable_legend():\n        return mock.Mock(x=x, y=y, inaxes=None, inview=None, button=button)\n\n        legend = mlegend.Legend(ax, [])\n        legend.set_bbox_to_anchor(expected_bbox)\n        draggable = legend.set_draggable(True)\n        event = create_mock_event(x, y)\n        draggable._onmotion(event)\n        event = create_mock_event(x + dx, y + dy)\n        draggable._onmotion(event)\n        draggable._finalize()\n        return legend.get_bbox_to_anchor()\n\n    # Test loc update\n    ax = plt.gca()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    draggable = mlegend.Legend(ax, []).set_draggable(True)\n    draggable._update = 'loc'\n    bbox = mtransforms.Bbox.from_bounds(1, 1, 1, 1)\n    assert check_draggable_legend(ax, bbox, 1, 1, 1, 1) == mtransforms.Bbox.from_bounds(2, 2, 1, 1)\n\n    # Test bbox update\n    ax = plt.gca()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    draggable = mlegend.Legend(ax, []).set_draggable(True)\n    draggable._update = 'bbox'\n    bbox = mtransforms.Bbox.from_bounds(1, 1, 1, 1)\n    assert check_draggable_legend(ax, bbox, 1, 1, 1, 1) == mtransforms.Bbox.from_bounds(2, 2, 1, 1)\n\n    # Test shadow and frame alpha\n    with rc_context(rc={'legend.shadow': True, 'legend.framealpha': 0.5}):\n        ax = plt.gca()\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        draggable = mlegend.Legend(ax, []).set_draggable(True)\n        draggable._update = 'loc'\n        bbox = mtransforms.Bbox.from_bounds(1, 1, 1, 1)\n        assert check_draggable_legend(ax, bbox, 1, 1, 1, 1) == mtransforms.Bbox.from_bounds(2, 2, ", "def test_drawing_legend():\n    fig, ax = plt.subplots()\n    ax.plot(np.arange(10), np.arange(10))\n    legend = mlegend.Legend(ax, [mlines.Line2D([0], [0])], [\"\"], loc=0, frameon=True)\n    # Try drawing the legend with different values of fontsize\n    for fontsize in [10, 12, 14]:\n        with rc_context({\"legend.fontsize\": fontsize}):\n            legend.draw(fig.canvas.get_renderer())\n            assert legend.get_text().get_fontsize() == fontsize\n            # Ensure the legend frame is the correct size\n            assert np.allclose(legend.get_frame().get_frame_on(True).get_window_extent().bounds,\n                              (0, 0, 1, 1))\n"], "sample_193": ["    def test_related_name_clashes(self):\n        with isolate_apps('myapp', 'django.contrib.contenttypes'):\n            class Related(models.Model):\n                pass\n            class Model(models.Model):\n                related = RelatedField(Related, related_name='related')\n            Model._meta.add_field('related', RelatedField(Related, related_name='related'))\n\n            with self.assertRaisesMessage(\n                ValueError,\n                'Reverse accessor for \"Model.related\" clashes with field name \"Model.related\".'):\n                ModelState().add(Model, Model.objects.all())\n", "    def test_make_related_fields(self):\n        # Define a custom field class that is a subclass of RelatedField\n        class CustomRelatedField(RelatedField):\n            pass\n\n        # Create a model with a custom related field\n        class CustomModel(models.Model):\n            custom_field = CustomRelatedField()\n\n        # Get the state of the model\n        state = ModelState.from_model(CustomModel)\n\n        # Make related fields\n        related_fields = get_related_models_recursive(state, CustomModel)\n        self.assertEqual(related_fields, [CustomModel, CustomModel])\n\n        # Make related fields on a specific instance of the model\n        instance = CustomModel()\n        related_fields = get_related_models_recursive(state, instance)\n        self.assertEqual(related_fields, [CustomModel])\n", "    def test_related_model_attrs_get_correctly_resolved(self):\n        # Make sure the `related_model` attribute of `RelatedField` is resolved correctly.\n        # We test two scenarios: one where the related model is an app label.model name,\n        # and one where it's just a model name.\n        class Model(models.Model):\n            field = models.CharField()\n\n        class ModelRelated(models.Model):\n            field = models.CharField()\n\n        apps = isolate_apps('tests')\n        apps.register_model(apps.get_model('tests', 'Model'))\n        apps.register_model(apps.get_model('tests', 'ModelRelated'))\n\n        class RelatedField(RelatedField):\n            related_model = 'tests.ModelRelated'\n\n        field = RelatedField()\n        field._meta.model = Model\n        field.remote_field.model = 'tests.ModelRelated'\n        self.assertEqual(field.related_model, ModelRelated)\n\n        field.related_model = 'tests.ModelRelated'\n        field.remote_field.model = 'tests.ModelRelated'\n        self.assertEqual(field.related_model, ModelRelated)\n\n        class RelatedField(RelatedField):\n            related_model = 'ModelRelated'\n\n        field = RelatedField()\n        field._meta.model = Model\n        field.remote_field.model = 'ModelRelated'\n        self.assertEqual(field.related_model, ModelRelated)\n\n        field.related_model = 'ModelRelated'\n        field.remote_field.model = 'ModelRelated'\n        self.assertEqual(field.related_model, ModelRelated)\n", "    def test_ManyToManyField_through_model_deconstruction(self):\n        # Tests that a ManyToManyField's through model is properly reconstructed\n        # from its deconstructed representation.\n        with isolate_apps('tests.testapp') as apps:\n            apps.clear_cache()\n            m2m = models.ManyToManyField(UnicodeModel)\n            apps.register_model(m2m)\n            # Create a model with a ManyToManyField that uses an auto-created\n            # through model.\n            class TestModel(models.Model):\n                m2m_field = m2m\n\n            m2m.deconstruct()\n            # Reconstruct the ManyToManyField.\n            name, path, args, kwargs = m2m.deconstruct()\n            m2m.deconstruct()\n            # Now check that the deconstructed representation of the model is\n            # equal to the original deconstructed representation.\n            self.assertEqual(m2m.deconstruct(), (name, path, args, kwargs))\n            # Now check that the ManyToManyField's through model is properly\n            # reconstructed.\n            self.assertEqual(m2m.through, models.ManyToManyField(UnicodeModel))\n", "    def test_many_to_many_field_through_model_is_registered(self):\n        # ManyToManyField can have a through model that is not specified.\n        # In this case, a ManyToManyField should be created.\n        model = ModelWithCustomBase\n        model._meta.register_model_apps()\n        apps = Apps(model._meta.apps)\n\n        # The model in the database should have the intermediary model registered\n        # as an auto-created model.\n        self.assertIn(model._meta.auto_created, apps.get_models())\n\n        # The auto-created model should be the model that is created by the\n        # create_many_to_many_intermediary_model function.\n        auto_created = model._meta.auto_created\n        self.assertEqual(auto_created._meta.object_name, f\"{model._meta.object_name}_food\")\n\n        # The intermediary model should have the correct fields.\n        self.assertEqual(auto_created._meta.fields, [\n            models.ForeignKey(model, related_name=\"+\", on_delete=models.CASCADE),\n            models.ForeignKey('food', related_name=\"+\", on_delete=models.CASCADE),\n        ])\n"], "sample_1000": ["def test_octave_code_hadamard_product():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = HadamardProduct(A, B)\n    assert octave_code(C) == 'A.*B'\n", "def test_octave_code_loggamma():\n    x = symbols('x')\n    assert octave_code(log(x)).replace('log', 'log') == \"log(x)\"\n    assert octave_code(gamma(x)).replace('gamma', 'log') == \"loggamma(x)\"\n", "def test_print_piecewise_with_nested_conditions():\n    from sympy import symbols, Piecewise\n    x, y, z = symbols('x,y,z')\n    pw = Piecewise(\n        (x + y, x > 0),\n        (x + z, y > z),\n        (x + 1, True)\n    )\n    code = octave_code(pw, assign_to='tau', inline=False)\n    expected_code = \"if (x > 0)\\ntau = x + y;\\nelse if (y > z)\\ntau = x + z;\\nelse tau = x + 1;\\nend\"\n    assert code.strip() == expected_code.strip()\n", "def test_octave_code_matrix_power():\n    A = MatrixSymbol('A', 2, 2)\n    result = octave_code(A**3, contract=False)\n    expected = \"A^3 = [A(1, 1)^3, A(1, 2)^3; A(2, 1)^3, A(2, 2)^3];\"\n    assert result == expected\n", "def test_octave_code_with_hadamard_product():\n    x, y, z = symbols('x,y,z')\n    A, B = MatrixSymbol('A', 2, 2), MatrixSymbol('B', 2, 2)\n    f = Function('f')\n    mcode(f(x) + f(x)*y + f(x)*y*z, user_functions={'f': 'my_fcn'})\n    mcode(f(x) + f(x)*y + f(x)*y*z, user_functions={'f': (lambda x: x.is_Matrix, 'my_mat_fcn')})\n    mcode(f(x)*A + f(x)*B, user_functions={'f': 'my_fcn'})\n    mcode(f(x)*A + f(x)*B, user_functions={'f': (lambda x: x.is_Matrix, 'my_mat_fcn')})\n    mcode(f(x)*A + f(x)*y*A, user_functions={'f': 'my_fcn'})\n    mcode(f(x)*A + f(x)*y*A, user_functions={'f': (lambda x: x.is_Matrix, 'my_mat_fcn')})\n"], "sample_737": ["def test_TfidfVectorizer_count_zero_features():\n    # Test that TfidfVectorizer raises an error when counting zero features\n    vectorizer = TfidfVectorizer()\n    X = np.zeros((1, 10))\n    with pytest.raises(ValueError):\n        vectorizer.transform(X)\n", "def test_TfidfVectorizer():\n    # Test empty string\n    X = TfidfVectorizer().fit_transform([\"\"])\n    assert_array_equal(X.toarray(), np.array([[0.0]]))\n\n    # Test empty list\n    X = TfidfVectorizer().fit_transform([])\n    assert_array_equal(X.toarray(), np.array([]))\n\n    # Test vectorizer with ngram_range=(1, 1)\n    X = TfidfVectorizer(ngram_range=(1, 1)).fit_transform([\"Hello world\"])\n    assert_array_almost_equal(X.toarray(), np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                                                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                                                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                                                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                                                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n                                                   ", "def test_TfidfVectorizer_with_empty_vocabulary():\n    vectorizer = TfidfVectorizer(vocabulary={}, stop_words=None)\n    with pytest.raises(ValueError):\n        vectorizer.fit_transform([\"\"])\n", "def test_hashing_vectorizer_fitted_before_transform():\n    X = HashingVectorizer().fit_transform([\"the\", \"a\", \"the\", \"a\"])\n    X = X.toarray()\n    assert_equal(X.shape, (1, 2 ** 18))\n", "    def test_hashing_vectorizer_invalid_norm(self):\n        # check that a ValueError is raised when norm is not valid\n        with assert_raise_message(ValueError,\n                                \"Invalid value for norm=None \"\n                                \"lower boundary larger than the upper boundary.\"):\n            HashingVectorizer(norm='l1').fit_transform(['doc'])\n"], "sample_849": ["def test_PredefinedSplit():\n    # Test PredefinedSplit with a predefined test_fold list\n    X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n    y = np.array([0, 0, 1, 1])\n    test_fold = np.array([0, 1, -1, 1])\n    ps = PredefinedSplit(test_fold)\n    assert ps.get_n_splits() == 2\n    for train_index, test_index in ps.split():\n        assert np.array_equal(train_index, np.array([1, 2, 3]))\n        assert np.array_equal(test_index, np.array([0]))\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        assert np.array_equal(X_train, np.array([[1, 2], [3, 4]]))\n        assert np.array_equal(X_test, np.array([[1, 2]]))\n        assert np.array_equal(y_train, np.array([0, 1]))\n        assert np.array_equal(y_test, np.array([0]))\n\n    # Test PredefinedSplit with a predefined test_fold array and a mismatched length\n    with pytest.raises(ValueError):\n        test_fold = np.array([0, 1, -1, 1, 5])\n        ps = PredefinedSplit(test_fold)\n\n    # Test PredefinedSplit with a predefined test_fold array and a non-integer value\n    with pytest.raises(ValueError):\n        test_fold = np.array([0, 1, -1, 1, 'a'])\n        ps = PredefinedSplit(test_fold)\n\n    # Test PredefinedSplit with a predefined test_fold array and a non-array-like value\n    with pytest.raises(ValueError):\n        test_fold = 5\n        ps = PredefinedSplit(test_fold)\n", "    def test_split_validate_shuffle_split(self):\n        n_samples = 10\n        test_size = 2\n        train_size = 4\n        with assert_raise_message(ValueError,\n                               \"test_size=2 should be either positive and \"\n                               \"smaller than the number of samples 10 or a \"\n                               \"float in the (0, 1) range\",\n                               _validate_shuffle_split, n_samples, test_size,\n                               train_size):\n            _validate_shuffle_split(n_samples, test_size, train_size)\n", "    def test_predefined_split_on_binary_classification(self):\n        X, y = make_classification(n_samples=10, n_features=5, n_classes=2)\n        test_fold = [0, 0, -1, 0, 0, 0, 0, 0, 0, 0]\n        ps = PredefinedSplit(test_fold)\n        for train_index, test_index in ps.split():\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            assert y_train.shape[0] + y_test.shape[0] == len(y)\n", "    def test_PredefinedSplit(self):\n        test_fold = np.array([0, 1, -1, 1, 1, 0, 1])\n        ps = PredefinedSplit(test_fold)\n        assert ps.get_n_splits() == 3\n        train, test = next(ps.split())\n        assert train == [0, 4, 5, 6, 7]\n        assert test == [1, 2, 3]\n", "    def test_base_cross_validator_methods(self):\n        cv = BaseCrossValidator()\n        assert_raises(TypeError, cv.get_n_splits)\n        assert_raises_message(TypeError, \"get_n_splits is abstract in class BaseCrossValidator\",\n                            cv.get_n_splits, X=None, y=None, groups=None)\n        assert_raises_message(TypeError, \"split is abstract in class BaseCrossValidator\",\n                            cv.split, X=None, y=None, groups=None)\n        cv._iter_test_masks = lambda X, y, groups: ['test_mask']\n        assert_raises_message(TypeError, \"BaseCrossValidator._iter_test_indices is abstract\",\n                            cv._iter_test_indices, X=None, y=None, groups=None)\n        cv._iter_test_indices = lambda X, y, groups: ['test_index']\n        assert_raises_message(TypeError, \"BaseCrossValidator._iter_test_indices is abstract\",\n                            cv._iter_test_indices, X=None, y=None, groups=None)\n        cv._iter_test_masks = lambda X, y, groups: ['test_mask']\n        cv._iter_test_indices = lambda X, y, groups: ['test_index']\n        assert_raises_message(TypeError, \"BaseCrossValidator._iter_test_indices is abstract\",\n                            cv._iter_test_indices, X=None, y=None, groups=None)\n        assert_raises_message(TypeError, \"BaseCrossValidator._iter_test_indices is abstract\",\n                            cv._iter_test_indices, X=None, y=None, groups=None)\n        cv.get_n_splits = lambda X, y, groups: 2\n        cv.split = lambda X, y, groups: [('train', 'test')]\n        assert cv.get_n_splits(X=None, y=None, groups=None) == 2\n        assert cv.split(X=None, y=None, groups=None) == [('train', 'test')]\n"], "sample_1132": ["def test_rotate_left():\n    # Test rotation on empty list\n    assert rotate_left([], 5) == []\n\n    # Test rotation on list with one element\n    assert rotate_left([1], 5) == [1]\n\n    # Test rotation on list with multiple elements\n    assert rotate_left([1, 2, 3, 4, 5], 2) == [3, 4, 5, 1, 2]\n    assert rotate_left([1, 2, 3, 4, 5], 0) == [1, 2, 3, 4, 5]\n    assert rotate_left([1, 2, 3, 4, 5], 3) == [4, 5, 1, 2, 3]\n    assert rotate_left([1, 2, 3, 4, 5], 5) == [1, 2, 3, 4, 5]\n\n    # Test negative rotation\n    assert rotate_left([1, 2, 3, 4, 5], -2) == [4, 5, 1, 2, 3]\n", "    def test_roundrobin(self):\n        assert list(roundrobin('ABC', 'DEF')) == ['A', 'D', 'E', 'B', 'F', 'C']\n        assert list(roundrobin('ABC', 'DEF', 'G')) == ['A', 'D', 'G', 'E', 'B', 'F', 'C']\n        assert list(roundrobin('ABC', 'DEF', 'G', 'H')) == ['A', 'D', 'G', 'H', 'E', 'B', 'F', 'C']\n        assert list(roundrobin('ABC')) == ['A', 'B', 'C']\n        assert list(roundrobin()) == []\n", "def test_kbins_ordered():\n    # test that the ordering of elements in each partition is correct\n    for p in kbins(list(range(3)), 2, ordered=11):\n        assert all(p1 < p2 for i, p1 in enumerate(p) for p2 in p[i + 1:])\n\n    # test that the order of the partitions is correct\n    p1 = [[0], [1, 2]]\n    p2 = [[1], [0, 2]]\n    p3 = [[1], [2, 0]]\n    p4 = [[0], [2, 1]]\n    assert kbins(list(range(3)), 2, ordered=11) == [p1, p2, p3, p4]\n\n    # test that the order of the partitions is correct when there is a tie\n    p1 = [[0, 1], [2]]\n    p2 = [[2], [0, 1]]\n    p3 = [[0, 2], [1]]\n    p4 = [[1], [0, 2]]\n    assert kbins(list(range(3)), 2, ordered=10) == [p1, p2, p3, p4]\n", "def test_multiset_partitions_taocp():\n    # Test that we can get partitions for a small integer\n    for m, q in multiset_partitions(3):\n        assert m == len(q)\n        assert all(sum(q[i].values()) == 3 for i in range(m))\n        assert q == list(_partition('abc', q, m))\n\n    # Test that we can get partitions with a specified number of parts\n    for m, q in multiset_partitions(3, 2):\n        assert m == 2\n        assert all(sum(q[i].values()) == 3 for i in range(m))\n        assert q == list(_partition('abc', q, m))\n\n    # Test that we can get partitions for a multiset\n    for m, q in multiset_partitions('abc'):\n        assert m == len(q)\n        assert all(sum(q[i].values()) == len(q[i]) for i in range(m))\n        assert q == list(_partition('abc', q, m))\n\n    # Test that we can get partitions for a small multiset\n    for m, q in multiset_partitions('aab'):\n        assert m == len(q)\n        assert all(sum(q[i].values()) == len(q[i]) for i in range(m))\n        assert q == list(_partition('aab', q, m))\n\n    # Test that we can get partitions with a specified number of parts for a multiset\n    for m, q in multiset_partitions('aab', 2):\n        assert m == 2\n        assert all(sum(q[i].values()) == len(q[i]) for i in range(m))\n        assert q == list(_partition('aab', q, m))\n\n    # Test that we can get partitions for a multiset with many repetitions\n    for m, q in multiset_partitions('a' * 10):\n        assert m == len(q)\n        assert all(sum(q[i].values()) == len(q[i]) for i in range(m))\n        assert q == list(_partition('a' * 10, q, m))\n\n    # Test that we can get partitions with a specified number of parts for a multiset\n    for m, q in multiset_partitions('a' * 10, 3):\n        assert m == 3\n        assert all(sum(q[i].values()) == len(q[i]) for i in range(m))\n        assert q == list(_partition('a' * 10, q, m", "def test_is_palindromic():\n    assert is_palindromic((1, 2, 1))\n    assert not is_palindromic((1, 2, 3))\n    assert is_palindromic((1, 1, 1))\n    assert not is_palindromic((1, 1, 2))\n    assert is_palindromic((1, 2, 2, 1))\n    assert not is_palindromic((1, 2, 3, 1))\n    assert is_palindromic((1, 2, 1, 2, 1))\n    assert not is_palindromic((1, 2, 3, 2, 1))\n"], "sample_1163": ["def test_polar_lift():\n    x = Symbol('x')\n    expr = exp_polar(3*I*pi) * x\n    result, subs = polarify(expr, lift=True)\n    assert N_equals(result, polar_lift(x))\n    assert subs == {x: expr}\n", "def test_unpolarify_with_pi():\n    from sympy import pi\n    assert unpolarify(pi) == pi\n", "def test_re_unpolarify():\n    from sympy import unpolarify, exp, exp_polar, I, polar_lift\n    from sympy.abc import x\n    from sympy.testing.pytest import N_equals\n\n    assert N_equals(unpolarify(exp_polar(10*I)*x), exp(10*I)*x)\n    assert N_equals(unpolarify(polar_lift(x)), x)\n    assert N_equals(unpolarify(exp_polar(5*I)), exp(5*I))\n\n    assert unpolarify(exp_polar(0)) == 1\n    assert unpolarify(polar_lift(0)) == 0\n    assert unpolarify(exp_polar(-0)) == 1\n    assert unpolarify(polar_lift(-0)) == 0\n\n    assert unpolarify(exp_polar(10*I)*x) == exp(10*I)*x\n    assert unpolarify(polar_lift(x)) == x\n\n    assert unpolarify(exp_polar(5*I)) == exp(5*I)\n    assert unpolarify(exp_polar(5*I + 7)) == exp(5*I + 7)\n    assert unpolarify(exp(5*I + 7)) == exp(5*I + 7)\n", "def test_conjugate_is_antilinear():\n    x = symbols('x', complex=True)\n    y = symbols('y', complex=True)\n    z = symbols('z', complex=True)\n\n    eq1 = conjugate(x + y)\n    eq2 = conjugate(x + conjugate(y))\n    eq3 = conjugate(conjugate(x) + y)\n    eq4 = conjugate(conjugate(x + y))\n    eq5 = conjugate(x * y)\n\n    assert Eq(eq1, conjugate(x) + conjugate(y))\n    assert Eq(eq2, conjugate(x) + y)\n    assert Eq(eq3, x + conjugate(y))\n    assert Eq(eq4, conjugate(x) + conjugate(y))\n    assert Eq(eq5, conjugate(x) * conjugate(y))\n\n    eq6 = conjugate(x + y + z)\n    eq7 = conjugate(x * (y + z))\n\n    assert Eq(eq6, conjugate(x) + conjugate(y) + conjugate(z))\n    assert Eq(eq7, conjugate(x) * (conjugate(y) + conjugate(z)))\n\n    eq8 = conjugate(x + y + z + 1)\n    assert Eq(eq8, conjugate(x) + conjugate(y) + conjugate(z) + 1)\n\n    eq9 = conjugate(x + y + z - 1)\n    assert Eq(eq9, conjugate(x) + conjugate(y) + conjugate(z) - 1)\n\n    eq10 = conjugate(x + y + z + x)\n    assert Eq(eq10, conjugate(x) + conjugate(y) + conjugate(z) + conjugate(x))\n", "def test_sign():\n    x, y = symbols('x y', real=True)\n    assert sign(0) == 0\n    assert sign(-1) == -1\n    assert sign(1) == 1\n    assert sign(-x) == -1\n    assert sign(x) == 1\n    assert sign(2*x) == 2\n    assert sign(-x**2) == -1\n    assert sign(x**2) == 1\n    assert N_equals(sign(x + I), S.One)\n    assert N_equals(sign(-x - I), S.NegativeOne)\n    assert N_equals(sign(x - I), S.NegativeOne)\n    assert N_equals(sign(2*I), S.ImaginaryUnit)\n    assert N_equals(sign(-2*I), -S.ImaginaryUnit)\n    assert N_equals(sign(3 + 2*I), S.One)\n    assert N_equals(sign(3 - 2*I), S.One)\n    assert N_equals(sign(-3 + 2*I), -S.ImaginaryUnit)\n    assert N_equals(sign(-3 - 2*I), -S.ImaginaryUnit)\n    assert N_equals(sign(sin(x)), sign(sin(x)))\n    assert N_equals(sign(cos(x)), sign(cos(x)))\n    assert N_equals(sign(exp(x)), sign(exp(x)))\n    assert N_equals(sign(sin(atan(x))), S.One)\n    assert N_equals(sign(cos(atan(x))), sign(sin(x)))\n    assert N_equals(sign(sin(x + pi/2)), S.ImaginaryUnit)\n    assert N_equals(sign(cos(x + pi/2)), sign(cos(x)))\n    assert N_equals(sign(sin(atan(x + pi/2))), S.NegativeOne)\n    assert N_equals(sign(cos(atan(x + pi/2))), -S.ImaginaryUnit)\n"], "sample_860": ["def test_check_scalar_with_min_max_value():\n    # Test check_scalar with valid min and max values\n    x = 5\n    name = \"x\"\n    target_type = int\n    min_val = 0\n    max_val = 10\n    check_scalar(x, name, target_type, min_val, max_val)\n\n    # Test check_scalar with invalid min value\n    with assert_raise_message(ValueError,\n                            \"x=5, must be >= 0.\",\n                            check_scalar, 5, \"x\", int, min_val=-1):\n        check_scalar(5, \"x\", int, min_val=-1)\n\n    # Test check_scalar with invalid max value\n    with assert_raise_message(ValueError,\n                            \"x=5, must be <= 10.\",\n                            check_scalar, 15, \"x\", int, max_val=10):\n        check_scalar(15, \"x\", int, max_val=10)\n\n    # Test check_scalar with min and max values of None\n    with pytest.raises(TypeError):\n        check_scalar(5, \"x\", int, min_val=None, max_val=None)\n\n    # Test check_scalar with non-numeric type\n    with pytest.raises(TypeError):\n        check_scalar(5, \"x\", str)\n\n    # Test check_scalar with non-numeric value\n    with pytest.raises(TypeError):\n        check_scalar(\"x\", \"x\", int)\n", "def test_check_X_y_sparse_matrix():\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    y = np.array([1, 2])\n    with assert_no_warnings():\n        X_converted, y_converted = check_X_y(X, y)\n        assert isinstance(X_converted, sp.csr_matrix)\n        assert isinstance(y_converted, np.ndarray)\n        assert _num_samples(X_converted) == 2\n        assert y_converted.shape == (2,)\n", "def test_check_symmetric():\n    # Test symmetric matrix\n    X = np.array([[1, 2], [3, 4]])\n    assert np.array_equal(check_symmetric(X), X)\n\n    # Test non-symmetric matrix\n    X = np.array([[1, 2], [4, 3]])\n    result = check_symmetric(X)\n    assert np.array_equal(result, np.array([[1, 2], [2, 1]]))\n\n    # Test sparse matrix\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    assert np.array_equal(check_symmetric(X), X)\n\n    # Test non-square matrix\n    X = np.array([[1, 2], [3, 4, 5]])\n    with pytest.raises(ValueError):\n        check_symmetric(X)\n\n    # Test non-2D matrix\n    X = np.array([1, 2, 3])\n    with pytest.raises(ValueError):\n        check_symmetric(X)\n\n    # Test tolerance\n    X = np.array([[1e-5, 0], [0, 1e-5]])\n    assert np.array_equal(check_symmetric(X, tol=1e-4), X)\n\n    # Test warnings\n    X = np.array([[1, 2], [3, 3]])\n    with pytest.warns(UserWarning):\n        check_symmetric(X, raise_warning=False)\n\n    # Test exceptions\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError):\n        check_symmetric(X, raise_exception=True)\n\n    # Test sparse matrix with different formats\n    X = sp.coo_matrix(np.array([[1, 2], [3, 4]]))\n    result = check_symmetric(X)\n    assert np.array_equal(result, X)\n\n    X = sp.lil_matrix(np.array([[1, 2], [3, 4]]))\n    result = check_symmetric(X)\n    assert np.array_equal(result, X)\n\n    X = sp.dok_matrix(np.array([[1, 2], [3, 4]]))\n    result = check_symmetric(X)\n    assert np.array_equal(result, X)\n", "def test_check_array_sparse_no_conversion():\n    # Test check_array with sparse matrix, no conversion needed\n    X = sp.csr_matrix([[1, 0, 2], [0, 0, 3]])\n    expected_X = X.copy()\n    X_converted = check_array(X, accept_sparse=True)\n    assert_array_equal(X_converted, expected_X)\n\n", "def test_check_scalar():\n    \"\"\"Test check_scalar for positive float.\"\"\"\n    est = ARDRegression()\n    with pytest.raises(TypeError):\n        check_scalar(-1, 'alpha', float)\n    check_scalar(1, 'alpha', float)\n    with pytest.raises(ValueError):\n        check_scalar(1, 'alpha', float, min_val=0)\n\n    with pytest.raises(TypeError):\n        check_scalar('a', 'alpha', float)\n    with pytest.raises(TypeError):\n        check_scalar(1.0, 'alpha', (int, list))\n    with pytest.raises(ValueError):\n        check_scalar(3, 'alpha', float, max_val=0)\n    with pytest.raises(ValueError):\n        check_scalar(1, 'alpha', float, min_val=1, max_val=0)\n\n    with pytest.raises(TypeError):\n        check_scalar(1, 'alpha', 'string')\n"], "sample_591": ["def test_dataset_copy_deeplevel(self):\n    dset = create_test_data()\n    dset2 = dset.copy(deep=True)\n    assert_identical(dset, dset2)\n    assert not np.shares_memory(dset.data_vars['a'].values, dset2.data_vars['a'].values)\n", "    def test_merge_variables_with_different_chunks(self):\n        # Create two datasets with different chunking\n        chunks1 = {\"x\": 2, \"y\": 2}\n        chunks2 = {\"x\": 3, \"y\": 3}\n        ds1 = create_test_data(chunks=chunks1)\n        ds2 = create_test_data(chunks=chunks2)\n\n        # Merge the datasets\n        merged_ds = xr.merge(ds1, ds2)\n\n        # Check that the chunks of the merged dataset are correctly\n        # determined\n        assert merged_ds.chunks == {\"x\": 3, \"y\": 3}\n", "    def test_merge_variables(self):\n        # Create a test dataset with two variables 'x' and 'y'\n        da1 = xr.DataArray(np.random.rand(10), dims=['x'], name='x')\n        da2 = xr.DataArray(np.random.rand(10), dims=['x'], name='y')\n        ds = xr.Dataset({'x': da1, 'y': da2})\n\n        # Create another test dataset with the same variables 'x' and 'y'\n        ds2 = xr.Dataset({'x': da1, 'y': da2})\n\n        # Test that merging the two datasets returns a new dataset\n        merged_ds = ds.merge(ds2)\n        assert len(merged_ds.variables) == 2\n        assert len(merged_ds.data_vars) == 2\n\n        # Test that merging two identical datasets returns the original dataset\n        identical_ds = ds.merge(ds)\n        assert merged_ds.equals(identical_ds)\n\n        # Test that merging two datasets with different variables raises an error\n        ds3 = xr.Dataset({'z': da1})\n        with raises_regex(r'conflicting variables'):\n            ds.merge(ds3)\n\n        # Test that merging two datasets with one variable in common raises a warning\n        ds4 = xr.Dataset({'y': da1})\n        with pytest.warns(UserWarning):\n            ds.merge(ds4)\n", "def test_load_dataframe_sparse():\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    d = df.to_dict()\n    ds = xr.Dataset.from_dict(d, sparse=True)\n    assert_identical(ds.to_dataframe(), df)\n\n    ds = xr.Dataset.from_dict(d, sparse=False)\n    assert_identical(ds.to_dataframe(), df)\n\n    # Test merge for sparse\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    d = df.to_dict()\n    ds = xr.Dataset.from_dict(d, sparse=True)\n    ds2 = xr.Dataset.from_dict(d, sparse=True)\n    ds_merge = ds.merge(ds2)\n    assert_identical(ds_merge.to_dataframe(), pd.concat([df, df], axis=1))\n\n    # Test merge for non sparse\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    d = df.to_dict()\n    ds = xr.Dataset.from_dict(d, sparse=False)\n    ds2 = xr.Dataset.from_dict(d, sparse=False)\n    ds_merge = ds.merge(ds2)\n    assert_identical(ds_merge.to_dataframe(), pd.concat([df, df], axis=1))\n\n    # Test sparse with indexes\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    idx = pd.Index([1, 2], name=\"index\")\n    d = df.to_dict()\n    d[\"coords\"] = {\"index\": {\"dims\": [\"index\"], \"data\": idx, \"attrs\": {}}}\n    ds = xr.Dataset.from_dict(d, sparse=True)\n    ds2 = xr.Dataset.from_dict(d, sparse=True)\n    ds_merge = ds.merge(ds2)\n    assert_identical(ds_merge.to_dataframe(), pd.concat([df, df], axis=1))\n\n    # Test non sparse with indexes\n    df = pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]})\n    idx = pd.Index([1, 2], name=\"index\")\n    d = df.to_dict()\n    d[\"coords\"] = {\"index\": {\"dims\": [\"index\"], \"data\": idx, \"attrs\": {}}}\n    ds = xr.Dataset.from_dict(d, sparse=False)\n    ds2 = xr.Dataset.from_dict(d, sparse=False)\n    ds_merge =", "def test_merge_dataset_no_conflicts_broadcast_equals():\n    \"\"\"Test Dataset.merge() with no conflicts and compat='broadcast_equals'.\"\"\"\n    # Test that when two datasets are broadcasted against each other,\n    # their variables are compared for broadcast equality\n    ds1 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 3]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n    ds2 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 3]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n\n    merged = ds1.merge(ds2, compat=\"broadcast_equals\")\n    ds1_combined = xr.merge([ds1, ds2])\n    assert_identical(merged, ds1_combined)\n    assert_identical(merged, ds1)\n\n    ds1 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 3]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n    ds2 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 10]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n\n    with raises_regex(\"variables x and y have conflicting values\"):\n        ds1.merge(ds2, compat=\"broadcast_equals\")\n\n    ds1 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 3]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n    ds2 = xr.Dataset(\n        {\n            \"x\": (\"time\", [1, 2, 3]),\n            \"y\": (\"time\", [1, 2, 3]),\n            \"z\": (\"time\", [1, 2, 3]),\n        },\n        coords={\"time\": [0, 1, 2]},\n    )\n\n    merged = ds1.merge(ds2, compat=\"broadcast_equals\")\n   "], "sample_747": ["    def test_fit_transform_and_inverse_transform(self):\n        # Test with 'onehot' encoding\n        encoder = CategoricalEncoder(encoding='onehot', categories=None)\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv_transformed = encoder.inverse_transform(X_transformed.toarray())\n        assert_array_equal(X_inv_transformed, X)\n\n        # Test with 'ordinal' encoding\n        encoder = CategoricalEncoder(encoding='ordinal')\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv_transformed = encoder.inverse_transform(X_transformed)\n        assert_array_equal(X_inv_transformed, X)\n\n        # Test with 'onehot-dense' encoding\n        encoder = CategoricalEncoder(encoding='onehot-dense', categories=None)\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv_transformed = encoder.inverse_transform(X_transformed)\n        assert_array_equal(X_inv_transformed, X)\n\n        # Test with handle_unknown='ignore'\n        encoder = CategoricalEncoder(encoding='onehot', categories=None,\n                                    handle_unknown='ignore')\n        X = [['Male', 1], ['Female', 3], ['Female', 4]]\n        encoder.fit(X)\n        X_transformed = encoder.transform(X)\n        X_inv_transformed = encoder.inverse_transform(X_transformed.toarray())\n        assert_array_equal(X_inv_transformed, X)\n\n        # Test with handle_unknown='error'\n        encoder = CategoricalEncoder(encoding='onehot', categories=None,\n                                    handle_unknown='error')\n        X = [['Male', 1], ['Female', 3], ['Female', 4]]\n        encoder.fit(X)\n        with assert_raises_regex(ValueError, \"Found unknown categories\"):\n            encoder.transform(X)\n", "def test_PowerTransformer_inverse_transform_check_shape():\n    X = np.array([[1, 2], [3, 2], [4, 5]])\n\n    pt = PowerTransformer(method='box-cox', standardize=True)\n    pt.fit(X)\n\n    # Test inverse transform with standardize\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_equal(X.shape, X_inv.shape)\n\n    # Test inverse transform without standardize\n    pt.standardize = False\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_equal(X.shape, X_inv.shape)\n\n    # Test inverse transform with shape mismatch\n    pt.standardize = True\n    pt.lambdas_ = np.array([0, 1])\n    with assert_raises_regex(ValueError, \"Input data has a different number of features \"\n                                       \"than fitting data\"):\n        pt.inverse_transform(X)\n\n    pt = PowerTransformer(method='box-cox', standardize=False)\n    pt.fit(X)\n\n    # Test inverse transform with shape mismatch\n    X_trans = pt.transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_equal(X.shape, X_inv.shape)\n", "def test_quantile_transform_non_sparse():\n    \"\"\"Test quantile_transform with a non-sparse array\"\"\"\n    n_samples = 100\n    n_features = 3\n    rng = np.random.RandomState(0)\n    X = rng.randn(n_samples, n_features)\n    quantiles = QuantileTransformer(n_quantiles=10, random_state=42).fit_transform(X)\n    assert_equal(quantiles.shape, (n_samples, n_features))\n    assert_array_almost_equal(np.mean(quantiles, axis=0), np.zeros(n_features))\n    assert_array_almost_equal(np.std(quantiles, axis=0), np.ones(n_features))\n", "def test_PowerTransformer_inverse_transform():\n    # Test that the inverse transform is able to invert the transform\n    # successfully\n    transformer = PowerTransformer(method=\"box-cox\")\n    data = np.array([[1, 2], [3, 2], [4, 5]])\n    data_transformed = transformer.fit_transform(data)\n    data_inverse = transformer.inverse_transform(data_transformed)\n    assert_array_almost_equal(data_inverse, data)\n\n    # Test that the inverse transform is able to handle lambdas equal to 0\n    transformer = PowerTransformer(method=\"box-cox\")\n    data = np.array([[1, 2], [3, 2], [4, 5]])\n    data_transformed = transformer.fit_transform(data)\n    transformer.lambdas_ = np.array([0, 1])\n    data_inverse = transformer.inverse_transform(data_transformed)\n    assert_array_almost_equal(data_inverse, data)\n\n    # Test that the inverse transform raises an error when given invalid lambdas\n    transformer = PowerTransformer(method=\"box-cox\")\n    data = np.array([[1, 2], [3, 2], [4, 5]])\n    data_transformed = transformer.fit_transform(data)\n    transformer.lambdas_ = np.array([-1, 1])\n    with assert_raises(ValueError):\n        transformer.inverse_transform(data_transformed)\n\n    # Test that the inverse transform is able to handle standardization\n    transformer = PowerTransformer(method=\"box-cox\", standardize=True)\n    data = np.array([[1, 2], [3, 2], [4, 5]])\n    data_transformed = transformer.fit_transform(data)\n    data_inverse = transformer.inverse_transform(data_transformed)\n    assert_array_almost_equal(data_inverse, data)\n", "def test_RobustScaler_sparse():\n    X = sparse.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    scaler = RobustScaler()\n    scaler.fit(X)\n    X_scaled = scaler.transform(X)\n    assert_array_almost_equal(X_scaled.toarray(), [[-2., -0.5, 1.5],\n                                                 [2., 1.5, -0.5]])\n\n"], "sample_511": ["def test_colorbar_cmap():\n    fig, ax = plt.subplots()\n    cmap = plt.get_cmap('viridis')\n    im = ax.imshow(np.random.rand(10, 10), cmap=cmap)\n    cbar = plt.colorbar(im)\n    assert cbar.get_cmap().name == cmap.name\n    assert cbar.get_cmap().N == cmap.N\n    plt.close()\n", "def test_matshow(tmpdir):\n    filename = tmpdir / \"test.png\"\n    arr = np.arange(9).reshape(3, 3)\n    plt.matshow(arr)\n    plt.savefig(filename)\n    assert filename.exists()\n\n    plt.figure()\n    with pytest.raises(RuntimeWarning) as w:\n        plt.matshow(arr, fignum=0)\n    assert \"origin is set at the upper left hand corner\" in str(w.value)\n\n    with pytest.raises(TypeError):\n        plt.matshow(arr, fignum=\"test\")\n", "def test_matshow_matrix_with_zero_values():\n    matrix = np.zeros((5, 5))\n    with pytest.raises(ValueError):\n        plt.matshow(matrix)\n    with pytest.raises(ValueError):\n        plt.matshow(matrix, fignum=0)\n    with pytest.raises(ValueError):\n        plt.matshow(matrix, fignum=10)\n", "def test_plot_commands():\n    # Get the list of commands\n    plot_commands = get_plot_commands()\n\n    # Test that all commands can be imported from pyplot\n    for command in plot_commands:\n        try:\n            getattr(plt, command)\n        except AttributeError:\n            pytest.fail(f\"Command {command} is missing from pyplot\")\n\n    # Test that a command that should be present is not missing\n    # (This is a safeguard against accidentally removing a command)\n    assert 'savefig' in get_plot_commands(), \"savefig command is missing from pyplot\"\n\n    # Test that commands that should be deprecated raise a DeprecationWarning\n    with pytest.warns(MatplotlibDeprecationWarning):\n        plt.bar()\n\n    # Test that calling a non-existent command raises an AttributeError\n    with pytest.raises(AttributeError):\n        plt.non_existent_command()\n", "def test_matshow():\n    # Test basic usage\n    A = np.random.rand(10, 10)\n    fig, ax = plt.subplots()\n    ax.matshow(A)\n    assert ax.get_images()[0].get_array().shape == (10, 10)\n    # Test figure and axis label\n    ax.set_xlabel('X Axis')\n    ax.set_ylabel('Y Axis')\n    assert ax.get_xlabel() == 'X Axis'\n    assert ax.get_ylabel() == 'Y Axis'\n    # Test using a specific figure number\n    plt.figure(5)\n    ax = plt.gca()\n    ax.matshow(A)\n    assert ax.get_images()[0].get_array().shape == (10, 10)\n    # Test that matshow doesn't alter the axis limits\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.matshow(A)\n    assert ax.get_xlim() == (0, 1)\n    assert ax.get_ylim() == (0, 1)\n"], "sample_645": ["def test_log_capture_handler(caplog):\n    logger.info(\"Hello, world!\")\n    logger.debug(\"Debug message\")\n    caplog.records.append(logging.LogRecord(\n        name=\"root\",\n        level=logging.INFO,\n        pathname=\"/path/to/file.py\",\n        lineno=123,\n        msg=\"Some message\",\n        args=(),\n        exc_info=None,\n        func_name=\"function_name\"\n    ))\n\n    assert caplog.records == [\n        logging.LogRecord(\n            name=\"root\",\n            level=logging.INFO,\n            pathname=\"/path/to/file.py\",\n            lineno=123,\n            msg=\"Hello, world!\",\n            args=(),\n            exc_info=None,\n            func_name=\"function_name\"\n        ),\n        logging.LogRecord(\n            name=\"root\",\n            level=logging.DEBUG,\n            pathname=\"/path/to/file.py\",\n            lineno=123,\n            msg=\"Debug message\",\n            args=(),\n            exc_info=None,\n            func_name=\"function_name\"\n        )\n    ]\n\n    assert caplog.text == \"INFO    root:/path/to/file.py:123 Hello, world!\\nDEBUG    root:/path/to/file.py:123 Debug message\\n\"\n", "def test_log_capture_fixture(caplog: LogCaptureFixture, capsys):\n    logger.warning(\"Hello\")\n    caplog.set_level(logging.INFO, logger=logger.name)\n    logger.info(\"Info message\")\n    caplog.clear()\n    assert caplog.records == []\n    assert caplog.text == \"\"\n    assert caplog.messages == []\n    assert caplog.record_tuples == []\n    caplog.set_level(logging.INFO)\n    logger.warning(\"Hello\")\n    assert len(caplog.records) == 1\n    assert \"Hello\" in caplog.text\n    assert \"Hello\" in caplog.messages\n    assert (\"root\", 30, \"Hello\") in caplog.record_tuples\n\n    # Test at_level context manager\n    with caplog.at_level(logging.INFO):\n        logger.warning(\"Hello\")\n    assert len(caplog.records) == 1\n    assert \"Hello\" in caplog.text\n    assert \"Hello\" in caplog.messages\n    assert (\"root\", 30, \"Hello\") in caplog.record_tuples\n\n    # Test log records by phase\n    logger.warning(\"Hello\")\n    assert len(caplog.get_records(\"setup\")) == 0\n    assert len(caplog.get_records(\"call\")) == 1\n    assert len(caplog.get_records(\"teardown\")) == 0\n    assert len(caplog.get_records(\"finish\")) == 1\n\n    # Test log records by phase after setup, call, teardown\n    logger.warning(\"Hello\")\n    caplog.clear()\n    with caplog.at_level(logging.INFO):\n        logger.warning(\"Hello\")\n    assert len(caplog.get_records(\"setup\")) == 0\n    assert len(caplog.get_records(\"call\")) == 1\n    assert len(caplog.get_records(\"teardown\")) == 0\n    assert len(caplog.get_records(\"finish\")) == 1\n\n    # Test log messages by phase\n    logger.warning(\"Hello\")\n    assert len(caplog.messages) == 1\n    assert len(caplog.get_records(\"setup\")) == 0\n    assert len(caplog.get_records(\"call\")) == 1\n    assert len(caplog.get_records(\"teardown\")) == 0\n    assert len(caplog.get_records(\"finish\")) == 1\n\n    # Test log records by phase after setup, call, teardown\n    logger.warning(\"Hello\")\n    caplog.clear()\n    with caplog.at_level(logging.INFO):\n        logger.warning(\"Hello\")\n    assert len(caplog.messages) == 1\n", "def test_caplog_clear(tmpdir, pytester: Pytester) -> None:\n    \"\"\"Check that caplog.clear() resets caplog.\"\"\"\n    tmpdir.join(\"log.txt\").write(\"test log\\n\")\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        from _pytest.logging import caplog\n\n        logger = logging.getLogger(__name__)\n        logger.info(\"log message\")\n        caplog.clear()\n        logger.info(\"cleared log message\")\n    \"\"\"\n    )\n    result = pytester.runpytest(tmpdir.strpath)\n    assert \"log message\" in result.stderr.str()\n    assert \"cleared log message\" not in result.stderr.str()\n", "def test_log_capture_fixture(caplog, pytester):\n    # Test that a logger gets created and the level is set.\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n    logger.info(\"Hello, world!\")\n\n    pytester.caplog.records[0].assert_equal(caplog.records[0])\n", "def test_get_log_level_for_setting(tmp_path: Path) -> None:\n    \"\"\"Test get_log_level_for_setting().\"\"\"\n    config = Config()\n    value = \"WARNING\"\n    config.getoption(\"log_level\") = None\n    config.getini(\"log_level\") = None\n    assert get_log_level_for_setting(config, \"log_level\") is None\n\n    config.getoption(\"log_level\") = value\n    assert get_log_level_for_setting(config, \"log_level\") == logging.getLevelName(value)\n\n    config.getoption(\"log_level\") = None\n    config.getini(\"log_level\") = value\n    assert get_log_level_for_setting(config, \"log_level\") == logging.getLevelName(value)\n\n    config.getoption(\"log_level\") = value\n    config.getini(\"log_level\") = \"InvalidLevel\"\n    with pytest.raises(UsageError):\n        get_log_level_for_setting(config, \"log_level\")\n"], "sample_1049": ["def test_plane_perpendicular_plane():\n    p = Plane((1, 0, 0), (0, 0, 1))\n    q = Plane((0, 0, 0), (1, 0, 0))\n    assert p.is_perpendicular(q)\n    assert q.is_perpendicular(p)\n    assert p.is_perpendicular(p)\n    assert q.is_perpendicular(q)\n    assert not p.is_perpendicular(Point3D(1, 0, 0))\n", "def test_parallel_plane():\n    from sympy import Point3D, Plane\n    # Test case 1: plane parallel to the given plane and passing through the point pt.\n    a = Plane(Point3D(1, 4, 6), normal_vector=(2, 4, 6))\n    b = Plane(a.p1 + Point3D(3, 4, 5), normal_vector=(2, 4, 6))\n    assert b.equals(a.parallel_plane(b.p1))\n\n    # Test case 2: plane parallel to the given plane and passing through the point pt.\n    a = Plane(Point3D(1, 4, 6), normal_vector=(2, 4, 6))\n    b = Plane(Point3D(2, 3, 7), normal_vector=(2, 4, 6))\n    assert b.equals(a.parallel_plane(b.p1))\n\n    # Test case 3: plane not parallel to the given plane.\n    a = Plane(Point3D(1, 4, 6), normal_vector=(2, 4, 6))\n    b = Plane(Point3D(2, 3, 7), normal_vector=(3, 4, 6))\n    assert not b.equals(a.parallel_plane(b.p1))\n", "def test_parallel_plane():\n    from sympy.geometry.point import Point3D\n    p = Plane(Point3D(1, 4, 6), normal_vector=(2, 4, 6))\n    pt = Point3D(2, 3, 5)\n    p1 = p.parallel_plane(pt)\n    assert p1.p1.equals(pt)\n    assert p1.normal_vector.equals((2, 4, 6))\n    assert p1.parallel_plane(pt).equals(p1)\n", "def test_plane_random_point():\n    from sympy.geometry.util import are_coplanar\n    from sympy.geometry.point import Point\n    from sympy.geometry.line import LinearEntity3D\n\n    p1, p2, p3 = Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0)\n    p = Plane(p1, p2, p3)\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 1, 1), Point(1, 0, 0))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 0, 0), Point(0, 1, 0))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 1, 1), Point(0, 1, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 0, 0), Point(1, 0, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(0, 1, 0), Point(0, 1, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(0, 0, 0), Point(0, 0, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 1, 1), Point(1, 1, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 1, 1), Point(1, 0, 1))\n    r = p.random_point()\n    assert are_coplanar(p1, p, r)\n\n    p = Plane(p1, Point(1, 0, 1), Point(1,", "def test_plane_arbitrary_point2D():\n    from sympy.geometry.line import LinearEntity2D\n\n    # Test case for Plane.arbitrary_point method with 2D parameters\n    # A 2D plane should be treated as a special case of 3D plane\n    p = Plane((0, 0), normal_vector=(0, 1))\n    p = Plane((0, 0), normal_vector=(0, 1, 0))  # must convert to 3D\n    u, v = symbols('u v')\n    pt = p.arbitrary_point(u, v)\n    assert pt.args[0] == 0\n    assert pt.args[1] == 0\n    assert pt.args[2] == v\n\n    # Test case for Plane.arbitrary_point method with 1D parameters\n    # A 2D plane should be treated as a special case of 3D plane\n    t = symbols('t')\n    pt = p.arbitrary_point(t)\n    assert pt.args[0] == 0\n    assert pt.args[1] == 0\n    assert pt.args[2] == 0\n\n    # Test case for Plane.arbitrary_point method with default parameters\n    pt = p.arbitrary_point()\n    assert pt.args[0] == 0\n    assert pt.args[1] == 0\n    assert pt.args[2] == 0\n\n    # Test case for Plane.arbitrary_point method with invalid parameters\n    with raises(ValueError):\n        p.arbitrary_point(1, 2, 3)\n\n    # Test case for Plane.arbitrary_point method with invalid parameter values\n    with raises(ValueError):\n        p.arbitrary_point(t=1)\n    with raises(ValueError):\n        p.arbitrary_point(t='a')\n    with raises(ValueError):\n        p.arbitrary_point(u=1, v='a')\n    with raises(ValueError):\n        p.arbitrary_point(u='a', v=1)\n\n    # Test case for Plane.arbitrary_point method with 2D parameters and\n    # point not on the plane\n    p = Plane((1, 1), normal_vector=(0, 1))\n    t = symbols('t')\n    pt = p.arbitrary_point(t)\n    assert not pt in p\n\n    # Test case for Plane.arbitrary_point method with 2D parameters and\n    # point on the plane\n    p = Plane((1, 1"], "sample_773": ["def test_logistic_regression_path_with_zero_tol():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=2, random_state=0)\n    Cs = np.logspace(-4, 4, 10)\n    tol = 0.0\n    clf = LogisticRegressionCV(Cs=Cs, cv=3, tol=tol, random_state=42)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.n_iter_, 0)\n", "def test_logistic_regression_path_fitting():\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=2)\n    Cs = [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\n    scoring = get_scorer('accuracy')\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    logreg = LogisticRegressionCV(Cs=Cs, cv=cv, scoring=scoring)\n    logreg.fit(X, y)\n\n    # Check that the fit method is called with the correct parameters\n    assert hasattr(logreg, 'coef_')\n    assert hasattr(logreg, 'intercept_')\n    assert hasattr(logreg, 'classes_')\n\n    # Check that the predict method is called with the correct parameters\n    y_pred = logreg.predict(X)\n    assert_array_equal(y_pred, y)\n\n    # Check that the predict_proba method is called with the correct parameters\n    probas = logreg.predict_proba(X)\n    assert_equal(probas.shape, (X.shape[0], len(logreg.classes_)))\n    assert_array_almost_equal(probas.sum(axis=1), np.ones(X.shape[0]))\n\n    # Check that the score method is called with the correct parameters\n    score = logreg.score(X, y)\n    assert score >= 0 and score <= 1\n\n    # Check that the classes_ attribute is updated correctly\n    assert_array_equal(logreg.classes_, np.unique(y))\n\n    # Check that the n_iter_ attribute is updated correctly\n    assert logreg.n_iter_.shape == (len(logreg.classes_),)\n\n    # Check that the Cs_ attribute is updated correctly\n    assert_array_equal(logreg(Cs_.shape, logreg(Cs_.size))\n\n    # Check that the l1_ratios_ attribute is updated correctly\n    if hasattr(logreg, 'l1_ratios_'):\n        assert_array_equal(logreg.l1_ratios_.shape, (len(logreg.classes_,))\n\n    # Check that the coefs_paths_ attribute is updated correctly\n    if hasattr(logreg, 'coefs_paths_'):\n        assert_array_equal(logreg.coefs_paths_.shape,\n                          (len(logreg.classes_), cv.n_splits, Cs.size,\n                           X.shape[1]))\n    else:\n        assert_array_equal", "def test_logistic_regression_path_logistic_loss_and_grad():\n    \"\"\"Tests the output of _logistic_loss_and_grad against the logistic loss\n    function in scipy.special.expit\"\"\"\n    # Test the gradient\n        return _logistic_loss_and_grad(w, X, Y1, 1, sample_weight=[0.5, 0.3, 0.2])[1]\n    w0 = np.array([1.0, 2.0, 3.0])\n    assert_array_almost_equal(grad_fcn(w0), 2 * (np.exp(-np.dot(X, w0)) / (1 + np.exp(-np.dot(X, w0))))\n\n    # Test the loss\n        return _logistic_loss_and_grad(w, X, Y1, 1, sample_weight=[0.5, 0.3, 0.2])[0]\n    assert_array_almost_equal(loss_fcn(w0), -np.mean([np.log(0.5), np.log(0.7), np.log(0.8)]))\n", "def test_logistic_regression_path_hessian():\n    \"\"\"Test the hessian function of _logistic_regression_path\"\"\"\n    X = [[-1, 0], [0, 1], [1, 1]]\n    y = [0, 1, 1]\n    Cs = 10\n    class_weight = None\n    pos_class = None\n    max_iter = 100\n    tol = 1e-4\n    verbose = 0\n    solver = 'lbfgs'\n    penalty = 'l2'\n    intercept_scaling = 1\n    multi_class = 'auto'\n    random_state = None\n    check_input = True\n    max_squared_sum = None\n    sample_weight = None\n    l1_ratio = None\n\n    w0 = np.array([-1.])\n    C_ = 1.0\n    X_train, y_train = X, y\n    n_samples, n_features = X_train.shape\n\n    fit_intercept = True\n    # Using copy to ensure the dictionary has the correct shape\n    w0_dict = {'coef': np.expand_dims(w0, axis=1)}\n\n    fold_coefs, Cs, n_iter = _logistic_regression_path(\n        X_train, y_train, Cs=Cs, l1_ratio=l1_ratio,\n        fit_intercept=fit_intercept, solver=solver, max_iter=max_iter,\n        class_weight=class_weight, pos_class=pos_class,\n        multi_class=multi_class, tol=tol, verbose=verbose,\n        penalty=penalty, intercept_scaling=intercept_scaling,\n        random_state=random_state, check_input=check_input,\n        max_squared_sum=max_squared_sum, sample_weight=sample_weight)\n\n    assert isinstance(n_iter, np.ndarray)\n    assert n_iter.shape == (Cs.shape[0],)\n\n    # Check that the hessian function is called for the first iteration\n    assert (n_iter[0] == optimize.fmin_l_bfgs_b(\n        lambda x, *args: _multinomial_loss_grad(x, *args)[0], w0, args=(\n            X_train, y_train, 1.0 / C_, sample_weight),\n        pgtol=tol, maxiter=max_iter, iprint=50, fectol=1e-20)[1])\n\n    # Check that the n_iter array is filled with the correct values\n    for i in range(1, Cs.shape[0]):\n       ", "def test_logistic_regression():\n    \"\"\"Test Logistic Regression.\"\"\"\n    X = np.array([[1., 2., 3.], [4., 5., 6.]])\n    y = np.array([0, 1])\n    model = LogisticRegression()\n    model.fit(X, y)\n    assert_array_equal(model.classes_, np.array([0, 1]))\n    assert_array_equal(model.coef_, np.array([[1., 1., 1.]]))\n    assert_array_equal(model.intercept_, np.array([-1.35245239]))\n"], "sample_792": ["def test_nb_fit_predict_empty_array(clf):\n    # Check that fit_predict with an empty array doesn't raise a warning\n    with assert_no_warnings():\n        X = np.empty((0, 3))\n        y = np.empty((0,))\n        clf.fit(X, y)\n        clf.fit_predict(X, y)\n", "def test_GaussianNB_with_non_zero_variance_smoothing():\n    # Test that GaussianNB correctly handles non-zero variance smoothing\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([0, 0, 1])\n\n    # Test case where the variance is zero\n    gnb = GaussianNB(var_smoothing=0)\n    gnb.fit(X, y)\n    with pytest.raises(ValueError):\n        gnb._partial_fit(X, y, np.unique(y), _refit=True)\n\n    # Test case where the variance is non-zero\n    gnb = GaussianNB(var_smoothing=1e-6)\n    gnb.fit(X, y)\n\n    # Test that the variance is correctly updated\n    gnb._partial_fit(X, y, np.unique(y), _refit=True)\n    assert_array_almost_equal(gnb.sigma_[0, :], [0, 0])\n    assert_array_almost_equal(gnb.sigma_[1, :], [0, 0])\n\n    # Test case where the data is sparse\n    X_sparse = scipy.sparse.csr_matrix([[1, 2], [3, 4], [5, 6]])\n    gnb = GaussianNB(var_smoothing=1e-6)\n    gnb.fit(X_sparse, y)\n    gnb._partial_fit(X_sparse, y, np.unique(y), _refit=True)\n    assert_array_almost_equal(gnb.sigma_[0, :], [0, 0])\n    assert_array_almost_equal(gnb.sigma_[1, :], [0, 0])\n", "def test_partial_fit_classes_not_specified():\n    # Test on the 2d separable data\n    clf = ComplementNB()\n    clf.partial_fit(X, y)\n    assert_equal(clf.classes_, np.array([1, 2]))\n    assert_array_equal(clf.predict(X), np.array([1, 1, 1, 2, 2, 2]))\n\n    # Test on the random 2d data\n    clf = ComplementNB()\n    clf.partial_fit(X1, y1)\n    assert_equal(clf.classes_, np.array([0, 1]))\n    assert_array_equal(clf.predict(X1), np.array([1, 0, 1, 0, 0, 1, 0, 1, 0, 1]))\n\n    # Test on the random 100d data\n    clf = MultinomialNB()\n    clf.partial_fit(X2, y2)\n    assert_equal(clf.classes_, np.array([1, 2, 3]))\n    assert_array_equal(clf.predict(X2), np.array([1, 1, 2, 2, 3, 3]))\n\n    # Test on the random 100d data\n    clf = BernoulliNB()\n    clf.partial_fit(X2, y2)\n    assert_equal(clf.classes_, np.array([1, 2, 3]))\n    assert_array_equal(clf.predict(X2), np.array([1, 1, 2, 2, 3, 3]))\n\n    # Test with specifying classes in partial_fit\n    clf = MultinomialNB()\n    clf.partial_fit(X2, y2, classes=np.array([2, 1, 3]))\n    assert_equal(clf.classes_, np.array([1, 2, 3]))\n    assert_array_equal(clf.predict(X2), np.array([1, 1, 2, 2, 3, 3]))\n\n    # Test with specifying classes in partial_fit with no unique classes\n    clf = MultinomialNB()\n    clf.partial_fit(X2, y2, classes=np.array([2, 2, 3]))\n    assert_equal(clf.classes_, np.array([2,", "def test_nb_error_on_sparse_input_without_csr_format(mocker, nb, request):\n    X = scipy.sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    assert_raises(ValueError, nb.fit, X, [1, 1])\n", "    def test_partial_fit_with_empty_X(self):\n        \"\"\"Check partial_fit with empty array as input\"\"\"\n        estimator = GaussianNB()\n        assert_raises(ValueError, estimator.partial_fit, [], [], [])\n"], "sample_268": ["    def test_base_reloader(self):\n        reloader = autoreload.BaseReloader()\n        self.assertFalse(reloader.should_stop)\n        reloader.stop()\n        self.assertTrue(reloader.should_stop)\n", "    def test_watched_roots(self):\n        with self.assertRaises(WatchmanUnavailable):\n            reloader = autoreload.WatchmanReloader()\n            reloader._watch_root(Path('/non/existing/directory'))\n        reloader = autoreload.StatReloader()\n        self.assertIn('/non/existing/directory', reloader.watched_roots([]))\n", "    def setUp(self):\n        self.temporary_file = self.temporary_file('temp.py')\n        self.temporary_file.write_text('import time; time.sleep(10)\\n')\n        self.addCleanup(shutil.rmtree, self.temporary_file.parent)\n        self.addCleanup(self.clear_autoreload_caches)\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        t = reloader.tick()\n        self.assertIsInstance(next(t), type(None))\n        self.assertRaises(StopIteration, next, t)\n", "    def test_iter_modules_and_files_cached_result_for_same_inputs(self):\n        self.import_and_cleanup('test_module')\n        with self.assertWarns(UserWarning):\n            autoreload.iter_modules_and_files([], set())\n        # Test that two calls with the same inputs return the same result\n        with self.assertWarns(UserWarning):\n            result1 = autoreload.iter_modules_and_files([], set())\n        with self.assertWarns(UserWarning):\n            result2 = autoreload.iter_modules_and_files([], set())\n        self.assertEqual(result1, result2)\n"], "sample_760": ["def test_make_scorer_needs_proba_and_threshold():\n    \"\"\"Check that a ValueError is raised when needs_proba and needs_threshold\n    are both True.\"\"\"\n    with assert_raises(ValueError):\n        make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True,\n                    needs_threshold=True)\n", "def test_check_scoring_allow_none():\n    \"\"\"Check that check_scoring returns None when the estimator has no score\n    method and allow_none is True\"\"\"\n    with ignore_warnings():\n        # Check that check_scoring returns None with a non-estimator\n        assert check_scoring(EstimatorWithoutFit(), allow_none=True) is None\n        # Check that check_scoring returns None with an estimator that has no score\n        # method and allow_none is True\n        assert check_scoring(EstimatorWithFit(), allow_none=True) is None\n        # Check that check_scoring raises a TypeError with an estimator that has no\n        # fit method\n        with assert_raises(TypeError):\n            check_scoring(EstimatorWithoutFit())\n        # Check that check_scoring raises a TypeError with an estimator that has no\n        # score method and allow_none is False\n        with assert_raises(TypeError):\n            check_scoring(EstimatorWithFit(), allow_none=False)\n", "def test_check_scoring_multimetric():\n    # Test with single string scorer name\n    estimator = EstimatorWithFit()\n    scorer = check_scoring(estimator, scoring='accuracy')\n    assert scorer == accuracy_score\n\n    # Test with single callable scorer\n    estimator = EstimatorWithFit()\n    scorer = check_scoring(estimator, scoring=accuracy_score)\n    assert scorer == accuracy_score\n\n    # Test with multimetric scorer names\n    estimator = EstimatorWithFit()\n    scorers = check_scoring(estimator, scoring=['accuracy', 'f1_macro'])\n    assert isinstance(scorers, dict)\n    assert set(scorers.keys()) == set(['accuracy', 'f1_macro'])\n\n    # Test with multimetric scorer names and a default scorer\n    estimator = EstimatorWithFit()\n    scorers = check_scoring(estimator, scoring=['accuracy', 'f1_macro', 'default'])\n    assert isinstance(scorers, dict)\n    assert set(scorers.keys()) == set(['accuracy', 'f1_macro', 'default'])\n\n    # Test with multimetric scorer names and a non-existent scorer\n    with pytest.raises(ValueError):\n        estimator = EstimatorWithFit()\n        check_scoring(estimator, scoring=['accuracy', 'non_existent'])\n\n    # Test with multimetric scorer names and a scorer with non-string keys\n    with pytest.raises(ValueError):\n        estimator = EstimatorWithFit()\n        check_scoring(estimator, scoring={'a': accuracy_score, 'b': 'f1_macro'})\n\n    # Test with multimetric scorer names and a scorer with empty list\n    with pytest.raises(ValueError):\n        estimator = EstimatorWithFit()\n        check_scoring(estimator, scoring=[])\n", "    def test_check_scoring_estimator_with_no_score_method(self):\n        # Test that we raise an error when the estimator has no score method.\n        with assert_raises(ValueError, match=\"If no scoring is specified, the estimator passed should have a 'score' method.\"):\n            check_scoring(EstimatorWithoutFit())\n", "def test_check_scoring_no_fit():\n    # Test that check_scoring raises a TypeError when estimator has no fit method\n    with assert_raises_message(TypeError, \"'estimator should be an estimator \"\n                             \"implementing 'fit' method, EstimatorWithoutFit was \"\n                             \"passed\"):\n        check_scoring(EstimatorWithoutFit)\n\n"], "sample_162": ["    def _run_makemessages(self, **options):\n        out = StringIO()\n        management.call_command('makemessages', locale=[LOCALE], verbosity=2, stdout=out, **options)\n        output = out.getvalue()\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        return output, po_contents\n", "    def test_keep_pot(self):\n        # Make sure POT file remains after running makemessages\n        out, po_contents = self._run_makemessages(keep_pot=True)\n        self.assertTrue(os.path.exists(self.PO_FILE))\n        with open(self.PO_FILE) as fp:\n            self.assertEqual(po_contents, fp.read())\n", "    def test_localization_directories(self):\n        # Create a test project structure\n        project_dir = Path(self.work_subdir)\n        project_dir.mkdir(parents=True, exist_ok=True)\n        (project_dir / 'conf').mkdir()\n        (project_dir / 'conf' / 'locale').mkdir()\n        (project_dir / 'locale').mkdir()\n        (project_dir / 'templates').mkdir()\n        (project_dir / 'templates' / 'base.html').write_text('{{ gettext(\"Hello\") }}')\n        (project_dir / 'templates' / 'base.html').write_text('{{ gettext(\"Hello\") }}')\n\n        # Run makemessages\n        output, po_contents = self._run_makemessages()\n\n        # Check that both locale directories were considered\n        self.assertRegex(po_contents, r'#: templates\\/base\\.html:1')\n        self.assertRegex(po_contents, r'#: conf\\/locale\\/de\\/LC_MESSAGES\\/django\\.po:1')\n\n        # Check that contents are as expected\n        self.assertMsgIdPlural('Hello', po_contents, use_quotes=False)\n        self.assertMsgStr('Hello', po_contents, use_quotes=False)\n\n        # Check that a non-existent directory is ignored\n        (project_dir / 'non_existent_dir').mkdir()\n        (project_dir / 'non_existent_dir' / 'templates' / 'base.html').write_text('{{ gettext(\"Hello\") }}')\n        self._run_makemessages()\n        self.assertFalse(os.path.exists(self.PO_FILE))\n\n        # Clean up\n        shutil.rmtree(project_dir)\n", "    def test_msgmerge_add_location(self):\n        # Test if msgmerge option \"--add-location\" is being applied\n        # correctly. This is only relevant for gettext 0.19 and later.\n\n        # Disable msgmerge options to make sure it is being added later\n        self._run_makemessages(msgmerge_options=[])\n\n        # Check the PO file for the new option\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        self.assertRegex(po_contents, r'\\-\\-add\\-location=full')\n", "    def test_preprocessed_extractor(self):\n        self._create_test_project('preprocessed_extractor')\n        # Ensure the extractor will handle preprocessed files.\n        with self.settings(DEFAULT_FILE_STORAGE='django.core.files.storage.FileSystemStorage'):\n            output, po_contents = self._run_makemessages('--domain', 'djangojs')\n            self.assertTrue(self._check_po_file(po_contents, 'djangojs'))\n            # Check that the extractor used the preprocessed file.\n            self._assertPoLocComment(True, 'djangojs.po', 'path/to/file.js')\n"], "sample_814": ["def test_gradient_boosting_regressor_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    regressor = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                         random_state=1, presort=presort)\n\n    X_toy = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n    y_toy = np.array([10, 11, 12, 13, 14])\n\n    X_toy = check_array(X_toy)\n    y_toy = check_array(y_toy)\n\n    assert_raises(ValueError, regressor.predict, T)\n\n    regressor.fit(X_toy, y_toy)\n    assert_array_almost_equal(regressor.predict(X_toy), y_toy)\n\n    leaves = regressor.apply(X_toy)\n    assert_equal(leaves.shape, (5, 10, 1))\n", "def test_regression_presort_not_allowed_sparse_input(auto):\n    # Check that presort is not allowed when input is a sparse matrix\n    X = csr_matrix([[1, 2], [3, 4]])\n    estimator = GradientBoostingRegressor(presort=presort)\n    with pytest.raises(ValueError):\n        estimator.fit(X, [1, 2])\n", "def test_classification_toy_regression(loss, presort):\n    # Check classification on a toy dataset.\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n\n    # regression classification\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                   random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n\n    # test predict_proba\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert_array_almost_equal(np.sum(proba, axis=1), np.ones(6))\n    assert_array_almost_equal(np.sum(proba, axis=0), np.ones(2))\n\n    # test predict_log_proba\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -", "def test_gradient_boosting_classifier_loss_choice():\n    # Test the choice of loss function for GradientBoostingClassifier\n    loss_functions = ['deviance', 'exponential']\n    for loss in loss_functions:\n        clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                        random_state=1)\n        assert_equal(loss, clf.loss_)\n        assert_equal(loss, clf.loss_func_.__class__.__name__)\n        assert_equal(loss in ['deviance', 'exponential'],\n                    loss == 'deviance' or loss == 'exponential')\n        if loss == 'deviance':\n            assert_equal(2, clf.loss_func_.n_classes)\n        elif loss == 'exponential':\n            assert_equal(2, clf.loss_func_.n_classes)\n\n    # Test the choice of loss function when using early stopping\n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=10,\n                                    random_state=1, n_iter_no_change=5)\n    assert_equal('deviance', clf.loss_)\n    assert_equal('deviance', clf.loss_func_.__class__.__name__)\n\n    # Test the choice of loss function when using warm start\n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=10,\n                                    random_state=1, warm_start=True)\n    assert_equal('deviance', clf.loss_)\n    assert_equal('deviance', clf.loss_func_.__class__.__name__)\n\n    # Test the choice of loss function when using custom loss\n    class CustomLoss(LossFunction):\n            super().__init__(n_classes)\n            self.n_classes = n_classes\n\n            return ZeroEstimator()\n\n            return np.mean((y - pred.ravel()) ** 2.0)\n\n            return y - pred.ravel()\n\n                                  sample_weight, sample_mask,\n                                  learning_rate=0.1, k=0):\n            # update predictions\n            y_pred[:, k] += learning_rate * tree.predict(X).ravel()\n\n    clf = GradientBoostingClassifier(loss='custom', n_estimators=10,\n                                    custom_loss=CustomLoss(2), random_state=1)\n    assert_equal('custom', clf.loss_)\n    assert_equal('custom', clf.loss_func_.__", "def test_gradient_boosting_regressor_toy(presort, loss):\n    # Check regression on a toy dataset.\n    # First we try with a dataset that isn't invertible\n    X = np.array([[1., 2.], [2., 3.], [3., 3.], [4., 5.], [5., 6.],\n                  [7., 8.]])\n    y = np.array([2., 3., 3., 5., 5., 7.])\n\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                   random_state=1, presort=presort)\n    assert_raises(ValueError, clf.predict, X)\n\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), y, decimal=1)\n\n    assert_equal(10, len(clf.estimators_))\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n\n    # Now we try with a dataset that is invertible\n    X = np.array([[2., 1.], [1., 2.], [2., 2.], [3., 3.], [3., 4.],\n                  [4., 3.]])\n    y = np.array([3., 3., 3., 4., 4., 3.])\n\n    clf = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                   random_state=1, presort=presort)\n    assert_raises(ValueError, clf.predict, X)\n\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), y, decimal=1)\n\n    assert_equal(10, len(clf.estimators_))\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n"], "sample_1014": ["def test_reshape_too_small_shape():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    with raises(ValueError):\n        a.reshape(1, 1)\n", "def test_zeros():\n    # Test zeros() method\n    arr = ImmutableDenseNDimArray.zeros(3, 3)\n    assert arr.shape == (3, 3)\n    assert arr.rank() == 2\n    assert all(all(i == 0 for i in row) for row in arr._array)\n\n    # Test zeros() method with single argument\n    arr = ImmutableDenseNDimArray.zeros(6)\n    assert arr.shape == (1, 6)\n    assert arr.rank() == 2\n    assert all(i == 0 for i in arr._array[0])\n\n    # Test zeros() method with non-integer values\n    arr = ImmutableDenseNDimArray.zeros(3, Rational(1, 2))\n    assert arr.shape == (3, Rational(1, 2))\n    assert arr.rank() == 2\n    assert all(all(i == 0 for i in row) for row in arr._array)\n\n    # Test zeros() method with non-integer values and single argument\n    arr = ImmutableDenseNDimArray.zeros(Rational(1, 2))\n    assert arr.shape == (1, Rational(1, 2))\n    assert arr.rank() == 2\n    assert all(i == 0 for i in arr._array[0])\n\n    # Test zeros() method with non-integer values and non-integer shape\n    raises(ValueError, lambda: ImmutableDenseNDimArray.zeros(3, Rational(1, 2), Rational(1, 2)))\n", "def test_immutable_dense_ndim_array_as_mutable():\n    # Test that creating an ImmutableDenseNDimArray and calling as_mutable on it results in a MutableDenseNDimArray\n    a = ImmutableDenseNDimArray([1, 2, 3], (1, 1))\n    assert isinstance(a.as_mutable(), MutableDenseNDimArray)\n", "def test_immutabledense_ndim_array_zeros():\n    # Test ImmutableDenseNDimArray.zeros method\n    # Test that zeros returns an ImmutableDenseNDimArray with correct shape and values\n    a = ImmutableDenseNDimArray.zeros(2, 2, 2)\n    assert isinstance(a, ImmutableDenseNDimArray)\n    assert a.shape == (2, 2, 2)\n    assert a._array == [0, 0, 0, 0]\n\n    # Test that zeros can create an array with shape (1, 1, 1)\n    b = ImmutableDenseNDimArray.zeros(1, 1, 1)\n    assert isinstance(b, ImmutableDenseNDimArray)\n    assert b.shape == (1, 1, 1)\n    assert b._array == [0]\n\n    # Test that zeros can create an array with shape (0,)\n    c = ImmutableDenseNDimArray.zeros(0)\n    assert isinstance(c, ImmutableDenseNDimArray)\n    assert c.shape == (0,)\n    assert c._array == []\n", "def test_ImmutableDenseNDimArray_unhashable_element():\n    x = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    # create an unhashable list and try to add it\n    unhashable_list = [[1, 2, 3], [4, 5, 6]]\n    with raises(TypeError):\n        x._array = unhashable_list\n"], "sample_1046": ["def test_substitute_indices():\n    # test 1\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i0, i1 = tensor_indices('i0, i1', Lorentz)\n    A = tensorhead('A', [Lorentz, Lorentz], [[1]*2])\n    t = A(i0, i1)*A(-i1, -i0)\n    assert _is_equal(t.substitute_indices((i0, i1), (-i1, -i0)), t)\n    assert _is_equal(t, t.substitute_indices((i0, i1), (i0, i1)))\n\n    # test 2\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i0, i1 = tensor_indices('i0, i1', Lorentz)\n    A = tensorhead('A', [Lorentz, Lorentz], [[1]*2])\n    t = A(i0, i1)*A(-i1, -i0)\n    assert _is_equal(t.substitute_indices((i0, i1), (-i1, i1)), 0)\n    assert _is_equal(t, t.substitute_indices((i0, i1), (i1, i0)))\n\n    # test 3\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i0, i1 = tensor_indices('i0, i1', Lorentz)\n    A = tensorhead('A', [Lorentz, Lorentz], [[1]*2])\n    t = A(i0, i1)*A(-i1, i1)\n    assert _is_equal(t.substitute_indices((i0, i1), (-i1, i1)), 0)\n    assert _is_equal(t, t.substitute_indices((i0, i1), (i1, -i1)))\n\n    # test 4\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i0, i1 = tensor_indices('i0, i1', Lorentz)\n    A = tensorhead('A', [Lorentz, Lorentz], [[1]*2])\n    t = A(i0, i1)*A(-i1, -i0)\n    assert _is", "def test_tensor_cyclic():\n    # Test that riemann_cyclic works on a tensor with multiple instances of the Riemann tensor\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    R = tensorhead('R', [Lorentz]*4, [[2, 2]])\n    t = (R(i,j,k,l) + R(i,j,k,l))*R(-i,-j,-k,-l)\n    t = riemann_cyclic(t)\n    assert _is_equal(t, 0)\n\n    # Test that riemann_cyclic works on a tensor with a single instance of the Riemann tensor\n    t = R(i,j,k,l)*R(-i,-j,-k,-l)\n    t = riemann_cyclic(t)\n    assert _is_equal(t, 0)\n\n    # Test that riemann_cyclic works on a tensor with no Riemann tensors\n    t = R(i,j,k,l)\n    t = riemann_cyclic(t)\n    assert _is_equal(t, R(i,j,k,l))\n\n    # Test that riemann_cyclic works on a tensor with the cyclic identity\n    t = (R(i,j,k,l) - R(i,j,l,k) + R(i,k,j,l))\n    t = riemann_cyclic(t)\n    assert _is_equal(t, 0)\n\n    # Test that riemann_cyclic works on a tensor with the cyclic identity and some other terms\n    t = R(i,j,k,l) - R(i,j,l,k) + R(i,k,j,l) + R(i,j,k,l)*R(-i,-j,-k,-l)\n    t = riemann_cyclic(t)\n    assert _is_equal(t, R(i,j,k,l)*R(-i,-j,-k,-l))\n\n    # Test that riemann_cyclic works on a tensor with multiple instances of the Riemann tensor and some other terms\n    t = R(i,j,k,l) + R(i,j,k,l) - R(i,j,l,k) + R(i,k,j,l) + R(i,j,k,l)*R(-i,-j,-k,-l)\n    t = riemann_cyclic(t)\n    assert _is_equal(t, 0)\n", "def test_indices_canon_args():\n    # test the indices_canon_args method\n    from sympy.tensor.tensor import _IndexStructure\n    from sympy.combinatorics import Permutation\n    L = TensorIndexType('Lorentz')\n    a, b, c, d = tensor_indices('a,b,c,d', L)\n    t = tensorhead('A', [L, L, L, L], [[1], [1], [1], [1]])\n    t = t(a, -b, c, -d)\n    assert _IndexStructure.from_indices(*t.indices).indices_canon_args() == ([0, 1, 2, 3], [[(0, 1)], [(0, 2)], [(0, 3)], [(1, 2)]], [0, 0, 0, 0], [(0, 1)])\n\n    # test the indices_canon_args method with a tensor with no free indices\n    t = tensorhead('A', [L, L], [[1], [1]])\n    t = t(a, -b)\n    assert _IndexStructure.from_indices(*t.indices).indices_canon_args() == ([], [], [0, 0], [(0, 1)])\n\n    # test the indices_canon_args method with a tensor with a single free index\n    t = tensorhead('A', [L, L], [[1], [1]])\n    t = t(a)\n    assert _IndexStructure.from_indices(*t.indices).indices_canon_args() == ([0], [], [0, 0], [])\n\n    # test the indices_canon_args method with a tensor with a single free index and a dummy index\n    t = tensorhead('A', [L, L], [[1], [1]])\n    t = t(a, -b)\n    assert _IndexStructure.from_indices(*t.indices).indices_canon_args() == ([0], [], [0, 0], [(0, 1)])\n\n    # test the indices_canon_args method with a tensor with multiple free indices\n    t = tensorhead('A', [L, L, L], [[1], [1], [1]])\n    t = t(a, b, c)\n    assert _IndexStructure.from_indices(*t.indices).indices_canon_args() == ([0, 1, 2], [], [0, 0, 0], [])\n\n    # test the", "def test_symmetric_tensor_canonicalization():\n    from sympy.tensor.tensor import TensorIndexType, tensor_indices, tensorhead, canon_bp\n    from sympy.tensor.tensor import tensor_mul\n    from sympy.tensor.tensor import TensorManager\n\n    # Testing a symmetric tensor with two contravariant and one covariant indices\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    m0, m1, m2 = tensor_indices('m0,m1,m2', Lorentz)\n    A = tensorhead('A', [Lorentz]*3, [[1]*3])\n    t = A(m0, m1, m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with three contravariant indices\n    t = A(m0, m1, m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with one contravariant and two covariant indices\n    t = A(m0, -m1, -m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with no contravariant or covariant indices\n    t = A(m0)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with two contravariant indices and two dummy indices\n    t = A(m0, -m1)*A(m1, -m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with no contravariant indices and two dummy indices\n    t = A(m0)*A(m1, -m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with one contravariant index and one dummy index\n    t = A(m0)*A(-m1, m2)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with two contravariant indices and three dummy indices\n    t = A(m0, -m1)*A(m1, -m2)*A(m2, -m3)\n    assert _is_equal(t, canon_bp(t))\n\n    # Testing a symmetric tensor with two contravariant indices and two dummy indices and a non-symmetric metric\n    Lorentz = TensorIndexType('Lorentz', metric=True, dummy_fmt='L')\n    m0, m1, m2, m", "def test_get_free_indices():\n    # Test cases from tensor_can.py\n    from sympy.tensor.tensor import TensorIndexType, tensorhead\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    m0, m1, m2, m3 = tensor_indices('m0,m1,m2,m3', Lorentz)\n    g = Lorentz.metric\n    p, q = tensorhead('p,q', [Lorentz], [[1]])\n    t = p(m0, m1) + q(m0, m1)\n    assert get_free_indices(t) == (m0, m1)\n\n    t = p(m0, m1)*g(m2, m3)\n    assert get_free_indices(t) == (m0, m1)\n\n    t = p(m0, m1)*q(-m1, m2)\n    assert get_free_indices(t) == (m0,)\n\n    # Test cases to ensure get_free_indices works correctly with dummy indices\n    t = p(m0, m1)*p(-m0, m1)\n    assert get_free_indices(t) == (m0, m1)\n\n    t = p(m0, m1)*g(-m0, m1)\n    assert get_free_indices(t) == (m0, m1)\n\n    t = p(m0, m1)*g(m2, -m1)\n    assert get_free_indices(t) == (m0,)\n\n    # Test cases for different types of tensors\n    A = tensorhead('A', [Lorentz, Lorentz, Lorentz], [[1], [1], [1]])\n    t = A(m0, m1, m2)\n    assert get_free_indices(t) == (m0, m1, m2)\n\n    # Test case for a tensor with no free indices\n    t = A(m0, -m0)\n    assert get_free_indices(t) == ()\n\n    # Test case for a tensor with multiple free indices and dummy indices\n    t = p(m0, m1)*p(-m0, -m1)\n    assert get_free_indices(t) == (m0, m1)\n\n    # Test case for a tensor with a single free index and multiple dummy indices\n    t = p(m0)*p(-m0, m1)\n    assert get_free_indices(t) =="], "sample_963": ["def test_restify_invalid_builtins():\n    \"\"\"Check restify() handles invalid builtins.\"\"\"\n    assert restify(int) == ':py:class:`int`'\n    assert restify(bool) == ':py:class:`bool`'\n    assert restify(float) == ':py:class:`float`'\n    assert restify(complex) == ':py:class:`complex`'\n", "def test_restify_newtype():\n    # Test restify for NewType\n    assert restify(MyInt) == ':py:obj:`~typing.NewType`'\n    # Test restify for NewType with module\n    MyInt = NewType('MyInt', int)\n    assert restify(MyInt) == ':py:obj:`sphinx.test.util_typing.MyInt`'\n    # Test restify for NewType with module and name\n    MyInt = NewType('MyInt', int)\n    MyInt.__module__ = 'sphinx.test.util_typing'\n    assert restify(MyInt) == ':py:obj:`sphinx.test.util_typing.MyInt`'\n", "def test_stringify_TypeVar():\n    T = TypeVar('T')\n    assert stringify(T) == 'T'\n", "def test_restify_union_with_multiple_system_defined_typevars():\n    \"\"\"Test restify with a Union type having multiple system defined TypeVars.\"\"\"\n    class MyDict(Dict[T, T]):\n        pass\n\n    my_union = Union[MyDict, T, T]\n    expected = ':py:obj:`~sphinx.util.typing.Union`\\\\ [Dict[T, T]]'\n    assert restify(my_union) == expected\n", "def test_restify_callables():\n        pass\n\n    assert restify(my_callable) == ':py:obj:`my_callable[[int], int]`'\n\n        pass\n\n    assert restify(my_callable_with_defaults) == ':py:obj:`my_callable[[int = 1, str = hello], int]`'\n\n        pass\n\n    assert restify(my_callable_with_varargs_and_kwargs) == ':py:obj:`my_callable[[int, *str, str, **str], int]`'\n\n    class MyCallableType(typing.Generic[T]):\n            pass\n\n    assert restify(MyCallableType) == ':py:obj:`~typing.GenericMeta[[MyCallableType], typing.TypeVar(\"T\"), typing.TypeVar(\"T\")]'\n"], "sample_223": ["    def test_update_or_create(self):\n        from django.db import transaction\n        obj = MyObject.objects.create()\n        with transaction.atomic():\n            obj1, created = MyObject.objects.update_or_create(name='obj1', defaults={'value': 1})\n            self.assertTrue(created)\n        with transaction.atomic():\n            obj2, created = MyObject.objects.update_or_create(name='obj1', defaults={'value': 2})\n            self.assertFalse(created)\n            self.assertEqual(obj2.value, 2)\n            with transaction.atomic():\n                MyObject.objects.filter(name='obj1').update(value=3)\n                obj3, created = MyObject.objects.update_or_create(name='obj1', defaults={'value': 4})\n                self.assertFalse(created)\n                self.assertEqual(obj3.value, 3)\n", "    def test_prefetch_related_ordering(self):\n        from django.db.models import Prefetch\n        from .models import Item, Note, Tag\n\n        obj = Item.objects.prefetch_related('tags__notes').get(id=1)\n        self.assertEqual(list(obj.tags.all()), [Tag.objects.get(id=1), Tag.objects.get(id=2)])\n        self.assertEqual(list(obj.tags.first().notes.all()), [Note.objects.get(id=1)])\n", "    def test_values_select_related(self):\n        # Test values() and select_related() together.\n        qs = Item.objects.select_related('creator').values('name', 'id', 'creator__name')\n        self.assertEqual(list(qs), [\n            {'name': 'one', 'id': 1, 'creator__name': 'a1'},\n            {'name': 'two', 'id': 2, 'creator__name': 'a2'},\n            {'name': 'three', 'id': 3, 'creator__name': 'a2'},\n            {'name': 'four', 'id': 4, 'creator__name': 'a4'},\n        ])\n", "    def setUpTestData(cls):\n        cls.c1 = Channel.objects.create(name='channel1', description='desc1')\n        cls.c2 = Channel.objects.create(name='channel2', description='desc2')\n        cls.c3 = Channel.objects.create(name='channel3', description='desc3')\n\n        cls.ch1 = Chapter.objects.create(title='chap1', channel=cls.c1, type='video')\n        cls.ch2 = Chapter.objects.create(title='chap2', channel=cls.c2, type='video')\n        cls.ch3 = Chapter.objects.create(title='chap3', channel=cls.c3, type='video')\n\n        cls.p1 = Program.objects.create(title='program1', channel=cls.c1, date='2007-12-19')\n        cls.p2 = Program.objects.create(title='program2', channel=cls.c2, date='2007-12-19')\n        cls.p3 = Program.objects.create(title='program3', channel=cls.c3, date='2007-12-19')\n\n        cls.t1 = Ticket21203Parent.objects.create(title='ticket1', parent=None)\n        cls.t2 = Ticket21203Child.objects.create(title='ticket2', parent=cls.t1)\n        cls.t3 = Ticket21203Child.objects.create(title='ticket3', parent=cls.t1)\n        cls.t4 = Ticket23605A.objects.create(title='ticket4', parent=None)\n        cls.t5 = Ticket23605B.objects.create(title='ticket5', parent=cls.t4)\n", "    def test_values(self):\n        from django.db.models import Value\n        QuerySet = self.model.QuerySet\n        qs = QuerySet.objects.all()\n        values_q = qs.values('id', 'name')\n        self.assertEqual(values_q.query.values_select, ['id', 'name'])\n        self.assertEqual(list(values_q), [{'id': 1, 'name': 'a1'}, {'id': 2, 'name': 'a2'}])\n\n        values_q = qs.values('name')\n        self.assertEqual(values_q.query.values_select, ['name'])\n        self.assertEqual(list(values_q), [{'name': 'a1'}, {'name': 'a2'}])\n\n        values_q = qs.values_list('id', 'name', flat=True)\n        self.assertEqual(values_q.query.values_select, ['id', 'name'])\n        self.assertEqual(list(values_q), [1, 'a1', 2, 'a2'])\n\n        values_q = qs.values_list('name')\n        self.assertEqual(values_q.query.values_select, ['name'])\n        self.assertEqual(list(values_q), ['a1', 'a2'])\n\n        values_q = qs.values_list('id', 'name', named=True)\n        self.assertEqual(values_q.query.values_select, ['id', 'name'])\n        self.assertEqual(list(values_q), [{'id': 1, 'name': 'a1'}, {'id': 2, 'name': 'a2'}])\n\n        values_q = qs.values_list('name', flat=True)\n        self.assertEqual(values_q.query.values_select, ['name'])\n        self.assertEqual(list(values_q), ['a1', 'a2'])\n\n        # Check that Model.objects.all().values() raises an exception\n        with self.assertRaises(TypeError):\n            Model.objects.all().values()\n\n        # Check that Model.objects.all().values_list(flat=True) raises an exception\n        with self.assertRaises(TypeError):\n            Model.objects.all().values_list(flat=True)\n\n        # Check that Model.objects.all().values_list(named=True) raises an exception\n        with self.assertRaises(TypeError):\n            Model.objects.all().values_list(named=True)\n\n        # Check that values_list() raises an exception when called with a Q() object\n        with self.assertRaises(TypeError):\n            Model.objects.all().values_list(Q(id=1))\n\n        # Check that values_list() raises an exception when called with a field instance\n        with self.assertRaises(TypeError):\n            Model.objects.all().values_list(Model._meta.get_field('id'))\n"], "sample_756": ["def test_optics_max_eps():\n    rng = np.random.RandomState(0)\n    n_points = 50\n    X = rng.rand(n_points, 2)\n    max_eps = 0.5\n    min_samples = 10\n    cluster = OPTICS(min_samples=min_samples, max_eps=max_eps)\n    cluster.fit(X)\n    assert cluster.max_eps == max_eps\n    assert cluster.min_samples == min_samples\n    assert cluster.maxima_ratio == 0.75\n    assert cluster.rejection_ratio == 0.7\n    assert cluster.similarity_threshold == 0.4\n    assert cluster.significant_min == 0.003\n    assert cluster.min_cluster_size == 0.005\n    assert cluster.min_maxima_ratio == 0.001\n    assert cluster.algorithm == 'ball_tree'\n    assert cluster.metric == 'minkowski'\n    assert cluster.metric_params is None\n    assert cluster.p == 2\n    assert cluster.leaf_size == 30\n    assert cluster.n_jobs is None\n    assert cluster.max_eps == max_eps\n    assert cluster.min_samples == min_samples\n    assert cluster.maxima_ratio == 0.75\n    assert cluster.rejection_ratio == 0.7\n    assert cluster.similarity_threshold == 0.4\n    assert cluster.significant_min == 0.003\n    assert cluster.min_cluster_size == 0.005\n    assert cluster.min_maxima_ratio == 0.001\n    assert cluster.algorithm == 'ball_tree'\n    assert cluster.metric == 'minkowski'\n    assert cluster.metric_params is None\n    assert cluster.p == 2\n    assert cluster.leaf_size == 30\n    assert cluster.n_jobs is None\n    assert cluster.max_eps == max_eps\n    assert cluster.maxima_ratio == 0.75\n    assert cluster.rejection_ratio == 0.7\n    assert cluster.similarity_threshold == 0.4\n    assert cluster.significant_min == 0.003\n    assert cluster.min_cluster_size == 0.005\n    assert cluster.min_maxima_ratio == 0.001\n    assert cluster.algorithm == 'ball_tree'\n    assert cluster.metric == 'minkowski'\n    assert cluster.metric_params is None\n    assert cluster.p == 2\n    assert cluster.leaf_size == 30\n    assert cluster.n_jobs is None\n    assert cluster.max_eps == max_eps\n", "def test_optics_smoke():\n    X = generate_clustered_data(100, 4, rng=np.random.RandomState(0))\n    optics = OPTICS()\n    labels = optics.fit(X).labels_\n    assert labels.max() != -1\n    assert np.unique(labels).size > 1\n", "def test_extract_dbscan_optics():\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, C6))\n\n    # Test DBSCAN extraction with max_eps close to eps\n    eps = 0.1\n    opt = OPTICS(max_eps=0.1)\n    opt.fit(X)\n    labels_optics = opt.labels_\n    labels_dbscan = opt.extract_dbscan(eps).labels_\n\n    # Check that the labels are the same when epsilon is set to the same\n    # value as max_eps\n    assert_array_equal(labels_optics, labels_dbscan)\n\n    # Test DBSCAN extraction with max_eps far from eps\n    max_eps = 1.0\n    opt = OPTICS(max_eps=max_eps)\n    opt.fit(X)\n    labels_optics = opt.labels_\n    labels_dbscan = opt.extract_dbscan(eps).labels_\n    # Check that the labels are different when epsilon is much smaller than max_eps\n    assert_not_equal(labels_optics, labels_dbscan)\n\n", "def test_optics_max_eps():\n    \"\"\"Test OPTICS with max_eps parameter\"\"\"\n    rng = np.random.RandomState(0)\n    n_points_per_cluster = 10\n    C1 = [-5, -2] + .8 * rng.randn(n_points_per_cluster, 2)\n    C2 = [4, -1] + .1 * rng.randn(n_points_per_cluster, 2)\n    C3 = [1, -2] + .2 * rng.randn(n_points_per_cluster, 2)\n    C4 = [-2, 3] + .3 * rng.randn(n_points_per_cluster, 2)\n    C5 = [3, -2] + 1.6 * rng.randn(n_points_per_cluster, 2)\n    C6 = [5, 6] + 2 * rng.randn(n_points_per_cluster, 2)\n    X = np.vstack((C1, C2, C3, C4, C5, C6))\n\n    max_eps = 5.0\n    min_samples = 5\n    # OPTICS will be able to detect clusters at this max_eps\n    max_eps_over_min_samples = max_eps / min_samples\n    optics = OPTICS(max_eps=max_eps, min_samples=min_samples)\n    labels = optics.fit_predict(X)\n\n    # Check that cluster labels are correct\n    assert_equal(np.unique(labels), np.array([-1, 0, 1, 2, 3, 4, 5]))\n\n    # Test that points are classified correctly\n    cluster_labels = {0: np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n                     1: np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n                     2: np.array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n                     3: np.array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n                     4: np.array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n                     5: np", "def test_optics_max_eps_warning():\n    # Test that OPTICS raises a warning when max_eps is close to eps\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 2)\n\n    cluster = OPTICS(max_eps=5.0)\n    with assert_warns(UserWarning):\n        cluster.fit(X)\n    with assert_raise_message(UserWarning,\n                             \"Warning, max_eps (5.0) is close to eps \"\n                             \"(2.5): Output may be unstable.\"):\n        cluster.fit(X, eps=2.5)\n"], "sample_233": ["    def test_make_token_with_timestamp(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2024, 7, 26))\n        token = generator.make_token(user)\n        expected_token = generator._make_token_with_timestamp(user, generator._num_seconds(generator._now()))\n        self.assertEqual(token, expected_token)\n", "    def test_make_token(self):\n        # Test that making a token returns a string with the expected format.\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 1, 1))\n        user = User.objects.create_user(username='testuser', email='test@example.com')\n        token = generator.make_token(user)\n        self.assertIsInstance(token, str)\n        self.assertEqual(token.count('-'), 1)\n", "    def test_token_generator_timeout(self):\n        \"\"\"\n        Test that a password reset token is invalid after the timeout period.\n        \"\"\"\n        # Create a user\n        user = User.objects.create_user('testuser', 'test@example.com')\n\n        # Create a token generator\n        token_generator = MockedPasswordResetTokenGenerator(now=datetime.now() - timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT - 1))\n\n        # Make a token\n        token = token_generator.make_token(user)\n\n        # Check that the token is valid\n        self.assertTrue(token_generator.check_token(user, token))\n\n        # Wait for the timeout period to pass\n        token_generator._now_val = datetime.now() + timedelta(seconds=1)\n\n        # Check that the token is no longer valid\n        self.assertFalse(token_generator.check_token(user, token))\n", "    def test_make_token_with_invalid_user(self):\n        generator = MockedPasswordResetTokenGenerator(datetime.now() - timedelta(days=1))\n        user = User()\n        with self.assertRaises(ValueError):\n            generator.make_token(user)\n", "    def test_check_token_with_valid_token(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com')\n        token_generator = MockedPasswordResetTokenGenerator(datetime(2022, 1, 1))\n        token = token_generator.make_token(user)\n        self.assertTrue(token_generator.check_token(user, token))\n"], "sample_862": ["def test_hashing_vectorizer_transform_hash_function_collision():\n    from sklearn.feature_extraction.text import HashingVectorizer\n    vectorizer = HashingVectorizer(n_features=16)\n    X = vectorizer.fit_transform(['the', 'the', 'the'])\n    assert X.toarray().shape == (3, 16)\n    assert X[0, 2] == X[1, 2] == X[2, 2]\n    assert X[0, 0] != X[1, 0]\n    assert X[0, 1] != X[1, 1]\n    assert X[0, 3] != X[1, 3]\n    assert X[0, 4] != X[1, 4]\n    assert X[0, 5] != X[1, 5]\n    assert X[0, 6] != X[1, 6]\n    assert X[0, 7] != X[1, 7]\n    assert X[0, 8] != X[1, 8]\n    assert X[0, 9] != X[1, 9]\n    assert X[0, 10] != X[1, 10]\n    assert X[0, 11] != X[1, 11]\n    assert X[0, 12] != X[1, 12]\n    assert X[0, 13] != X[1, 13]\n    assert X[0, 14] != X[1, 14]\n    assert X[0, 15] != X[1, 15]\n", "    def test_get_feature_names(self):\n        vectorizer = TfidfVectorizer()\n        vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_array_equal(vectorizer.get_feature_names(), sorted(vectorizer.vocabulary_.keys()))\n", "def test_HashingVectorizer_empty_input():\n    with pytest.raises(ValueError, match=\"Iterable over raw text documents expected, string object received.\"):\n        HashingVectorizer().fit_transform(\"Hello\")\n", "def test_hashing_vectorizer_binary():\n    vectorizer = HashingVectorizer(binary=True, n_features=10)\n    X = vectorizer.fit_transform([\"This is a test document.\", \"This is another test document.\"])\n    assert X.nnz == 2\n    assert X.shape[1] == 10\n    expected = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.int64)\n    assert_array_equal(X[0].toarray(), expected)\n    expected = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.int64)\n    assert_array_equal(X[1].toarray(), expected)\n", "    def test_hasher_exceptions(self):\n        hv = HashingVectorizer(n_features=10)\n        with pytest.raises(ValueError, match=\"negative value for max_df\"):\n            hv.max_df = -1\n        with pytest.raises(ValueError, match=\"negative value for min_df\"):\n            hv.min_df = -1\n        with pytest.raises(ValueError, match=\"max_features=.*neither a positive integer\"):\n            hv.max_features = -1\n        with pytest.raises(ValueError, match=\"'ascii' is not a valid tokenization scheme/analyzer\"):\n            hv.analyzer = 'ascii'\n        with pytest.raises(ValueError, match=\"'ascii' is not a valid tokenization scheme/analyzer\"):\n            hv.analyzer = 'unicode'\n        with pytest.raises(ValueError, match=\"'ascii' is not a valid tokenization scheme/analyzer\"):\n            hv.analyzer = None\n        with pytest.raises(ValueError, match=\"Input has n_features=.*while the model\"):\n            hv = HashingVectorizer(n_features=10)\n            X = np.array([['the pizza', 'pizza', 'pizza'], ['the pizza', 'burger', 'burger']])\n            hv.fit_transform(X)\n            hv.transform(X)\n"], "sample_1182": ["def test_PythonCodePrinter_simple_numbers():\n    printer = PythonCodePrinter()\n    assert printer.doprint(1) == '1'\n    assert printer.doprint(-1) == '-1'\n    assert printer.dopprint(2.5) == '2.5'\n    assert printer.doprint(-2.5) == '-2.5'\n", "def test_MpmathPrinter_unsupported_math_functions():\n    from sympy import tanh, erf, gamma, factorial\n    printer = MpmathPrinter()\n    assert printer.doprint(tanh(x)) == 'mpmath.tanh(x)'\n    assert printer.doprint(erf(x)) == 'mpmath.erf(x)'\n    assert printer.doprint(gamma(x)) == 'mpmath.gamma(x)'\n    assert printer.doprint(factorial(x)) == 'mpmath.factorial(x)'\n", "def test_print_Piecewise():\n    # Test basic usage of Piecewise\n    p = PythonCodePrinter()\n    expr = Piecewise((1, x > 1), (0, True))\n    expected = \"(1 if x > 1 else 0)\"\n    assert p._print_Piecewise(expr) == expected\n\n    # Test multiple pieces in Piecewise\n    expr = Piecewise((1, x > 1), (2, x == 1), (0, True))\n    expected = \"(1 if x > 1 else (2 if x == 1 else 0))\"\n    assert p._print_Piecewise(expr) == expected\n\n    # Test empty Piecewise\n    expr = Piecewise()\n    expected = ''\n    assert p._print_Piecewise(expr) == expected\n\n    # Test Piecewise with non-True condition\n    expr = Piecewise((1, x > 1), (2, x == 1), (0, False))\n    expected = \"(1 if x > 1 else (2 if x == 1 else None))\"\n    assert p._print_Piecewise(expr) == expected\n\n    # Test Piecewise with None as the default value\n    expr = Piecewise((1, x > 1), (2, x == 1), (None, True))\n    expected = \"(1 if x > 1 else (2 if x == 1 else None))\"\n    assert p._print_Piecewise(expr) == expected\n", "def test_printing_piecewise_with_multiple_args():\n    from sympy import Piecewise\n    from sympy.printing.pycode import PythonCodePrinter\n    from sympy import Or, And\n\n    pw = Piecewise((1, Or(x>0, x<0)), (x, x == 0))\n    printer = PythonCodePrinter()\n    result = printer.doprint(pw)\n    assert result == \"(1 if (x > 0 or x < 0) else x)\"\n", "def test_python_printer():\n    # Test that no error is raised for a simple expression\n    pycode(x + y)\n    \n    # Test that error is raised when a reserved word is used as a"], "sample_742": ["def test_logistic_regression_path():\n    \"\"\"Test logistic_regression_path with multiclass and binary case\"\"\"\n    X = [[-1, 0], [0, 1], [1, 1]]\n    X_sp = sp.csr_matrix(X)\n    Y1 = [0, 1, 1]\n    Y2 = [2, 1, 0]\n\n    # Test multiclass case\n    Cs = [10.0, 100.0]\n    path = logistic_regression_path(X, Y2, Cs=Cs, fit_intercept=True)\n    assert_equal(len(path), 2)\n    assert_equal(path[0].shape, (2, 2 + 1))\n    assert_equal(path[1].shape, (2, 2 + 1))\n\n    # Test binary case\n    Cs = [10.0, 100.0]\n    path = logistic_regression_path(X, Y1, Cs=Cs, fit_intercept=True)\n    assert_equal(len(path), 2)\n    assert_equal(path[0].shape, (1, 2 + 1))\n    assert_equal(path[1].shape, (1, 2 + 1))\n\n    # Test multiclass case with multilogit\n    Cs = [10.0, 100.0]\n    path = logistic_regression_path(X, Y2, Cs=Cs, fit_intercept=True,\n                                   multi_class='multinomial')\n    assert_equal(len(path), 2)\n    assert_equal(path[0].shape, (3, 2))\n    assert_equal(path[1].shape, (3, 2))\n\n    # Test binary case with multilogit\n    Cs = [10.0, 100.0]\n    path = logistic_regression_path(X, Y1, Cs=Cs, fit_intercept=True,\n                                   multi_class='multinomial')\n    assert_equal(len(path), 2)\n    assert_equal(path[0].shape, (2, 2))\n    assert_equal(path[1].shape, (2, 2))\n\n    # Test logistic_regression_path with different solvers\n    Cs = [10.0, 100.0]\n    path = logistic_regression_path(X, Y2, Cs=Cs, fit_intercept=True,\n                                   solver='lbfgs')\n    assert_equal(len(path), 2)\n    assert_equal(path[0].shape, (2, 2 + 1))\n    assert_equal", "def test_logistic_regression_path():\n    # Test for multinomial case\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[1, 0], [0, 1], [1, 0]])\n    w = np.array([0.5, 0.5, 0.5, 0.5])\n    path = logistic_regression_path(X, Y, solver='lbfgs', Cs=10,\n                                   fit_intercept=True, max_iter=100,\n                                   tol=1e-4, verbose=0, multi_class='multinomial')\n    assert_array_equal(path[0].shape, (3, 2))\n    assert_array_equal(path[1].shape, (10,))\n\n    # Test for ovr case\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([0, 1, 0])\n    w = np.array([0.5, 0.5])\n    path = logistic_regression_path(X, Y, solver='lbfgs', Cs=10,\n                                   fit_intercept=True, max_iter=100,\n                                   tol=1e-4, verbose=0, multi_class='ovr')\n    assert_array_equal(path[0].shape, (2, 3))\n    assert_array_equal(path[1].shape, (10,))\n\n    # Test for case where classes are not equally distributed\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([0, 0, 1])\n    w = np.array([0.5, 0.5])\n    path = logistic_regression_path(X, Y, solver='lbfgs', Cs=10,\n                                   fit_intercept=True, max_iter=100,\n                                   tol=1e-4, verbose=0, multi_class='ovr')\n    assert_array_equal(path[0].shape, (2, 3))\n    assert_array_equal(path[1].shape, (10,))\n", "def test_logistic_regression_sample_weight_class_weight():\n    \"\"\"Test sample_weight and class_weight on a binary classification problem.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_classes=2,\n                               n_informative=5, n_redundant=3,\n                               random_state=0)\n\n    sample_weight = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0,\n                             9.0, 10.0])\n    class_weight = {0: 1.0, 1: 2.0}\n\n    clf = LogisticRegression(max_iter=100, tol=1e-4, C=1.0,\n                            fit_intercept=True, penalty='l1',\n                            class_weight=class_weight, solver='liblinear',\n                            random_state=0)\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    check_predictions(clf, X, y)\n\n    # Test that the sample_weight is correctly applied\n    assert_greater(clf.score(X, y, sample_weight=sample_weight), 0.5,\n                   \"Sample weights are not correctly applied\")\n\n    # Test that the class_weight is correctly applied\n    assert_greater(clf.score(X, y, sample_weight=sample_weight),\n                  0.7, \"Class weights are not correctly applied\")\n\n", "def test_logistic_regression_path_binary_multinomial():\n    \"\"\"Test the binary and multinomial paths in logistic_regression_path\"\"\"\n    # Test the binary path\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 0])\n    path = logistic_regression_path(X, y, Cs=10, solver='lbfgs')\n    coefs = path[0]\n    Cs = path[1]\n    n_iter = path[2]\n    assert_array_almost_equal(coefs[0, 0], 0)\n    assert_array_almost_equal(coefs[-1, 0], 0)\n    assert_array_almost_equal(coefs[0, 1], 0.009999999999999998)\n    assert_array_almost_equal(coefs[-1, 1], 0.009999999999999998)\n    assert_array_almost_equal(Cs, np.logspace(-4, 4, 10))\n    assert_array_equal(n_iter, np.array([100]))\n\n    # Test the multinomial path\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 1])\n    path = logistic_regression_path(X, y, Cs=10, solver='lbfgs',\n                                   multi_class='multinomial')\n    coefs = path[0]\n    Cs = path[1]\n    n_iter = path[2]\n    assert_array_almost_equal(coefs[0, 0], 0)\n    assert_array_almost_equal(coefs[-1, 0], 0)\n    assert_array_almost_equal(coefs[0, 1], 0.009999999999999998)\n    assert_array_almost_equal(coefs[-1, 1], 0.009999999999999998)\n    assert_array_almost_equal(Cs, np.logspace(-4, 4, 10))\n    assert_array_equal(n_iter, np.array([100]))\n", "def test_logistic_regression_cv_scoring():\n    \"\"\"Test LogisticRegressionCV with different scoring functions.\"\"\"\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    # Test the case where scoring is a callable\n    cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n    clf = LogisticRegressionCV(cv=cv, scoring=lambda y_test, y_pred: np.mean(y_test == y_pred))\n    clf.fit(X, y)\n\n    # Test the case where scoring is a string\n    cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n    clf = LogisticRegressionCV(cv=cv, scoring=\"accuracy\")\n    clf.fit(X, y)\n\n    # Test the case where scoring is None\n    cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n    clf = LogisticRegressionCV(cv=cv, scoring=None)\n    clf.fit(X, y)\n\n    # Test the case where the scoring function is not a string or a callable\n    cv = StratifiedKFold(y, n_folds=5, shuffle=True)\n    clf = LogisticRegressionCV(cv=cv, scoring=0)\n    assert_raise_message(ValueError, \"scoring must be a callable or a string\",\n                       clf.fit, X, y)\n"], "sample_230": ["    def test_to_python(self):\n        field = UUIDField()\n        self.assertIsNone(field.to_python(None))\n        self.assertEqual(field.to_python(''), '')\n        with self.assertRaises(ValidationError):\n            field.to_python('invalid')\n        self.assertEqual(field.to_python(uuid.UUID('123e4567-e89b-12d3-a456-426655440000')), '123e4567-e89b-12d3-a456-426655440000')\n", "    def test_init(self):\n        field = CharField()\n        self.assertIsNotNone(field.widget)\n        self.assertEqual(field.widget, TextInput)\n        self.assertIsNotNone(field.default_validators)\n        self.assertEqual(field.default_validators, [])\n", "    def test_valid_uuid(self):\n        field = UUIDField()\n        self.assertEqual(field.to_python('123e4567-e89b-12d3-a456-426655440000'), uuid.UUID('123e4567-e89b-12d3-a456-426655440000'))\n        self.assertEqual(field.to_python('123e4567-e89b-12d3-a456-426655440000'), uuid.UUID('123e4567-e89b-12d3-a456-426655440000'))\n", "    def test_json_field_without_encoder(self):\n        class MyForm(Form):\n            field = JSONField()\n\n        form = MyForm()\n        self.assertEqual(form.field.widget, Textarea)\n", "    def test_json_field_destructuring(self):\n        json_str = '{\"foo\": \"bar\", \"baz\": 42, \"qux\": [1, 2, 3]}'\n        field = JSONField(encoder=DjangoJSONEncoder)\n        self.assertEqual(field.to_python(json_str), json.loads(json_str))\n        self.assertEqual(field.to_python(InvalidJSONInput(json_str)), InvalidJSONInput(json_str))\n        self.assertEqual(field.to_python(\"not json\"), InvalidJSONInput(\"not json\"))\n"], "sample_58": ["def test_form_default_renderer(self):\n    form = Person()\n    self.assertEqual(form.renderer, get_default_renderer())\n    self.assertIsInstance(form.renderer, DjangoTemplates)\n\n    form.default_renderer = DjangoTemplates\n    form.renderer = None\n    self.assertIsInstance(form.renderer, DjangoTemplates)\n\n    form.renderer = 'InvalidRenderer'\n    self.assertIsInstance(form.renderer, DjangoTemplates)\n    self.assertIsInstance(form.renderer(), DjangoTemplates)\n", "    def test_bound_field_initial(self):\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        field = form['first_name']\n        self.assertEqual(field.initial, '')\n        self.assertEqual(field.value(), 'John')\n        self.assertEqual(field.value_from_datadict(form.data, form.files, form.add_prefix('first_name')), 'John')\n", "    def test_default_renderer(self):\n        form = BaseForm()\n        self.assertTrue(form.default_renderer is not None)\n        self.assertTrue(hasattr(form.default_renderer, '__call__'))\n", "    def test_default_renderer(self):\n        form = BaseForm()\n        self.assertIsNotNone(form.renderer)\n        self.assertIsInstance(form.renderer, MediaDefiningClass)\n", "    def test_form_clean_with_callable_validator(self):\n        form = Person(first_name='John', last_name='Doe', birthday='1990-01-01')\n        form.full_clean()\n        self.assertTrue(form.is_valid())\n"], "sample_376": ["    def stored_messages_count(self, storage, response):\n        return stored_cookie_messages_count(storage, response)\n", "    def test_encode_decode_with_safedata(self):\n        storage = self.get_storage()\n        message = Message(constants.INFO, mark_safe('Test <b>safedata</b>'))\n        encoded = storage._encode(message)\n        self.assertTrue(isinstance(storage._decode(encoded).message, SafeData))\n        self.assertEqual(storage._decode(encoded).message, mark_safe('Test <b>safedata</b>'))\n", "    def stored_messages_count(self, storage, response):\n        return stored_cookie_messages_count(storage, response)\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        messages = ['message1', 'message2']\n        encoded_data = storage._encode(messages + [CookieStorage.not_finished], encode_empty=True)\n        response = self.get_response()\n        set_cookie_data(storage, messages, encode_empty=True)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n        # check that the not_finished sentinel value is not included\n        self.assertNotIn(CookieStorage.not_finished, storage._decode(response.COOKIES[storage.cookie_name]))\n", "def test_update_cookie_removes_cookie(self):\n    \"\"\"\n    When a list of messages is empty, an update operation should result in\n    the cookie being deleted.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    self.assertEqual(storage._update_cookie('', response), None)\n"], "sample_384": ["    def test_prefetch_related(self):\n        # Create objects and related objects\n        food1 = Food.objects.create(name=\"Food1\")\n        food2 = Food.objects.create(name=\"Food2\")\n        individual1 = Individual.objects.create(food=food1, name=\"Individual1\")\n        individual2 = Individual.objects.create(food=food2, name=\"Individual2\")\n\n        # Prefetch related objects\n        individuals = Individual.objects.prefetch_related('food').all()\n\n        # Check that the related objects are prefetched\n        self.assertTrue(individuals[0].food)\n        self.assertTrue(individuals[1].food)\n\n        # Check that the related objects are the correct instances\n        self.assertEqual(individuals[0].food, food1)\n        self.assertEqual(individuals[1].food, food2)\n", "    def test_bulk_update_multiple_objects_returned(self):\n        note1 = Note.objects.create(note=\"note1\", misc=\"misc1\")\n        note2 = Note.objects.create(note=\"note1\", misc=\"misc2\")\n        note3 = Note.objects.create(note=\"note1\", misc=\"misc3\")\n\n        with self.assertRaises(Note.MultipleObjectsReturned):\n            Note.objects.filter(note=\"note1\").update(misc=\"new_misc\")\n\n        with self.assertRaises(Note.MultipleObjectsReturned):\n            Note.objects.filter(note=\"note1\").update(misc=\"new_misc\")\n\n        Note.objects.filter(note=\"note1\").bulk_update([note1, note2, note3], [\"misc\"], batch_size=1)\n\n        self.assertEqual(Note.objects.get(note=\"note1\").misc, \"new_misc\")\n        self.assertEqual(Note.objects.get(note=\"note1\", misc=\"misc2\").misc, \"new_misc\")\n        self.assertEqual(Note.objects.get(note=\"note1\", misc=\"misc3\").misc, \"new_misc\")\n", "    def setUpTestData(cls):\n        cls.raw_query = \"\"\"\n            SELECT \"raw_queryset_tests_rawqueryset_testmodel\".\"id\", \"raw_queryset_tests_rawqueryset_testmodel\".\"name\"\n            FROM \"raw_queryset_tests_rawqueryset_testmodel\"\n        \"\"\"\n        cls.model = RawQuerySetTestModel\n", "    def test_related_populator(self):\n        note = Note.objects.create(note=\"test\", misc=\"test\")\n        single_object = SingleObject.objects.create(note=note)\n        related_object = RelatedObject.objects.create(object=single_object)\n        self.assertEqual(RelatedObject.objects.get(id=related_object.id).object, single_object)\n        self.assertEqual(single_object.note, note)\n", "    def test_raw_queryset_iterator(self):\n        qs = Note.objects.raw(\"SELECT * FROM django_rawqueryset_tests_note\")\n        self.assertEqual(list(qs), self.notes)\n"], "sample_601": ["    def test_datetime_accessor_properties(self, field):\n        # Ensure that the field is accessed correctly for DataArrays with cftime\n        # datetimes\n        result = getattr(self.times_data.dt, field)\n        expected_result = getattr(self.times, field)\n        assert_array_equal(result, expected_result)\n\n        # Ensure that the field is accessed correctly for DataArrays with numpy\n        # datetimes\n        result = getattr(self.data.dt, field)\n        expected_result = getattr(self.times, field)\n        assert_array_equal(result, expected_result)\n", "    def test_properties(self, field):\n        # Test that the property is accessible and returns a DataArray\n        getattr(self.data.dt, field)\n        assert isinstance(getattr(self.data.dt, field), xr.DataArray)\n", "    def test_datetime_accessor_field(self, field):\n        # Test for different dtypes\n        for data in [self.data, self.times_data]:\n            with raise_if_dask_computes():\n                assert_equal(\n                    getattr(data.dt, field), getattr(self.times, field)\n                )\n\n        # Test with dask arrays\n        for data in [self.data, self.times_data]:\n            if not is_np_datetime_like(data.dtype):\n                with raise_if_dask_computes():\n                    assert_equal(\n                        getattr(data.dt, field), getattr(self.times, field)\n                    )\n", "    def test_timedelta_accessor_fields(self, field):\n        self._test_dt_accessor_fields(field)\n", "    def test_timedelta_accessor(self, field):\n        assert hasattr(self.data.dt, field)\n        assert hasattr(self.times_data.dt, field)\n        assert_equal(self.data.dt[field], self.data.dt[field].compute())\n        assert_equal(self.times_data.dt[field], self.times_data.dt[field].compute())\n\n        # Test setting and getting\n        if field == \"days\":\n            value = 2\n        else:\n            value = 1\n        if field == \"seconds\":\n            value = 60\n        elif field == \"microseconds\":\n            value = 1000\n        elif field == \"nanoseconds\":\n            value = 1000000\n\n        self.data_dt = self.data.dt[field]\n        self.data_dt.values[:] = value\n        assert self.data.dt[field] == value\n\n        if field == \"days\":\n            value = 2\n        else:\n            value = 1\n        self.times_data_dt = self.times_data.dt[field]\n        self.times_data_dt.values[:] = value\n        assert self.times_data.dt[field] == value\n\n        # Test the CombinedDatetimelikeAccessor\n        combined_accessor = DatetimeAccessor(self.data)\n        assert not hasattr(combined_accessor, field)\n        combined_accessor = CombinedDatetimelikeAccessor(self.data)\n        assert hasattr(combined_accessor, field)\n        assert_equal(combined_accessor[field], self.data.dt[field])\n"], "sample_551": ["def test_text3d_with_zdir_none():\n    fig, ax = plt.subplots()\n    ax.text(0.5, 0.5, 'Hello, world!', ha='center', va='center', z=0, zdir=None)\n    ax.legend(['Hello, world!'])\n    plt.show()\n", "def test_line3d_properties():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.linspace(0, 10, 100)\n    y = np.linspace(0, 10, 100)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(np.sqrt(X**2 + Y**2))\n    ax.plot_surface(X, Y, Z)\n\n    line = art3d.Line3D([1, 2, 3, 4], [4, 3, 2, 1], [0, 0, 0, 0])\n    ax.add_line(line)\n\n    line_3d = art3d.line_2d_to_3d(line, zs=5, zdir='z')\n    ax.add_line(line_3d)\n\n    plt.show()\n", "def test_line3d_projection():\n    # Create a figure and axis\n    fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n    \n    # Define some data\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    z = np.cos(x)\n    \n    # Plot the data as a 3D line\n    line, = ax.plot(x, y, z)\n    \n    # Convert the line to a 3D line\n    art3d.line_2d_to_3d(line)\n    \n    # Check that the line was correctly converted\n    assert line.__class__ == art3d.Line3D\n    \n    # Check that the z position was correctly set\n    assert line.get_zdata()[0] == 0\n    \n    # Check that the zdir was correctly set\n    assert line.get_zdir() == 'z'\n    \n    # Test that the line can be projected onto the 3D axes\n    line.draw(ax.get_proj())\n    \n    # Check that the line was projected correctly\n    assert np.allclose(line.get_xdata(), np.sin(x))\n    assert np.allclose(line.get_ydata(), x)\n    assert np.allclose(line.get_zdata(), np.cos(x))\n", "def test_poly_collection_zorder():\n    # Test that Poly3DCollection sorts by the specified z-order method.\n    fig, ax = plt.subplots(projection='3d')\n    ax.bar3d([0], [0], [0], [1], [1], [1], color='b', alpha=0.5)\n    ax.bar3d([0], [0], [2], [1], [1], [1], color='g', alpha=0.5)\n    poly = art3d.Poly3DCollection([[([0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0])]], zsort='max')\n    poly.set_facecolor('r')\n    ax.add_collection3d(poly)\n\n    # Test that the z-order is correctly set to the maximum z value\n    z_order = ax.get_zorder()\n    assert z_order == 2\n\n    # Test that the z-order is correctly set to the minimum z value\n    poly.set_zsort('min')\n    z_order = ax.get_zorder()\n    assert z_order == 0\n\n    # Test that the z-order is correctly set to the average z value\n    poly.set_zsort('average')\n    z_order = ax.get_zorder()\n    assert z_order == 1\n\n    # Test that the z-order is correctly set to None\n    poly.set_zsort(None)\n    z_order = ax.get_zorder()\n    assert z_order == None\n", "def test_patch_3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.plot([0, 1], [0, 1], [0, 1])\n    ax.plot([1, 0], [0, 1], [0, 1])\n    ax.plot([1, 0], [1, 0], [0, 1])\n    ax.plot([0, 1], [1, 0], [0, 1])\n\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n\n    # Create a square patch with some rotation.\n    p = art3d.Patch3D(patch_type='square', facecolor='blue', edgecolor='black')\n    p.set_3d_properties([0, 1, 1], zdir='x')\n    ax.add_patch(p)\n\n    # Create a circle patch\n    c = art3d.Cone(x=1, y=0, z=0, ax=ax, theta=0, r=0.5, z_r=1)\n    c.set_3d_properties([1, 0, 1], zdir='x')\n    ax.add_patch(c)\n\n    # Test that the patches are correctly placed in the z-order.\n    ax.set_zorder(1)\n    ax.plot([0, 1], [0, 1], [0, 1], zorder=0)\n    ax.plot([1, 0], [0, 1], [0, 1], zorder=2)\n    ax.plot([1, 0], [1, 0], [0, 1], zorder=3)\n    ax.plot([0, 1], [1, 0], [0, 1], zorder=4)\n\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    ax.set_zlim(-1, 2)\n\n    plt.show()\n"], "sample_1128": ["def test_locatenew_set_pos():\n    N = ReferenceFrame('N')\n    P1 = Point('P1')\n    P2 = P1.locatenew('P2', 10 * N.x)\n    assert P2.pos_from(P1) == 10*N.x\n    assert P1.pos_from(P2) == -10*N.x\n    P1.set_pos(P2, 5 * N.x)\n    assert P2.pos_from(P1) == 5*N.x\n    assert P1.pos_from(P2) == -5*N.x\n", "def test_partial_velocity_frame_mismatch():\n    from sympy import symbols\n    from sympy.physics.vector import ReferenceFrame, Point, dynamicsymbols\n\n    t = symbols('t')\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    p = Point('p')\n    q, qd = dynamicsymbols('q q\\'')\n    u1, u2 = dynamicsymbols('u1 u2')\n\n    p.set_vel(N, qd * N.x + u1 * A.y)\n    p.set_vel(A, q * A.x + u2 * A.y)\n\n    with raises(ValueError):\n        p.partial_velocity(N, u1, u2)\n", "def test_partial_velocity_on_nonexistent_frame():\n    \"\"\"Test the partial velocity method when the velocity has not been defined\n    in the frame.\"\"\"\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    p = Point('p')\n    u1, u2 = dynamicsymbols('u1, u2')\n    p.set_vel(N, u1 * N.x + u2 * A.y)\n    raises(ValueError, lambda: p.partial_velocity(N, u1))\n", "def test_partial_velocity_partial_velocities():\n    N = ReferenceFrame('N')\n    A = ReferenceFrame('A')\n    p = Point('p')\n    u1, u2 = dynamicsymbols('u1, u2')\n    q, qd = dynamicsymbols('q q\\'')\n\n    p.set_vel(N, u1 * N.x + u2 * A.y)\n    p2 = Point('p2')\n    p2.set_pos(p, q * N.x)\n    p.set_pos(p2, qd * A.y)\n\n    assert p.partial_velocity(N, u1) == N.x\n    assert p.partial_velocity(N, u1, u2) == (N.x, A.y)\n    assert p.partial_velocity(N, q) == Vector(0)\n    assert p.partial_velocity(N, q, u1) == N.x\n    assert p.partial_velocity(N, q, u2) == Vector(0)\n    assert p.partial_velocity(N, qd) == A.y\n    assert p.partial_velocity(N, q, qd) == Vector(0)\n    assert p.partial_velocity(N, u1, q) == Vector(0)\n    assert p.partial_velocity(N, u1, qd) == Vector(0)\n    assert p.partial_velocity(N, u2, q) == Vector(0)\n    assert p.partial_velocity(N, u2, qd) == Vector(0)\n    assert p.partial_velocity(N, u1, u2) == (N.x, A.y)\n", "def test_invalid_frames():\n    q, qd = dynamicsymbols('q q\\'')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = Point('P')\n    P.set_vel(B, qd * B.x)\n\n    # Test with invalid frame in v1pt_theory\n    raises(TypeError, lambda: P.v1pt_theory(O, 'invalid_frame', B))\n    raises(TypeError, lambda: P.v1pt_theory(O, N, 'invalid_frame'))\n\n    # Test with invalid frame in v2pt_theory\n    raises(TypeError, lambda: P.v2pt_theory(O, 'invalid_frame', B))\n    raises(TypeError, lambda: P.v2pt_theory(O, B, 'invalid_frame'))\n\n    # Test with invalid frame in v1pt_theory and v2pt_theory\n    raises(TypeError, lambda: P.v1pt_theory(O, 'invalid_frame', B))\n    raises(TypeError, lambda: P.v2pt_theory(O, 'invalid_frame', B))\n"], "sample_646": ["def test_is_skipped():\n    class TestClass:\n        __unittest_skip__ = True\n\n    assert _is_skipped(TestClass) is True\n\n    class TestClass2:\n        pass\n\n    assert _is_skipped(TestClass2) is False\n\n    class TestClass3:\n        __unittest_skip__ = False\n\n    assert _is_skipped(TestClass3) is False\n", "def test_make_xunit_fixture_scope():\n    \"\"\"Test that the scope parameter of _make_xunit_fixture is passed correctly.\"\"\"\n    class TestClass:\n            pass\n\n    fixture = _make_xunit_fixture(TestClass, \"setup\", \"teardown\", None, scope=Scope.Function, pass_self=True)\n    assert fixture.scope == Scope.Function\n", "def test_runtest_make_xunit_fixture():\n    # Arrange\n    class TestClass:\n        @pytest.fixture(autouse=True, scope=\"class\")\n            pass\n\n        @pytest.fixture(autouse=True, scope=\"function\")\n            pass\n\n    # Act\n    pytest_pycollect_makeitem = pytest_pycollect_makeitem\n    pytest_pycollect_makeitem = lambda *args: UnitTestCase.from_parent(None, *args)\n\n    item = UnitTestCase.from_parent(None, name=\"test_method\", obj=TestClass)\n\n    # Assert\n    fixture = _make_xunit_fixture(TestClass, \"setUpClass\", \"tearDownClass\", None, Scope.Class, False)\n    assert fixture is not None\n    assert fixture.__name__ == \"_unittest_setupClass_fixture_TestClass\"\n\n    fixture = _make_xunit_fixture(TestClass, \"setup_method\", \"teardown_method\", None, Scope.Function, True)\n    assert fixture is not None\n    assert fixture.__name__ == \"_unittest_setup_method_fixture_TestClass\"\n\n    pytest_pycollect_makeitem = pytest_pycollect_makeitem  # Restore the original function\n", "def test_collect_fixtures_in_multiple_modules(tmpdir):\n    # Create two modules with test cases\n    mod1 = tmpdir / \"mod1.py\"\n    mod1.write_text(\n        \"\"\"\n        import unittest\n\n        class TestMod1(unittest.TestCase):\n            @pytest.mark.parametrize(\"x\", [1, 2, 3])\n                pass\n        \"\"\"\n    )\n    mod2 = tmpdir / \"mod2.py\"\n    mod2.write_text(\n        \"\"\"\n        import unittest\n\n        class TestMod2(unittest.TestCase):\n                pass\n        \"\"\"\n    )\n\n    # Run pytest and collect fixtures\n    pytester = Pytester()\n    pytester.makepyfile(pyfile=mod1)\n    pytester.makepyfile(pyfile=mod2)\n    pytester.runpytest_inprocess()\n\n    # Check that fixtures are collected correctly\n    collector = pytester.get_namespace_runner().walker.items[0]\n    assert len(collector.items) == 3\n    assert all(isinstance(item, pytest.Function) for item in collector.items)\n", "def test_collect_testcase_no_test_method(capfd):\n    \"\"\"Verify that a testcase without any test methods is not collected.\"\"\"\n    result = pytester.runpytest(\"-v\", \"-k\", \"UnitTestCaseWithoutMethods\")\n    result.assert_outcomes(1)\n    result.stdout.received.output\n    assert \"no tests run\" in result.stdout\n"], "sample_166": ["    def test_salted_hmac_invalid_algorithm(self):\n        with self.assertRaises(InvalidAlgorithm):\n            salted_hmac('test_salt', 'test_value', algorithm='sha256')\n", "    def test_salted_hmac_invalid_algorithm(self):\n        with self.assertRaises(InvalidAlgorithm):\n            salted_hmac(key_salt='secret', value='value', algorithm='md5')\n", "    def test_salted_hmac(self):\n        # Test with valid algorithm\n        key_salt = 'test_key_salt'\n        secret = 'test_secret'\n        value = 'test_value'\n        expected_hash = hmac.new(\n            hashlib.sha1(key_salt.encode() + secret.encode()).digest(),\n            msg=value.encode(),\n            digestmod=hashlib.sha1\n        ).hexdigest()\n        self.assertEqual(salted_hmac(key_salt, value, secret, algorithm='sha1').hexdigest(), expected_hash)\n\n        # Test with invalid algorithm\n        with self.assertRaises(InvalidAlgorithm):\n            salted_hmac(key_salt, value, secret, algorithm='invalid_algorithm')\n", "    def test_pbkdf2_defaults(self):\n        password = 'mysecretpassword'\n        salt = 'mysalt'\n        iterations = 1000\n        self.assertIsNotNone(pbkdf2(password, salt, iterations))\n        self.assertEqual(pbkdf2(password, salt, iterations).digest()[:16], pbkdf2(password, salt, iterations).digest()[-16:])\n        self.assertEqual(pbkdf2(password, salt, iterations, dklen=20).digest(), pbkdf2(password, salt, iterations, dklen=20).digest()[:20])\n", "    def test_salted_hmac_different_algorithms(self):\n        # Test with different algorithms\n        salted_value_sha1 = salted_hmac('salt', 'value', algorithm='sha1')\n        salted_value_sha256 = salted_hmac('salt', 'value', algorithm='sha256')\n        self.assertNotEqual(salted_value_sha1, salted_value_sha256)\n"], "sample_750": ["def test_omp_cv_default():\n    X, y = np.array([[1, 2], [3, 4]]), np.array([1, 2])\n    estimator = OrthogonalMatchingPursuitCV()\n    estimator.fit(X, y)\n    assert_array_almost_equal(estimator.coef_, np.array([1.0, 1.0]))\n", "def test_orthogonal_mp_tol_zero():\n    X, y = np.array([[1, 2], [3, 4]]), np.array([5, 6])\n    with assert_warns(UserWarning):\n        orthogonal_mp(X, y, n_nonzero_coefs=None, tol=0.0)\n", "def test_omp_path_residues():\n    rng = check_random_state(0)\n    X = rng.rand(10, 30)\n    y = rng.rand(10)\n    test_X = rng.rand(5, 30)\n    test_y = rng.rand(5)\n    resid = _omp_path_residues(X, y, test_X, test_y, copy=False)\n    assert resid.shape == (5, 30)\n\n", "def test_orthogonal_mp_gram_nan():\n    n_samples, n_features, n_nonzero_coefs, n_targets = 20, 30, 5, 3\n    y, X, gamma = make_sparse_coded_signal(n_targets, n_features, n_samples,\n                                           n_nonzero_coefs, random_state=0)\n    # Make X not of norm 1 for testing\n    X *= 10\n    y *= 10\n    G, Xy = np.dot(X.T, X), np.dot(X.T, y)\n\n    # Make some entries of G NaN\n    G[0, 0] = np.nan\n    G[1, 1] = np.nan\n\n    # This should not raise a warning\n    try:\n        orthogonal_mp_gram(G, Xy, copy_Gram=True, copy_Xy=True)\n    except AssertionError:\n        assert False, \"Should not raise an assertion error\"\n\n    # But should raise a warning when not copying\n    with ignore_warnings(UserWarning):\n        orthogonal_mp_gram(G, Xy, copy_Gram=False, copy_Xy=False)\n\n", "def test_orthogonal_mp_tol():\n    # We use a design matrix X with n_samples = n_features\n    n_samples, n_features = 20, 10\n    X = np.random.rand(n_samples, n_features)\n    y = np.random.rand(n_samples)\n\n    # This will fail because tol cannot be None and n_nonzero_coefs is None\n    assert_raises(ValueError, orthogonal_mp, X, y, tol=1.0, n_nonzero_coefs=None)\n\n    # This will succeed\n    coef, _ = orthogonal_mp(X, y, tol=1.0)\n\n    # Test the case where tol is set to a very small value\n    coef, _ = orthogonal_mp(X, y, tol=1e-12, copy_X=False)\n    assert_array_almost_equal(coef, np.zeros_like(coef))\n\n    # Test that orthogonal_mp raises an error if tol is too large\n    X = np.array([[1.0, 2.0], [3.0, 4.0]])\n    y = np.array([1.0, 1.0])\n    assert_raises(ValueError, orthogonal_mp, X, y, tol=np.inf, n_nonzero_coefs=1)\n\n    # Test the case where n_nonzero_coefs is set to a very large value\n    coef, _ = orthogonal_mp(X, y, n_nonzero_coefs=100)\n    assert_array_almost_equal(coef, np.zeros_like(coef))\n\n    # Test the case where n_nonzero_coefs is set to a value larger than n_features\n    assert_raises(ValueError, orthogonal_mp, X, y, n_nonzero_coefs=n_features + 1)\n"], "sample_1003": ["    def test_domain_preprocess_ZZ(self):\n        options = Options((x, y, z), {'domain': 'ZZ'})\n        assert options['domain'] == ZZ\n", "def test_build_options_invalid_args():\n    \"\"\"Check that ``build_options`` handles invalid arguments correctly. \"\"\"\n    from sympy.polys.polyoptions import build_options\n\n    # Test invalid keyword argument count\n    with raises(ValueError):\n        build_options((x, y), {'opt': Options})\n\n    # Test invalid keyword argument name\n    with raises(ValueError):\n        build_options((x, y), {'opt': Options, 'x': True})\n\n    # Test invalid number of arguments\n    with raises(TypeError):\n        build_options((x, y, z, z, z), {'opt': Options})\n\n    # Test that ``build_options`` returns the correct options\n    options = build_options((x, y), {'opt': Options})\n    assert options.get('expand', False) is True\n    assert options.get('gens') == (x, y)\n", "def test_order_option():\n    options = Options((x, y, z), {'order': 'lex', 'domain': ZZ})\n    assert options['order'] == lex\n    assert options['domain'] == ZZ\n", "    def test_options_class_methods(self):\n        from sympy.polys.polyoptions import Options\n\n        # Test clone method\n        args = {'expand': True, 'domain': ZZ}\n        options = Options((x, y), args)\n        new_options = options.clone({'expand': False})\n        self.assertEqual(new_options, {'expand': False, 'domain': ZZ})\n\n        # Test clone method with no updates\n        new_options = options.clone()\n        self.assertEqual(new_options, args)\n\n        # Test __setattr__ method\n        options = Options((x, y), {'expand': True})\n        options.foo = 'bar'\n        self.assertEqual(options.foo, 'bar')\n        self.assertEqual(options, {'expand': True, 'gens': (x, y), 'foo': 'bar'})\n\n        # Test args property\n        options = Options((x, y), {'expand': True})\n        self.assertEqual(options.args, {'expand': True})\n\n        # Test options property\n        options = Options((x, y), {'expand': True})\n        self.assertEqual(options.options, {'expand': True})\n\n        # Test flags property\n        options = Options((x, y), {'expand': True, 'auto': True})\n        self.assertEqual(options.flags, {'auto': True})\n\n        # Test clone method with invalid key\n        options = Options((x, y), {'expand': True})\n        with self.assertRaises(AttributeError):\n            options.clone({'invalid_key': True})\n", "def test_build_options():\n    from sympy.polys.polyoptions import build_options\n\n    # Test with multiple options\n    args = {\n        'domain': ZZ,\n        'order': lex,\n        'expand': False,\n        'gens': (x, y, z),\n        'field': True,\n        'greedy': True,\n        'split': False,\n        'gaussian': False,\n        'extension': set([I]),\n        'modulus': 10,\n        'symmetric': True,\n        'strict': False,\n        'auto': False,\n        'frac': True,\n        'formal': False,\n        'polys': True,\n        'include': True,\n        'all': False,\n        'gen': 0,\n        'series': False,\n        'symbols': numbered_symbols('s', start=1),\n        'method': 'sqrt'\n    }\n    options = build_options(args)\n    assert options == args\n\n    # Test with single options\n    options = build_options({'domain': ZZ})\n    assert options == {'domain': ZZ}\n\n    # Test with no options\n    options = build_options()\n    assert options == {'gens': (), 'auto': True}\n\n    # Test with invalid options\n    raises(OptionError, build_options, {'gens': (x, y, z), 'opt': 10})\n    raises(OptionError, build_options, (x, y, z, {'opt': 10}))\n"], "sample_371": ["    def setUp(self):\n        self.request = HttpRequest()\n        self.request.META = {'HTTP_HOST': 'example.com'}\n        self.request.user = User()\n\n        self.sensitive_filter = SafeExceptionReporterFilter()\n        self.default_filter = get_default_exception_reporter_filter()\n", "    def test_sensitive_post_parameters(self):\n        filter = SafeExceptionReporterFilter()\n        request = HttpRequest()\n        request.POST = {'key': 'value'}\n        filter.get_post_parameters(request)\n        self.assertIn('key', request.POST)\n        self.assertEqual(request.POST['key'], 'value')\n\n        filter.sensitive_post_parameters = ['key']\n        filter.get_post_parameters(request)\n        self.assertNotIn('key', request.POST)\n        self.assertEqual(request.POST['key'], filter.cleansed_substitute)\n", "    def test_cleanses_sensitive_keys(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = {\n            'SECRET_KEY': '1234567890',\n            'DEBUG': True,\n            'ALLOWED_HOSTS': ['localhost']\n        }\n        expected = {\n            'SECRET_KEY': filter.cleansed_substitute,\n            'DEBUG': True,\n            'ALLOWED_HOSTS': filter.cleansed_substitute\n        }\n        self.assertEqual(filter.get_safe_settings(), expected)\n", "    def test_exception_reporter_filter(self):\n        \"\"\"Test exception reporter filter\"\"\"\n        request = HttpRequest()\n        request.exception_reporter_filter = ExceptionReporterFilter()\n        reporter = ExceptionReporter(request, Exception, Exception(), None, False)\n        self.assertIsInstance(reporter.filter, ExceptionReporterFilter)\n", "    def test_technical_500_response(self):\n        request = HttpRequest()\n        with self.assertRaises(HttpResponse):\n            technical_500_response(request, Exception, Exception(), None, status_code=500)\n"], "sample_256": ["    def test_admin_password_change_form_invalid_password(self):\n        form = AdminPasswordChangeForm(user=self.u1)\n        form.cleaned_data = {'password1': 'weak', 'password2': 'weak'}\n        with self.assertRaises(ValidationError):\n            form.clean_password2()\n", "    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashField()\n        self.assertFalse(form.widget.read_only)\n        self.assertIsInstance(form.widget, ReadOnlyPasswordHashWidget)\n        self.assertEqual(form.widget.template_name, 'auth/widgets/read_only_password_hash.html')\n", "    def test_read_only_password_hash_field(self):\n        form = UserChangeForm(instance=User())\n        password_hash_field = form.fields['password']\n        self.assertIsInstance(password_hash_field, ReadOnlyPasswordHashField)\n", "    def test_read_only_password_hash_field(self):\n        form = ReadOnlyPasswordHashField()\n        self.assertTrue(form.read_only)\n        self.assertIsInstance(form.widget, ReadOnlyPasswordHashWidget)\n", "    def test_password_field_disabled_and_required_false(self):\n        form = UserChangeForm(instance=self.u1)\n        self.assertFalse(form.fields['password'].disabled)\n        self.assertTrue(form.fields['password'].required)\n"], "sample_925": ["def test_mocked_module_repr():\n    \"\"\"Test the repr of a mocked module.\"\"\"\n    with mock(['target.module.name']):\n        module = import_module('target.module.name')\n        assert repr(module) == 'target.module.name'\n", "def test_MockLoader_create_module():\n    with mock(['test_module']):\n        spec = ModuleSpec('test_module', MockLoader(MockFinder(['test_module'])))\n        module = MockLoader(MockFinder(['test_module'])).create_module(spec)\n        assert isinstance(module, _MockModule)\n        assert module.__name__ == 'test_module'\n", "def test_mock_object():\n    \"\"\"Test _MockObject behavior.\"\"\"\n    mock_obj = _MockObject()\n    assert mock_obj.__display_name__ == '_MockObject'\n    assert mock_obj.__sphinx_mock__ is True\n    assert isinstance(mock_obj, _MockObject)\n    assert len(mock_obj) == 0\n    assert not mock_obj.__contains__('key')\n    assert isinstance(mock_obj.__iter__(), Iterator)\n    assert mock_obj.__mro_entries__((object,)) == (_MockObject,)\n    mock_obj_list = mock_obj.__getitem__('key')\n    assert isinstance(mock_obj_list, _MockObject)\n    assert mock_obj.__getattr__('key') == mock_obj_list\n    assert mock_obj.__call__('func') == mock_obj\n    assert mock_obj.__repr__() == '_MockObject'\n", "def test_mock_context_exit():\n    \"\"\"Test that mock context manager removes all mocked modules.\"\"\"\n    # Mock a module\n    with mock(['target.module.name']):\n        pass\n\n    # Check that the module was imported\n    target_module = import_module('target.module.name')\n    assert target_module.__name__ == 'target.module.name'\n\n    # Check that mock context was exited\n    assert 'target.module.name' not in sys.modules\n", "def test_make_subclass() -> None:\n    \"\"\"Test the _make_subclass function.\"\"\"\n    # Test when making a new class with no superclass\n    new_class = _make_subclass('NewClass', 'module.name')\n    assert isinstance(new_class, type)\n    assert new_class.__module__ == 'module.name'\n    assert new_class.__display_name__ == 'module.name.NewClass'\n\n    # Test when making a new class with a superclass\n    new_class = _make_subclass('NewSubclass', 'module.name', superclass=_MockObject)\n    assert isinstance(new_class, type)\n    assert new_class.__module__ == 'module.name'\n    assert new_class.__display_name__ == 'module.name.NewSubclass'\n\n    # Test when passing custom attributes to make_subclass\n    new_class = _make_subclass('NewClass', 'module.name', attributes={'attr1': 'value1'})\n    assert isinstance(new_class, type)\n    assert new_class.__module__ == 'module.name'\n    assert new_class.__display_name__ == 'module.name.NewClass'\n    assert hasattr(new_class, 'attr1')\n    assert new_class.attr1 == 'value1'\n"], "sample_644": ["    def test_check_reimport(self):\n        \"\"\"Check reimport.\"\"\"\n        self._run_test(\n            \"\"\"\n            from mymodule import foo\n            from mymodule import foo\n            \"\"\",\n            id=\"check_reimport\",\n            options={\"reimported\": True},\n            expected_messages=[MessageTest(\n                msg_id=\"reimported\",\n                line=2,\n                col=1,\n                args=(\"foo\", 1),\n            )],\n        )\n", "def test_imports_order_with_local_module_before_third_party_module(self: TestImportsChecker) -> None:\n    \"\"\"Check that local modules are imported after third-party modules.\"\"\"\n    source = \"\"\"", "    def test_check_position(self) -> None:\n        \"\"\"Test imports are correctly placed at the top of the module.\"\"\"\n        source = \"\"\"\n            a = 1\n            import b\n            \"\"\"\n        self.checker.add_option('analyze_fallback_blocks', True)\n        self._run_check(source, expected_messages=[])\n        self._run_check(source, expected_messages=[(\"wrong-import-position\", 3)], option_values={'import_graph': ''})\n\n        source = \"\"\"\n            a = 1\n            from b import c\n            \"\"\"\n        self.checker.add_option('analyze_fallback_blocks', True)\n        self._run_check(source, expected_messages=[])\n        self._run_check(source, expected_messages=[(\"wrong-import-position\", 3)], option_values={'import_graph': ''})\n\n        source = \"\"\"\n            a = 1\n            b = 2\n            import c\n            \"\"\"\n        self.checker.add_option('analyze_fallback_blocks', True)\n        self._run_check(source, expected_messages=[])\n        self._run_check(source, expected_messages=[(\"wrong-import-position\", 4)], option_values={'import_graph': ''})\n", "    def test_misplaced_future(self):\n        # Arrange\n        source = \"\"\"", "    def test_imports_order(self) -> None:\n        test = \"\"\"\n            import os\n            from .submodule import submodule_function\n            import pytest\n            from pytest import mark\n            from .another_submodule import another_submodule_function\n            from .local_module import local_function\n        \"\"\"\n        self.assertWARNINGS(\n            \"\"\"multiple-imports\n                 import-outside-toplevel (from local_module import local_function)\n                 import-outside-toplevel (from pytest import mark)\n                 wrong-import-order (from .submodule import submodule_function; 'os')\n                 wrong-import-order (from pytest import mark; 'os')\n                 wrong-import-order (from .another_submodule import another_submodule_function; 'os')\n                 ungrouped-imports (.submodule)\n                 ungrouped-imports (.another_submodule)\n                 ungrouped-imports (os)\n                 ungrouped-imports (pytest)\n                 ungrouped-imports (local_module)\n                 \"\"\",\n            test,\n            options={\"import-order\": \"F403\"},\n        )\n"], "sample_48": ["    def test_Avg_invalid_aggregate_in_aggregate(self):\n        with self.assertRaises(FieldError):\n            Book.objects.all().annotate(avg_rating=Avg(Avg('rating')))\n", "    def test_Aggregate_repr(self):\n        expression = Aggregate(expression=F('name'))\n        with self.assertRaises(TypeError):\n            expression.default_alias\n\n        expression = Aggregate(expression=F('name'), alias='custom_alias')\n        self.assertEqual(expression.default_alias, 'custom_alias')\n\n        expression = Aggregate(expression=F('name'), expressions=[F('name'), F('age')])\n        self.assertEqual(expression.default_alias, 'name__avg')\n", "    def test_Aggregate__complex_expression(self):\n        from .models import Book\n\n        books = Book.objects.all()\n        expression = When(\n            books.price < 30,\n            then=Avg('pages'),\n            else=Value(0),\n        )\n        result = books.annotate(avg_pages=expression).values_list('avg_pages', flat=True).get()\n        self.assertEqual(result, 0)\n", "    def test_Aggregate_resolve_expression(self):\n        # Test that Aggregate.resolve_expression() raises FieldError when used in UPDATE query.\n        with self.assertRaises(FieldError):\n            from django.db.models.sql.query import UpdateQuery\n            UpdateQuery.from_clauses([Case('CASE WHEN 1=1 THEN 1 ELSE 2 END')])\n            Aggregate('AVG', 'CASE WHEN 1=1 THEN 1 ELSE 2 END').resolve_expression(None)\n", "    def test_Aggregate_filter(self):\n        with CaptureQueriesContext(connection) as queries:\n            Author.objects.filter(age__gte=35).annotate(avg_age=Avg('age')).values_list('avg_age', flat=True)\n            self.assertEqual(queries.count(), 1)\n            self.assertIn('AVG(age)', queries[0].query)\n"], "sample_377": ["    def setUp(self):\n        self.request = RequestFactory().request\n        self.request.path = '/'\n        self.request.user = User()\n", "    def test_safe_request_meta(self):\n        request = HttpRequest()\n        request.META = {\"PATH_INFO\": \"/path\", \"QUERY_STRING\": \"key=value\"}\n        filter = SafeExceptionReporterFilter()\n        result = filter.get_safe_request_meta(request)\n        self.assertEqual(result, {\"PATH_INFO\": \"*********\", \"QUERY_STRING\": \"*********\"})\n        request.META[\"HTTP_COOKIE\"] = \"cookie=value\"\n        result = filter.get_safe_request_meta(request)\n        self.assertEqual(result, {\"PATH_INFO\": \"*********\", \"QUERY_STRING\": \"*********\", \"HTTP_COOKIE\": \"*********\"})\n        filter.hidden_settings = _lazy_re_compile(\"PATH|COOKIE\", flags=re.I)\n        result = filter.get_safe_request_meta(request)\n        self.assertEqual(result, {\"PATH_INFO\": \"*********\", \"QUERY_STRING\": \"*********\", \"HTTP_COOKIE\": \"cookie=value\"})\n", "    def test_is_active(self):\n        with self.settings(DEBUG=True):\n            reporter = SafeExceptionReporterFilter()\n            self.assertFalse(reporter.is_active(None))\n        with self.settings(DEBUG=False):\n            reporter = SafeExceptionReporterFilter()\n            self.assertTrue(reporter.is_active(None))\n", "    def test_get_safe_settings(self):\n        request = HttpRequest()\n        with self.assertRaises(TypeError):\n            SafeExceptionReporterFilter().get_safe_settings()\n\n        request.META = {\"PATH_INFO\": \"/\"}\n        safe_settings = SafeExceptionReporterFilter().get_safe_settings()\n        self.assertIn(\"SETTING1\", safe_settings)\n        self.assertIn(\"SETTING2\", safe_settings)\n\n        # Test that only uppercase settings are included\n        request.META = {\"PATH_INFO\": \"/\"}\n        safe_settings = SafeExceptionReporterFilter().get_safe_settings()\n        self.assertNotIn(\"setting1\", safe_settings)\n        self.assertNotIn(\"setting2\", safe_settings)\n\n        # Test that sensitive settings are replaced with ***\n        request.META = {\"PATH_INFO\": \"/\", \"SENSITIVE_SETTING\": \"sensitive_value\"}\n        safe_settings = SafeExceptionReporterFilter().get_safe_settings()\n        self.assertNotEqual(\"sensitive_value\", safe_settings[\"SENSITIVE_SETTING\"])\n        self.assertEqual(\"*********\", safe_settings[\"SENSITIVE_SETTING\"])\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        with override_settings(DEBUG=True):\n            self.assertEqual(filter.get_safe_settings(), settings.__dict__)\n"], "sample_537": ["def test_stride_windows_overlap(noverlap):\n    x = np.arange(10)\n    result = mlab.stride_windows(x, n=3, noverlap=noverlap)\n    target = mlab.calc_window_target(x, 3, noverlap)\n    assert_array_equal(result, target)\n", "    def test_detrend_mean(self):\n        # Test that detrend_mean removes the mean\n        x = np.array([1, 2, 3, 4, 5])\n        expected = np.array([-1.5, -0.5, 0.5, 1.5, 2.5])\n        assert_array_almost_equal(detrend(x, key='mean'), expected)\n", "    def set_deprecated_warnings(self):\n        with _api.suppress_matplotlib_deprecation_warning():\n            yield\n", "def test_stride_windows_noverlap(noverlap, expected):\n    x = np.arange(10)\n    result = mlab.stride_windows(x, 5, noverlap=noverlap, axis=0)\n    assert_array_equal(result, expected)\n", "    def test_init(self):\n        # Test that an instance of GaussianKDE can be created with a dataset\n        # and no bandwidth method\n        kde = GaussianKDE(np.array([[1, 2], [3, 4]]))\n\n        # Test that the initialization attributes are set correctly\n        assert kde.dataset.shape == (2, 2)\n        assert kde.dim == 2\n        assert kde.num_dp == 2\n"], "sample_820": ["def test_voting_classifier_weighted():\n    \"\"\"Test weighted voting classifier.\"\"\"\n    X, y = iris.data[:, 1:3], iris.target\n    clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                             random_state=1)\n    clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n    clf3 = GaussianNB()\n\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,1,1])\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n    eclf1.fit(X_train, y_train)\n    pred = eclf1.predict(X_test)\n    assert_array_equal(pred, y_test)\n", "def test_VotingClassifier_predict_proba():\n    X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n    clf1 = LogisticRegression()\n    clf2 = RandomForestClassifier(n_estimators=5)\n    eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2)], voting='soft')\n    eclf.fit(X, y)\n    probas = eclf.predict_proba(X)\n    assert probas.shape == (6, 2)\n    assert_almost_equal(probas.sum(axis=1), np.ones(6))\n    assert_almost_equal(probas, (probas.T / probas.sum(axis=1)).T)\n", "def test_voting_regressor_weights():\n    \"\"\"Test VotingRegressor with non-uniform weights.\"\"\"\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([2, 6, 12, 20, 30, 42])\n    r1 = LinearRegression()\n    r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n    er = VotingRegressor([('lr', r1), ('rf', r2)], weights=[0.7, 0.3])\n    er.fit(X, y)\n    predicted = er.predict(X)\n    assert_array_almost_equal(predicted, [3.3, 5.7, 11.8, 19.7, 28., 40.3])\n\n    # Test that weights are used correctly when fitting\n    r1.fit(X, y)\n    r2.fit(X, y)\n    assert r1.score(X, y) != er.score(X, y)\n\n    # Test that weights are used correctly when predicting\n    assert_array_almost_equal(er.predict(X), [3.3, 5.7, 11.8, 19.7, 28., 40.3])\n    weights = [0.7, 0.3]\n    predictions = [0.7 * r1.predict(X)[0] + 0.3 * r2.predict(X)[0],\n                  0.7 * r1.predict(X)[1] + 0.3 * r2.predict(X)[1],\n                  0.7 * r1.predict(X)[2] + 0.3 * r2.predict(X)[2],\n                  0.7 * r1.predict(X)[3] + 0.3 * r2.predict(X)[3],\n                  0.7 * r1.predict(X)[4] + 0.3 * r2.predict(X)[4],\n                  0.7 * r1.predict(X)[5] + 0.3 * r2.predict(X)[5]]\n    assert_array_almost_equal(predicted, predictions)\n\n", "def test_VotingRegressor_copy_weights():\n    \"\"\"Test that VotingRegressor copies the weights properly.\"\"\"\n    # Create a VotingRegressor with weights\n    rf = RandomForestRegressor(n_estimators=10, random_state=1)\n    svr = SVC(kernel='linear', probability=True)\n    regressor = VotingRegressor([('rf', rf), ('svr', svr)], weights=[1, 1])\n    # Check that the weights are copied correctly\n    regressor.fit([[1], [2]], [1, 2])\n    assert regressor.weights is not None\n    assert regressor._weights_not_none is not None\n    assert regressor._weights_not_none == regressor.weights\n", "def test_VotingRegressor_weights():\n    \"\"\"Test VotingRegressor weights\"\"\"\n    X, y = iris.data[:, 1:3], iris.target\n\n    # Test with a list of lists\n    weights = [[0.4, 0.6], [0.3, 0.7]]\n    voting_regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=weights)\n    voting_regressor.fit(X, y)\n    assert_array_almost_equal(voting_regressor.predict(X), np.array([3.1, 3.4, 3.8, 3.4, 3.6, 3.7]))\n\n    # Test with a 2D array\n    weights = np.array([[0.4, 0.6], [0.3, 0.7]])\n    voting_regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=weights)\n    voting_regressor.fit(X, y)\n    assert_array_almost_equal(voting_regressor.predict(X), np.array([3.1, 3.4, 3.8, 3.4, 3.6, 3.7]))\n\n    # Test with None\n    weights = None\n    voting_regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=weights)\n    voting_regressor.fit(X, y)\n    assert_array_almost_equal(voting_regressor.predict(X), np.array([3.0, 3.5, 4.0, 3.5, 4.0, 4.5]))\n\n    # Test with weights that don't sum to 1\n    weights = [1.0, 1.1]\n    voting_regressor = VotingRegressor([('lr', LinearRegression()), ('rf', RandomForestRegressor())], weights=weights)\n    with assert_raise_message(ValueError, \"weights must sum to 1\"):\n        voting_regressor.fit(X, y)\n"], "sample_88": ["    def test_default_encoding(self):\n        email = EmailMessage('subject', 'body')\n        message = email.message()\n        self.assertEqual(message['Content-Type'], 'text/plain; charset=utf-8')\n", "    def test_safe_mime_message_setitem(self):\n        \"\"\"\n        Test that setting a key-value pair on a SafeMIMEMessage instance with\n        an ASCII-only header value does not raise a BadHeaderError.\n        \"\"\"\n        msg = SafeMIMEMessage()\n        msg['X-Test'] = 'Hello, World!'\n        self.assertIsInstance(msg, SafeMIMEMessage)\n        self.assertIn('X-Test', msg)\n", "    def test_attach_file_encoding(self):\n        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False) as tmp:\n            tmp.write('Hello, World!')\n            tmp.close()\n            msg = EmailMessage(subject='Test', body='Hello', from_email='test@example.com', to=['test@example.com'])\n            msg.attach_file(tmp.name, mimetype='text/plain')\n            decoded_attachments = self.get_decoded_attachments(msg)\n            self.assertEqual(len(decoded_attachments), 1)\n            filename, content, mimetype = decoded_attachments[0]\n            self.assertEqual(filename, 'test')\n            self.assertEqual(mimetype, 'text/plain')\n            self.assertIsInstance(content, str)\n", "    def test_attachments_properly_encoded_as_string(self):\n        subject = 'Test subject'\n        body = 'Test body'\n        from_email = 'test@example.com'\n        to = ['recipient@example.com']\n        attachment_filename = 'test.txt'\n        attachment_content = b'Test content'\n\n        email_message = EmailMessage(subject, body, from_email, to)\n        email_message.attach(attachment_filename, attachment_content, 'text/plain')\n\n        message = email_message.message()\n\n        self.assertMessageHasHeaders(message, {\n            ('Content-Type', 'multipart/mixed; boundary=\"===============1234567890==)'),\n            ('Content-Disposition', 'multipart/mixed; boundary=\"===============1234567890==)'),\n            ('Subject', subject),\n            ('From', from_email),\n            ('To', ', '.join(to)),\n        })\n\n        attachments = email_message.get_attachments()\n        self.assertEqual(len(attachments), 1)\n        filename, content, mimetype = attachments[0]\n        self.assertEqual(filename, attachment_filename)\n        self.assertEqual(mimetype, 'text/plain')\n        self.assertEqual(content, attachment_content)\n", "    def test_attachment_filename_encoding(self):\n        \"\"\"\n        Test that attachment filenames are encoded correctly, even if they contain non-ASCII characters.\n        \"\"\"\n        message = EmailMessage(subject='Test Message', body='Hello, world!')\n        message.attachments = [('filename with \u00e4\u00f6\u00fc.pdf', b'content', 'application/pdf')]\n        encoded_message = message.message()\n        self.assertIsInstance(encoded_message.get_filename('attachment with \u00e4\u00f6\u00fc.pdf'), bytes)\n        self.assertEqual(encoded_message.get_filename('attachment with \u00e4\u00f6\u00fc.pdf').decode('utf-8'), 'attachment with \u00e4\u00f6\u00fc.pdf')\n"], "sample_287": ["    def test_save_as_boolean(self):\n        class MyAdmin(admin.ModelAdmin):\n            save_as = True\n        check = AdminChecks()\n        self.assertEqual(check.check(MyAdmin()), [])\n", "    def test_check_inline_model_admin_subclass(self):\n        from django.contrib.admin.options import InlineModelAdmin\n\n        class InlineSubclass(InlineModelAdmin):\n            parent_model = Album\n            model = Song\n\n        checks = InlineModelAdminChecks(InlineSubclass)\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 0)\n\n        InlineSubclass.model = None\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E106')\n\n        InlineSubclass.model = Book\n        InlineSubclass.parent_model = None\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E202')\n\n        InlineSubclass.parent_model = Album\n        InlineSubclass.fk_name = 'invalid_foo'\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E202')\n\n        InlineSubclass.fk_name = None\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 0)\n\n        InlineSubclass.formset = None\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E206')\n\n        class FormsetSubclass(BaseModelFormSet):\n            pass\n\n        InlineSubclass.formset = FormsetSubclass\n        errors = checks.check(InlineSubclass)\n        self.assertEqual(len(errors), 0)\n", "    def test_inline_max_num_with_min_num_and_extra(self):\n        # Check that max_num is validated when min_num is not None and extra is not None\n        class InlineModel(admin.StackedInline):\n            model = TwoAlbumFKAndAnE\n            fk_name = 'album'\n            extra = 2\n            max_num = 3\n            min_num = 1\n\n        checks = InlineModelChecks(InlineModel).check(InlineModel)\n        self.assertEqual(len(checks), 1)\n", "    def test_base_inline_formset(self):\n        class InlineFormsetInline(admin.StackedInline):\n            model = Song\n            formset = forms.modelformset_factory(Song, fields=('title',))\n        self.assertIsInstance(InlineFormsetInline.formset, admin.BaseInlineFormSet)\n        self.assertEqual(InlineFormsetInline.formset.Meta.model, Song)\n        self.assertEqual(InlineFormsetInline.formset.Meta.fields, ('title',))\n", "    def test_admin_site_with_no_models(self):\n        # Test that admin site with no registered models doesn't raise any errors.\n        class EmptyAdmin(admin.AdminSite):\n            pass\n\n        errors = EmptyAdmin.check(self)\n        self.assertEqual(errors, [])\n"], "sample_1187": ["def test_gradient_terms():\n    # Test that gradient_terms returns the correct terms\n    terms = gradient_terms(binomial_power=2, no_of_gens=2)\n    expected_terms = [[1, 0, 0, 0], [y, 0, 1, 0], [y**2, 0, 2, 0], [x, 1, 0, 0],\n                      [x*y, 1, 1, 0], [x**2, 2, 0, 0]]\n    assert terms == expected_terms\n\n    # Test that gradient_terms returns the correct terms for 3D case\n    terms = gradient_terms(binomial_power=2, no_of_gens=3)\n    expected_terms = [[[[[1, 0, 0, 0, 0, 0, 0, 0]]], [[[y, 0, 1, 0, 1, 0, 0, 0],\n    [z, 0, 0, 1, 1, 0, 1, 0]], [[x, 1, 0, 0, 1, 1, 0, 0]]],\n    [[[y**2, 0, 2, 0, 2, 0, 0, 0], [y*z, 0, 1, 1, 2, 0, 1, 0],\n    [z**2, 0, 0, 2, 2, 0, 2, 0]], [[x*y, 1, 1, 0, 2, 1, 0, 0],\n    [x*z, 1, 0, 1, 2, 1, 1, 0]], [[x**2, 2, 0, 0, 2, 2, 0, 0]]]]\n    assert terms == expected_terms\n", "def test_polytope_integrate_hermitian_polynomial():\n    x = Symbol('x')\n    y = Symbol('y')\n    # Define a Hermitian polynomial\n    hermitian_polynomial = x**2 + y**2 + 2*x*y\n\n    # Define the polytope\n    polytope = Polygon(Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1))\n\n    # Integrate the Hermitian polynomial over the polytope\n    result = polytope_integrate(polytope, hermitian_polynomial)\n\n    # Check if the result is equal to 1, which is the expected value\n    assert result == 1\n", "    def test_main_integrate3d_invalid_input(self):\n        with self.assertRaises(TypeError):\n            main_integrate3d(None, [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n                             [[1, 0, 0], -5, [0, 1, 0]])\n", "def test_polytope_integrate_three_dimensional_expression():\n    # Test to check if the polytope_integrate function works for 3D polytope\n    # and a trivariate expression.\n    from sympy import Point, Polygon\n    from sympy.abc import x, y, z\n    # Define a 3D polytope\n    polytope = Polygon(Point(0, 0, 0), Point(0, 0, 5), Point(0, 5, 0), Point(0, 5, 5), Point(5, 0, 0),\n                       Point(5, 0, 5), Point(5, 5, 0), Point(5, 5, 5))\n    # Define a trivariate expression\n    expr = x**3 + y**2 + z\n    # Check if the polytope_integrate function returns the correct result\n    result = polytope_integrate(polytope, expr)\n    assert result == {1: 3750, x: 250, y**2: 250, x**3: 250, z: 250}\n\n", "    def test_polytope_integrate_unknown_variable(self):\n        from sympy import Symbol\n        t = Symbol('t')\n        polys = [t**2, t**3]\n        expr = t**2 + 1\n        poly = Polygon(Point(0, 0), Point(1, 1), Point(1, 0))\n        self.assertEqual(polytope_integrate(poly, polys, max_degree=3), {1: 0, t**2: Rational(2, 3), t**3: Rational(1, 4)})\n"], "sample_259": ["    def test_prefetch_related_objects_select_related(self):\n        # Test prefetch_related_objects() with select_related()\n        authors = Author.objects.select_related('books').all()\n        prefetch_related_objects(authors, 'books__readers')\n\n        self.assertEqual(authors[0].books[0].readers[0].name, 'Amy')\n        self.assertEqual(authors[0].books[0].readers[1].name, 'Belinda')\n        self.assertEqual(authors[0].books[1].readers[0].name, 'Amy')\n        self.assertEqual(authors[0].books[1].readers[1].name, 'Belinda')\n\n        self.assertEqual(authors[1].books[0].readers[0].name, 'Amy')\n        self.assertEqual(authors[1].books[0].readers[1].name, 'Belinda')\n\n        self.assertEqual(authors[2].books[0].readers[0].name, 'Amy')\n        self.assertEqual(authors[2].books[0].readers[1].name, 'Belinda')\n\n        self.assertEqual(authors[3].books[0].readers[0].name, 'Amy')\n        self.assertEqual(authors[3].books[0].readers[1].name, 'Belinda')\n\n        self.assertEqual(authors[0].books[0].readers_count, 2)\n        self.assertEqual(authors[0].books[1].readers_count, 2)\n        self.assertEqual(authors[1].books[0].readers_count, 2)\n        self.assertEqual(authors[2].books[0].readers_count, 2)\n        self.assertEqual(authors[3].books[0].readers_count, 2)\n", "    def test_prefetch_related_objects_diamond_relationship(self):\n        book1 = Book.objects.create(title='Poems')\n        book2 = Book.objects.create(title='Jane Eyre')\n        book3 = Book.objects.create(title='Wuthering Heights')\n        book4 = Book.objects.create(title='Sense and Sensibility')\n\n        author1 = Author.objects.create(name='Charlotte', first_book=book1)\n        author2 = Author.objects.create(name='Anne', first_book=book1)\n        author3 = Author.objects.create(name='Emily', first_book=book1)\n        author4 = Author.objects.create(name='Jane', first_book=book4)\n\n        book1.authors.add(author1, author2, author3)\n        book2.authors.add(author1)\n        book3.authors.add(author3)\n        book4.authors.add(author4)\n\n        reader1 = Reader.objects.create(name='Amy')\n        reader2 = Reader.objects.create(name='Belinda')\n\n        reader1.books_read.add(book1, book4)\n        reader2.books_read.add(book2, book4)\n\n        prefetch_related_objects([book1], 'authors__books__readers')\n        prefetch_related_objects([author1], 'books__authors')\n        prefetch_related_objects([book1], 'authors__books__readers__books__authors')\n\n        self.assertEqual(author1.books.count(), 1)\n        self.assertEqual(author2.books.count(), 1)\n        self.assertEqual(author3.books.count(), 1)\n        self.assertEqual(author4.books.count(), 1)\n\n        self.assertEqual(book1.authors.count(), 3)\n        self.assertEqual(book2.authors.count(), 1)\n        self.assertEqual(book3.authors.count(), 1)\n        self.assertEqual(book4.authors.count(), 1)\n\n        self.assertEqual(reader1.books_read.count(), 2)\n        self.assertEqual(reader2.books_read.count(), 2)\n\n        self.assertEqual(reader1.books_read.first().authors.count(), 3)\n        self.assertEqual(reader2.books_read.first().authors.count(), 1)\n        self.assertEqual(reader1.books_read.last().authors.count(), 1)\n        self.assertEqual(reader2.books_read.last().authors.count(), 1)\n", "    def test_prefetch_related_objects_with_prefetch_through(self):\n        # Test prefetch_related_objects() with a prefetch through.\n        reader1 = Reader.objects.prefetch_related(Prefetch('books_read', Book.objects.all(), 'books')).get(name='Amy')\n        self.assertEqual(len(reader1.books_read), 2)\n        self.assertEqual(reader1.books_read[0].title, 'Poems')\n        self.assertEqual(reader1.books_read[1].title, 'Sense and Sensibility')\n", "    def test_prefetch_related_objects_with_prefetch(self):\n        prefetch = Prefetch('books__author', queryset=Author.objects.all())\n        authors = Author.objects.prefetch_related(prefetch).all()\n        books = authors[0].books.all()\n\n        # We can now access the prefetched related objects\n        self.assertEqual(books[0].author, authors[0])\n        self.assertEqual(books[1].author, authors[0])\n        self.assertEqual(books[2].author, authors[2])\n\n        # We can also access the prefetched related objects through the related manager\n        books = authors[0].books.all()\n        self.assertEqual(books[0].author, authors[0])\n        self.assertEqual(books[1].author, authors[0])\n        self.assertEqual(books[2].author, authors[2])\n", "def test_prefetch_related_objects_with_nested_relationships(self):\n    # Create the necessary data\n    author1 = Author.objects.create(name='Author 1')\n    book1 = Book.objects.create(title='Book 1', authors=[author1])\n    author2 = Author.objects.create(name='Author 2')\n    book2 = Book.objects.create(title='Book 2', authors=[author2])\n    book3 = Book.objects.create(title='Book 3')\n    book3.authors.add(author1)\n    book3.books.add(book1)\n\n    # Define the prefetches\n    prefetch1 = Prefetch('authors', to_attr='author')\n    prefetch2 = Prefetch('books', to_attr='book')\n\n    # Prefetch the objects\n    prefetch_related_objects([author1, author2], prefetch1, prefetch2)\n\n    # Check that the objects have the correct attributes\n    self.assertEqual(author1.author, author2)\n    self.assertEqual(author1.book, book1)\n    self.assertEqual(author2.author, None)\n    self.assertEqual(author2.book, None)\n    self.assertEqual(book1.author, author1)\n    self.assertEqual(book1.book, None)\n    self.assertEqual(book2.author, author2)\n    self.assertEqual(book2.book, None)\n    self.assertEqual(book3.author, author1)\n    self.assertEqual(book3.book, book1)\n\n    # Check that the objects have the correct attributes when using prefetch_related_objects with a list of objects\n    objects = [author1, author2, book1, book2, book3]\n    prefetch_related_objects(objects, prefetch1, prefetch2)\n    self.assertEqual(author1.author, author2)\n    self.assertEqual(author1.book, book1)\n    self.assertEqual(author2.author, None)\n    self.assertEqual(author2.book, None)\n    self.assertEqual(book1.author, author1)\n    self.assertEqual(book1.book, None)\n    self.assertEqual(book2.author, author2)\n    self.assertEqual(book2.book, None)\n    self.assertEqual(book3.author, author1)\n    self.assertEqual(book3.book, book1)\n\n    # Check that prefetch_related_objects works with an empty list of objects\n    objects = []\n    prefetch_related_objects(objects, prefetch1, prefetch2)\n    self.assertEqual(objects, [])\n\n    # Check that prefetch_related_objects works with a single object\n    objects = [author1]\n    prefetch_related_objects(objects, prefetch1, prefetch2)\n    self.assertEqual(author1.author, author2)\n    self.assertEqual(author1"], "sample_854": ["def test_get_liblinear_solver_type():\n    # Test with multi_class='crammer_singer'\n    assert _get_liblinear_solver_type('crammer_singer', 'l1', 'logistic_regression', False) == 4\n\n    # Test with multi_class='ovr'\n    assert _get_liblinear_solver_type('ovr', 'l2', 'hinge', True) == 3\n\n    # Test with unsupported loss function\n    with pytest.raises(ValueError):\n        _get_liblinear_solver_type('ovr', 'l2', 'unsupported_loss', True)\n\n    # Test with unsupported penalty\n    with pytest.raises(ValueError):\n        _get_liblinear_solver_type('ovr', 'unsupported_penalty', 'logistic_regression', True)\n\n    # Test with unsupported dual\n    with pytest.raises(ValueError):\n        _get_liblinear_solver_type('ovr', 'l2', 'logistic_regression', 'unsupported_dual')\n", "def test_LibSVM_sparse_predict_proba():\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                              n_redundant=0, n_repeated=0, n_classes=3,\n                              random_state=42)\n    clf = svm.SVC(kernel='linear', probability=True, random_state=42)\n    clf.fit(X, y)\n    y_pred_proba = clf.predict_proba(X)\n    X_test = X[:10]\n    y_pred_proba_sparse = clf.predict_proba(X_test)\n    assert_array_almost_equal(y_pred_proba[:10, :], y_pred_proba_sparse)\n", "def test_libsvm_sparse_predict():\n    \"\"\"Test the prediction function using a precomputed kernel.\"\"\"\n    # Create a precomputed kernel matrix\n    X_train, Y = make_classification(n_samples=10, n_features=2, n_informative=2,\n                                    n_redundant=0, n_repeated=0, n_classes=2,\n                                    random_state=42)\n    kernel = rbf_kernel(X_train, X_train)\n    X_test = [[-1, -1], [2, 2]]\n\n    # Create a LibSVM classifier with a precomputed kernel\n    clf = svm.SVC(kernel='precomputed')\n    clf.fit(kernel, Y)\n\n    # Test the predict function\n    assert_array_equal(clf.predict(X_test), np.array([1, 1]))\n\n    # Test the predict_proba function\n    probas = clf.predict_proba(X_test)\n    assert_array_equal(probas.shape, (2, 2))\n\n    # Test the predict_log_proba function\n    log_proba = clf.predict_log_proba(X_test)\n    assert_array_equal(log_proba.shape, (2, 2))\n\n    # Test the decision_function function\n    dec_func = clf.decision_function(X_test)\n    assert_array_equal(dec_func.shape, (2, 1))\n", "def test_svc_predict_proba():\n    X = np.array([[-1, -1], [2, 2]])\n    y = np.array([1, 2])\n    svc = svm.SVC(probability=True)\n    svc.fit(X, y)\n    X_test = np.array([[-1.5, -1], [1.5, 2]])\n    assert_array_almost_equal(svc.predict_proba(X_test), svc.predict_proba(X_test), decimal=4)\n\n", "def test_libsvm_svm_classify_svm_type(kernel):\n    \"\"\"Test LibSVM classification with different types of SVM.\n\n    This function tests if the classification using LibSVM can produce the same\n    results as the classification using liblinear.\n\n    Parameters\n    ----------\n    kernel : str\n        kernel to be used for the classification\n    \"\"\"\n    X, y = datasets.make_classification(n_samples=1000, n_features=10,\n                                       n_informative=5, n_redundant=0,\n                                       n_repeated=0, n_classes=2,\n                                       n_clusters_per_class=1, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                       random_state=42)\n\n    libsvm_clf = svm.SVC(kernel=kernel, random_state=42)\n    libsvm_clf.fit(X_train, y_train)\n    libsvm_pred = libsvm_clf.predict(X_test)\n    assert_array_equal(libsvm_pred, svm.SVC(kernel=kernel, random_state=42).fit(X_train, y_train).predict(X_test))\n"], "sample_931": ["def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List[int]'\n\n    annotation = \"List[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List[int, str]'\n\n    annotation = \"List[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], nodes.Text)\n    assert result[0].astext() == 'List[int, str]'\n\n    annotation = \"None\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], addnodes.pending_xref)\n\n    annotation = \"int\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], addnodes.pending_xref)\n\n    annotation = \"int, str\"\n    result = _parse_annotation(annotation, env)\n    assert isinstance(result, list)\n    assert isinstance(result[0], addnodes.pending_xref)\n", "def test_pyclasslike():\n    env = Mock(spec=BuildEnvironment)\n    env.ref_context['py:module'] = 'module'\n    env.ref_context['py:class'] = 'class'\n    env.docname = 'docname'\n    env.state.document = Mock(spec=nodes.Node)\n    env.state.document.note_explicit_target = Mock()\n    env.get_domain = Mock(return_value=PythonDomain())\n    signode = desc_signature('class C', '')\n    PyClasslike('class').handle_signature('class C', signode)\n    assert signode['class'] == 'class'\n    assert signode['module'] == 'module'\n    assert signode['fullname'] == 'module.class.C'\n    assert signode.astext() == 'class C'\n", "def test_PyVariable_handle_signature():\n    # test case 1: handle_signature with type\n    env = BuildEnvironment()\n    signode = desc_signature('')\n    var = PyVariable()\n    sig = \"int x: int\"\n    expected = \"int x: int\"\n    var.handle_signature(sig, signode)\n    assert parse(sig) == expected\n\n    # test case 2: handle_signature with value\n    env = BuildEnvironment()\n    signode = desc_signature('')\n    var = PyVariable()\n    sig = \"x = 5\"\n    expected = \"x = 5\"\n    var.handle_signature(sig, signode)\n    assert parse(sig) == expected\n\n    # test case 3: handle_signature with type and value\n    env = BuildEnvironment()\n    signode = desc_signature('')\n    var = PyVariable()\n    sig = \"int x: int = 5\"\n    expected = \"int x: int = 5\"\n    var.handle_signature(sig, signode)\n    assert parse(sig) == expected\n\n    # test case 4: handle_signature with only value\n    env = BuildEnvironment()\n    signode = desc_signature('')\n    var = PyVariable()\n    sig = \"x = 5\"\n    expected = \"x = 5\"\n    var.handle_signature(sig, signode)\n    assert parse(sig) == expected\n\n    # test case 5: handle_signature with only type\n    env = BuildEnvironment()\n    signode = desc_signature('')\n    var = PyVariable()\n    sig = \"int x\"\n    expected = \"int x\"\n    var.handle_signature(sig, signode)\n    assert parse(sig) == expected\n", "def test_python_domain_filter_meta_fields():\n    app = Mock()\n    domain = PythonDomain()\n    node = nodes.field_list()\n    content = nodes.document()\n    content += node\n    filter_meta_fields(app, 'py', 'object', content)\n    assert node.parent is content\n    assert len(node) == 0\n", "def test_parse_annotation():\n    # test that an annotation can be parsed\n    env = Mock()\n    env.ref_context = {}\n    env.ref_context['py:class'] = 'foo.bar'\n    annotation = _parse_annotation('int', env)\n    assert isinstance(annotation, list)\n    assert annotation == [nodes.Text('int')]\n\n    # test that an annotation with a Tuple can be parsed\n    annotation = _parse_annotation('Tuple[int, str]', env)\n    assert isinstance(annotation, list)\n    assert annotation == [addnodes.desc_sig_punctuation('(', '('),\n                         nodes.Text('int'),\n                         addnodes.desc_sig_punctuation(',', ','),\n                         nodes.Text('str'),\n                         addnodes.desc_sig_punctuation(')', ')')]\n\n    # test that an annotation with an Ellipsis can be parsed\n    annotation = _parse_annotation('...', env)\n    assert annotation == [addnodes.desc_sig_punctuation('...', ...)]\n\n    # test that an annotation with a NameConstant can be parsed\n    annotation = _parse_annotation('True', env)\n    assert annotation == [nodes.Text('True')]\n\n    # test that an annotation with a Class can be parsed\n    annotation = _parse_annotation('bool', env)\n    assert annotation == [type_to_xref('bool', env)]\n\n    # test that an annotation with a List can be parsed\n    annotation = _parse_annotation('[int, str]', env)\n    assert annotation == [addnodes.desc_sig_punctuation('[', '['),\n                         nodes.Text('int'),\n                         addnodes.desc_sig_punctuation(',', ','),\n                         nodes.Text('str'),\n                         addnodes.desc_sig_punctuation(']', ']')]\n\n    # test that an annotation with a Dict can be parsed\n    annotation = _parse_annotation('{int: str}', env)\n    assert annotation == [addnodes.desc_sig_punctuation('{', '{'),\n                         nodes.Text('int'),\n                         addnodes.desc_sig_punctuation(':', ':'),\n                         nodes.Text('str'),\n                         addnodes.desc_sig_punctuation('}', '}')]\n\n    # test that an annotation with a Subscript can be parsed\n    annotation = _parse_annotation('list[int]', env)\n    assert annotation == [nodes.Text('list'),\n                         addnodes.desc_sig_punctuation('[', '['),\n                         nodes.Text('int'),\n                         addnodes.desc_sig_punctuation(']', ']')]\n\n    # test that an annotation with an Attribute can be parsed\n    annotation = _parse_annotation('list.__getitem__',"], "sample_468": ["    def test_context_processors(self):\n        # Create a mock request\n        request = HttpRequest()\n\n        # Create a context with processors\n        processors = (\"django.template.context_processors.debug\",)\n        request._current_processor_path = processors\n\n        # Create a context with the processors\n        context = RequestContext(request)\n\n        # Check that the processors are correctly added to the context\n        self.assertIn(\"debug\", context)\n        self.assertTrue(context[\"debug\"])\n\n        # Check that the processors are correctly removed when popping the context\n        context.pop()\n        self.assertNotIn(\"debug\", context)\n", "    def test_render_context_push_and_pop(self):\n        request = HttpRequest()\n        context = RequestContext(request)\n        rc = RenderContext()\n        self.assertEqual(rc.template, None)\n        with rc.push_state(template=\"template_name\"):\n            self.assertEqual(rc.template, \"template_name\")\n            self.assertEqual(rc.pop(), \"template_name\")\n            self.assertEqual(rc.template, None)\n", "    def test_push_and_pop(self):\n        context = Context()\n        self.assertEqual(context.dicts, [{}])\n        inner = context.push('a', 'b')\n        self.assertEqual(context.dicts, [({}, {'a': 'b'})])\n        self.assertEqual(inner.context, context)\n        context.pop()\n        self.assertEqual(context.dicts, [{}])\n", "    def test_Context_push_context_processor(self):\n        request = HttpRequest()\n        context = make_context({\"x\": 1})\n        context.push({\"y\": 2})\n        context.push_context_processor(lambda req: {\"z\": 3})\n\n        self.assertEqual(context.get(\"x\"), 1)\n        self.assertEqual(context.get(\"y\"), 2)\n        self.assertEqual(context.get(\"z\"), 3)\n\n        new_context = context.pop()\n        self.assertEqual(new_context.get(\"z\"), 3)\n        self.assertEqual(new_context.get(\"y\"), 2)\n        self.assertEqual(new_context.get(\"x\"), 1)\n", "    def test_context_processors(self):\n        request = mock.Mock()\n        context_processors = [\n            mock.Mock(),\n            mock.Mock(),\n        ]\n        with override_settings(TEMPLATE_CONTEXT_PROCESSORS=context_processors):\n            context = make_context({}, request=request)\n            self.assertEqual(context._processors, context_processors)\n"], "sample_525": ["def test_figure_pick():\n    # Create a figure and a line plot on it\n    fig = Figure(figsize=(8, 6))\n    ax = fig.add_subplot(111)\n    x = np.linspace(0, 2*np.pi, 100)\n    ax.plot(x, np.sin(x))\n\n    # Create a timer to simulate a pick event\n        fig.canvas.mpl_connect('pick_event', lambda event: None)\n\n    timer = Timer(0.001, pick_event)\n    timer.start()\n\n    # Try to pickle the figure\n    try:\n        pickle.dumps(fig)\n    except Exception as e:\n        assert str(e) == \"can't pickle swappable objects\"\n\n    timer.cancel()\n", "    def test_figure_subplot_methods(self):\n        fig = plt.figure()\n        ax1 = fig.add_subplot(111)\n        ax2 = fig.add_subplot(211)\n        ax3 = fig.add_subplot(212)\n        assert ax1 == fig.axes[0]\n        assert ax2 == fig.axes[1]\n        assert ax3 == fig.axes[2]\n\n        fig.delaxes(ax1)\n        assert len(fig.axes) == 2\n", "    def test_constrained_layout_engine(self):\n        # Create a figure with two subplots\n        fig = Figure(figsize=(8, 6))\n        gs = fig.add_gridspec(2, 2, width_ratios=[1, 3], height_ratios=[1, 1])\n        ax1 = fig.add_subplot(gs[0, 0])\n        ax2 = fig.add_subplot(gs[0, 1])\n        ax3 = fig.add_subplot(gs[1, 0])\n        ax4 = fig.add_subplot(gs[1, 1])\n\n        # Set up the constrained layout engine\n        fig.set_layout_engine('constrained')\n\n        # Set the x-axis of ax2 to share the x-axis of ax1\n        ax2.sharex(ax1)\n\n        # Set the y-axis of ax3 to share the y-axis of ax4\n        ax3.sharey(ax4)\n\n        # Call the constrained layout engine\n        fig.canvas.draw()\n\n        # Check that the layout has been constrained\n        assert ax1.get_position().get_points()[1, 1] < ax2.get_position().get_points()[1, 1]\n        assert ax3.get_position().get_points()[0, 0] < ax4.get_position().get_points()[0, 0]\n", "    def setup_method(self):\n        self.fig, ax = plt.subplots()\n        self.ax = ax\n", "def test_savefig_filelike():\n    with io.BytesIO() as bio:\n        fig = plt.figure()\n        fig.savefig(bio, format='png')\n        assert bio.getvalue() != b''\n"], "sample_289": ["    def test_getitem_with_multiple_values(self):\n        d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n        self.assertEqual(d['name'], 'Simon')\n", "    def test_init(self):\n        d = MultiValueDict({'a': ['1', '2'], 'b': '3'})\n        self.assertEqual(d.getlist('a'), ['1', '2'])\n        self.assertEqual(d['b'], '3')\n", "    def test_getlist_default(self):\n        d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n        self.assertEqual(d.getlist('name'), ['Adrian', 'Simon'])\n        self.assertEqual(d.getlist('doesnotexist'), [])\n        self.assertEqual(d.getlist('doesnotexist', ['John', 'Paul']), ['John', 'Paul'])\n        self.assertEqual(d.getlist('doesnotexist', ['John', 'Paul', 'George']), ['John', 'Paul', 'George'])\n", "    def test_getitem_and_getlist(self):\n        d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n        self.assertEqual(d['name'], 'Simon')\n        self.assertEqual(d.getlist('name'), ['Adrian', 'Simon'])\n        self.assertEqual(d.getlist('doesnotexist'), [])\n", "    def test_immutable_list_cannot_be_modified(self):\n        immutable_list = ImmutableList([1, 2, 3])\n        with self.assertRaises(AttributeError):\n            immutable_list.append(4)\n"], "sample_448": ["    def test_create_sql(self):\n        with atomic():\n            with connection.schema_editor() as schema_editor:\n                constraint = UniqueConstraint(fields=[\"field1\", \"field2\"], name=\"test_constraint\")\n                self.assertEqual(constraint.create_sql(Product, schema_editor), schema_editor._create_unique_sql(Product, [\"field1\", \"field2\"], \"test_constraint\", None, None, (), ()))\n", "    def test_unique_constraint_with_condition_and_deferrable(self):\n        with self.assertRaises(ValueError):\n            UniqueConstraint(\n                fields=[\"id\"],\n                condition=Q(a=1),\n                deferrable=Deferrable.DEFERRED,\n            )\n", "    def test_valid_unique_constraint(self):\n        product = Product.objects.create(name=\"Product A\", price=10.99)\n        UniqueConstraintProduct().validate(product)\n", "    def test_check_constraint_violation_message(self):\n        model = models.Model()\n        constraint = CheckConstraint(check=models.Q(a=1), name=\"test\")\n        with self.assertRaises(ValidationError):\n            constraint.validate(model, model())\n", "    def test_check_constraint_repr(self):\n        check_constraint = CheckConstraint(check=Q(a=1), name=\"test_constraint\")\n        self.assertIn(\"CheckConstraint: check=Q(a=1)\", str(check_constraint))\n"], "sample_981": ["def test_unrank_nonlex():\n    p = Permutation.unrank_nonlex(4, 5)\n    assert p == Permutation([2, 0, 3, 1])\n\n    p = Permutation.unrank_nonlex(4, 10)\n    assert p == Permutation([3, 0, 2, 1])\n\n    p = Permutation.unrank_nonlex(4, 24)\n    assert p == Permutation([3, 0, 1, 2])\n\n    p = Permutation.unrank_nonlex(4, -1)\n    assert p == Permutation([0, 1, 2, 3])\n\n    with raises(ValueError):\n        Permutation.unrank_nonlex(4, 25)\n", "def test_rank():\n    p = Permutation([0, 1, 2, 3])\n    assert p.rank() == 0\n    p = Permutation([3, 2, 1, 0])\n    assert p.rank() == 23\n", "def test_from_sequence():\n    p = Permutation.from_sequence('abcde')\n    assert p.array_form == [3, 1, 2, 4, 0]\n    assert p.rank() == 18\n\n    p = Permutation.from_sequence('ab')\n    assert p.array_form == [1, 0]\n    assert p.rank() == 1\n\n    p = Permutation.from_sequence('a', key=lambda x: x.lower())\n    assert p.array_form == [2, 0]\n    assert p.rank() == 1\n\n    p = Permutation.from_sequence('abcde', key=lambda x: x.lower())\n    assert p.array_form == [3, 1, 2, 4, 0]\n    assert p.rank() == 18\n\n    with raises(ValueError):\n        Permutation.from_sequence('abcde', key=lambda x: x.lower(), size=3)\n\n    with raises(ValueError):\n        Permutation.from_sequence('abcde', key=lambda x: x.lower())\n\n    with raises(ValueError):\n        Permutation.from_sequence('ab')\n\n    p = Permutation.from_sequence('abcde', size=5)\n    assert p.array_form == [3, 1, 2, 4, 0]\n    assert p.rank() == 18\n\n    p = Permutation.from_sequence('abcde', size=10)\n    assert p.array_form == [3, 1, 2, 4, 0, 6, 7, 8, 9]\n    assert p.rank() == 181\n\n    p = Permutation.from_sequence('abcde', size=0)\n    with raises(ValueError):\n        p.array_form\n\n    p = Permutation.from_sequence('abcde', size=-1)\n    with raises(ValueError):\n        p.array_form\n\n    p = Permutation.from_sequence('abcde', size=1)\n    with raises(ValueError):\n        p.array_form\n", "def test_cycle_structure():\n    p = Permutation([2, 0, 1, 3])\n    assert p.cycle_structure == {1: 3, 2: 1}\n\n    p = Permutation([[2, 0], [3, 1]])\n    assert p.cycle_structure == {1: 2, 2: 2}\n\n    p = Permutation([0, 1, 2, 3, 4, 5])\n    assert p.cycle_structure == {1: 6}\n\n    p = Permutation([[0, 1, 4, 3], [2, 5]])\n    assert p.cycle_structure == {3: 2, 2: 1}\n\n    p = Permutation([[0, 1, 2, 4, 3], [5]])\n    assert p.cycle_structure == {4: 1, 3: 2}\n\n    p = Permutation([[0, 4, 3, 2, 1]])\n    assert p.cycle_structure == {5: 1}\n", "def test_Permutation_from_cycle():\n    p = Permutation((2, 4, 5)(1, 3))\n    assert p.array_form == [0, 4, 3, 1, 5, 2]\n    assert p.cycles == 2\n    assert p.cycles == p.size - p.is_Singleton - p.is_Empty\n"], "sample_298": ["    def test_make_token_with_timestamp(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com')\n        token_generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 1, 1))\n        token = token_generator.make_token(user)\n        self.assertRegex(token, r'^[0-9a-zA-Z-]+$')\n        self.assertLessEqual(len(token), 128)\n", "    def test_make_token_with_timestamp(self):\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 1, 1))\n        user = User.objects.create_user('testuser', email='test@example.com')\n        token = generator.make_token(user)\n        expected_timestamp = generator._num_seconds(datetime(2022, 1, 1))\n        expected_hash_value = generator._make_hash_value(user, expected_timestamp)\n        expected_token = generator._make_token_with_timestamp(user, expected_timestamp)\n        self.assertEqual(token, expected_token)\n        self.assertEqual(token.split('-')[1], expected_hash_value)\n", "    def test_make_token_with_invalid_algorithm(self):\n        # Given\n        generator = MockedPasswordResetTokenGenerator(datetime(2024, 1, 1))\n        generator.algorithm = 'non-existent-algorithm'\n\n        # When\n        with self.assertRaises(ImproperlyConfigured):\n            generator._make_token_with_timestamp(User.objects.create(), 1)\n", "    def test_invalid_algorithm(self):\n        with self.assertRaises(ImproperlyConfigured):\n            PasswordResetTokenGenerator(algorithm='invalid_algorithm')\n", "    def test_make_hash_value_with_empty_email(self):\n        user = User()\n        token_generator = PasswordResetTokenGenerator()\n        hash_value = token_generator._make_hash_value(user, 0)\n        self.assertNotIn(user.email, hash_value)\n"], "sample_791": ["def test_ordinal_encoder_handle_unknown():\n    \"\"\"Test that OrdinalEncoder raises a ValueError when encountering unknown categories.\"\"\"\n    encoder = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2], ['Unknown', 1]]\n    with assert_raises_regex(ValueError, \"Unknown categories\"):\n        encoder.fit_transform(X)\n", "def test_OneHotEncoder_categories_order():\n    X = [['Male', 1], ['Female', 2]]\n    encoder = OneHotEncoder(categories=[['Female', 'Male'], [1, 2]])\n    encoder.fit(X)\n    encoded = encoder.transform(X)\n    expected = np.array([[0., 0., 1., 0.],\n                        [0., 1., 0., 1.]])\n    assert_array_equal(toarray(encoded), expected)\n", "def test_OneHotEncoder_warning_on_unknown_categories():\n    encoder = OneHotEncoder(handle_unknown='ignore', categorical_features='all')\n    X = [['cat1'], ['cat2'], ['cat1'], ['cat4']]\n    with pytest.warns(UserWarning):\n        encoder.fit_transform(X)\n", "def test_fit_transform_ignores_unknown_category():\n    \"\"\"Test OneHotEncoder with unknown categories during fit and transform.\"\"\"\n    # create a OneHotEncoder with unknown categories during fit\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X = np.array([[1, 'a'], [2, 'b'], [3, 'c'], [4, 'd']])\n    encoder.fit(X)\n    \n    # check that the encoder has the right categories\n    categories = encoder.categories_\n    assert categories == [[1, 2, 3, 4], ['a', 'b', 'c', 'd']]\n    \n    # create a new dataset with an unknown category during transform\n    X_unknown = np.array([[1, 'a'], [2, 'b'], [3, 'e']])\n    \n    # check that the encoder ignores the unknown category\n    X_out = encoder.transform(X_unknown)\n    X_out_array = toarray(X_out)\n    assert_array_equal(X_out_array, [[1, 0, 0, 0, 1],\n                                    [0, 1, 0, 0, 0],\n                                    [0, 0, 1, 0, 0]])\n    \n    # check that the inverse transform also ignores the unknown category\n    X_tr = encoder.inverse_transform(X_out)\n    assert_array_equal(X_tr, [['a', 1], ['b', 2], [None, 3]])\n", "def test_ordinal_encoder_warning():\n    with pytest.warns(DeprecationWarning) as record:\n        oe = OrdinalEncoder(categories=[['foo', 'bar'], [1, 2, 3]])\n    assert len(record) == 1\n    assert \"The categories keyword is deprecated\" in str(record[0].message)\n"], "sample_283": ["    def test_settings_to_cmd_args_env_passfile(self):\n        settings_dict = {\n            'NAME': 'mydatabase',\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n            }\n        }\n        expected_args = ['psql', '-U', 'myuser', '-h', 'localhost', 'mydatabase', '/path/to/passfile']\n        expected_env = {'PGPASSWORD': 'mypassword', 'PGPASSFILE': '/path/to/passfile'}\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "    def test_settings_to_cmd_args_env_empty_settings(self):\n        with self.assertRaises(KeyError):\n            DatabaseClient.settings_to_cmd_args_env({})\n", "    def test_settings_to_cmd_args_env_passfile(self):\n        settings_dict = {\n            'OPTIONS': {\n                'passfile': '/path/to/passfile'\n            }\n        }\n        params = ['-c', 'SHOW SEARCH_PATH;']\n        expected_args = ['/path/to/executable', '-U', 'postgres', '-h', 'localhost', '-p', '5432', 'postgres', '-c', 'SHOW SEARCH_PATH;']\n        expected_env = {'PGPASSFILE': '/path/to/passfile'}\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, params)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n", "    def test_empty_settings_dict(self):\n        settings_dict = {}\n        parameters = ['--help']\n        with self.assertRaises(KeyError):\n            DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n", "    def test_settings_to_cmd_args_env_without_credentials(self):\n        settings_dict = {\n            'ENGINE': 'django.db.backends.postgresql',\n            'NAME': 'mydb',\n            'USER': 'myuser',\n            'HOST': 'localhost',\n            'PORT': 5432,\n        }\n        parameters = ['--help']\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, ['-psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb', '--help'])\n        self.assertEqual(env, {})\n"], "sample_720": ["def test_quantile_transform_unfitted():\n    with assert_raises_regex(NotFittedError, \"QuantileTransformer\"):\n        QuantileTransformer().transform(X_2d)\n", "    def test_fit(self):\n        scaler = MinMaxScaler()\n        X = [[1, 2], [3, 4]]\n        scaler.fit(X)\n        assert_equal(scaler.data_min_, [1, 2])\n        assert_equal(scaler.data_max_, [3, 4])\n        assert_equal(scaler.scale_, [2., 2.])\n        assert_equal(scaler.min_, [0., 0.])\n        assert_equal(scaler.data_range_, [2, 2])\n", "def test_maxabs_scale_squeeze():\n    X = np.array([[1, 2], [1, 2]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X)\n    np.testing.assert_array_equal(X_scaled, [[1, 2], [1, 2]])\n\n    X = np.array([[1, 2, 3], [1, 2, 4]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X)\n    np.testing.assert_array_equal(X_scaled, [[1, 2, 3], [1, 2, 4]])\n\n    X = np.array([[1, 2], [1, 2]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X, axis=1)\n    np.testing.assert_array_equal(X_scaled, [[1, 1], [2, 2]])\n\n    X = np.array([[1, 2, 3], [1, 2, 4]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X, axis=1)\n    np.testing.assert_array_equal(X_scaled, [[1, 1, 3], [2, 2, 4]])\n\n    X = np.array([[1, 2]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X)\n    np.testing.assert_array_equal(X_scaled, [[1, 2]])\n\n    X = np.array([[1, 2]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X, axis=1)\n    np.testing.assert_array_equal(X_scaled, [[1], [2]])\n\n    X = np.array([1, 2])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X)\n    np.testing.assert_array_equal(X_scaled, [1, 2])\n\n    X = np.array([1, 2])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X, axis=0)\n    np.testing.assert_array_equal(X_scaled, [1, 2])\n\n    X = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n    scaler = MaxAbsScaler()\n    X_scaled = maxabs_scale(X)\n    np.testing.assert_array_equal(X_scaled, [[[1, 2], [3, 4]], [[5, 6], [7,", "def test_min_max_scaler_trivial():\n    \"\"\"Test that MinMaxScaler works with trivial input\"\"\"\n    scaler = MinMaxScaler()\n    X = [[0, 0], [0, 0], [1, 1], [1, 1]]\n    X_scaled = [[0., 0.],\n                [0., 0.],\n                [1., 1.],\n                [1., 1.]]\n    assert_array_equal(scaler.fit_transform(X), X_scaled)\n\n", "def test_onehot_transform_with_csc_matrix():\n    # Note: this test is not currently run in the testsuite, but could be\n    # in the future if the issue is still present\n    X = np.array([[1, 2, 3], [1, 2, 4]], dtype=np.int)\n    X_csc = sparse.csc_matrix(X)\n    enc = OneHotEncoder()\n    enc.fit(X)\n    X_oh = enc.transform(X)\n    X_csc_oh = enc.transform(X_csc)\n    assert_array_equal(X_oh.toarray(), X_csc_oh.toarray())\n"], "sample_681": ["def test_log_level():\n    caplog = pytest.cache_fixture(\"caplog\")\n    assert caplog.handler.level == logging.NOTSET\n    caplog.set_level(logging.DEBUG)\n    assert caplog.handler.level == logging.DEBUG\n    caplog.set_level(logging.INFO, \"mylogger\")\n    logger = logging.getLogger(\"mylogger\")\n    assert logger.level == logging.INFO\n    caplog.clear()\n    assert caplog.records == []\n", "def test_LoggingPlugin_init_logging_configuration(tmp_path):\n    config = Config()\n    plugin = LoggingPlugin(config)\n    assert plugin.formatter._style._fmt == DEFAULT_LOG_FORMAT\n    assert plugin.log_level is None\n", "def test_catching_logs(caplog):\n    log = caplog.handler\n    log.setLevel(logging.ERROR)\n    logging.error(\"Error message\")\n    assert log.records[0].levelname == \"ERROR\"\n    assert log.records[0].name == \"root\"\n    assert log.records[0].getMessage() == \"Error message\"\n", "def test_pytest_configure_logging_plugin(caplog):\n    caplog.handler.setLevel(logging.INFO)\n    assert caplog.handler.stream.getvalue() == \"\"\n    assert not caplog.records\n    assert caplog.text == \"\"\n    assert caplog.record_tuples == []\n    assert caplog.messages == []\n    assert caplog.get_records(\"setup\") == []\n    caplog.clear()\n    assert caplog.handler.stream.getvalue() == \"\"\n    assert not caplog.records\n    assert caplog.text == \"\"\n    assert caplog.record_tuples == []\n    assert caplog.messages == []\n", "def test_log_capture_fixture_reverts_log_level(tmp_path: Path) -> None:\n    \"\"\"Check if caplog fixture reverts the log level of a logger after the test.\"\"\"\n    logging.getLogger(\"test_logger\").setLevel(logging.WARNING)\n    log_level = logging.getLogger(\"test_logger\").getEffectiveLevel()\n\n    caplog = pytest.fixture(config=None).setup()\n    caplog.set_level(logging.INFO, logger=\"test_logger\")\n    caplog.clear()\n    caplog.set_level(log_level, logger=\"test_logger\")\n\n    assert logging.getLogger(\"test_logger\").getEffectiveLevel() == log_level\n"], "sample_363": ["    def test_autocomplete_select_widget(self):\n        # Create a test model and field\n        class TestModel(models.Model):\n            name = CharField(max_length=255)\n\n        class TestAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {'widget': AutocompleteSelect},\n            }\n\n        # Create an admin instance and get the formfield for the test field\n        admin = TestAdmin(TestModel, admin.site)\n        formfield = admin.formfield_for_dbfield(TestModel._meta.get_field('name'), request=None)\n\n        # Check that the widget is the correct type\n        self.assertIsInstance(formfield.widget, AutocompleteSelect)\n", "    def test_autocomplete_widget_html(self):\n        field = CharField('name')\n        widget = AutocompleteSelect(field, admin.site)\n        html = widget.render('name', None, {})\n        self.assertIn('class=\"admin-autocomplete\"', html)\n        self.assertIn('data-ajax--cache=\"true\"', html)\n        self.assertIn('data-ajax--delay=\"250\"', html)\n        self.assertIn('data-ajax--type=\"GET\"', html)\n        self.assertIn('data-ajax--url=\"/admin:autocomplete\"', html)\n        self.assertIn('data-app-label=\"app_label\"', html)\n        self.assertIn('data-model-name=\"model_name\"', html)\n        self.assertIn('data-field-name=\"name\"', html)\n        self.assertIn('data-theme=\"admin-autocomplete\"', html)\n        self.assertIn('data-allow-clear=\"true\"', html)\n        self.assertIn('data-placeholder=\"\"', html)\n        self.assertIn('lang=\"en\"', html)\n", "    def test_AdminSplitDateTimeWidget(self):\n        from .widgetadmin import site as widget_admin_site\n        class TestModel(models.Model):\n            date = models.DateField()\n            time = models.TimeField()\n        admin_class = admin.ModelAdmin(TestModel, widget_admin_site)\n        formfield = admin_class.formfield_for_dbfield(TestModel._meta.get_field('date'), request=None)\n        self.assertIsInstance(formfield.widget, widgets.AdminSplitDateTime)\n        admin_class = admin.ModelAdmin(TestModel, widget_admin_site)\n        formfield = admin_class.formfield_for_dbfield(TestModel._meta.get_field('time'), request=None)\n        self.assertIsInstance(formfield.widget, widgets.AdminSplitDateTime)\n", "    def test_get_context_returns_context_with_related_url(self):\n        from .models import Car\n        car = Car.objects.create(owner=self.superuser)\n        rel = car._meta.get_field('owner').remote_field\n        widget = ForeignKeyRawIdWidget(rel, admin.site, attrs={'id': 'owner'})\n        context = widget.get_context('owner', car.id, attrs={'id': 'owner'})\n        self.assertIn('related_url', context)\n        self.assertEqual(context['related_url'], reverse('admin:auth_user_changelist'))\n", "    def test_autocomplete_ajax_request(self):\n        from .widgetadmin import site as widget_admin_site\n        form = forms.Form()\n        widget = AutocompleteSelect(field=form.fields['username'], admin_site=widget_admin_site, attrs={'data-foo': 'bar'})\n        request = forms.Form({'username': 'test'})\n        url = widget.get_url()\n        response = self.client.get(url, {'q': 'test'})\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'username')\n        self.assertContains(response, 'bar')\n"], "sample_509": ["def test_datestr2num_empty_list():\n    assert np.all(datestr2num([]) == np.array([]))\n", "def test_AutoDateFormatter():\n    # Arrange\n    with rc_context({'timezone': 'UTC', 'date.converter': 'auto',\n                    'date.interval_multiples': True}):\n        fig, ax = plt.subplots()\n        locator = mdates.AutoDateLocator()\n        formatter = mdates.AutoDateFormatter(locator)\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(formatter)\n        dstart = datetime.date(2022, 1, 1)\n        dend = datetime.date(2022, 12, 31)\n        x = mdates.drange(dstart, dend, datetime.timedelta(days=30))\n        ax.plot(x, np.random.rand(len(x)))\n\n    # Act\n    with rc_context({'timezone': 'UTC', 'date.converter': 'concise',\n                    'date.interval_multiples': True}):\n        fig, ax = plt.subplots()\n        locator = mdates.AutoDateLocator()\n        formatter = mdates.ConciseDateFormatter(locator)\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(formatter)\n        dstart = datetime.date(2022, 1, 1)\n        dend = datetime.date(2022, 12, 31)\n        x = mdates.drange(dstart, dend, datetime.timedelta(days=30))\n        ax.plot(x, np.random.rand(len(x)))\n\n    # Assert\n    assert formatter.get_offset() == ''\n    assert formatter.format_data_short(x[0]) == '2022-01-01 00:00:00'\n", "def test_date_locator_nonsingular():\n    # Test that the nonsingular method of a DateLocator is called with\n    # a range that is not singular.\n    locator = mdates.DateLocator()\n    vmin, vmax = mdates.date2num([datetime.date(2000, 1, 1), datetime.date(2000, 1, 2)])\n    locator.nonsingular(vmin, vmax)\n", "def test_julian2num():\n    # test with single value\n    assert np.allclose(julian2num(2451545.0), 539148.5)\n    # test with multiple values\n    assert np.allclose(julian2num([2451545.0, 2451546.0]), [539148.5, 539149.5])\n", "def test_date2num_with_tz(dt, exp_num):\n    num = mdates.date2num(dt)\n    assert np.allclose(num, exp_num)\n"], "sample_817": ["def test_VarianceThreshold_threshold_is_zero_and_all_features_constant():\n    X = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n    selector = VarianceThreshold()\n    selector.threshold = 0\n    assert_raises(ValueError, selector.fit_transform, X)\n", "def test_threshold_zero_sparse_matrix():\n    X = csr_matrix([[0, 0, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]])\n    selector = VarianceThreshold(threshold=0)\n    selector.fit(X)\n    expected_support = [False, False, False, True]\n    assert_array_equal(selector._get_support_mask(), expected_support)\n\n", "def test_fit_transform_sparse_constant():\n    X = csr_matrix(np.array([[0, 0, 0, 0, 0],\n                            [0, 0, 0, 0, 0],\n                            [0, 0, 0, 0, 0]]))\n    selector = VarianceThreshold(threshold=0.1)\n    selector.fit(X)\n    assert_equal(selector.get_support_mask(), np.array([False, False, False, False, False]))\n", "def test_threshold_equality_to_zero():\n    \"\"\"Test threshold equals to zero.\"\"\"\n    selector = VarianceThreshold(threshold=0.)\n    X = np.array([[1, 1, 1, 1],\n                  [1, 1, 1, 1],\n                  [1, 1, 1, 1]])\n    with pytest.raises(ValueError):\n        selector.fit_transform(X)\n", "def test_VarianceThreshold_threshold_0__all_features_removed():\n    \"\"\"Test that all features are removed when threshold is 0\"\"\"\n    X = csr_matrix([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n    selector = VarianceThreshold(threshold=0)\n    with assert_raises(ValueError):\n        selector.fit_transform(X)\n"], "sample_615": ["def test_unify_chunks():\n    # Test that unify_chunks works with DataArray\n    da = xr.DataArray(\n        np.random.rand(5, 3),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n        chunks={\"x\": 2, \"y\": 2},\n    )\n    da_unified, = unify_chunks(da)\n\n    assert da_unified.chunks == {\"x\": 5, \"y\": 3}\n\n    # Test that unify_chunks works with Dataset\n    ds = xr.Dataset(\n        {\"x\": (\"x\", np.arange(5)), \"y\": (\"y\", np.arange(3))},\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n        chunks={\"x\": 2, \"y\": 2},\n    )\n    ds_unified = unify_chunks(ds)\n\n    assert ds_unified.chunks == {\"x\": 5, \"y\": 3}\n\n    # Test that unify_chunks works with multiple DataArray\n    da1 = xr.DataArray(\n        np.random.rand(5, 3),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n        chunks={\"x\": 2, \"y\": 2},\n    )\n    da2 = xr.DataArray(\n        np.random.rand(5, 3),\n        dims=[\"x\", \"y\"],\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n        chunks={\"x\": 3, \"y\": 1},\n    )\n    da_unified1, da_unified2 = unify_chunks(da1, da2)\n\n    assert da_unified1.chunks == {\"x\": 5, \"y\": 3}\n    assert da_unified2.chunks == {\"x\": 5, \"y\": 3}\n\n    # Test that unify_chunks works with multiple Dataset\n    ds1 = xr.Dataset(\n        {\"x\": (\"x\", np.arange(5)), \"y\": (\"y\", np.arange(3))},\n        coords={\"x\": np.arange(5), \"y\": np.arange(3)},\n        chunks={\"x\": 2, \"y\": 2},\n    )\n    ds2 = xr.Dataset(\n        {\"x\": (\"x\", np.arange(5)), \"y\": (\"y\",", "def test_broadcast_compat_data():\n    da = xr.DataArray(np.arange(3), dims=[\"x\"], coords={\"x\": np.arange(3)})\n\n    # Test broadcast_dims == []\n    broadcast_compat_data(da, broadcast_dims=(), core_dims=(\"x\",))\n    # Test broadcast_dims == (x,)\n    broadcast_compat_data(da, broadcast_dims=(\"x\",), core_dims=())\n    # Test broadcast_dims == (x, y)\n    broadcast_compat_data(da, broadcast_dims=(\"x\", \"y\"), core_dims=())\n    # Test broadcast_dims == (x, y, z) and core_dims == (x, y)\n    broadcast_compat_data(da, broadcast_dims=(\"x\", \"y\", \"z\"), core_dims=(\"x\", \"y\"))\n\n    # Test unexpected dimensions\n    with pytest.raises(ValueError):\n        broadcast_compat_data(da, broadcast_dims=(\"x\", \"y\", \"z\"), core_dims=(\"x\",))\n\n    # Test missing core dimensions\n    with pytest.raises(ValueError):\n        broadcast_compat_data(da, broadcast_dims=(\"x\",), core_dims=(\"x\", \"y\"))\n\n    # Test reorder dimensions\n    da = da.transpose(\"y\", \"x\")\n    broadcast_compat_data(da, broadcast_dims=(\"x\",), core_dims=(\"y\",))\n", "def test_unify_chunks():\n    # Test unify_chunks with one dataset\n    da1 = xr.DataArray(\n        np.random.rand(10),\n        dims=[\"x\"],\n        coords={\"x\": np.arange(10)},\n        name=\"var1\",\n    )\n    da1 = da1.chunk({\"x\": 2})\n\n    da2 = unify_chunks(da1)\n    assert_identical(da2, da1)\n\n    # Test unify_chunks with multiple datasets\n    da1 = xr.DataArray(\n        np.random.rand(10),\n        dims=[\"x\"],\n        coords={\"x\": np.arange(10)},\n        name=\"var1\",\n    )\n    da1 = da1.chunk({\"x\": 2})\n    da2 = xr.DataArray(\n        np.random.rand(10),\n        dims=[\"x\"],\n        coords={\"x\": np.arange(10)},\n        name=\"var2\",\n    )\n    da2 = da2.chunk({\"x\": 3})\n\n    da3 = unify_chunks(da1, da2)\n    assert_identical(da3, da1)\n    assert_identical(da3, da2)\n\n    # Test unify_chunks with dask version 2021.10.0\n    with raises_if_dask_computes(dask_version(\"2021.10.0\")):\n        unify_chunks(da1, da2)\n\n    # Test unify_chunks with a non-chunked DataArray\n    da1 = xr.DataArray(\n        np.random.rand(10),\n        dims=[\"x\"],\n        coords={\"x\": np.arange(10)},\n        name=\"var1\",\n    )\n    da2 = unify_chunks(da1, da2)\n    assert_identical(da2, da1)\n", "def test_unified_dim_sizes():\n    # Test that unified_dim_sizes handles a variable with a size of 1.\n    var1 = xr.Variable((\"dim\", [1]), 1)\n    var2 = xr.Variable((\"dim\", [2]), 2)\n    assert unified_dim_sizes([var1, var2]) == {\"dim\": 2}\n\n    # Test that unified_dim_sizes raises an error if a variable has duplicate dimensions.\n    var3 = xr.Variable((\"dim\", [1]), 1)\n    var4 = xr.Variable((\"dim\", [1, 1]), 2)\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var2, var3, var4])\n\n    # Test that unified_dim_sizes raises an error if a variable has a dimension with a different size.\n    var5 = xr.Variable((\"dim\", [2]), 1)\n    var6 = xr.Variable((\"dim\", [1]), 2)\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var2, var5, var6])\n\n    # Test that unified_dim_sizes raises an error if a variable has a dimension that is not present in another variable.\n    var7 = xr.Variable((\"dim1\", [2]), 1)\n    var8 = xr.Variable((\"dim2\", [1]), 2)\n    with pytest.raises(ValueError):\n        unified_dim_sizes([var1, var2, var7, var8])\n\n    # Test that unified_dim_sizes handles variables with no dimensions.\n    var9 = xr.Variable([], 1)\n    assert unified_dim_sizes([var9]) == {}\n", "def test_cov():\n    # Test that cov and corr with scalar arguments raise TypeError\n    with pytest.raises(TypeError):\n        cov(xr.DataArray([1, 2, 3]), 2)\n    with pytest.raises(TypeError):\n        cov(xr.DataArray([1, 2, 3]), xr.DataArray([4, 5, 6]), dim=None, 2)\n    with pytest.raises(TypeError):\n        cov(xr.DataArray([1, 2, 3]), 2, dim=None)\n    with pytest.raises(TypeError):\n        corr(xr.DataArray([1, 2, 3]), 2)\n    with pytest.raises(TypeError):\n        corr(xr.DataArray([1, 2, 3]), xr.DataArray([4, 5, 6]), dim=None, 2)\n    with pytest.raises(TypeError):\n        corr(xr.DataArray([1, 2, 3]), 2, dim=None)\n\n    # Test cov with broadcasting\n    da_a = xr.DataArray([1, 2, 3])\n    da_b = xr.DataArray([4, 5, 6])\n    assert_identical(cov(da_a, da_b), -3.0)\n\n    # Test cov with broadcasting and axis\n    da_a = xr.DataArray([1, 2, 3])\n    da_b = xr.DataArray([4, 5, 6])\n    assert_identical(cov(da_a, da_b, dim=\"x\"), [0.2, -0.5, 1.69333333])\n\n    # Test corr with broadcasting\n    assert_identical(corr(da_a, da_b), -0.57087777)\n\n    # Test corr with broadcasting and axis\n    assert_identical(corr(da_a, da_b, dim=\"x\"), [1.0, -1.0, 1.0])\n\n    # Test cov and corr with multiple input arrays\n    da_a = xr.DataArray([1, 2, 3])\n    da_b = xr.DataArray([4, 5, 6])\n    da_c = xr.DataArray([7, 8, 9])\n    assert_identical(cov(da_a, da_b, da_c), -10.0)\n    assert_identical(corr(da_a, da_b, da_c), -0.90630677)\n\n    # Test cov and corr with broadcasting and axis\n    da_a ="], "sample_332": ["    def test_formset_factory_defaults(self):\n        form = form_factory(Form)\n        FormSet = formset_factory(form)\n        self.assertEqual(FormSet.__name__, 'FormFormSet')\n        self.assertEqual(FormSet.min_num, DEFAULT_MIN_NUM)\n        self.assertEqual(FormSet.max_num, DEFAULT_MAX_NUM)\n        self.assertEqual(FormSet.absolute_max, DEFAULT_MAX_NUM + DEFAULT_MAX_NUM)\n", "    def make_formset_with_custom_kwarg(\n            self, formset_data=None, formset_class=formset_factory(CustomKwargForm, extra=1),\n            total_forms=None, initial_forms=0, max_num_forms=0, min_num_forms=0, **kwargs):\n        \"\"\"\n        Make a FormSet from the given formset_data.\n        The data should be given as a list of dictionaries.\n        \"\"\"\n        kwargs.setdefault('prefix', 'custom_kwarg')\n        kwargs.setdefault('auto_id', False)\n        kwargs.setdefault('form_kwargs', {})\n\n        if formset_data is None:\n            return formset_class(**kwargs)\n\n        if total_forms is None:\n            total_forms = len(formset_data)\n\n            args = (kwargs['prefix'],) + args\n            return '-'.join(args)\n\n        data = {\n            prefixed('TOTAL_FORMS'): str(total_forms),\n            prefixed('INITIAL_FORMS'): str(initial_forms),\n            prefixed('MAX_NUM_FORMS'): str(max_num_forms),\n            prefixed('MIN_NUM_FORMS'): str(min_num_forms),\n        }\n        for i, form_data in enumerate(formset_data):\n            data[prefixed(str(i), 'custom_kwarg')] = form_data['custom_kwarg']\n            data[prefixed(str(i), 'choice')] = form_data['choice']\n\n        return formset_class(data, **kwargs)\n", "    def test_management_form_init(self):\n        formset = ChoiceFormSet(auto_id='auto_id')\n        self.assertIn('auto_id', formset.management_form.fields)\n        self.assertEqual(formset.management_form.fields['auto_id'].widget, HiddenInput())\n", "    def test_base_formset_initial_form_count(self):\n        \"\"\"Test BaseFormSet correctly calculates initial form count.\"\"\"\n        formset = BaseFormSet(initial=[1, 2, 3])\n        self.assertEqual(formset.initial_form_count(), 3)\n        self.assertEqual(formset.initial_forms, [form for form in formset])\n\n        formset = BaseFormSet(initial=None)\n        self.assertEqual(formset.initial_form_count(), 0)\n\n        formset = BaseFormSet(initial=[1, 2, 3], data={'INITIAL_FORMS': '2'})\n        self.assertEqual(formset.initial_form_count(), 2)\n", "    def test_formset_factory_defaults(self):\n        ChoiceFormSet = formset_factory(Choice)\n        self.assertEqual(ChoiceFormSet.form.__name__, 'Choice')\n        self.assertEqual(ChoiceFormSet.form.fields['choice'].label, 'choice')\n        self.assertEqual(ChoiceFormSet.form.fields['votes'].label, 'votes')\n"], "sample_968": ["def test_type_to_xref():\n    env = Mock()\n    node = type_to_xref('None', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'obj'\n    assert node['reftarget'] == 'None'\n\n    node = type_to_xref('str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n\n    node = type_to_xref('.str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n\n    node = type_to_xref('~str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n\n    env.config.python_use_unqualified_type_names = True\n    node = type_to_xref('str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n    assert len(node.children) == 2\n    assert isinstance(node.children[0], nodes.Text)\n    assert node.children[0].astext() == 'str'\n    assert isinstance(node.children[1], pending_xref_condition)\n    assert node.children[1]['condition'] == 'resolved'\n\n    node = type_to_xref('.str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n    assert len(node.children) == 2\n    assert isinstance(node.children[0], nodes.Text)\n    assert node.children[0].astext() == 'str'\n    assert isinstance(node.children[1], pending_xref_condition)\n    assert node.children[1]['condition'] == 'resolved'\n\n    node = type_to_xref('~str', env)\n    assert isinstance(node, pending_xref)\n    assert node['reftype'] == 'class'\n    assert node['reftarget'] == 'str'\n    assert len(node.children) == 2\n    assert isinstance(node.children[0], nodes.Text)\n    assert node.children[0].astext() == 'str'\n    assert isinstance(node.children[1], pending_xref_condition)\n    assert node.children[1]['", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {}\n    env.config.python_use_unqualified_type_names = False\n    env.config.add_module_names = False\n\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Union'), nodes.Text('['),\n                     nodes.Text('int'), nodes.Text(','),\n                     nodes.Text('str'), nodes.Text(']')]\n\n    annotation = \"List[int]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('List'), nodes.Text('['),\n                     nodes.Text('int'), nodes.Text(']')]\n\n    annotation = \"Dict[str, int]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Dict'), nodes.Text('['),\n                     nodes.Text('str'), nodes.Text(','),\n                     nodes.Text('int'), nodes.Text(']')]\n\n    annotation = \"Set[str]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Set'), nodes.Text('['),\n                     nodes.Text('str'), nodes.Text(']')]\n\n    annotation = \"Tuple[int, str]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Tuple'), nodes.Text('('),\n                     nodes.Text('int'), nodes.Text(','),\n                     nodes.Text('str'), nodes.Text(')')]\n\n    annotation = \"Type['int']\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Type'), nodes.Text('['),\n                     nodes.Text(\"'\"), nodes.Text('int'), nodes.Text(\"'\"), nodes.Text(']')]\n\n    annotation = \"Optional[int]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Optional'), nodes.Text('['),\n                     nodes.Text('int'), nodes.Text(']')]\n\n    annotation = \"List[Dict[str, int]]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('List'), nodes.Text('['),\n                     nodes.Text('Dict'), nodes.Text('['),\n                     nodes.Text('str'), nodes.Text(','),\n                     nodes.Text('int'), nodes.Text(']'), nodes.Text(']')]\n\n    annotation = \"Union[Dict[str, int], List[int]]\"\n    result = _parse_annotation(annotation, env)\n    assert result == [nodes.Text('Union'), nodes.Text('['),\n                     nodes.Text('", "def test_py_classlike():\n    app = Mock()\n    env = Mock()\n    signode = addnodes.desc_signature('class Test', '')\n    classlike = PyClasslike()\n    fullname, prefix = classlike.handle_signature('class Test', signode)\n    assert fullname == 'Test'\n    assert prefix is None\n    assert signode['module'] is None\n    assert signode['class'] is None\n\n    signode = addnodes.desc_signature('class Test(parent)', '')\n    classlike = PyClasslike()\n    fullname, prefix = classlike.handle_signature('class Test(parent)', signode)\n    assert fullname == 'Test'\n    assert prefix == 'parent'\n\n    classlike = PyClasslike(objtype='exception')\n    signode = addnodes.desc_signature('class Test', '')\n    fullname, prefix = classlike.handle_signature('class Test', signode)\n    assert fullname == 'Test'\n    assert prefix is None\n    assert signode['module'] is None\n    assert signode['class'] == 'exception'\n\n    signode = addnodes.desc_signature('class Test(parent)', '')\n    classlike = PyClasslike(objtype='exception')\n    fullname, prefix = classlike.handle_signature('class Test(parent)', signode)\n    assert fullname == 'Test'\n    assert prefix == 'parent'\n    assert signode['class'] == 'exception'\n", "def test_pyclasslike_rendered_index_text():\n    class TestClasslike(PythonClasslike):\n        objtype = 'class'\n\n    env = Mock()\n    directive = TestClasslike()\n    directive.env = env\n    directive.state = Mock()\n    directive.state.document = Mock()\n    sig = 'SomeClass'\n    signode = addnodes.desc_signature(sig, '')\n    fullname, prefix = directive.handle_signature(sig, signode)\n    assert directive.get_index_text('some_module', (fullname, prefix)) == _('class in some_module')\n", "def test_PyModuleIndex_sorting():\n    domain = PythonDomain()\n    index = PythonModuleIndex()\n    entries = [\n        IndexEntry('a', 1, 'b', 'c', 'd', 'e', 'f'),\n        IndexEntry('x', 1, 'y', 'z', 'a', 'b', 'c'),\n        IndexEntry('b', 1, 'c', 'd', 'e', 'f', 'g'),\n        IndexEntry('y', 1, 'z', 'a', 'b', 'c', 'd'),\n    ]\n    index.index_data = {\n        'entries': entries,\n    }\n    sorted_entries = index.generate()[0]\n    assert sorted_entries == [\n        ('a', [IndexEntry('a', 1, 'b', 'c', 'd', 'e', 'f')]),\n        ('b', [IndexEntry('b', 1, 'c', 'd', 'e', 'f', 'g')]),\n        ('x', [IndexEntry('x', 1, 'y', 'z', 'a', 'b', 'c')]),\n        ('y', [IndexEntry('y', 1, 'z', 'a', 'b', 'c', 'd')]),\n    ]\n"], "sample_1146": ["def test_latex_parenthesize():\n    expr = sin(x)\n    lp = LatexPrinter()\n    assert lp.parenthesize(expr, 1, strict=True) == r\"\\left(\\sin{\\left(x \\right)}\\right)\"\n    assert lp.parenthesize(expr, 1, strict=False) == \"sin(x)\"\n    assert lp.parenthesize(expr, 1, strict=True, is_neg=True) == r\"\\left(-\\sin{\\left(x \\right)}\\right)\"\n", "def test_latex_rendering():\n    assert latex(x**2 + 3*x*y + 2) == r'2 x + 3 x y + 2'\n    assert latex(3*x**2/y) == r'\\frac{3 x^{2}}{y}'\n    assert latex(3*x**2/(y + 1)) == r'\\frac{3 x^{2}}{y + 1}'\n    assert latex(3*x**2/(y + 1)) == r'\\frac{3 x^{2}}{y + 1}'\n    assert latex(2 + 3*x) == r'2 + 3 x'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x + 2) == r'x + 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex(x - 2) == r'x - 2'\n    assert latex", "def test_latex_inequality():\n    assert latex(x >= 0) == r\"0 \\leq x\"\n    assert latex(x < 0) == r\"x < 0\"\n    assert latex(x > 0) == r\"x > 0\"\n    assert latex(x <= 0) == r\"x \\leq 0\"\n    assert latex(x != 0) == r\"x \\neq 0\"\n    assert latex(0 <= x) == r\"0 \\leq x\"\n    assert latex(0 < x) == r\"x > 0\"\n    assert latex(0 > x) == r\"x < 0\"\n    assert latex(0 <= x) == r\"x \\leq 0\"\n    assert latex(0 != x) == r\"x \\neq 0\"\n", "def test_latex_Printer_Mul():\n    latex_printer = LatexPrinter()\n    expr = Mul(3, 2, 4)\n    assert latex_printer.doprint(expr) == r\"3 \\cdot 2 \\cdot 4\"\n    latex_printer.settings['mul_symbol_latex'] = \"ldot\"\n    assert latex_printer.doprint(expr) == r\"3 \\ldot 2 \\ldot 4\"\n    latex_printer.settings['mul_symbol_latex'] = \"dot\"\n    assert latex_printer.doprint(expr) == r\"3 \\cdot 2 \\cdot 4\"\n    latex_printer.settings['mul_symbol_latex'] = \"times\"\n    assert latex_printer.doprint(expr) == r\"3 \\times 2 \\times 4\"\n", "def test_LatexPrinter_with_transfer_function():\n    f = TransferFunction(1, 1, x)\n    g = TransferFunction(1, 1, x)\n    expr = Series(f, g)\n    printer = LatexPrinter()\n    assert printer.doprint(expr) == r'\\frac{1}{s} \\circ \\frac{1}{s}'\n"], "sample_837": ["    def test_get_sys_info(self):\n        sys_info = _get_sys_info()\n        self.assertEqual(len(sys_info), 3)\n        self.assertIn('python', sys_info)\n        self.assertIn('executable', sys_info)\n        self.assertIn('machine', sys_info)\n", "def test_get_blas_info():\n    \"\"\"Test BLAS information retrieval\"\"\"\n    import importlib\n    from sklearn.utils._show_versions import _get_blas_info, get_blas_info\n    \n    # Test that the function does not crash when BLAS is not installed\n    with importlib.mock.patch('sklearn._build_utils.get_blas_info', side_effect=ImportError('BLAS not installed')):\n        blas_info = _get_blas_info()\n        assert blas_info == {}\n        \n    # Test that the function returns the correct information when BLAS is installed\n    with importlib.mock.patch('sklearn._build_utils.get_blas_info', return_value=(['libblas.so.3'], {'define_macros': [('BLAS_MKL', 1)]})):\n        blas_info = _get_blas_info()\n        assert blas_info == {\n            'macros': 'BLAS_MKL=1',\n            'lib_dirs': '',\n            'cblas_libs': 'libblas.so.3'\n        }\n", "def test_get_blas_info():\n    \"\"\"Test get_blas_info function\n\n    Test that get_blas_info returns a dictionary with the correct keys\n    \"\"\"\n    # Setup\n    from sklearn.utils._show_versions import _get_blas_info\n\n    # Test case: no BLAS found\n    with mock.patch('sklearn._build_utils.get_blas_info', return_value=([], {})):\n        blas_info = _get_blas_info()\n        expected = {'macros': '', 'lib_dirs': '', 'cblas_libs': ''}\n        assert blas_info == expected\n\n    # Test case: BLAS found\n    with mock.patch('sklearn._build_utils.get_blas_info', return_value=(['libblas.so'], {'define_macros': [('BLAS_DEF_ENABLE_OPENMP', '1')}))):\n        blas_info = _get_blas_info()\n        expected = {'macros': 'BLAS_DEF_ENABLE_OPENMP=1', 'lib_dirs': '', 'cblas_libs': 'libblas.so'}\n        assert blas_info == expected\n", "    def test_get_sys_info(self):\n        sys_info = _get_sys_info()\n        self.assertIsInstance(sys_info, dict)\n        self.assertIn('python', sys_info)\n        self.assertIn('executable', sys_info)\n        self.assertIn('machine', sys_info)\n", "def test_get_blas_info():\n    \"\"\"Test system BLAS information\n\n    This test will cover the functionality of the `_get_blas_info` function\n    by checking for the presence of BLAS macros, library dirs and cblas libs.\n\n    \"\"\"\n    blas_info = _get_blas_info()\n    assert blas_info.get('macros') is not None\n    assert blas_info.get('lib_dirs') is not None\n    assert blas_info.get('cblas_libs') is not None\n"], "sample_354": ["    def test_create_superuser_no_input_password_set_from_env_var(self):\n        with override_settings(\n            AUTH_USER_MODEL='app.CustomUser',\n            DJANGO_SUPERUSER_PASSWORD='mysecretpassword',\n        ):\n            mock_inputs({'username': 'alice', 'email': 'alice@example.com'})\n            call_command('createsuperuser', '--database=mydb', '--email=alice@example.com', '--noinput')\n            self.assertTrue(User.objects.filter(username='alice', email='alice@example.com').exists())\n", "    def test_create_superuser_noinput(self):\n        # Test that creating a superuser in non-interactive mode raises an error\n        # if the required field is not provided.\n        out = StringIO()\n        sys.stdout = out\n        with self.assertRaises(CommandError) as e:\n            call_command('createsuperuser', '--database', 'default',\n                        '--noinput', '--email', 'test@example.com')\n        self.assertIn('You must use --email with --noinput.', str(e.exception))\n        sys.stdout = sys.__stdout__\n", "    def setUp(self):\n        self.mock_getpass = mock.Mock(side_effect=[None, 'password123'])\n        self.mock_input = mock.Mock(side_effect=['password123', 'password123'])\n        self.mock_env = {'DJANGO_SUPERUSER_PASSWORD': 'password123'}\n        self.mock_env['DJANGO_SUPERUSER_USERNAME'] = 'bob'\n        self.mock_env['DJANGO_SUPERUSER_EMAIL'] = 'bob@example.com'\n        with mock.patch('django.core.management.commands.createsuperuser.getpass', side_effect=self.mock_getpass):\n            with mock.patch('django.core.management.commands.createsuperuser.input', side_effect=self.mock_input):\n                with mock.patch('os.environ', self.mock_env):\n                    self.command = createsuperuser.Command()\n", "    def test_username_input_empty(self):\n        with self.assertRaises(CommandError):\n            call_command('createsuperuser', interactive=False)\n", "    def test_superuser_creation_with_interactive_password_prompt(self):\n        with mock_inputs({'password': lambda: 'test_password', 'password2': lambda: 'test_password'}):\n            call_command('createsuperuser', verbosity=0)\n\n        try:\n            User.objects.get(username='alice', password='test_password')\n        except User.DoesNotExist:\n            self.fail('Superuser not created successfully')\n"], "sample_92": ["    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.user2 = User.objects.create_user('test2', 'test2@example.com', 'test2')\n        cls.group = Group.objects.create(name='test_group')\n        cls.permission = Permission.objects.create(codename='test_perm', content_type=ContentType.objects.get(app_label='auth', model='user'))\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n        cls.user.save()\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n        cls.perm = Permission.objects.create(codename='test_perm', content_type=ContentType.objects.get(model='test_model'))\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user('test', 'test@example.com', 'test')\n        cls.group = Group.objects.create(name='test_group')\n        cls.user.groups.add(cls.group)\n"], "sample_1147": ["def test_latex_LatexPrinter_parenthesize_super():\n    p = LatexPrinter()\n    assert p.parenthesize_super(\"alpha\") == r\"\\alpha\"\n    assert p.parenthesize_super(\"alpha\") == r\"\\alpha\"\n    assert p.parenthesize_super(\"alpha^2\") == r\"\\alpha^{2}\"\n    assert p.parenthesize_super(\"alpha^2\") == r\"\\alpha^{2}\"\n    assert p.parenthesize_super(\"\\\\left(alpha\\\\right)\") == r\"\\left(\\alpha\\right)\"\n    assert p.parenthesize_super(\"\\\\left(alpha\\\\right)\") == r\"\\left(\\alpha\\right)\"\n", "def test_latex():\n    assert latex(bernoulli(5)) == r\"B_5\"\n    assert latex(bernoulli(5, 6)) == r\"B_{5, 6}\"\n    assert latex(bell(5)) == r\"B_5!!\"\n    assert latex(bell(5, 6)) == r\"B_{5, 6}!!\"\n    assert latex(fibonacci(5)) == r\"F_5\"\n    assert latex(fibonacci(5, 6)) == r\"F_{5, 6}\"\n    assert latex(tribonacci(5)) == r\"T_5\"\n    assert latex(tribonacci(5, 6)) == r\"T_{5, 6}\"\n    assert latex(acot(x)) == r\"\\cot^{{-1}}{\\left(x \\right)}\"\n    assert latex(acot(x), 'inv_trig_style': 'full') == r\"\\arccot{\\left(x \\right)}\"\n    assert latex(acot(x), 'inv_trig_style': 'power') == r\"\\cot^{-1}{\\left(x \\right)}\"\n    assert latex(acos(x)) == r\"\\cos^{{-1}}{\\left(x \\right)}\"\n    assert latex(acos(x), 'inv_trig_style': 'full') == r\"\\arccos{\\left(x \\right)}\"\n    assert latex(acos(x), 'inv_trig_style': 'power') == r\"\\cos^{-1}{\\left(x \\right)}\"\n    assert latex(asin(x)) == r\"\\sin^{{-1}}{\\left(x \\right)}\"\n    assert latex(asin(x), 'inv_trig_style': 'full') == r\"\\arcsin{\\left(x \\right)}\"\n    assert latex(asin(x), 'inv_trig_style': 'power') == r\"\\sin^{-1}{\\left(x \\right)}\"\n    assert latex(asec(x)) == r\"\\csc^{{-1}}{\\left(x \\right)}\"\n    assert latex(asec(x), 'inv_trig_style': 'full') == r\"\\arccsc{\\left(x \\right)}\"\n    assert latex(asec(x), 'inv_trig_style': 'power') == r\"\\csc^{-1}{\\left(x \\right)}\"\n    assert latex(acsc(x)) == r\"\\sec^{{-1}}{\\left(x \\right)}\"\n    assert latex(acsc(x), '", "def test_mathieu_functions():\n    from sympy.abc import mu\n    expr1 = mathieu_c(mu, 1, 1)\n    expr2 = mathieu_s(mu, 1, 1)\n    expr3 = mathieu_c_prime(mu, 1, 1)\n    expr4 = mathieu_s_prime(mu, 1, 1)\n\n    assert latex(expr1) == r'C^{1}\\left(\\mu, 1, 1\\right)'\n    assert latex(expr2) == r'S^{1}\\left(\\mu, 1, 1\\right)'\n    assert latex(expr3) == r'{C}^{\\prime^{1}}\\left(\\mu, 1, 1\\right)'\n    assert latex(expr4) == r'{S}^{\\prime^{1}}\\left(\\mu, 1, 1\\right)'\n", "def test_latex_mathieusprime():\n    from sympy import latex, symbols\n    x, y, z = symbols('x y z')\n    expr = mathieusprime(x, z)\n    assert latex(expr) == r\"S'^{\\left( %s, %s \\right)}\\left(%s\\right)\" % (x, z, x)\n\n", "def test_latex_printer_numerator_dots():\n    assert LatexPrinter().doprint(Integral(1, 0, 1, s)) == r\"\\int_{0}^{1}\\, ds\"\n"], "sample_425": ["    def test_decimal_serializer(self):\n        decimal_value = decimal.Decimal('1.23')\n        serializer = serializer_factory(decimal_value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, 'Decimal(\"1.23\")')\n        self.assertEqual(imports, {'from decimal import Decimal'})\n", "    def test_simple_serializer_factory(self):\n        instance = DeconstructibleInstances()\n        serializer = serializer_factory(instance)\n        self.assertIsInstance(serializer, BaseSerializer)\n", "    def test_decimal_serializer(self):\n        decimal_value = decimal.Decimal('3.14')\n        serializer = serializer_factory(decimal_value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n        self.assertEqual(serializer.serialize(), ('\"3.14\"', {'from decimal import Decimal'}))\n", "    def test_inheritance(self):\n        with self.assertRaises(NotImplementedError):\n            BaseSerializer().serialize()\n", "    def test_serializer_factory_default_base_serializer(self):\n        with self.assertRaises(ValueError):\n            serializer_factory(1)\n"], "sample_94": ["    def test_create_superuser_with_no_input(self):\n        with self.assertRaises(CommandError):\n            call_command('createsuperuser', '--noinput', '--username', 'alice', '--email', 'alice@example.com', '--date_of_birth', '2000-01-01', '--first_name', 'Alice', database='default')\n", "    def test_create_superuser_no_input(self):\n        # Test creating a superuser with no input and password from environment\n        os.environ['DJANGO_SUPERUSER_PASSWORD'] = '123'\n        os.environ['DJANGO_SUPERUSER_EMAIL'] = 'bob@example.com'\n        os.environ['DJANGO_SUPERUSER_DATE_OF_BIRTH'] = '2000-01-01'\n        os.environ['DJANGO_SUPERUSER_FIRST_NAME'] = 'Bob'\n        self.assertEqual(User.objects.count(), 0)\n        call_command('createsuperuser', 'noinput', database='default')\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='bob')\n        self.assertTrue(user.is_staff)\n        self.assertTrue(user.is_superuser)\n        self.assertEqual(user.email, 'bob@example.com')\n        self.assertEqual(user.date_of_birth, date(2000, 1, 1))\n        self.assertEqual(user.first_name, 'Bob')\n", "    def test_create_superuser_with_password_validation(self):\n        with self.assertRaises(exceptions.ValidationError):\n            call_command('createsuperuser', '--noinput', '--database=nonexistent_database', username='alice')\n            # Password validation should fail for nonexistent database\n", "    def test_create_superuser_with_required_fields(self):\n        # Test that all required fields are prompted for and used when creating a superuser.\n        with mock_inputs({\n            'email': 'example@example.com',\n            'date_of_birth': '1990-01-01',\n            'first_name': 'John',\n            'bypass': 'y',\n        }) as inputs:\n            call_command('createsuperuser', stdout=StringIO())\n            # Check that the superuser was created with the expected fields.\n            superuser = User.objects.get(username='alice')\n            self.assertEqual(superuser.email, 'example@example.com')\n            self.assertEqual(superuser.date_of_birth, date(1990, 1, 1))\n            self.assertEqual(superuser.first_name, 'John')\n", "    def test_noinput_without_required_fields(self):\n        with self.assertRaises(CommandError):\n            mock_inputs({'username': 'alice', 'email': ''})(\n                call_command('createsuperuser', 'noinput', 'alice')\n            )\n"], "sample_118": ["    def test_year_lookup_exact(self):\n        article = Article.objects.create(\n            headline='Article 8',\n            pub_date=datetime(2005, 7, 28),\n            author=self.au1,\n            slug='a8',\n        )\n        self.assertTrue(YearExact('pub_date', 2005).resolve_expression(self.query).process_sql(self.connection, article))\n        self.assertFalse(YearExact('pub_date', 2006).resolve_expression(self.query).process_sql(self.connection, article))\n", "    def setUpTestData(cls):\n        super().setUpTestData()\n\n        # Create a few Seasons.\n        cls.s1 = Season.objects.create(year=2005)\n        cls.s2 = Season.objects.create(year=2004)\n", "    def test_year_exact(self):\n        a = Article.objects.create(\n            headline='Article 8',\n            pub_date=datetime(2005, 7, 26),\n            author=self.au1,\n            slug='a8',\n        )\n        a_year = datetime(2005, 7, 26).year\n        results = Article.objects.filter(pub_date__year__exact=a_year)\n        self.assertEqual(list(results), [a])\n", "    def test_year_gt_lookup(self):\n        start, finish = datetime(2005, 7, 26).year, datetime(2005, 7, 27).year\n        article = Article.objects.create(pub_date=datetime(2005, 8, 1))\n        result = Article.objects.filter(pub_date__year__gt=start).exists()\n        self.assertTrue(result)\n        result = Article.objects.filter(pub_date__year__gt=finish).exists()\n        self.assertFalse(result)\n", "    def test_year_lookup_with_different_date_fields(self):\n        class DateFieldYearLookup(YearLookup):\n                return connection.ops.year_lookup_bounds_for_date_field(year)\n\n        lookup = DateFieldYearLookup(lhs=OuterRef('pub_date'), rhs=2005)\n        qs = Article.objects.filter(lookup)\n        self.assertEqual(list(qs), [self.a1, self.a2, self.a3, self.a4])\n\n        lookup = DateFieldYearLookup(lhs=OuterRef('pub_date'), rhs=2005, lhs=output_field=DateField())\n        qs = Article.objects.filter(lookup)\n        self.assertEqual(list(qs), [self.a1, self.a2, self.a3, self.a4])\n"], "sample_244": ["    def test_management_form_field_names(self):\n        formset = ChoiceFormSet(auto_id=False, prefix='choices')\n        self.assertEqual(formset.management_form.base_fields.keys(), [\n            'TOTAL_FORMS', 'INITIAL_FORMS', 'MIN_NUM_FORMS', 'MAX_NUM_FORMS'\n        ])\n", "    def test_form_kwargs(self):\n        class CustomForm(Form):\n                self.custom_arg = custom_arg\n                super().__init__(*args, **kwargs)\n\n        CustomFormSet = formset_factory(CustomForm)\n        formset = CustomFormSet(form_kwargs={'custom_arg': 'custom_value'})\n        self.assertEqual(formset.forms[0].custom_arg, 'custom_value')\n", "    def test_management_form_is_bound(self):\n        formset = ChoiceFormSet(prefix='test_prefix', auto_id=False)\n        self.assertTrue(formset.management_form.is_bound)\n", "    def test_delete_formset(self):\n        formset = formset_factory(CustomKwargForm, formset=BaseFormSet, can_delete=True, extra=2)\n        formset.forms[0].fields['custom_kwarg'] = IntegerField(required=True)\n        self.assertTrue(formset.can_delete)\n", "    def test_ManagementForm_clean(self):\n        # Test that TOTAL_FORM_COUNT, INITIAL_FORM_COUNT, and MIN_NUM_FORM_COUNT are cleaned.\n        data = {\n            'TOTAL_FORMS': '10',\n            'INITIAL_FORMS': '5',\n            'MIN_NUM_FORMS': '1',\n        }\n        form = ManagementForm(data)\n        self.assertEqual(form.clean(), {'TOTAL_FORM_COUNT': 10, 'INITIAL_FORM_COUNT': 5, 'MIN_NUM_FORMS': 1})\n\n        # Test that the form is cleaned even when there are errors.\n        data = {\n            'TOTAL_FORMS': 'abc',\n            'INITIAL_FORMS': '5',\n            'MIN_NUM_FORMS': '1',\n        }\n        form = ManagementForm(data)\n        self.assertEqual(form.clean(), {'TOTAL_FORM_COUNT': 0, 'INITIAL_FORM_COUNT': 5, 'MIN_NUM_FORMS': 1})\n"], "sample_213": ["    def test_upload_to_starts_with_slash(self):\n        from django.core.files.storage import default_storage\n        field = FileField(upload_to='/test')\n        self.assertEqual(field._check_upload_to(), [\n            checks.Error(\n                \"%s's 'upload_to' argument must be a relative path, not an \"\n                \"absolute path.\" % field.__class__.__name__,\n                obj=field,\n                id='fields.E202',\n                hint='Remove the leading slash.',\n            )\n        ])\n", "    def test_init_with_instance_and_field(self):\n        class Model(models.Model):\n            file = FileField(upload_to='uploads/')\n\n        model = Model()\n        field = FileField(upload_to='uploads/')\n        file = FieldFile(model, field, 'test.txt')\n        self.assertEqual(file.instance, model)\n        self.assertEqual(file.field, field)\n        self.assertEqual(file.name, 'test.txt')\n", "    def test_field_file_init(self):\n        field = FileField(upload_to='test')\n        instance = object()\n        field_file = field.attr_class(instance, field, 'test.txt')\n        self.assertEqual(field_file.instance, instance)\n        self.assertEqual(field_file.field, field)\n        self.assertEqual(field_file.storage, field.storage)\n        self.assertEqual(field_file.name, 'test.txt')\n", "    def test_image_field_filename_generation(self):\n        from .models import ImageModel\n\n        storage = Storage()\n        storage.save('path/to/image.jpg', b'file contents')\n        file = storage.open('path/to/image.jpg')\n        field = ImageField(upload_to='path/to/')\n        instance = ImageModel()\n        filename = field.generate_filename(instance, 'image.jpg')\n        self.assertEqual(filename, 'path/to/image.jpg')\n        self.assertEqual(storage.path(filename), 'path/to/image.jpg')\n", "    def test_deconstruct_preserves_max_length(self):\n        field = FileField(max_length=50)\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(kwargs.get('max_length'), 50)\n"], "sample_157": ["    def test_create_test_db_with_autoclobber(self):\n        connection_copy = get_connection_copy()\n        creation = BaseDatabaseCreation(connection_copy)\n        with mock.patch('sys.stderr.write') as mock_write:\n            creation._create_test_db(verbosity=1, autoclobber=True, keepdb=False)\n            mock_write.assert_called_once()\n", "    def test_serialize_deserialize(self):\n        test_connection = get_connection_copy()\n        creation = BaseDatabaseCreation(test_connection)\n        original_data = creation.serialize_db_to_string()\n        creation.deserialize_db_from_string(original_data)\n        self.assertEqual(original_data, creation.serialize_db_to_string())\n", "    def test_clone_test_db(self):\n        # Arrange\n        original_settings = copy.deepcopy(connections[DEFAULT_DB_ALIAS].settings_dict)\n        original_connection = get_connection_copy()\n        test_connection = get_connection_copy()\n        test_connection.settings_dict = copy.deepcopy(original_settings)\n        test_connection.settings_dict['NAME'] = 'original_db'\n        test_connection.settings_dict['TEST']['NAME'] = 'original_test_db'\n\n        # Act\n        test_connection._clone_test_db('test_suffix')\n\n        # Assert\n        self.assertIn('test_suffix', test_connection.settings_dict['NAME'])\n        self.assertIn('test_suffix', test_connection.settings_dict['TEST']['NAME'])\n        self.assertNotEqual(original_settings, test_connection.settings_dict)\n", "    def test_clone_test_db(self):\n        connection = get_connection_copy()\n        creation = BaseDatabaseCreation(connection)\n        creation.clone_test_db('clone_suffix')\n        self.assertEqual(\n            creation.get_test_db_clone_settings('clone_suffix'),\n            {'ENGINE': 'django.db.backends.sqlite3',\n             'NAME': 'test_test_clone_suffix_clone_suffix',\n             'TEST': {'MIGRATE': True, 'CHARSET': 'utf8mb4'},\n             'HOST': '',\n             'PORT': '',\n             'USER': '',\n             'PASSWORD': '',\n             'ATOMIC_REQUESTS': True,\n             'ATOMIC_CONTAINERS': True}\n        )\n", "    def test_clone_test_db(self):\n        connection_copy = get_connection_copy()\n        database_creation = BaseDatabaseCreation(connection_copy)\n        with mock.patch.object(database_creation, '_clone_test_db') as mock_clone:\n            database_creation.clone_test_db(suffix='1', keepdb=False)\n            mock_clone.assert_called_once()\n"], "sample_444": ["    def test_manifest_files_storage(self):\n        self.assertEqual(\n            storage.staticfiles_storage.manifest_name,\n            ManifestFilesMixin.manifest_name,\n        )\n        self.assertEqual(\n            storage.staticfiles_storage.manifest_version,\n            ManifestFilesMixin.manifest_version,\n        )\n        self.assertTrue(storage.staticfiles_storage.manifest_strict)\n        self.assertFalse(storage.staticfiles_storage.keep_intermediate_files)\n", "    def test_save_manifest(self):\n        # Arrange\n        temp_dir = tempfile.mkdtemp()\n        manifest_name = \"staticfiles.json\"\n        self.settings = self.mk_settings(STATIC_ROOT=temp_dir, STATIC_URL=\"/static/\")\n\n        # Act\n        manifest_storage = storage.ManifestStaticFilesStorage(manifest_storage=temp_dir)\n\n        # Assert\n        self.assertTrue(os.path.exists(os.path.join(temp_dir, manifest_name)))\n", "    def test_manifest_version(self):\n        \"\"\"\n        Test the manifest version.\n        \"\"\"\n        mixin = ManifestFilesMixin()\n        self.assertEqual(mixin.manifest_version, \"1.1\")\n", "    def test_stored_name_hashes_empty_file(self):\n        \"\"\"\n        Test that the hashed storage backend correctly returns the hashed name\n        for a file with an empty content.\n        \"\"\"\n        path = \"static/empty.txt\"\n        self.hashed_file_path(self, path)\n        storage = staticfiles_storage\n        content = b\"\"\n\n        with mock.patch.object(storage, \"file_hash\") as file_hash:\n            file_hash.return_value = \"abc123\"\n            hashed_name = storage.hashed_name(path, content)\n            self.assertEqual(hashed_name, \"static/abc123.txt\")\n            file_hash.assert_called_once_with(path, content)\n", "    def test_hash_key_hash_path_without_hash(self):\n        test = TestHashedFiles()\n        storage = storage.staticfiles_storage\n        # test hash key for path without hash\n        expected_hash_key = \"path/to/file.css\"\n        self.assertEqual(storage.hash_key(\"path/to/file.css\"), expected_hash_key)\n        # test hash key for path with hash\n        expected_hash_key = \"path/to/file.12345678.css\"\n        self.assertEqual(\n            storage.hash_key(\"path/to/file.12345678.css\"), expected_hash_key\n        )\n        # test hash key for path with multiple hashes\n        expected_hash_key = \"path/to/file.12345678.23456789.css\"\n        self.assertEqual(\n            storage.hash_key(\"path/to/file.12345678.23456789.css\"), expected_hash_key\n        )\n"], "sample_535": ["    def test_table(self):\n        fig, ax = plt.subplots()\n        table = Table(ax)\n        table.add_cell(0, 0, 'Hello, World!')\n        ax.add_table(table)\n        fig.canvas.draw()\n        fig.canvas.flush_events()\n        assert table.get_celld()[0, 0].get_text().get_text() == 'Hello, World!'\n", "def test_table_auto_font_size():\n    fig, ax = plt.subplots()\n    table = Table(ax, loc='bottom')\n    cell = table.add_cell(0, 0, text='Hello, World!', fontsize=20)\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    assert cell.get_fontsize() == 10\n    assert table.get_celld()[0, 0].get_fontsize() == 10\n", "def test_auto_set_font_size():\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    cell = table.add_cell(0, 0, text='Hello, world!', fontsize=20)\n    table.auto_set_font_size(value=False)\n    cell.set_fontsize(20)\n    table.auto_set_font_size(value=False)\n    assert cell.get_fontsize() == 20\n", "def test_table_edges():\n    # Test that table_edges is properly set for newly added cells\n    fig, ax = plt.subplots()\n    table = Table(ax)\n    cell = table.add_cell(0, 0, text='Hello')\n    assert cell.visible_edges == 'closed'\n\n    # Test that table_edges can be changed\n    table.edges = 'horizontal'\n    assert cell.visible_edges == 'horizontal'\n\n    # Test that table_edges is inherited by newly added cells\n    cell2 = table.add_cell(1, 1, text='World')\n    assert cell2.visible_edges == 'horizontal'\n\n    # Test that table_edges can be changed for individual cells\n    cell2.visible_edges = 'vertical'\n    assert cell2.visible_edges == 'vertical'\n\n    # Test that table_edges can be set to invalid values\n    with pytest.raises(ValueError):\n        table.edges = 'invalid'\n    with pytest.raises(ValueError):\n        cell.visible_edges = 'invalid'\n\n    # Test that table_edges can be set to valid aliases\n    table.edges = 'open'\n    assert table.edges == 'open'\n    table.edges = 'closed'\n    assert table.edges == 'closed'\n    table.edges = 'horizontal'\n    assert table.edges == 'horizontal'\n    table.edges = 'vertical'\n    assert table.edges == 'vertical'\n\n    # Test that table_edges can be set to a string of edges\n    table.edges = 'BT'\n    assert table.edges == 'BT'\n    table.edges = 'TRL'\n    assert table.edges == 'TRL'\n    table.edges = 'BRTL'\n    assert table.edges == 'BRTL'\n\n    # Test that table_edges can be set to a substring of 'BRTL'\n    table.edges = 'TL'\n    assert table.edges == 'TL'\n    table.edges = 'BR'\n    assert table.edges == 'BR'\n\n    # Test that table_edges can be set to None\n    table.edges = None\n    assert table.edges == 'closed'\n\n    # Test that table_edges is properly set for cells that are not added to the table\n    cell3 = CustomCell((0, 0), text='Hello')\n    assert cell3.visible_edges == 'closed'\n\n    # Test that table_edges is properly set for cells with negative indices\n    table.add_cell(-1, 0, text='Hello')\n    cell4 = table[-1, 0]\n    assert cell4.visible_edges == 'closed'\n\n    # Test that table_edges is properly set for", "def test_table_edges():\n    fig, ax = plt.subplots()\n    table = Table(ax, loc='bottom')\n\n    # Test default edges\n    cell = CustomCell([0, 0], 1, 1, visible_edges='closed')\n    table[row, col] = cell\n    cell.visible_edges = 'horizontal'\n    cell.draw(fig.canvas.renderer)\n\n    # Test invalid edge string\n    cell.visible_edges = 'invalid'\n    with pytest.raises(ValueError):\n        cell.draw(fig.canvas.renderer)\n\n    # Test edge aliases\n    cell.visible_edges = 'open'\n    cell.visible_edges = 'closed'\n\n    # Test edge string with single characters\n    cell.visible_edges = 'B'\n    cell.visible_edges = 'R'\n    cell.visible_edges = 'T'\n    cell.visible_edges = 'L'\n\n    # Test edge string with all characters\n    cell.visible_edges = 'BRTL'\n\n    # Test edge string with invalid characters\n    with pytest.raises(ValueError):\n        cell.visible_edges = 'BRX'\n"], "sample_458": ["    def test_floatformat_positive_number_with_one_decimal_place(self):\n        self.assertEqual(self.engine.render_template(\"floatformat01\", {\"a\": 34.23234, \"b\": 34.23234}), \"34.2 34.2\")\n", "    def test_floatformat_with_positive_precision(self):\n        self.render_template(\"floatformat01\", {\"a\": 34.23234, \"b\": 34.00000})\n", "    def test_floatformat01_with_positive_decimal_places(self):\n        self.assertEqual(self.engine.render(\"floatformat02\", {\"a\": 34.23234, \"b\": 34.23234}), \"34.2 34.2\")\n", "    def test_floatformat_specified_places(self):\n        context = self.setup_context({\n            \"a\": Decimal(\"34.23234\"),\n            \"b\": Decimal(\"34.00000\"),\n            \"c\": Decimal(\"34.26000\"),\n        })\n        rendered = self.engine.from_string(\n            self.context[\"floatformat02\"],\n            context\n        ).render()\n        self.assertEqual(\n            rendered,\n            \"34.2 34 34.3\",\n        )\n", "    def test_floatformat_with_negative_argument(self):\n        with translation.override('en'):\n            with self.settings(DEFAULT_DATE_FORMAT='YYYY-MM-DD', DEFAULT_TIME_FORMAT='HH:MM:SS'):\n                parser = Parser()\n                parser.compile_filter('floatformat')\n                try:\n                    parser.parse('a|floatformat:-2')\n                except template.TemplateSyntaxError as e:\n                    self.fail(f\"Unexpected template syntax error: {e}\")\n"], "sample_658": ["    def test_repr_failure_missing_docstring(self):\n        # Arrange\n        path = \"path/to/missing_docstring_module.py\"\n        parent = pytest.Item(\"module\")\n        dtest = doctest.DocTest(\"test\", \"module\", \"path/to/missing_docstring_module.py\", 1)\n        item = DoctestItem(\"test\", parent, runner=None, dtest=dtest)\n\n        # Act\n        item.dtest.lineno = None\n\n        # Assert\n        assert isinstance(item.repr_failure(ExceptionInfo(Exception())), ReprFailDoctest)\n", "    def test_repr_failure_multiple_failures(self):\n        doctest_module = DoctestModule.fromconf(self)\n        item = doctest_module._items[0]\n        failures = [doctest.DocTestFailure(test=item.dtest, example=item.dtest.examples[0], got=\"got\")]\n        failures.append(doctest.DocTestFailure(test=item.dtest, example=item.dtest.examples[1], got=\"got\"))\n        excinfo = ExceptionInfo(None, None, None, MultipleDoctestFailures(failures))\n        assert isinstance(item.repr_failure(excinfo), ReprFailDoctest)\n", "    def test_repr_failure(self, doctest_namespace):\n        # Create a doctest item with a failure\n        runner = _get_runner()\n        test = doctest.DocTest(\n            \"test\",\n            \"example\",\n            \"example\",\n            \"\"\"\n            >>> 1 + 2\n            3\n            >>> 1 + 2\n            4\n            \"\"\",\n            \"test\",\n            0,\n        )\n        doctest_item = DoctestItem(test.name, None, runner, test)\n        failures = [doctest.DocTestFailure(test, test.examples[0], \"Got 4\")]\n        doctest_item.runner.out = [failures[0]]\n\n        # Test repr_failure method\n        repr_info = doctest_item.repr_failure(ExceptionInfo(None, None, failures))\n        assert isinstance(repr_info, ReprFailDoctest)\n        assert len(repr_info.reprlocation_lines) == 1\n        repr_location, lines = repr_info.reprlocation_lines[0]\n        assert repr_location.filename == \"test\"\n        assert repr_location.lineno == 1\n        assert repr_location.message == \"DocTestFailure\"\n\n        # Test the repr output\n        output = repr_info.toterminal(None).strip()\n        lines = textwrap.dedent(output).splitlines()\n        assert len(lines) == 5\n        assert lines[0] == \">>> 1 + 2\"\n        assert lines[1] == \"3\"\n        assert lines[2] == \">>> 1 + 2\"\n        assert lines[3] == \"4\"\n        assert lines[4] == \"DocTestFailure\"\n", "    def test_doctestmodule_with_import_error(self, tmpdir, capsys):\n        # create a test file with a module that raises an ImportError\n        test_module = tmpdir.join(\"test_module.py\")\n        test_module.write(\"\"\"\n                import non_existent_module\n        \"\"\")\n        # run the test\n        pytest.main([\"-ra\", str(tmpdir)])\n        # check the captured output\n        captured = capsys.readouterr()\n        assert \"ImportError\" in captured.out\n", "def test_repr_failure_unexpected_exception():\n    \"\"\"Test repr failure of UnexpectedException.\"\"\"\n    class MockRunner:\n            self.out = []\n\n            failure = doctest.UnexpectedException(dtest, None, (Exception,))\n            out.append(failure)\n\n    runner = MockRunner()\n    item = DoctestItem(\"test_name\", None, runner, None)\n    item.fixture_request = FixtureRequest(item)\n    item.fixture_request._fillfixtures()\n    item.setup()\n    failure = doctest.UnexpectedException(item.dtest, None, (Exception,))\n    with pytest.raises(MultipleDoctestFailures) as excinfo:\n        item.runtest()\n    assert excinfo.value.failures == [failure]\n    assert (\n        item.repr_failure(excinfo)\n        == ReprFailDoctest([(ReprFileLocation(\"test_name.py\", None, \"UnexpectedException\"), [\n            \"EXAMPLE LOCATION UNKNOWN, not showing all tests of that example\",\n            \">>> x = 1\",\n            \"??? Exception\",\n            \"UNEXPECTED EXCEPTION: <class 'Exception'>\",\n            \"Traceback (most recent call last):\",\n            \"  File \\\"<string>\",  # Changed from 'test_name.py'\n            \"  File \\\"<string>\",  # Changed from 'test_name.py'\n            \"\",\n            \"Exception\",\n        ])]\n    )\n"], "sample_1186": ["def test_NDimArray_diff():\n    from sympy import ImmutableDenseNDimArray, diff, sin, cos\n    from sympy.abc import x, y\n\n    a = ImmutableDenseNDimArray([[x, y], [sin(x), cos(x)]])\n    assert diff(a, x) == ImmutableDenseNDimArray([[1, 0], [cos(x), -sin(x)]])\n", "def test_NDimArray_conjugate_transpose():\n    from sympy.tensor.array import NDimArray\n    from sympy.tensor.array.dense_ndim_array import ImmutableDenseNDimArray\n    from sympy.tensor.array.sparse_ndim_array import ImmutableSparseNDimArray\n\n    a = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    b = ImmutableSparseNDimArray({(0, 0): 5, (1, 1): 6})\n    c = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n\n    assert a.conjugate().shape == a.shape\n    assert b.conjugate().shape == b.shape\n    assert c.conjugate().shape == c.shape\n\n    assert a.conjugate() == a.conjugate()\n    assert b.conjugate() == b.conjugate()\n    assert c.conjugate() == c.conjugate()\n\n    assert a.transpose().shape == (2, 2)\n    assert b.transpose().shape == (2, 2)\n    assert c.transpose().shape == (2, 2)\n\n    assert a.transpose() == a.transpose()\n    assert b.transpose() == b.transpose()\n    assert c.transpose() == c.transpose()\n", "def test_NDimArray_conjugate():\n    from sympy.tensor.array import Array\n    from sympy import I\n\n    # Test conjugate of a single element array\n    a = ImmutableDenseNDimArray([1+I])\n    assert a.conjugate() == ImmutableDenseNDimArray([1-I])\n\n    # Test conjugate of a multi-dimensional array\n    a = ImmutableDenseNDimArray([[1+I, 2+I], [3+I, 4+I]])\n    assert a.conjugate() == ImmutableDenseNDimArray([[1-I, 2-I], [3-I, 4-I]])\n\n    # Test conjugate of an array with non-commutative elements\n    a = ImmutableDenseNDimArray([[1+I, 2+I], [3+I, 4+I]])\n    b = ImmutableDenseNDimArray([[I, 2+I], [3+I, I]])\n    assert a.conjugate() == b.conjugate()\n\n    # Test conjugate of a zero-sized array\n    a = ImmutableDenseNDimArray([])\n    assert a.conjugate() == ImmutableDenseNDimArray([])\n\n    # Test conjugate of an empty array\n    a = ImmutableDenseNDimArray()\n    assert a.conjugate() == ImmutableDenseNDimArray()\n", "def test_array_rank_and_shape():\n    # Test rank and shape properties\n    a = ImmutableDenseNDimArray([1, 2, 3], (2, 3))\n    assert a.rank() == 2\n    assert a.shape == (2, 3)\n\n    b = ImmutableDenseNDimArray([1, 2, 3, 4, 5], (2, 3))\n    assert b.rank() == 2\n    assert b.shape == (2, 3)\n\n    c = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    assert c.rank() == 2\n    assert c.shape == (2, 3)\n\n    # Test rank and shape for arrays with different element kinds\n    a = ImmutableDenseNDimArray([1, 2, 3], (2, 3), element_kind=NumberKind)\n    assert a.rank() == 2\n    assert a.shape == (2, 3)\n\n    b = ImmutableDenseNDimArray([True, False, True, False], (2, 3), element_kind=bool)\n    assert b.rank() == 2\n    assert b.shape == (2, 3)\n\n    # Test rank and shape for multidimensional arrays\n    d = ImmutableDenseNDimArray([[1, 2], [3, 4]], (2, 2))\n    assert d.rank() == 2\n    assert d.shape == (2, 2)\n\n    e = ImmutableDenseNDimArray([[1, 2], [3, 4], [5, 6]], (2, 2))\n    assert e.rank() == 2\n    assert e.shape == (2, 2)\n", "def test_array_subtraction():\n    from sympy.tensor.array import NDimArray\n\n    a = NDimArray([1, 2, 3], (2, 2))\n    b = NDimArray([3, 2, 1], (2, 2))\n    assert (a - b).tolist() == [[-2, -1], [1, 2]]\n\n    # Test error for shape mismatch\n    c = NDimArray([1, 2, 3], (3, 3))\n    with raises(ValueError):\n        a - c\n\n    # Test array subtration with scalar\n    d = NDimArray([1, 2, 3], (2, 2))\n    e = NDimArray([3, 3, 3], (2, 2))\n    assert (d - 2).tolist() == [[-1, 0], [1, 1]]\n    with raises(TypeError):\n        d - 2.5\n\n    # Test array subtration with mixed types\n    f = NDimArray([1, 2, 3], (2, 2))\n    g = NDimArray([2, 3, 4], (2, 2))\n    with raises(TypeError):\n        f - 2\n    with raises(TypeError):\n        2 - f\n\n    # Test subtration with NDimArray instance of different types\n    h = ImmutableDenseNDimArray([1, 2, 3], (2, 2))\n    i = MutableDenseNDimArray([3, 2, 1], (2, 2))\n    with raises(TypeError):\n        h - i\n\n    j = ImmutableSparseNDimArray({0: 1, 2: 2}, (2, 2))\n    k = MutableSparseNDimArray({0: 3, 2: 4}, (2, 2))\n    with raises(TypeError):\n        j - k\n\n    # Test subtration of empty arrays\n    l = ImmutableDenseNDimArray([1, 2], ())\n    m = ImmutableDenseNDimArray([3, 4], ())\n    with raises(ValueError):\n        l - m\n"], "sample_1008": ["def test_coordinate_sym_equality():\n    from sympy.physics.vector import ReferenceFrame\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Body', [pi, 0, 0], 'XYZ')\n    assert A[0] == CoordinateSym('A_x', A, 0)\n    assert A[1] == CoordinateSym('A_y', A, 1)\n    assert A[2] == CoordinateSym('A_z', A, 2)\n", "def test_orientnew_axis():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/2, N.x])\n    assert A.x.express(N) == -N.y\n    assert A.y.express(N) == N.x\n    assert A.z.express(N) == N.z\n    assert A.orient('Axis', [pi/2, N.x])\n", "def test_orientnew_without_variables():\n    from sympy import symbols\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [symbols('theta'), N.x])\n    assert A.x == A.x\n    assert A.y == A.y\n    assert A.z == A.z\n    assert A.orient('DCM', eye(3))\n    assert A == N\n    assert A == N.orientnew('B', 'Axis', [symbols('theta'), N.x])\n    assert A == N.orientnew('B', 'Axis', [symbols('theta'), N.y])\n    assert A == N.orientnew('B', 'Axis', [symbols('theta'), N.z])\n", "def test_orientnew_redundant_definitions():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/2, N.x])\n    B = N.orientnew('A', 'Axis', [pi/2, N.x])\n    assert A == B\n", "def test_orientnew():\n    from sympy import symbols, cos, sin, pi, diff, tan\n    q = symbols('q')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q, N.z])\n\n    # Test for angular velocity\n    w1 = A.ang_vel_in(N)\n    w2 = A.ang_vel_in(N).diff(N._t)\n    assert w1 == w2\n\n    # Test for angular acceleration\n    a1 = A.ang_acc_in(N)\n    a2 = A.ang_acc_in(N).diff(N._t)\n    assert a1 == a2\n\n    # Test for orientation matrix\n    dcm = A.dcm(N)\n    assert dcm.det() == 1\n\n    # Test for rotation matrix\n    rot_matrix = A._x.as_expr() * A.x.as_expr().T + A._y.as_expr() * A.y.as_expr().T + A._z.as_expr() * A.z.as_expr().T\n    assert dcm == rot_matrix\n\n    # Test for variable mapping\n    var_map = A.variable_map(N)\n    assert var_map[A._x] == A._x\n    assert var_map[A._y] == cos(q) * A._x - sin(q) * A._y\n    assert var_map[A._z] == sin(q) * A._x + cos(q) * A._y\n"], "sample_795": ["    def test_check_estimator_sparse_input_with_dict(self):\n        class SparseDictEstimator(BaseEstimator):\n                self.X = None\n                self.y = None\n\n                self.X = X\n                self.y = y\n                return self\n\n                X = check_array(X)\n                return np.ones(X.shape[0])\n\n        estimator = SparseDictEstimator()\n        rng = np.random.RandomState(0)\n        X = rng.rand(10, 5)\n        X[X < .8] = 0\n        X = sp.csr_matrix(X)\n        y = np.ones(10)\n        y = sp.csr_matrix(y)\n\n        with assert_warns(UserWarning):\n            check_estimator(estimator, X=X, y=y)\n", "    def fit(self, X, y):\n        self.coef_ = np.ones(X.shape[1])\n        return self\n", "    def test_not_fitted_error_on_predict(self):\n        e = CorrectNotFittedErrorClassifier()\n        with self.assertRaises(CorrectNotFittedError):\n            e.predict(np.array([[1, 2], [3, 4]]))\n", "    def __init__(self, p=0):\n        self.p = p\n", "    def test_correct_not_fitted_error(self):\n        e = CorrectNotFittedErrorClassifier()\n        self.assertRaises(CorrectNotFittedError, e.predict, np.array([[1, 2]]))\n"], "sample_276": ["    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_template_exists(self):\n        self.client.force_login(self.superuser)\n        template_name = 'some_template.html'\n        response = self.client.get(reverse('admin_doc:template_detail', kwargs={'template': template_name}))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, 'admin_doc/template_detail.html')\n        self.assertIn(template_name, response.context_data['templates'][0]['file'].name)\n", "    def test_bookmarklets_view_accessible_for_staff(self):\n        response = self.client.get(reverse('admin:admin_doc:bookmarklets'))\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(response.context['request'].user.is_staff)\n", "    def test_view_detail_context_data(self):\n        view_func = views.ViewDetailView._get_view_func(views.ViewIndexView)\n        view = \"view_index\"\n        response = self.client.get(reverse(\"admin:admin_docs:view_detail\", args=(view,)))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn(view, response.context_data)\n        self.assertIn(\"name\", response.context_data)\n        self.assertIn(\"summary\", response.context_data)\n        self.assertIn(\"body\", response.context_data)\n        self.assertIn(\"meta\", response.context_data)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n"], "sample_904": ["def test_envvar():\n    env = mock.Mock(spec='sphinx.environment.BuildEnvironment')\n    env.get_domain.return_value = StandardDomain(env)\n\n    domain = StandardDomain(env)\n    envvar = EnvVar(env)\n    node = envvar.run()[0]\n    assert node['ids'] == ['envvar-python']\n    assert node['names'] == ['python']\n    assert node['reftarget'] == 'python'\n    assert node['std:program'] is None\n", "def test_GlossaryDirective():\n    env = mock.Mock(spec='sphinx.environment.BuildEnvironment')\n    env.docname = 'test'\n    env.ref_context = {}\n    env.app = mock.Mock(spec='sphinx.application.Sphinx')\n    env.titles = {'test': 'Test'}\n    env.all_docs = ['test']\n\n    env.get_doctree.return_value = mock.Mock()\n\n    directive = Glossary(env, source='test', current_node=nodes.document())\n    directive.state.document = nodes.document()\n    directive.state.document.nameids = {'name': 'name'}\n    directive.state.document.nameids = {'name': 'name'}\n    directive.state.nested_parse = mock.Mock()\n    directive.state.inline_text = mock.Mock()\n    directive.content = [nodes.Text('term1'), nodes.Text('term2')]\n\n    result = directive.run()\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.definition_list)\n\n    env.get_doctree.return_value.ids.assert_called_once()\n    directive.state.nested_parse.assert_called_once()\n    directive.state.inline_text.assert_called_once()\n", "def test_env_var_xref_role():\n    # Test the EnvVarXRefRole class\n    env = mock.Mock()\n    env.ref_context.return_value = 'program'\n    parser = restructuredtext.Parser()\n    node = parser.parse('.. envvar:: MY_VAR')\n\n    env_var_role = EnvVarXRefRole()\n    result = env_var_role.result_nodes(None, env, node, True)\n    assert len(result) == 2\n    assert isinstance(result[0], index)\n    assert isinstance(result[1], nodes.target)\n    assert result[1]['ids'] == ['index-MY_VAR']\n    assert result[1]['refuri'] == 'index-MY_VAR'\n\n    # Test that EnvVarXRefRole adds an index entry\n    env_var_role = EnvVarXRefRole()\n    result = env_var_role.result_nodes(None, env, node, True)\n    assert len(result[0]['entries']) == 2\n    assert result[0]['entries'][0][1] == 'MY_VAR'\n    assert result[0]['entries'][1][1] == 'environment variable; MY_VAR'\n", "def test_STD_domain_resolve_option_xref():\n    env = mock.Mock(spec='sphinx.environment.BuildEnvironment')\n    env.docname = 'docname'\n    env.all_docs = ['docname']\n    env.titles = {'docname': 'doc title'}\n\n    node = pending_xref('reftarget', 'refdomain', 'reftarget')\n    contnode = nodes.Text('caption')\n    builder = mock.Mock(spec='sphinx.builders.Builder')\n    builder.name = 'latex'\n    builder.get_relative_uri.return_value = 'uri'\n\n    domain = StandardDomain()\n    domain.progoptions = {('program', 'option'): ('docname', 'labelid')}\n    result = domain.resolve_xref(env, env.docname, builder, 'option', 'reftarget', node, contnode)\n    expected = make_refnode(builder, env.docname, 'docname', 'labelid', contnode)\n    assert result == expected\n\n    # test handling of subcommands\n    node = pending_xref('subcommand reftarget', 'refdomain', 'reftarget')\n    result = domain.resolve_xref(env, env.docname, builder, 'option', 'reftarget', node, contnode)\n    expected = make_refnode(builder, env.docname, 'docname', 'labelid', contnode)\n    assert result == expected\n\n    # test case where progname is None\n    domain.progoptions = {('None', 'option'): ('docname', 'labelid')}\n    result = domain.resolve_xref(env, env.docname, builder, 'option', 'option', node, contnode)\n    expected = make_refnode(builder, env.docname, 'docname', 'labelid', contnode)\n    assert result == expected\n\n    # test case where option is not found\n    del domain.progoptions[('program', 'option')]\n    result = domain.resolve_xref(env, env.docname, builder, 'option', 'reftarget', node, contnode)\n    assert result is None\n\n    # test case where target is empty\n    node = pending_xref('', 'refdomain', 'reftarget')\n    result = domain.resolve_xref(env, env.docname, builder, 'option', 'reftarget', node, contnode)\n    assert result is None\n\n    # test case where progname is not found\n    domain.progoptions = {('program', 'other'):", "    def test_envvar(self, envvar, app, setup_dir):\n        env = app.builder.env\n        node = nodes.document()\n        env.state.document = node\n        env.state.document.settings.in_document = True\n\n        directive = EnvVar(env, envvar)\n        directive.run()\n\n        assert env.domaindata['std']['envvars'] == {'VARIABLE': ('index-environment-variable', 'envvar')}\n\n        assert_node(node, 'envvar', 'VARIABLE', 'envvar', 'index-environment-variable')\n        assert 'std:envvar' in node.get('roles')\n\n        env.domaindata['std']['envvars']['VARIABLE'] = ('index-environment-variable', 'envvar')\n\n        # test with duplicate\n        directive = EnvVar(env, envvar)\n        directive.run()\n\n        assert env.domaindata['std']['envvars'] == {'VARIABLE': ('index-environment-variable', 'envvar')}\n        assert len(node['roles']) == 1\n\n        # test with invalid envvar\n        directive = EnvVar(env, 'invalid-envvar')\n        directive.run()\n\n        assert env.domaindata['std']['envvars'] == {'VARIABLE': ('index-environment-variable', 'envvar')}\n        assert len(node['roles']) == 1\n"], "sample_579": ["    def test_clustermap_square(self):\n        df = self.rs.rand(10, 10)\n        clustermap(df, square=True)\n", "    def test_dendrogram_ratio(self):\n        data = self.rs.rand(20, 20)\n        grid = mat.clustermap(data, dendrogram_ratio=0.5)\n        assert grid.gs.height_ratios == [0.9, 0.1]\n        assert grid.gs.width_ratios == [1]\n", "    def test_init(self):\n        data = pd.DataFrame(self.x)\n        plotter = mat._DendrogramPlotter(data, linkage=None, axis=1,\n                                         metric='euclidean',\n                                         method='average', label=True,\n                                         rotate=False)\n        assert plotter.data.shape == (20, 13)\n        assert plotter.array.shape == (20, 13)\n        assert plotter.linkage is None\n        assert plotter.axis == 1\n        assert plotter.label == True\n        assert plotter.rotate == False\n        assert plotter.dendrogram is None\n        assert plotter.reordered_ind is None\n", "    def test_HeatMapper_determine_cmap_params(self):\n        \"\"\"Test that the default cmap is correctly determined based on center\"\"\"\n        plotter = _HeatMapper(self.x_norm, vmin=None, vmax=None, cmap=None,\n                              center=None, robust=False, annot=False, fmt=\".2f\",\n                              annot_kws=None, cbar=False, cbar_kws=None,\n                              xticklabels=True, yticklabels=True, mask=None)\n\n        assert plotter.cmap.name == \"rocket\"\n\n        plotter = _HeatMapper(self.x_norm, vmin=None, vmax=None, cmap=None,\n                              center=0, robust=False, annot=False, fmt=\".2f\",\n                              annot_kws=None, cbar=False, cbar_kws=None,\n                              xticklabels=True, yticklabels=True, mask=None)\n\n        assert plotter.cmap.name == \"icefire\"\n", "    def test_plot(self):\n        # Test with all possible combinations of row and col cluster\n        for row_cluster in [True, False]:\n            for col_cluster in [True, False]:\n                df = self.rs.rand(10, 20)\n                grid = ClusterGrid(df)\n                with pytest.raises(RuntimeError):\n                    grid.plot(metric='average', method='average', colorbar_kws={},\n                              row_cluster=row_cluster, col_cluster=col_cluster,\n                              row_linkage=None, col_linkage=None)\n\n                grid.plot(metric='average', method='average', colorbar_kws={},\n                          row_cluster=row_cluster, col_cluster=col_cluster,\n                          row_linkage=None, col_linkage=None)\n"], "sample_120": ["    def test_decimal_serializer(self):\n        decimal_value = decimal.Decimal('10.5')\n        serializer = serializer_factory(decimal_value)\n        self.assertIsInstance(serializer, DecimalSerializer)\n        self.assertEqual(serializer.serialize()[0], 'decimal.Decimal(\"10.5\")')\n        self.assertEqual(serializer.serialize()[1], {'from decimal import Decimal'})\n", "    def test_serializer_factory_with_enum(self):\n        class Color(enum.Enum):\n            RED = 1\n            GREEN = 2\n            BLUE = 3\n\n        serializer = serializer_factory(Color.GREEN)\n        self.assertIsInstance(serializer, EnumSerializer)\n        self.assertEqual(serializer.serialize(), ('Color.RED', {'import enum'}))\n", "    def test_complex_deconstructable_instance(self):\n        class DeconstructableClass:\n                return 'module.name', ['arg1', 'arg2'], {'kwarg1': 'value1'}\n\n        serializer = serializer_factory(DeconstructableClass())\n        expected_imports = {'import module.name'}\n        expected_string = 'module.name(arg1, arg2, kwarg1=value1)'\n        self.assertEqual(serializer.serialize(), (expected_string, expected_imports))\n", "    def test_serializer_factory_value_is_string(self):\n        value = \"hello\"\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, BaseSimpleSerializer)\n        self.assertEqual(serializer.serialize(), ('\"hello\"', set()))\n", "    def test_serialization_of_custom_decimal_class(self):\n        # Create a custom decimal class with deconstruct method\n        class CustomDecimal(decimal.Decimal):\n                return 'myapp.mymodule.CustomDecimal', [str(self)], {}\n        \n        # Test serialization of the custom decimal class\n        serializer = serializer_factory(CustomDecimal(10))\n        self.assertEqual(serializer.serialize(), ('myapp.mymodule.CustomDecimal[\"10\"]', {'import decimal'}))\n"], "sample_950": ["def test_parse_annotation():\n    env = Mock()\n    annotation = \"Union[int, str]\"\n    result = _parse_annotation(annotation, env)\n    expected = [\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('Union'),\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    assert_node(result, expected)\n", "def test_handle_signature_pyfunction():\n    env = Mock()\n    signode = addnodes.desc_signature()\n    directive = PyFunction()\n    name_cls = ('f', 'function')\n    sig = \"def f(x: int, y: str) -> None\"\n    ret = directive.handle_signature(sig, signode)\n    assert ret == name_cls\n    assert signode.astext() == \"def f(x: int, y: str) -> None\"\n    assert signode['module'] == ''\n    assert signode['class'] == ''\n    assert signode['fullname'] == 'f'\n    assert signode['ids'] == []\n", "def test_handle_signature_function():\n    env = Mock()\n    env.ref_context = {'py:module': 'module', 'py:class': 'class'}\n    env.config.add_module_names = True\n    env.config.python_use_unqualified_type_names = False\n\n    parser = Mock()\n    parser.parse_args.return_value = ['function']\n    parser.get_option.return_value = 'module'\n    parser.env = env\n\n    directive = PyFunction(parser)\n    signode = desc_signature('', '')\n    directive.handle_signature('def foo(self, a: int, b: str) -> None', signode)\n    assert signode.astext() == 'def foo(self, a, b) -> None'\n", "def test_handle_signature():\n    # Test a class with the 'final' option\n    signode = desc_signature('final class MyClass')\n    sig = _parse_arglist('self, x, y', None)\n    signode += sig\n    obj = PyClasslike(None, None, None, None, None)\n    obj.handle_signature('final class MyClass(self, x, y)', signode)\n    assert signode.astext() == \"final class MyClass(self, x, y)\"\n\n    # Test a function with the 'async' option\n    signode = desc_signature('async def my_function(a, b)')\n    sig = _parse_arglist('a, b', None)\n    signode += sig\n    obj = PyFunction(None, None, None, None, None)\n    obj.handle_signature('async def my_function(a, b)', signode)\n    assert signode.astext() == \"async def my_function(a, b)\"\n\n    # Test a method with the 'abstractmethod' option\n    signode = desc_signature('abstract def my_method(self)')\n    sig = _parse_arglist('self', None)\n    signode += sig\n    obj = PyMethod(None, None, None, None, None)\n    obj.handle_signature('abstract def my_method(self)', signode)\n    assert signode.astext() == \"abstract def my_method(self)\"\n\n    # Test a property with the 'abstractmethod' option\n    signode = desc_signature('abstract @property my_property()')\n    obj = PyProperty(None, None, None, None, None)\n    obj.handle_signature('abstract @property my_property()', signode)\n    assert signode.astext() == \"abstract property my_property()\"\n", "def test_py_function_signature():\n    env = Mock()\n    env.ref_context['py:module'] = 'module'\n    env.ref_context['py:class'] = 'class'\n    env.config.add_module_names = True\n    signode = addnodes.desc_signature('class Foo():', '')\n    py_function = PyFunction()\n    py_function.handle_signature('class Foo():', signode)\n    assert parse('class Foo()') == \"class Foo\\n\"\n    assert parse('class module.Foo()') == \"class module.Foo\\n\"\n"], "sample_799": ["def test_cross_validate():\n    \"\"\"Test that cross_validate returns train scores when requested.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    estimator = LogisticRegression()\n    cv = ShuffleSplit(test_size=0.2, random_state=42)\n    scores = cross_validate(estimator, X, y, cv=cv, return_train_score=True)\n    assert scores['train_score'].shape == (5,)\n    assert scores['train_score'].shape == scores['test_score'].shape\n", "    def test_empty_dict(self):\n        from sklearn.model_selection._validation import validation_curve\n\n        estimator = LogisticRegression()\n        X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                                  n_redundant=0, random_state=1)\n        param_grid = {}\n        train_sizes, train_scores, test_scores = validation_curve(\n            estimator, X, y, param_name='n_jobs', param_range=param_grid,\n            cv=5, scoring='accuracy', verbose=0)\n        assert len(train_scores) == 1\n        assert len(test_scores) == 1\n", "def test_cross_validate_parameter_validation():\n    # Test that the default scorer is used when 'scoring' is None\n    estimator = SGDClassifier()\n    cv = KFold(n_splits=2, shuffle=True, random_state=42)\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                              random_state=42)\n    with pytest.raises(ValueError):\n        cross_validate(estimator, X, y, cv=cv, scoring=None, n_jobs=-1)\n    cv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=-1)\n    assert cv_results['test_score'] is not None\n\n", "def test_cross_val_score_on_classifier_with_sparse_inputs():\n    X = csr_matrix(np.ones((10, 2)))\n    y = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n\n    clf = LogisticRegression()\n    # check that it doesn't error\n    cross_val_score(clf, X, y)\n", "def test_cross_validate_partial_fit_supported():\n    \"\"\"Check that cross_validate works when estimator has a partial_fit method\"\"\"\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5)\n    est = SGDClassifier()\n    est.partial_fit(X[:20], y[:20])\n    est.partial_fit(X[20:40], y[20:40])\n    est.partial_fit(X[40:], y[40:])\n\n    # Use a function that supports partial_fit, i.e. SGDClassifier\n    # Use a function that does not support partial_fit, i.e. clone of SGDClassifier\n    est_no_partial_fit = clone(SGDClassifier())\n\n    cv = ShuffleSplit(n_splits=1, train_size=0.7)\n    cv_results = cross_validate(est, X, y, cv=cv, n_jobs=-1)\n\n    # test that fit_params is passed correctly\n    assert cv_results['estimator'][0].get_params()[\n        'max_iter'] == 1000\n\n    # test that partial_fit works correctly\n    assert cv_results['estimator'][0].partial_fit(X[:20], y[:20]).get_params()['max_iter'] == 500\n    assert cv_results['estimator'][0].partial_fit(X[20:40], y[20:40]).get_params()['max_iter'] == 500\n\n    # check that a clone that doesn't support partial_fit gives the\n    # correct error message\n    with assert_warns_message(FitFailedWarning,\n                             \"This estimator instance is not exactly identical \"\n                             \"to estimator because of the config of the \"\n                             \"ClonableUncloneableEstimator. Cloning it might \"\n                             \"fail. Use clone to create a new instance \"\n                             \"with an independent state, potentially a \"\n                             \"fit status\"):\n        cross_validate(est_no_partial_fit, X, y, cv=cv, n_jobs=-1)\n\n    # test that the clone does not support partial_fit\n    assert not hasattr(est_no_partial_fit, 'partial_fit')\n"], "sample_728": ["def test_make_spd_matrix_seed():\n    np.random.seed(0)\n    matrix = make_spd_matrix(n_dim=5)\n    np.testing.assert_allclose(matrix, np.dot(matrix.T, matrix))\n    assert np.all(np.linalg.eigvals(matrix) > 0)\n", "def test_make_low_rank_matrix_eigenvectors():\n    \"\"\"Check that the eigenvectors of a low rank matrix are approximately\n    equal to the ones of the structured signal part of the matrix.\"\"\"\n    n_samples = 100\n    n_features = 100\n    effective_rank = 10\n    tail_strength = 0.5\n    random_state = 0\n\n    X = make_low_rank_matrix(n_samples=n_samples,\n                             n_features=n_features,\n                             effective_rank=effective_rank,\n                             tail_strength=tail_strength,\n                             random_state=random_state)\n\n    # Compute the eigenvectors of the low rank matrix\n    _, eigs = np.linalg.eig(np.dot(X.T, X))\n    low_rank_eigs = eigs[:effective_rank]\n\n    # Compute the eigenvectors of the original matrix\n    _, eigs = np.linalg.eig(X)\n    all_eigs = eigs[:effective_rank]\n\n    # Check that the eigenvectors are approximately equal\n    for i in range(effective_rank):\n        assert_array_almost_equal(low_rank_eigs[:, i], all_eigs[:, i], decimal=2)\n", "def test_make_sparse_spd_matrix_norm_diag():\n    # test that setting norm_diag=True does not change the matrix\n    # when it's not sparse\n    matrix = make_spd_matrix(n_dim=10)\n    norm_matrix = make_sparse_spd_matrix(n_dim=10, norm_diag=True)\n    assert_array_almost_equal(matrix, norm_matrix)\n\n    # test that setting norm_diag=True creates a matrix with ones on the diagonal\n    matrix = make_sparse_spd_matrix(n_dim=10, norm_diag=True)\n    assert_array_equal(np.diag(matrix), np.ones(10))\n\n    # test that setting norm_diag=True does not change the matrix\n    # when it's sparse\n    matrix = make_sparse_spd_matrix(n_dim=10, alpha=0.9)\n    norm_matrix = make_sparse_spd_matrix(n_dim=10, alpha=0.9, norm_diag=True)\n    assert_array_almost_equal(matrix, norm_matrix)\n\n    # test that setting norm_diag=True creates a matrix with ones on the diagonal\n    # when it's sparse\n    matrix = make_sparse_spd_matrix(n_dim=10, alpha=0.9, norm_diag=True)\n    assert_array_equal(np.diag(matrix), np.ones(10))\n", "def test_make_sparse_spd_matrix_shape():\n    # check that the shape is correct\n    for dim in [1, 10, 50]:\n        matrix = make_spd_matrix(dim)\n        assert_equal(matrix.shape, (dim, dim))\n    matrix = make_spd_matrix(5, random_state=0)\n    assert_equal(matrix.shape, (5, 5))\n", "def test_make_biclusters():\n    # Smoke test\n    data = make_biclusters((10, 10), 5)\n    assert len(data) == 3\n\n    # Test row and column labels are in the correct order\n    _, rows, cols = make_biclusters((10, 10), 5)\n    for i in range(5):\n        assert np.sum(rows[i]) == 10\n        assert np.sum(cols[i]) == 10\n\n    # Test n_clusters\n    with pytest.raises(ValueError):\n        make_biclusters((10, 10), 'five')\n\n    # Test noise\n    data = make_biclusters((10, 10), 5, noise=0.1)\n    assert not np.allclose(data, 0)\n\n    # Test minval and maxval\n    data = make_biclusters((10, 10), 5, minval=5, maxval=5)\n    assert np.allclose(data, 5)\n\n    # Test shuffle\n    data = make_biclusters((10, 10), 5, shuffle=True)\n    _, rows, cols = make_biclusters((10, 10), 5, shuffle=True)\n    _, rows_shuffled, cols_shuffled = _shuffle(data, random_state=np.random.RandomState(0))\n    assert not np.array_equal(rows, rows_shuffled)\n    assert not np.array_equal(cols, cols_shuffled)\n\n    # Test n_clusters with two-dimensional tuple\n    data = make_biclusters((10, 10), (3, 4))\n    assert len(data) == 3\n    _, rows, cols = make_biclusters((10, 10), (3, 4))\n    for i in range(3):\n        assert np.sum(rows[i]) == 10\n        assert np.sum(cols[i]) == 4\n\n    # Test random_state\n    data1 = make_biclusters((10, 10), 5, random_state=0)\n    data2 = make_biclusters((10, 10), 5, random_state=0)\n    assert np.array_equal(data1, data2)\n\n    # Test check validation\n    with pytest.raises(ValueError):\n        make_biclusters((10, 10), 5, n_clusters='five')\n    with pytest.raises(ValueError):\n        make_biclusters((10, 10), (3, 4), n_clusters='"], "sample_174": ["    def test_window_frame_rows_start_end_unsupported(self):\n        \"\"\"Window frames are only supported by PostgreSQL.\"\"\"\n        with self.assertRaises(NotSupportedError):\n            self.ops.window_frame_rows_start_end(start=-1, end=1)\n", "    def test_window_frame_start(self):\n        # Test for window_frame_start method\n        self.assertEqual(self.ops.window_frame_start(0), self.ops.CURRENT_ROW)\n        self.assertEqual(self.ops.window_frame_start(1), '1 PRECEDING')\n        self.assertEqual(self.ops.window_frame_start(-1), '1 PRECEDING')\n        self.assertEqual(self.ops.window_frame_start(None), self.ops.UNBOUNDED_PRECEDING)\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_sql_flush(self):\n        with self.assertRaises(NotImplementedError) as cm:\n            self.ops.sql_flush(no_style(), [], tables=['auth.group'], reset_sequences=True)\n        self.assertIn('sql_flush() method', str(cm.exception))\n", "    def test_integer_field_range(self):\n        self.assertEqual(\n            self.ops.integer_field_range('IntegerField'),\n            (-2147483648, 2147483647)\n        )\n"], "sample_364": ["def test_include_invalid_namespace_hint(self):\n    \"\"\"Tests include() with invalid namespace hint.\"\"\"\n    with self.assertRaises(ImproperlyConfigured):\n        include(('hello', 'world'))\n    with self.assertRaises(ImproperlyConfigured):\n        include(('hello'))\n    with self.assertRaises(ImproperlyConfigured):\n        include(('hello', 'world', 'app_name'))\n", "    def test_url_resolver_with_i18n_patterns(self):\n        class TestView(View):\n                return self\n\n        with self.assertRaises(ImproperlyConfigured):\n            _path('/i18n/', TestView(), Pattern=LocalePrefixPattern)\n", "    def test_include_with_callable_returning_namespace(self):\n            return ('url', 'app_name')\n\n        with self.assertRaises(ImproperlyConfigured):\n            include(test_namespace)\n", "    def test_include_callable(self):\n        with self.assertRaises(ImproperlyConfigured):\n            _path('/path/', lambda x: x, Pattern=LocalePrefixPattern)\n", "    def test_include_with_empty_list(self):\n        with self.assertRaises(ImproperlyConfigured):\n            include(('',))\n"], "sample_1164": ["def test_cg_simp_CG_sum_with_multiple_terms():\n    a, b, c, d = symbols('a b c d')\n    CG1 = CG(a, 1, b, 1, c, d)\n    CG2 = CG(a, 2, b, 2, c, d)\n    CG3 = CG(a, 3, b, 3, c, d)\n    expr = CG1 + CG2 + CG3\n    expected_expr = CG(a, 1, b, 1, c, d) + CG(a, 2, b, 2, c, d) + CG(a, 3, b, 3, c, d)\n    result = cg_simp(expr)\n    assert srepr(result) == srepr(expected_expr)\n\n", "def test_cg_simp_clebsch_gordan_coefficients():\n    a = symbols('a')\n    b = symbols('b')\n    c = symbols('c')\n    alpha = symbols('alpha')\n    beta = symbols('beta')\n    cg = CG(a, alpha, b, beta, c, 0)\n    s = cg.doit()\n    assert s.is_real\n\n", "def test_cg_simp_wigner3j():\n    j1, j2, j3, m1, m2, m3 = symbols('j1 j2 j3 m1 m2 m3')\n    # Test Wigner3j(1,0,1,0,0,0) to be 0\n    w3j = Wigner3j(j1, 0, j2, 0, j3, 0)\n    assert w3j.doit() == 0\n\n    # Test Wigner3j(2,0,3,0,0,0)\n    w3j = Wigner3j(j1, 0, j2, 0, j3, 0)\n    assert w3j.doit() == 0\n\n    # Test Wigner3j(2,1,2,1,0,0)\n    w3j = Wigner3j(j1, 1, j2, 1, j3, 0)\n    assert w3j.doit() == 1\n\n    # Test Wigner3j(3,0,3,0,0,0)\n    w3j = Wigner3j(j1, 0, j2, 0, j3, 0)\n    assert w3j.doit() == 1\n\n    # Test cg_simp(Wigner3j(1,0,1,0,0,0) + Wigner3j(2,0,3,0,0,0))\n    cg = cg_simp(Wigner3j(1, 0, 1, 0, 0, 0) + Wigner3j(2, 0, 3, 0, 0, 0))\n    assert cg.doit() == 0\n\n    # Test cg_simp(Wigner3j(2,1,2,1,0,0) + Wigner3j(2,1,2,-1,0,0))\n    cg = cg_simp(Wigner3j(2, 1, 2, 1, 0, 0) + Wigner3j(2, 1, 2, -1, 0, 0))\n    assert cg.doit() == 1\n\n    # Test cg_simp(Wigner3j(3,0,3,0,0", "def test_clebsch_gordan_coefficient_doit():\n    from sympy.physics.quantum.cg import CG\n    from sympy import symbols, Rational\n\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3')\n    cg = CG(j1, m1, j2, m2, j3, m3)\n    assert isinstance(cg.doit(), float)\n\n    # Test with numerical arguments\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3', integer=True)\n    cg = CG(j1, m1, j2, m2, j3, m3)\n    assert isinstance(cg.doit(), float)\n\n    # Test with invalid arguments\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3')\n    with XFAIL(\"Clebsch-Gordan coefficients must be numerical\"):\n        cg = CG(j1, m1, j2, m2, j3, m3)\n        cg.doit()\n\n    j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3', integer=True)\n    with XFAIL(\"Clebsch-Gordan coefficients must be numerical\"):\n        cg = CG(j1, m1, j2, m2, j3, Rational(3)/2)\n        cg.doit()\n\n    # Test that raising an error when j3 != j1+j2\n    with XFAIL(\"Test that Clebsch-Gordan coefficients are correctly evaluated\"):\n        j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3', integer=True)\n        cg = CG(j1, m1, j2, m2, j1+1, m3)\n        cg.doit()\n\n    # Test that raising an error when m3 != m1+m2\n    with XFAIL(\"Test that Clebsch-Gordan coefficients are correctly evaluated\"):\n        j1, m1, j2, m2, j3, m3 = symbols('j1 m1 j2 m2 j3 m3', integer", "def test_cg_simp_872_9():\n    # Test the 3-1-3-1-3 symmetry of Wigner-9j symbols\n    a = symbols('a')\n    b = symbols('b')\n    c = symbols('c')\n    d = symbols('d')\n    e = symbols('e')\n    expr = Wigner9j(a, b, a + c, a + d, a + e, c + d, b + c, b + d, e)\n    expected = (e + 1)*(e + d + 1)*(e + c + 1)*(d + e + c + a + 1)\n    assert cg_simp(expr) == expected\n"], "sample_95": ["def test_patch_cache_control(self):\n    # Test patch_cache_control\n    response = HttpResponse()\n    patch_cache_control(response, max_age=30)\n    self.assertEqual(response['Cache-Control'], 'max-age=30')\n    patch_cache_control(response, private=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=30, private')\n    patch_cache_control(response, public=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=30, private, public')\n    patch_cache_control(response, no_cache=True, must_revalidate=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=30, private, public, no-cache, must-revalidate')\n\n    # Test that cache_control function also works\n    response = HttpResponse()\n    cache_control(max_age=30)(response)\n    self.assertEqual(response['Cache-Control'], 'max-age=30')\n    cache_control(private=True)(response)\n    self.assertEqual(response['Cache-Control'], 'private, max-age=30')\n", "    def test_vary_header_adds_new_headers(self):\n        request = HttpRequest()\n        response = HttpResponse('<html><body>dummy</body></html>')\n        vary_on_headers('Accept-language')(fully_decorated)(request)\n        self.assertIn('Accept-language', response['Vary'])\n", "def test_patch_vary_headers(self):\n    request = HttpRequest()\n    response = HttpResponse()\n    newheaders = ['Accept', 'Language']\n    patch_vary_headers(response, newheaders)\n    self.assertEqual(response['Vary'], 'Accept, Language')\n    patch_vary_headers(response, ['Cookie', 'Referer'])\n    self.assertEqual(response['Vary'], 'Accept, Language, Cookie, Referer')\n", "def test_patch_cache_control(self):\n    response = HttpResponse()\n    response['Cache-Control'] = 'max-age=3600, private'\n    patch_cache_control(response, max_age=3600, public=True)\n    self.assertEqual(response['Cache-Control'], 'public, max-age=3600')\n    patch_cache_control(response, must_revalidate=True, no_cache=True)\n    self.assertEqual(response['Cache-Control'], 'max-age=0, must-revalidate, no-cache')\n    patch_cache_control(response, max_age=3600, must_revalidate=False)\n    self.assertEqual(response['Cache-Control'], 'max-age=3600, no-cache')\n", "def test_patch_cache_control(self):\n    \"\"\"\n    Test patch_cache_control function correctly adds and updates Cache-Control headers.\n    \"\"\"\n    response = HttpResponse()\n    patch_cache_control(response, max_age=60, must_revalidate=True)\n    self.assertIn('max-age=60', response['Cache-Control'])\n    self.assertIn('must-revalidate', response['Cache-Control'])\n\n    # Test updating Cache-Control header\n    patch_cache_control(response, max_age=30, must_revalidate=False)\n    self.assertEqual(response['Cache-Control'], 'max-age=30, must-revalidate')\n\n    # Test adding new headers\n    patch_cache_control(response, public=True, private=True)\n    self.assertIn('public', response['Cache-Control'])\n    self.assertIn('private', response['Cache-Control'])\n\n    # Test private and public headers\n    patch_cache_control(response, private=True, public=True)\n    self.assertIn('private', response['Cache-Control'])\n    self.assertNotIn('public', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of True\n    response = HttpResponse()\n    patch_cache_control(response, public=True)\n    self.assertIn('public', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of False\n    response = HttpResponse()\n    patch_cache_control(response, private=False)\n    self.assertNotIn('private', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of an integer\n    response = HttpResponse()\n    patch_cache_control(response, max_age=60)\n    self.assertIn('max-age=60', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of a string\n    response = HttpResponse()\n    patch_cache_control(response, stale_while_revalidate='86400')\n    self.assertIn('stale-while-revalidate=86400', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of None\n    response = HttpResponse()\n    patch_cache_control(response, stale_while_revalidate=None)\n    self.assertNotIn('stale-while-revalidate', response['Cache-Control'])\n\n    # Test patching Cache-Control header with a value of False\n    response = HttpResponse()\n    patch_cache_control(response, stale_while_revalidate=False)\n    self.assertNotIn('stale-while-revalidate', response['Cache-Control'])\n"], "sample_704": ["def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node()\n    node._nodeid = nodeid\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids(nodeid, expected):\n    node = nodes.Node(name=\"\", parent=None)\n    assert list(iterparentnodeids(nodeid)) == expected\n", "def test_iterparentnodeids(nodeid: str, expected: List[str]) -> None:\n    node = nodes.Node(name=\"test\", nodeid=nodeid)\n    assert list(iterparentnodeids(node.nodeid)) == expected\n", "def test_iterparentnodeids_nodeid_has_colons(nodeid: str) -> None:\n    \"\"\"Check that iterparentnodeids correctly splits nodeid with colons.\"\"\"\n    result = list(iterparentnodeids(nodeid))\n    # For the node ID \"testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source\"\n    # the expected result would be\n    # [\"\", \"testing\", \"testing/code\", \"testing/code/test_excinfo.py\",\n    #  \"testing/code/test_excinfo.py::TestFormattedExcinfo\", \"testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source\"]\n    assert result == [\n        \"\",\n        \"testing\",\n        \"testing/code\",\n        \"testing/code/test_excinfo.py\",\n        \"testing/code/test_excinfo.py::TestFormattedExcinfo\",\n        \"testing/code/test_excinfo.py::TestFormattedExcinfo::test_repr_source\",\n    ]\n\n", "def test_iterparentnodeids_with_fspath_and_path(\n    fspath: LEGACY_PATH = \"path/to/this/file.py\",\n    path: Path = Path(\"path/to/this/file.py\"),"], "sample_85": ["    def setUp(self):\n        self.model = models.Model()\n        self.model2 = models.Model()\n", "    def setUp(self):\n        self.model = models.Model()\n        self.field = models.ForeignKey(self.model, on_delete=models.CASCADE)\n", "    def test_through_model_not_installed(self):\n        class User(models.Model):\n            pass\n        class Group(models.Model):\n            pass\n        field = models.ManyToManyField(User, through='non_existent_through_model')\n        with self.assertRaises(IntegrityError):\n            field.contribute_to_class(User, 'groups')\n", "    def test_check_table_uniqueness_with_through_model(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(models.Model):\n            parent = models.ManyToManyField(Parent, through='ParentChild')\n\n        ParentChild = models.ManyToManyFieldField(Parent, to=Child, related_name='parent')\n        ParentChild.check()\n        self.assertEqual(ParentChild._check_table_uniqueness(), [])\n\n        class ManyToManyFieldTestsParent(models.Model):\n            pass\n\n        class ManyToManyFieldTestsChild(models.Model):\n            parent = models.ManyToManyField(ManyToManyFieldTestsParent, through='ManyToManyFieldTestsParentChild')\n\n        ManyToManyFieldTestsParentChild = models.ManyToManyFieldField(\n            ManyToManyFieldTestsParent, to=ManyToManyFieldTestsChild, related_name='parent'\n        )\n        with self.assertRaisesRegex(\n            checks.Error,\n            r'The field\\'s intermediary table \\'ManyToManyFieldTestsParent_child_id_child_id\\' clashes with the table name of \\'ManyToManyFieldTestsChild\\''\n        ):\n            ManyToManyFieldTestsChild.check()\n", "    def setUp(self):\n        self.model = models.Model()\n        self.field = models.ForeignKey(to=Origin, on_delete=models.CASCADE, related_name='origin')\n"], "sample_489": ["    def setUp(self):\n        self.country1 = Country.objects.create(name=\"United States of America\", iso_two_letter=\"US\")\n        self.country2 = Country.objects.create(name=\"The Netherlands\", iso_two_letter=\"NL\")\n", "    def test_bulk_update(self):\n        Country.objects.all().delete()\n        Country.objects.bulk_create(self.data)\n        bulk_data = [\n            {\"pk\": 1, \"name\": \"Test\", \"iso_two_letter\": \"TE\"},\n            {\"pk\": 2, \"name\": \"Test2\", \"iso_two_letter\": \"TE2\"},\n        ]\n        Country.objects.bulk_update(bulk_data, [\"name\", \"iso_two_letter\"])\n        country = Country.objects.get(pk=1)\n        self.assertEqual(country.name, \"Test\")\n        self.assertEqual(country.iso_two_letter, \"TE\")\n", "    def test_bulk_create_unique_field(self):\n        fields = [\"id\", \"field1\", \"field2\"]\n        with self.assertRaises(ValueError):\n            Country.objects.bulk_create(\n                [Country(id=1, field1=\"test1\", field2=\"test2\")],\n                fields,\n                update_fields=fields,\n            )\n", "    def test_prefetch_related_objects(self):\n        pizzeria = Pizzeria.objects.create(name=\"Pizzeria\")\n        restaurant1 = pizzeria.restaurant_set.create(name=\"Restaurant 1\")\n        restaurant2 = pizzeria.restaurant_set.create(name=\"Restaurant 2\")\n\n        related_objects = list(Restaurant.objects.prefetch_related(\"pizzeria\").all())\n        self.assertEqual(len(related_objects), 2)\n\n        related_pizzerias = [obj.pizzeria for obj in related_objects]\n        self.assertEqual(related_pizzerias, [pizzeria, pizzeria])\n\n        # Test that prefetched objects are shared between objects\n        pizzeria_1 = related_pizzerias[0]\n        pizzeria_2 = related_pizzerias[1]\n        self.assertEqual(id(pizzeria_1), id(pizzeria_2))\n\n        # Test that prefetched objects are not overriden when creating a new object\n        new_restaurant = Restaurant.objects.create(name=\"New Restaurant\")\n        new_restaurant.pizzeria = pizzeria\n        new_related_objects = list(Restaurant.objects.prefetch_related(\"pizzeria\").all())\n        self.assertEqual(len(new_related_objects), 3)\n\n        new_related_pizzerias = [obj.pizzeria for obj in new_related_objects]\n        self.assertEqual(new_related_pizzerias, [pizzeria, pizzeria, new_restaurant.pizzeria])\n", "    def test_raw_queryset__in_bulk(self):\n        RawQuerySet(\n            raw_query=\"SELECT * FROM django_country\"\n        ).in_bulk(id_list=[1, 2, 3])\n"], "sample_853": ["def test_TransformedTargetRegressor_check_inverse_transformer():\n    X, y = datasets.make_regression(n_samples=100, n_features=10, noise=0, random_state=0)\n    transformer = StandardScaler()\n    reg = LinearRegression()\n    tt = TransformedTargetRegressor(regressor=reg, transformer=transformer)\n    tt.fit(X, y)\n    pred = tt.predict(X)\n    assert_warns_message(UserWarning, tt.transformer_.inverse_transform, pred)\n    assert_allclose(tt.transformer_.inverse_transform(pred), y)\n", "def test_transformed_target_regressor_inverse_check():\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=0)\n    tt = TransformedTargetRegressor(transformer=StandardScaler(), check_inverse=False)\n    tt.fit(X, y)\n    assert_not_equal(tt.transformer_, StandardScaler())\n", "def test_transformed_target_regressor_same_transformation():\n    # Check that TransformedTargetRegressor with same transformation does not issue a warning\n    np.random.seed(0)\n    X, y = datasets.make_regression(n_samples=100, n_features=1, noise=0, random_state=0)\n    tt = TransformedTargetRegressor(regressor=LinearRegression(), func=np.log, inverse_func=np.exp)\n    tt.fit(X, y)\n    assert_no_warnings(tt.predict, X)\n\n    # Check that TransformedTargetRegressor with same transformation has the correct output\n    y_pred = tt.predict(X)\n    assert_allclose(y, np.exp(tt.regressor_.predict(X)), rtol=1e-8)\n\n    # Test that it also works with Lasso regressor\n    tt = TransformedTargetRegressor(regressor=Lasso(), func=np.log, inverse_func=np.exp)\n    tt.fit(X, y)\n    assert_no_warnings(tt.predict, X)\n    y_pred = tt.predict(X)\n    assert_allclose(y, np.exp(tt.regressor_.predict(X)), rtol=1e-8)\n", "def test_transformer_clone():\n    X, y = datasets.make_friedman1(random_state=0)\n    transformer = FunctionTransformer(func=np.log, inverse_func=np.exp, validate=True)\n    regressor = TransformedTargetRegressor(regressor=LinearRegression(),\n                                          transformer=transformer)\n    regressor.fit(X, y)\n    assert isinstance(regressor.transformer_, FunctionTransformer)\n    assert isinstance(regressor.regressor_, LinearRegression)\n    assert regressor.regressor_.fit_params_ == {}\n    assert regressor.transformer_.fit_params_ == {'validate': True}\n", "def test_transformer_regressor_fit_transformer_inverse_check():\n    X, y = datasets.make_regression(n_samples=100, n_features=10, random_state=0)\n    transformer = StandardScaler()\n    tt = TransformedTargetRegressor(transformer=transformer, check_inverse=True)\n    tt.fit(X, y)\n    # This will warn because StandardScaler and inverse_transform are not inverses\n    tt._fit_transformer(y)\n    tt.check_inverse = False  # Disable inverse check\n    tt._fit_transformer(y)\n    # Check that the transformer_ is an instance of FunctionTransformer\n    assert isinstance(tt.transformer_, FunctionTransformer)\n"], "sample_808": ["def test_isolation_forest_custom_dataset():\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    y = np.array([0, 0, 1, 1])\n    model = IsolationForest(contamination=0.1, n_estimators=10)\n    model.fit(X, y)\n    assert isinstance(model.estimators_, list)\n    assert len(model.estimators_) == 10\n    assert model.score_samples(X) < 1\n\n    # Test with a sparse matrix\n    X_sparse = csc_matrix(X)\n    model.fit(X_sparse.toarray(), y)\n    assert isinstance(model.estimators_, list)\n    assert len(model.estimators_) == 10\n    assert model.score_samples(X_sparse.toarray()) < 1\n\n    # Test with a custom random_state\n    model = IsolationForest(random_state=42)\n    model.fit(X, y)\n    assert isinstance(model.estimators_, list)\n    assert len(model.estimators_) == 10\n    assert model.score_samples(X) < 1\n\n    # Test with a custom n_jobs\n    model = IsolationForest(n_jobs=2)\n    model.fit(X, y)\n    assert isinstance(model.estimators_, list)\n    assert len(model.estimators_) == 10\n    assert model.score_samples(X) < 1\n", "def test_fit_with_inconsistent_max_features():\n    X = np.array([[1., 2.], [3., 4.], [5., 6.], [7., 8.]])\n    model = IsolationForest(max_features='auto')\n    with assert_raises_regex(ValueError, 'max_features'):\n        model.fit(X, y=np.array([1, 1, 1, 0]))\n", "def test_iforest_n_estimators_value_error():\n    X = np.random.rand(100, 5)\n    iforest = IsolationForest(n_estimators=-1)\n    with pytest.raises(ValueError):\n        iforest.fit(X)\n\n", "def test_isolation_forest_behaviour_new_contamination_auto():\n    rng = check_random_state(0)\n    X = rng.rand(100, 2)\n    y = np.ones(100)\n    # Create a regression target variable with 50% of data points\n    # being outliers\n    y[rng.rand(50) < 0.5] = -1\n    # Create an IsolationForest object with behaviour='new' and contamination='auto'\n    iforest = IsolationForest(random_state=rng, behaviour='new', contamination='auto')\n    # Train the model\n    iforest.fit(X, y)\n    # Check that the offset_ attribute is equal to -0.5\n    assert np.isclose(iforest.offset_, -0.5)\n    # Check that the decision_function and score_samples methods return\n    # the same results\n    scores = iforest.score_samples(X)\n    decision_scores = iforest.decision_function(X)\n    assert_allclose(scores, decision_scores)\n    # Check that the predict method returns the correct results\n    y_pred = iforest.predict(X)\n    assert_array_equal(y_pred, np.ones(100))\n", "def test_isolation_forest_n_estimators():\n    \"\"\"Test n_estimators parameter.\n\n    Check that the anomaly score is not significantly changed if the number of\n    trees is increased. We test this for the 2 datasets.\n    \"\"\"\n    X, y = load_iris().data, load_iris().target\n    X_boston, y_boston = load_boston().data, load_boston().target\n\n    rng = check_random_state(0)\n    X = rng.permutation(X)\n\n    # for different n_estimators, compute the anomaly scores\n    n_estimators_values = [10, 50, 100, 500]\n    for n_estimators in n_estimators_values:\n        iforest = IsolationForest(n_estimators=n_estimators)\n        iforest.fit(X)\n        y_pred = iforest.decision_function(X)\n        if n_estimators == 10:\n            # score for 10 trees is expected to be worse than for 50, 100 or 500\n            assert_greater(y_pred.mean(), -1.2)\n\n    # repeat the test with Boston dataset\n    rng = check_random_state(0)\n    X = rng.permutation(X_boston)\n\n    for n_estimators in n_estimators_values:\n        iforest = IsolationForest(n_estimators=n_estimators)\n        iforest.fit(X_boston)\n        y_pred = iforest.decision_function(X_boston)\n        if n_estimators == 10:\n            # score for 10 trees is expected to be worse than for 50, 100 or 500\n            assert_greater(y_pred.mean(), -1.2)\n"], "sample_134": ["compilation error", "    def test_serializer_factory_type(self):\n        # Test that serializer_factory returns the correct serializer for type()\n        self.assertEqual(serializer_factory(type), TypeSerializer)\n        self.assertEqual(serializer_factory(str), TypeSerializer)\n        self.assertEqual(serializer_factory(int), TypeSerializer)\n        self.assertEqual(serializer_factory(bool), TypeSerializer)\n", "    def test_FloatSerializer(self):\n        serializer = serializer_factory(1.2)\n        self.assertEqual(serializer.serialize(), ('1.2', set()))\n", "    def test_base_serializer_subclasses(self):\n        class MySerializer(BaseSerializer):\n                return 'custom serialized value'\n\n        with self.assertRaises(NotImplementedError):\n            serializer_factory('test_value').serialize()\n\n        with self.assertRaises(NotImplementedError):\n            serializer_factory(MySerializer()).serialize()\n", "    def test_simple_types(self):\n        self.assertEqual(serializer_factory(1).serialize(), ('1', set()))\n        self.assertEqual(serializer_factory(1.0).serialize(), ('1.0', set()))\n        self.assertEqual(serializer_factory(True).serialize(), ('True', set()))\n        self.assertEqual(serializer_factory(None).serialize(), ('None', set()))\n"], "sample_1168": ["compilation error", "def test_group():\n    # Test that group returns the correct groups of equal elements\n    assert group([1, 1, 1, 2, 2, 3]) == [[1, 1, 1], [2, 2], [3]]\n    assert group([1, 1, 3, 2, 2, 1]) == [(1, 2), (3, 1), (2, 2), (1, 1)]\n    assert group([1, 2, 3]) == [[1], [2], [3]]\n\n    # Test that group returns the correct groups when multiple=False\n    assert group([1, 1, 1, 2, 2, 3], multiple=False) == [(1, 3), (2, 2), (3, 1)]\n    assert group([1, 1, 3, 2, 2, 1], multiple=False) == [(1, 2), (3, 1), (2, 2), (1, 1)]\n\n    # Test that group raises a ValueError when multiple=True and not all groups are of the same length\n    with raises(ValueError):\n        group([1, 1, 2, 3, 3, 3], multiple=True)\n", "def test_kbins():\n    # Test that kbins returns all possible partitions\n    for k in range(1, 5):\n        for p in kbins([1, 2, 3, 4], k):\n            assert set(p) <= set([1, 2, 3, 4])\n\n    # Test that kbins returns the correct number of partitions\n    for k in range(1, 5):\n        assert len(list(kbins([1, 2, 3, 4], k))) == 4**k - 2**k\n\n    # Test that kbins returns the correct partitions when ordered is None\n    for p in kbins([1, 2, 3, 4], 2):\n        assert len(p) == 2\n\n    # Test that kbins returns the correct partitions when ordered is 11\n    for p in kbins([1, 2, 3, 4], 2, ordered=11):\n        assert len(p) == 2\n        assert sum(len(i) for i in p) == 4\n\n    # Test that kbins returns the correct partitions when ordered is 00\n    for p in kbins([1, 2, 3, 4], 2, ordered=00):\n        assert len(p) == 2\n        assert sum(len(i) for i in p) == 4\n\n    # Test that kbins returns the correct partitions when ordered is 10\n    for p in kbins([1, 2, 3, 4], 2, ordered=10):\n        assert len(p) == 2\n        assert sum(len(i) for i in p) == 4\n\n    # Test that kbins returns the correct partitions when ordered is 1\n    for p in kbins([1, 2, 3, 4], 2, ordered=1):\n        assert len(p) == 2\n        assert sum(len(i) for i in p) == 4\n\n    # Test that kbins raises a ValueError when ordered is not 00, 01, 10 or 11\n    raises(ValueError, lambda: kbins([1, 2, 3, 4], 2, ordered=5))\n\n", "def test_ordered_partitions_size():\n    from sympy.utilities.iterables import ordered_partitions\n\n    # Test partitions with size m\n    m = 2\n    result = set()\n    for p in ordered_partitions(6, m):\n        result.add(tuple(sorted(p)))\n    expected = {(1, 5), (2, 4), (3, 3)}\n    assert result == expected\n\n    # Test partitions with size n when n is a multiple of m\n    n = 6\n    m = 2\n    result = set()\n    for p in ordered_partitions(n, m):\n        result.add(tuple(sorted(p)))\n    expected = {(1, 5), (2, 4), (3, 3)}\n    assert result == expected\n\n    # Test partitions with size m when m is not None and n is a multiple of m\n    n = 6\n    m = 2\n    result = set()\n    for p in ordered_partitions(n, m, sort=False):\n        result.add(tuple(sorted(p)))\n    expected = {(1, 5), (3, 3), (2, 4)}\n    assert result == expected\n\n    # Test partitions with size m when m is not None and n is not a multiple of m\n    n = 7\n    m = 2\n    result = set()\n    for p in ordered_partitions(n, m):\n        result.add(tuple(sorted(p)))\n    expected = {(1, 5), (2, 4), (3, 3), (1, 4, 2)}\n    assert result == expected\n\n    # Test partitions with size n\n    n = 7\n    result = set()\n    for p in ordered_partitions(n):\n        result.add(tuple(sorted(p)))\n    expected = {(1, 1, 1, 1, 1, 1, 1), (1, 1, 1, 1, 1, 2, 2), (1, 1, 1, 1, 2, 1, 2), (1, 1, 1, 1, 2, 2, 1),\n                (1, 1, 1, 2, 1, 1, 2), (1, 1, 1, 2, 1, 2, 1), (1, 1, 1, 2, ", "compilation error"], "sample_1090": ["def test_comp():\n    assert comp(S(3.14159, evaluate=False), S(3.1416, evaluate=False))\n    assert comp(S(3.14159, evaluate=False), S(3.1416, evaluate=False), tol=1e-2)\n    assert not comp(S(3.14159, evaluate=False), S(3.1416, evaluate=False), tol=1e-3)\n    assert not comp(S(3.14159, evaluate=False), S(3.142, evaluate=False))\n    assert comp(S(3.14159, evaluate=False), S(3.142, evaluate=False), tol=1e-2)\n    assert comp(S(3.14159, evaluate=False), S(3.142, evaluate=False), tol=1e-3)\n    assert comp(S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False), S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False))\n    assert comp(S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False), S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False), tol=1e-1)\n    assert not comp(S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False), S(3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679, evaluate=False), tol=1e-2)\n    assert comp(S(3.141592653589793238462643383279502884197169399375105820974944592307816406286208", "def test_rational_creation():\n    assert Rational(1, 2) == Rational(1, 2)\n    assert Rational(2, 2) == S.One\n    assert Rational(0, 2) == S.Zero\n    assert Rational(-1, -2) == Rational(1, 2)\n    assert Rational(-1, 2) == S.NegativeOne\n    assert Rational(-2, 2) == S.NegativeOne\n    assert Rational(2, -2) == S.NegativeOne\n    assert Rational(0, -2) == S.Zero\n    assert Rational(-1, -2) == S.One\n", "def test_negative_infinity():\n    assert (S.NegativeInfinity < S.Infinity)\n    assert (S.NegativeInfinity > S.Infinity)\n    assert (S.NegativeInfinity < S.NegativeInfinity)\n    assert (S.NegativeInfinity > S.NegativeInfinity)\n    assert not (S.NegativeInfinity == S.Infinity)\n    assert not (S.NegativeInfinity != S.Infinity)\n", "def test_igcd():\n    assert igcd(10, 15) == 5\n    assert igcd(13, 17) == 1\n    assert igcd(12, 15) == 3\n    assert igcd(15, 17) == 1\n", "    def test_creation_from_float(self):\n        from sympy import Rational\n        from sympy import Float\n        # A float can be converted to a Rational\n        assert Rational(0.5) == Rational(1, 2)\n        # However, the original float's precision is lost\n        assert Float(0.5)._mpf_ != Rational(1, 2)._as_mpf_val(53)\n"], "sample_577": ["    def test_single_layer(self):\n        p = Plot()\n        mark = Dot()\n        stat = Agg()\n        p = p.add(mark, stat)\n        assert p._layers == [\n            {\n                \"mark\": mark,\n                \"stat\": stat,\n                \"move\": None,\n                \"vars\": {},\n                \"source\": None,\n                \"legend\": True,\n                \"orient\": None,\n            }\n        ]\n", "    def test_pair(self):\n        p = Plot()\n        p.pair(x=[\"x1\", \"x2\"], y=[\"y1\", \"y2\"], wrap=2, cross=False)\n        assert p._pair_spec == {\n            \"variables\": {\"x\": [\"x1\", \"x2\"], \"y\": [\"y1\", \"y2\"]},\n            \"structure\": {\"x\": [\"x0\", \"x1\"], \"y\": [\"y0\", \"y1\"]},\n            \"wrap\": 2,\n            \"cross\": False\n        }\n", "    def test_plot_add(self):\n        p = Plot(data=Default())\n        m = Mark()\n        p = p.add(m)\n        assert p._layers == [{'data': PlotData(data=Default(), variables={}), 'legend': True, 'mark': m, 'move': None, 'orient': None, 'source': None, 'stat': None, 'vars': {}}]\n", "    def test_on_with_axes(self):\n        fig, ax = plt.subplots()\n        plot = Plot(data=pd.DataFrame({\"x\": [1, 2, 3]}))\n        plot = plot.add(Mark(\"point\"), x=\"x\")\n        plot = plot.on(ax)\n        plot.show()\n", "    def test_facet_defaults(self):\n        p = Plot()\n        p.facet(col=\"x\", row=\"y\", order=\"ascending\")\n        assert p._facet_spec == {\n            \"variables\": {\"col\": \"x\", \"row\": \"y\"},\n            \"structure\": {\"col\": [\"ascending\"], \"row\": [\"ascending\"]},\n            \"wrap\": None\n        }\n"], "sample_1185": ["def test_decompogen_with_Negations():\n    from sympy import decompogen, sqrt, Abs, cos\n    x = symbols('x')\n    f = decompogen(-cos(x), x)\n    assert f == [-cos(x)], f'Unexpected output, expected [-cos(x)] but got {f}'\n", "def test_decompogen_composition():\n    # Test that decompogen and compogen are inverses of each other\n    x = symbols('x')\n    f = sin(x)\n    g = cos(x)\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    f = sin(sqrt(cos(x**2 + 1)))\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with Polynomials\n    f = x**4 + 2*x**3 - x - 1\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with Min and Max\n    f = Min(sin(x), cos(x))\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with Pow\n    f = x**2\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with Exp\n    f = exp(x)\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with Abs\n    f = Abs(x)\n    decomposition = decompogen(f, x)\n    recomp = compogen(decomposition, x)\n    assert recomp == f, \"Decomposition and recompansion are not inverses\"\n\n    # Test that decompogen works with non-function inputs\n    with raises(TypeError):\n        decompogen(\"string\", x)\n    with raises(TypeError):\n        decompogen(123, x)\n    with raises(TypeError):\n        decompogen([1, 2, 3], x", "def test_decompogen_with_two_Powers():\n    from sympy import Pow\n    f = Pow(Pow(x, 2), 3)\n    result = decompogen(f, x)\n    expected = [Pow(x, 6)]\n    assert result == expected\n", "def test_decompogen_max_multiple_variables():\n    x, y = symbols('x y')\n    f = Max(x**2 + y**2, x**2 + 1)\n    expected_output = [x**2 + 1, Max(y, 1)]\n    assert decompogen(f, x) == expected_output\n", "def test_decompogen_sin_pow():\n    x = symbols('x')\n    # Test case: sin(x^2 + 1) - should be decomposed into sin(x), x^2 + 1\n    f = sin(x**2 + 1)\n    expected = [sin(x), x**2 + 1]\n    assert decompogen(f, x) == expected\n\n    # Test case: sin(x) should not be decomposed further\n    f = sin(x)\n    expected = [sin(x)]\n    assert decompogen(f, x) == expected\n"], "sample_79": ["    def check_values(self, *tests):\n        for value, arg, expected in tests:\n            with self.subTest(value=value, arg=arg):\n                output = self.engine.render_to_string('t', {'value': value, 'arg': arg})\n                self.assertEqual(output, expected)\n", "    def check_values(self, *tests):\n        for value, arg, expected in tests:\n            with self.subTest(value=value, arg=arg):\n                output = self.engine.render_to_string('t', {'value': value, 'arg': arg})\n                self.assertEqual(output, expected)\n", "    def check_values(self, *tests):\n        for value, expected, arg in tests:\n            with self.subTest(value=value, arg=arg):\n                output = self.engine.render_to_string('t', {'value': value, 'arg': arg})\n                self.assertEqual(output, expected)\n", "    def check_values(self, *tests):\n        for value, expected in tests:\n            with self.subTest(value=value):\n                output = self.engine.render_to_string('t', {'value': value})\n                self.assertEqual(output, expected)\n", "    def test_timesince_today(self):\n        \"\"\"The timesince filter for today should be empty.\"\"\"\n        self.check_values(Decimal('0'))\n"], "sample_11": ["def test_sliced_wcs_pixel_to_world_values_invalid_slices():\n    \"\"\"\n    Test pixel_to_world_values with invalid slices.\n    \"\"\"\n    with pytest.raises(ValueError):\n        wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (-10, slice(None), slice(None)))\n    with pytest.raises(ValueError):\n        wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(None), -10, slice(None)))\n    with pytest.raises(ValueError):\n        wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(None), slice(None), 10))\n    with pytest.raises(ValueError):\n        wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(5, 10, 2), slice(None), slice(None)))\n", "def test_world_axis_object_classes():\n    world_axis_classes = WCS_SPECTRAL_CUBE.world_axis_object_classes\n    sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(1, 3), slice(None), slice(0, 2)))\n    assert_equal(sliced_wcs.world_axis_object_classes, world_axis_classes)\n", "def test_pixel_to_world_values_invalid_slice():\n    with pytest.raises(IndexError):\n        WCS_SPECTRAL_CUBE.sliced_wcs(slices=(slice(None), slice(1, None, 2), 2))\n", "def test_sliced_wcs_multiple_slices():\n    # Given WCS object\n    WCS_SPECTRAL_CUBE_SLICED = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(1, 3), 5, slice(None)))\n\n    # Test that dropped world dimensions have correct information\n    dropped_info = WCS_SPECTRAL_CUBE_SLICED.dropped_world_dimensions\n    assert len(dropped_info[\"value\"]) == 1\n    assert dropped_info[\"world_axis_names\"][0] == \"Latitude\"\n    assert dropped_info[\"world_axis_physical_types\"][0] == \"angle\"\n    assert dropped_info[\"world_axis_units\"][0] == \"deg\"\n    assert dropped_info[\"world_axis_object_components\"][0] == (\"latitude\",)\n    assert dropped_info[\"world_axis_object_classes\"] == {\"latitude\": [\"Latitude\"]}\n    assert dropped_info[\"serialized_classes\"] == WCS_SPECTRAL_CUBE.serialized_classes\n\n    # Test that pixel_n_dim and world_n_dim properties return correct values\n    assert WCS_SPECTRAL_CUBE_SLICED.pixel_n_dim == 3\n    assert WCS_SPECTRAL_CUBE_SLICED.world_n_dim == 2\n\n    # Test that world_axis_physical_types, world_axis_units, pixel_axis_names,\n    # world_axis_names, world_axis_object_components, world_axis_object_classes\n    # and world_axis_names properties return correct values\n    assert WCS_SPECTRAL_CUBE_SLICED.world_axis_physical_types == [\"angle\", \"frequency\"]\n    assert WCS_SPECTRAL_CUBE_SLICED.world_axis_units == [\"deg\", \"Hz\"]\n    assert WCS_SPECTRAL_CUBE_SLICED.pixel_axis_names == [\"Frequency\", \"Longitude\"]\n    assert WCS_SPECTRAL_CUBE_SLICED.world_axis_names == [\"Latitude\", \"Longitude\"]\n    assert WCS_SPECTRAL_CUBE_SLICED.world_axis_object_components == [(\"latitude\",), (\"longitude\",)]\n    assert WCS_SPECTRAL_CUBE_SLICED.world_axis_object_classes == {\"latitude\": [\"Latitude\"], \"longitude\": [\"Longitude\"]}\n\n    # Test that array_shape, pixel_shape and pixel_bounds properties return correct values\n    assert WCS_SPECTRAL_CUBE_SLICED.array_shape == (29, 2, 15)\n    assert WCS_SPECTRAL_CUBE_SLICED.pixel_shape == (15, 2, 29)\n    assert WCS_SPECTRAL_CUBE_SLICED.pixel_bounds == [(5, 10), (4, 6), (10, 20)]\n\n", "def test_sliced_low_level_wcs_pixel_bounds():\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', FITSFixedWarning)\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore', VerifyWarning)\n            wcs = WCS(Header.fromstring(HEADER_SPECTRAL_CUBE, sep='\\n'))\n            wcs.pixel_bounds = [(-1, 11), (-2, 18), (5, 15)]\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(5, 15), slice(None), slice(None)))\n\n    assert_equal(sliced_wcs.pixel_bounds, ((0, 6), (-2, 18), None))\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None, None, -1), slice(5, 10), slice(None)))\n    assert_equal(sliced_wcs.pixel_bounds, ((10, 11), (5, 10), None))\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None, None, 2), slice(None), slice(None)))\n    assert_equal(sliced_wcs.pixel_bounds, ((30, 30), None, None))\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None), slice(None, None, -2), slice(None)))\n    assert_equal(sliced_wcs.pixel_bounds, (None, (18, 18), None))\n\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None), slice(None), slice(0, 5)))\n    assert_equal(sliced_wcs.pixel_bounds, (None, None, (5, 5)))\n\n    # Test dropping pixel bounds when slicing\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(5, 15),))\n    assert_equal(sliced_wcs.pixel_bounds, None)\n\n    # Test dropping bounds for a single slice\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None), slice(5, 10)))\n    assert_equal(sliced_wcs.pixel_bounds, None)\n\n    # Test dropping bounds for a single pixel index\n    sliced_wcs = SlicedLowLevelWCS(wcs, 5)\n    assert_equal(sliced_wcs.pixel_bounds, None)\n\n    # Test dropping bounds when there are no pixel bounds\n    wcs.pixel_bounds = None\n    sliced_wcs = SlicedLowLevelWCS(wcs,"], "sample_623": ["def test_open_dataset_return_value_type():\n    \"\"\"Test that open_dataset() returns the correct type\"\"\"\n    ds = xr.Dataset()\n    assert isinstance(xr.open_dataset(ds), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"pydap\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"pynio\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"cfgrib\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"pseudonetcdf\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"h5netcdf\"), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\", chunks=5), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\", chunks=5), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\", chunks=5), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\", cache=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\", cache=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\", cache=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\", decode_cf=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\", decode_cf=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\", decode_cf=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\", mask_and_scale=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\", mask_and_scale=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\", mask_and_scale=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"netcdf4\", decode_times=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"scipy\", decode_times=True), xr.Dataset)\n    assert isinstance(xr.open_dataset(ds, engine=\"zarr\", decode_times=True), xr.Dataset)\n    assert isinstance", "def test_encode_dataset_coordinates():\n    \"\"\"Test that xarray encodes coordinates to the correct data types in the netcdf file.\"\"\"\n    da = xr.DataArray(np.arange(4), dims=['x', 'y'])\n    ds = da.to_dataset()\n    ds['lat'] = xr.DataArray(np.arange(4), dims=['y'])\n    ds['lon'] = xr.DataArray(np.arange(4), dims=['x'])\n    ds.attrs['units'] = 'm'\n\n    ds_encoded = ds.to_netcdf(BytesIO(), encoding={'lat': {'dtype': 'f4'}})\n\n    with xr.open_dataset(ds_encoded) as ds_loaded:\n        assert ds_loaded['lat'].dtype == np.dtype('float32')\n        assert ds_loaded['lon'].dtype == np.dtype('int32')\n        assert ds_loaded['lat'].attrs['units'] == 'm'\n", "def test_to_zarr_region():\n    \"\"\"Test to_zarr() with a region.\"\"\"\n    # create a 3D dataset\n    ds = xr.Dataset(\n        {\n            \"a\": (\"x\", np.linspace(0, 1, 10)),\n            \"b\": (\"y\", np.linspace(0, 1, 10)),\n            \"c\": (\"z\", np.linspace(0, 1, 10)),\n            \"d\": ((\"x\", \"y\", \"z\"), np.random.rand(10, 10, 10)),\n        },\n        coords={\"x\": np.linspace(0, 1, 10), \"y\": np.linspace(0, 1, 10), \"z\": np.linspace(0, 1, 10)},\n    )\n\n    # save to zarr with a region\n    store = xr.backends.zarr.ZarrStore.open_group(\n        store=\"memory://\",\n        mode=\"w\",\n        group=\"test_to_zarr_region\",\n        consolidated=True,\n        consolidate_on_close=True,\n    )\n    zds = ds.to_zarr(store=store, region={\"z\": slice(0, 1)})\n    assert zds.z.size == 10\n\n    # check that all variables are in the region\n    for var in zds.data_vars:\n        assert set(zds[var].dims) <= {\"x\", \"y\", \"z\"}\n", "def test_to_zarr_region():\n    # TODO: verify ZarrStore.close is called\n    ds = xr.Dataset(\n        {\"x\": (\"y\", np.arange(5))}, coords={\"y\": np.arange(5)}\n    )\n    zstore = to_zarr(\n        ds, compute=False, region={\"y\": slice(1, 3)}, append_dim=\"y\"\n    )\n    assert isinstance(zstore, dask.delayed)\n    assert isinstance(zstore.result().zarr_group.array_keys(), list)\n    zstore.close()\n    assert len(zstore.result().zarr_group.array_keys()) == 1  # must close before delete\n    zstore.result().close()  # close the underlying store\n    assert len(zstore.result().zarr_group.array_keys()) == 0\n    zstore.result().delete()  # delete the underlying store\n", "def test_open_dataset_dask_open_dataset_close():\n    from dask.delayed import delayed\n    from dask.core import Closure\n\n    # Test open_dataset, with dask delayed objects created for open_dataset and close\n    # Note: we are creating a delayed object of the close method to make the test more robust\n    #       to changes in the open_dataset implementation.\n        return delayed(xr.open_dataset)(path).ascomputable()\n\n        return delayed(ds.close).of_type(Closure)\n\n    path = \"path/to/file.nc\"\n    ds = delayed(xr.open_dataset)(path).ascomputable()\n    close = close_dataset(ds)\n    assert ds is not None\n    assert close is not None\n"], "sample_721": ["def test_check_symmetric_sparse_matrix():\n    # Test a sparse matrix\n    X = sparse_random_matrix(10, density=0.01, random_state=0)\n    X = check_symmetric(X, raise_warning=False)\n    assert_array_equal(X.toarray(), np.tril(X.toarray()))\n\n    # Test a non-symmetric sparse matrix\n    X = sparse_random_matrix(10, density=0.01, random_state=0)\n    X = check_symmetric(X, raise_warning=False, raise_exception=True)\n    assert_raise_message(ValueError, r\"Array must be symmetric\", check_symmetric, X)\n\n    # Test a sparse matrix with different formats\n    for format in ['csr', 'csc', 'coo', 'dok', 'lil', 'dia']:\n        X = sparse_random_matrix(10, density=0.01, random_state=0, format=format)\n        X = check_symmetric(X, raise_warning=False)\n        assert_array_equal(X.toarray(), np.tril(X.toarray()))\n\n    # Test a dense matrix\n    X = np.random.rand(10, 10)\n    X = check_symmetric(X, raise_warning=False)\n    assert_array_equal(X, np.tril(X))\n\n    # Test a non-symmetric dense matrix\n    X = np.random.rand(10, 10)\n    X = check_symmetric(X, raise_warning=False, raise_exception=True)\n    assert_raise_message(ValueError, r\"Array must be symmetric\", check_symmetric, X)\n\n    # Test a matrix with different tolerance\n    X = sparse_random_matrix(10, density=0.01, random_state=0)\n    X = check_symmetric(X, raise_warning=False, tol=1e-3)\n    assert_array_equal(X.toarray(), np.tril(X.toarray()))\n\n    # Test a matrix with different tolerance and raise exception\n    X = sparse_random_matrix(10, density=0.01, random_state=0)\n    X = check_symmetric(X, raise_warning=False, tol=1e-3, raise_exception=True)\n    assert_raise_message(ValueError, r\"Array must be symmetric\", check_symmetric, X)\n", "def test_check_symmetric_symmetrization():\n    X = np.array([[1, 2], [3, 4]])\n    with warnings.catch_warnings():\n        warnings.simplefilter('error', UserWarning)\n        check_symmetric(X, raise_warning=True, raise_exception=False)\n    assert_array_equal(X, check_symmetric(X))\n", "def test_check_symmetric():\n    # Test that a symmetric matrix is not modified\n    np.random.seed(0)\n    A = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    expected_A = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    symmetric_A = check_symmetric(A)\n    assert_array_equal(symmetric_A, expected_A)\n\n    # Test that a non-symmetric matrix is converted\n    np.random.seed(0)\n    A = np.array([[1, 1, 1], [1, 2, 1], [1, 1, 1]])\n    expected_A = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    with ignore_warnings():\n        symmetric_A = check_symmetric(A)\n    assert_array_equal(symmetric_A, expected_A)\n\n    # Test that a non-square matrix raises an error\n    np.random.seed(0)\n    A = np.array([[1, 2], [3, 4], [5, 6, 7]])\n    with assert_raises(ValueError):\n        check_symmetric(A)\n\n    # Test that a matrix with non-zero tolerance is symmetric\n    np.random.seed(0)\n    A = np.array([[1, 1.0001], [1.0001, 1]])\n    symmetric_A = check_symmetric(A, tol=0.001)\n    assert_array_equal(symmetric_A, A)\n\n    # Test that a matrix with a non-zero tolerance is not symmetric\n    np.random.seed(0)\n    A = np.array([[1, 1.0001], [1.0001, 1]])\n    symmetric_A = check_symmetric(A, tol=0.000001)\n    expected_A = np.array([[1, 0.9999999999999998], [0.9999999999999998, 1]])\n    assert_array_equal(symmetric_A, expected_A)\n\n    # Test that a sparse matrix is symmetric\n    np.random.seed(0)\n    A = sp.csr_matrix([[1, 2], [3, 4]])\n    expected_A = sp.csr_matrix([[1, 2], [2, 4]])\n    symmetric_A = check_sym", "def test_check_array_dtype():\n    # Check that check_array allows None as dtype\n    with ignore_warnings():\n        X = np.array([1, 2, 3])\n        check_array(X, dtype=None)\n\n    # Check that check_array raises an error when dtype is not a string\n    with assert_raises_regex(ValueError, 'force_all_finite'):\n        check_array(X, dtype=123)\n\n    # Check that check_array raises an error when dtype is a list of strings\n    with assert_raises_regex(ValueError, 'list of types'):\n        check_array(X, dtype=['int', 'float'])\n\n    # Check that check_array raises an error when dtype is an invalid string\n    with assert_raises_regex(ValueError, 'Unknown data type'):\n        check_array(X, dtype='unknown')\n\n    # Check that check_array converts object dtype to float64 when force_all_finite is True\n    X = np.array([1, 2, 3], dtype=object)\n    result = check_array(X, force_all_finite=True)\n    assert result.dtype == np.float64\n    assert_array_equal(result, np.array([1.0, 2.0, 3.0]))\n\n    # Check that check_array raises an error when object dtype is not converted to float64\n    X = np.array([1, 2, 3], dtype=object)\n    with assert_raises_regex(ValueError, 'not supported'):\n        check_array(X, dtype='float32')\n", "def test_check_X_y_warn_on_dtype():\n    \"\"\"Check that a DataConversionWarning is raised when input data has the wrong dtype.\"\"\"\n    # Create a mock estimator with a wrong dtype\n    class MockEstimator:\n            self.dtype = dtype\n\n            pass\n\n    estimator = MockEstimator()\n\n    # Create a mock dataset with the wrong dtype\n    X = np.array([1, 2, 3], dtype=np.float16)\n    y = np.array([1, 2, 3])\n\n    # Check that a DataConversionWarning is raised\n    with assert_warns(DataConversionWarning):\n        check_X_y(X, y, y_numeric=True)\n"], "sample_800": ["    def __init__(self, p=0):\n        self.p = p\n", "    def __init__(self, random_state=None):\n        self.random_state = random_state\n", "    def fit(self, X, y):\n        from sklearn.utils.validation import warn as _warn\n        _warn(\"Something is wrong\", DeprecationWarning)\n        return self\n", "def check_fit_idempotent_no_warnings(name, estimator_orig):\n    # Check that fit doesn't raise any warnings when called idempotently.\n    rng = np.random.RandomState(0)\n\n    estimator = clone(estimator_orig)\n    set_random_state(estimator)\n    if 'warm_start' in estimator.get_params().keys():\n        estimator.set_params(warm_start=False)\n\n    n_samples = 100\n    X = rng.normal(loc=100, size=(n_samples, 2))\n    X = _pairwise_estimator_convert_X(X, estimator)\n    if is_regressor(estimator_orig):\n        y = rng.normal(size=n_samples)\n    else:\n        y = rng.randint(low=0, high=2, size=n_samples)\n    y = _enforce_estimator_tags_y(estimator, y)\n\n    train, test = next(ShuffleSplit(test_size=.2, random_state=rng).split(X))\n    X_train, y_train = _safe_split(estimator, X, y, train)\n    X_test, y_test = _safe_split(estimator, X, y, test, train)\n\n    # Fit for the first time\n    estimator.fit(X_train, y_train)\n\n    result = {method: getattr(estimator, method)(X_test)\n              for method in ['predict', 'transform', 'decision_function',\n                             'predict_proba'] if hasattr(estimator, method)}\n\n    # Fit again\n    set_random_state(estimator)\n    with warnings.catch_warnings(record=True) as w:\n        estimator.fit(X_train, y_train)\n        warnings.simplefilter(\"always\", DeprecationWarning)\n        warnings.simplefilter(\"always\", FutureWarning)\n        for method in result:\n            if hasattr(estimator, method):\n                new_result = getattr(estimator, method)(X_test)\n                if np.issubdtype(new_result.dtype, np.floating):\n                    tol = 2*np.finfo(new_result.dtype).eps\n                else:\n                    tol = 2*np.finfo(np.float64).eps\n                assert_allclose_dense_sparse(\n                    result[method], new_result,\n                    atol=max(tol, 1e-9), rtol=max(tol, 1e-7),\n                    err_msg=\"Idempotency check failed for method {}\".format(method))\n    assert len(w) == 0, \"Fitting idempotently raised {} warning(s)\".format(len(w", "    def fit(self, X, y):\n        raise ValueError(\"Fitting failed\")\n"], "sample_1144": ["def test_requires_partial_derivative():\n    # Test a derivative with multiple free variables but all are integers\n    x, y = symbols('x y')\n    expr = Derivative(x*y, x, y)\n    assert not requires_partial(expr)\n\n    # Test a derivative with a single non-integer free variable\n    expr = Derivative(x, x)\n    assert requires_partial(expr)\n\n    # Test a derivative with multiple free variables and some are non-integers\n    expr = Derivative(x*y, x, y)\n    assert requires_partial(expr)\n\n    # Test a derivative with no free variables\n    expr = Derivative(x, x, x)\n    assert not requires_partial(expr)\n\n    # Test a derivative with a variable list\n    expr = Derivative(x, [x, y])\n    assert not requires_partial(expr)\n\n    # Test an expression without free variables\n    expr = 1\n    assert not requires_partial(expr)\n\n    # Test an expression with a single free variable that is an integer\n    x = symbols('x')\n    expr = x\n    assert not requires_partial(expr)\n", "def test_requires_partial_derivatives():\n    x, y = symbols('x y')\n    expr = Derivative(exp(x + y), (x, y))\n    assert requires_partial(expr)\n\n    expr = exp(x + y)\n    assert not requires_partial(expr)\n\n    expr = Derivative(besselj(0, x), x)\n    assert not requires_partial(expr)\n\n    expr = Derivative(legendre(1, x), x)\n    assert not requires_partial(expr)\n\n    expr = Derivative(bell(3, x), x)\n    assert not requires_partial(expr)\n\n    expr = Integral(exp(x + y), (x, 0, 1)).doit()\n    assert not requires_partial(expr)\n\n    # Test that it handles a function of only one variable\n    expr = Derivative(Function('f')(x), x)\n    assert requires_partial(expr)\n\n    # Test that it handles a function of no variables\n    expr = Derivative(Function('f')(y), y)\n    assert not requires_partial(expr)\n", "def test_split_super_sub_various_inputs():\n    # Test with string\n    name, supers, subs = split_super_sub(\"a_x^1\")\n    assert name == \"a\"\n    assert supers == ['1']\n    assert subs == ['x']\n\n    # Test with string with multiple subscripts\n    name, supers, subs = split_super_sub(\"var_sub1__sup_sub2\")\n    assert name == \"var\"\n    assert supers == ['sup']\n    assert subs == ['sub1', 'sub2']\n\n    # Test with string with no subscripts\n    name, supers, subs = split_super_sub(\"xyz\")\n    assert name == \"xyz\"\n    assert supers == []\n    assert subs == []\n\n    # Test with string with multiple superscripts\n    name, supers, subs = split_super_sub(\"a^1^2\")\n    assert name == \"a\"\n    assert supers == ['1', '2']\n    assert subs == []\n\n    # Test with string starting with subscript\n    name, supers, subs = split_super_sub(\"_x\")\n    assert name == \"x\"\n    assert supers == []\n    assert subs == ['x']\n\n    # Test with string starting with superscript\n    name, supers, subs = split_super_sub(\"^x\")\n    assert name == \"x\"\n    assert supers == ['x']\n    assert subs == []\n\n    # Test with string with subscript at the end\n    name, supers, subs = split_super_sub(\"x_1\")\n    assert name == \"x\"\n    assert supers == []\n    assert subs == ['1']\n\n    # Test with string with superscript at the end\n    name, supers, subs = split_super_sub(\"x^1\")\n    assert name == \"x\"\n    assert supers == ['1']\n    assert subs == []\n\n    # Test with empty string\n    name, supers, subs = split_super_sub(\"\")\n    assert name == \"\"\n    assert supers == []\n    assert subs == []\n\n    # Test with string that matches _name_with_digits_p\n    name, subs = split_super_sub(\"a123\")\n    assert name == \"a\"\n    assert subs == ['123']\n\n    # Test with string that does not match _name_with_digits_p\n    name, subs = split_super_sub(\"abc\")\n    assert name == \"abc\"\n    assert subs == []\n\n", "def test_requires_partial():\n    # Test case when expr is not an instance of Derivative\n    x, y = symbols('x y')\n    f = Function('f')\n    expr1 = f(x)\n    expr2 = f(x) + f(y)\n    assert requires_partial(expr1) is False\n    assert requires_partial(expr2) is False\n\n    # Test case when expr has no free variables\n    expr = exp(x)\n    assert requires_partial(expr) is False\n\n    # Test case when expr has free variables but no partial derivatives\n    expr = exp(x + y)\n    assert requires_partial(expr) is False\n\n    # Test case when expr has free variables and partial derivatives\n    expr = Derivative(f(x + y), x).doit()\n    assert requires_partial(expr) is True\n\n    # Test case when expr has multiple partial derivatives\n    expr = Derivative(Derivative(f(x + y), x), x).doit()\n    assert requires_partial(expr) is True\n\n    # Test case when expr has complex free variables\n    z = symbols('z', real=False)\n    expr = exp(x + z)\n    assert requires_partial(expr) is False\n\n    # Test case when expr has an integer free variable\n    expr = Derivative(f(x), x).doit()\n    assert requires_partial(expr) is False\n", "def test_split_super_sub_non_empty_string():\n    \"\"\"Test split_super_sub function with non-empty string.\"\"\"\n    name, supers, subs = split_super_sub('a_x^1')\n    assert name == 'a_x'\n    assert supers == ['1']\n    assert subs == []\n"], "sample_632": ["def test_similar_with_imports_and_signatures():\n    \"\"\"test Similar with imports and signatures\"\"\"\n    with open(SIMILAR1) as stream:\n        with open(SIMILAR2) as stream2:\n            with open(SIMILAR3) as stream3:\n                with open(SIMILAR4) as stream4:\n                    sim = similar.Similar(min_lines=5, ignore_comments=False, ignore_docstrings=False)\n                    sim.append_stream(\"file1\", stream)\n                    sim.append_stream(\"file2\", stream2)\n                    sim.append_stream(\"file3\", stream3)\n                    sim.append_stream(\"file4\", stream4)\n                    sim.run()\n                    assert len(sim.linesets) == 4\n                    assert len(sim.linesets[0]) == 5\n                    assert len(sim.linesets[1]) == 5\n                    assert len(sim.linesets[2]) == 5\n                    assert len(sim.linesets[3]) == 5\n                    assert len(sim._compute_sims()) == 3\n", "def test_report_similarities_no_similarities():\n    with StringIO() as f:\n        with redirect_stdout(f):\n            report_similarities(\"similar\", {\"nb_duplicated_lines\": 0, \"percent_duplicated_lines\": 0}, {\"nb_duplicated_lines\": 0, \"percent_duplicated_lines\": 0})\n        output = f.getvalue()\n        assert output == \"\\nnow  previous  difference\\nTOTAL lines=0 duplicates=0 percent=0.00\\n\"\n", "def test_run_with_different_min_lines():\n    \"\"\"Test that Similar class can handle different min_lines value\"\"\"\n    linter = PyLinter()\n    reporter = Reporter(linter)\n    with open(SIMILAR1) as stream:\n        linter.add_stream(stream, SIMILAR1)\n    with open(SIMILAR2) as stream:\n        linter.add_stream(stream, SIMILAR2)\n    similar = similar.Similar(min_lines=2)\n    similar.append_stream(SIMILAR1, stream, encoding='utf-8')\n    similar.append_stream(SIMILAR2, stream, encoding='utf-8')\n    with StringIO() as output:\n        with redirect_stdout(output):\n            similar.run()\n        output = output.getvalue()\n        assert \"2 similar lines in 2 files\" in output\n        assert \"line1\" in output\n        assert \"line2\" in output\n    with StringIO() as output:\n        with redirect_stdout(output):\n            similar.run()\n        output = output.getvalue()\n        assert \"2 similar lines in 2 files\" in output\n        assert \"line1\" in output\n        assert \"line2\" in output\n    similar.min_lines = 5\n    with StringIO() as output:\n        with redirect_stdout(output):\n            similar.run()\n        output = output.getvalue()\n        assert \"0 similar lines in 2 files\" in output\n    similar.min_lines = 1\n    with StringIO() as output:\n        with redirect_stdout(output):\n            similar.run()\n        output = output.getvalue()\n        assert \"2 similar lines in 2 files\" in output\n        assert \"line1\" in output\n        assert \"line2\" in output\n", "def test_empty_file_reports_no_duplicates():\n    \"\"\"test similar checks against empty files\"\"\"\n    linter = PyLinter()\n    with open(EMPTY_FUNCTION_1) as stream:\n        linter.add_file(EMPTY_FUNCTION_1, stream)\n    with open(EMPTY_FUNCTION_2) as stream:\n        linter.add_file(EMPTY_FUNCTION_2, stream)\n    reporter = Reporter(linter, mode=\"normal\")\n    with redirect_stdout(StringIO()) as output:\n        linter.check(EMPTY_FUNCTION_1)\n        linter.check(EMPTY_FUNCTION_2)\n        reporter.output_results(output)\n    report = output.getvalue()\n    assert \"R0801\" not in report\n", "def test_empty_function_similarity(tmpdir):\n    # Create two empty function files\n    empty_function_1 = tmpdir / \"empty_function_1.py\"\n    empty_function_2 = tmpdir / \"empty_function_2.py\"\n    empty_function_1.write(\"\")\n    empty_function_2.write(\"\")\n\n    # Create a similar checker\n    linter = PyLinter()\n    reporter = Reporter()\n    checker = similar.SimilarChecker(linter)\n\n    # Add the files to the linter\n    linter.current_name = \"test\"\n    with open(empty_function_1, \"r\") as stream:\n        checker.append_stream(\"test\", stream)\n    with open(empty_function_2, \"r\") as stream:\n        checker.append_stream(\"test\", stream)\n\n    # Run the linter\n    with StringIO() as f:\n        with redirect_stdout(f):\n            linter.check([empty_function_1, empty_function_2])\n        report = f.getvalue()\n\n    # Check that a similarity message was emitted\n    assert \"similar lines in 2 files\" in report\n    assert \"==test:0\" in report\n"], "sample_815": ["def test_precision_recall_fscore_support_average_micro():\n    y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n    y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n    labels = ['cat', 'dog', 'pig']\n    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred,\n                                                  average='micro',\n                                                  labels=labels)\n    assert_almost_equal(p, 0.5)\n    assert_almost_equal(r, 0.5)\n    assert_almost_equal(f1, 0.5)\n", "def test_fbeta_score_average_macro(y_true, y_pred, score):\n    assert fbeta_score(y_true, y_pred, average='macro') == score\n", "def test_hamming_loss_multiclass():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([2, 2, 3, 4])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.0)\n\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([2, 1, 3, 4])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.25)\n\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1, 2, 5, 4])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.25)\n\n    y_true = np.array([[1, 2], [2, 3]])\n    y_pred = np.array([[2, 2], [2, 3]])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.0)\n\n    y_true = np.array([[1, 2], [2, 3]])\n    y_pred = np.array([[2, 2], [1, 3]])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.25)\n\n    y_true = np.array([[1, 2], [2, 3]])\n    y_pred = np.array([[1, 2], [2, 5]])\n    assert_almost_equal(hamming_loss(y_true, y_pred), 0.25)\n", "def test_fbeta_score_binary():\n    y_true = np.array([1, 0, 1, 0, 1, 0])\n    y_pred = np.array([1, 0, 1, 0, 0, 1])\n    f_score = fbeta_score(y_true, y_pred, 1)\n    assert_almost_equal(f_score, 0.8333333333333334)\n\n    # Test that the function raises an error when beta is less than or equal to 0\n    with assert_raises(ValueError):\n        fbeta_score(y_true, y_pred, 0)\n\n    # Test that the function raises a warning when true positive + false positive == 0\n    with assert_warns_message(UndefinedMetricWarning, 'f-score is ill-defined'):\n        y_true = np.array([1, 1, 1, 1, 1, 1])\n        y_pred = np.array([0, 0, 0, 0, 0, 0])\n        f_score = fbeta_score(y_true, y_pred, 1)\n\n    # Test that the function raises a warning when true positive + false negative == 0\n    with assert_warns_message(UndefinedMetricWarning, 'f-score is ill-defined'):\n        y_true = np.array([0, 0, 0, 0, 0, 0])\n        y_pred = np.array([1, 1, 1, 1, 1, 1])\n        f_score = fbeta_score(y_true, y_pred, 1)\n", "def test_multilabel_confusion_matrix_samplewise():\n    # Create a multilabel classification problem\n    X, y = make_multilabel_classification(n_samples=100, n_features=20,\n                                          n_classes=3, n_labels=2,\n                                          random_state=1)\n    y_pred = np.random.randint(0, 2, (100, 3))\n\n    # Test that multilabel_confusion_matrix returns the correct result\n    # when samplewise=True\n    MCM = multilabel_confusion_matrix(y, y_pred, samplewise=True)\n    assert_array_equal(MCM.shape, (100, 2, 2))\n\n    # Test that multilabel_confusion_matrix raises an error when samplewise=True\n    # and y is not a multilabel array\n    y = np.argmax(y, axis=1)\n    with pytest.raises(ValueError):\n        multilabel_confusion_matrix(y, y_pred, samplewise=True)\n"], "sample_366": ["    def test_time_parse_with_no_seconds(self):\n        self.assertEqual(parse_time('14:30'), time(14, 30))\n", "def test_parse_duration_iso8601(self):\n    # Test a simple positive duration\n    self.assertEqual(parse_duration('P3D'), timedelta(days=3))\n    self.assertEqual(parse_duration('P3DT3H'), timedelta(days=3, hours=3))\n    self.assertEqual(parse_duration('P3DT3H15M'), timedelta(days=3, hours=3, minutes=15))\n    self.assertEqual(parse_duration('P3DT3H15M30S'), timedelta(days=3, hours=3, minutes=15, seconds=30))\n    self.assertEqual(parse_duration('P3DT3H15M30.5S'), timedelta(days=3, hours=3, minutes=15, microseconds=300000))\n\n    # Test a negative duration\n    self.assertEqual(parse_duration('-P3D'), timedelta(days=-3))\n    self.assertEqual(parse_duration('-P3DT3H'), timedelta(days=-3, hours=3))\n    self.assertEqual(parse_duration('-P3DT3H15M'), timedelta(days=-3, hours=3, minutes=15))\n    self.assertEqual(parse_duration('-P3DT3H15M30S'), timedelta(days=-3, hours=3, minutes=15, seconds=30))\n    self.assertEqual(parse_duration('-P3DT3H15M30.5S'), timedelta(days=-3, hours=3, minutes=15, microseconds=300000))\n\n    # Test a duration with decimal seconds\n    self.assertEqual(parse_duration('P3DT3H15M30.5S'), timedelta(days=3, hours=3, minutes=15, microseconds=300000))\n    self.assertEqual(parse_duration('-P3DT3H15M30.5S'), timedelta(days=-3, hours=3, minutes=15, microseconds=300000))\n\n    # Test a duration with decimal milliseconds\n    self.assertEqual(parse_duration('P3DT3H15M30.500S'), timedelta(days=3, hours=3, minutes=15, microseconds=300000))\n    self.assertEqual(parse_duration('-P3DT3H15M30.500S'), timedelta(days=-3, hours=3, minutes=15, microseconds=300000))\n", "    def test_parse_date_invalid_format(self):\n        self.assertIsNone(parse_date('2022'))\n        self.assertIsNone(parse_date('2022-02'))\n        self.assertIsNone(parse_date('2022-02-02 '))\n", "    def test_parse_duration_postgres_interval_with_days(self):\n        # Test a PostgreSQL day-time interval\n        self.assertEqual(\n            parse_duration('3 days 04:05:06.000123'),\n            timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=123)\n        )\n", "    def test_parse_duration_with_zero_values(self):\n        self.assertEqual(parse_duration('0 days'), timedelta(0))\n        self.assertEqual(parse_duration('0 hours'), timedelta(0))\n        self.assertEqual(parse_duration('0 minutes'), timedelta(0))\n        self.assertEqual(parse_duration('0 seconds'), timedelta(0))\n"], "sample_562": ["def test_line2d_dashed_line_properties():\n    fig, ax = plt.subplots()\n    ax.axhline(linestyle=(0, [10, 5]), color='r', lw=2)\n    assert ax.lines[0].get_linestyle() == (0, [10, 5])\n    assert ax.lines[0].get_color() == 'r'\n    assert ax.lines[0].get_linewidth() == 2.0\n\n    ax.axhline(linestyle=(0, [10, 5]), color='r', lw=2, drawstyle='steps-mid')\n    assert ax.lines[1].get_linestyle() == (0, [10, 5])\n    assert ax.lines[1].get_drawstyle() == 'steps-mid'\n\n    ax.axhline(linestyle=(0, [10, 5]), color='r', lw=2, drawstyle='steps-mid')\n    assert ax.lines[2].get_linestyle() == (0, [10, 5])\n    assert ax.lines[2].get_drawstyle() == 'steps-mid'\n\n    ax.axhline(linestyle=(0, [10, 5]), color='r', lw=2, drawstyle='steps-mid',\n                markersize=5, marker='o')\n    assert ax.lines[3].get_linestyle() == (0, [10, 5])\n    assert ax.lines[3].get_drawstyle() == 'steps-mid'\n    assert ax.lines[3].get_markersize() == 5.0\n    assert ax.lines[3].get_marker() == 'o'\n\n    ax.axhline(linestyle=(0, [10, 5]), color='r', lw=2, drawstyle='steps-mid',\n                markersize=5, marker='o', markeredgecolor='k', markerfacecolor='b')\n    assert ax.lines[4].get_linestyle() == (0, [10, 5])\n    assert ax.lines[4].get_drawstyle() == 'steps-mid'\n    assert ax.lines[4].get_markersize() == 5.0\n    assert ax.lines[4].get_marker() == 'o'\n    assert ax.lines[4].get_markeredgecolor() == 'k'\n    assert ax.lines[4].get_markerfacecolor() == 'b'\n\n    plt.close(fig)\n", "def test_line2d_drawstyle():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.sin(x)\n    line = mlines.Line2D(x, y, drawstyle='steps')\n    ax.add_line(line)\n    ax.draw()\n    assert line.get_path().codes[0] == mtransforms.STEP_TYPE1\n    line.set_drawstyle('default')\n    ax.draw()\n    assert line.get_path().codes[0] == mtransforms.LINE_TO\n", "def test_line2d_markersize_roundtrip():\n    # Test that set_markersize and get_markersize do not change the marker size.\n    fig, ax = plt.subplots()\n    line, = ax.plot([0, 1], [0, 1], marker='o', markersize=10)\n    assert line.get_markersize() == 10\n    new_size = 20\n    line.set_markersize(new_size)\n    assert line.get_markersize() == new_size\n    assert line.get_markeredgecolor() == 'auto'\n    assert line.get_markerfacecolor() == 'auto'\n    assert line.get_marker() == 'o'\n    assert line.get_fillstyle() == 'full'\n    line.set_marker('o')\n    assert line.get_marker() == 'o'\n    assert line.get_fillstyle() == 'full'\n    line.set_marker('o', fillstyle='left')\n    assert line.get_marker() == 'o'\n    assert line.get_fillstyle() == 'left'\n    assert line.get_markeredgecolor() == 'auto'\n    assert line.get_markerfacecolor() == 'auto'\n", "def test_Line2D_subslice_optim_min_size():\n    # Test that the subslice optimization only occurs when the data size is greater than the min size\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 100)\n    ax.set_ylim(0, 100)\n\n    # Create a Line2D instance with a large number of data points\n    x = np.linspace(0, 100, 10000)\n    y = np.linspace(0, 100, 10000)\n    line = ax.plot(x, y)[0]\n\n    # Set the minimum subslice size to a value smaller than the number of data points\n    line._subslice_optim_min_size = 5000\n\n    # Set the data\n    line.set_data(x, y)\n\n    # Check that the Line2D instance has a transformed path\n    assert line._transformed_path is not None\n\n    # Now decrease the minimum subslice size to a value smaller than the number of data points\n    line._subslice_optim_min_size = 4000\n\n    # Set the data again\n    line.set_data(x, y)\n\n    # Check that the Line2D instance has a transformed path\n    assert line._transformed_path is not None\n\n    # Now decrease the minimum subslice size to a value smaller than the number of data points\n    line._subslice_optim_min_size = 3000\n\n    # Set the data again\n    line.set_data(x, y)\n\n    # Check that the Line2D instance has a transformed path\n    assert line._transformed_path is not None\n\n    # Now decrease the minimum subslice size to a value larger than the number of data points\n    line._subslice_optim_min_size = 15000\n\n    # Set the data again\n    line.set_data(x, y)\n\n    # Check that the Line2D instance does not have a transformed path\n    assert line._transformed_path is None\n\n    plt.close(fig)\n", "def test_set_markeredgecolor_auto():\n    # Make sure the auto default doesn't raise an exception\n    line = mlines.Line2D([0], [0], marker='.', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == (0.0, 0.0, 0.0, 1.0)\n\n    # Test auto default with 'auto' value\n    line = mlines.Line2D([0], [0], marker='.', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == 'auto'\n\n    # Test 'auto' with a non-'auto' marker\n    line = mlines.Line2D([0], [0], marker='.', markeredgecolor='auto')\n    line._marker = MarkerStyle('x', 'full')\n    assert line.get_markeredgecolor() == 'auto'\n\n    # Test non-'auto' value with a 'auto' marker\n    line = mlines.Line2D([0], [0], marker='.', markeredgecolor='auto')\n    line._marker = MarkerStyle('o', 'auto')\n    assert line.get_markeredgecolor() == 'auto'\n\n    # Test 'auto' with a marker that requires edgecolor\n    line = mlines.Line2D([0], [0], marker=',', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == (0.0, 0.0, 0.0, 1.0)\n\n    # Test 'auto' with a marker that doesn't require edgecolor\n    line = mlines.Line2D([0], [0], marker='x', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == 'auto'\n\n    # Test 'auto' with a filled marker\n    line = mlines.Line2D([0], [0], marker='o', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == 'auto'\n\n    # Test 'auto' with a non-filled marker\n    line = mlines.Line2D([0], [0], marker='o', markerfacecolor='red', markeredgecolor='auto')\n    assert line.get_markeredgecolor() == (0.0, 0.0, 0.0, 1.0)\n"], "sample_261": ["    def test_parse_duration_with_days(self):\n        self.assertEqual(parse_duration('10 days'), timedelta(days=10))\n        self.assertEqual(parse_duration('-10 days'), timedelta(days=-10))\n", "    def test_parse_datetime_with_timezone_offset(self):\n        # Test a string with a timezone offset\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00+02:00'), datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(120)))\n        \n        # Test a string with a negative timezone offset\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00-02:00'), datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(-120)))\n\n        # Test a string with a timezone offset without a colon\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00+0200'), datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(120)))\n", "    def test_parse_duration_iso8601_with_float_seconds(self):\n        # Test parsing a duration with decimal seconds\n        self.assertEqual(\n            parse_duration('PT1.25S'),\n            timedelta(seconds=1.25)\n        )\n", "    def test_parse_duration_iso8601_multiple_decimal_places(self):\n        # Test parsing a duration with multiple decimal places.\n        duration = parse_duration('P1.25D')\n        self.assertEqual(duration, timedelta(days=1.25))\n", "    def test_parse_date_with_year_out_of_range(self):\n        self.assertIsNone(parse_date('9999-01-01'))\n"], "sample_829": ["def test_incremental_pca_inverse_transform():\n    \"\"\"Check that the inverse transform is equivalent to the original data.\"\"\"\n    X = iris.data\n    ipca = IncrementalPCA(n_components=2, batch_size=3)\n    X_ipca = ipca.fit_transform(X)\n    X_inv = ipca.inverse_transform(X_ipca)\n    assert_array_almost_equal(X, X_inv)\n", "def test_inconsistent_n_components():\n    \"\"\"Test setting n_components after partial_fit.\"\"\"\n    ipca = IncrementalPCA(n_components=2)\n    ipca.partial_fit(iris.data[:10])\n    with assert_raises_regex(ValueError, \"n_components=10 must be less or \"\n                                        \"equal to the batch number of samples \"\n                                        \"10\"):\n        ipca.n_components = 10\n        ipca.partial_fit(iris.data[10:20])\n", "def test_partial_fit_with_zero_data():\n    \"\"\"Test partial_fit with zero data.\"\"\"\n    X = np.zeros((10, 10))\n    ipca = IncrementalPCA(n_components=5)\n    ipca.partial_fit(X)\n    assert ipca.components_.shape == (5, 10)\n    assert ipca.mean_.shape == (10,)\n    assert ipca.var_.shape == (10,)\n    assert ipca.explained_variance_.shape == (5,)\n    assert ipca.explained_variance_ratio_.shape == (5,)\n    assert ipca.singular_values_.shape == (5,)\n    assert ipca.n_samples_seen_ == 10\n    assert ipca.n_components_ == 5\n", "def test_incremental_pca_whitening():\n    X = np.array([[-1, -1], [-2, -1], [-3, -2],\n                  [1, 1], [2, 1], [3, 2]])\n    ipca = IncrementalPCA(n_components=2, batch_size=3, whiten=True)\n    ipca.fit(X)\n    X_white = ipca.transform(X)\n    ipca = IncrementalPCA(n_components=2, batch_size=3, whiten=False)\n    ipca.fit(X)\n    X_standard = ipca.transform(X)\n\n    assert_array_almost_equal(np.var(X_white, axis=0), np.ones(2))\n    assert_array_almost_equal(np.var(X_standard, axis=0), np.var(X, axis=0))\n\n    # Check that whitening doesn't change the result of fit_transform\n    ipca = IncrementalPCA(n_components=2, batch_size=3, whiten=True)\n    X_white_fit_transform = ipca.fit_transform(X)\n    ipca = IncrementalPCA(n_components=2, batch_size=3, whiten=False)\n    X_standard_fit_transform = ipca.fit_transform(X)\n\n    assert_array_almost_equal(X_white_fit_transform, X_standard_fit_transform)\n", "def test_incremental_pca_n_components_float():\n    # Test with n_components = 0.5\n    ipca = IncrementalPCA(n_components=0.5)\n    X = iris.data\n    ipca.fit(X)\n    assert ipca.n_components_ == 3\n\n    # Test with n_components = 0\n    ipca = IncrementalPCA(n_components=0)\n    ipca.fit(X)\n    assert ipca.n_components_ == 0\n\n    # Test with n_components = 1\n    ipca = IncrementalPCA(n_components=1)\n    ipca.fit(X)\n    assert ipca.n_components_ == 1\n\n    # Test with n_components = 1.5 (should raise an error)\n    with assert_raises(ValueError):\n        IncrementalPCA(n_components=1.5)\n"], "sample_816": ["def test_TfidfVectorizer_inverse_transform_empty_input():\n    vectorizer = TfidfVectorizer()\n    X = vectorizer.fit_transform([''])\n    with pytest.raises(ValueError):\n        vectorizer.inverse_transform(X)\n", "def test_HashingVectorizer():\n    vectorizer = HashingVectorizer(analyzer='word')\n    X = vectorizer.fit_transform(['This is a test'])\n    expected_shape = (1, 2 ** 20)\n    assert X.shape == expected_shape\n", "def test_count_vectorizer_binary_flag():\n    vectorizer = CountVectorizer(binary=True)\n    X = vectorizer.fit_transform([\"hello world\", \"world cup\"])\n    assert_array_equal(X.toarray(), [[1, 1], [1, 1]])\n", "def test_HashingVectorizer_sparse_input():\n    X = sparse.csr_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n    vectorizer = HashingVectorizer()\n    with pytest.raises(ValueError):\n        vectorizer.transform(X)\n", "def test_HashingVectorizer_fit_transform():\n    \"\"\"Test HashingVectorizer.fit_transform()\"\"\"\n    X = HashingVectorizer(n_features=2 ** 10).fit_transform(\n        [\"hello world\", \"world\", \"hello\"])\n    assert X.shape == (3, 1024)\n    assert X.nnz == 3\n    assert X[0, :5].sum() == 1.0\n    assert X[1, 2].x == 1.0\n    assert X[2, 0].x == 1.0\n"], "sample_830": ["def test_get_blas_info_without_sklearn():\n    \"\"\"Test that get_blas_info does not crash when sklearn is not installed\"\"\"\n    import importlib\n    import unittest.mock as mock\n    from sklearn.utils._show_versions import _get_blas_info\n\n    # Mock sklearn._build_utils.get_blas_info to return None\n    with mock.patch('sklearn._build_utils.get_blas_info', return_value=(None, None)):\n        with mock.patch('importlib.import_module', side_effect=ImportError):\n            with mock.patch('importlib.import_module') as mock_importlib:\n                mock_importlib.side_effect = ImportError\n                assert _get_blas_info() == {}\n", "def test_get_blas_info():\n    \"\"\"Test _get_blas_info function\"\"\"\n    # Test case with expected BLAS info\n    expected_blas_info = {\n        'macros': '',\n        'lib_dirs': '',\n        'cblas_libs': ''\n    }\n    blas_dict = {'define_macros': [], 'library_dirs': ''}\n    with mock.patch('sklearn._build_utils.get_blas_info',\n                   return_value=([], blas_dict)):\n        blas_info = _get_blas_info()\n        assert blas_info == expected_blas_info\n\n    # Test case with expected ImportError\n    with mock.patch('sklearn._build_utils.get_blas_info',\n                   side_effect=ImportError):\n        with pytest.raises(ImportError):\n            _get_blas_info()\n\n    # Test case with no macros in BLAS info\n    blas_dict = {'define_macros': []}\n    with mock.patch('sklearn._build_utils.get_blas_info',\n                   return_value=([], blas_dict)):\n        blas_info = _get_blas_info()\n        assert 'macros' not in blas_info\n\n    # Test case with no library_dirs in BLAS info\n    blas_dict = {'library_dirs': ''}\n    with mock.patch('sklearn._build_utils.get_blas_info',\n                   return_value=([], blas_dict)):\n        blas_info = _get_blas_info()\n        assert 'lib_dirs' not in blas_info\n\n    # Test case with no cblas_libs in BLAS info\n    blas_dict = {'library_dirs': '', 'define_macros': []}\n    with mock.patch('sklearn._build_utils.get_blas_info',\n                   return_value=([], blas_dict)):\n        blas_info = _get_blas_info()\n        assert 'cblas_libs' not in blas_info\n", "def test_get_deps_info():\n    \"\"\"Test _get_deps_info function\"\"\"\n    # Test case: all dependencies are installed\n    deps_info = _get_deps_info()\n    expected_deps_info = {\n        \"pip\": None,\n        \"setuptools\": None,\n        \"sklearn\": None,\n        \"numpy\": None,\n        \"scipy\": None,\n        \"Cython\": None,\n        \"pandas\": None,\n        \"matplotlib\": None,\n    }\n    assert deps_info == expected_deps_info\n\n    # Test case: some dependencies are not installed\n    # Import the dependencies that will not be installed\n    import numpy as np\n    import scipy as sp\n\n    # Update sys.modules so that the dependencies are not re-imported\n    sys.modules[\"numpy\"] = np\n    sys.modules[\"scipy\"] = sp\n\n    deps_info = _get_deps_info()\n    expected_deps_info = {\n        \"pip\": None,\n        \"setuptools\": None,\n        \"sklearn\": None,\n        \"numpy\": np.__version__,\n        \"scipy\": sp.__version__,\n        \"Cython\": None,\n        \"pandas\": None,\n        \"matplotlib\": None,\n    }\n    assert deps_info == expected_deps_info\n\n    # Test case: some dependencies raise ImportError\n    # Import the dependencies that will raise ImportError\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    # Update sys.modules so that the dependencies are not re-imported\n    sys.modules[\"pandas\"] = pd\n    sys.modules[\"matplotlib\"] = plt\n\n    # Remove the dependencies from sys.modules so that they raise ImportError\n    del sys.modules[\"pandas\"]\n    del sys.modules[\"matplotlib\"]\n\n    deps_info = _get_deps_info()\n    expected_deps_info = {\n        \"pip\": None,\n        \"setuptools\": None,\n        \"sklearn\": None,\n        \"numpy\": None,\n        \"scipy\": None,\n        \"Cython\": None,\n        \"pandas\": None,\n        \"matplotlib\": None,\n    }\n    assert deps_info == expected_deps_info\n", "    def test_blas_info(self):\n        blas_info = _get_blas_info()\n        self.assertIsNotNone(blas_info)\n        self.assertIn('macros', blas_info)\n        self.assertIn('lib_dirs', blas_info)\n        self.assertIn('cblas_libs', blas_info)\n", "def test_get_sys_info():\n    \"\"\"Test system information function\n\n    \"\"\"\n    expected = {\n        \"python\": sys.version.replace('\\n', ' '),\n        \"executable\": sys.executable,\n        \"machine\": platform.platform(),\n    }\n    assert _get_sys_info() == expected\n\n"], "sample_567": ["def test_text_wrap():\n    fig, ax = plt.subplots()\n    t = Text('Hello World', 0.5, 0.5)\n    ax.add_patch(mpatches.Rectangle((0, 0), 1, 1))\n    t.set_wrap(True)\n    t.set_horizontalalignment('left')\n    t.draw(ax.figure.canvas.renderer)\n    t.set_linespacing(1.5)\n    t.draw(ax.figure.canvas.renderer)\n    fig.canvas.draw()\n\n    # Testing for get_wrap\n    assert t.get_wrap() is True\n    t.set_wrap(False)\n    assert t.get_wrap() is False\n\n    # Testing for get_wrap_line_width\n    assert isinstance(t._get_wrap_line_width(), int)\n\n    # Testing for get_unitless_position\n    assert t.get_unitless_position() == (0.5, 0.5)\n\n    # Testing for get_position\n    assert t.get_position() == (0.5, 0.5)\n\n    # Testing for get_text\n    assert t.get_text() == 'Hello World'\n\n    # Testing for set_text\n    t.set_text('Hello World 2')\n    assert t.get_text() == 'Hello World 2'\n\n    # Testing for set_fontproperties\n    t.set_fontproperties(FontProperties('Arial'))\n    assert t.get_fontproperties().get_name() == 'Arial'\n\n    # Testing for set_usetex\n    t.set_usetex(True)\n    assert t.get_usetex() is True\n    t.set_usetex(False)\n    assert t.get_usetex() is False\n\n    # Testing for set_parse_math\n    t.set_parse_math(True)\n    assert t.get_parse_math() is True\n    t.set_parse_math(False)\n    assert t.get_parse_math() is False\n\n    # Testing for set_fontname\n    t.set_fontname('Times')\n    assert t.get_fontname() == 'Times'\n    t.set_fontname('serif')\n    assert t.get_fontname() == 'serif'\n\n    # Testing for set_fontsize\n    t.set_fontsize(12)\n    assert t.get_fontsize() == 12\n    t.set_fontsize('large')\n    assert t.get_fontsize() == 12\n\n    # Testing for set_fontweight\n    t.set_fontweight('bold')\n    assert t.get_fontweight() == 'bold'\n    t.set_fontweight(500)\n    assert t.get_font", "def test_text_fontsize_from_fontfamily(fontprop, expected_fontsize):\n    fig, ax = plt.subplots()\n    text = Text(x=0.5, y=0.5, text=\"Test\", fontproperties=fontprop)\n    ax.add_artist(text)\n    fig.canvas.draw()\n    bbox = text.get_window_extent(fig.canvas.get_renderer())\n    assert_almost_equal(bbox.width, expected_fontsize)\n", "def test_vertical_alignment():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n\n    annotation = Annotation('test', (5, 5), textcoords='data',\n                          xytext=(5, 5), xycoords='data',\n                          verticalalignment='center_baseline')\n\n    ax.add_patch(annotation)\n\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n\n    bbox = annotation.get_window_extent(renderer)\n\n    assert bbox.y0 == 5  # Center of the text at y=5\n    assert bbox.y1 == 9  # Top of the text at y=9\n\n    # Testing other vertical alignments\n    annotation.set_verticalalignment('bottom')\n    bbox = annotation.get_window_extent(renderer)\n    assert bbox.y0 == 5  # Bottom of the text at y=5\n    assert bbox.y1 == 5  # Top of the text at y=5\n\n    annotation.set_verticalalignment('top')\n    bbox = annotation.get_window_extent(renderer)\n    assert bbox.y0 == 9  # Bottom of the text at y=9\n    assert bbox.y1 == 9  # Top of the text at y=9\n\n    annotation.set_verticalalignment('baseline')\n    bbox = annotation.get_window_extent(renderer)\n    assert bbox.y0 == 5  # Bottom of the text at y=5\n    assert bbox.y1 == 5  # Top of the text at y=5\n\n    annotation.set_verticalalignment('center')\n    bbox = annotation.get_window_extent(renderer)\n    assert bbox.y0 == 4.5  # Center of the text at y=4.5\n    assert bbox.y1 == 5.5  # Top of the text at y=5.5\n", "def test_text_wrap_not_working_with_rcParams_tight_bbox():\n    \"\"\"Tests that wrapping is not working with rcParams['savefig.bbox'] set to 'tight'.\"\"\"\n    import matplotlib as mpl\n    from matplotlib.testing.decorators import check_figures_equal\n    import matplotlib.pyplot as plt\n\n    # Make sure that rcParams['savefig.bbox'] is 'tight'\n    mpl.rcParams['savefig.bbox'] = 'tight'\n\n    # Create a figure\n    fig, ax = plt.subplots()\n\n    # Create a Text object with wrapping enabled\n    text = Text('Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.')\n\n    # Add the text to the axes\n    ax.add_artist(text)\n\n    # Save the figure with bbox_inches='tight'\n    buf = io.BytesIO()\n    fig.savefig(buf, bbox_inches='tight')\n\n    # Check that the figure has been saved correctly\n    check_figures_equal(fig, plt.imread(buf))\n\n    # Check that the wrapping is not working\n    assert text.get_text() == 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'\n\n    # Reset rcParams['savefig.bbox']\n    mpl.rcParams['savefig.bbox'] = 'standard'\n\n    # Check that the wrapping is working\n    text.set_wrap(True)\n    text.set_position((0.5, 0.5))\n    text.draw(ax.figure.canvas.renderer)\n    assert text.get_text() == 'Lorem ipsum\\ndolor sit amet,\\nconsectetur adipiscing elit,\\nsed do eiusmod tempor incididunt ut labore et dolore magna aliqua.'\n\n    # Restore the original rcParams['savefig.bbox']\n    mpl.rcParams['savefig.bbox'] = 'tight'\n", "def test_text_alignment(alignment, figure):\n    ax = figure.add_subplot(111)\n    t = Text(0.5, 0.5, \"Test\", horizontalalignment=alignment)\n    ax.add_artist(t)\n    figure.canvas.draw()\n    assert t.get_horizontalalignment() == alignment\n    assert t.get_transform_rotates_text() is False\n\n    # Check that alignment is respected when rotation is not 0\n    t.set_rotation(45)\n    assert t.get_transform_rotates_text() is True\n    assert t.get_horizontalalignment() == alignment\n"], "sample_912": ["def test_handle_signature_pyclasslike():\n    signode = addnodes.desc_signature('', '')\n    pyclass = PyClasslike()\n    pyclass.handle_signature('class TestClass', signode)\n    assert signode.astext() == 'class TestClass'\n", "def test_parse_annotation():\n    annotation = 'typing.List[str]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'typing.List[str]', nodes.Text, 'typing.List[str]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'typing.List[str][int]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'typing.List[str][int]', nodes.Text, 'typing.List[str][int]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'List[str]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'List[str]', nodes.Text, 'List[str]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'List[int]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'List[int]', nodes.Text, 'List[int]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'List'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'List', nodes.Text, 'List')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'typing.List'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'typing.List', nodes.Text, 'typing.List')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'List[str, int]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'List[str, int]', nodes.Text, 'List[str, int]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n\n    annotation = 'List[int, str]'\n    result = _parse_annotation(annotation)\n    assert_node(result, 'List[int, str]', nodes.Text, 'List[int, str]')\n    assert len(result) == 1\n    assert isinstance(result[0], nodes.Text)\n", "def test_parse_annotation():\n        result = _parse_annotation(annotation)\n        expected = [\n            nodes.Text('typing.List'),\n            addnodes.desc_sig_punctuation(''),\n            nodes.Text('['),\n            nodes.Text('int'),\n            addnodes.desc_sig_punctuation(''),\n            nodes.Text(']'),\n            addnodes.desc_sig_punctuation(''),\n            nodes.Text('float'),\n        ]\n        assert result == expected\n\n    assert_parse_annotation('List[int, float]')\n    assert_parse_annotation('List[int, float] -> float')\n\n        with pytest.raises(SyntaxError):\n            _parse_annotation(annotation)\n\n    assert_parse_annotation_syntax_error('List[int,')\n    assert_parse_annotation_syntax_error('List[int, float,')\n    assert_parse_annotation_syntax_error('List[int, float,]')\n\n        result = _parse_annotation(annotation)\n        assert result == [\n            make_xref(annotation, '')\n        ]\n\n    assert_parse_annotation_unparseable('typing.List[int, float] = value')\n    assert_parse_annotation_unparseable('typing.List[int, float] = value -> float')\n", "def test_PyFunction_run():\n    app = Mock()\n    env = Mock()\n    state = Mock()\n    node = desc_signature('')\n    directive = PyFunction()\n    result = directive.run()\n    assert node.astext() == ''\n", "def test_parse_annotation():\n    assert _parse_annotation('str') == [\n        nodes.Text('str'),\n    ]\n    assert _parse_annotation('List[int]') == [\n        addnodes.desc_sig_name('', 'List'),\n        nodes.Text('['),\n        nodes.Text('int'),\n        nodes.Text(']'),\n    ]\n    assert _parse_annotation('Dict[str, int]') == [\n        addnodes.desc_sig_name('', 'Dict'),\n        nodes.Text('['),\n        nodes.Text('str'),\n        nodes.Text(', '),\n        nodes.Text('int'),\n        nodes.Text(']'),\n    ]\n    assert _parse_annotation('List[int] -> int') == [\n        addnodes.desc_sig_name('', 'List'),\n        nodes.Text('['),\n        nodes.Text('int'),\n        nodes.Text(']'),\n        nodes.Text(' -> '),\n        nodes.Text('int'),\n    ]\n"], "sample_507": ["    def test_conversion_with_numeric_values(self):\n        data = np.array([1, 2, 3])\n        converter = StrCategoryConverter()\n        with pytest.raises(ValueError):\n            converter.convert(data, None, None)\n", "def test_unit_data_invalid_data():\n    \"\"\"Test creation of UnitData with invalid data.\"\"\"\n    with pytest.raises(TypeError):\n        UnitData([1, 2, 3])\n    with pytest.raises(TypeError):\n        UnitData(\"hello world\")\n    with pytest.raises(TypeError):\n        UnitData(123)\n", "    def test_default_units(self):\n        fig, ax = plt.subplots()\n        ax.plot([\"a\", \"b\", \"c\"], [1, 2, 3])\n        units = StrCategoryConverter.default_units([\"a\", \"b\", \"c\"], ax)\n        assert units._mapping == {\"a\": 0, \"b\": 1, \"c\": 2}\n        assert units._counter == itertools.count(2)\n", "    def test_empty_unit(self):\n        # Test that a ValueError is raised when unit is None\n        with pytest.raises(ValueError):\n            StrCategoryConverter.convert('a', None, None)\n", "    def test_str_is_convertible(self):\n        unit_data = UnitData([\"hello world\"])\n        assert unit_data._str_is_convertible(\"hello world\")\n        assert not unit_data._str_is_convertible(\"hello\")\n        assert unit_data._str_is_convertible(\"3.14\")\n        with pytest.raises(ValueError):\n            unit_data._str_is_convertible(\"np.nan\")\n"], "sample_677": ["def test_empty_expression():\n    # Test empty expression evaluates to False\n    assert not evaluate(\"\", lambda _: True)\n", "def test_empty_expression():\n    \"\"\"Test that an empty match expression evaluates to False.\"\"\"\n    assert not evaluate(\"\", lambda x: True)\n", "def test_parse_empty_expression():\n    assert evaluate(\"\", lambda x: True) is False\n", "def test_evaluate_empty_expression():\n    # Ensure that an empty expression evaluates to False\n    assert not evaluate(\"\", lambda _: True)\n", "def test_evaluate_not_ident():\n    \"\"\"Test that a 'not' expression is correctly evaluated.\"\"\"\n    matcher = lambda ident: ident != \"fail\"\n    assert evaluate('not not fail', matcher) is True\n    assert evaluate('not not pass', matcher) is False\n    assert evaluate('not (not fail)', matcher) is False\n"], "sample_565": ["def test_inset_axes_loc(loc):\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 10)\n    ax.set_ylim(0, 10)\n    ax.plot([1, 2, 3])\n    ax.set_title(\"Parent axes\")\n\n    axins = inset_axes(ax, width=\"30%\", height=\"40%\", loc=loc)\n    axins.plot([2, 3, 4])\n\n    fig.canvas.draw_idle()\n\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n\n    assert ax.get_title() == \"Parent axes\"\n    assert ax.get_xlabel() is None\n    assert ax.get_ylabel() is None\n    assert axins.get_title() is None\n    assert axins.get_xlabel() is None\n    assert axins.get_ylabel() is None\n\n    plt.close(fig)\n", "def test_mark_inset_invalid_loc():\n    fig, ax = plt.subplots()\n    with pytest.raises(ValueError):\n        mark_inset(ax, ax, loc1=5, loc2=3)\n\n", "def test_zoomed_inset_axes_zoom_zoom_label(zoom, expected_zoom):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n    zoomed_inset_axes(ax1, zoom, loc=\"upper right\", borderpad=0.5)\n    ax1.plot(np.linspace(0, 10, 100), np.sin(np.linspace(0, 10, 100)))\n    ax1.set_xlim(0, 5)\n    ax1.set_ylim(-1, 1)\n    ax2.plot(np.linspace(0, 10, 100), np.sin(np.linspace(0, 10, 100)))\n    ax2.set_xlim(0, 5)\n    ax2.set_ylim(-1, 1)\n    ax2.set_title(f\"Zoom: {expected_zoom}\")\n    fig.tight_layout()\n    plt.show()\n", "def test_InsetPosition():\n    # Test InsetPosition with a parent axes that has a custom aspect ratio\n    fig, ax = plt.subplots(figsize=(4, 2), aspect='equal')\n    ip = InsetPosition(ax, [0.1, 0.1, 0.5, 0.3])\n    ax2 = plt.axes(ip)\n    ax2.set_aspect('equal')\n    ax2.set_xlim(-1, 1)\n    ax2.set_ylim(-1, 1)\n    # Check that the inset axes have the correct aspect ratio and size\n    assert ax2.get_aspect() == 'equal'\n    assert ax2.get_position().width == 0.5\n    assert ax2.get_position().height == 0.3\n\n    # Test InsetPosition with a parent axes that has a default aspect ratio\n    fig, ax = plt.subplots(figsize=(4, 2))\n    ip = InsetPosition(ax, [0.1, 0.1, 0.5, 0.3])\n    ax2 = plt.axes(ip)\n    ax2.set_xlim(-1, 1)\n    ax2.set_ylim(-1, 1)\n    # Check that the inset axes have the correct size\n    assert ax2.get_position().width == 0.5\n    assert ax2.get_position().height == 0.3\n\n    # Test InsetPosition with a parent axes that has a rotated position\n    fig, ax = plt.subplots(figsize=(4, 2))\n    ip = InsetPosition(ax, [0.2, 0.2, 0.4, 0.3])\n    ax2 = plt.axes(ip)\n    ax2.set_xlim(-1, 1)\n    ax2.set_ylim(-1, 1)\n    # Check that the inset axes have the correct position and size\n    assert ax2.get_position().x == 0.2\n    assert ax2.get_position().y == 0.2\n    assert ax2.get_position().width == 0.4\n    assert ax2.get_position().height == 0.3\n\n    # Test InsetPosition with a parent axes that has a negative position\n    fig, ax = plt.subplots(figsize=(4, 2))\n    ip = InsetPosition(ax, [-0.2, 0.2, 0.4, 0.3])\n    ax2 = plt.axes(ip)\n    ax2.set", "def test_zoomed_inset_axes_png():\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    ax1 = axs[0]\n    ax2 = axs[1]\n    x = np.linspace(0, 5, 500)\n    y1 = np.sin(x)\n    y2 = np.sin(x * 1.5)\n    ax1.plot(x, y1)\n    ax2.plot(x, y2)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('sin(x)')\n    ax2.set_title('Zoomed-In inset plot')\n    ax2 = zoomed_inset_axes(ax2, zoom=2, loc='upper left', borderpad=0.1)\n    ax2.plot(x, y2)\n    ax2.set_xlim(2.5, 5)\n    ax2.set_ylim(0, 2)\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    mark_inset(ax2, ax2, loc1=2, loc2=4, fc='none', edgecolor='black')\n    plt.show()\n"], "sample_547": ["    def test_bbox_artist(self, plt):\n        fig, ax = plt.subplots()\n        ax.add_patch(mpatches.Rectangle((0, 0), 1, 1, facecolor='r'))\n        off = OffsetBox()\n        off.add_artist(ax.patches[0])\n        off.set_offset((1, 1))\n        off.draw(ax.figure.canvas.renderer)\n        assert ax.patches[0].get_offset() == (1, 1)\n", "def test_AnchoredOffsetbox():\n    fig, ax = plt.subplots()\n    box = AnchoredOffsetbox(loc=\"center\", pad=1.0, borderpad=1.0)\n    ax.add_artist(box)\n    assert box.get_visible() is False\n    assert box.get_zorder() == 5\n    ax.set_xlim(-10, 10)\n    ax.set_ylim(-10, 10)\n    assert box.get_window_extent(fig.canvas.get_renderer()) == Bbox([[0.0, 0.0], [0.0, 0.0]])\n\n    box.set_visible(True)\n    assert box.get_visible() is True\n    assert box.get_zorder() == 5\n\n    box.set_frame_on(False)\n    assert box.get_frame_on() is False\n    box.set_frame_on(True)\n    assert box.get_frame_on() is True\n\n    box.set_bbox_to_anchor((0, 0))\n    assert box.get_bbox_to_anchor() == Bbox([[0, 0], [0, 0]])\n\n    box.set_bbox_to_anchor((0.5, 0.5))\n    assert box.get_bbox_to_anchor() == Bbox([[0.5, 0.5], [0.5, 0.5]])\n\n    box.set_bbox_to_anchor((0.5, 0.5), fig.transFigure)\n    assert box.get_bbox_to_anchor() == TransformedBbox(Bbox([[0.5, 0.5], [0.5, 0.5]], None), fig.transFigure)\n\n    box.set_bbox_to_anchor((0.5, 0.5), None)\n    assert box.get_bbox_to_anchor() == Bbox([[0.5, 0.5], [0.5, 0.5]])\n\n    box.set_bbox_transform(fig.transFigure)\n    assert box.get_bbox_transform() is fig.transFigure\n\n    box.set_bbox_transform(None)\n    assert box.get_bbox_transform() is None\n\n    box.set_child(OffsetBox())\n    assert box.get_child() is not None\n\n    assert box.codes == {'upper right': 1, 'upper left': 2, 'lower left': 3, 'lower right': 4, 'right': 5, 'center left': 6, 'center right': 7, 'lower center': 8, 'upper center': 9, 'center': 10}\n\n    box", "def test_AnchoredOffsetbox_frameon_properties():\n    fig, ax = plt.subplots()\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n\n    anchored = AnchoredOffsetbox(\n        loc=\"center\",\n        child=TextArea(\"Text\"),\n        prop=FontProperties(size=10),\n        frameon=True,\n        borderpad=0.2,\n        pad=0.3,\n        bbox_transform=None,\n    )\n    anchored.set_bbox_to_anchor((0.5, 0.5))\n    ax.add_artist(anchored)\n    fig.canvas.draw_idle()\n\n    assert anchored.patch.get_facecolor() == (1, 1, 1, 1)\n    assert anchored.patch.get_edgecolor() == (0, 0, 0, 1)\n    assert anchored.patch.get_linewidth() == 1\n    assert anchored.patch.get_boxstyle() == \"square,pad=0.3\"\n\n    anchored.patch.set_facecolor((0, 0, 0, 0.5))\n    anchored.patch.set_edgecolor((1, 0, 0, 1))\n    anchored.patch.set_linewidth(2)\n    anchored.patch.set_boxstyle(\"round,pad=0.4\")\n\n    fig.canvas.draw_idle()\n\n    assert anchored.patch.get_facecolor() == (0, 0, 0, 0.5)\n    assert anchored.patch.get_edgecolor() == (1, 0, 0, 1)\n    assert anchored.patch.get_linewidth() == 2\n    assert anchored.patch.get_boxstyle() == \"round,pad=0.4\"\n", "def test_TextArea_get_text():\n    fig, ax = plt.subplots()\n    text_area = TextArea('Hello, World!')\n    assert text_area.get_text() == 'Hello, World!'\n    text_area.set_text('New text')\n    assert text_area.get_text() == 'New text'\n", "    def test_packer_base_invalid_mode(self):\n        with pytest.raises(ValueError):\n            HPacker(mode=\"invalid\")\n"], "sample_627": ["def test_concat_data_vars_invalid_variables():\n    \"\"\"Ensure that invalid data_vars values raise an error\"\"\"\n    da = DataArray(\n        np.arange(6).reshape(2, 3), [(\"x\", [\"a\", \"b\"]), (\"y\", [10, 20, 30])]\n    )\n\n    with pytest.raises(ValueError):\n        concat([da.isel(y=slice(0, 1)), da.isel(y=slice(1, None))], dim=\"y\", data_vars=\"invalid\")\n\n    with pytest.raises(ValueError):\n        concat([da.isel(x=0), da.isel(x=1)], \"x\", data_vars=[1, 2])\n\n    with pytest.raises(ValueError):\n        concat([da.isel(x=0), da.isel(x=1)], \"x\", data_vars=[\"a\", \"b\", 1])\n\n    with pytest.raises(ValueError):\n        concat([da.isel(x=0), da.isel(x=1)], \"x\", data_vars={\"a\": 1, \"b\": 2})\n\n    with pytest.raises(ValueError):\n        concat([da.isel(x=0), da.isel(x=1)], \"x\", data_vars=[1, 2, \"a\"])\n", "def test_concat_attrs_with_override_and_conflicting_attrs():\n    \"\"\"Test that attrs with override combine attrs option are correctly handled\n    when there are conflicts between them.\n    \"\"\"\n    from xarray import Dataset\n\n    ds1 = Dataset(\n        data_vars={\"x\": ([\"y\", \"z\"], np.random.rand(2, 2))},\n        coords={\"y\": (\"y\", [1, 2])},\n        attrs={\"a\": \"b\", \"c\": \"d\"},\n    )\n    ds2 = Dataset(\n        data_vars={\"x\": ([\"y\", \"z\"], np.random.rand(2, 2))},\n        coords={\"y\": (\"y\", [1, 2])},\n        attrs={\"c\": \"e\", \"f\": \"g\"},\n    )\n\n    result = concat([ds1, ds2], dim=\"y\", combine_attrs=\"override\")\n    assert_equal(result.attrs, {\"a\": \"b\", \"c\": \"e\", \"f\": \"g\"})\n", "def test_concat_compat_equals_overrides_attrs():\n    from xarray.tests import assert_equal\n\n    ds1 = Dataset(\n        data_vars={\"a\": ([\"x\"], np.ones(3)), \"b\": ([\"x\"], np.ones(3))},\n        coords={\"x\": [1, 2, 3]},\n    )\n    ds2 = Dataset(\n        data_vars={\"a\": ([\"x\"], np.ones(3)), \"b\": ([\"x\"], np.ones(3))},\n        attrs={\"description\": \"Test dataset\"},\n        coords={\"x\": [1, 2, 3]},\n    )\n\n    # Test that the attrs are overridden when using compat='equals'\n    result = concat([ds1, ds2], dim=\"x\", compat=\"equals\")\n    assert_equal(result.attrs, {\"description\": \"Test dataset\"})\n\n", "def test_concat_datasets_with_empty_dim():\n    # Test that concatenating datasets with an empty dimension works as expected\n    datasets = [\n        Dataset({\"a\": ([\"x\"], np.array([1, 2]))},\n                coords={\"x\": [\"x1\", \"x2\"]}),\n        Dataset({\"a\": ([\"x\"], np.array([3, 4]))},\n                coords={\"x\": [\"x1\", \"x2\"]}),\n    ]\n    concatenated = concat(datasets, dim=\"x\")\n    assert_equal(concatenated[\"a\"], np.array([1, 2, 3, 4]))\n    assert_equal(concatenated.coords[\"x\"], [\"x1\", \"x1\", \"x2\", \"x2\"])\n", "def test_concat_compat_different_equals():\n    \"\"\"Tests that ``compat='equals'`` works as expected when comparing variables with different values\"\"\"\n    datasets = create_concat_datasets(num_datasets=2, seed=42)\n    ds1 = datasets[0]\n    ds2 = datasets[1]\n\n    # create new dataset with a variable with the same name but different values\n    ds2[\"temperature\"] = ds2[\"temperature\"].assign_attrs({\"units\": \"K\"})\n\n    # ensure that variables are not equal\n    assert ds1[\"temperature\"].equals(ds2[\"temperature\"]) == False\n\n    # ensure that all variables are concatenated except for \"temperature\"\n    result = concat([ds1, ds2], \"day\", data_vars=\"different\", coords=\"different\", compat=\"equals\")\n    assert \"temperature\" not in result.data_vars\n    assert len(result.coords) == 4  # x, y, day, lat, lon\n\n    # ensure that variable is still a scalar coordinate\n    result[\"day\"].dims\n    result[\"day\"].shape\n"], "sample_167": ["    def test_intword_value_less_than_1000000(self):\n        value = 12345\n        self.assertEqual(humanize.intword(value), value)\n", "    def setUp(self):\n        translation.activate('en')\n", "    def test_intcomma_with_l10n_enabled(self):\n        with override_settings(USE_L10N=True):\n            self.assertEqual(humanize.intcomma(12345), '12,345')\n            self.assertEqual(humanize.intcomma(123456789), '123,456,789')\n", "def test_naturaltime_with_timezone(self):\n    \"\"\"\n    Test naturaltime filter with different timezones.\n    \"\"\"\n    # Test with aware datetime\n    tz = get_fixed_timezone(0)\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz)\n    result = naturaltime(value)\n    self.assertEqual(result, \"12:00\")\n\n    # Test with naive datetime\n    value = datetime.datetime(2022, 1, 1, 12, 0)\n    result = naturaltime(value)\n    self.assertEqual(result, \"12:00\")\n\n    # Test with datetime in future\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) + datetime.timedelta(days=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"tomorrow\")\n\n    # Test with datetime in past\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) - datetime.timedelta(days=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"yesterday\")\n\n    # Test with datetime in the same day\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) - datetime.timedelta(hours=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"an hour ago\")\n\n    # Test with datetime in the same minute\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) - datetime.timedelta(minutes=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"a minute ago\")\n\n    # Test with datetime in the same second\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) - datetime.timedelta(seconds=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"a second ago\")\n\n    # Test with datetime in the same second, but in the future\n    value = datetime.datetime(2022, 1, 1, 12, 0, tzinfo=tz) + datetime.timedelta(seconds=1)\n    result = naturaltime(value)\n    self.assertEqual(result, \"a second from now\")\n\n    # Test with datetime in the same minute, but in the future\n    value = datetime.datetime(202", "    def test_naturalday_with_day_of_week(self):\n        # Test naturalday with a date value that includes a day of the week\n        context = Context({'value': datetime.date(2022, 7, 25)})\n        template = Template('{% load humanize %}{{ value|naturalday }}')\n        self.assertEqual(template.render(context).strip(), _('%a %b %d'))\n"], "sample_76": ["    def test_check_setting_language_code_empty_string(self):\n        # Test LANGUAGE_CODE setting is empty string\n        with self.settings(LANGUAGE_CODE=''):\n            result = check_setting_language_code([], **{})\n            self.assertEqual(result, [Error(E001.msg.format(''), id=E001.id)])\n", "def test_check_setting_languages_empty(self):\n    \"\"\"Test LANGUAGES setting is empty.\"\"\"\n    # Set settings.LANGUAGES to an empty list\n    from django.conf import settings\n    settings.LANGUAGES = []\n    \n    # Run the checks\n    errors = check_setting_languages(None)\n    \n    # Check that the correct error was raised\n    self.assertEqual(len(errors), 1)\n    self.assertEqual(errors[0].id, E002.id)\n    self.assertEqual(errors[0].msg, E002.msg.format(''))\n", "def test_check_setting_language_code_valid(self):\n    # Test LANGUAGE_CODE setting with a valid value.\n    with self.settings(LANGUAGE_CODE='en'):\n        result = check_setting_language_code([], LanguageConfig())\n        self.assertEqual(result, [])\n", "    def test_check_setting_language_code_with_empty_string(self):\n        \"\"\"Check that an empty string LANGUAGE_CODE setting is invalid.\"\"\"\n        with self.settings(LANGUAGE_CODE=''):\n            result = check_setting_language_code([])\n            self.assertEqual(result, [Error(E001.msg.format(''), id=E001.id)])\n", "    def test_language_code_re_matches_language_tag(self):\n        \"\"\"Test if language_code_re matches a valid language tag.\"\"\"\n        valid_tags = self.valid_tags\n        for tag in valid_tags:\n            self.assertTrue(language_code_re.match(tag), msg=f\"{tag} is a valid language code\")\n"], "sample_573": ["    def test_fitting_curve(self, df):\n        df_grouped = df.groupby(\"group\")\n        result = PolyFit(order=1)(df, df_grouped, 'columns', 'auto')\n        assert len(result) == 2\n        assert result[0].shape == (101, 2)\n        assert result[1].shape == (101, 2)\n", "    def test_order_parameter(self, df):\n\n        # Arrange\n        poly_fit = PolyFit(order=5)\n        grouped = df.groupby(\"group\")\n\n        # Act\n        result = poly_fit(grouped, df, \"values\", None)\n\n        # Assert\n        assert result.shape[0] == 2\n        assert result.shape[1] == poly_fit.gridsize\n        assert result.loc[0, \"x\"].nunique() == poly_fit.gridsize\n        assert result.loc[1, \"x\"].nunique() == poly_fit.gridsize\n", "    def test_poly_fit_data_with_fewer_than_order_unique_points(self, df):\n        # This test should cover the TODO comment in the PolyFit class\n        poly_fit = PolyFit(order=3)\n        result = poly_fit(df, df.groupby(\"group\"), \"x\", \"y\")\n        assert result.shape[0] == 2  # Two groups\n\n        # Check that the result is empty when x values are not unique\n        df_copy = df.copy()\n        df_copy[\"x\"] = np.repeat(df_copy[\"x\"].unique()[0], len(df_copy))\n        poly_fit = PolyFit(order=3)\n        result = poly_fit(df_copy, df_copy.groupby(\"group\"), \"x\", \"y\")\n        assert result.empty\n", "    def test_fit_predict_with_insufficient_data_points(self, df):\n        # Test that when there are not enough unique data points, an empty DataFrame is returned\n        poly_fit = PolyFit()\n        df_subset = df[df[\"group\"] == \"x\"]\n        result = poly_fit._fit_predict(df_subset)\n        assert result.empty\n", "    def test_order_specified(self, df):\n\n        df[\"y\"] = df[\"y\"] + df[\"x\"]**2 + df[\"x\"]**3\n\n        poly_fit = PolyFit(order=3)\n        result = poly_fit(df, df.groupby(\"color\"), \"index\", \"squarify\")\n\n        # We should get a curve with 100 points\n        assert result.shape[0] == poly_fit.gridsize\n\n        # We should be able to see the curvature of the fitted curve\n        assert np.any(np.diff(result[\"y\"].values) < 0)\n        assert np.any(np.diff(result[\"y\"].values) > 0)\n"], "sample_595": ["def test_translate():\n    # Arrange\n    arr = xr.DataArray([\"hello\", \"world\", \"python\"])\n\n    # Act and Assert\n    result = arr.str.translate({ord('o'): '0', ord('P'): 'p'})\n    expected_result = xr.DataArray(['hell0', 'w0rld', 'pyth0n'])\n    assert_equal(result, expected_result)\n\n    # Test edge case\n    result = arr.str.translate({ord('o'): ''})\n    expected_result = xr.DataArray(['hell', 'wrd', 'pythn'])\n    assert_equal(result, expected_result)\n", "def test_upper():\n    \"\"\"Test that upper() returns uppercase strings.\"\"\"\n    da = xr.DataArray([\"hello\", \"world\", \"foo\", \"bar\"])\n    result = da.str.upper()\n    expected = xr.DataArray([\"HELLO\", \"WORLD\", \"FOO\", \"BAR\"])\n    assert_equal(result, expected)\n", "def test_str_find():\n    da = xr.DataArray([\"hello world\", \"hello python\", \"hello earth\"])\n    expected = np.array([6, 7, 5])\n    result = da.str.find(\"hello\")\n    assert_equal(result, expected)\n", "def test_encode_decode(dtype):\n    da = xr.DataArray([\"\\U0001F600\", \"\\U0001F601\", \"\\U0001F600\"])\n    da_str = da.astype(dtype)\n    da_bytes = da_str.astype(np.bytes_)\n\n    assert_equal(da_bytes.encode(\"utf-8\").decode(\"utf-8\"), da_str.values)\n    assert_equal(da_bytes.decode(\"utf-8\").encode(\"utf-8\"), da_bytes.values)\n\n    # Test with encoding/decoding errors\n    da_bytes = da_str.astype(np.bytes_)\n    assert_equal(da_bytes.encode(\"ascii\", errors=\"replace\").decode(\"ascii\"), da_str.values)\n    assert_equal(da_bytes.decode(\"ascii\", errors=\"replace\").encode(\"ascii\"), da_bytes.values)\n\n    # Test with non-optimized encoding/decoding\n    da_bytes = da_str.astype(np.bytes_)\n    assert_equal(da_bytes.encode(\"utf-32\", errors=\"strict\").decode(\"utf-32\"), da_str.values)\n    assert_equal(da_bytes.decode(\"utf-32\", errors=\"strict\").encode(\"utf-32\"), da_bytes.values)\n\n    # Test with non-string-like input\n    with pytest.raises(TypeError):\n        da.encode(\"utf-8\")\n    with pytest.raises(TypeError):\n        da_str.encode(\"utf-8\", dtype)\n", "def encoding(request):\n    return request.param\n\n"], "sample_342": ["    def test_process_request_invalid_term(self):\n        \"\"\"Test that process_request raises PermissionDenied for invalid term.\"\"\"\n        request = self.factory.get(self.url, {'term': ''})\n        request.user = self.user\n        view = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        with self.assertRaises(PermissionDenied):\n            view.process_request(request)\n", "    def test_process_request_permission_denied_field_name_missing(self):\n        \"\"\"Test process_request() raises PermissionDenied when field_name is missing.\"\"\"\n        opts = {\n            'app_label': Answer._meta.app_label,\n            'model_name': Answer._meta.model_name,\n        }\n        request = self.factory.get(self.url, {'term': '', 'app_label': opts['app_label'], 'model_name': opts['model_name']})\n        request.user = self.user\n        view = AutocompleteJsonView()\n        with self.assertRaises(PermissionDenied):\n            view.process_request(request)\n", "    def setUpTestData(self):\n        super().setUpTestData()\n        self.user = User.objects.create_user(\n            username='user', password='secret',\n            email='user@example.com', is_staff=False,\n        )\n        self.model_admin = AuthorAdmin()\n", "    def test_permission_denied_app_label(self):\n        \"\"\"Test permission denied when app_label is invalid.\"\"\"\n        opts = self.opts.copy()\n        opts['app_label'] = 'non_existent_app'\n        request = self.factory.get(self.url, **opts)\n        view = AutocompleteJsonView.as_view(**self.as_view_args)\n        with self.assertRaises(PermissionDenied):\n            view(request).render()\n", "    def test_has_perm_denied_on_missing_perm(self):\n        self.user.is_staff = False\n        self.user.save()\n        request = self.factory.get(self.url, **self.opts)\n        request.user = self.user\n        view = AutocompleteJsonView()\n        view.model_admin = self.opts['model_admin']\n        with self.assertRaises(PermissionDenied):\n            view.has_perm(request)\n"], "sample_281": ["    def test_process_request_invalid_app_label(self):\n        \"\"\"Test process_request raises PermissionDenied with an invalid app_label.\"\"\"\n        request = self.factory.get(self.url, {'term': 'test', 'app_label': 'invalid_app'})\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView().process_request(request)\n", "    def test_user_without_perm(self):\n        # Create a user without staff status\n        user = User.objects.create_user(\n            username='user', password='secret',\n            email='user@example.com',\n        )\n        self.client.force_login(user)\n        opts = self.opts\n        request = self.factory.get(self.url, **opts)\n        view = AutocompleteJsonView()\n        view.request = request\n        view.model_admin = self.model_admin\n        with self.assertRaises(PermissionDenied):\n            view.has_perm(request)\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='user', password='secret',\n            email='user@example.com', is_staff=False,\n        )\n        cls.admin_user = User.objects.create_user(\n            username='admin', password='secret',\n            email='admin@example.com', is_staff=True,\n        )\n        super().setUpTestData()\n", "    def test_permission_denied_unauthenticated(self):\n        # User is not logged in\n        request = self.factory.get(self.url, **self.opts)\n        view = AutocompleteJsonView.as_view(**self.as_view_args)\n        with self.assertRaises(PermissionDenied):\n            view(request).render()\n", "    def test_remote_field_permission_denied(self):\n        with model_admin(Author, AuthorAdmin, site):\n            opts = {\n                'app_label': Author._meta.app_label,\n                'model_name': Author._meta.model_name,\n                'field_name': 'non_existent_field',\n            }\n            response = self.client.get(self.url, self.get_request_data(**opts))\n            self.assertIsInstance(response, PermissionDenied)\n"], "sample_442": ["    def test_signer_default_key_used(self):\n        signer = signing.Signer()\n        self.assertEqual(signer.key, \"my_secret_key\")\n", "    def test_unsign_object_compression(self):\n        signer = signing.TimestampSigner(key=\"my_secret_key\")\n        obj = {\"key\": \"value\"}\n        compressed_data = zlib.compress(json.dumps(obj, separators=(\",\", \":\")).encode(\"latin-1\"))\n        compressed_base64 = b64_encode(compressed_data).decode()\n        # The compressor should prepend a '.' to signify compression\n        compressed_base64 = \".\" + compressed_base64\n        signed_obj = signer.sign_object(obj, compress=True)\n        self.assertEqual(signed_obj, f\"{compressed_base64}{signer.sep}{signer.signature(compressed_base64)}\")\n        unsigned_obj = signer.unsign_object(signed_obj)\n        self.assertEqual(unsigned_obj, obj)\n", "    def test_signer_deprecated_arguments(self):\n        with freeze_time():\n            signer = Signer(\"key\", sep=\".\", salt=\"salt\", algorithm=\"sha256\")\n            self.assertEqual(signer.key, \"key\")\n            self.assertEqual(signer.sep, \".\")\n            self.assertEqual(signer.salt, \"salt\")\n            self.assertEqual(signer.algorithm, \"sha256\")\n            self.assertEqual(signer.fallback_keys, [])\n", "    def test_signer_with_fallback_keys(self):\n        fallback_key = \"fallback_key\"\n        self.settings.SECRET_KEY_FALLBACKS = [\"fallback_key1\", fallback_key]\n        signer = signing.Signer(key=None, fallback_keys=None)\n        with self.assertRaises(BadSignature):\n            signer.unsign(\"value:fallback_key_signature\")\n        self.assertEqual(signer.unsign(\"value:fallback_key_signature\", key=fallback_key), \"value\")\n        self.assertEqual(signer.unsign(\"value:fallback_key_signature\", key=\"fallback_key1\"), \"value\")\n", "    def test_timestamp_signer_unsign_object_compression(self):\n        # The SIGNER backend instance is created with the secret key from the settings\n        # module.\n        signer = signing.get_cookie_signer()\n        obj = \"test_object\"\n        signed_obj = signer.sign_object(obj, compress=True)\n        decompressed_signed_obj = signer.unsign_object(signed_obj, compress=True)\n        self.assertEqual(obj, decompressed_signed_obj)\n"], "sample_930": ["def test_create_index_empty_env():\n    env = BuildEnvironment()\n    index_entries = IndexEntries(env)\n    builder = object()  # dummy builder object\n    result = index_entries.create_index(builder)\n    assert result == []\n", "def test_create_index_empty_domain():\n    env = BuildEnvironment()\n    env.add_domain(IndexDomain())\n    index_entries = IndexEntries(env)\n    builder = DummyBuilder()\n    result = index_entries.create_index(builder, group_entries=True)\n    assert result == []\n", "def test_create_index_group_entries_false():\n    class MockBuilder:\n            return 'base'\n\n    env = BuildEnvironment()\n    env.get_domain('index').entries = {\n        'index.rst': [\n            ('single', 'single', 'single', 'single', 'single'),\n            ('pair', 'pair', 'pair', 'pair', 'pair'),\n            ('triple', 'triple', 'triple', 'triple', 'triple'),\n            ('see', 'see', 'see', 'see', 'see'),\n            ('seealso', 'seealso', 'seealso', 'seealso', 'seealso'),\n        ]\n    }\n    env.get_domain('index').entries['index.rst'][0] = ('triple', 'triple', 'triple', 'triple', 'triple')\n    env.get_domain('index').entries['index.rst'][1] = ('single', 'single', 'single', 'single', 'single')\n    env.get_domain('index').entries['index.rst'][2] = ('pair', 'pair', 'pair', 'pair', 'pair')\n    env.get_domain('index').entries['index.rst'][3] = ('seealso', 'seealso', 'seealso', 'seealso', 'seealso')\n    env.get_domain('index').entries['index.rst'][4] = ('see', 'see', 'see', 'see', 'see')\n    index = IndexEntries(env)\n    result = index.create_index(MockBuilder(), group_entries=False)\n    assert result == [\n        [('single', [('single', 'index.rst#single')])],\n        [('see', [('see', None)]),\n         ('see also see', [('see', None)])],\n        [('pair', [('pair', 'index.rst#pair')]),\n         ('pair', [('pair', 'index.rst#pair')])],\n        [('seealso', [('seealso', 'index.rst#seealso')])],\n        [('triple', [('triple', 'index.rst#triple')]),\n         ('triple in triple', [('triple', 'index.rst#triple')]),\n         ('triple in triple', [('triple', 'index.rst#triple')])]]\n    ]\n", "def test_create_index_group_entries_false():\n    \"\"\"Test the create_index method when group_entries is False.\"\"\"\n    env = BuildEnvironment()\n    index_entries = IndexEntries(env)\n    builder = Builder()\n    result = index_entries.create_index(builder, group_entries=False)\n    assert isinstance(result, list)\n    assert all(isinstance(item, tuple) and len(item) == 2 for item in result)\n    assert all(isinstance(key, str) for key, _ in result)\n    assert all(isinstance(value, list) for _, value in result)\n    assert all(isinstance(entry, tuple) and len(entry) == 2 for item in result for entry in item[1])\n    assert all(isinstance(key, str) for key, _ in item[1] for entry in item[1] for key, _ in entry)\n", "def test_create_index():\n    env = BuildEnvironment()\n    builder = object()  # mock builder\n    env.get_domain('index').entries = {'test.rst': [('single', 'entry', 'tid', 'main', 'key')]}\n    index_entries = IndexEntries(env)\n    result = index_entries.create_index(builder)\n    assert isinstance(result, list)\n    assert isinstance(result[0], tuple)\n"], "sample_110": ["    def test_expression_deconstruction(self):\n        class MyExpression(BaseExpression):\n                super().__init__(output_field=output_field)\n                self.a = a\n                self.b = b\n\n                return 'my_expression({},{})'.format(self.a, self.b)\n\n        expression = MyExpression(1, 2)\n        deconstructed = pickle.loads(pickle.dumps(expression))\n        self.assertIsInstance(deconstructed, MyExpression)\n        self.assertEqual(deconstructed.a, 1)\n        self.assertEqual(deconstructed.b, 2)\n", "    def test_combined_expression_sqlite(self):\n        from .models import Container\n        obj = Container.objects.create()\n        qs = Container.objects.filter(id=obj.id).annotate(id=Func('id'))\n        self.assert_pickles(qs)\n", "    def test_expression_equality(self):\n        # Test that expressions are equal when their identities are equal\n        e1 = F('foo')\n        e2 = F('foo')\n        self.assertEqual(e1, e2)\n\n        # Test that expressions are not equal when their identities are different\n        e3 = F('bar')\n        self.assertNotEqual(e1, e3)\n\n        # Test that expressions are equal when their identities are equal, but\n        # their output fields are different\n        e4 = F('foo', output_field=models.IntegerField())\n        e5 = F('foo', output_field=models.CharField(max_length=10))\n        self.assertEqual(e4, e5)\n\n        # Test that expressions are not equal when their identities are not equal,\n        # even if their output fields are the same\n        e6 = F('foo')\n        e7 = F('bar')\n        self.assertNotEqual(e6, e7)\n", "    def test_CombinedExpression_resolve_expression(self):\n        lhs = F('foo')\n        rhs = F('bar')\n        combined = CombinedExpression(lhs, '+', rhs)\n        self.assertEqual(combined.resolve_expression().lhs.name, 'foo')\n        self.assertEqual(combined.resolve_expression().rhs.name, 'bar')\n", "    def test_F_pickleability(self):\n        F('name').value  # make sure F doesn't error when accessing value\n        self.assert_pickles(F('name'))\n"], "sample_840": ["def test_PLSSVD_fit_transform():\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls_svd = PLSSVD(n_components=1, scale=True)\n    pls_svd.fit(X, Y)\n    X_c, Y_c = pls_svd.fit_transform(X, Y)\n    assert_array_almost_equal(X_c, np.dot(X, pls_svd.x_weights_))\n    assert_array_almost_equal(Y_c, np.dot(Y, pls_svd.y_weights_))\n", "def test_PLSSVD_transform_n_components_zero():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    pls = PLSSVD(n_components=0)\n    with pytest.raises(ValueError):\n        pls.fit_transform(X, Y)\n", "def test_PLSRegression_prediction():\n    \"\"\"Test PLSRegression prediction method.\"\"\"\n    # Test 1: Simple case\n    X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\n    Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\n    pls2 = PLSRegression(n_components=2)\n    pls2.fit(X, Y)\n    Y_pred = pls2.predict(X)\n    assert_array_almost_equal(Y_pred, Y, decimal=5)\n\n    # Test 2: Check that predict method raises an error when fit method is not called\n    pls2 = PLSRegression(n_components=2)\n    with assert_raise_message(ValueError, \"not fitted\"):\n        pls2.predict([[1., 2., 3.]])\n\n    # Test 3: Check that predict method raises an error when X has a different number of features\n    pls2 = PLSRegression(n_components=2)\n    pls2.fit(X, Y)\n    X_wrong = [[1., 2., 3., 4.]]\n    with assert_raise_message(ValueError, \"number of features\"):\n        pls2.predict(X_wrong)\n\n    # Test 4: Check that predict method raises an error when X is not an array-like\n    pls2 = PLSRegression(n_components=2)\n    pls2.fit(X, Y)\n    X_not_array_like = \"not an array like\"\n    with assert_raise_message(TypeError, \"'X' must be an array-like\"):\n        pls2.predict(X_not_array_like)\n\n    # Test 5: Check that predict method returns an array with the same shape as Y\n    pls2 = PLSRegression(n_components=2)\n    pls2.fit(X, Y)\n    Y_pred = pls2.predict(X)\n    assert_array_equal(Y_pred.shape, Y.shape)\n\n    # Test 6: Check that predict method returns an array with the correct dtype\n    pls2 = PLSRegression(n_components=2)\n    pls2.fit(X, Y)\n    Y_pred = pls2.predict(X)\n    assert Y_pred.dtype == Y.dtype\n", "def test_PLSSVD_n_components_too_large():\n    \"\"\"Test for PLSSVD with too large number of components\"\"\"\n    rng = check_random_state(42)\n    X = rng.rand(10, 5)\n    Y = rng.rand(10, 3)\n    with pytest.raises(ValueError):\n        PLSSVD(n_components=15).fit(X, Y)\n", "def test_PLSSVD_svd_vs_nipals():\n    # Test that PLSSVD using svd algorithm matches the result of PLSRegression\n    X, y = load_linnerud()\n    Y = y[['Glucose', 'Insulin']]\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)\n    Y = scaler.fit_transform(Y)\n\n    # Compare results for PLSRegression and PLSSVD\n    pls = PLSRegression(n_components=1, max_iter=100)\n    pls.fit(X, Y)\n    X_pls, Y_pls = pls.transform(X, Y)\n\n    pls_svd = PLSSVD(n_components=1)\n    pls_svd.fit(X, Y)\n    X_svd, Y_svd = pls_svd.transform(X, Y)\n\n    assert_array_almost_equal(X_pls, X_svd)\n    assert_array_almost_equal(Y_pls, Y_svd)\n\n    # Test that PLSSVD with n_components > rank(X')Y' gives an error\n    with assert_raise_message(ValueError, \"Invalid number of components\"):\n        pls_svd = PLSSVD(n_components=X.shape[1] + 1)\n        pls_svd.fit(X, Y)\n"], "sample_189": ["    def test_make_key(self):\n        cache = cache\n        self.assertEqual(cache.make_key('test_key', 1), 'default:1:test_key')\n        self.assertEqual(cache.make_key('test_key', 2), 'default:2:test_key')\n        self.assertEqual(cache.make_key('test_key', 2), 'default:2:test_key')\n", "    def test_init(self):\n        cache = BaseCache({'timeout': 60, 'max_entries': 10, 'OPTIONS': {}, 'KEY_PREFIX': 'test', 'VERSION': 1})\n        self.assertEqual(cache.default_timeout, 60)\n        self.assertEqual(cache._max_entries, 10)\n        self.assertEqual(cache._cull_frequency, 3)\n        self.assertEqual(cache.key_prefix, 'test')\n        self.assertEqual(cache.version, 1)\n        self.assertEqual(cache.key_func, default_key_func)\n", "    def test_default_key_func(self):\n        cache = BaseCache({\n            'KEY_PREFIX': 'test_key_prefix',\n            'VERSION': 2,\n        })\n        key = cache.make_key('test_key')\n        self.assertEqual(key, 'test_key_prefix:2:test_key')\n\n        # Test with default key prefix\n        cache2 = BaseCache({\n            'VERSION': 2,\n        })\n        key2 = cache2.make_key('test_key')\n        self.assertEqual(key2, '2:test_key')\n", "    def test_set_many(self):\n        cache = cache\n        cache.set_many({'key1': 'value1', 'key2': 'value2'}, timeout=60)\n        self.assertEqual(cache.get_many(['key1', 'key2']), {'key1': 'value1', 'key2': 'value2'})\n        self.assertEqual(cache.get_many(['key1', 'key3']), {'key1': 'value1'})\n", "    def test_get_or_set_with_callable_default(self):\n        cache = caches['default']\n        cache.add('test_key', 'old_value')\n        with self.assertRaises(InvalidCacheKey):\n            cache.get_or_set('test_key', 42)  # Non-callable default\n        self.assertEqual(cache.get_or_set('test_key', lambda: 42), 42)\n        self.assertEqual(cache.get_or_set('test_key', lambda: 42, timeout=10), 42)\n"], "sample_757": ["def test_onehotencoder_get_feature_names():\n    \"\"\"Test that OneHotEncoder returns the correct feature names\"\"\"\n    encoder = OneHotEncoder(categories=[['a', 'b'], [1, 2, 3]])\n    X = [['a', 1], ['b', 3]]\n    encoder.fit(X)\n    feature_names = encoder.get_feature_names()\n    expected_feature_names = np.array(['x0_a', 'x0_b', 'x1_1', 'x1_2', 'x1_3'],\n                                     dtype=object)\n    assert_array_equal(feature_names, expected_feature_names)\n", "def test_ordinal_encoder_inverse_transform_with_unknowns():\n    # test that inverse_transform correctly handles unknown categories when the\n    # OrdinalEncoder was trained on categorical data\n    encoder = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder.fit(X)\n    X_unknown = [['Male', 4], ['Female', 3], ['Female', 2]]\n    expected_result = np.array([['Male', 1], ['Female', 2], ['Female', 2]])\n    assert_array_equal(encoder.inverse_transform(encoder.transform(X_unknown)), expected_result)\n", "def test_one_hot_encoder_inverse_transform_unknown_category():\n    \"\"\"Test inverse_transform with unknown categories.\"\"\"\n    encoder = OneHotEncoder(handle_unknown='ignore')\n    X = np.array([['Male', 1], ['Female', 3], ['Female', 2]])\n    encoder.fit(X)\n    X_transformed = encoder.transform(X)\n    X_reconstructed, _ = encoder.inverse_transform(X_transformed)\n    assert_equal(X_reconstructed, np.array([['Male', 1], ['Female', 3], ['Female', 2]]))\n    assert_equal(X_reconstructed.dtype, object)\n    X = np.array([['Male', 1], ['Female', 4], ['Female', 2]])\n    X_transformed = encoder.transform(X)\n    X_reconstructed, _ = encoder.inverse_transform(X_transformed)\n    assert_equal(X_reconstructed, np.array([['Male', 1], [None, 2], [None, 2]]))\n", "def test_onehot_encoder_handle_unknown_error():\n    encoder = OneHotEncoder(handle_unknown='error')\n    X = np.array([[1, 2], [1, 3]])\n    with pytest.raises(ValueError):\n        encoder.fit_transform(X)\n", "def test_ordinal_encoder_single_feature():\n    # Test that we can fit and transform a single feature\n    enc = OrdinalEncoder()\n    X = np.array(['Male', 'Female', 'Male', 'Female'])\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    assert_array_equal(X_transformed, np.array([0., 1., 0., 1.]))\n\n    # Test that inverse_transform works correctly\n    X_inv = enc.inverse_transform(X_transformed)\n    assert_array_equal(X_inv, np.array(['Male', 'Female', 'Male', 'Female']))\n\n    # Test that we can fit and transform a single feature with custom categories\n    enc = OrdinalEncoder(categories=[['Male', 'Female']])\n    X = np.array(['Male', 'Female', 'Male', 'Female'])\n    enc.fit(X)\n    X_transformed = enc.transform(X)\n    assert_array_equal(X_transformed, np.array([0., 1., 0., 1.]))\n\n    # Test that inverse_transform works correctly with custom categories\n    X_inv = enc.inverse_transform(X_transformed)\n    assert_array_equal(X_inv, np.array(['Male', 'Female', 'Male', 'Female']))\n"], "sample_763": ["def test_check_X_y_with_sparse_X_and_sparse_y():\n    X = sparse_random_matrix(100, density=0.5)\n    y = sparse_random_matrix(100, density=0.5)\n    with pytest.raises(ValueError):\n        check_X_y(X, y, accept_sparse=False)\n\n", "def test_check_non_negative():\n    # Test check_non_negative on a normal array\n    X = np.array([1, 2, 3, 4, 5])\n    check_non_negative(X, \"check_non_negative\")\n    check_non_negative(X, \"check_non_negative\")\n\n    # Test check_non_negative on a sparse matrix\n    X = sp.csr_matrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n    check_non_negative(X, \"check_non_negative\")\n    check_non_negative(X, \"check_non_negative\")\n\n    # Test check_non_negative on a matrix with negative values\n    with pytest.raises(ValueError):\n        X = np.array([1, 2, -3, 4, 5])\n        check_non_negative(X, \"check_non_negative\")\n\n    # Test check_non_negative on a sparse matrix with negative values\n    X = sp.csr_matrix([[1, 0, 0], [0, -2, 0], [0, 0, 3]])\n    with pytest.raises(ValueError):\n        check_non_negative(X, \"check_non_negative\")\n\n    # Test check_non_negative on a sparse matrix with negative values and\n    # different formats\n    X = sp.lil_matrix([[1, 0, 0], [0, -2, 0], [0, 0, 3]])\n    with pytest.raises(ValueError):\n        check_non_negative(X, \"check_non_negative\")\n\n    X = sp.coo_matrix([[1, 0, 0], [0, -2, 0], [0, 0, 3]])\n    with pytest.raises(ValueError):\n        check_non_negative(X, \"check_non_negative\")\n\n    # Test check_non_negative with a warning when the input is None\n    with pytest.warns(UserWarning):\n        check_non_negative(None, \"check_non_negative\")\n", "def test_check_array_dtype_numeric_conversion():\n    # Test that check_array can convert object to float64\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_float = check_array(X)\n    assert isinstance(X_float, np.ndarray)\n    assert X_float.dtype == np.float64\n\n    # Test that check_array can convert object to float32\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_float32 = check_array(X, dtype=np.float32)\n    assert isinstance(X_float32, np.ndarray)\n    assert X_float32.dtype == np.float32\n\n    # Test that check_array raises an error if object to float conversion fails\n    X = np.array([[1, 2], [3, 'string']], dtype=np.object)\n    with assert_raises(ValueError):\n        check_array(X)\n\n    # Test that check_array does not convert object to float when dtype is None\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_no_conversion = check_array(X, dtype=None)\n    assert isinstance(X_no_conversion, np.ndarray)\n    assert X_no_conversion.dtype == np.object\n\n    # Test that check_array does not convert object to float when dtype is \"numeric\"\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_no_conversion = check_array(X, dtype=\"numeric\")\n    assert isinstance(X_no_conversion, np.ndarray)\n    assert X_no_conversion.dtype == np.object\n\n    # Test that check_array does not convert object to float when dtype is object\n    X = np.array([[1, 2], [3, 4]], dtype=np.object)\n    X_no_conversion = check_array(X, dtype=np.object)\n    assert isinstance(X_no_conversion, np.ndarray)\n    assert X_no_conversion.dtype == np.object\n\n    # Test that check_array can handle float arrays with object dtype\n    X = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.object)\n    X_float = check_array(X)\n    assert isinstance(X_float, np.ndarray)\n    assert X_float.dtype == np.float64\n\n    # Test that check_array can handle float32 arrays with object dtype\n    X = np.array([[1.0, 2.0], [3.0, 4.0]],", "def test_check_array_sparse_numeric_dtype():\n    # Test that a sparse matrix with a numeric dtype is not converted to\n    # float64 when the input dtype is set to 'numeric'\n    X = sp.csr_matrix(np.ones((10, 10)))\n    with warnings.catch_warnings():\n        warnings.simplefilter('error', UserWarning)\n        check_array(X, dtype='numeric')\n", "def test_check_consistent_length_list_of_none():\n    \"\"\"Test that check_consistent_length raises a ValueError if one of the\n    input is None and the others are not None.\n\n    Parameters\n    ----------\n    X : list of array-like objects\n        List of arrays with consistent length.\n    \"\"\"\n    with assert_raises(ValueError):\n        check_consistent_length([None, [1, 2], [3, 4]])\n"], "sample_38": ["def test_sip_pix2foc_with_invalid_input(self):\n    # Create a WCS object with a SIP distortion\n    wcs = WCS('sip.hdr', key='A')\n\n    # Create some invalid pixel coordinates\n    x = np.array([1, 2, 3])\n    y = np.array([4, 5, 6])\n\n    # Try to convert the invalid pixel coordinates to focal plane coordinates\n    with catch_warnings(record=True) as w:\n        result = wcs.sip_pix2foc(x, y)\n\n    # Check that a warning is raised\n    assert len(w) == 1\n    assert issubclass(w[-1].category, UserWarning)\n\n    # Check that the result is correct\n    assert_array_almost_equal(result, [1.0, 2.0, 3.0])\n\n    # Try to convert the invalid pixel coordinates to focal plane coordinates\n    # with a negative y value\n    x = np.array([1, 2, 3])\n    y = np.array([-4, -5, -6])\n\n    # Check that a ValueError is raised\n    with raises(ValueError):\n        wcs.sip_pix2foc(x, y)\n\n    # Try to convert the invalid pixel coordinates to focal plane coordinates\n    # with an array of invalid values\n    x = np.array([1, 2, 'a'])\n    y = np.array([4, 5, 6])\n\n    # Check that a TypeError is raised\n    with raises(TypeError):\n        wcs.sip_pix2foc(x, y)\n", "    def test_all_world2pix_naxis1(self):\n        # Test that all_world2pix works with 1D WCS\n        # create a 1D WCS\n        naxis = [1]\n        wcsprm = _wcs.Wcsprm(naxis=naxis, keysel=_wcs.WCSHDR_IMGHEAD)\n        wcs = WCS(None, wcsprm=wcsprm)\n        # test that WCS has 1 axis\n        assert wcs.naxis == 1\n        # test that all_world2pix works with 1D WCS\n        x = np.array([1, 2, 3])\n        world = np.array([1, 2, 3])\n        result = wcs.all_world2pix(world, 0)\n        assert_array_almost_equal(result, x)\n        # test that WCS has 1 axis\n        assert wcs.naxis == 1\n", "    def test_all_world2pix_deterministic_behavior(self):\n        # Given\n        w = wcs.WCS(get_pkg_data_contents(\"fitsutil_test.fits\"))\n\n        # When\n        pix1 = w.all_world2pix([[0.0, 0.0], [1.0, 1.0]], 1)\n        pix2 = w.all_world2pix([[0.0, 0.0], [1.0, 1.0]], 1)\n\n        # Then\n        assert_array_equal(pix1, pix2)\n", "def test_read_sip_kw_header_with_no_sip(self):\n    # Test that WCS object correctly identifies when SIP coefficients are\n    # present in the header, but SIP distortion parameters are not.\n    # This was a problem that was fixed in astropy/astropy#299.\n    header = fits.Header()\n    header['CTYPE1'] = 'RA---TAN-SIP'\n    header['CTYPE2'] = 'DEC--TAN'\n    header['A_ORDER'] = 1\n    with warnings.catch_warnings(record=True) as warning_lines:\n        wcs = WCS(header)\n    assert len(warning_lines) == 0\n    assert wcs.sip is not None\n    assert wcs.wcs.ctype == ['RA---TAN-SIP', 'DEC--TAN']\n", "    def test_all_world2pix_tangent_point(self):\n        filename = get_pkg_data_filename(\"maps\", \"tangent-arc-fits/fits/000000.00.fits\")\n        hdulist = fits.open(filename)\n        w = wcs.WCS(hdulist[0].header, hdulist)\n        hdulist.close()\n\n        ra = 23.000\n        dec = -30.0\n\n        world = np.array([[ra, dec]])\n\n        pix = w.all_world2pix(world, origin=1)\n\n        # RA should be ~ 23.000, and Dec should be ~ -30.000\n        assert_allclose(pix[0, 0], 53.562, atol=0.001)\n        assert_allclose(pix[0, 1], 141.502, atol=0.001)\n"], "sample_1191": ["def test_smith_normal_form_domain_not_PID():\n    from sympy import Integer\n    m = DomainMatrix([[Integer(12), Integer(6), Integer(4)],\n                      [Integer(3), Integer(9), Integer(6)],\n                      [Integer(2), Integer(16), Integer(14)]], (3, 3), ZZ)\n    with raises(DMDomainError):\n        smith_normal_form(m)\n", "def test_hermite_normal_form_nonzero_entries():\n    x = Symbol('x')\n    m = DomainMatrix([[x, 12, 4, 4], [x, 9, 3, 9], [x, 2, 16, 14]], (3, 4), ZZ)\n    expected = DomainMatrix([[x, 0, 0, 4], [0, 0, 3, 3], [0, 0, 0, 14]], (3, 4), ZZ)\n    result = hermite_normal_form(m)\n    assert result.to_Matrix().equals(expected.to_Matrix())\n", "def test_hermite_normal_form_D_invalid():\n    # Test that an error is raised if D is not a positive integer in ZZ\n    matrix = DomainMatrix([[2, 1], [1, 2]], (2, 2), ZZ)\n    with raises(DMDomainError):\n        hermite_normal_form(matrix, D=1.5)\n    with raises(DMDomainError):\n        hermite_normal_form(matrix, D=ZZ(-1))\n    with raises(DMDomainError):\n        hermite_normal_form(matrix, D=ZZ.of_type('1/2'))\n", "def test_hermite_normal_form_modulo_D_check_rank():\n    A = DomainMatrix([[2, 3], [4, 6]], (2, 2), ZZ)\n    with raises(DMShapeError):\n        hermite_normal_form(A, D=10, check_rank=True)\n", "def test_invariant_factors_and_hermite_negative_entries():\n    from sympy import Matrix, ZZ\n    from sympy.polys.matrices import DomainMatrix\n    from sympy.polys.matrices.normalforms import invariant_factors, hermite_normal_form\n\n    m = Matrix([[-1, 2], [3, -4]])\n    dm = DomainMatrix(m, ZZ)\n    assert invariant_factors(dm) == (1, 2)\n    assert hermite_normal_form(dm).to_Matrix() == Matrix([[-1, 0], [0, 2]])\n\n    m = Matrix([[-3, 4], [5, -6]])\n    dm = DomainMatrix(m, ZZ)\n    assert invariant_factors(dm) == (1, 1)\n    assert hermite_normal_form(dm).to_Matrix() == Matrix([[1, 0], [0, 1]])\n"], "sample_613": ["def test_check_reduce_dims(array, reduce_dims, expected_dim, expected_msg):\n    with pytest.raises(ValueError, match=expected_msg):\n        check_reduce_dims(reduce_dims, array.dims)\n", "def test_quantile_index_variables(dataset):\n    ds = dataset.groupby(\"z\").quantile(0.5)\n    expected_index = xr.Variable(\"z\", [1, 2])\n    assert_equal(ds.indexes[\"z\"], expected_index)\n    assert_equal(ds.dims, (\"z\",))\n", "def test_groupby_quantile_scalar_result():\n    da = xr.DataArray([1, 2, 3, 4], dims=[\"x\"])\n    g = da.groupby(\"x\")\n    assert_equal(g.quantile(0), 1)\n    assert_equal(g.quantile(0.5), 2.5)\n    assert_equal(g.quantile(1), 4)\n", "def test_groupby_quantile_reduce():\n    \"\"\"Test that groupby with quantile reduction returns an array with a new dim.\"\"\"\n    ds = xr.Dataset(\n        {\"a\": ((\"x\",), [1, 2, 3])},\n        {\"x\": [1, 2, 3]},\n    )\n    ds[\"b\"] = ((\"x\",), [1.1, 2.2, 3.3])\n    da = ds[\"b\"].groupby(\"x\").quantile([0.25, 0.5, 0.75])\n\n    assert da.dims == (\"quantile\",)\n    assert da.coords[\"quantile\"].values == [0.25, 0.5, 0.75]\n\n    ds = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), np.random.randn(2, 3))},\n        {\"x\": [1, 2, 3], \"y\": [1, 2, 3, 4, 5, 6]},\n    )\n    ds[\"b\"] = ((\"x\", \"y\"), np.random.randn(2, 3))\n    da = ds[\"b\"].groupby(\"x\").quantile([0.25, 0.5, 0.75]\n\n    assert da.dims == (\"y\", \"quantile\")\n    assert da.coords[\"quantile\"].values == [0.25, 0.5, 0.75]\n\n    ds = xr.Dataset(\n        {\"a\": ((\"x\", \"y\"), np.random.randn(3, 4))},\n        {\"x\": [1, 2, 3], \"y\": [1, 2, 3, 4]},\n    )\n    ds[\"b\"] = ((\"x\", \"y\"), np.random.randn(3, 4))\n    da = ds[\"b\"].groupby(\"x\").quantile([0.25, 0.5, 0.75]\n\n    assert da.dims == (\"y\", \"quantile\")\n    assert da.coords[\"quantile\"].values == [0.25, 0.5, 0.75]\n\n", "def test_groupby_concat_and_unstack():\n    # Create a dataset with a multi-dimensional groupby\n    ds = Dataset(\n        {\n            \"foo\": ((\"x\", \"y\", \"z\"), np.random.randn(2, 2, 2)),\n            \"bar\": ((\"x\", \"z\"), np.random.randn(2, 2)),\n        },\n        {\"x\": [\"a\", \"b\"], \"y\": [1, 2, 3, 4], \"z\": [1, 2]},\n    )\n\n    # Groupby a 2D variable and stack the dimensions into a single dimension\n    groupby_obj = ds.groupby(\"y\")\n    ds_grouped = groupby_obj.map(lambda ds: ds[\"foo\"])\n\n    # Check that the resulting dataset has the correct dimensions\n    assert ds_grouped.dims == (\"y\", \"stacked_x_z\")\n\n    # Unstack the dimensions back into separate dimensions\n    ds_unstacked = ds_grouped.unstack(\"stacked_x_z\")\n\n    # Check that the resulting dataset has the correct dimensions\n    assert ds_unstacked.dims == (\"y\", \"x\", \"z\")\n\n    # Check that the data is correctly unstacked\n    assert_array_equal(ds_unstacked.data, ds[\"foo\"])\n\n    # Groupby a 1D variable and apply a function\n    groupby_obj = ds.groupby(\"z\")\n    ds_grouped = groupby_obj.map(lambda ds: ds[\"bar\"])\n\n    # Check that the resulting dataset has the correct dimensions\n    assert ds_grouped.dims == (\"z\",)\n\n    # Apply a function to the grouped data\n    ds_grouped = ds_grouped.mean(dim=\"z\")\n\n    # Check that the resulting dataset has the correct dimensions\n    assert ds_grouped.dims == ()\n\n"], "sample_424": ["    def test_create_model_reduces_alter_field(self):\n        # Create a model\n        create_model = CreateModel(\n            name=\"model\",\n            fields=[(\"field\", models.IntegerField())],\n            options={\"verbose_name\": \"Model\"},\n            bases=(models.Model,),\n        )\n        self.apply_operation(create_model)\n        self.assert_model_state(\n            \"model\",\n            [\n                (\"field\", models.IntegerField(verbose_name=\"Model\")),\n            ],\n            {\"verbose_name\": \"Model\"},\n            (models.Model,),\n        )\n\n        # Alter the field\n        alter_field = AlterField(\n            name=\"field\",\n            field=models.IntegerField(max_length=10),\n        )\n        self.apply_operation(alter_field)\n        self.assert_model_state(\n            \"model\",\n            [\n                (\"field\", models.IntegerField(verbose_name=\"Model\", max_length=10)),\n            ],\n            {\"verbose_name\": \"Model\"},\n            (models.Model,),\n        )\n\n        # CreateModel should reduce the AlterField\n        self.assert_operation_list(\n            [CreateModel(\n                name=\"model\",\n                fields=[\n                    (\"field\", models.IntegerField(max_length=10, verbose_name=\"Model\")),\n                ],\n                options={\"verbose_name\": \"Model\"},\n                bases=(models.Model,),\n            )]\n        )\n", "    def test_rename_model(self):\n        # Test that we can rename a model\n        self.assertNoOperations()\n        with transaction.atomic():\n            RenameModel('oldname', 'newname').apply(self.migration_plan)\n            self.assertAppliedOperations()\n        self.assertNoOperations()\n        self.assertEqual(Model._meta.db_table, 'newname')\n        self.assertEqual(Model._meta.db_table, 'newname')\n        with transaction.atomic():\n            RenameModel('newname', 'oldname').apply(self.migration_plan)\n            self.assertAppliedOperations()\n        self.assertNoOperations()\n        self.assertEqual(Model._meta.db_table, 'oldname')\n", "    def test_options_changes_on_alter_model_options(self):\n        # Create a model with some options\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create()\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(\n                name='new_value', description='new_value')\n\n        self.assertEqual(\n            UnicodeModel._meta.get_field('name').choices,\n            [(1, 'Name 1'), (2, 'Name 2')]\n        )\n\n        self.assertEqual(\n            UnicodeModel._meta.get_field('description').choices,\n            [(1, 'Desc 1'), (2, 'Desc 2')]\n        )\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(name='new_value', description='new_value')\n\n        # Alter model options\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create()\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(\n                name='new_value', description='new_value')\n\n        self.assertEqual(\n            UnicodeModel._meta.get_field('name').choices,\n            [(1, 'Name 1'), (2, 'Name 2')]\n        )\n\n        self.assertEqual(\n            UnicodeModel._meta.get_field('description').choices,\n            [(1, 'Desc 1'), (2, 'Desc 2')]\n        )\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(name='new_value', description='new_value')\n\n        # Test altering model options without affecting the database\n        migration = Migration(\n            'tests', '0004_alter_unicodemodel_options',\n            [\n                AlterModelOptions('unicode_model', {\n                    'choices': [(1, 'Name 1'), (2, 'Name 2')],\n                    'default_permissions': ['add'],\n                    'get_latest_by': 'name',\n                    'ordering': 'name',\n                    'permissions': ['add_unicodemodel', 'change_unicodemodel'],\n                    'select_on_save': False,\n                    'verbose_name': 'Unicode model',\n                    'verbose_name_plural': 'Unicode models',\n                })\n            ]\n        )\n        with transaction.atomic():\n            migration.apply(connection)\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create()\n\n        with self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(\n                name='new_value', description='new_value')\n\n        self.assertEqual(\n            UnicodeModel._meta.get_field('name').choices,\n            [(1, 'Name", "    def test_rename_model_down(self):\n        \"\"\"\n        Test renaming a model backwards.\n        \"\"\"\n        model = UnicodeModel.objects.create(id=1, name='a')\n        initial_query = UnicodeModel.objects.raw(\"SELECT * FROM django_test_unicode_model_unicode_model\")\n        self.assertEqual(list(initial_query), [(1, 'a')])\n        self.assertEqual(models.UnicodeModel.objects.count(), 1)\n        self.assertEqual(models.UnicodeModel.objects.get().name, 'a')\n\n        with CaptureQueriesContext(connection) as queries:\n            with transaction.atomic():\n                with self.assertRaises(IntegrityError):\n                    UnicodeModel.objects.update(id=1, name='b')\n        self.assertEqual(len(queries), 1)\n        self.assertEqual(queries[0], \"SELECT id FROM django_test_unicode_model_unicode_model WHERE id = 1\")\n\n        with CaptureQueriesContext(connection) as queries:\n            with transaction.atomic():\n                self.create_migration_operation(models.UnicodeModel, RenameModel, 'new_name')\n                self.apply_migration()\n                self.assertMigrationApplied()\n                self.assertEqual(models.UnicodeModel.objects.count(), 1)\n                self.assertEqual(models.UnicodeModel.objects.get().name, 'a')\n        self.assertEqual(len(queries), 0)\n\n        with CaptureQueriesContext(connection) as queries:\n            with transaction.atomic():\n                self.create_migration_operation(models.UnicodeModel, RenameModel, 'new_name')\n                self.apply_migration()\n                self.assertMigrationApplied()\n                self.assertEqual(models.UnicodeModel.objects.count(), 1)\n                self.assertEqual(models.UnicodeModel.objects.get().name, 'a')\n        self.assertEqual(len(queries), 0)\n\n        with CaptureQueriesContext(connection) as queries:\n            with transaction.atomic():\n                with self.assertRaises(IntegrityError):\n                    UnicodeModel.objects.update(id=1, name='b')\n        self.assertEqual(len(queries), 1)\n        self.assertEqual(queries[0], \"SELECT id FROM django_test_unicode_model_new_name WHERE id = 1\")\n\n        with CaptureQueriesContext(connection) as queries:\n            with transaction.atomic():\n                self.create_migration_operation(models.UnicodeModel, RenameModel, 'new_name')\n                self.apply_migration()\n                self.assertMigrationApplied()\n                self.assertEqual(models.UnicodeModel.objects.count(), 1)\n                self.assertEqual(models.UnicodeModel.objects.get().name, 'a')\n        self.assertEqual(len(queries), 0)\n", "    def test_empty_managers(self):\n        with transaction.atomic():\n            manager = FoodManager()\n            model = UnicodeModel.objects.using(\"default\").raw(\"SELECT * FROM django_unicode_model\")\n            self.assertEqual(model.managers, manager)\n\n        with transaction.atomic():\n            operation = AlterModelManagers(\"unicode\", [])\n            model = UnicodeModel.objects.using(\"default\").raw(\"SELECT * FROM django_unicode_model\")\n            self.assertEqual(model.managers, operation.managers)\n\n        # Now, delete the table so we can test that the migration removes the manager\n        with connection.schema_editor().delete_model(UnicodeModel):\n            pass\n\n        # Add the original model back, but with no managers\n        with transaction.atomic():\n            UnicodeModel._meta.managers = []\n            UnicodeModel._meta.managed = True\n            UnicodeModel.objects.using(\"default\").raw(\"CREATE TABLE django_unicode_model (id serial PRIMARY KEY, foo varchar(255), CONSTRAINT django_unicode_model_pkey PRIMARY KEY (id));\")\n\n        with transaction.atomic():\n            operation = AlterModelManagers(\"unicode\", [])\n            # The operation should delete the model before it tries to alter its managers\n            self.assertRaises(IntegrityError, operation.database_forwards, \"default\", connection.schema_editor(), ModelState(\"default\", \"unicode\", [(0, 'charfield'), (1, 'charfield')], {'default_permissions': ['add', 'change', 'delete']}, (models.Model,), []), ProjectState())\n            self.assertEqual(UnicodeModel._meta.managers, [manager])\n\n        # Add the original model back, with the managers\n        with transaction.atomic():\n            UnicodeModel._meta.managers = [manager]\n            UnicodeModel._meta.managed = True\n            UnicodeModel.objects.using(\"default\").raw(\"CREATE TABLE django_unicode_model (id serial PRIMARY KEY, foo varchar(255), CONSTRAINT django_unicode_model_pkey PRIMARY KEY (id));\")\n\n        with transaction.atomic():\n            operation = AlterModelManagers(\"unicode\", [])\n            # The operation should delete the model before it tries to alter its managers\n            self.assertRaises(IntegrityError, operation.database_backwards, \"default\", connection.schema_editor(), ModelState(\"default\", \"unicode\", [(0, 'charfield'), (1, 'charfield')], {'default_permissions': ['add', 'change', 'delete']}, (models.Model,), []), ProjectState())\n            self.assertEqual(UnicodeModel._meta.managers, [manager])\n\n   "], "sample_208": ["    def test_alter_model_options(self):\n        before_states = [\n            self.author_with_options,\n            self.author_with_options,\n            self.author_custom_pk,\n        ]\n        after_states = [\n            self.author_with_options,\n            self.author_with_options,\n            self.author_custom_pk,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        migration = changes['testapp'][0]\n        self.assertEqual(migration.operations[0].__class__.__name__, 'AlterModelOptions')\n        self.assertEqual(migration.operations[0].options, {\n            'permissions': [('can_hire', 'Can hire')],\n            'verbose_name': 'Authi',\n        })\n", "    def test_renamed_field_rename_default(self):\n        author_name_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default='Ada Lovelace')),\n        ])\n        author_name_renamed_default = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"names\", models.CharField(max_length=200, default='Ada Lovelace')),\n        ])\n        changes = self.get_changes([author_name_default], [author_name_renamed_default])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n", "    def test_alter_options(self):\n        before_states = [self.author_empty]\n        after_states = [self.author_options]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, **self.author_options.options)\n", "    def test_alter_model_table_operation(self):\n        from testapp import models\n        before_states = [models.author_empty]\n        after_states = [models.author_with_db_table_options]\n        questioner = mock.Mock()\n        changes = self.get_changes(before_states, after_states, questioner)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelTable'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', table='author_one')\n", "    def test(self):\n        with override_settings(USE_TZ=True):\n            before_states = [author_with_book, author_renamed_with_book]\n            after_states = [author_renamed_with_book, book_proxy_fk]\n            changes = self.get_changes(before_states, after_states)\n            self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n            self.assertOperationTypes(changes, 'otherapp', 0, ['RenameModel'])\n            self.assertOperationTypes(changes, 'otherapp', 1, ['AddIndex'])\n            self.assertOperationFieldAttributes(changes, 'otherapp', 1, 0, name='book_author_idx')\n            self.assertEqual(changes['otherapp'][1].operations[0].dependencies, [('testapp', 'Author', None, True)])\n"], "sample_1200": ["def test_get_units_non_prefixed():\n    # Test case 1: Empty unit system\n    unit_system = UnitSystem([], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == set()\n\n    # Test case 2: Unit system with one unit without prefix\n    unit_system = UnitSystem([Quantity(\"meter\", Dimension(length))], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == {Quantity(\"meter\", Dimension(length))}\n\n    # Test case 3: Unit system with units with and without prefixes\n    unit_system = UnitSystem([Quantity(\"meter\", Dimension(length)), Quantity(\"kilometer\", Dimension(length))], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == {Quantity(\"meter\", Dimension(length))}\n\n    # Test case 4: Unit system with all units having prefixes\n    unit_system = UnitSystem([Quantity(\"km\", Dimension(length)), Quantity(\"kg\", Dimension(mass))], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == set()\n\n    # Test case 5: Unit system with units that are physical constants\n    unit_system = UnitSystem([PhysicalConstant(\"c\", Dimension(length/time), 299792458), PhysicalConstant(\"G\", Dimension(gravity), 6.67430e-11)], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == set()\n\n    # Test case 6: Unit system with a mix of physical constants and non-physical units\n    unit_system = UnitSystem([PhysicalConstant(\"c\", Dimension(length/time), 299792458), Quantity(\"meter\", Dimension(length))], name=\"Test System\")\n    assert unit_system.get_units_non_prefixed() == {Quantity(\"meter\", Dimension(length))}\n", "def test_get_units_non_prefixed():\n    # Test that it returns the correct set of units\n    si = SI()\n    assert si.get_units_non_prefixed() == {m, kg, s, A, K, cd, mol, rad, sr}\n\n    # Test that it ignores prefixed units\n    assert not any(u.is_prefixed for u in si.get_units_non_prefixed())\n\n    # Test that it ignores physical constants\n    assert not any(u.is_physical_constant for u in si.get_units_non_prefixed())\n\n    # Test that it returns an empty set for a custom unit system\n    sys = UnitSystem((Dimension(length), Dimension(time)), name='Custom')\n    assert sys.get_units_non_prefixed() == set()\n\n    # Test that it returns an empty set for a unit system with no non-prefixed units\n    sys = UnitSystem((kilo * m, Dimension(time)), name='Custom')\n    assert sys.get_units_non_prefixed() == set()\n\n    # Test that it raises an error for a non-physical unit system\n    with warns_deprecated_sympy():\n        class NonPhysicalUnitSystem(_QuantityMapper):\n            pass\n        sys = NonPhysicalUnitSystem()\n        raises(ValueError, sys.get_units_non_prefixed)\n", "    def test_extend_empty_system(self):\n        # Given\n        unit_system = UnitSystem((), name=\"CustomSystem\", description=\"Test unit system\")\n        # When\n        new_system = unit_system.extend((length, time), (\"meter\", \"second\"))\n        # Then\n        assert new_system.name == \"CustomSystem\"\n        assert new_system._base_units == (length, time)\n        assert new_system._units == (length, time, \"meter\", \"second\")\n", "def test_extend_unit_system():\n    # Test extending the SI unit system\n    si_system = SI()\n    extended_system = si_system.extend(base=kg, units=(kilogram,), name=\"Extended SI\")\n    assert len(extended_system._base_units) == 2\n    assert len(extended_system._units) == 4  # Includes the newly added unit\n\n    # Test extending a custom unit system\n    custom_system = UnitSystem(base_units=(kg, m), units=(), name=\"Custom System\", derived_units={Dimension(length): S(1) * Dimension(time)})\n    extended_custom_system = custom_system.extend(base=hour, units=(kilogram,), name=\"Extended Custom\")\n    assert len(extended_custom_system._base_units) == 3\n    assert len(extended_custom_system._units) == 4  # Includes the newly added unit\n\n    # Test extending a unit system with a name that already exists\n    with warns_deprecated_sympy():\n        si_system = SI()\n        try:\n            si_system.extend(base=kg, units=(kilogram,), name=\"SI\")\n            assert False, \"Expected a ValueError to be raised\"\n        except ValueError as e:\n            assert str(e) == \"Unit system already exists with name 'SI'\"\n\n    # Test extending a unit system with an inconsistent dimension system\n    custom_system = UnitSystem(base_units=(kg, m), units=(), name=\"Custom System\", derived_units={Dimension(length): S(1) * Dimension(time)})\n    custom_system._dimension_system = DimensionSystem([(Dimension(length), Dimension(time))])  # Make the dimension system inconsistent\n    with raises(ValueError):\n        custom_system.extend(base=hour, units=(kilogram,))\n", "def test_derived_units():\n    si = SI()\n    cgs = si.extend(base=(), units=(kilometer, kilogram, second))\n    assert cgs._derived_units == {Dimension(mass): kilogram,\n                                 Dimension(time): second,\n                                 Dimension(length): kilometer}\n\n    m = meter\n    s = second\n    km = kilometer\n    g = gram\n    derived_system = cgs.extend(base=(m, s, g), units=(km,))\n\n    assert derived_system._derived_units == {Dimension(mass): g,\n                                           Dimension(time): s,\n                                           Dimension(length): km,\n                                           Dimension(length / time): km/s,\n                                           Dimension(length / time**2): km/s**2,\n                                           Dimension((length**2)/time**2): (km**2/s**2),\n                                           Dimension((length**3)/time**2): (km**3/s**2),\n                                           Dimension((length**2)/time): (km/s),\n                                           Dimension(length/time): (km/s),\n                                           Dimension((length**2)/time**3): (km**2/s**3),\n                                           Dimension((length**3)/time**3): (km**3/s**3),\n                                           Dimension(mass / time**2): (g/s**2),\n                                           Dimension((mass / time) / time): (g/s),\n                                           Dimension(length / time**4): (km/s**4),\n                                           Dimension((length**2) / time**4): (km**2/s**4),\n                                           Dimension((length**3) / time**4): (km**3/s**4),\n                                           Dimension(length * (mass / time**2)): (km * g/s**2),\n                                           Dimension((length**2) * (mass / time**2)): (km**2 * g/s**2),\n                                           Dimension((length**3) * (mass / time**2)): (km**3 * g/s**2),\n                                           Dimension((length**4) / time**2): (km**4/s**2),\n                                           Dimension((length**5) / time**2): (km**5/s**2),\n                                           Dimension((length**6) / time**2): (km**6/s**2),\n                                           Dimension(mass / (length / time**2)): (g / (km/s**2)),\n                                           Dimension(mass / (length**2 / time**2)): (g / (km**2/s**2)),\n                                           Dimension((length"], "sample_1139": ["def test_ImageSet_lamda():\n    x = Symbol('x')\n    N = S.Naturals\n    squares = ImageSet(Lambda(x, x**2), N)\n    assert squares.lamda(2) == 4\n    assert squares.lamda(S(1)/2) == 1/4\n    assert squares.lamda(0) == 0\n", "def test_ImageSet_equality():\n    a, b = symbols('a b', real=True)\n    c, d = symbols('c d', real=True)\n    # test equality of ImageSet with a single argument\n    assert ImageSet(Lambda(a, a**2), S.Naturals) == ImageSet(Lambda(a, a**2), S.Naturals)\n    assert ImageSet(Lambda(a, a**2), S.Naturals) != ImageSet(Lambda(a, a), S.Naturals)\n    # test equality of ImageSet with multiple arguments\n    assert ImageSet(Lambda((a, b), a**2 + b**2), S.Naturals*S.Naturals) == \\\n        ImageSet(Lambda((a, b), a**2 + b**2), S.Naturals*S.Naturals)\n    assert ImageSet(Lambda((a, b), a**2 + b**2), S.Naturals*S.Naturals) != \\\n        ImageSet(Lambda((a, b), a**2 - b**2), S.Naturals*S.Naturals)\n    # test equality of ImageSet with different types of sets\n    assert ImageSet(Lambda((a, b), a**2 + b**2), Union(S.Naturals*S.Naturals, S.Rationals*S.Rationals)) == \\\n        ImageSet(Lambda((a, b), a**2 + b**2), Union(S.Naturals*S.Naturals, S.Rationals*S.Rationals))\n    assert ImageSet(Lambda((a, b), a**2 + b**2), Union(S.Naturals*S.Naturals, S.Rationals*S.Rationals)) != \\\n        ImageSet(Lambda((a, b), a**2 - b**2), Union(S.Naturals*S.Naturals, S.Rationals*S.Rationals))\n    # test that ImageSet is not commutative\n    assert ImageSet(Lambda((a, b), a**2 + b**2), S.Naturals*S.Naturals) != \\\n        ImageSet(Lambda((b, a), a**2 + b**2), S.Naturals*S.Naturals)\n", "def test_ImageSet_polar_complex_region():\n    # Test that ImageSet with a polar complex region is handled correctly\n    r = Interval(1, 2)\n    theta = Interval(0, pi)\n    image_set = ImageSet(Lambda((r, theta), r*(cos(theta) + I*sin(theta))), r*theta, polar=True)\n    assert image_set.is_ComplexRegion\n    assert image_set.polar\n    assert image_set.variables == (r, theta)\n", "def test_Naturals():\n    n = symbols('n', integer=True)\n    assert Naturals(n) == Naturals\n    assert Naturals(n + 1) == Naturals\n", "def test_ImageSet_multiple_variables():\n    x, y = symbols('x y')\n    lam = Lambda((x, y), x + y)\n    s1 = S.Naturals0\n    s2 = S.Integers\n    image_set = ImageSet(lam, s1, s2)\n    assert isinstance(image_set, ImageSet)\n    expected_set = FiniteSet(2, 3, 4)\n    assert image_set.as_relational(x) == expected_set.as_relational(x)\n\n    # Test with Union of sets as domain\n    u = Union(S.Naturals0, S.Integers)\n    image_set = ImageSet(lam, u)\n    expected_set = FiniteSet(0, 1, 2)\n    assert image_set.as_relational(x) == expected_set.as_relational(x)\n\n    # Test with ProductSet as domain\n    s1 = S.Naturals0\n    s2 = S.Integers\n    p = ProductSet(s1, s2)\n    image_set = ImageSet(lam, p)\n    expected_set = FiniteSet((0, 0), (1, 0), (0, 1))\n    assert image_set.as_relational(x) == expected_set.as_relational(x)\n\n    # Test with PolarComplexRegion as domain\n    r = Interval(0, 2)\n    theta = Interval(0, pi)\n    p = ProductSet(r, theta)\n    image_set = ImageSet(lam, p, polar=True)\n    expected_set = FiniteSet(1 + I, 2 + I)\n    assert image_set.as_relational(x) == expected_set.as_relational(x)\n"], "sample_383": ["    def setUpTestData(cls):\n        cls.category1 = Category.objects.create(name=\"category1\")\n        cls.category2 = Category.objects.create(name=\"category2\")\n        cls.category3 = Category.objects.create(name=\"category3\")\n\n        cls.item1 = Item.objects.create(name=\"item1\", category=cls.category1)\n        cls.item2 = Item.objects.create(name=\"item2\", category=cls.category2)\n        cls.item3 = Item.objects.create(name=\"item3\", category=cls.category3)\n", "    def setUpTestData(cls):\n        cls.book1 = Book.objects.create(name='Book 1', author=cls.a1)\n        cls.book2 = Book.objects.create(name='Book 2', author=cls.a2)\n        cls.book3 = Book.objects.create(name='Book 3', author=cls.a3)\n        cls.book4 = Book.objects.create(name='Book 4', author=cls.a4)\n        cls.chapter1 = Chapter.objects.create(title='Chapter 1', book=cls.book1)\n        cls.chapter2 = Chapter.objects.create(title='Chapter 2', book=cls.book1)\n        cls.chapter3 = Chapter.objects.create(title='Chapter 3', book=cls.book2)\n        cls.chapter4 = Chapter.objects.create(title='Chapter 4', book=cls.book3)\n\n        cls.student1 = Student.objects.create(name='Student 1', school=cls.s1)\n        cls.student2 = Student.objects.create(name='Student 2', school=cls.s1)\n        cls.student3 = Student.objects.create(name='Student 3', school=cls.s2)\n        cls.teacher1 = Teacher.objects.create(name='Teacher 1', school=cls.s1)\n        cls.teacher2 = Teacher.objects.create(name='Teacher 2', school=cls.s2)\n        cls.staff1 = Staff.objects.create(name='Staff 1', school=cls.s1)\n        cls.staff2 = Staff.objects.create(name='Staff 2', school=cls.s1)\n", "    def setUpTestData(cls):\n        cls.user1 = StaffUser.objects.create(\n            name=\"user1\",\n            staff_number=1001,\n            staff_email=\"user1@example.com\",\n            staff_phone=\"1234567890\",\n            role=Staff.objects.create(name=\"role1\"),\n        )\n        cls.user2 = StaffUser.objects.create(\n            name=\"user2\",\n            staff_number=2002,\n            staff_email=\"user2@example.com\",\n            staff_phone=\"9876543210\",\n            role=Staff.objects.create(name=\"role2\"),\n        )\n\n        cls.c1 = Category.objects.create(name=\"cat1\", description=\"desc1\")\n        cls.c2 = Category.objects.create(name=\"cat2\", description=\"desc2\")\n        cls.c3 = Category.objects.create(name=\"cat3\", description=\"desc3\")\n        cls.c4 = Category.objects.create(name=\"cat4\", description=\"desc4\")\n\n        cls.item1 = Item.objects.create(name=\"item1\", category=cls.c1, price=10.99)\n        cls.item2 = Item.objects.create(name=\"item2\", category=cls.c2, price=20.99)\n        cls.item3 = Item.objects.create(name=\"item3\", category=cls.c3, price=30.99)\n        cls.item4 = Item.objects.create(name=\"item4\", category=cls.c4, price=40.99)\n\n        cls.s1 = School.objects.create(name=\"school1\")\n        cls.s2 = School.objects.create(name=\"school2\")\n\n        cls.c1.item.set([cls.item1, cls.item2])\n        cls.c2.item.set([cls.item3, cls.item4])\n        cls.s1.category.set([cls.c1, cls.c2])\n        cls.s2.category.set([cls.c3, cls.c4])\n", "    def setUpTestData(cls):\n        cls.d1 = DumbCategory.objects.create(name=\"d1\")\n        cls.d2 = DumbCategory.objects.create(name=\"d2\")\n        cls.d3 = DumbCategory.objects.create(name=\"d3\")\n\n        cls.c1 = CategoryItem.objects.create(category=cls.d1)\n        cls.c2 = CategoryItem.objects.create(category=cls.d2)\n        cls.c3 = CategoryItem.objects.create(category=cls.d3)\n\n        cls.e1 = Eaten.objects.create(category=cls.d2)\n", "    def setUpTestData(cls):\n        cls.c = Channel.objects.create(title=\"c\", owner=cls.a2)\n        cls.t6 = Tag.objects.create(name=\"t6\", parent=cls.t1, category=cls.c)\n        cls.t7 = Tag.objects.create(name=\"t7\", parent=cls.t1, category=cls.c)\n        cls.t8 = Tag.objects.create(name=\"t8\", parent=cls.t6)\n        cls.t9 = Tag.objects.create(name=\"t9\", parent=cls.t7)\n        cls.t10 = Tag.objects.create(name=\"t10\", parent=cls.t7)\n\n        cls.o1 = ObjectA.objects.create(name=\"o1\", item=cls.i4)\n        cls.o2 = ObjectA.objects.create(name=\"o2\", item=cls.i4)\n        cls.o3 = ObjectB.objects.create(name=\"o3\", item=cls.i4)\n\n        cls.chapter = Chapter.objects.create(title=\"c1\", item=cls.i4)\n        cls.chapter.content = \"content\"\n        cls.chapter.save()\n\n        cls.child1 = Child.objects.create(\n            name=\"child1\", parent=cls.a4, relationship=cls.chapter\n        )\n        cls.child2 = Child.objects.create(\n            name=\"child2\", parent=cls.a4, relationship=cls.chapter\n        )\n        cls.child3 = Child.objects.create(\n            name=\"child3\", parent=cls.a4, relationship=cls.chapter\n        )\n\n        cls.fc = Food.objects.create(name=\"fc\", author=cls.a4)\n        cls.fc2 = Food.objects.create(name=\"fc2\", author=cls.a4)\n\n        cls.hc = Home.objects.create(name=\"hc\", author=cls.a4)\n        cls.hc2 = Home.objects.create(name=\"hc2\", author=cls.a4)\n\n        cls.note1 = Note.objects.create(note=\"note1\", item=cls.i4, author=cls.a4)\n        cls.note2 = Note.objects.create(note=\"note2\", item=cls.i4, author=cls.a4)\n        cls.note3 = Note.objects.create(note=\"note3\", item=cls.i4, author=cls.a4)\n"], "sample_395": ["def test_is_django_path(self):\n    # Test is_django_path function\n    # Create a Django app with an app_dir\n    app_dir = Path(\"my_app\")\n    app_dir.mkdir()\n    (app_dir / \"__init__.py\").touch()\n    (app_dir / \"templates\" / \"base.html\").touch()\n\n    # Test that is_django_path returns True for a Django app directory\n    self.assertTrue(is_django_path(app_dir))\n\n    # Test that is_django_path returns False for a non-Django app directory\n    non_django_dir = Path(\"non_django_dir\")\n    non_django_dir.mkdir()\n    (non_django_dir / \"__init__.py\").touch()\n    (non_django_dir / \"base.html\").touch()\n    self.assertFalse(is_django_path(non_django_dir))\n\n    # Remove the app directory\n    app_dir.rmdir()\n    (app_dir / \"__init__.py\").unlink()\n    (app_dir / \"templates\" / \"base.html\").unlink()\n    app_dir.rmdir()\n\n    # Test that is_django_path returns False for a non-existent directory\n    self.assertFalse(is_django_path(app_dir))\n", "    def test_get_template_directories(self):\n        template_dir = EXTRA_TEMPLATES_DIR / \"template1.txt\"\n        template_dir.parent.mkdir(parents=True, exist_ok=True)\n        template_dir.write_text(\"Hello, world!\")\n\n        template_dir = EXTRA_TEMPLATES_DIR / \"template2.txt\"\n        template_dir.parent.mkdir(parents=True, exist_ok=True)\n        template_dir.write_text(\"Hello, world!\")\n\n        self.assertIn(template_dir, get_template_directories())\n\n        template_dir = Path(\"/non/existing/dir\")\n        self.assertNotIn(template_dir, get_template_directories())\n\n        reset_loaders()\n\n        # Test for is_django_path\n        self.assertTrue(is_django_path(to_path(\"templates\")))\n        self.assertFalse(is_django_path(to_path(\"/non/existing/dir\")))\n\n        # Test for get_dirs\n        class MockTemplateLoader:\n                return [\"/path1\", \"/path2\"]\n\n        loader = MockTemplateLoader()\n        self.assertEqual(get_template_directories(), {Path(\"/path1\"), Path(\"/path2\")})\n", "    def test_get_template_directories_returns_empty_set_if_no_django_templates(self):\n        with self.settings(INSTALLED_APPS=[\"template_tests\", \"another_app\"]) as cm:\n            self.assertEqual(get_template_directories(), set())\n", "    def test_get_template_directories_empty_template_backends(self, mock_reset_loaders):\n        # Create an instance of DjangoTemplates with no directories\n        backend = DjangoTemplates()\n        backend.engine.dirs = []\n        backend.engine.template_loaders = []\n        engines.all()[0] = backend\n\n        self.assertEqual(get_template_directories(), set())\n", "def test_watch_for_template_changes_initiates_watch_dir(self, mock_receiver, mock_reset_loaders):\n    from django.template import autoreload\n    mock_autoreload_started = mock.Mock()\n    mock_receiver.return_value.__name__ = \"mock_receiver\"\n    mock_receiver.return_value.send.return_value = None\n\n    # Set up mock autoreload_started signal\n    autoreload.autoreload_started.send = mock_autoreload_started.send\n\n    # Call the watch_for_template_changes function\n    watch_for_template_changes(mock_autoreload_started)\n\n    # Check that watch_dir was called with all template directories\n    assert len(mock_autoreload_started.mock_calls) == 1\n    mock_receiver.assert_called_once_with(mock_autoreload_started)\n    mock_autoreload_started.send.assert_called_once_with(mock_receiver.return_value)\n    assert len(mock_autoreload_started.send.return_value.mock_calls) == 1\n    watch_for_template_changes_dirs = mock_autoreload_started.send.return_value.mock_calls[0][1][0].args[0]\n    assert watch_for_template_changes_dirs == get_template_directories()\n"], "sample_44": ["    def test_mul_log_unit(self):\n        dex = u.dex\n        mag = u.mag\n        decibel = u.decibel\n\n        for u1 in lu_units:\n            for u2 in lu_units:\n                result = dex * u1\n                if u1 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == dex\n                elif u2 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == dex * u1\n                else:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == dex * u1\n\n                result = mag * u1\n                if u1 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == mag\n                elif u2 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == mag * u1\n                else:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == mag * u1\n\n                result = decibel * u1\n                if u1 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == decibel\n                elif u2 == dimensionless_unscaled:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == decibel * u1\n                else:\n                    assert isinstance(result, u.FunctionUnitBase)\n                    assert result.function_unit == decibel * u1\n", "    def test_multiplication(self):\n        for lu_unit, lq_class in zip(lu_units, lq_subclasses):\n            for other_unit in lu_units + [u.dimensionless_unscaled]:\n                if other_unit is lu_unit:\n                    with pytest.raises(UnitTypeError):\n                        lq_class(10) * lu_unit\n                else:\n                    result = lq_class(10) * lu_unit\n                    assert isinstance(result, lq_class)\n                    assert_quantity_allclose(result.value, 10 * lu_unit.to_value(lq_class._unit_class._default_function_unit))\n", "    def test_multiplication(self):\n        for lu, lq in zip(lu_units, lq_subclasses):\n            lu_quantity = lu(10)\n            lq_quantity = lq(10)\n            result = lu_quantity * lq_quantity\n            assert_quantity_allclose(result, lu(100))\n", "    def test_equivalencies(self):\n        for lu, subclass in zip(lu_units, lu_subclasses):\n            for other, other_subclass in itertools.product(lu_units, lu_subclasses):\n                if lu.physical_unit != other.physical_unit:\n                    assert lu.is_equivalent(other, equivalencies=subclass.equivalencies)\n                else:\n                    assert not lu.is_equivalent(other)\n", "    def test_function_unit_multiplication(self):\n        m = u.Magnitude(10)\n        m2 = u.Magnitude(5)\n        assert (m * u.Magnitude(2)) == u.Magnitude(20)\n        assert (m * m2) == u.Magnitude(50)\n"], "sample_929": ["def test_filter_meta_fields():\n    app = Mock()\n    env = Mock()\n    env.data = {'objects': {}}\n    content = nodes.field_list()\n    field = nodes.field()\n    field.astext = lambda: 'meta'\n    content.append(field)\n    filter_meta_fields(app, 'py', 'function', content)\n    assert len(content) == 0\n\n    field = nodes.field()\n    field.astext = lambda: 'meta '\n    content.append(field)\n    filter_meta_fields(app, 'py', 'function', content)\n    assert len(content) == 0\n\n    field = nodes.field()\n    field.astext = lambda: 'other'\n    content.append(field)\n    filter_meta_fields(app, 'py', 'function', content)\n    assert len(content) == 1\n\n    filter_meta_fields(app, 'other', 'function', content)\n    assert len(content) == 1\n", "def test_parse_annotation_no_annotation():\n    env = Mock(spec=BuildEnvironment)\n    signode = addnodes.desc_annotation('', '')\n    annotation = _parse_annotation('str', env)\n    assert_node(signode, nodes.Text('str'))\n", "    def test_parse_method_signature(self):\n        signature = 'def my_method(self, arg1, arg2, *, kwarg):'\n        signode = parse(signature)\n        assert signode == 'def my_method(self, arg1, arg2, /, *, kwarg)'\n", "def test_parse_annotation():\n    env = Mock()\n    assert _parse_annotation('int') == [addnodes.desc_sig_name('', 'int')]\n    assert _parse_annotation('str') == [addnodes.desc_sig_name('', 'str')]\n    assert _parse_annotation('List[int]') == [\n        addnodes.desc_sig_name('', 'List'),\n        addnodes.desc_sig_operator('['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_operator(']'),\n    ]\n    assert _parse_annotation('Dict[str, int]') == [\n        addnodes.desc_sig_name('', 'Dict'),\n        addnodes.desc_sig_operator('['),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_operator(', '),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_operator(']'),\n    ]\n    assert _parse_annotation('Optional[int]') == [\n        addnodes.desc_sig_name('', 'Optional'),\n        addnodes.desc_sig_operator('['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_operator(']'),\n    ]\n    assert _parse_annotation('Union[int, str]') == [\n        addnodes.desc_sig_name('', 'Union'),\n        addnodes.desc_sig_operator('['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_operator(', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_operator(']'),\n    ]\n    assert _parse_annotation('Set[int]') == [\n        addnodes.desc_sig_name('', 'Set'),\n        addnodes.desc_sig_operator('['),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_operator(']'),\n    ]\n", "def test_parse_annotation():\n    env = Mock()\n    env.ref_context = {'py:module': 'mock_module'}\n    env.config.add_module_names = True\n\n    # Test a simple annotation\n    annotation = 'int'\n    expected = [nodes.Text('int')]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test a more complex annotation\n    annotation = 'List[str]'\n    expected = [\n        addnodes.desc_sig_name('', 'List'),\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test a type hint with a default value\n    annotation = 'int = 5'\n    expected = [\n        nodes.Text('int'),\n        addnodes.desc_sig_operator('', '='),\n        nodes.Text('5')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test an annotation with a nested type hint\n    annotation = 'List[List[str]]'\n    expected = [\n        addnodes.desc_sig_name('', 'List'),\n        addnodes.desc_sig_punctuation('', '['),\n        addnodes.desc_sig_name('', 'List'),\n        addnodes.desc_sig_punctuation('', '['),\n        nodes.Text('str'),\n        addnodes.desc_sig_punctuation('', ']'),\n        addnodes.desc_sig_punctuation('', ']')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n\n    # Test an annotation with a parameter type\n    annotation = 'foo(int, str)'\n    expected = [\n        addnodes.desc_sig_name('', 'foo'),\n        addnodes.desc_sig_punctuation('', '('),\n        addnodes.desc_sig_name('', 'int'),\n        addnodes.desc_sig_punctuation('', ', '),\n        addnodes.desc_sig_name('', 'str'),\n        addnodes.desc_sig_punctuation('', ')')\n    ]\n    result = _parse_annotation(annotation, env)\n    assert assert_node(result, expected)\n"], "sample_832": ["def test_bayesianridge_alpha_init_as_variance():\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    y = np.array([0, 1, 2])\n    clf = BayesianRidge(alpha_init=np.var(y))\n    clf.fit(X, y)\n    assert_almost_equal(clf.alpha_, 1.0)\n", "def test_ard_regression_high_dim():\n    n_samples = 100\n    n_features = 100\n    rng = check_random_state(0)\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples)\n    model = ARDRegression()\n    model.fit(X, y)\n    y_pred = model.predict(X)\n    assert_array_almost_equal(y_pred, model.predict(X))\n", "    def test_init(self):\n        model = ARDRegression(n_iter=100)\n        assert isinstance(model.n_iter, int)\n        assert model.n_iter > 0\n", "def test_fit_intercept_false():\n    X, y = diabetes.data, diabetes.target\n    model = ARDRegression(fit_intercept=False)\n    model.fit(X, y)\n    assert model.intercept_ == 0\n    assert model._intercept_ is None\n", "def test_bayesian_ridge_convergence(self):\n    # Test that BayesianRidge converges to the expected values\n    rng = check_random_state(0)\n    X = rng.rand(100, 5)\n    y = rng.rand(100)\n\n    # Test with convergence\n    model = BayesianRidge(max_iter=10)\n    model.fit(X, y)\n    assert_almost_equal(model.n_iter_, 10)\n\n    # Test without convergence\n    model = BayesianRidge(max_iter=5)\n    model.fit(X, y)\n    assert model.n_iter_ >= 5\n\n    # Test with initial values\n    model = BayesianRidge(alpha_init=1.0, lambda_init=1.0, max_iter=10)\n    model.fit(X, y)\n    assert_almost_equal(model.alpha_, 1.0)\n    assert_almost_equal(model.lambda_, 1.0)\n\n    # Test with compute_score\n    model = BayesianRidge(compute_score=True, max_iter=10)\n    model.fit(X, y)\n    assert len(model.scores_) == 10\n"], "sample_369": ["    def test_rename_order_with_respect_to(self):\n        # Arrange\n        before_states = [self.author_with_book_order_wrt]\n        after_states = [self.author_with_book_order_wrt]\n        autodetector = MigrationAutodetector(\n            before_states,\n            after_states,\n            questioner=MigrationQuestioner()\n        )\n        changes = autodetector._detect_changes()\n\n        # Act\n        self.assertEqual(len(changes.get(\"testapp\", [])), 0)\n\n        # Assert\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n", "    def test_order_with_respect_to(self):\n        # Test that AlterOrderWithRespectTo works\n        project_state = self.make_project_state([\n            self.author_with_book,\n            self.author_with_book_order_wrt,\n            self.book,\n        ])\n        before_states = project_state.clone()\n        after_states = project_state.clone()\n        changes = self.get_changes(before_states.models, after_states.models)\n        self.assertEqual(len(changes['testapp']), 1)\n        migration = changes['testapp'][0]\n        self.assertEqual(migration.operations[0].__class__.__name__, 'AlterOrderWithRespectTo')\n        self.assertEqual(migration.operations[0].name, 'Author')\n        self.assertEqual(migration.operations[0].order_with_respect_to, 'book')\n        self.assertNumberMigrations(changes, 'testapp', 1, 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterOrderWithRespectTo'])\n", "    def test_rename_model(self):\n        new_author = ModelState(\"testapp\", \"AuthorRenamed\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        changes = {\n            \"testapp\": [\n                RenameModel(\n                    \"Author\",\n                    \"AuthorRenamed\",\n                ),\n            ]\n        }\n        project_state = self.make_project_state([self.author])\n        after_state = self.make_project_state([new_author])\n        autodetector = MigrationAutodetector(project_state, after_state)\n        self.assertEqual(autodetector._detect_changes(), changes)\n", "    def test_model_autodetector_reduces_operations_together(self):\n        # Create a migration that creates a model with two fields that are together.\n        before_states = self.make_project_state([\n            self.author_name,\n        ])\n        after_states = self.make_project_state([\n            self.book_foo_together,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes['app']), 1)\n        migration = changes['app'][0]\n        self.assertEqual(migration.name, '0002_create_book')\n        self.assertEqual(len(migration.operations), 1)\n        operation = migration.operations[0]\n        self.assertIsInstance(operation, CreateModel)\n        self.assertEqual(operation.name, 'book')\n        self.assertEqual(operation.options, {\n            'index_together': {(\"author\", \"title\")},\n            'unique_together': {(\"author\", \"title\")},\n        })\n\n        # Now create a migration that removes this operation and adds a new one.\n        before_states = self.make_project_state([\n            self.book_foo_together,\n        ])\n        after_states = self.make_project_state([\n            self.book_foo_together_4,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes['app']), 1)\n        migration = changes['app'][0]\n        self.assertEqual(migration.name, '0003_create_book')\n        self.assertEqual(len(migration.operations), 1)\n        operation = migration.operations[0]\n        self.assertIsInstance(operation, CreateModel)\n        self.assertEqual(operation.name, 'book')\n        self.assertEqual(operation.options, {\n            'index_together': {(\"title\", \"newfield2\")},\n            'unique_together': {(\"title\", \"newfield2\")},\n        })\n\n        # Check that the operations are reduced correctly.\n        self.assertEqual(migration.dependencies, [changes['app'][0].name])\n        self.assertEqual(migration.dependencies, [changes['app'][0].name])\n\n        self.assertEqual(len(migration.dependencies), 1)\n        self.assertEqual(migration.dependencies[0], '0002_create_book')\n", "def test_index_operation_options(self):\n    \"\"\"Test that IndexOperation operations can be reduced.\"\"\"\n    loader = MigrationLoader()\n    project_state = ProjectState()\n    project_state.add_model(self.author_foo_indexes.clone())\n    project_state.add_model(self.author_foo_indexes_2.clone())\n    project_state.add_model(self.author_foo_indexes_3.clone())\n    project_state.add_model(self.book_indexes.clone())\n    project_state.add_model(self.book_indexes_2.clone())\n\n    changes = loader.detect_changes(\n        project_state,\n        MigrationGraph(),\n        MigrationQuestioner(),\n        only_dependencies=False,\n    )\n\n    self.assertNumberMigrations(changes, \"testapp\", 4)\n    self.assertNumberMigrations(changes, \"otherapp\", 2)\n\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n    self.assertOperationTypes(changes, \"testapp\", 1, [\"AlterModelOptions\"])\n    self.assertOperationTypes(changes, \"testapp\", 2, [\"AddIndex\"])\n    self.assertOperationTypes(changes, \"testapp\", 3, [\"CreateModel\"])\n    self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateModel\"])\n    self.assertOperationTypes(changes, \"otherapp\", 1, [\"AlterModelOptions\"])\n\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name='foo_indexes', options={'indexes': [{'name': 'foo_indexes', 'fields': ['field1', 'field2']}]})\n    self.assertOperationAttributes(changes, \"testapp\", 1, 0, name='foo_indexes_2', options={'indexes': [{'name': 'foo_indexes_2', 'fields': ['field1', 'field2']}]})\n    self.assertOperationAttributes(changes, \"testapp\", 2, 0, name='foo_indexes_3', options={'indexes': [{'name': 'foo_indexes_3', 'fields': ['field1', 'field2']}]})\n    self.assertOperationAttributes(changes, \"testapp\", 3, 0, name='foo_indexes_4', options={'indexes': [{'name': 'foo_indexes_4', 'fields': ['field1', 'field2']}]})\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='indexes', options={'indexes': [{'name': 'book_title_author_idx', 'fields': ['author', 'title']}]})\n    self.assertOperationAttributes(changes, \"otherapp\", 1"], "sample_637": ["    def test_by_id_managed_messages_enabled(self):\n        \"\"\"Check that ByIdManagedMessagesChecker reports messages enabled by id.\"\"\"\n        input_code = _tokenize_str('''\n            # pylint: enable=E1101\n        ''')\n        self._run_test(['use-symbolic-message-instead'], input_code)\n", "    def test_id_managed_msgs_enabled_by_id(self):\n        \"\"\"Test if id-managed msgs are enabled by id.\"\"\"\n        # Setup\n        self._load_source(\"\"\"\n            # pylint: enable=id001\n        \"\"\")\n        self._include_message(\"use-symbolic-message-instead\", id=\"id001\")\n\n        # Run the test\n        self.checker.process_module(self._get_module())\n\n        # Verify the result\n        self.assertMessageMissing(\"use-symbolic-message-instead\")\n", "    def test_enabled_by_id(self):\n        \"\"\"Check that a message is not raised when a message is enabled by id.\"\"\"\n        # Arrange\n        self.set_config(option=\"disable=use-symbolic-message-instead\")\n        self.add_message(\"I0023\")\n\n        # Act\n        self._run_test_case()\n\n        # Assert\n        self._assert_no_messages()\n", "    def test_empty_by_id_managed_msgs(self):\n        self._test_empty_linter_state(\n            \"\"\"\"\"\"\n        )\n", "    def test_use_symbolic_message_instead(self):\n        # Test with no messages managed by id\n        self._run_test(use_symbolic_message_instead=0, expected_output=0)\n        # Test with one message managed by id\n        self._run_test(use_symbolic_message_instead=1, expected_output=1)\n        # Test with two messages managed by id\n        self._run_test(use_symbolic_message_instead=2, expected_output=2)\n        # Test with disabled messages\n        self._run_test(use_symbolic_message_instead=3, expected_output=1)\n"], "sample_708": ["def test_deindent_preserves_empty_lines():\n    source = Source([\"\\n\", \"\", \"  foo = 5\"])\n    deindented = source.deindent()\n    assert deindented.lines == [\"foo = 5\"]\n", "def test_getstatementrange_ast_edge_cases() -> None:\n    # Test case for lineno out of range\n    source = Source(\"def foo():\\n    pass\")\n    with pytest.raises(IndexError):\n        source.getstatementrange(lineno=10)\n\n    # Test case for empty source\n    source = Source()\n    with pytest.raises(IndexError):\n        source.getstatementrange(lineno=0)\n\n    # Test case for no statements in source\n    source = Source(\"pass\")\n    with pytest.raises(IndexError):\n        source.getstatementrange(lineno=1)\n\n    # Test case for empty statement at the end of source\n    source = Source(\"def foo():\\n    pass\\n\")\n    start, end = source.getstatementrange(lineno=1)\n    assert start == 0\n    assert end == 3\n\n    # Test case for empty statement at the start of source\n    source = Source(\"\\ndef foo():\\n    pass\")\n    start, end = source.getstatementrange(lineno=0)\n    assert start == 0\n    assert end == 3\n\n    # Test case for multiple statements\n    source = Source(\"def foo():\\n    pass\\n    bar():\\n    baz():\")\n    start, end = source.getstatementrange(lineno=1)\n    assert start == 0\n    assert end == 4\n\n    # Test case for last statement\n    source = Source(\"def foo():\\n    pass\")\n    start, end = source.getstatementrange(lineno=1)\n    assert start == 0\n    assert end == 2\n\n    # Test case for first statement\n    source = Source(\"def foo():\\n    pass\")\n    start, end = source.getstatementrange(lineno=0)\n    assert start == 0\n    assert end == 2\n\n    # Test case for multiline statement\n    source = Source(\"def foo():\\n    if True:\\n        pass\")\n    start, end = source.getstatementrange(lineno=1)\n    assert start == 0\n    assert end == 3\n", "def test_Source_class():\n    # Test that Source is an immutable object\n    source = Source(\"def test():\\n    pass\")\n    with pytest.raises(AttributeError):\n        source.lines.append(\"new line\")\n\n    # Test that Source.__getitem__ returns a single line as a string\n    source = Source(\"def test():\\n    pass\")\n    line = source[0]\n    assert isinstance(line, str)\n    assert line == \"def test():\"\n\n    # Test that Source.__getitem__ returns a new Source object for a slice\n    source = Source(\"def test():\\n    pass\")\n    new_source = source[:1]\n    assert isinstance(new_source, Source)\n    assert new_source[0] == \"def test():\"\n\n    # Test that Source.__len__ returns the number of lines in the source\n    source = Source(\"def test():\\n    pass\")\n    assert len(source) == 2\n\n    # Test that Source.__iter__ returns an iterator over the lines in the source\n    source = Source(\"def test():\\n    pass\")\n    it = iter(source)\n    assert next(it) == \"def test():\"\n    assert next(it) == \"    pass\"\n\n    # Test that Source.strip removes leading and trailing blank lines\n    source = Source(\"\\n\\ndef test():\\n    pass\\n\\n\")\n    stripped_source = source.strip()\n    assert stripped_source[0] == \"def test():\"\n    assert stripped_source[-1] == \"    pass\"\n\n    # Test that Source.indent adds indentation to the source\n    source = Source(\"def test():\\n    pass\")\n    indented_source = source.indent()\n    assert indented_source[0] == \"    def test():\"\n    assert indented_source[-1] == \"        pass\"\n\n    # Test that Source.getstatement returns the statement containing the given line number\n    source = Source(\"def test():\\n    pass\")\n    statement = source.getstatement(1)\n    assert statement[0] == \"def test():\"\n\n    # Test that Source.getstatementrange returns the range of statements containing the given line number\n    source = Source(\"def test():\\n    if True:\\n        pass\")\n    statement_range = source.getstatementrange(2)\n    assert statement_range == (1, 3)\n\n    # Test that Source.deindent removes leading indentation from the source\n    source = Source(\"    def", "def test_deindent_iterable_with_empty_lines():\n    source = Source([\"  def foo():\\n\", \"    pass\\n\", \"\"])\n    expected = Source([\"def foo():\\n\", \"    pass\"])\n    assert source.deindent().lines == expected.lines\n", "def test_getstatementrange_ast_assertion_true(tmp_path: Path) -> None:\n    \"\"\"Test that getstatementrange_ast assertion does not raise when assertion is True.\"\"\"\n    content = \"\"\""], "sample_1192": ["def test_Str_init():\n    # Test that Str instances are properly created with a string name\n    s = Str('foo')\n    assert s.name == 'foo'\n\n    # Test that a TypeError is raised when trying to create a Str instance with a non-string name\n    with raises(TypeError):\n        Str(42)\n", "def test_uniquely_named_symbol():\n    # Test that the name is correctly incremented\n    x = uniquely_named_symbol('x')\n    assert x == Symbol('x')\n    assert uniquely_named_symbol('x') == Symbol('x0')\n\n    # Test that the name is correctly incremented even if the base name is already used\n    x = uniquely_named_symbol('x', (Symbol('x'),))\n    assert x == Symbol('x1')\n\n    # Test that the name is correctly incremented if the base name is already used multiple times\n    x = uniquely_named_symbol('x', (Symbol('x'), Symbol('x1'),))\n    assert x == Symbol('x2')\n\n    # Test that the name is correctly incremented if the base name is a string\n    x = uniquely_named_symbol('x', (Symbol('y'),))\n    assert x == Symbol('x')\n\n    # Test that the name is correctly incremented if the base name is not a string\n    with raises(TypeError):\n        uniquely_named_symbol(123, (Symbol('x'),))\n\n    # Test that the name is correctly incremented if the compare function is not the default\n    x = uniquely_named_symbol('x', (Symbol('x'),), compare=lambda s: s.lower())\n    assert x == Symbol('x')\n\n    # Test that the name is correctly incremented if the modify function is not the default\n    x = uniquely_named_symbol('x', (Symbol('x'),), modify=lambda s: s + '_mod')\n    assert x == Symbol('x_mod')\n\n    # Test that the name is correctly incremented if the assumptions are provided\n    x = uniquely_named_symbol('x', (Symbol('x'),), assumptions={'commutative': False})\n    assert x == Symbol('x', commutative=False)\n\n    # Test that the name is correctly incremented if the default symbol is provided\n    x = uniquely_named_symbol('x', (Symbol('x'),), default=Symbol('y'))\n    assert x == Symbol('x')\n\n    # Test that the name is correctly incremented if the base name is a Symbol with assumptions\n    x = uniquely_named_symbol('x', (Symbol('x', commutative=True),))\n    assert x == Symbol('x')\n\n    # Test that the name is correctly incremented if the base name is a Symbol with non-unique name\n    x = uniquely_named_symbol('x', (Symbol('y'),))\n    assert x == Symbol('x')\n\n   ", "def test_str():\n    s = Str(\"hello\")\n    assert s.name == \"hello\"\n    assert s.is_Symbol is False\n    assert s.is_symbol is False\n", "    def test__sanitize(self):\n        # Test that Symbol._sanitize is strict about commutativity\n        with raises(ValueError, match=\"commutativity must be True or False\"):\n            Symbol('x', commutative=None)\n        with raises(ValueError, match=\"commutativity must be True or False\"):\n            Symbol('x', commutative=42)\n\n        # Test that Symbol._sanitize correctly sanitizes other assumptions\n        symbol = Symbol('x', integer=True, commutative=False)\n        assert symbol.assumptions0['integer']\n        assert not symbol.assumptions0['commutative']\n\n        # Test that Symbol._sanitize correctly removes None values\n        symbol = Symbol('x', integer=None, commutative=True)\n        assert not symbol.assumptions0\n", "def test_symbols():\n    # Test that _range correctly splits on multiple ranges\n    assert symbols('a:1 b:2').as_ordered_tuple() == (a_0, b_0, b_1)\n    assert symbols('a:1 (b:2 c:3)').as_ordered_tuple() == (a_0, b_0, c_0, c_1, c_2)\n    assert symbols('a:1 :2').as_ordered_tuple() == (a_0, a_1, b_0, b_1)\n\n    # Test that _range correctly handles empty range\n    assert symbols('a:c').as_ordered_tuple() == ()\n    assert symbols(':c').as_ordered_tuple() == (a_0, b_0, c_0)\n\n    # Test that _range correctly handles escaped characters\n    assert symbols('a\\\\,b').as_ordered_tuple() == (a_0, b_0)\n    assert symbols('a\\\\:\\\\,b').as_ordered_tuple() == (a_0, a_0, b_0)\n\n    # Test that symbols correctly handles non-string arguments\n    assert symbols('a,b', cls=Function) == (f_a, f_b)\n    assert isinstance(symbols('a,b', cls=Function)[0], Function)\n    assert symbols(symbols('a,b')) == (Symbol('a'), Symbol('b'))\n\n    # Test that var correctly injects symbols into the global namespace\n    x, y = symbols('x,y')\n    assert x == x\n    assert y == y\n\n    # Test that var correctly handles a single symbol\n    x = symbols('x')\n    assert x == x\n\n    # Test that symbols with different classes are correctly handled\n    x, y = symbols('x,y', cls=Function)\n    assert isinstance(x, Function)\n    assert isinstance(y, Function)\n\n    # Test that symbols with excluded ranges are correctly handled\n    assert symbols('a:10, b:5', exclude=[1, 3]).as_ordered_tuple() == (a_2, b_0, b_4)\n\n    # Test that symbols with custom properties are correctly handled\n    assert symbols('a:10', properties=[lambda k: k.is_integer]).as_ordered_tuple() == (a_0, a_1, a_2, a_3, a_4, a_5, a"], "sample_944": ["def test_restify_on_newtype():\n    my_int = MyInt\n    expected = ':class:`MyInt`'\n    assert restify(my_int) == expected\n", "def test_restify_TypeVar():\n    T = TypeVar('T')\n    assert restify(T) == ':obj:`T`'\n", "def test_restify_Union_with_ellipsis():\n    class MyUnion(Union[int, ...]):\n        pass\n    assert restify(MyUnion) == ':obj:`...`'\n", "def test_restify_union_with_literal_type():\n    \"\"\"Test restify() for Union with Literal type.\"\"\"\n    from typing import Literal\n\n    union_type = Union[int, Literal[1, 2, 3]]\n    assert restify(union_type) == ':obj:`Union`\\\\ [[int], 1 | 2 | 3]'\n", "def test_restify_newstyle_class():\n    class MyNewClass:\n        __module__ = 'mymodule'\n        __qualname__ = 'mymodule.MyNewClass'\n\n    assert restify(MyNewClass) == ':class:`mymodule.MyNewClass`'\n"], "sample_662": ["    def test_json_deserialization_failure(self):\n        reportdict = {\"$report_type\": \"UnknownReportType\"}\n        with pytest.raises(RuntimeError) as excinfo:\n            pytest_report_from_serializable(reportdict)\n        assert \"Unknown report_type unserialize data: UnknownReportType\" in str(excinfo.value)\n", "    def test_from_serializable_with_test_report(self):\n        test_report = TestReport(\n            nodeid=\"test_example\",\n            location=(Path(\"test_file.py\"), 10, \"domain\"),\n            keywords={\"keyword1\": \"value1\"},\n            outcome=\"passed\",\n            longrepr=\"Long repr\",\n            when=\"call\",\n        )\n        serialized_report = pytest_report_to_serializable(test_report)\n        deserialized_report = pytest_report_from_serializable(serialized_report)\n        assert isinstance(deserialized_report, TestReport)\n        assert deserialized_report.nodeid == \"test_example\"\n        assert deserialized_report.location == (Path(\"test_file.py\"), 10, \"domain\")\n        assert deserialized_report.keywords == {\"keyword1\": \"value1\"}\n        assert deserialized_report.outcome == \"passed\"\n        assert deserialized_report.when == \"call\"\n        assert deserialized_report.longrepr == \"Long repr\"\n", "    def test_pytest_report_to_serializable_test_report(self):\n        test_report = TestReport(\n            nodeid=\"test_node\",\n            location=(\"path/to/file\", 1, \"domain\"),\n            keywords={\"keyword1\": True, \"keyword2\": False},\n            outcome=\"passed\",\n            longrepr=\"longrepr\",\n            when=\"setup\",\n            sections=[(\"Captured stdout\", \"stdout content\")],\n            duration=1.0,\n            user_properties=[(\"prop1\", \"value1\")],\n        )\n        result = pytest_report_to_serializable(test_report)\n        expected_result = {\n            \"$report_type\": \"TestReport\",\n            \"location\": (\"path/to/file\", 1, \"domain\"),\n            \"keywords\": {\"keyword1\": True, \"keyword2\": False},\n            \"outcome\": \"passed\",\n            \"longrepr\": \"longrepr\",\n            \"when\": \"setup\",\n            \"sections\": [(\"Captured stdout\", \"stdout content\")],\n            \"duration\": 1.0,\n            \"user_properties\": [(\"prop1\", \"value1\")],\n        }\n        assert result == expected_result\n", "    def test_serialize_repr_traceback(self):\n        repr_traceback = ReprTraceback(reprentries=[ReprEntry(lines=[\"line1\", \"line2\"])]\n        serialized_repr_traceback = _report_to_json({\"__dict__\": repr_traceback.__dict__.copy()})\n        deserialized_repr_traceback = deserialize_repr_traceback(serialized_repr_traceback[\"reprtraceback\"])\n        assert deserialized_repr_traceback == repr_traceback\n", "    def test_serialize_repr_traceback(self):\n        from _pytest.code import ReprEntry, ReprEntryNative, ReprLocals, ReprTraceback\n\n        reprentry = ReprEntry(\n            lines=[\"line1\", \"line2\"],\n            reprfuncargs=ReprFuncArgs(name=\"func\", args=[\"arg1\", \"arg2\"]),\n            reprlocals=ReprLocals([\"var1\", \"var2\"]),\n            filelocrepr=ReprFileLocation(\"path/to/file.py\"),\n            style=\"nice\",\n        )\n\n        reprentry_native = ReprEntryNative([\"line1\", \"line2\"])\n\n        repr_traceback = ReprTraceback(reprentries=[reprentry])\n        repr_traceback_native = ReprTraceback(reprentries=[reprentry_native])\n\n        reportdict = {\n            \"longrepr\": {\n                \"reprtraceback\": repr_traceback.__dict__,\n                \"reprcrash\": {\"lines\": [\"crash line1\", \"crash line2\"]},\n            }\n        }\n\n        _report_kwargs_from_json(reportdict)\n\n        assert reportdict[\"longrepr\"][\"reprcrash\"][\"lines\"] == [\"crash line1\", \"crash line2\"]\n"], "sample_913": ["def test_py_field_make_xref():\n    field = PyField('test', 'field')\n    assert field.make_xref('class', 'py', 'None') is None\n    assert field.make_xref('obj', 'py', 'None') is None\n    assert field.make_xref('class', 'py', 'some_class') is None\n    assert field.make_xref('obj', 'py', 'some_obj') is None\n    assert field.make_xref('class', 'py', 'some_class') == nodes.emphasis('some_class')\n    assert field.make_xref('obj', 'py', 'some_obj') == nodes.emphasis('some_obj')\n    assert field.make_xref('class', 'py', 'some_class').get('refspecific') is True\n    assert field.make_xref('obj', 'py', 'some_obj').get('refspecific') is True\n    assert field.make_xref('class', 'py', 'some_class~') == nodes.emphasis('some_class')\n    assert field.make_xref('obj', 'py', 'some_obj~') == nodes.emphasis('some_obj')\n    assert field.make_xref('class', 'py', 'some_class~').get('refspecific') is True\n    assert field.make_xref('obj', 'py', 'some_obj~').get('refspecific') is True\n", "def test_PyModule():\n    app = Mock()\n    domain = PythonDomain()\n    domain.env = app.env\n    domain.env.ref_context['py:module'] = 'test_module'\n    directive = PyModule(options={'module': 'test_module'})\n    node = directive.run()[0]\n    assert isinstance(node, nodes.target)\n    assert node['ids'] == ['test_module']\n", "def test_parse_function_signature():\n    # test function without arguments\n    signode = parse('my_function()')\n    assert signode == 'my_function()'\n\n    # test function with arguments\n    signode = parse('my_function(a, b, c)')\n    assert signode == 'my_function(a, b, c)'\n\n    # test function with positional-only arguments\n    signode = parse('my_function(/a, b)')\n    assert signode == 'my_function(/a, b)'\n\n    # test function with keyword-only arguments\n    signode = parse('my_function(*a)')\n    assert signode == 'my_function(*a)'\n\n    # test function with positional-only and keyword-only arguments\n    signode = parse('my_function(/a, *b)')\n    assert signode == 'my_function(/a, *b)'\n\n    # test function with default argument value\n    signode = parse('my_function(a, b, c=3)')\n    assert signode == 'my_function(a, b, c=3)'\n\n    # test function with default argument value for positional-only argument\n    signode = parse('my_function(/a, b, c=3)')\n    assert signode == 'my_function(/a, b, c=3)'\n\n    # test function with default argument value for keyword-only argument\n    signode = parse('my_function(*a, b=3)')\n    assert signode == 'my_function(*a, b=3)'\n\n    # test function with default argument value for positional-only and keyword-only arguments\n    signode = parse('my_function(/a, *b, c=3)')\n    assert signode == 'my_function(/a, *b, c=3)'\n\n    # test function with return annotation\n    signode = parse('my_function() -> int')\n    assert signode == 'my_function() -> int'\n\n    # test function with return annotation and arguments\n    signode = parse('my_function(a, b) -> int')\n    assert signode == 'my_function(a, b) -> int'\n", "def test_py_domain_resolve_xref():\n    domain = PythonDomain()\n    node = pending_xref('', nodes.Text('obj'),\n                        refdomain='py', reftype='obj', reftarget='typing.List')\n    contnode = nodes.Text('obj')\n    env = Mock(spec=['objects', 'modules'])\n    env.objects = {'typing.List': ('index', 'index', 'class')}\n    builder = Mock()\n    result = domain.resolve_xref(env, 'index', builder, 'obj', node, contnode)\n    assert isinstance(result, nodes.reference)\n    assert result.astext() == 'obj'\n", "def test_parse_annotation():\n    assert parse('int') == '::int'\n    assert parse('int, str') == '::int, ::str'\n    assert parse('int, str = 1') == '::int, ::str = 1'\n    assert parse('int -> str') == '::int -> ::str'\n    assert parse('int, str -> None') == '::int, ::str -> None'\n    assert parse('int, str = 1 -> None') == '::int, ::str = 1 -> None'\n    assert parse('int, str = 1 -> int') == '::int, ::str = 1 -> ::int'\n    assert parse('int, str = 1 -> str') == '::int, ::str = 1 -> ::str'\n    assert parse('int, str = 1 -> None') == '::int, ::str = 1 -> None'\n    assert parse('int -> str, str -> int') == '::int -> ::str, ::str -> ::int'\n    assert parse('int, str = 1 -> int, str = 2') == '::int, ::str = 1 -> ::int, ::str = 2'\n    assert parse('int, str') == '::int, ::str'\n    assert parse('int, str -> int') == '::int, ::str -> ::int'\n    assert parse('int, str -> None') == '::int, ::str -> None'\n    assert parse('int, str = 1 -> int') == '::int, ::str = 1 -> ::int'\n    assert parse('int, str = 1 -> None') == '::int, ::str = 1 -> None'\n    assert parse('int, str, int') == '::int, ::str, ::int'\n    assert parse('int, str -> int, int -> str') == '::int, ::str -> ::int, ::int -> ::str'\n    assert parse('int, str -> int, str -> int') == '::int, ::str -> ::int, ::str -> ::int'\n    assert parse('int, str -> int, str -> None') == '::int, ::str -> ::int, ::str -> None'\n    assert parse('int, str = 1 -> int, str = 2 -> None') == '::int, ::str = 1 -> ::int, ::str = 2 -> None'\n    assert parse('int"], "sample_56": ["    def test_check_fields_empty(self):\n        admin_obj = admin.ModelAdmin()\n        admin_obj.fields = None\n        self.assertEqual(len(admin_obj.check()), 1)\n", "    def test_check_dependencies(self):\n        # Test that check_dependencies returns an empty list if all dependencies are met\n        self.assertEqual(check_dependencies(), [])\n\n        # Test that check_dependencies returns an error when django.contrib.admin is not installed\n        with self.assertRaises(checks.Error):\n            check_dependencies(apps.get_app_config('nonexistent_app'))\n\n        # Test that check_dependencies returns an error when django.contrib.auth is not installed\n        with self.assertRaises(checks.Error):\n            check_dependencies(apps.get_app_config('auth'))\n\n        # Test that check_dependencies returns an error when django.contrib.messages is not installed\n        with self.assertRaises(checks.Error):\n            check_dependencies(apps.get_app_config('messages'))\n\n        # Test that check_dependencies returns an error when django.contrib.sessions is not installed\n        with self.assertRaises(checks.Error):\n            check_dependencies(apps.get_app_config('sessions'))\n\n        # Test that check_dependencies returns an error when DjangoTemplates is not configured in TEMPLATES\n        with self.assertRaises(checks.Error):\n            with override_settings(TEMPLATES=[]):\n                check_dependencies()\n\n        # Test that check_dependencies returns an error when django.contrib.auth.context_processors.auth is not enabled\n        with self.assertRaises(checks.Error):\n            with override_settings(TEMPLATES=[\n                {\n                    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                    'context_processors': []\n                }\n            ]):\n                check_dependencies()\n\n        # Test that check_dependencies returns an error when django.contrib.messages.context_processors.messages is not enabled\n        with self.assertRaises(checks.Error):\n            with override_settings(TEMPLATES=[\n                {\n                    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                    'context_processors': ['django.contrib.auth.context_processors.auth']\n                }\n            ]):\n                check_dependencies()\n\n        # Test that check_dependencies returns an error when django.contrib.auth.middleware.AuthenticationMiddleware is not installed\n        with self.assertRaises(checks.Error):\n            with override_settings(MIDDLEWARE=[]):\n                check_dependencies()\n\n        # Test that check_dependencies returns an error when django.contrib.messages.middleware.MessageMiddleware is not installed\n        with self.assertRaises(checks.Error):\n            with override", "    def test_inline_model_admin_checks(self):\n        class ValidInline(admin.StackedInline):\n            model = Album\n            extra = 3\n            fk_name = 'author'\n\n        class ValidExtraInline(admin.StackedInline):\n            model = Album\n            extra = 3\n            fk_name = 'author'\n            max_num = 5\n\n        class ValidMinNumInline(admin.StackedInline):\n            model = Album\n            extra = 3\n            fk_name = 'author'\n            min_num = 2\n\n        class ValidInlineModelAdminChecks(admin.ModelAdmin):\n            inlines = [ValidInline]\n\n        class ValidInlineModelAdminChecksWithExtra(admin.ModelAdmin):\n            inlines = [ValidInlineModelAdminChecks]\n            formset = forms.models.BaseInlineFormSet\n\n        class ValidInlineModelAdminChecksWithExtraAndMaxNum(admin.ModelAdmin):\n            inlines = [ValidInlineModelAdminChecksWithExtra]\n            formset = forms.models.BaseInlineFormSet\n\n        class ValidInlineModelAdminChecksWithExtraAndMinNum(admin.ModelAdmin):\n            inlines = [ValidMinNumInline]\n            formset = forms.models.BaseInlineFormSet\n\n        class ValidInlineModelAdminChecksWithMaxNum(admin.ModelAdmin):\n            inlines = [ValidExtraInline]\n            formset = forms.models.BaseInlineFormSet\n\n        class ValidInlineModelAdminChecksWithMinNum(admin.ModelAdmin):\n            inlines = [ValidMinNumInline]\n            formset = forms.models.BaseInlineFormSet\n\n        checks_instance = InlineModelAdminChecks()\n        self.assertEqual(checks_instance.check(ValidInline), [])\n        self.assertEqual(checks_instance.check(ValidExtraInline), [])\n        self.assertEqual(checks_instance.check(ValidMinNumInline), [])\n        self.assertEqual(checks_instance.check(ValidInlineModelAdminChecks), [])\n        self.assertEqual(checks_instance.check(ValidInlineModelAdminChecksWithExtra), [])\n        self.assertEqual(checks_instance.check(ValidInlineModelAdminChecksWithExtraAndMaxNum), [])\n        self.assertEqual(checks_instance.check(ValidInlineModelAdminChecksWithExtraAndMinNum), [])\n        self.assertEqual(checks_instance.check(ValidInlineModel", "    def test_fields_not_in_model(self):\n        class TestModelAdmin(admin.ModelAdmin):\n            model = models.Model\n            fields = ['non_existent_field']\n\n        self.assertEqual(len(ModelAdminChecks(TestModelAdmin()).check(TestModelAdmin)), 1)\n", "    def test_valid_model_admin(self):\n        admin = admin.site.register(Album)\n        self.assertEqual(len(admin.check()), 0)\n"], "sample_729": ["def test_enet_path_1d():\n    # Create a simple 1D path\n    X = np.array([1, 2, 3])\n    y = np.array([1, 2, 3])\n\n    # Check that the path is 1D\n    assert lasso_path(X, y).shape == (100, X.shape[1])\n\n    # Check that the path is correctly interpolated\n    alphas, coefs, _ = lasso_path(X, y)\n    f = interpolate.interp1d(alphas[::-1], coefs, axis=0)\n    coefs_interp = f(alphas[::-1])\n    assert_array_almost_equal(coefs, coefs_interp)\n", "def test_enet_path_parallel():\n    # Test enet_path with n_jobs > 1\n    X, y = load_boston(return_X_y=True)\n    X = np.array(X, dtype=np.float32)\n    y = np.array(y, dtype=np.float32)\n    model = ElasticNetCV(l1_ratio=0.5, cv=5, n_jobs=2)\n    model.fit(X, y)\n    # Check if the path has been computed correctly\n    assert_array_almost_equal(model.mse_path_, model._mse_path)\n\n    # Test enet_path with n_jobs > 1 and fit_intercept=False\n    model = ElasticNetCV(l1_ratio=0.5, cv=5, fit_intercept=False, n_jobs=2)\n    model.fit(X, y)\n    # Check if the path has been computed correctly\n    assert_array_almost_equal(model.mse_path_, model._mse_path)\n\n    # Test enet_path with n_jobs > 1 and n_l1_ratio > 1\n    model = ElasticNetCV(l1_ratio=[0.5, 0.7, 0.9], cv=5, n_jobs=2)\n    model.fit(X, y)\n    # Check if the path has been computed correctly\n    assert_array_almost_equal(model.mse_path_, model._mse_path)\n\n    # Test enet_path with n_jobs = 1 and n_l1_ratio > 1\n    model = ElasticNetCV(l1_ratio=[0.5, 0.7, 0.9], cv=5, n_jobs=1)\n    model.fit(X, y)\n    # Check if the path has been computed correctly\n    assert_array_almost_equal(model.mse_path_, model._mse_path)\n\n    # Test enet_path with n_jobs > 1 and fit_intercept=False and n_l1_ratio > 1\n    model = ElasticNetCV(l1_ratio=[0.5, 0.7, 0.9], cv=5, fit_intercept=False,\n                         n_jobs=2)\n    model.fit(X, y)\n    # Check if the path has been computed correctly\n    assert_array_almost_equal(model.mse_path_, model._mse_path)\n", "def test_enet_path_with_l1_ratio_1():\n    # Test that the path function works correctly when l1_ratio=1\n    # This is the same as the Lasso model\n    X, y = load_boston(return_X_y=True)\n    model = ElasticNet(l1_ratio=1.0)\n    model.fit(X, y)\n    path = model.path\n    alphas, coefs, _ = path(X, y, l1_ratio=1.0, return_n_iter=False)\n\n    # Check that the coefficients are the same as the Lasso model\n    lasso_model = Lasso()\n    lasso_model.fit(X, y)\n    lasso_coefs = lasso_model.coef_\n    assert_array_almost_equal(coefs, lasso_coefs, decimal=5)\n", "def test_lasso_lassocv_input_types():\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    y = np.array([0, 1, 2])\n    assert_raises_regex(ValueError, \"X should be dense but a sparse matrix was passed\",\n                      LassoCV().fit, sparse.csr_matrix(X), y)\n    assert_raises_regex(ValueError, \"For mono-task outputs, use Lasso\",\n                      LassoCV().fit, X, y[:, np.newaxis])\n    assert_raises_regex(ValueError, \"For mono-task outputs, use Lasso\",\n                      Lasso().fit, X, y[:, np.newaxis])\n    assert_raises_regex(ValueError, \"For mono-task outputs, use Lasso\",\n                      LassoCV().fit, X, y[np.newaxis, :])\n    assert_raises_regex(ValueError, \"For mono-task outputs, use Lasso\",\n                      Lasso().fit, X, y[np.newaxis, :])\n    X_sparse = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    assert_raises_regex(ValueError, \"X and y have inconsistent dimensions\",\n                      LassoCV().fit, X_sparse, y)\n    # Test with proper input\n    lasso = LassoCV().fit(X, y)\n    lasso_cv = LassoCV().fit(X, y[:, np.newaxis])\n    assert_array_almost_equal(lasso.coef_, lasso_cv.coef_.T)\n    assert_array_almost_equal(lasso.intercept_, lasso_cv.intercept_)\n", "    def test_elasticnetcv_path(self):\n        X, y = load_boston(return_X_y=True)\n        model = ElasticNetCV(cv=3, n_jobs=1)\n        model.fit(X, y)\n        assert_equal(model.alpha_, model.elastic_net_path(X, y)[0])\n"], "sample_607": ["def test_guess_engine_installed_backends_single_match(tmp_path):\n    # Setup\n    BACKEND_ENTRYPOINTS = {\n        \"store\": common.BackendEntrypoint,\n    }\n    BACKEND_ENTRYPOINTS[\"store\"].available = True\n    with tmp_path / \"file.nc\" as f:\n        f.write_text(\"\")\n\n    # Test\n    with pytest.warns(UserWarning) as record:\n        guess_engine(str(tmp_path / \"file.nc\"))\n\n    # Assert\n    assert len(record.list) == 1\n    assert record.list[0].message.args[0].startswith(\"found the following matches\")\n", "def test_sort_backends():\n    specs = [\n        \"engine1 = xarray.tests.test_plugins:backend_2\",\n        \"engine2 = xarray.tests.test_plugins:backend_1\",\n        \"engine3 = xarray.tests.test_plugins:backend_3\",\n    ]\n    eps = [pkg_resources.EntryPoint.parse(spec) for spec in specs]\n    expected_sorted_backends = {\n        \"engine1\": \"backend_2\",\n        \"engine2\": \"backend_1\",\n        \"engine3\": \"backend_3\",\n    }\n    sorted_backends = sort_backends(backends_dict_from_pkg(eps))\n    assert sorted_backends == expected_sorted_backends\n", "def test_detect_parameters():\n    class DummyBackendEntrypointArgsKWAndPositional(common.BackendEntrypoint):\n            pass\n\n    backend = DummyBackendEntrypointArgsKWAndPositional()\n    with pytest.raises(TypeError):\n        detect_parameters(backend.open_dataset)\n\n", "def test_build_engines_pkg_resources_iter_entry_points():\n    class DummyBackend(common.BackendEntrypoint):\n            pass\n\n    specs = [\"engine1 = xarray.tests.test_plugins:backend_1\"]\n    eps = [pkg_resources.EntryPoint.parse(spec) for spec in specs]\n    with mock.patch(\n        \"pkg_resources.iter_entry_points\",\n        return_value=[eps[0]],\n    ):\n        engines = list_engines()\n        assert len(engines) == 1\n        assert \"engine1\" in engines\n", "def test_detect_parameters(backend_cls, expected):\n    open_dataset = backend_cls.open_dataset\n    parameters = detect_parameters(open_dataset)\n    assert parameters == expected\n\n"], "sample_910": ["def test_pending_warnings():\n    # Test pending_warnings context manager\n    with pytest.raises(SphinxWarning):\n        with pending_warnings() as memhandler:\n            logging.getLogger('sphinx').warning('Test warning')\n\n    # Check that the warning is not logged until the context manager is exited\n    with pending_warnings() as memhandler:\n        logging.getLogger('sphinx').warning('Test warning')\n    assert memhandler.buffer == []\n\n    # Test pending_warnings with multiple warnings\n    with pending_warnings() as memhandler:\n        logging.getLogger('sphinx').warning('Test warning 1')\n        logging.getLogger('sphinx').warning('Test warning 2')\n    assert len(memhandler.buffer) == 2\n\n    # Test pending_warnings with a custom prefix\n    with prefixed_warnings('Prefix: ') as _:\n        with pending_warnings() as memhandler:\n            logging.getLogger('sphinx').warning('Test warning 1')\n            logging.getLogger('sphinx').warning('Test warning 2')\n    assert len(memhandler.buffer) == 2\n    for record in memhandler.buffer:\n        assert record.msg.startswith('Prefix: Test warning 1')\n        assert record.msg.startswith('Prefix: Test warning 2')\n\n    # Test pending_warnings with a custom prefix and multiple warnings\n    with prefixed_warnings('Prefix: ') as _:\n        with pending_warnings() as memhandler:\n            logging.getLogger('sphinx').warning('Test warning 1')\n            logging.getLogger('sphinx').warning('Test warning 2')\n    assert len(memhandler.buffer) == 2\n    for record in memhandler.buffer:\n        assert record.msg.startswith('Prefix: Test warning 1')\n        assert record.msg.startswith('Prefix: Test warning 2')\n", "def test_WarningIsErrorFilter(tmpdir):\n    from sphinx.application import Sphinx\n\n    app = Sphinx(__name__, srcdir=tmpdir.strpath, outdir=tmpdir.strpath)\n\n    with app.initialize_statuslog():\n        app.warningiserror = True\n\n        with prefixed_warnings(\"prefix:\"):\n            logger = logging.getLogger(\"test_logger\")\n            logger.warning(\"warning message\")\n\n        assert app._warncount == 1\n\n        try:\n            logger.warning(\"warning message\")\n        except SphinxWarning as e:\n            assert str(e) == \"prefix: warning message\"\n\n        app.warningiserror = False\n        logger.warning(\"warning message\")\n\n        assert app._warncount == 2\n", "def test_warning_is_suppressed():\n    with pytest.raises(SystemExit):\n        with prefixed_warnings(\"prefix:\"):\n            logging.getLogger(\"sphinx\").warning(\"test warning message\")\n", "def test_warning_is_suppressed():\n    # Given\n    logger = logging.getLogger('sphinx.util.logging')\n    suppress_warnings = ['foo.warning', 'bar.warning']\n    logger.warning('foo warning')\n    logger.warning('bar warning')\n    logger.warning('baz warning')\n\n    # When\n        with prefixed_warnings('prefix:'):\n            logger.warning('foo warning')\n            logger.warning('bar warning')\n            logger.warning('baz warning')\n\n    # Then\n    with pytest.raises(SphinxWarning):\n        test_function()\n\n    # And\n    logger = logging.getLogger('sphinx.util.logging')\n    with pytest.raises(SphinxWarning):\n        logger.warning('foo warning')\n    logger.warning('baz warning')\n", "def test_color_map(capfd):\n    logger = logging.getLogger('test')\n    with prefixed_warnings('Prefix:'):\n        logger.warning('Test warning')\n        logger.info('Test info')\n\n    captured = capfd.readouterr()\n    assert 'Prefix: Test warning' in captured.out\n    assert not captured.err\n    assert 'Prefix: Test info' in captured.out\n    assert not captured.err\n"], "sample_891": ["def test_ndcg_score():\n    # Test case with k = None\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[.1, .2, .3, 4, 70]])\n    assert_almost_equal(ndcg_score(y_true, y_score), 0.6909, decimal=4)\n\n    # Test case with k = 3\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[.1, .2, .3, 4, 70]])\n    assert_almost_equal(ndcg_score(y_true, y_score, k=3), 0.4923, decimal=4)\n\n    # Test case with k = 1\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[1, 0, 0, 0, 1]])\n    assert_almost_equal(ndcg_score(y_true, y_score, k=1), 0.75, decimal=2)\n\n    # Test case with ignore_ties=True\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[1, 0, 0, 0, 1]])\n    assert_almost_equal(ndcg_score(y_true, y_score, k=1, ignore_ties=True), 0.5, decimal=2)\n\n    # Test case with sample_weight\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[.1, .2, .3, 4, 70]])\n    sample_weight = np.array([1, 2, 3, 4, 5])\n    assert_almost_equal(ndcg_score(y_true, y_score, sample_weight=sample_weight), 0.6909, decimal=4)\n\n    # Test case with ignore_ties and sample_weight\n    y_true = np.array([[10, 0, 0, 1, 5]])\n    y_score = np.array([[1, 0, 0, 0, 1]])\n    sample_weight = np.array([1, 2, 3, 4, 5])\n    assert_almost_equal(nd", "def test_coverage_error():\n    # create some data to test coverage error\n    y_true = np.array([[1, 1, 0], [0, 1, 0]])\n    y_score = np.array([[0.8, 0.9, 0.1], [0.2, 0.7, 0.3]])\n    sample_weight = np.array([1.0, 0.5])\n\n    # test coverage error with all weights\n    assert coverage_error(y_true, y_score, sample_weight=sample_weight) > 0\n\n    # test coverage error with no weights\n    assert coverage_error(y_true, y_score) > 0\n\n    # test coverage error with no positive labels\n    y_true = np.array([[0, 0, 0], [0, 0, 0]])\n    assert coverage_error(y_true, y_score) == 0\n\n    # test coverage error with no negative labels\n    y_true = np.array([[1, 1, 1], [1, 1, 1]])\n    assert coverage_error(y_true, y_score) == 0\n", "def test_label_ranking_loss():\n    # test with a sample that returns a perfect ranking\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    loss = label_ranking_loss(y_true, y_score)\n    assert_almost_equal(loss, 0)\n\n    # test with a sample that returns a bad ranking\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n    loss = label_ranking_loss(y_true, y_score)\n    assert_almost_equal(loss, 3)\n\n    # test with a sample that returns a bad ranking and a perfect ranking\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n    loss = label_ranking_loss(y_true, y_score)\n    assert_almost_equal(loss, 2)\n\n    # test with sample weight\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    loss = label_ranking_loss(y_true, y_score, sample_weight=[0.5, 1, 0.5])\n    assert_almost_equal(loss, 0)\n\n    # test with a sample with all zero weights\n    y_true = np.array([[1, 0, 0], [0, 1, 0], [1, 0, 1]])\n    y_score = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    loss = label_ranking_loss(y_true, y", "def test_ndcg_score_multiclass_scores():\n    \"\"\"Check that ndcg_score scores are correct for multiclass classification.\"\"\"\n    X, y = datasets.load_iris(return_X_y=True)\n    y_pred = np.argmax(svm.SVC(kernel=\"linear\", probability=True).fit(X, y).predict_proba(X), axis=1)\n\n    # check that ndcg score matches dcg_score\n    for k in [1, 2, 3, 4, 5]:\n        dcg = dcg_score(y, y_pred, k=k)\n        ndcg = ndcg_score(y, y_pred, k=k)\n        assert_allclose(ndcg, dcg, rtol=1e-6)\n\n    # check that ndcg score matches dcg_score when ignoring ties\n    for k in [1, 2, 3, 4, 5]:\n        dcg = dcg_score(y, y_pred, k=k, ignore_ties=True)\n        ndcg = ndcg_score(y, y_pred, k=k, ignore_ties=True)\n        assert_allclose(ndcg, dcg, rtol=1e-6)\n\n    # check that ndcg score is zero when all labels are irrelevant\n    X, y = datasets.load_digits(return_X_y=True)\n    y_pred = np.argmax(svm.SVC(kernel=\"linear\", probability=True).fit(X, y).predict_proba(X), axis=1)\n    dcg = dcg_score(y, y_pred)\n    ndcg = ndcg_score(y, y_pred)\n    assert_allclose(ndcg, 0, rtol=1e-6)\n\n    # check that ndcg score matches dcg_score when sample weights are used\n    for k in [1, 2, 3, 4, 5]:\n        dcg = dcg_score(y, y_pred, k=k, sample_weight=np.ones(len(y)))\n        ndcg = ndcg_score(y, y_pred, k=k, sample_weight=np.ones(len(y)))\n        assert_allclose(ndcg, dcg, rtol=1e-6)\n", "def test_ndcg_score_with_sample_weights():\n    \"\"\"Test with sample weights.\"\"\"\n    X, y, y_pred_proba = make_prediction()\n    # Add sample weights\n    sample_weights = np.random.rand(len(y))\n    # Only 2 classes in this binary classification problem\n    y_type = type_of_target(y, input_name=\"y_true\")\n    if y_type != \"binary\":\n        raise ValueError(\"Expected a binary classification problem\")\n\n    if y_type == \"binary\":\n        # only interested in probabilities of the positive case\n        # XXX: do we really want a special API for the binary case?\n        y_pred_proba = y_pred_proba[:, 1]\n        n_classes = 2\n    else:\n        y_type = type_of_target(y, input_name=\"y_true\")\n        if y_type not in (\"multiclass\", \"multiclass-multioutput\"):\n            raise ValueError(\n                \"Expected a multiclass classification problem, got: %s\"\n                % y_type\n            )\n        n_classes = y_pred_proba.shape[1]\n\n    y_true_encoded = _encode(y, uniques=range(n_classes))\n\n    # DCG should be computed with respect to the encoded labels\n    dcg_with_sample_weight = ndcg_score(\n        y_true_encoded, y_pred_proba, sample_weight=sample_weights, k=3\n    )\n    dcg_without_sample_weight = ndcg_score(\n        y_true_encoded, y_pred_proba, k=3\n    )\n\n    assert_almost_equal(dcg_with_sample_weight, dcg_without_sample_weight)\n\n"], "sample_494": ["    def test_serializer_factory_frozenset(self):\n        serializer = serializer_factory(frozenset([1, 2, 3]))\n        self.assertIsInstance(serializer, FrozensetSerializer)\n        self.assertEqual(serializer.serialize()[0], \"frozenset([1, 2, 3])\")\n", "    def test_datetime_datetime_serializer(self):\n        serializer = DatetimeDatetimeSerializer(datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=zoneinfo.ZoneInfo('UTC')))\n        result = serializer.serialize()\n        self.assertEqual(result, (repr(datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=datetime.timezone.utc)), {\"import datetime\"}))\n", "    def test_serialize_model_manager(self):\n        manager = FoodManager()\n        serializer = ModelManagerSerializer(manager)\n        expected_imports = {\n            \"import django.db.models.manager\",\n            \"import %s\" % manager._meta.model.__module__,\n        }\n        self.assertEqual(\n            serializer.serialize(),\n            (\"FoodManager.as_manager()\", expected_imports),\n        )\n", "    def test_uuid_serializer(self):\n        serializer = serializer_factory(uuid.uuid4())\n        self.assertEqual(serializer.serialize(), (\"uuid.UUID('%s')\" % str(uuid.uuid4()), {\"import uuid\"}))\n", "    def test_serializer_factory_type_and_module(self):\n        value = type('TestType', (), {})\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, TypeSerializer)\n        self.assertEqual(serializer.serialize()[0], 'TestType')\n\n        value = type('TestType', (), {'__module__': 'tests.test_module'})\n        serializer = serializer_factory(value)\n        self.assertIsInstance(serializer, TypeSerializer)\n        self.assertEqual(serializer.serialize()[0], 'tests.test_module.TestType')\n"], "sample_235": ["    def setUp(self):\n        self.notified = []\n", "    def setUp(self):\n        self.notified = []\n", "    def test_savepoint_commit(self):\n        self.do(1)\n        self.do(2)\n        self.do(3)\n        # Test that savepoint commit is triggered before the atomic block is exited\n        with transaction.atomic():\n            transaction.savepoint()\n            self.do(4)\n            transaction.savepoint()\n            self.do(5)\n            transaction.savepoint_commit()\n            self.assertDone([1, 2, 3, 4])\n            self.assertNotified([1, 2, 3, 4])\n        self.assertDone([1, 2, 3, 4, 5])\n        self.assertNotified([1, 2, 3, 4])\n", "    def setUp(self):\n        self.notified = []\n", "    def setUp(self):\n        self.notified = []\n"], "sample_844": ["def test_cluster_optics_xi_predecessor_correction():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    reachability = np.array([10, 8.9, 8.8])\n    predecessor = np.array([-1, 0, 1])\n    ordering = np.array([0, 1, 2])\n\n    clusters = _xi_cluster(reachability, predecessor, ordering, 0.05, 2, 3, True)\n    labels = _extract_xi_labels(ordering, clusters)\n    assert_array_equal(labels, np.array([0, 0, 0]))\n", "def test_optics_init_parameter_validation(max_eps, cluster_method, min_samples, eps, xi, min_cluster_size):\n    with pytest.raises(ValueError):\n        OPTICS(max_eps=max_eps, cluster_method=cluster_method,\n               min_samples=min_samples, eps=eps, xi=xi, min_cluster_size=min_cluster_size)\n", "def test_optics_cluster_method_and_params(X, cluster_method, xi, min_cluster_size, expected_clusters):\n    # Create an OPTICS object with the specified cluster method and parameters\n    opt = OPTICS(cluster_method=cluster_method, xi=xi, min_cluster_size=min_cluster_size)\n    opt.fit(X)\n\n    # Create a DBSCAN object for comparison\n    dbscan = DBSCAN(eps=0.5, min_samples=10)\n    dbscan.fit(X)\n\n    # Get the labels from the OPTICS and DBSCAN objects\n    labels_optics = opt.labels_\n    labels_dbscan = dbscan.labels_\n\n    # Get the cluster assignments for the OPTICS object\n    clusters_optics = []\n    for c in opt.cluster_hierarchy_:\n        clusters_optics.append((c[0], c[1]))\n\n    # Check that the number of clusters is correct\n    assert len(clusters_optics) == len(expected_clusters)\n\n    # Check that the cluster assignments are correct\n    for i, cluster in enumerate(expected_clusters):\n        start, end = cluster\n        cluster_optics = X[opt.ordering_[start:end+1]]\n        cluster_dbscan = X[dbscan.labels_ == dbscan.labels_[start]]\n        assert_array_equal(cluster_optics, cluster_dbs", "def test_cluster_optics_xi_min_cluster_size(min_samples, min_cluster_size, xi,\n                                            expected_clusters):\n    clusters = generate_clustered_data(n_samples=62, n_features=2,\n                                       n_clusters=5, rng=rng)\n    X = clusters[:, :2]\n    expected_clusters = np.array(expected_clusters)\n    model = OPTICS(min_samples=min_samples, min_cluster_size=min_cluster_size,\n                   xi=xi, algorithm='auto', metric='euclidean')\n    model.fit(X)\n    labels, _ = cluster_optics_xi(reachability=model.reachability_,\n                                  predecessor=model.predecessor_,\n                                  ordering=model.ordering_,\n                                  min_samples=min_samples,\n                                  min_cluster_size=min_cluster_size,\n                                  xi=xi,\n                                  predecessor_correction=False)\n    expected_labels, _ = cluster_optics_xi(reachability=model.reachability_,\n                                           predecessor=model.predecessor_,\n                                           ordering=model.ordering_,\n                                           min_samples=min_samples,\n                                           min_cluster_size=min_cluster_size,\n                                           xi=xi,\n                                           predecessor_correction=False)\n    assert_array_equal(model.cluster_hierarchy_, expected_clusters)\n    assert_array_equal(labels, expected_labels)\n", "def test_optics_cluster_xi_epsilon(xi, cluster_method, min_samples, min_cluster_size, pred_correct):\n    n_samples = 100\n    X = rng.rand(n_samples, 2)\n    eps = 0.5\n    clustering = OPTICS(min_samples=min_samples, cluster_method=cluster_method, xi=xi,\n                       predecessor_correction=pred_correct, min_cluster_size=min_cluster_size,\n                       eps=eps)\n    clustering.fit(X)\n\n    # Create a DBSCAN instance with the same parameters as OPTICS\n    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n    dbscan.fit(X)\n\n    # Calculate the contingency matrix\n    contingency = contingency_matrix(clustering.labels_, dbscan.labels_)\n\n    # Check that the cluster assignments are consistent\n    assert_array_equal(contingency[0, 0], contingency[0, 3])\n    assert_array_equal(contingency[1, 1], contingency[1, 3])\n\n    # Check that the clusters are consistent\n    clusters = clustering.cluster_hierarchy_\n    dbscan_clusters = dbscan.core_sample_indices_\n    assert_array_equal(clusters[:, 0], dbscan_clusters)\n    assert_array_equal(clusters[:, 1], dbscan_clusters +"], "sample_89": ["    def test_check_availability(self):\n        with mock.patch('django.utils.autoreload.pywatchman', side_effect=[ImportError]):\n            self.assertRaises(WatchmanUnavailable, autoreload.WatchmanReloader.check_availability)\n\n        with mock.patch('django.utils.autoreload.pywatchman', side_effect=[Exception]):\n            self.assertRaises(WatchmanUnavailable, autoreload.WatchmanReloader.check_availability)\n", "    def test_tick(self):\n        with mock.patch.object(autoreload.StatReloader, 'snapshot_files') as mock_snapshot_files:\n            reloader = autoreload.StatReloader()\n            reloader.tick()\n            mock_snapshot_files.assert_called_once()\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with self.assertRaises(StopIteration):\n            next(reloader.tick())\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        # Test that tick is a generator\n        self.assertIsInstance(reloader.tick(), types.GeneratorType)\n        # Test that it raises NotImplementedError when run_loop is not implemented\n        with self.assertRaises(NotImplementedError):\n            list(reloader.tick())\n", "    def test_tick(self):\n        with mock.patch.object(autoreload.StatReloader, 'snapshot_files'):\n            reloader = autoreload.StatReloader()\n            gen = reloader.tick()\n            # Ensure tick doesn't raise StopIteration immediately\n            self.assertIsNone(next(gen))\n            # Ensure tick yields when no files have changed\n            next(gen)\n            self.assertFalse(reloader.should_stop)\n"], "sample_784": ["def test_calibrated_classifier_fit():\n    X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,\n                              n_redundant=2, n_classes=3, random_state=0)\n    clf = LinearSVC(random_state=0)\n    clf.fit(X, y)\n\n    calibrated_clf = _CalibratedClassifier(clf, method='sigmoid')\n    calibrated_clf.fit(X, y)\n\n    assert_array_equal(calibrated_clf.classes_, clf.classes_)\n    assert calibrated_clf.calibrators_ is not None\n", "def test_calibration_curve_with_quantile_strategy():\n    X, y = make_classification(n_samples=100, n_features=10, n_redundant=0,\n                              n_informative=5, random_state=42)\n    clf = MultinomialNB()\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)\n\n    # use calibration curve with quantile strategy\n    prob_true, prob_pred = calibration_curve(y, y_pred[:, 1], strategy='quantile')\n    assert_array_almost_equal(prob_true, prob_pred)\n\n    # test that quantile strategy produces same result as uniform strategy\n    _, prob_pred_uniform = calibration_curve(y, y_pred[:, 1], strategy='uniform')\n    assert_array_almost_equal(prob_true, prob_pred_uniform)\n\n", "def test_calibration_curve_different_normalization():\n    \"\"\"Check that calibration curve with different normalization strategies\n    returns the same result for identical inputs.\"\"\"\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n                              n_redundant=5, n_repeated=0, n_classes=2,\n                              random_state=42)\n    y_prob = np.random.rand(1000)\n\n    prob_true_uniform, prob_pred_uniform = calibration_curve(y, y_prob, normalize=False, n_bins=5, strategy='uniform')\n    prob_true_quantile, prob_pred_quantile = calibration_curve(y, y_prob, normalize=False, n_bins=5, strategy='quantile')\n\n    assert_array_almost_equal(prob_true_uniform, prob_true_quantile)\n    assert_array_almost_equal(prob_pred_uniform, prob_pred_quantile)\n", "def test_calibration_method():\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=15,\n                               n_redundant=3, n_repeated=0, n_classes=3,\n                               n_clusters_per_class=1, weights=None,\n                               random_state=0)\n    clf = LinearSVC(probability=True)\n    clf.fit(X, y)\n\n    calibrator = CalibratedClassifierCV(clf, method='sigmoid')\n    calibrator.fit(X, y)\n\n    assert calibrator.method == 'sigmoid'\n    assert calibrator.base_estimator is clf\n    assert isinstance(calibrator.calibrated_classifiers_[0],\n                     _CalibratedClassifier)\n", "def test_isotonic_calibration():\n    # Test that isotonic regression calibration works correctly\n    X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                               n_redundant=0, n_classes=2, random_state=42)\n    clf = LinearSVC(random_state=42)\n    clf.fit(X, y)\n\n    # Create an instance of the calibrated classifier with isotonic calibration\n    calibrated_clf = CalibratedClassifierCV(base_estimator=clf, method='isotonic')\n    calibrated_clf.fit(X, y)\n\n    # Test that the calibrated classifier has the correct calibration\n    prob_true, prob_pred = calibration_curve(y, calibrated_clf.predict_proba(X)[:, 1])\n    assert_array_almost_equal(prob_true, prob_pred)\n"], "sample_295": ["    def test_Case_with_When(self):\n        expr = Case(\n            When(F('name') == 'Example Inc.', then=F('num_employees')),\n            When(F('name') == 'Foobar Ltd.', then=F('num_chairs')),\n            default=F('salary')\n        )\n        self.assertEqual(expr.__class__, Case)\n        self.assertEqual(expr.cases[0].condition.__class__, When)\n        self.assertEqual(expr.cases[0].condition.args[0].__class__, F)\n        self.assertEqual(expr.cases[0].condition.args[0].name, 'name')\n        self.assertEqual(expr.cases[0].result.__class__, F)\n        self.assertEqual(expr.cases[0].result.value, F('num_employees'))\n        self.assertEqual(expr.cases[1].condition.__class__, When)\n        self.assertEqual(expr.cases[1].condition.args[0].__class__, F)\n        self.assertEqual(expr.cases[1].condition.args[0].name, 'name')\n        self.assertEqual(expr.cases[1].result.__class__, F)\n        self.assertEqual(expr.cases[1].result.value, F('num_chairs'))\n        self.assertEqual(expr.default.__class__, F)\n        self.assertEqual(expr.default.value, F('salary'))\n", "    def test_combined_expression_simplification(self):\n        expression = F('num_chairs') + F('num_chairs')\n        self.assertEqual(expression.__class__, CombinedExpression)\n", "    def test_combine_expression_types(self):\n        expression = F('id') + F('name')\n        self.assertIsInstance(expression, CombinedExpression)\n        self.assertEqual(expression.lhs.output_field.get_internal_type(), 'IntegerField')\n        self.assertEqual(expression.rhs.output_field.get_internal_type(), 'CharField')\n", "    def test_subquery_expression(self):\n        query = Company.objects.values('name', 'num_employees', 'num_chairs')\n        subquery = query.values('name', 'num_employees')\n        subquery = subquery.filter(num_employees__gt=100)\n        subquery = subquery.values('name')\n        subquery = subquery.annotate(numer_of_employees=F('num_employees'))\n        subquery = subquery.annotate(max_salary=Max('numer_of_employees'))\n        result = query.annotate(ceo_name=F('ceo__firstname')).filter(numer_of_employees__gt=100)\n        result = result.annotate(num_employees=subquery)\n        result = result.annotate(max_salary=subquery)\n        self.assertEqual(result.values_list('max_salary', flat=True), [3])\n", "    def test_window_expression_func(self):\n        simulation_run = SimulationRun.objects.create(\n            start_time=Time.objects.create(),\n            end_time=Time.objects.create(),\n            min_time=Time.objects.create(),\n            max_time=Time.objects.create(),\n            average_time=Time.objects.create(),\n            std_time=Time.objects.create(),\n            avg_time=Time.objects.create(),\n            max_time_in_simulation=Time.objects.create(),\n            min_time_in_simulation=Time.objects.create(),\n            average_time_in_simulation=Time.objects.create(),\n            std_time_in_simulation=Time.objects.create(),\n            avg_time_in_simulation=Time.objects.create(),\n            mean_time=Time.objects.create(),\n            standard_deviation=Time.objects.create(),\n            variance=Time.objects.create(),\n            num_failures=Number.objects.create(),\n            num_attempts=Number.objects.create(),\n            num_successes=Number.objects.create(),\n            time_to_failure=Time.objects.create(),\n            time_to_success=Time.objects.create(),\n            time_between_failures=Time.objects.create(),\n            time_between_successes=Time.objects.create(),\n            time_to_event=Time.objects.create(),\n            experiment=Experiment.objects.create(),\n            result=Result.objects.create(),\n        )\n        Experiment.objects.create(\n            results=Result.objects.create(),\n            remote_employee=RemoteEmployee.objects.create(),\n            employee=Employee.objects.create(),\n            time=Time.objects.create(),\n            company=Company.objects.create(\n                name='Test Company',\n                num_employees=10,\n                num_chairs=2,\n                based_in_eu=False,\n            ),\n            user=UUID.objects.create(),\n        )\n        Experiment.objects.create(\n            results=Result.objects.create(),\n            remote_employee=RemoteEmployee.objects.create(),\n            employee=Employee.objects.create(),\n            time=Time.objects.create(),\n            company=Company.objects.create(\n                name='Test Company',\n                num_employees=10,\n                num_chairs=2,\n                based_in_eu=False,\n            ),\n            user=UUID.objects.create(),\n        )\n        Experiment.objects.create(\n            results=Result.objects.create(),\n            remote_employee=RemoteEmployee.objects.create(),\n            employee=Employee.objects.create(),\n            time=Time.objects.create(),\n            company=Company.objects.create(\n                name='Test Company',\n                num_employees=10,\n                num_chairs=2,\n                based_in_eu=False,\n            ),\n            user=UUID.objects.create(),\n        )\n        Experiment.objects.create(\n            results=Result.objects.create(),\n            remote_employee=RemoteEmployee"], "sample_609": ["def test_unified_dim_sizes():\n    da1 = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    da2 = xr.DataArray(np.array([4, 5, 6]), dims=[\"x\"])\n    da3 = xr.DataArray(np.array([7, 8, 9]), dims=[\"y\"])\n\n    assert unified_dim_sizes([da1, da2]) == {\"x\": 3}\n    assert unified_dim_sizes([da1, da3]) == {}\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da1, da2, da3])\n\n    da1 = xr.DataArray(np.array([[1, 2], [3, 4]]), dims=[\"x\", \"y\"])\n    da2 = xr.DataArray(np.array([[5, 6], [7, 8]]), dims=[\"x\", \"y\"])\n\n    assert unified_dim_sizes([da1, da2]) == {\"x\": 2, \"y\": 2}\n\n    da1 = xr.DataArray(np.array([[1, 2], [3, 4]]), dims=[\"x\", \"y\"])\n    da2 = xr.DataArray(np.array([[5, 6], [7, 8]]), dims=[\"x\", \"z\"])\n\n    assert unified_dim_sizes([da1, da2]) == {}\n\n    da1 = xr.DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    da2 = xr.DataArray(np.array([4, 5, 6]), dims=[\"y\"])\n\n    with pytest.raises(ValueError):\n        unified_dim_sizes([da1, da2])\n", "def test_apply_ufunc_dtype():\n    # Test that apply_ufunc returns an array with the correct dtype\n    from .dataarray import DataArray\n    from .variable import Variable\n\n    x = DataArray(np.array([1, 2, 3]), dims=[\"x\"])\n    y = DataArray(np.array([4, 5, 6]), dims=[\"x\"])\n\n    func = lambda x, y: x + y\n\n    result = apply_ufunc(func, x, y, output_dtypes=[np.float32])\n    assert result.dtype == np.float32\n\n    result = apply_ufunc(func, x, y, output_dtypes=np.float32)\n    assert result.dtype == np.float32\n\n    result = apply_ufunc(func, x, y)\n    assert result.dtype == np.result_type(x.dtype, y.dtype)\n\n    func = lambda x, y: np.sin(x) + np.sin(y)\n\n    result = apply_ufunc(func, x, y, output_dtypes=[np.float32])\n    assert result.dtype == np.float32\n\n    result = apply_ufunc(func, x, y, output_dtypes=np.float32)\n    assert result.dtype == np.float32\n\n    result = apply_ufunc(func, x, y)\n    assert result.dtype == np.result_type(x.dtype, y.dtype)\n", "def test_apply_ufunc_with_dataset_join_outer():\n    \"\"\"Test that the `dataset_join` option of `apply_ufunc` works as expected when joining datasets with different data variables.\"\"\"\n    # Create two datasets with different data variables\n    ds1 = xr.Dataset({\"var1\": (\"x\", np.arange(5)), \"var2\": (\"x\", np.arange(3))})\n    ds2 = xr.Dataset({\"var2\": (\"x\", np.arange(3)), \"var3\": (\"x\", np.arange(4))})\n\n    # Create a ufunc that adds two variables\n        return var1 + var2\n\n    # Apply the ufunc to the datasets with dataset_join=\"outer\"\n    result = apply_ufunc(add, ds1, ds2, dataset_join=\"outer\")\n\n    # Check that the result has the expected data variables\n    assert set(result.data_vars) == {\"var1\", \"var2\", \"var3\"}\n\n    # Check that the result has the expected shape\n    assert result[\"var1\"].shape == (8,)\n    assert result[\"var2\"].shape == (6,)\n    assert result[\"var3\"].shape == (4,)\n", "def test_cov_broadcast():\n    \"\"\"Test that cov broadcasts data correctly.\"\"\"\n    da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"])\n    da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"])\n    da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=[\"c\", \"d\"])\n\n    cov_da = xr.cov(da_a, da_b)\n    assert cov_da.shape == (2,)\n    assert cov_da.name == \"cov\"\n\n    cov_da = xr.cov(da_a, da_b, dim=\"a\")\n    assert cov_da.shape == (2, 2)\n    assert cov_da.name == \"cov\"\n\n    cov_da = xr.cov(da_a, da_b, dim=[\"a\", \"c\"])\n    assert cov_da.shape == (1, 1)\n    assert cov_da.name == \"cov\"\n\n    cov_da = xr.cov(da_a, da_b, dim=[\"a\", \"b\", \"c\"])\n    assert cov_da.shape == (1, 1)\n    assert cov_da.name == \"cov\"\n\n    cov_da = xr.cov(da_a, da_b, dim=[\"c\"])\n    assert cov_da.shape == (2, 2)\n    assert cov_da.name == \"cov\"\n\n    # Test with xarray with missing values\n    da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"], attrs={\"missing\": True})\n    da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"], attrs={\"missing\": True})\n    cov_da = xr.cov(da_a, da_b, dim=\"a\")\n    assert cov_da.name == \"cov\"\n\n    # Test with xarray with custom name\n    da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=[\"a\", \"b\"], name=\"my_cov\")\n    da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=[\"a\", \"b\", \"c\"], name=\"my_cov\")\n    cov_da =", "def test_apply_ufunc_broadcasting():\n    da1 = xr.DataArray(np.array([1, 2, 3]), dims=(\"x\",))\n    da2 = xr.DataArray(np.array([4, 5, 6]), dims=(\"x\",))\n    var1 = xr.Variable((\"x\",), np.array([7, 8, 9]))\n    var2 = xr.Variable((\"x\",), np.array([10, 11, 12]))\n\n        return x + y\n\n    result = apply_ufunc(add, da1, da2, var1, var2)\n    assert_identical(result, xr.DataArray(np.array([12, 15, 18]), dims=(\"x\",)))\n\n    da3 = xr.DataArray(np.array([13, 14, 15]), dims=(\"y\",))\n    result = apply_ufunc(add, da1, da2, var1, var2, da3)\n    assert_identical(result, xr.DataArray(np.array([12, 15, 18]), dims=(\"x\",)))\n\n    # Test broadcasting with Variable objects\n    result = apply_ufunc(add, var1, var2, da1, da2)\n    assert_identical(result, xr.DataArray(np.array([12, 15, 18]), dims=(\"x\",)))\n\n    # Test broadcasting with different shapes\n    da4 = xr.DataArray(np.array([[1, 2], [3, 4]]), dims=(\"y\", \"x\"))\n    result = apply_ufunc(add, da1, da4, var1, var2)\n    assert_identical(result, xr.DataArray(np.array([[12, 13], [15, 16]]), dims=(\"y\", \"x\")))\n\n    # Test broadcasting with masked values\n    da5 = xr.DataArray(np.array([1, 2, np.nan]), dims=(\"x\",), attrs={\"units\": \"m\"})\n    result = apply_ufunc(add, da1, da5, var1, var2)\n    assert_identical(result, xr.DataArray(np.array([2, 4, np.nan]), dims=(\"x\",), attrs={\"units\": \"m\"}))\n\n    # Test broadcasting with variable of different type (DataArray and Variable)\n    da6 = xr.DataArray(np.array([1, 2,"], "sample_544": ["def test_image_composite():\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n    axs[0, 0].imshow([[1, 2, 3], [4, 5, 6]], cmap='viridis')\n    axs[0, 1].imshow([[7, 8, 9], [10, 11, 12]], cmap='viridis')\n    axs[1, 0].imshow([[13, 14, 15], [16, 17, 18]], cmap='viridis')\n    axs[1, 1].imshow([[19, 20, 21], [22, 23, 24]], cmap='viridis')\n    axs[0, 0].set_alpha(0.5)\n    axs[1, 1].set_alpha(0.5)\n    plt.savefig('tmp.png')\n    im = mimage.imread('tmp.png')\n    assert_array_equal(im[:, :, 3], 128)\n\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n    axs[0, 0].imshow([[1, 2, 3], [4, 5, 6]], cmap='viridis')\n    axs[0, 1].imshow([[7, 8, 9], [10, 11, 12]], cmap='viridis')\n    axs[1, 0].imshow([[13, 14, 15], [16, 17, 18]], cmap='viridis')\n    axs[1, 1].imshow([[19, 20, 21], [22, 23, 24]], cmap='viridis')\n    axs[0, 0].set_interpolation('bilinear')\n    axs[1, 1].set_interpolation('bicubic')\n    plt.savefig('tmp.png')\n    im = mimage.imread('tmp.png')\n    assert_array_equal(im[:, :, 3], 128)\n\n    fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n    axs[0, 0].imshow([[1, 2, 3], [4, 5, 6]], cmap='viridis')\n    axs[0, 1].imshow([[7, 8, ", "def test_BboxImage():\n    # Test that BboxImage's bbox property is working correctly\n    bbox = Bbox.from_bounds(0.2, 0.2, 0.8, 0.8)\n    img = BboxImage(bbox, cmap='viridis')\n    assert img.bbox == bbox\n\n    # Test that BboxImage's get_window_extent method works correctly\n    renderer = mpl.backends.backend_agg.FigureCanvasAgg()\n    img.set_figure(renderer.figure)\n    extent = img.get_window_extent(renderer)\n    assert isinstance(extent, Bbox)\n    assert extent.xmin == 0\n    assert extent.ymin == 0\n    assert extent.xmax == 1\n    assert extent.ymax == 1\n\n    # Test that BboxImage's make_image method works correctly\n    img.set_data(np.random.rand(100, 100))\n    im, left, bottom, trans = img.make_image(renderer)\n    assert im is not None\n    assert left == 0\n    assert bottom == 0\n    assert trans.is_affine\n\n    # Test that BboxImage's contains method works correctly\n    event = mpl.MouseEvent('button_press_event', x=0.5, y=0.5)\n    assert img.contains(event)\n    assert not img.contains(event.copy(x=0.5, y=0.6))\n", "def test_figureimage_draw():\n    # Generate a figure with an image\n    fig = plt.figure()\n    fig.figimage([[1, 2], [3, 4]], cmap='gray')\n    \n    # Set the renderer to a mock renderer that doesn't have a draw_image method\n    class MockRenderer:\n            self.draw_image_called = False\n        \n            raise ValueError(\"draw_image not implemented\")\n    \n    fig.canvas.renderer = MockRenderer()\n    \n    # Draw the figure\n    fig.draw()\n    \n    # Check that draw_image was called\n    assert fig.canvas.renderer.draw_image_called\n", "def test_imshow_cmap_colorbar_range(cmap, expected):\n    from matplotlib import cm\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.zeros((10, 10)), cmap=cmap)\n    fig.colorbar(im)\n    im.set_cmap(cmap)\n    assert_array_equal(im.cmap.set_under('w').to_rgba(0), [0, 0, 0, 0])\n", "def test_make_image_max_size(imtype, imshape, max_size):\n    # Create an image of maximum size\n    if imtype == 'AxesImage':\n        im = imtype('dummy_axes')\n    elif imtype == 'BboxImage':\n        im = imtype(Bbox(np.array([[0, 0], [100, 100]]))\n                    if imshape == (10, 10) else Bbox(np.array([[0, 0],\n                                                              [max_size, max_size]])))\n    elif imtype == 'FigureImage':\n        im = imtype('dummy_figure')\n    elif imtype == 'NonUniformImage':\n        x = np.linspace(0, 100, 11)\n        y = np.linspace(0, 100, 11)\n        im = imtype('dummy_axes', x=x, y=y)\n    elif imtype == 'PcolorImage':\n        im = imtype(x=np.linspace(0, 100, 11), y=np.linspace(0, 100, 11),\n                    A=np.random.rand(10, 10))\n\n    # Get the original image data\n    data = copy(im._A)\n    original_data = copy(im._A)\n\n    # Make a thumbnail of the image\n    thumbnail('dummy_infile', 'dummy_thumbfile', scale=0.1)\n\n    # Get the thumbnail image\n    with ExitStack() as stack:\n        thumb_file = stack.enter_context(open('dummy_thumbfile.png', 'rb'))\n        thumb_data = Image.open(thumb_file).convert('RGB')\n        thumb_array = np.array(thumb_data)\n        thumb_array = thumb_array[:imshape[0], :imshape[1], :]\n\n    # Check if the thumbnail image was made with the correct interpolation\n    if imtype == 'AxesImage' and im.origin == 'upper':\n        assert_array_equal(data, original_data)\n        assert_array_equal(data, thumb_array)\n    elif imtype in ['BboxImage', 'FigureImage']:\n"], "sample_1078": ["def test_base_with_symbolic_dimension():\n    from sympy import IndexedBase, Idx, symbols, oo\n    n, m = symbols('n m', integer=True)\n\n    # Test base with a symbolic dimension\n    A = IndexedBase('A', shape=(n, n))\n    assert A[i, j].shape == (n, n)\n    assert A[i, j].ranges == [(0, n-1), (0, n-1)]\n    A = IndexedBase('A', shape=(n, oo))\n    assert A[i, j].shape == (n, oo)\n    assert A[i, j].ranges == [(0, n-1), (0, oo)]\n\n    # Test base with a symbolic dimension and an index with range\n    A = IndexedBase('A', shape=(n, n))\n    i, j = symbols('i j', cls=Idx, integer=True)\n    assert A[i, j].shape == (n, n)\n    assert A[i, j].ranges == [(0, n-1), (0, n-1)]\n\n    # Test base with a symbolic dimension and an index with an integer range\n    A = IndexedBase('A', shape=(n, n))\n    i, j = symbols('i j', cls=Idx, integer=True)\n    assert A[i, j].shape == (n, n)\n    assert A[i, j].ranges == [(0, n-1), (0, n-1)]\n\n    # Test base with a symbolic dimension and an index with an oo range\n    A = IndexedBase('A', shape=(n, n))\n    i, j = symbols('i j', cls=Idx, integer=True)\n    assert A[i, oo].shape == (n, oo)\n    assert A[i, oo].ranges == [(0, n-1), (0, oo)]\n\n    # Test base with a symbolic dimension and an index with an oo lower bound\n    A = IndexedBase('A', shape=(n, n))\n    i, j = symbols('i j', cls=Idx, integer=True)\n    assert A[0, j].shape == (n, n)\n    assert A[0, j].ranges == [(0, n-1), (0, n-1)]\n\n    # Test base with a symbolic dimension and an index with an oo upper bound\n    A = IndexedBase('A', shape=(n,", "def test_Assumptions():\n    from sympy import IndexedBase, Idx, IdxBase, S, assumptions\n\n    assert IndexedBase('A', integer=True).is_integer\n    assert IndexedBase('A', integer=True).is_finite\n    assert IndexedBase('A', integer=True).is_real\n    assert IndexedBase('A', integer=True).assumptions0 == {'integer': True, 'finite': True, 'real': True}\n\n    assert IndexedBase('A', integer=True, commutative=False).assumptions0 == {'integer': True, 'finite': True, 'real': True, 'commutative': False}\n\n    assert IndexedBase('A', commutative=False).assumptions0 == {'finite': True, 'real': True, 'commutative': False}\n\n    assert IndexedBase('A', real=False).assumptions0 == {'finite': True, 'commutative': True, 'real': False}\n\n    assert IndexedBase('A', real=False, commutative=False).assumptions0 == {'finite': True, 'commutative': False, 'real': False}\n\n    assert IndexedBase('A', integer=False, commutative=False).assumptions0 == {'finite': True, 'commutative': False}\n\n    assert IndexedBase('A', integer=True, commutative=True).assumptions0 == {'integer': True, 'finite': True, 'real': True, 'commutative': True}\n\n    assert IndexedBase('A', integer=True, commutative=True, finite=False).assumptions0 == {'integer': True, 'finite': False, 'real': True, 'commutative': True}\n\n    assert IndexedBase('A', integer=True, commutative=True, real=False).assumptions0 == {'integer': True, 'finite': True, 'real': False, 'commutative': True}\n\n    assert IndexedBase('A', integer=True, commutative=True, real=False, finite=False).assumptions0 == {'integer': True, 'finite': False, 'real': False, 'commutative': True}\n\n    assert Idx('x').assumptions0 == {'finite': True, 'real': True}\n\n    assert Idx('x', integer=True).assumptions0 == {'finite': True, 'integer': True, 'real': True}\n\n    assert Idx('", "def test_IndexedBase_copy():\n    from sympy.tensor.indexed import IndexedBase, Idx\n    from sympy import symbols\n\n    # Test creating a copy of an IndexedBase object\n    A = IndexedBase('A', shape=(1, 2))\n    B = A.copy()\n    assert A is not B\n    assert B == A\n    assert B.shape == (1, 2)\n\n    # Test creating a copy of an IndexedBase object with shape\n    A = IndexedBase('A', shape=(1, 2))\n    B = A.copy()\n    B.shape = (2, 1)\n    assert B.shape == (2, 1)\n\n    # Test creating a copy of an IndexedBase object with assumptions\n    A = IndexedBase('A', shape=(1, 2), real=True)\n    B = A.copy()\n    assert B.is_real\n\n    # Test creating a copy of an IndexedBase object with a non-numeric shape\n    A = IndexedBase('A', shape=(S.Symbol('x'), 2))\n    B = A.copy()\n    assert B.shape == (S.Symbol('x'), 2)\n\n    # Test creating a copy of an IndexedBase object with a non-numeric shape and assumptions\n    A = IndexedBase('A', shape=(S.Symbol('x'), 2), real=True)\n    B = A.copy()\n    assert B.is_real\n\n    # Test creating a copy of an IndexedBase object with a stride\n    A = IndexedBase('A', strides=(1, 2), offset=3)\n    B = A.copy()\n    assert B.strides == (1, 2)\n    assert B.offset == 3\n\n    # Test creating a copy of an IndexedBase object with a stride and shape\n    A = IndexedBase('A', shape=(1, 2), strides=(1, 2), offset=3)\n    B = A.copy()\n    assert B.shape == (1, 2)\n    assert B.strides == (1, 2)\n    assert B.offset == 3\n\n    # Test creating a copy of an IndexedBase object with a stride, shape and assumptions\n    A = IndexedBase('A', shape=(1, 2), strides=(1, 2), offset=3, real=True)\n    B = A.copy()\n    assert B.is_real\n    assert B.shape == (1, 2)\n    assert B.str", "def test_Indexed_get_contraction_structure():\n    from sympy.tensor import get_contraction_structure\n\n    # Test with a simple array\n    A = IndexedBase('A')\n    A_i = A[Idx('i', 3)]\n    A_j = A[Idx('j', 4)]\n    A_k = A[Idx('k', 5)]\n    expr = A_i*A_j*A_k\n    expected = {(i, j, k): {A_i*A_j*A_k}}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with repeated indices\n    expr = A_i*A_j*A_i\n    expected = {(i, j): {A_i*A_j*A_i}}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with no repeated indices\n    expr = A_i*A_j\n    expected = {}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with repeated indices and a Sum\n    expr = Sum(A_i*A_j*A_i, (i, 0, 2))\n    expected = {(i, j): {A_i*A_j}}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with repeated indices and a Sum with a non-repeated index\n    expr = Sum(A_i*A_j*A_i, (i, 0, 2))\n    expected = {(i, j): {A_i*A_j}}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with an Idx and a Symbol\n    A_i = A[Idx('i', 3)]\n    expr = A_i*sin(A_i)\n    expected = {i: {A_i*sin(A_i)}}\n    assert get_contraction_structure(expr) == expected\n\n    # Test with a non-Idx index\n    A_oo = A[Idx('oo')]\n    expr = A_oo*sin(A_oo)\n    expected = {A_oo: {A_oo*sin(A_oo)}}\n    assert get_contraction_structure(expr) == expected\n", "def test_IndexedBase_shape_and_strides():\n    from sympy import IndexedBase, Idx\n    A = IndexedBase('A', shape=(3, 4), strides=(1, 2))\n    assert A.shape == (3, 4)\n    assert A.strides == (1, 2)\n\n    # Test that the shape and strides are copied to the Indexed object\n    i, j = map(Idx, 'ij')\n    assert A[i, j].shape == (3, 4)\n    assert A[i, j].strides == (1, 2)\n\n    # Test that a single index changes the shape of the Indexed object\n    assert A[i, 0].shape == (1, 4)\n    assert A[i, 0].strides == (1, 2)\n\n    # Test that the shape is not changed if the index has a range\n    assert A[i, Idx('k', 4)].shape == (3, 4)\n    assert A[i, Idx('k', 4)].strides == (1, 2)\n\n    # Test that the strides are not changed if the index has a range\n    assert A[i, Idx('k', strides=4)].shape == (3, 4)\n    assert A[i, Idx('k', strides=4)].strides == (1, 2)\n"], "sample_388": ["    def setUp(self):\n        self.backend = ModelBackend()\n        self.user = User.objects.create(username='testuser', is_active=True)\n", "    def setUp(self):\n        self.user = User.objects.create_user(\"testuser\", \"test@example.com\", \"password\")\n", "    def test_create_unknown_user(self):\n        backend = self.backend()\n        self.assertTrue(backend.create_unknown_user)\n        self.assertEqual(backend.authenticate(\"unknownuser\", None), None)\n\n        # Test user creation\n        user = backend.authenticate(\"unknownuser\", None)\n        self.assertEqual(user.username, \"unknownuser\")\n\n        # Test user creation with custom username\n        backend = self.backend(create_unknown_user=True)\n        user = backend.authenticate(\"customuser\", None)\n        self.assertEqual(user.username, \"customuser\")\n\n        # Test user creation with clean_username method\n        backend = self.backend(create_unknown_user=True)\n        backend.clean_username = lambda x: \"cleanuser\"\n        user = backend.authenticate(\"unknownuser\", None)\n        self.assertEqual(user.username, \"cleanuser\")\n", "    def test_remote_user_backend_create_unknown_user(self):\n        self.backend.create_unknown_user = True\n        request = self._make_request(username=\"newuser\")\n        user = self.backend.authenticate(request, remote_user=\"newuser\")\n        self.assertIsNotNone(user)\n        self.assertEqual(user.username, \"newuser\")\n", "    def setUp(self):\n        self.remote_user_backend = RemoteUserBackend()\n        self.request = Client().request()\n"], "sample_556": ["    def test_savefig_transparent_background(self):\n        fig, ax = plt.subplots()\n        fig.patch.set_facecolor('red')\n        fig.patch.set_edgecolor('red')\n        fig.savefig('test.png', transparent=True)\n        im = Image.open('test.png')\n        assert np.allclose(im.convert('RGB').load()[0], (255, 0, 0))\n", "def test_savefig():\n    # Test that savefig does not break the figure\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    fig.savefig('test.png')\n    with open('test.png', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'PNG'\n    os.remove('test.png')\n    fig.savefig('test.jpg')\n    with open('test.jpg', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'JPEG'\n    os.remove('test.jpg')\n    fig.savefig('test.pdf')\n    with open('test.pdf', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'PDF'\n    os.remove('test.pdf')\n    fig.savefig('test.pgf')\n    with open('test.pgf', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'PDF'\n    os.remove('test.pgf')\n    fig.savefig('test.svg')\n    with open('test.svg', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'SVG'\n    os.remove('test.svg')\n    fig.savefig('test.eps')\n    with open('test.eps', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'EPS'\n    os.remove('test.eps')\n    fig.savefig('test.tiff')\n    with open('test.tiff', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'TIFF'\n    os.remove('test.tiff')\n    fig.savefig('test.raw')\n    with open('test.raw', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'RAW'\n    os.remove('test.raw')\n    fig.savefig('test.rgba')\n    with open('test.rgba', 'rb') as f:\n        img = Image.open(f)\n        assert img.format == 'RGBA'\n    os.remove('test.rgba')\n", "def test_subfigures():\n    fig = Figure(figsize=(8, 6))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2)\n    sfigs = fig.subplots(1, 2)\n    axs1 = sfigs[0].subplots(1, 2)\n    axs2 = sfigs[1].subplots(1, 2)\n    assert len(fig.axes) == 4\n    assert len(sfigs[0].axes) == 2\n    assert len(sfigs[1].axes) == 2\n", "def test_subplots_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n    assert np.allclose(fig.subplotpars.left, 0.1)\n    assert np.allclose(fig.subplotpars.right, 0.9)\n    assert np.allclose(fig.subplotpars.top, 0.9)\n    assert np.allclose(fig.subplotpars.bottom, 0.1)\n\n    fig.subplots_adjust(left=None, right=None, top=None, bottom=None)\n    assert np.allclose(fig.subplotpars.left, mpl.rcParams['figure.subplot.left'])\n    assert np.allclose(fig.subplotpars.right, mpl.rcParams['figure.subplot.right'])\n    assert np.allclose(fig.subplotpars.top, mpl.rcParams['figure.subplot.top'])\n    assert np.allclose(fig.subplotpars.bottom, mpl.rcParams['figure.subplot.bottom'])\n\n    fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.2, hspace=0.2)\n    assert np.allclose(fig.subplotpars.wspace, 0.2)\n    assert np.allclose(fig.subplotpars.hspace, 0.2)\n", "def test_set_layout_engine():\n    fig = Figure(figsize=(10, 10))\n\n    # Test with a valid layout engine\n    fig.set_layout_engine('tight')\n    assert fig.get_layout_engine().get_info()['adjust_compatible'] is True\n\n    # Test with an invalid layout engine\n    with pytest.raises(ValueError):\n        fig.set_layout_engine('invalid')\n\n    # Test with a custom layout engine\n    class CustomLayoutEngine(LayoutEngine):\n            pass\n            return {'adjust_compatible': True}\n\n    fig.set_layout_engine(CustomLayoutEngine())\n    assert fig.get_layout_engine().__class__ == CustomLayoutEngine\n\n    # Test setting layout engine with kwargs\n    engine = TightLayoutEngine(pad=2, h_pad=2, w_pad=2)\n    fig.set_layout_engine('tight', **engine.get_info())\n    assert fig.get_layout_engine().pad == 2\n    assert fig.get_layout_engine().h_pad == 2\n    assert fig.get_layout_engine().w_pad == 2\n\n    # Test that layout engine is properly removed when set to 'none'\n    fig.set_layout_engine('tight')\n    fig.set_layout_engine('none')\n    assert fig.get_layout_engine() is None\n\n    # Test that layout engine is properly removed when set to a new layout engine\n    fig.set_layout_engine('tight')\n    fig.set_layout_engine('constrained')\n    assert fig.get_layout_engine().__class__ == ConstrainedLayoutEngine\n\n    # Test that layout engine is not modified if the new engine is not compatible\n    fig.set_layout_engine('tight')\n    engine = PlaceHolderLayoutEngine(False, False)\n    with pytest.raises(RuntimeError):\n        fig.set_layout_engine('tight', **engine.get_info())\n\n    # Test that layout engine is not modified if the new engine is not compatible\n    # and a colorbar has been added\n    fig.set_layout_engine('tight')\n    engine = PlaceHolderLayoutEngine(False, False)\n    fig.colorbar(mpl.colorbar.Colorbar(None, None))\n    with pytest.raises(RuntimeError):\n        fig.set_layout_engine('tight', **engine.get_info())\n\n    # Test that layout engine is not modified if the new engine has the same\n    # colorbar_gridspec as the old engine\n    fig.set_layout_engine('tight')\n    engine = PlaceHolderLayoutEngine(True, False)\n    fig.set_layout_engine('tight', **engine.get_info())\n    assert fig.get"], "sample_449": ["    def test_is_broken_pipe_error(self):\n        with captured_stderr() as stderr:\n            # Simulate a broken pipe error\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            sys.exc_info = (BrokenPipeError, None, None)\n            is_broken_pipe_error()\n            self.assertIn(\"Broken pipe from\", stderr.getvalue())\n            sys.exc_info = (exc_type, exc_value, exc_traceback)\n\n        with captured_stderr() as stderr:\n            # Simulate a non-broken pipe error\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            sys.exc_info = (ConnectionAbortedError, None, None)\n            self.assertFalse(is_broken_pipe_error())\n            sys.exc_info = (exc_type, exc_value, exc_traceback)\n\n        with captured_stderr() as stderr:\n            # Simulate a non-broken pipe error\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            sys.exc_info = (ConnectionResetError, None, None)\n            self.assertFalse(is_broken_pipe_error())\n            sys.exc_info = (exc_type, exc_value, exc_traceback)\n", "    def test_broken_pipe_error(self):\n        wsgi_handler = get_internal_wsgi_application()\n        request = self.request_factory.get(\"/\", HTTP_CONNECTION=\"close\")\n        with captured_stderr() as stderr:\n            WSGIServer((socket.gethostbyname(\"localhost\"), 8080), WSGIRequestHandler).serve_forever()\n            # This will cause the server to raise a BrokenPipeError\n            request.content = b\"GET / HTTP/1.1\\r\\n\\r\\n\"\n            wsgi_handler(request)\n        self.assertIn(\"Broken pipe from\", stderr.getvalue())\n", "    def test_run_ipv6(self):\n        # test that the run function correctly handles ipv6 address\n        with captured_stderr() as stderr:\n            addr = \"::1\"\n            port = 8000\n            wsgi_handler = get_internal_wsgi_application()\n            run(addr, port, wsgi_handler, ipv6=True)\n        self.assertIn(\"Listening on ::1:8000\", stderr.getvalue())\n", "    def test_cleanup_headers(self):\n        environ = self.request_factory().environ.copy()\n        headers = {\"Content-Length\": \"1024\", \"Connection\": \"keep-alive\"}\n        server_handler = ServerHandler(None, None, None, environ, headers=headers)\n        server_handler.cleanup_headers()\n        self.assertNotIn(\"Content-Length\", server_handler.headers)\n        self.assertIn(\"Connection\", server_handler.headers)\n        self.assertEqual(server_handler.headers[\"Connection\"], \"close\")\n\n        # Test HEAD request\n        environ[\"REQUEST_METHOD\"] = \"HEAD\"\n        server_handler = ServerHandler(None, None, None, environ, headers=headers)\n        server_handler.cleanup_headers()\n        self.assertNotIn(\"Content-Length\", server_handler.headers)\n        self.assertEqual(server_handler.headers[\"Connection\"], \"close\")\n\n        # Test non-threading server\n        server_handler = ServerHandler(None, None, None, environ, headers=headers)\n        server_handler.request_handler.server = WSGIServer()\n        server_handler.cleanup_headers()\n        self.assertIn(\"Content-Length\", server_handler.headers)\n        self.assertEqual(server_handler.headers[\"Connection\"], \"close\")\n\n        # Test non-http/1.1 server\n        server_handler.http_version = \"HTTP/1.0\"\n        server_handler.cleanup_headers()\n        self.assertNotIn(\"Connection\", server_handler.headers)\n", "    def test_WSGIServer_ipv6(self):\n        class StubServer(WSGIServer):\n            ipv6 = True\n\n        # The test server will not be started but its __init__ method will be tested\n        # separately.\n        self.assertEqual(StubServer().address_family, socket.AF_INET6)\n"], "sample_496": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.test_dir = os.path.realpath(os.path.join(\n            tempfile.gettempdir(),\n            cls.__name__,\n            'test_project',\n        ))\n        if not os.path.exists(cls.test_dir):\n            os.makedirs(cls.test_dir)\n        with open(os.path.join(cls.test_dir, '__init__.py'), 'w'):\n            pass\n        cls.settings_file_path = os.path.join(cls.test_dir, 'settings.py')\n        cls.write_settings('settings.py')\n", "    def test_parse_args_missing_args_message(self):\n        cmd = BaseCommand()\n        cmd.missing_args_message = \"Missing required arguments\"\n        parser = cmd.create_parser('test', 'cmd')\n        with self.assertRaises(SystemExit):\n            parser.parse_args([])\n        parser.parse_args(['--help'])\n        with self.assertRaises(CommandError):\n            parser.parse_args([])\n", "    def test_output_transaction(self):\n        class TestCommand(BaseCommand):\n            output_transaction = True\n            help = 'Test command'\n\n                return 'Test output'\n\n        cmd = TestCommand(stdout=StringIO())\n        cmd.run_from_argv(['', 'test_command'])\n        self.assertEqual(cmd.stdout._out.getvalue(), 'BEGIN;\\nTest output\\nCOMMIT;\\n')\n", "    def setUp(self):\n        self.test_dir = AdminScriptTestCase.test_dir\n        self.test_settings_file = os.path.join(self.test_dir, 'settings.py')\n        self.write_settings(self.test_settings_file)\n", "    def test_run_from_argv(self):\n        # Test that run_from_argv() handles CommandError properly\n        cmd = BaseCommand()\n        with self.assertRaises(CommandError):\n            cmd.run_from_argv(['my_command', '--settings=my_project.settings'])\n\n        # Test that run_from_argv() handles exceptions other than CommandError\n        class MyException(Exception):\n            pass\n        cmd = BaseCommand()\n        with self.assertRaises(MyException):\n            cmd.run_from_argv(['my_command', '--settings=my_project.settings'])\n\n        # Test that run_from_argv() preserves existing settings\n        cmd = BaseCommand()\n        cmd.stdout.write = lambda x: None\n        cmd.stderr.write = lambda x: None\n        cmd.run_from_argv(['my_command', '--settings=my_project.settings'])\n"], "sample_265": ["    def test_autoescape_default_value(self):\n        engine = self.engine_class({'OPTIONS': {'autoescape': None}})\n        self.assertTrue(engine.engine.autoescape)\n", "    def test_get_templatetag_libraries(self):\n        # Test with no custom libraries\n        engine = self.engine_class({'OPTIONS': {'libraries': {}}})\n        self.assertEqual(engine.get_templatetag_libraries({}), get_installed_libraries())\n\n        # Test with custom libraries\n        engine = self.engine_class({'OPTIONS': {'libraries': {'custom_lib': 'package.module'}}})\n        libraries = get_installed_libraries()\n        libraries['custom_lib'] = 'package.module'\n        self.assertEqual(engine.get_templatetag_libraries({'custom_lib': 'package.module'}), libraries)\n\n        # Test with installed applications\n        app_config = apps.get_app_configs()[0]\n        engine = self.engine_class({'OPTIONS': {'libraries': {}}})\n        self.assertIn(app_config.name + '.templatetags', engine.get_templatetag_libraries({}))\n", "    def test_get_templatetag_libraries_with_custom_libraries(self):\n        # Test that custom libraries are added to the libraries dictionary\n        self.assertIn('my_library', self.engine_class.get_templatetag_libraries({}))\n        # Test that custom libraries are not overridden by installed libraries\n        self.assertIn('my_library', self.engine_class.get_templatetag_libraries({'my_library': 'path.to.my.library'}))\n        # Test that installed libraries are added to the libraries dictionary\n        self.assertIn('django.templatetags.i18n', self.engine_class.get_templatetag_libraries({}))\n", "def test_get_installed_libraries(self):\n    # Test that the get_installed_libraries function returns the built-in\n    # template tag libraries and those from installed applications.\n    libraries = get_installed_libraries()\n    self.assertIn('i18n', libraries)\n    self.assertIn('context_processors', libraries)\n    self.assertIn('coreapi', libraries)\n    self.assertIn('paginator', libraries)\n\n    # Test that libraries are stored in a dictionary where keys are the\n    # individual module names, not the full module paths.\n    self.assertEqual(libraries['i18n'], 'django.templatetags.i18n')\n    self.assertEqual(libraries['context_processors'], 'django.templatetags.context_processors')\n    self.assertEqual(libraries['coreapi'], 'django.templatetags.coreapi')\n    self.assertEqual(libraries['paginator'], 'django.templatetags.paginator')\n\n    # Test that the function correctly handles missing template tag libraries.\n    with self.assertRaises(InvalidTemplateLibrary):\n        get_installed_libraries()\n\n    # Test that the function correctly handles packages that do not have\n    # a __path__ attribute.\n    class MissingPathPackage:\n        pass\n\n    self.assertEqual(get_installed_libraries(), {})\n    self.assertEqual(get_installed_libraries(), {})\n\n    # Test that the function correctly handles packages that are not modules.\n    class NotAModule:\n        __name__ = 'not_a_module'\n\n    with self.assertRaises(ImportError):\n        get_installed_libraries()\n\n    # Test that the function correctly handles recursive import.\n    class RecursiveImport:\n        __path__ = ['']\n        __name__ = 'recursive_import'\n\n    with self.assertRaises(ImportError):\n        get_installed_libraries()\n", "def test_get_installed_libraries(self):\n    \"\"\"Test get_installed_libraries function.\"\"\"\n    # Create a custom app with a templatetags directory\n    Path('templatetags_tests').mkdir(parents=True)\n    Path('templatetags_tests/templatetags').mkdir(parents=True)\n    Path('templatetags_tests/templatetags/my_library.py').touch()\n\n    # Create a custom library\n    with open('templatetags_tests/templatetags/my_library.py', 'w') as f:\n        f.write('register = {}')\n\n    # Add the app to INSTALLED_APPS in settings\n    with open('settings.py', 'a') as f:\n        f.write('INSTALLED_APPS += [\"templatetags_tests\"]\\n')\n\n    # Reload the settings\n    with override_settings(TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': ['templatetags_tests/templates']}]):\n        # Test that the custom library is included in the list of libraries\n        libraries = get_installed_libraries()\n        self.assertIn('my_library', libraries)\n\n    # Clean up\n    import os\n    for file in Path('templatetags_tests').rglob('**/*'):\n        os.remove(file)\n"], "sample_652": ["def test_getpublicnames(tmpdir):\n    # test get_public_names with a module and a package\n    m = tmpdir.join('m')\n    m.mkdir()\n    m.join('f.py').write('def test(): pass')\n    m.join('f2.py').write('def test2(): pass')\n    m.join('t.py').write('class T: pass')\n    m.join('t2.py').write('class T2: pass')\n\n    p = m.join('p')\n    p.mkdir()\n    p.join('g.py').write('def g(): pass')\n    p.join('g2.py').write('def g2(): pass')\n    p.join('g3.py').write('class G3: pass')\n    p.join('g4.py').write('class G4: pass')\n\n    assert get_public_names(m) == ['f', 't', 'f2', 't2']\n    assert get_public_names(p) == ['g', 'g2', 'g3', 'g4']\n", "def test_callspec2_copy_calls_add_ids():\n    import pytest\n    from _pytest.nodes import CallSpec2\n\n    callspec1 = CallSpec2(None)\n    callspec1._idlist = [1, 2]\n    callspec1._globalid = 3\n    callspec1.marks = [\"mark1\", \"mark2\"]\n    callspec1._arg2scopenum = {\"arg1\": 4, \"arg2\": 5}\n\n    callspec2 = callspec1.copy()\n    assert callspec2._idlist == [1, 2]\n    assert callspec2._globalid == 3\n    assert callspec2.marks == [\"mark1\", \"mark2\"]\n    assert callspec2._arg2scopenum == {\"arg1\": 4, \"arg2\": 5}\n", "def test_parametrize_id_mixin():\n    class TestClass:\n        @pytest.mark.parametrize(\"x\", [1, 2, 3])\n            assert self._parametrize_id == \"1\"\n\n    test = TestClass()\n    mixin = fixtures.ParametrizeIdMixin()\n    mixin._parametrize_id = \"0\"\n    mixin._get_parametrize_id(test.test_parametrize_id)\n\n    assert mixin._parametrize_id == \"1\"\n", "def test_getlocation():\n    # https://github.com/pytest-dev/pytest/issues/4569\n    # test this to ensure we don't get a typeerror\n    path = Path(__file__).dirpath().join(\"path/to/fixture.module.py\")\n        pass\n        pass\n    fixtures.fixture(fixture_function)\n    fixtures.fixture(fixture_function_decorated)\n\n    # test cases\n    assert getlocation(fixture_function, path) == path.strpath\n    assert getlocation(fixture_function_decorated, path) == path.strpath\n\n    # test for a non-existent file\n    non_existent_path = Path(\"non/existent/path/to/file.py\")\n    assert getlocation(fixture_function, non_existent_path) == non_existent_path.strpath\n\n    # test for a file that's not in sys.modules\n    module_name = \"non.existent.module\"\n    sys.modules[module_name] = None\n    assert getlocation(fixture_function, path) == path.strpath\n", "def test_getfslineno():\n    import _pytest\n    from _pytest.compat import getfslineno\n    from _pytest.compat import getimfunc\n    from _pytest.compat import getlocation\n    from _pytest.compat import getreal_func\n    from _pytest.compat import isfunction\n    from _pytest.compat import isgeneratorfunction\n    from _pytest.compat import get_default_arg_names\n\n        pass\n\n        yield\n\n        pass\n\n        pass\n\n    class TestClass:\n        pass\n\n    class TestClassWithInit(object):\n            pass\n\n    class TestClassWithNew(object):\n            pass\n\n    class TestClassWithNewAndInit(object):\n            pass\n            pass\n\n        return sys.modules[__name__]\n\n        return sys.modules[__name__].__module__\n\n        return sys.modules[__name__].__package__\n\n    class TestClassWithObjectMethod(object):\n            pass\n\n    class TestClassWithFunctionMethod(object):\n            pass\n\n    class TestClassWithFunctionMethod2(object):\n            pass\n            pass\n\n        assert getfslineno(test_function).__repr__() == \"<module 'test_getfslineno' from 'test_getfslineno.py'>\"\n\n        assert getfslineno(TestClass) == (None, 1)\n\n        obj = TestClass()\n        assert getfslineno(obj) == (None, 1)\n\n        assert getfslineno(test_generator_function).__repr__() == \"<module 'test_getfslineno' from 'test_getfslineno.py'>\"\n\n        assert getfslineno"], "sample_743": ["def test_check_algorithm_metric():\n    # Test that _check_algorithm_metric raises a ValueError when the algorithm is not in\n    # ['auto', 'brute', 'kd_tree', 'ball_tree']\n    with assert_raises_regex(ValueError, \"unrecognized algorithm: 'foo'\"):\n        NeighborsBase(algorithm='foo')\n\n    # Test that _check_algorithm_metric raises a ValueError when the metric is not valid for the chosen algorithm\n    with assert_raises_regex(ValueError, \"Metric 'my_metric' not valid for algorithm 'brute'\"):\n        NeighborsBase(algorithm='brute', metric='my_metric')\n\n    # Test that _check_algorithm_metric correctly sets the effective_metric_ attribute\n    nb = NeighborsBase(algorithm='auto', metric='minkowski', p=2)\n    assert nb.effective_metric_ == 'euclidean'\n\n    # Test that _check_algorithm_metric correctly sets the effective_metric_params_ attribute\n    nb = NeighborsBase(algorithm='auto', metric='minkowski', p=2)\n    assert nb.effective_metric_params_ == {'p': 2}\n\n    # Test that _check_algorithm_metric correctly raises a ValueError when p is not greater than 1 for the minkowski metric\n    with assert_raises_regex(ValueError, \"p must be greater than one for minkowski metric\"):\n        NeighborsBase(algorithm='auto', metric='minkowski', p=0)\n\n    # Test that _check_algorithm_metric correctly raises a ValueError when p is not greater than 1 for the minkowski metric\n    with assert_raises_regex(ValueError, \"p must be greater than one for minkowski metric\"):\n        NeighborsBase(algorithm='auto', metric='minkowski', p=-1)\n\n    # Test that _check_algorithm_metric correctly raises a ValueError when p is not greater than 1 for the minkowski metric\n    with assert_raises_regex(ValueError, \"p must be greater than one for minkowski metric\"):\n        NeighborsBase(algorithm='auto', metric='minkowski', p=1)\n\n    # Test that _check_algorithm_metric correctly sets the effective_metric_ attribute to 'manhattan' when p is 1\n    nb = NeighborsBase(algorithm='auto', metric='minkowski', p=1)\n    assert nb.effective_metric_ == 'manhattan'\n\n    # Test that _check_algorithm_metric correctly sets the effective_metric_ attribute to 'chebyshev' when p is", "def test_NeighborsBase_init_invalid_algorithm():\n    # Test invalid algorithm\n    neigh = NeighborsBase()\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n\n    # Test invalid metric for brute algorithm\n    neigh = NeighborsBase(algorithm='brute')\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n\n    # Test invalid metric for ball_tree algorithm\n    neigh = NeighborsBase(algorithm='ball_tree')\n    neigh.metric = 'hamming'\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n\n    # Test invalid metric for kd_tree algorithm\n    neigh = NeighborsBase(algorithm='kd_tree')\n    neigh.metric = 'minkowski'\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n\n    # Test invalid metric with callable metric\n    neigh = NeighborsBase(algorithm='kd_tree')\n    neigh.metric = lambda x, y: 1\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n\n    # Test invalid metric_params for minkowski\n    neigh = NeighborsBase(algorithm='brute', metric='minkowski', p=1)\n    with assert_raises(ValueError):\n        neigh._fit(None)\n\n    # Test invalid metric_params for minkowski\n    neigh = NeighborsBase(algorithm='brute', metric='minkowski', p=np.inf)\n    with assert_raises(ValueError):\n        neigh._fit(None)\n\n    # Test invalid metric for sparse input\n    neigh = NeighborsBase(algorithm='brute', metric='manhattan')\n    X = csr_matrix(np.ones((10, 10)))\n    with assert_raises(ValueError):\n        neigh._fit(X)\n\n    # Test invalid metric for sparse input with callable metric\n    neigh = NeighborsBase(algorithm='brute', metric=lambda x, y: 1)\n    X = csr_matrix(np.ones((10, 10)))\n    with assert_raises(ValueError):\n        neigh._fit(X)\n\n    # Test invalid metric for sparse input with minkowski metric\n    neigh = NeighborsBase(algorithm='brute', metric='minkowski')\n    X = csr_matrix(np.ones((10, 10)))\n    with assert_raises(ValueError):\n        neigh._fit(X)\n", "def test_fails_if_algorithm_not_recognized():\n    # Test that fit fails if algorithm is not recognized.\n    # TODO: more tests to check the actual error message\n    X = np.array([[1, 2], [3, 4]])\n    neigh = neighbors.NeighborsBase()\n    with assert_raises(ValueError):\n        neigh._check_algorithm_metric()\n    with assert_raises_regex(ValueError, \"unrecognized algorithm: 'bad_algorithm'\"):\n        neigh._check_algorithm_metric(algorithm='bad_algorithm')\n", "def test_KNeighborsMixin_kneighbors_graph():\n    \"\"\"Test kneighbors_graph method on KNeighborsMixin\"\"\"\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    neigh = neighbors.KNeighbors(n_neighbors=2, algorithm='brute')\n    neigh.fit(X)\n    # test basic behavior\n    neigh_graph = neigh.kneighbors_graph(X)\n    assert_array_equal(neigh_graph.toarray(), [[1., 1., 0.],\n                                              [1., 1., 0.],\n                                              [0., 0., 1.]])\n    # test that kneighbors_graph returns None when X is None\n    neigh_graph = neigh.kneighbors_graph()\n    assert neigh_graph is None\n    # test that kneighbors_graph raises a ValueError when n_neighbors > X.shape[0]\n    neigh = neighbors.KNeighbors(n_neighbors=X.shape[0] + 1)\n    with assert_raises(ValueError):\n        neigh.kneighbors_graph()\n    # test that kneighbors_graph raises a ValueError when n_neighbors <= 0\n    neigh = neighbors.KNeighbors(n_neighbors=0)\n    with assert_raises(ValueError):\n        neigh.kneighbors_graph()\n    # test that kneighbors_graph raises a TypeError when n_neighbors is not an integer\n    neigh = neighbors.KNeighbors(n_neighbors=1.5)\n    with assert_raises(TypeError):\n        neigh.kneighbors_graph()\n", "def test_fit_with_one_sample():\n    X = np.array([[1, 2, 3]])\n    est = neighbors.NeighborsBase(n_neighbors=2)\n    est.fit(X)\n    assert est.n_neighbors == 2\n    assert est.radius is None\n    assert est.algorithm == 'auto'\n    assert est.leaf_size == 30\n    assert est.metric == 'minkowski'\n    assert est.p == 2\n    assert est.n_jobs == 1\n"], "sample_336": ["    def test_resolve_nested_namespaces(self):\n        get_resolver().reverse_dict = {\n            'en': [\n                ('testapp:ns1:urlobject-view', '/test1/inner/42/37/', [42, 37], {'arg1': '42', 'arg2': '37'})\n            ]\n        }\n        resolver = get_resolver('testapp.urls')\n        self.assertEqual(resolver.reverse('testapp:ns1:urlobject-view', kwargs={'arg1': '42', 'arg2': '37'}), '/test1/inner/42/37/')\n        self.assertEqual(resolver.reverse('testapp:ns1:urlobject-view', args=[42, 37]), '/test1/inner/42/37/')\n", "    def test_reverse_lazy_resolution(self):\n        # Create a lazy URL resolver\n        lazy_resolver = reverse_lazy('normal-view')\n        self.assertIsInstance(lazy_resolver, URLResolver)\n        self.assertEqual(lazy_resolver.urlconf_name, 'urlpatterns_reverse.no_urls')\n\n        # Resolve the lazy URL\n        resolved_path = lazy_resolver.resolve('/normal/42/37/')\n        self.assertIsInstance(resolved_path, ResolverMatch)\n        self.assertEqual(resolved_path.func, views.empty_view)\n        self.assertEqual(resolved_path.args, ())\n        self.assertEqual(resolved_path.kwargs, {'arg1': '42', 'arg2': '37'})\n\n        # Resolve the lazy URL with an incorrect path\n        with self.assertRaises(Resolver404):\n            lazy_resolver.resolve('/wrong/path/')\n\n        # Check that the lazy resolver is thread-safe\n            return lazy_resolver.resolve('/normal/42/37/')\n        thread = threading.Thread(target=resolve_lazy_in_thread)\n        thread.start()\n        thread.join()\n        self.assertEqual(lazy_resolver.urlconf_name, 'urlpatterns_reverse.no_urls')\n", "    def test_resolve_including_default_args(self):\n        urlconf = 'urlpatterns_reverse.urlconf_outer'\n        resolver = get_resolver(urlconf)\n        view = views.default_args_view\n        resolved = resolver.resolve('/urlconf/42/')\n        self.assertEqual(resolved.func, view)\n        self.assertEqual(resolved.args, ())\n        self.assertEqual(resolved.kwargs, {'default_arg': 42})\n", "    def test_resolve_error_handler_error(self):\n        # Test that resolve_error_handler() raises an ImportError for an\n        # unimportable view.\n        urlconf = URLResolver(pattern=RegexPattern(r'^/'), urlconf_name='urlpatterns_reverse')\n        with self.assertRaises(ImportError):\n            urlconf.resolve_error_handler(400)\n", "    def test_resolve_error_handler(self):\n        from django.urls import URLResolver, include\n        urlconf = include('urlpatterns_reverse.no_urls')\n        resolver = URLResolver(pattern='', urlconf=urlconf)\n        handler404 = resolver.resolve_error_handler(404)\n        self.assertIsNotNone(handler404)\n        self.assertIsInstance(handler404, callable)\n"], "sample_126": ["def test_deconstructible_default_value_unique(self):\n    before_states = [\n        self.author_name,\n    ]\n    after_states = [\n        self.author_name_default,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(len(changes.get('testapp', [])), 1)\n    self.assertEqual(changes['testapp'][0].operations[0].operations[0].field.default, 'Ada Lovelace')\n", "def test_m2m_through_rename(self):\n    before_states = [\n        self.author_with_m2m,\n        self.contract,\n        self.publisher,\n        self.other_pony,\n    ]\n    after_states = [\n        self.author_with_m2m_renamed,\n        self.contract_renamed,\n        self.publisher,\n        self.other_pony,\n    ]\n    changes = self.get_changes(before_states, after_states)\n    self.assertNumberMigrations(changes, 'testapp', 2)\n    self.assertNumberMigrations(changes, 'testapp', 2, app_label='testapp')\n    self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name='contract', new_name='deal')\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, dependencies=[('testapp', 'Author', None, True), ('testapp', 'Author', None, True)])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, dependencies=[('testapp', 'Contract', None, True), ('testapp', 'Contract', None, True)])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, name='deal', new_name='deal')\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, dependencies=[('testapp', 'Author', None, True), ('testapp', 'Author', None, True)])\n    self.assertOperationAttributes(changes, 'testapp', 1, 0, dependencies=[('testapp', 'Deal', None, True), ('testapp', 'Deal', None, True)])\n", "    def test_alterField_field_type_change_with_default(self):\n        # Test that a field type change with a default value is correctly handled\n        before_states = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ]),\n        ]\n        after_states = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default='Hello')),\n            ]),\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['AlterField'])\n", "def test_generate_deleted_models(self):\n    # Test that deleted models are handled correctly\n    before_states = [\n        self.author_empty,\n    ]\n    after_states = [\n        self.author_unmanaged,\n    ]\n    expected_changes = {\n        'testapp': [\n            operations.DeleteModel(\n                name='Author',\n            ),\n        ],\n    }\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, expected_changes)\n\n    # Test that unmanaged models are handled correctly\n    before_states = [\n        self.author_unmanaged,\n    ]\n    after_states = [\n        self.author_unmanaged_managed,\n    ]\n    expected_changes = {\n        'testapp': [\n            operations.AlterModelOptions(\n                name='AuthorUnmanaged',\n                options={'managed': True},\n            ),\n        ],\n    }\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, expected_changes)\n\n    # Test that managed models are handled correctly when unmanaged\n    before_states = [\n        self.author_managed,\n    ]\n    after_states = [\n        self.author_unmanaged_managed,\n    ]\n    expected_changes = {\n        'testapp': [\n            operations.DeleteModel(\n                name='Author',\n            ),\n            operations.CreateModel(\n                name='AuthorUnmanaged',\n                fields=[('id', models.AutoField(primary_key=True))],\n                options={'managed': True},\n                bases=(),\n                managers=(),\n            ),\n        ],\n    }\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, expected_changes)\n\n    # Test that managed models are handled correctly when switched to unmanaged\n    before_states = [\n        self.author_managed,\n    ]\n    after_states = [\n        self.author_unmanaged,\n    ]\n    expected_changes = {\n        'testapp': [\n            operations.AlterModelOptions(\n                name='Author',\n                options={'managed': False},\n            ),\n        ],\n    }\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, expected_changes)\n\n    # Test that unmanaged models are handled correctly when switched to managed\n    before_states = [\n        self.author_unmanaged,\n    ]\n    after_states = [\n        self.author_managed,\n    ]\n    expected_changes = {\n        'testapp': [\n            operations.AlterModelOptions(\n                name='AuthorUnmanaged',\n                options={'managed': True},\n            ),\n        ],\n    }\n    changes = self.get_changes(before_states, after_states)\n    self.assertEqual(changes, expected_changes)\n", "    def test_empty_model_with_options(self):\n        changes = self.get_changes([self.empty], [self.empty], questioner=mock.Mock())\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n"], "sample_945": ["def test_py_parse_annotation():\n    env = Mock(spec=BuildEnvironment)\n    env.ref_context = {'py:module': 'module_name', 'py:class': 'class_name'}\n    ast = _parse_annotation('int', env)\n    assert len(ast) == 1\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'int'\n\n    ast = _parse_annotation('Union[int, str]', env)\n    assert len(ast) == 3\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'Union'\n    assert isinstance(ast[1], nodes.Text)\n    assert ast[1].astext() == '['\n    assert isinstance(ast[2], nodes.Text)\n    assert ast[2].astext() == 'int'\n\n    ast = _parse_annotation('List[int]', env)\n    assert len(ast) == 3\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'List'\n    assert isinstance(ast[1], nodes.Text)\n    assert ast[1].astext() == '['\n    assert isinstance(ast[2], nodes.Text)\n    assert ast[2].astext() == 'int'\n\n    ast = _parse_annotation('Dict[str, int]', env)\n    assert len(ast) == 4\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'Dict'\n    assert isinstance(ast[1], nodes.Text)\n    assert ast[1].astext() == '['\n    assert isinstance(ast[2], nodes.Text)\n    assert ast[2].astext() == 'str'\n    assert isinstance(ast[3], nodes.Text)\n    assert ast[3].astext() == ','\n\n    ast = _parse_annotation('Set[int]', env)\n    assert len(ast) == 3\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'Set'\n    assert isinstance(ast[1], nodes.Text)\n    assert ast[1].astext() == '['\n    assert isinstance(ast[2], nodes.Text)\n    assert ast[2].astext() == 'int'\n\n    ast = _parse_annotation('Tuple[int, str]', env)\n    assert len(ast) == 4\n    assert isinstance(ast[0], nodes.Text)\n    assert ast[0].astext() == 'Tuple'\n   ", "def test_parse_annotation():\n    # Test parse annotation with ellipsis\n    annotation = \"List[int, ...]\"\n    env = Mock(spec=['ref_context'])\n    env.ref_context['py:module'] = 'module'\n    result = _parse_annotation(annotation, env)\n    assert_node(result, nodes.Text(\"List[int, ...]\"))\n\n    # Test parse annotation with binary operators\n    annotation = \"int | str\"\n    result = _parse_annotation(annotation)\n    assert_node(result, nodes.Text(\"int\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"|\"))\n    assert_node(result, nodes.Text(\" \"))\n    assert_node(result, nodes.Text(\"str\"))\n\n    # Test parse annotation with tuple\n    annotation = \"(int, str)\"\n    result = _parse_annotation(annotation)\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"(\"))\n    assert_node(result, nodes.Text(\"int\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \", \"))\n    assert_node(result, nodes.Text(\" \"))\n    assert_node(result, nodes.Text(\"str\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \")\"))\n\n    # Test parse annotation with list\n    annotation = \"[int, str]\"\n    result = _parse_annotation(annotation)\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"[\"))\n    assert_node(result, nodes.Text(\"int\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \", \"))\n    assert_node(result, nodes.Text(\" \"))\n    assert_node(result, nodes.Text(\"str\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"]\"))\n\n    # Test parse annotation with subscript\n    annotation = \"List[int, 1]\"\n    result = _parse_annotation(annotation)\n    assert_node(result, nodes.Text(\"List\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"[\"))\n    assert_node(result, nodes.Text(\"int\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \", \"))\n    assert_node(result, nodes.Text(\" \"))\n    assert_node(result, nodes.Text(\"1\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"]\"))\n\n    # Test parse annotation with nested subscript\n    annotation = \"List[List[int, 1], 2]\"\n    result = _parse_annotation(annotation)\n    assert_node(result, nodes.Text(\"List\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"[\"))\n    assert_node(result, addnodes.desc_sig_punctuation(\"\", \"[\"))\n", "def test_parse_annotation():\n    env = Mock()\n    env.config.python_use_unqualified_type_names = True\n\n        return _parse_annotation(text, env)\n\n        assert _make_annotation(annotation) == expected\n\n    _check_annotation('List[int]', [nodes.Text('List['), nodes.Text('int'), nodes.Text(']')])\n    _check_annotation('int', [nodes.Text('int')])\n    _check_annotation('str', [nodes.Text('str')])\n    _check_annotation('Tuple[int, str]', [addnodes.desc_sig_punctuation('', '('),\n                                          nodes.Text('int'),\n                                          nodes.Text(', '),\n                                          nodes.Text('str'),\n                                          addnodes.desc_sig_punctuation('', ')')])\n    _check_annotation('List[str, int]', [addnodes.desc_sig_punctuation('', '['),\n                                         nodes.Text('str'),\n                                         nodes.Text(', '),\n                                         nodes.Text('int'),\n                                         addnodes.desc_sig_punctuation('', ')')])\n    _check_annotation('Tuple[str, int, List[int]]', [addnodes.desc_sig_punctuation('', '('),\n                                                     nodes.Text('str'),\n                                                     nodes.Text(', '),\n                                                     nodes.Text('int'),\n                                                     nodes.Text(', '),\n                                                     addnodes.desc_sig_punctuation('', '['),\n                                                     nodes.Text('int'),\n                                                     addnodes.desc_sig_punctuation('', ']'),\n                                                     addnodes.desc_sig_punctuation('', ')')])\n    _check_annotation('int | str', [nodes.Text('int'), addnodes.desc_sig_punctuation('', '|'),\n                                   nodes.Text('str')])\n    _check_annotation('str | int', [nodes.Text('str'), addnodes.desc_sig_punctuation('', '|'),\n                                   nodes.Text('int')])\n    _check_annotation('str | int | float', [nodes.Text('str'), addnodes.desc_sig_punctuation('', '|'),\n                                           nodes.Text('int'), addnodes.desc_sig_punctuation('', '|'),\n                                           nodes.Text('float')])\n    _check_annotation('...', [addnodes.desc_sig_punctuation('', '...')])\n    _check_annotation('None', [type_to_xref('None')])\n", "def test_py_classlike_object():\n    env = Mock()\n    env.ref_context = {'py:module': 'module'}\n    env.ref_context['py:classes'] = ['class']\n    domain = PythonDomain()\n    obj = PyClasslike(option_spec={})\n    signode = desc_signature('classmethod method', '')\n    obj.handle_signature('classmethod method(self)', signode)\n    fullname, prefix = obj.handle_signature('classmethod method(self)', signode)\n    assert fullname == 'method'\n    assert prefix == 'classmethod '\n", "def test_module_index_entry():\n    # Test creating module index entry\n    env = Mock()\n    env.config['modindex_common_prefix'] = []\n    domain = PythonDomain()\n    domain.env = env\n    domain.data['modules'] = {}\n\n    modname = 'module.name'\n    docname = 'docname'\n    node_id = 'node_id'\n    synopsis = 'synopsis'\n    platforms = 'platforms'\n    deprecated = True\n\n    module_entry = ModuleEntry(docname, node_id, synopsis, platforms, deprecated)\n    domain.note_module(modname, node_id, synopsis, platforms, deprecated)\n\n    index_entry = IndexEntry(modname, 1, docname, node_id, platforms, '', synopsis)\n    assert index_entry in domain.get_objects()\n\n    # Test adding entry to module index\n    index = PythonModuleIndex(domain)\n    domain.data['modules'] = {}\n\n    index_entry = IndexEntry(modname, 1, docname, node_id, platforms, '', synopsis)\n    domain.note_module(modname, node_id, synopsis, platforms, deprecated)\n\n    index_data, collapse = index.generate(docnames=[docname])\n    assert len(index_data) == 1\n    assert index_data[0][0].startswith(modname[0])\n    assert index_data[0][1][0].docname == docname\n    assert index_data[0][1][0].node_id == node_id\n    assert index_data[0][1][0].synopsis == synopsis\n    assert index_data[0][1][0].platforms == platforms\n    assert index_data[0][1][0].deprecated == deprecated\n\n    # Test module index with common prefix\n    env.config['modindex_common_prefix'] = ['prefix.']\n    index = PythonModuleIndex(domain)\n    index_data, collapse = index.generate(docnames=[docname])\n    assert len(index_data) == 1\n    assert index_data[0][0] == 'm'\n    assert index_data[0][1][0].docname == docname\n    assert index_data[0][1][0].node_id == node_id\n    assert index_data[0][1][0].synopsis == synopsis\n    assert index_data[0][1][0].platforms == platforms\n    assert index_data[0][1][0].deprecated == deprecated\n"], "sample_516": ["compilation error", "def test_pdf_alpha_value():\n    import datetime\n    import decimal\n    import io\n    import os\n    from pathlib import Path\n    from tempfile import NamedTemporaryFile\n\n    import numpy as np\n    import pytest\n\n    import matplotlib as mpl\n    from matplotlib import pyplot as plt, rcParams\n    from matplotlib.cbook import _get_data_path\n    from matplotlib.ft2font import FT2Font\n    from matplotlib.font_manager import findfont, FontProperties\n    from matplotlib.backends._backend_pdf_ps import get_glyphs_subset\n    from matplotlib.backends.backend_pdf import PdfPages\n    from matplotlib.patches import Rectangle\n    from matplotlib.testing.decorators import check_figures_equal, image_comparison\n    from matplotlib.testing._markers import needs_usetex\n\n    rcParams['pdf.fonttype'] = 42\n\n        fig, ax = plt.subplots()\n        ax.add_patch(Rectangle((0.1, 0.1), 0.2, 0.2, facecolor=(1, 0, 0, 0.5),\n                              edgecolor=(1, 0, 0, 1), linewidth=2))\n        with PdfPages('test_alpha.pdf') as pdf:\n            pdf.savefig(fig)\n\n    test_alpha()\n\n        fig, ax = plt.subplots()\n        ax.add_patch(Rectangle((0.1, 0.1), 0.2, 0.2, facecolor=(1, 0, 0, 0.5),\n                              edgecolor=(1, 0, 0, 1), linewidth=2))\n        with PdfPages('test_alpha_3.pdf', metadata={'Creator': 'Test'}) as pdf:\n            pdf.savefig(fig)\n\n    test_alpha_3()\n\n        fig, ax = plt.subplots()\n        ax.add_patch(Rectangle((0.1, 0.1), 0.2, 0.2, facecolor=(1, 0, 0, 0.5),\n                              edgecolor=(1, 0, 0, 1), linewidth=2))\n        with PdfPages('test_alpha_4.pdf', metadata={'Creator': 'Test', 'Author': 'Test Author'}) as pdf:\n            pdf.savefig(fig)\n\n    test_alpha_4()\n\n        fig, ax = plt.subplots()\n        ax.add_patch(Rectangle((0.", "def test_gouraud_triangle_shape():\n    pdf = PdfPages('test_gouraud_triangle.pdf', metadata={'Title': 'Test Gouraud Triangle'})\n    fig, ax = plt.subplots()\n    points = np.array([[0, 0], [1, 0], [1, 1]])\n    colors = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    ax.add_patch(Rectangle((0, 0), 2, 1, facecolor=[0.5, 0.5, 0.5]))\n    ax.add_patch(Rectangle((1, 1), 1, 1, facecolor=[0.5, 0.5, 0.5]))\n    ax.add_patch(Rectangle((0, 1), 1, 1, facecolor=[0.5, 0.5, 0.5]))\n    pdf.savefig(fig)\n    pdf.close()\n    pdf_path = Path('test_gouraud_triangle.pdf')\n    assert os.path.getsize(pdf_path) > 0\n    with NamedTemporaryFile('w') as f:\n        f.write('%s\\n' % pdf_path)\n        f.flush()\n        assert os.path.getsize(f.name) == 0\n", "def test_draw_gouraud_triangle_alpha():\n    # Tests for the draw_gouraud_triangle method in the RendererPdf class\n\n    # Create a simple figure with a Gouraud triangle\n    fig, ax = plt.subplots()\n    x, y = np.meshgrid(np.linspace(0, 1, 100), np.linspace(0, 1, 100))\n    z = np.sin(x * np.pi) * np.cos(y * np.pi)\n    im = ax.imshow(z, cmap='hot', alpha=0.5)\n\n    # Create a PdfPages object to save the figure\n    with PdfPages('test.pdf') as pdf:\n        pdf.savefig(fig)\n\n    # Check that the Gouraud triangle is correctly rendered in the PDF\n    # file\n    with open('test.pdf', 'rb') as f:\n        data = f.read()\n    assert b'GT0' in data  # Check that the Gouraud triangle object is created\n    assert b'ShadingType 4' in data  # Check that the shading type is correct\n    assert b'BitsPerCoordinate 32' in data  # Check that the bit depth is correct\n    assert b'BitsPerComponent 8' in data  # Check that the bits per component are correct\n    assert b'ColorSpace DeviceRGB' in data  # Check that the color space is correct\n    assert b'AntiAlias False' in data  # Check that anti-aliasing is disabled\n    assert b'Decode' in data  # Check that the decode parameters are correct\n    assert b'flags 0' in data  # Check that the flags are correct\n    assert b'points' in data  # Check that the points are correct\n    assert b'colors' in data  # Check that the colors are correct\n    assert b'endstream' in data  # Check that the stream ends correctly\n\n    # Clean up\n    os.remove('test.pdf')\n", "def generate_dummy_pdf(file):\n    from matplotlib.backends.backend_pdf import PdfPages\n    from matplotlib.pyplot import figure\n    with PdfPages(file.name) as pdf:\n        fig = figure()\n        ax = fig.add_subplot(111)\n        ax.add_patch(Rectangle((0.1, 0.1), 0.8, 0.8, facecolor='blue', alpha=0.5))\n        pdf.savefig(fig)\n"], "sample_173": ["    def test_window_frame_start(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(ops.window_frame_start(1), '1 PRECEDING')\n        self.assertEqual(ops.window_frame_start(0), 'CURRENT ROW')\n        self.assertEqual(ops.window_frame_start(-1), '-1 PRECEDING')\n        self.assertEqual(ops.window_frame_start(None), 'UNBOUNDED PRECEDING')\n        with self.assertRaises(ValueError):\n            ops.window_frame_start('a')\n        with self.assertRaises(ValueError):\n            ops.window_frame_start(1.5)\n", "    def test_date_trunc_sql(self):\n        # Test the date_trunc_sql method\n        with self.assertRaises(NotImplementedError) as cm:\n            self.ops.date_trunc_sql('year', 'my_field')\n        self.assertIn('may require a date_trunc_sql() method', str(cm.exception))\n", "    def test_integer_field_range(self):\n        self.assertEqual(\n            self.ops.integer_field_range('PositiveIntegerField'), (0, 2147483647)\n        )\n        self.assertEqual(\n            self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647)\n        )\n        self.assertEqual(\n            self.ops.integer_field_range('SmallIntegerField'), (-32768, 32767)\n        )\n        self.assertEqual(\n            self.ops.integer_field_range('BigIntegerField'), (-9223372036854775808, 9223372036854775807)\n        )\n", "    def test_no_limit_value(self):\n        \"\"\"\n        Test the no_limit_value() method\n        \"\"\"\n        with self.assertRaises(NotImplementedError) as cm:\n            self.ops.no_limit_value()\n\n        self.assertEqual(\n            str(cm.exception),\n            'subclasses of BaseDatabaseOperations may require a no_limit_value() method'\n        )\n", "    def test_integer_field_ranges(self):\n        # Test that the integer field ranges are correctly set for each field type\n        for field_type, (min_value, max_value) in self.ops.integer_field_ranges.items():\n            self.assertGreaterEqual(min_value, 0)\n            self.assertLessEqual(max_value, 2147483647)  # Assuming IntegerField max value\n            self.assertEqual(self.ops.integer_field_range(field_type), (min_value, max_value))\n"], "sample_91": ["    def test_permission_denied_template_not_found(self):\n        request = self.request_factory.get('/nonexistent_url/', follow=True)\n        response = permission_denied(request, Exception())\n        self.assertEqual(response.status_code, 403)\n        self.assertEqual(response.content_type, 'text/html')\n        self.assertIn('403 Forbidden', response.content)\n", "    def test_server_error(self):\n        request = self.request_factory.post('/nonexistent_url/')\n        response = server_error(request, template_name='custom_500.html')\n        self.assertEqual(response.status_code, 500)\n        self.assertTemplateUsed(response, 'custom_500.html')\n", "    def test_server_error_with_custom_template(self):\n        template_name = 'custom_500.html'\n        response = server_error(self.request_factory.get('/'), template_name=template_name)\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response.content_type, 'text/html')\n        self.assertTemplateUsed(response, template_name)\n", "    def test_template_not_found(self):\n        response = page_not_found(self.request_factory.get('/nonexistent_url/'))\n        self.assertEqual(response.status_code, 404)\n        self.assertIsInstance(response, HttpResponseNotFound)\n\n        response = server_error(self.request_factory.get('/nonexistent_url/'))\n        self.assertEqual(response.status_code, 500)\n        self.assertIsInstance(response, HttpResponseServerError)\n\n        response = bad_request(self.request_factory.get('/nonexistent_url/'))\n        self.assertEqual(response.status_code, 400)\n        self.assertIsInstance(response, HttpResponseBadRequest)\n\n        response = permission_denied(self.request_factory.get('/nonexistent_url/'))\n        self.assertEqual(response.status_code, 403)\n        self.assertIsInstance(response, HttpResponseForbidden)\n", "    def test_server_error_template_rendering(self):\n        \"\"\"Test server error page is rendered with proper template\"\"\"\n        response = server_error(self.request_factory.get('/nonexistent_url/'))\n        self.assertEqual(response.status_code, 500)\n        self.assertEqual(response.content_type, 'text/html')\n        self.assertTemplateUsed(response, '500.html')\n"], "sample_1024": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_negativeone_power_complex():\n    assert -S.One**2 == 1\n    assert -S.One**S.Complex('2, 2') == Float('-1.4142135623730951', 53)\n"], "sample_171": ["    def test_migrate_command_with_multiple_databases(self):\n        call_command('migrate', '--database=other', '--database=default')\n        self.assertEqual(len(connections['other'].cursor()), 3)\n        self.assertEqual(len(connections['default'].cursor()), 3)\n", "    def test_migrate_command_with_fake_options(self):\n        out = io.StringIO()\n        err = io.StringIO()\n        sys.stdout = out\n        sys.stderr = err\n\n        # Test that the migrations are marked as fake when using the --fake option.\n        call_command('migrate', '--fake', '--database', 'default')\n\n        sys.stdout = sys.__stdout__\n        sys.stderr = sys.__stderr__\n\n        self.assertIn('Applied fake migration: ', out.getvalue())\n\n        # Test that the migration history is updated correctly when using the --fake option.\n        recorder = MigrationRecorder('default')\n        self.assertEqual(recorder.applied_migrations(), ['0001_initial'])\n        self.assertEqual(recorder.applied_migrations(verbosity=0), ['0001_initial'])\n\n        # Test that the migration history is not updated when using the --fake option with --dry-run.\n        out = io.StringIO()\n        err = io.StringIO()\n        sys.stdout = out\n        sys.stderr = err\n        call_command('migrate', '--fake', '--dry-run', '--database', 'default')\n        sys.stdout = sys.__stdout__\n        sys.stderr = sys.__stderr__\n        self.assertEqual(recorder.applied_migrations(), [])\n\n        # Test that the migration history is updated when using the --fake-initial option.\n        out = io.StringIO()\n        err = io.StringIO()\n        sys.stdout = out\n        sys.stderr = err\n        call_command('migrate', '--fake-initial', '--database', 'default')\n        sys.stdout = sys.__stdout__\n        sys.stderr = sys.__stderr__\n        self.assertEqual(recorder.applied_migrations(), ['0001_initial'])\n\n        # Test that the migration history is updated correctly when using the --fake-initial option with --dry-run.\n        out = io.StringIO()\n        err = io.StringIO()\n        sys.stdout = out\n        sys.stderr = err\n        call_command('migrate', '--fake-initial', '--dry-run', '--database', 'default')\n        sys.stdout = sys.__stdout__\n        sys.stderr = sys.__stderr__\n        self.assertEqual(recorder.applied_migrations(), [])\n\n        # Test that the migration history is updated correctly when using the --fake option with multiple databases.\n        out = io.StringIO()\n        err = io.StringIO()\n       ", "    def test_migration_plan_with_conflicting_migrations(self):\n        \"\"\"\n        Tests that migration_plan() raises an error if there are conflicting migrations.\n        \"\"\"\n        # Create a test app with conflicting migrations\n        self.create_test_app('conflicting_migrations', [\n            ('0001_initial.py', '0001_initial.py', 'initial migration'),\n            ('0002_conflict.py', '0002_conflict.py', 'conflicting migration'),\n            ('0002_conflict.py', '0002_conflict.py', 'conflicting migration'),\n        ])\n\n        # Run migrate with --plan option\n        with self.assertRaises(CommandError) as cm:\n            call_command('migrate', '--plan', '--database', 'default')\n        self.assertEqual(\n            str(cm.exception),\n            \"Conflicting migrations detected; multiple leaf nodes in the migration graph: (conflicting_migrations in conflicting_migrations).\\nTo fix them run 'python manage.py makemigrations --merge'\"\n        )\n", "    def test_migrate_plan_empty(self):\n        \"\"\"Test that migrate with --plan does not output anything when there are no migrations to apply.\"\"\"\n        output = io.StringIO()\n        with mock.patch.object(sys.stdout, 'write') as mock_write:\n            with self.assertRaises(SystemExit):\n                call_command('migrate', '--plan', stdout=output)\n        self.assertEqual(mock_write.call_count, 0)\n", "    def test_migrate_command_conflicting_migrations(self):\n        \"\"\"\n        Regression test for the `Conflicting migrations detected; multiple leaf nodes in the migration graph`\n        error message.\n        \"\"\"\n        with self.assertRaises(CommandError):\n            call_command('migrate', '--database=other', 'test_app')\n        self.assertEqual(self.stdout.getvalue(), \"Conflicting migrations detected; multiple leaf nodes in the migration graph: (test_app1 in test_app; test_app2 in test_app).\\nTo fix them run 'python manage.py makemigrations --merge'\\n\")\n"], "sample_335": ["    def test_field_max_length(self):\n        field = CharField(max_length=10)\n        with self.assertRaises(ValidationError):\n            field.clean('a' * 11)\n", "    def test_slug_field_default_validators(self):\n        field = SlugField()\n        self.assertEqual(len(field.default_validators), 1)\n        self.assertIsInstance(field.default_validators[0], validators.validate_slug)\n", "    def test_to_python_with_max_digits(self):\n        field = DecimalField(max_digits=5)\n        value = '12345'\n        self.assertEqual(field.to_python(value), decimal.Decimal(value))\n", "    def test_to_python_empty_value(self):\n        field = DateTimeField()\n        self.assertIsNone(field.to_python(''))\n        self.assertIsNone(field.to_python(None))\n        self.assertIsNone(field.to_python(Decimal('0')))\n        self.assertIsNone(field.to_python('0'))\n        self.assertIsNone(field.to_python(''))\n        self.assertIsNone(field.to_python(None))\n", "compilation error"], "sample_777": ["def test_gradient_boosting_regressor_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=10,\n                                   random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    assert_array_almost_equal(reg.predict(T), true_result)\n    assert_equal(10, len(reg.estimators_))\n\n    mse_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(mse_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n", "def test_gradient_boosting_regressor():\n    # Check regression on the california housing dataset\n    estimator = GradientBoostingRegressor(loss='ls', n_estimators=10,\n                                        random_state=1)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        test_size=0.1,\n                                                        random_state=42)\n    estimator.fit(X_train, y_train)\n    y_pred = estimator.predict(X_test)\n    assert_greater(mean_squared_error(y_test, y_pred), 0.0)\n\n    # Check regression on the iris dataset\n    X_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                        iris.target,\n                                                        test_size=0.1,\n                                                        random_state=42)\n    estimator.fit(X_train, y_train)\n    y_pred = estimator.predict(X_test)\n    assert_greater(mean_squared_error(y_test, y_pred), 0.0)\n\n    # Check regression on a toy dataset\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    estimator = GradientBoostingRegressor(loss='ls', n_estimators=10,\n                                        random_state=1)\n    estimator.fit(X, y)\n    y_pred = estimator.predict(X)\n    assert_greater(mean_squared_error(y, y_pred), 0.0)\n\n    # Check regression with early stopping on the california housing dataset\n    estimator = GradientBoostingRegressor(loss='ls', n_estimators=10,\n                                        random_state=1, validation_fraction=0.2)\n    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n                                                        boston.target,\n                                                        test_size=0.1,\n                                                        random_state=42)\n    estimator.fit(X_train, y_train)\n    y_pred = estimator.predict(X_test)\n    assert_greater(mean_squared_error(y_test, y_pred), 0.0)\n\n    # Check regression with early stopping on the iris dataset\n    X_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                        iris.target,\n                                                        test_size=0.1,\n                                                        random_state=42)\n    estimator.fit(X_train, y_train)\n    y_pred = estimator.predict(X_test)\n    assert_greater(mean_squared_error", "def test_gradient_boosting_classifier_predict_proba(presort, loss):\n    # Check classification on a toy dataset.\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    T = [[-1, -1], [2, 2], [3, 2]]\n    true_result = [-1, 1, 1]\n\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict_proba(T), [[0.6666666666666666, 0.3333333333333333],\n                                                    [0.0, 1.0],\n                                                    [0.0, 1.0]])\n    assert_array_almost_equal(clf.predict_log_proba(T), [[-0.4054651081081644, -1.0986122886681098],\n                                                       [-inf, -0.6931471805599453],\n                                                       [-inf, -0.6931471805599453]])\n\n    # Check that predict_proba raises NotFittedError when the estimator\n    # has not been fitted\n    assert_raise_message(NotFittedError, \"This estimator is not fitted yet\",\n                         clf.predict_proba, [[-1, -1]])\n\n    # Check that predict_log_proba raises NotFittedError when the estimator\n    # has not been fitted\n    assert_raise_message(NotFittedError, \"This estimator is not fitted yet\",\n                         clf.predict_log_proba, [[-1, -1]])\n", "def test_classification_gradient_boosting_loss_huber_quantile(presort, loss):\n    # Check classification on a toy dataset.\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n\n    # Check predict_proba method\n    proba = clf.predict_proba(T)\n    assert_equal(proba.shape, (3, 2))\n    assert_array_almost_equal(np.sum(proba, axis=1), np.ones(3))\n    assert_array_almost_equal(np.exp(proba), expit(proba))\n", "def test_grad_boosting_staged_predict_shape(n_samples, n_features):\n    # Check that the output shape of the staged decision function\n    # is correct for different shapes of X\n    rng = np.random.RandomState(0)\n    X = rng.rand(n_samples, n_features)\n    y = rng.rand(n_samples)\n    gbm = GradientBoostingRegressor(random_state=0)\n    gbm.fit(X, y)\n    expected_output_shape = (n_samples, 1)\n    # Staged decision function should return an iterator of arrays with shape\n    # expected_output_shape\n    for output in gbm.staged_decision_function(X):\n        assert output.shape == expected_output_shape\n\n    # For single stage, the output shape should be (n_samples,)\n    expected_output_shape = (n_samples,)\n    assert gbm.staged_decision_function(X).__next__().shape == expected_output_shape\n"], "sample_755": ["def test_check_number_of_labels():\n    with pytest.raises(ValueError):\n        check_number_of_labels(1, 10)\n\n    with pytest.raises(ValueError):\n        check_number_of_labels(10, 1)\n\n    check_number_of_labels(2, 10)\n\n    with pytest.raises(ValueError):\n        check_number_of_labels(3, 2)\n\n    with pytest.raises(ValueError):\n        check_number_of_labels(10, 10)\n", "def test_calinski_harabasz_score_cluster_size_1():\n    X, y = datasets.make_blobs(n_samples=10, centers=5, n_features=2)\n    with pytest.warns(UserWarning):\n        score = calinski_harabasz_score(X, y)\n        assert score != 0\n", "def test_silhouette_score_precomputed_distance_matrix():\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    labels = np.array([0, 0])\n    metric = \"precomputed\"\n    with assert_warns_message(UserWarning, 'The precomputed distance matrix contains non-zero '\n                             'elements on the diagonal. Use np.fill_diagonal(X, 0).'):\n        silhouette_score(X, labels, metric=metric)\n", "def test_calinski_harabasz_score_with_empty_labels():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.raises(ValueError):\n        calinski_harabasz_score(X, np.array([]))\n", "def test_silhouette_score_precomputed():\n    X = np.array([[0, 1, 2], [2, 0, 1], [1, 2, 0]])\n    labels = np.array([0, 0, 1])\n    expected_score = 0.7500000000000001\n    assert_array_equal(silhouette_score(X, labels, metric='precomputed'), expected_score)\n"], "sample_191": ["    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with self.assertRaises(StopIteration):\n            list(reloader.tick())\n", "    def setUp(self):\n        self.clear_autoreload_caches()\n", "    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, self.tempdir)\n", "    def test_iter_modules_and_files_invalid_module_names(self):\n        self.import_and_cleanup('invalid_module')\n        self.clear_autoreload_caches()\n        self.assertNotIn('invalid_module', list(autoreload.iter_all_python_module_files()))\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 1)\n", "    def test_tick(self):\n        reloader = autoreload.StatReloader()\n        with mock.patch.object(reloader, 'should_stop', return_value=False):\n            with mock.patch('time.sleep', side_effect=StopIteration):\n                reloader.tick()\n"], "sample_1084": ["    def test_ImageSet_and_ComplexRegion(self):\n        from sympy import S, ComplexRegion, ImageSet\n        expr = Lambda(t, 2*t)\n        c = ComplexRegion(Interval(1, 2), Interval(0, 2*pi))\n        i = ImageSet(expr, S.Reals)\n        result = intersection_sets(c, i)\n        assert result == ImageSet(expr, Interval(1, 2))\n", "def test_intersection_sets_ImageSet_Rationals():\n    from sympy.sets.fancysets import ImageSet\n    from sympy import Rational\n\n    # Test that the intersection of ImageSet with Rationals returns ImageSet\n    # with Rationals as base set\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert isinstance(intersection_sets(s, S.Rationals), ImageSet)\n\n    # Test that the intersection of ImageSet with Rationals returns ImageSet\n    # with Rationals as base set\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert isinstance(intersection_sets(s, S.Rationals), ImageSet)\n\n    # Test that the intersection of ImageSet with Rationals and then with\n    # Integers returns ImageSet with Integers as base set\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert isinstance(intersection_sets(s, S.Rationals).intersect(S.Integers), ImageSet)\n\n    # Test that the intersection of ImageSet with Rationals and then with\n    # Naturals returns ImageSet with Naturals as base set\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert isinstance(intersection_sets(s, S.Rationals).intersect(S.Naturals), ImageSet)\n\n    # Test that the intersection of ImageSet with Reals returns the original ImageSet\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert intersection_sets(s, S.Reals) == s\n\n    # Test that the intersection of ImageSet with Interval returns ImageSet\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert intersection_sets(s, Interval(1, 2)) == s\n\n    # Test that the intersection of ImageSet with EmptySet returns EmptySet\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert intersection_sets(s, S.EmptySet) == S.EmptySet\n\n    # Test that the intersection of ImageSet with UniversalSet returns the original ImageSet\n    f = Lambda(x, 1/x)\n    s = ImageSet(f, S.Integers)\n    assert intersection_sets(s, S.Universal", "def test_intersection_sets_ProductSet_with_ImageSet():\n    from sympy import Interval, ProductSet, ImageSet, Lambda\n    from sympy.abc import x, y\n\n    f = Lambda(x, x**2)\n    s = Interval(0, 2, left_open=False, right_open=False)\n    t = Interval(0, 4, left_open=False, right_open=False)\n    m = ProductSet(s, t)\n\n    expected = ProductSet(ImageSet(Lambda(x, x**2), s), t)\n    result = intersection_sets(m, ImageSet(f, m))\n    assert result == expected\n", "    def test_complex_region_intersection_with_real_interval(self):\n        a = ComplexRegion(FiniteSet(1, 2) * FiniteSet(3, 4), polar=False)\n        b = Interval(0, 1, left_open=False, right_open=False)\n        res = intersection_sets(a, b)\n        assert res == ComplexRegion(FiniteSet(1) * FiniteSet(3), polar=False)\n", "def test_intersection_sets_conditionset_conditionset():\n    from sympy import Interval, ConditionSet\n\n    # Check intersection of two ConditionSet instances\n    x = symbols('x')\n    cond1 = ConditionSet(x, x**2 + 1 > 0, S.Reals)\n    cond2 = ConditionSet(x, x**2 + 1 < 2, S.Reals)\n    assert isinstance(intersection_sets(cond1, cond2), ConditionSet)\n\n    # Check intersection of ConditionSet and ConditionSet with different conditions\n    cond1 = ConditionSet(x, x**2 + 1 > 0, S.Reals)\n    cond2 = ConditionSet(x, x**2 + 1 < 3, S.Reals)\n    assert isinstance(intersection_sets(cond1, cond2), ConditionSet)\n\n    # Check intersection of ConditionSet and ConditionSet with different base sets\n    cond1 = ConditionSet(x, x**2 + 1 > 0, S.Reals)\n    cond2 = ConditionSet(x, x**2 + 1 > 0, S.Naturals)\n    assert isinstance(intersection_sets(cond1, cond2), ConditionSet)\n\n    # Check intersection of ConditionSet and ConditionSet with one empty condition\n    cond1 = ConditionSet(x, x**2 + 1 > 0, S.Reals)\n    cond2 = ConditionSet(x, x**2 + 1 > 10, S.Reals)\n    assert isinstance(intersection_sets(cond1, cond2), ConditionSet)\n"], "sample_718": ["    def __init__(self, acceptable_key=0):\n        self._acceptable_key = acceptable_key\n        self.__private_key = 0\n", "    def __init__(self, attribute_name='foo'):\n        self.acceptable_key = 0\n        self.attribute_name = attribute_name\n", "    def test_warming_raised(self):\n        \"\"\"Check if estimators properly handle warnings during fit.\"\"\"\n        # Test that warnings are raised when deprecated parameters are used.\n        X, y = make_blobs(random_state=0, n_samples=9, n_features=4)\n        # some want non-negative input\n        X -= X.min()\n        if name == 'PowerTransformer':\n            # Box-Cox requires positive, non-zero data\n            X += 1\n        X = pairwise_estimator_convert_X(X, estimator_orig, kernel=rbf_kernel)\n        estimator = clone(estimator_orig)\n        y = multioutput_estimator_convert_y_2d(estimator, y)\n\n        set_random_state(estimator)\n\n        # Make a physical copy of the original estimator parameters before fitting.\n        params = estimator.get_params()\n        original_params = deepcopy(params)\n\n        # Fit the model with deprecated parameters\n        estimator.fit(X, y, verbose=2)  # Use verbose to trigger warnings\n\n        # Compare the state of the model parameters with the original parameters\n        new_params = estimator.get_params()\n        for param_name, original_value in original_params.items():\n            new_value = new_params[param_name]\n\n            # We should never change or mutate the internal state of input\n            # parameters by default. To check this we use the joblib.hash function\n            # that introspects recursively any subobjects to compute a checksum.\n            # The only exception to this rule of immutable constructor parameters\n            # is possible RandomState instance but in this check we explicitly\n            # fixed the random_state params recursively to be integer seeds.\n            assert_equal(hash(new_value), hash(original_value),\n                         \"Estimator %s should not change or mutate \"\n                         \" the parameter %s from %s to %s during fit.\"\n                         % (name, param_name, original_value, new_value))\n\n        # Test that warnings were raised\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\", UserWarning)\n            estimator.fit(X, y)\n        assert_greater(len(w), 0, \"Expected warnings to be raised.\")\n", "    def fit(self, X, y, sample_weight=None):\n        # Convert data\n        X, y = check_X_y(X, y,\n                         accept_sparse=(\"csr\", \"csc\"),\n                         multi_output=True,\n                         y_numeric=True)\n        # Function is only called after we verify that sample_weight is list\n        if not isinstance(sample_weight, list):\n            raise ValueError(\"Estimator does not accept 'sample_weight' of type list\")\n        return self\n", "    def __init__(self):\n        self.public_attr = 0\n        self._private_attr = 0\n"], "sample_198": ["    def setUpTestData(cls):\n        cls.model = Model.objects.create(id=1)\n        cls.window = Window(expression=F('id'), partition_by=F('id'), order_by=F('id'), frame=RowRange(start=1, end=1))\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def test_expression_string_repr(self):\n        expr = F('name')\n        self.assertEqual(str(expr), \"F('name')\")\n", "    def test_raw_sql_as_sql(self):\n        # Test that RawSQL.as_sql() works correctly\n        sql = \"SELECT 'Hello, World!'\"\n        params = [\"Hello, World!\"]\n        self.assertEqual(RawSQL(sql, params).as_sql(mock.Mock()), (sql, params))\n", "    def setUpTestData(cls):\n        cls.manager = Manager.objects.create(firstname='John', lastname='Doe', salary=40)\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=cls.manager\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=cls.manager\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n        cls.simulation = SimulationRun.objects.create(\n            name='Test simulation',\n            simulation_date=datetime.date(2022, 1, 1),\n        )\n        cls.results = Result.objects.bulk_create([\n            Result(simulation_run=cls.simulation, result=1, created_at=datetime.date(2022, 1, 2)),\n            Result(simulation_run=cls.simulation, result=2, created_at=datetime.date(2022, 1, 3)),\n            Result(simulation_run=cls.simulation, result=3, created_at=datetime.date(2022, 1, 4)),\n        ])\n"], "sample_1032": ["def test_cbrt_subtract():\n    x = Symbol('x')\n    expected = x**(1/3) - 1\n    assert cbrt(x) - 1 == expected\n", "    def test_min_max_real(self):\n        x = Symbol('x', real=True)\n        # Testing Min and Max with real values\n        assert Min(x, 2).evalf() == -2\n        assert Max(x, 2).evalf() == 2\n        assert Min(x, 2, x).evalf() == 2\n        assert Max(x, 2, x).evalf() == x\n", "    def test_real_root_positive_value(self):\n        x = Symbol('x', positive=True)\n        self.assertEqual(real_root(x**2, 2), x)\n", "def test_Mindiff():\n    from sympy import Min, Symbol\n    from sympy.abc import x\n    from sympy import Heaviside\n    from sympy.functions.elementary.integers import floor\n\n    assert Min(x, x).fdiff(1) == 0\n    assert Min(x, 0).fdiff(1) == -Heaviside(x)\n    assert Min(0, x).fdiff(1) == Heaviside(x)\n    assert Min(x, 0, 1).fdiff(1) == -Heaviside(x)\n    assert Min(0, x, 1).fdiff(1) == Heaviside(x)\n    assert Min(x, 0, 1, 2).fdiff(1) == -Heaviside(x)\n    assert Min(0, x, 1, 2).fdiff(1) == Heaviside(x)\n", "def test_real_root_unit_test():\n    from sympy import real_root, S, I, Symbol\n    x = Symbol('x')\n    n = Symbol('n')\n    a = Symbol('a', positive=True)\n    b = Symbol('b', negative=True)\n\n    assert real_root(x**2).simplify() == x\n    assert real_root(-a**2).simplify() == -a\n    assert real_root(b**2).simplify() == -b\n    assert real_root(-a**3).simplify() == -a\n    assert real_root(b**3).simplify() == -b\n    assert real_root(-a**(4*n)).simplify() == -a**(4*n)\n    assert real_root(b**(4*n)).simplify() == -b**(4*n)\n\n    assert real_root(-a**4).simplify() == a**2\n    assert real_root(b**4).simplify() == -b**2\n    assert real_root(-a**5).simplify() == a\n    assert real_root(b**5).simplify() == -b\n    assert real_root(-a**(6*n)).simplify() == a**(6*n)\n    assert real_root(b**(6*n)).simplify() == -b**(6*n)\n\n    assert real_root(a**3 - b**2).simplify() == a\n    assert real_root(b**3 - a**2).simplify() == -b\n    assert real_root(a**4 - b**4).simplify() == a**2\n    assert real_root(b**4 - a**4).simplify() == -b**2\n    assert real_root(a**6 - b**6).simplify() == a**3\n    assert real_root(b**6 - a**6).simplify() == -b**3\n\n    # Test with non-symbolic numbers\n    assert real_root(4).simplify() == 2\n    assert real_root(-4).simplify() == -2\n    assert real_root(9).simplify() == 3\n    assert real_root(-9).simplify() == -3\n    assert real_root(16).simplify() == 4\n    assert real_root(-16).simplify() == -4\n    assert real_root(25).simplify() == 5\n    assert real_root(-25).simplify() =="], "sample_1054": ["def test_ComplexRegion_polar_form_normalization():\n    from sympy.sets.fancysets import ComplexRegion, normalize_theta_set\n    from sympy import Interval, Union, S, pi\n    r = Interval(0, 1)\n    theta = Interval(0, 2*pi)\n    c1 = ComplexRegion(r*theta, polar=True)\n    c2 = ComplexRegion(Union(r*theta, r*theta), polar=True)\n    c3 = ComplexRegion(r*theta, polar=True)\n    c4 = ComplexRegion(Union(Interval(0, 1), r*theta), polar=True)\n    assert c1.polar\n    assert c2.polar\n    assert c3.polar\n    assert c4.polar\n    assert normalize_theta_set(c1.sets.args[1]) == c1.sets.args[1]\n    assert normalize_theta_set(c2.sets.args[1]) == c2.sets.args[1]\n    assert normalize_theta_set(c3.sets.args[1]) == c3.sets.args[1]\n    assert normalize_theta_set(c4.sets.args[1]) == c4.sets.args[1]\n", "def test_ImageSet_iter():\n    from sympy.sets.fancysets import ImageSet\n\n    a, b = S.Naturals0, S.Naturals0\n    x = symbols('x')\n\n    f = Lambda(x, x**2)\n    im = ImageSet(f, a*b)\n\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 4\n    assert next(gen) == 9\n    assert next(gen) == 16\n\n    f = Lambda(x, x**3)\n    im = ImageSet(f, a*b)\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 8\n    assert next(gen) == 27\n\n    f = Lambda(x, x**4)\n    im = ImageSet(f, a*b)\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 16\n    assert next(gen) == 81\n\n    f = Lambda(x, x**5)\n    im = ImageSet(f, a*b)\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 32\n    assert next(gen) == 243\n\n    f = Lambda(x, x**2 + x)\n    im = ImageSet(f, a*b)\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 6\n    assert next(gen) == 13\n\n    f = Lambda(x, x**2 - x)\n    im = ImageSet(f, a*b)\n    gen = iter(im)\n    assert next(gen) == 0\n    assert next(gen) == 1\n    assert next(gen) == 2\n    assert next(gen) == 5\n", "def test_ComplexRegion_properties():\n    x, y = symbols('x y')\n    R = Range(1, 3)\n    I = Interval(1, 3)\n    C = ComplexRegion(I*I, polar=False)\n    assert C.is_ComplexRegion is True\n    assert C.is_subset(S.Reals) is False\n    assert C._measure == 9\n    assert C.a_interval == I\n    assert C.b_interval == I\n    assert C.polar is False\n\n    C1 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    assert C1.polar is True\n\n    C2 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=True)\n    assert C2._measure == pi\n\n    C3 = ComplexRegion(Interval(0, 1)*Interval(0, pi), polar=False)\n    assert C3.polar is False\n\n    C4 = ComplexRegion(R*R, polar=False)\n    assert C4.polar is False\n    assert C4._measure == 4\n    assert C4.a_interval == R\n    assert C4.b_interval == R\n    assert C4.polar is False\n\n    C5 = ComplexRegion(R*I, polar=False)\n    assert C5.polar is False\n\n    C6 = ComplexRegion(I*R, polar=False)\n    assert C6.polar is False\n\n    C7 = ComplexRegion(ComplexRegion(I*R, polar=False), polar=True)\n    assert C7.polar is True\n    assert C7._measure == 3\n    assert C7.a_interval == I\n    assert C7.b_interval == R\n    assert C7.polar is True\n\n    C8 = ComplexRegion(ComplexRegion(I*R, polar=True), polar=False)\n    assert C8.polar is False\n    assert C8._measure == 3\n    assert C8.a_interval == I\n    assert C8.b_interval == R\n    assert C8.polar is False\n\n    C9 = ComplexRegion(ComplexRegion(I*R, polar=True), polar=True)\n    assert C9.polar is True\n    assert C9._measure == 3*pi\n    assert C9.a_interval == I\n    assert C9.b_interval == R\n    assert C9.polar is True\n", "def test_ImageSet_iterable():\n    # Test that an ImageSet can be iterated over\n    from sympy.sets.setexpr import SetExpr\n    x = Symbol('x')\n    # A simple ImageSet over Naturals\n    img = ImageSet(Lambda(x, x**2), S.Naturals)\n    # Check the first few elements of the ImageSet\n    for i in range(10):\n        assert img.next() == i**2\n    # Test that we can iterate over an ImageSet with a large number of elements\n    img = ImageSet(Lambda(x, x**2), S.Naturals0)\n    for i in range(10):\n        assert img.next() == i**2\n    # Test that we can iterate over a multivariate ImageSet\n    img = ImageSet(Lambda(x, x**2 + y**2), S.Naturals0, y)\n    for i in range(10):\n        for j in range(10):\n            assert img.next() == (i**2 + j**2)\n", "def test_ImageSet():\n    x = Symbol('x')\n    y = Symbol('y')\n    N = S.Naturals\n    N0 = S.Naturals0\n    I = S.Integers\n    Re = S.Reals\n\n    # test ImageSet with single variable\n    squares = ImageSet(Lambda(x, x**2), N)\n    assert squares.is_iterable\n    assert 1 in squares\n    assert 4 in squares\n    assert 5 not in squares\n    assert squares.measure == S.Infinity\n\n    # test ImageSet with multiple variables\n    squares = ImageSet(Lambda((x, y), x**2 + y**2), N*N)\n    assert squares.is_iterable\n    assert 1 in squares\n    assert 4 in squares\n    assert 5 not in squares\n    assert squares.measure == S.Infinity\n\n    # test ImageSet with finite base set\n    squares = ImageSet(Lambda(x, x**2), FiniteSet(1, 2, 3))\n    assert not squares.is_iterable\n    assert 1 in squares\n    assert 4 in squares\n    assert 5 not in squares\n    assert squares.measure == 3\n\n    # test ImageSet with complex numbers\n    c1 = Complexes\n    c2 = ComplexRegion(I*I, polar=True)\n    c3 = ImageSet(Lambda((x, y), x + y*I), c1)\n    assert c3.is_iterable\n    assert 1 + I in c3\n    assert 3 + 2*I in c3\n    assert 4 + 5*I not in c3\n\n    # test ImageSet with symbols as arguments\n    x = Symbol('x')\n    y = Symbol('y')\n    squares = ImageSet(Lambda((x, y), x**2 + y**2), N*N)\n    assert squares.is_iterable\n    assert 1 in squares\n    assert 4 in squares\n    assert 5 not in squares\n\n    # test ImageSet with more complex expression as argument\n    from sympy import sin, cos\n    arg = sin(x) + cos(y)\n    ImageSet(Lambda((x, y), arg), N*N)\n\n    # test ImageSet with intersection\n    squares = ImageSet(Lambda(x, x**2), N)\n    squares_intersect_interval = squares.intersect(Interval("], "sample_481": ["    def test_floatformat(self):\n        self.assertEqual(self.render_template(\"floatformat01\"), \"34.2\")\n        self.assertEqual(self.render_template(\"floatformat02\"), \"34\")\n        self.assertEqual(self.render_template(\"floatformat03\"), \"34.3\")\n        self.assertEqual(self.render_template(\"floatformat04\"), \"34.232\")\n        self.assertEqual(self.render_template(\"floatformat05\"), \"34\")\n        self.assertEqual(self.render_template(\"floatformat06\"), \"34.3\")\n        self.assertEqual(self.render_template(\"floatformat07\"), \"34.232\")\n        self.assertEqual(self.render_template(\"floatformat08\"), \"34\")\n        self.assertEqual(self.render_template(\"floatformat09\"), \"34.3\")\n        self.assertEqual(self.render_template(\"floatformat10\"), \"6,666.67\")\n        self", "compilation error", "    def test_unordered_list(self):\n        self.assertHTMLEqual(selfunordered_list, '<li>State\\n<ul>\\n<li>Kansas\\n<ul>\\n<li>Lawrence</li>\\n<li>Topeka</li>\\n</ul>\\n</li>\\n<li>Illinois</li>\\n</ul>')\n", "    def test_floatformat(self):\n        # Test floatformat with and without argument\n        self.assertEqual(self.engine.get_template(\"floatformat01\").render({\"a\": 123.4567}), \"123.5\")\n        self.assertEqual(self.engine.get_template(\"floatformat02\").render({\"a\": 123.4567}), \"123.5\")\n        self.assertEqual(self.engine.get_template(\"floatformat03\").render({\"a\": 6666.6666}), \"6,666.67\")\n        self.assertEqual(self.engine.get_template(\"floatformat04\").render({\"a\": 66666.6666}), \"66666.67\")\n        self.assertEqual(self.engine.get_template(\"floatformat05\").render({\"a\": 123.4567}), \"123.5, 123.5\")\n        self.assertEqual(self.engine.get_template(\"floatformat06\").render({\"a\": 6666.6666}), \"6,666.67\")\n        self.assertEqual(self.engine.get_template(\"floatformat07\").render({\"a\": 66666.6666}), \"66666.67\")\n", "    def test_date(self):\n        self.assertEqual(self.join01, \"01, 2024\")\n        self.assertEqual(self.join02, \"01, 2024\")\n        self.assertEqual(self.join03, \"\")\n        self.assertEqual(self.join04, \"\")\n        self.assertEqual(self.join05, \"01, 01, 2024\")\n        self.assertEqual(self.join06, \"12:00\")\n        self.assertEqual(self.join07, \"01, 01, 2024, 12:00\")\n        self.assertEqual(self.join08, \"01, 01, 2024, 12:00:00\")\n"], "sample_137": ["    def test_replace_named_groups(self):\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/(\\w+)$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>/$')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/(\\w+)')\n        self.assertEqual(replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n", "    def test_parse_docstring_metadata(self):\n        # Test case with metadata\n        docstring = \"\"\"\n        (TITLE)\n\n        Display an individual :model:`myapp.MyModel`.\n\n        **Context**\n\n        ``RequestContext``\n\n        ``mymodel``\n            An instance of :model:`myapp.MyModel`.\n\n        **Template:**\n\n        :template:`myapp/my_template.html` (DESCRIPTION)\n\n        some_metadata: some data\n        \"\"\"\n        title, body, metadata = parse_docstring(docstring)\n        self.assertEqual(title, '(TITLE)')\n        self.assertEqual(metadata, {'description': 'DESCRIPTION'})\n        self.assertEqual(body, 'Display an individual :model:`myapp.MyModel`.\\n\\n**Context**\\n\\n`RequestContext`\\n\\n`mymodel`\\n    An instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n\\n:template:`myapp/my_template.html`\\n\\nsome_metadata: some data')\n", "    def test_parse_docstring_empty_docstring(self):\n        self.assertEqual(parse_docstring(''), ('', '', {}))\n", "    def test_empty_docstring(self):\n        \"\"\"\n        Test case for empty docstring.\n        \"\"\"\n        title, body, metadata = parse_docstring('')\n        self.assertEqual(title, '')\n        self.assertEqual(body, '')\n        self.assertEqual(metadata, {})\n", "    def test_replace_named_groups(self):\n        pattern = r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'\n        expected = '^<a>/b/<c>/$'\n        self.assertEqual(replace_named_groups(pattern), expected)\n"], "sample_692": ["def test_get_user(tmp_path: Path) -> None:\n    \"\"\"Test that get_user() returns None when getpass.getuser() fails.\"\"\"\n    with pytest.raises((ImportError, KeyError)):\n        get_user()\n    assert get_user() is not None\n    # Simulate a user name\n    os.environ[\"USER\"] = \"testuser\"\n    assert get_user() == \"testuser\"\n    # Test that get_user() does not modify the USER environment variable\n    assert os.environ[\"USER\"] == \"testuser\"\n    # Test that get_user() does not work when USER environment variable is not set\n    del os.environ[\"USER\"]\n    assert get_user() is None\n", "def test_get_user_no_user(monkeypatch):\n    # Test that get_user returns None when getuser() fails\n    monkeypatch.setattr('getpass.getuser', side_effect=ImportError)\n    assert get_user() is None\n", "def test_temp_path_factory__mktemp__raises_valueerror_if_basename_is_not_relative_path(tmp_path_factory: TempPathFactory) -> None:\n    basename = \"absolute/path/to/directory\"\n    with pytest.raises(ValueError):\n        tmp_path_factory.mktemp(basename)\n\n", "def test_TempPathFactory_from_config(caplog, tmp_path):\n    config = Config()\n    config.option.basetemp = tmp_path\n    factory = TempPathFactory.from_config(config, _ispytest=True)\n\n    assert factory._given_basetemp == tmp_path\n    assert factory.getbasetemp() == tmp_path\n", "def test_get_user(tmp_path: Path) -> None:\n    \"\"\"Test get_user() works in the presence of invalid user name.\"\"\"\n    with tmp_path.joinpath(\".env\").open(\"w\") as f:\n        f.write(\"LOGNAME=invalid_user\")\n    user = get_user()\n    assert user == \"unknown\"\n"], "sample_493": ["    def test_explain(self):\n        # Check that explain() works with no arguments\n        Publisher.objects.all().explain()\n        # Check that explain() works with format argument\n        Publisher.objects.all().explain(format=\"JSON\")\n        # Check that explain() works with format and options arguments\n        Publisher.objects.all().explain(format=\"JSON\", options=[\"ANALYZE\", \"io_stats\"])\n        # Check that explain() raises an error if the format is not supported\n        with self.assertRaises(ValueError):\n            Publisher.objects.all().explain(format=\"CSV\")\n        # Check that explain() raises an error if an invalid option is provided\n        with self.assertRaises(ValueError):\n            Publisher.objects.all().explain(format=\"JSON\", options=[\"invalid_option\"])\n", "    def test_subquery_with_aggregate_and_exists(self):\n        authors = Author.objects.annotate(\n            count_books=Count('book')\n        ).filter(count_books__gt=0)\n        self.assertEqual(authors.count(), 8)\n        self.assertEqual(\n            authors.values('name').annotate(count_books=Count('id')).values('name', 'count_books'),\n            [{'name': 'Adrian Holovaty', 'count_books': 2}, {'name': 'Jacob Kaplan-Moss', 'count_books': 1}, {'name': 'Brad Dayley', 'count_books': 1}, {'name': 'James Bennett', 'count_books': 1}, {'name': 'Jeffrey Forcier', 'count_books': 1}, {'name': 'Paul Bissex', 'count_books': 1}, {'name': 'Wesley J. Chun', 'count_books': 1}, {'name': 'Peter Norvig', 'count_books': 2}, {'name': 'Stuart Russell', 'count_books': 1}]\n        )\n", "    def test_aggregate_expression_usage_in_order_by(self):\n        results = Author.objects.annotate(\n            avg_age=Avg(\"age\"), max_age=Max(\"age\"), min_age=Min(\"age\")\n        ).order_by(\"max_age\", \"-min_age\", \"avg_age\")\n        self.assertEqual(results[0].avg_age, 29.0)\n        self.assertEqual(results[0].max_age, 57)\n        self.assertEqual(results[0].min_age, 25)\n", "    def test_explain_query(self):\n        # Test that the explain() method returns the correct query.\n        query = Book.objects.annotate(avg_rating=Avg('rating')).explain('T').explain(\n            format='JSON'\n        )\n        expected_output = [\n            \"SELECT JSON_EXTRACT(json_agg(\",\n            \"json_build_object('rating', avg_rating),\",\n            \"json_build_object('count', COUNT(rating))) AS avg_rating\",\n            \"FROM \" 'book' \"GROUP BY \" 'rating'\",\n            \")\"\n        ]\n        self.assertEqual(query, expected_output)\n", "    def test_aggregate_simple(self):\n        # Testing aggregate() with simple expressions\n        authors = Author.objects.annotate(\n            age_avg=Avg(\"age\"), age_sum=Sum(\"age\"), age_max=Max(\"age\")\n        )\n        self.assertEqual(\n            authors.values_list(\"age_avg\", \"age_sum\", \"age_max\"),\n            [(34.6, 313, 57)],\n        )\n"], "sample_754": ["def test_mini_batch_sparse_pca_init(self):\n    # Test that default values are used correctly\n    mbspca = MiniBatchSparsePCA()\n    assert_equal(mbspca.n_components, None)\n    assert_equal(mbspca.alpha, 1)\n    assert_equal(mbspca.ridge_alpha, 0.01)\n    assert_equal(mbspca.n_iter, 100)\n    assert_equal(mbspca.batch_size, 3)\n    assert_equal(mbspca.verbose, 0)\n    assert_equal(mbspca.shuffle, True)\n    assert_equal(mbspca.n_jobs, 1)\n    assert_equal(mbspca.method, 'lars')\n    assert_equal(mbspca.normalize_components, False)\n\n    # Test that non-default values are used correctly\n    mbspca = MiniBatchSparsePCA(n_components=10, alpha=2, ridge_alpha=0.1,\n                                 n_iter=500, batch_size=5, verbose=1,\n                                 shuffle=False, n_jobs=2, method='cd',\n                                 normalize_components=True)\n    assert_equal(mbspca.n_components, 10)\n    assert_equal(mbspca.alpha, 2)\n    assert_equal(mbspca.ridge_alpha, 0.1)\n    assert_equal(mbspca.n_iter, 500)\n    assert_equal(mbspca.batch_size, 5)\n    assert_equal(mbspca.verbose, 1)\n    assert_equal(mbspca.shuffle, False)\n    assert_equal(mbspca.n_jobs, 2)\n    assert_equal(mbspca.method, 'cd')\n    assert_equal(mbspca.normalize_components, True)\n", "def test_sparse_pca_ridge_alpha_transform(ridge_alpha, expected_ridge_alpha):\n    X = np.array([[1, 2], [3, 4]])\n    spca = SparsePCA(ridge_alpha=expected_ridge_alpha)\n    spca.fit(X)\n    result = spca.transform(X, ridge_alpha=ridge_alpha)\n    assert_array_almost_equal(result, spca.transform(X, ridge_alpha=None))\n", "def test_sparse_pca_update_components():\n    X, _, V = generate_toy_data(n_components=3, n_samples=100, image_size=(10, 10))\n    spca = SparsePCA(n_components=3)\n    spca.fit(X)\n    spca.n_components = 2\n    spca.fit(X)\n    assert_equal(spca.components_.shape[0], 2)\n", "def test_sparsepca_transform(X):\n    X = np.vstack([X, np.random.rand(10, 100), np.random.rand(20, 50)])\n    np.random.seed(0)\n    spca = SparsePCA(n_components=2, alpha=1, ridge_alpha=0.01)\n    spca.fit(X)\n    X_new1 = spca.transform(X[:10])\n    X_new2 = spca.transform(X[10:20])\n    X_new3 = spca.transform(X[20:])\n    assert_array_almost_equal(X_new1, spca.transform(X[:10]))\n    assert_array_almost_equal(X_new2, spca.transform(X[10:20]))\n    assert_array_almost_equal(X_new3, spca.transform(X[20:]))\n", "def test_mini_batch_sparse_pca_empty_array():\n    pca = MiniBatchSparsePCA(n_components=1)\n    X = np.array([])\n    with pytest.raises(ValueError):\n        pca.fit(X)\n"], "sample_885": ["def test_interval_constraint(constraint):\n    valid_val = generate_valid_param(constraint)\n    invalid_val = generate_invalid_param_val(constraint)\n\n    with pytest.raises(InvalidParameterError):\n        constraint.is_satisfied_by(invalid_val)\n\n    assert constraint.is_satisfied_by(valid_val)\n", "def test_make_constraint():\n    # Test an invalid constraint type\n    with pytest.raises(ValueError):\n        make_constraint(\"InvalidConstraint\")\n\n    # Test an Interval constraint\n    interval = Interval(Integral, 0, 10, closed=\"both\")\n    constraint = make_constraint(interval)\n    assert isinstance(constraint, Interval)\n    assert constraint.type == Integral\n    assert constraint.left == 0\n    assert constraint.right == 10\n    assert constraint.closed == \"both\"\n\n    # Test an Options constraint\n    options = StrOptions({\"a\", \"b\", \"c\"})\n    constraint = make_constraint(options)\n    assert isinstance(constraint, StrOptions)\n    assert constraint.options == {\"a\", \"b\", \"c\"}\n    assert constraint.deprecated == set()\n\n    # Test a _InstancesOf constraint\n    constraint = make_constraint(int)\n    assert isinstance(constraint, _InstancesOf)\n    assert constraint.type == int\n\n    # Test a _NoneConstraint\n    constraint = make_constraint(None)\n    assert isinstance(constraint, _NoneConstraint)\n\n    # Test a Hidden constraint\n    class _HiddenConstraint(Hidden):\n            super().__init__(\"hidden_constraint\")\n\n    constraint = make_constraint(_HiddenConstraint())\n    assert isinstance(constraint, _InstancesOf)\n    assert constraint.type == str\n    assert constraint.hidden == True\n\n    # Test a _RandomStates constraint\n    constraint = make_constraint(_RandomStates())\n    assert isinstance(constraint, _RandomStates)\n\n    # Test a _Booleans constraint\n    constraint = make_constraint(_Booleans())\n    assert isinstance(constraint, _Booleans)\n\n    # Test a _VerboseHelper constraint\n    constraint = make_constraint(_VerboseHelper())\n    assert isinstance(constraint, _VerboseHelper)\n\n    # Test a _MissingValues constraint\n    constraint = make_constraint(_MissingValues())\n    assert isinstance(constraint, _MissingValues)\n\n    # Test a HasMethods constraint\n    constraint = make_constraint(HasMethods(\"method\"))\n    assert isinstance(constraint, HasMethods)\n    assert constraint.methods == [\"method\"]\n\n    # Test a _IterablesNotString constraint\n    constraint = make_constraint(_IterablesNotString())\n    assert isinstance(constraint, _IterablesNotString)\n\n    # Test a _CVObjects constraint\n    constraint = make_constraint(_CVObjects())\n    assert isinstance(constraint, _CVObjects)\n\n    # Test a StrOptions constraint with deprecated options\n    options = StrOptions({\"", "def test_make_constraint():\n    # Test that all valid constraints are correctly converted\n    constraints = [\n        Interval(Integral, -1, 2, closed=\"left\"),\n        Interval(Real, -1, 2, closed=\"right\"),\n        Interval(Real, -1, 2, closed=\"both\"),\n        Interval(Real, -1, 2, closed=\"neither\"),\n        Interval(\"real_not_int\", -1, 2, closed=\"left\"),\n        _ArrayLikes(),\n        _SparseMatrices(),\n        _Callables(),\n        _RandomStates(),\n        _NoneConstraint(),\n        _InstancesOf(Integral),\n        _InstancesOf(Real),\n        _InstancesOf(np.ndarray),\n        _InstancesOf(np.random.RandomState),\n        _Booleans(),\n        _VerboseHelper(),\n        _MissingValues(),\n        Options(Integral, [1, 2, 3], deprecated={1}),\n        Options(Real, [1.0, 2.0, 3.0]),\n        StrOptions([\"a\", \"b\", \"c\"], deprecated={\"a\"}),\n        StrOptions([\"a\", \"b\", \"c\"]),\n        _IterablesNotString(),\n        _CVObjects(),\n        HasMethods([\"method1\", \"method2\"]),\n        Hidden(Interval(Integral, -1, 2, closed=\"left\")),\n    ]\n\n    for constraint in constraints:\n        make_constraint(constraint)\n\n    # Test that invalid constraints raise a ValueError\n    with pytest.raises(ValueError):\n        make_constraint(\"invalid_constraint\")\n    with pytest.raises(ValueError):\n        make_constraint(None)\n    with pytest.raises(ValueError):\n        make_constraint(lambda: None)\n", "def test_iterables_not_string():\n    with pytest.raises(InvalidParameterError):\n        _func(a=1, b=2, c=[1, 2, 3], d=\"hello\")\n\n    _func(a=1, b=2, c=[1, 2, 3], d=3)\n\n    _func(a=1, b=2, c=5, d=\"hello\")\n\n    _func(a=1, b=2, c=\"hello\", d=\"world\")\n\n    _func(a=1, b=2, c=\"hello\", d=[1, 2, 3])\n\n    _func(a=1, b=2, c=np.array([1, 2, 3]), d=[1, 2, 3])\n\n    with pytest.raises(InvalidParameterError):\n        _func(a=1, b=2, c=np.array([1, 2, 3]), d=\"hello\")\n\n    with pytest.raises(InvalidParameterError):\n        _func(a=1, b=2, c=\"hello\", d=np.array([1, 2, 3]))\n\n    with pytest.raises(InvalidParameterError):\n        _func(a=1, b=2, c=[1, 2, 3], d=np.array([1, 2, 3]))\n\n    # Test with _IterablesNotString constraint on method parameters\n    with pytest.raises(InvalidParameterError):\n        _Class()._method(a=1, b=2, c=\"hello\")\n\n    _Class()._method(a=1, b=2, c=3)\n\n    _Class()._method(a=1, b=2, c=\"hello\", d=3)\n\n    _Class()._method(a=1, b=2, c=[1, 2, 3])\n\n    _Class()._method(a=1, b=2, c=np.array([1, 2, 3]))\n\n    with pytest.raises(InvalidParameterError):\n        _Class()._method(a=1, b=2, c=np.array([1, 2, 3]), d=\"hello\")\n\n    with pytest.raises(InvalidParameterError):\n        _Class()._method(a=1, b=2, c=\"hello\", d=np.array([1, 2, 3]))\n\n    with pytest.raises(InvalidParameterError):\n        _Class()._method(a=1, b=2", "    def _interval_method(self, a, b):\n        \"\"\"A validated method with interval constraints.\"\"\"\n\n"], "sample_32": ["    def test_de_density_scale(self):\n        \"\"\"Tests the de_density_scale method of Flatw0wzCDM.\"\"\"\n        cosmo = self._cls(*self._cls_args, **self.cls_kwargs)\n        z = np.linspace(0, 1, 100)\n        I = cosmo.de_density_scale(z)\n        assert np.allclose(I, (1 + z)**(3 * (1 + cosmo.w0 - cosmo.wz)) * np.exp(3 * cosmo.wz * z))\n", "    def test_w0_default_value(self):\n        \"\"\"Test default value of w0.\"\"\"\n        cosmology = self._cls(*self._cls_args, **self._cls_kwargs)\n        self.assertAlmostEqual(cosmology.w0.value, -1.0)\n", "    def test_w0_init_value(self):\n        cosmo = self._cls(\n            _cls_args=self._cls_args + [1.0],\n            cls_kwargs=self.cls_kwargs,\n        )\n        assert cosmo.w0.value == 1.0\n", "compilation error", "    def test_w_evaluate(self):\n        cosmo = self._cls(*self._cls_args, **self.cls_kwargs)\n        z = 1.5\n        with pytest.raises(UserWarning, match=self._expected_msg):\n            cosmo.w(z)\n"], "sample_1121": ["def same_and_same_prec(a, b):\n    # stricter matching for Floats\n    return a == b and a._prec == b._prec\n\n", "def test_Mul_with_powers():\n    assert Mul(x**2, x**3).args == [x**2*x**3]\n    assert Mul(x**2, x**3, evaluate=False).args == [x**2, x**3]\n    assert Mul(x**2, x**3).as_two_terms() == (x**2*x**3, S.One)\n    assert Mul(x**2, x**3).as_coefficients_dict() == {x**2*x**3: S.One}\n    assert Mul(x**2, x**3).as_coefficients_dict()[x**2*x**3] == S.One\n    assert Mul(x**2, x**3).as_coefficients_dict()[x**2] == 0\n    assert Mul(x**2, x**3).as_coefficients_dict()[x**3] == 0\n    assert Mul(x**2, x**3).as_coefficients_dict()[x] == 0\n    assert Mul(x**2, x**3).as_coefficients_dict()[1] == 0\n    assert Mul(x**2, x**3).as_coefficients_dict()[S.One] == 0\n    assert Mul(x**2, x**3).as_coefficients_dict()[x**2*x**3] == S.One\n", "def test_expand_2arg():\n    # Test that expand_2arg works for some Add instances\n    assert expand_2arg(2*(x + y)) == 2*x + 2*y\n    assert expand_2arg(2*(x + y + z)) == 2*x + 2*y + 2*z\n    assert expand_2arg(2*(x + y + z + w)) == 2*x + 2*y + 2*z + 2*w\n    # Test that expand_2arg works for some Mul instances\n    assert expand_2arg((x + y)*2) == 2*x + 2*y\n    assert expand_2arg((x + y + z)*2) == 2*x + 2*y + 2*z\n    assert expand_2arg((x + y + z + w)*2) == 2*x + 2*y + 2*z + 2*w\n    # Test that expand_2arg leaves non-Add instances unchanged\n    assert expand_2arg(x**2) == x**2\n    assert expand_2arg(x**2 + y**2) == x**2 + y**2\n    assert expand_2arg(x + y) == x + y\n    # Test that expand_2arg works with Float coefficients\n    assert expand_2arg(2.0*(x + y)) == 2.0*x + 2.0*y\n    assert expand_2arg(2.5*(x + y + z)) == 2.5*x + 2.5*y + 2.5*z\n    # Test that expand_2arg works with Rational coefficients\n    assert expand_2arg(3/2*(x + y)) == 3*x/2 + 3*y/2\n    assert expand_2arg(2/3*(x + y + z)) == 2*x/3 + 2*y/3 + 2*z/3\n", "def test_Mul_matches_noncomm():\n    # Check if _matches_noncomm correctly matches when the non-commutative\n    # arguments have different bases.\n    assert Mul(a, b).matches(Mul(b, c)) is None\n    assert Mul(a, c).matches(Mul(b, c)) is None\n", "def test_Mul_as_two_terms():\n    from sympy.core.expr import UnevaluatedExpr\n    from sympy.core.parameters import evaluate\n    from sympy.testing.randtest import verify_numerically\n\n    # Test that as_two_terms works with evaluation set to True or False\n    a, b = symbols('a b', cls=UnevaluatedExpr, evaluate=False)\n    assert a.as_two_terms() == (1, a)\n    assert a.as_two_terms(evaluate=True) == (1, a)\n    assert a.as_two_terms(evaluate=False) == (1, a)\n    assert (a + b).as_two_terms(evaluate=True) == (1, a + b)\n    assert (a + b).as_two_terms(evaluate=False) == (1, UnevaluatedExpr(a + b))\n\n    # Test that as_two_terms preserves arguments when evaluating\n    # to False\n    a, b = symbols('a b')\n    assert a.as_two_terms(evaluate=False) == (1, a)\n    assert a.as_two_terms(evaluate=True) == (1, a)\n    assert (a + b).as_two_terms(evaluate=False) == (1, a + b)\n    assert (a + b).as_two_terms(evaluate=True) == (1, a + b)\n    # Test that as_two_terms works with negative arguments\n    a = -symbols('a')\n    assert a.as_two_terms() == (1, -a)\n    assert a.as_two_terms(evaluate=False) == (1, a)\n    # Test that as_two_terms works with complex numbers\n    c = symbols('c', cls=complex)\n    assert c.as_two_terms() == (1, c)\n    assert c.as_two_terms(evaluate=False) == (1, c)\n    # Test that as_two_terms works with symbols with attributes\n    b = symbols('b', positive=True)\n    assert b.as_two_terms() == (1, b)\n    assert b.as_two_terms(evaluate=False) == (1, b)\n\n    # Test that as_two_terms preserves the class of the result\n    class Foo(Basic):\n        is_Foo = True\n            return Basic.__new__(cls, arg)\n\n    a = Foo('a')\n    assert a.as_two_terms() == (1, a)\n    assert a.as_two_terms(evaluate=False) == (1, a)\n\n"], "sample_331": ["    def test_parse_time_with_seconds(self):\n        \"\"\"Test that parse_time can handle times with seconds.\"\"\"\n        self.assertEqual(parse_time('12:34:56'), time(12, 34, 56))\n", "    def test_parse_datetime_with_timezone_offset(self):\n        # Test parsing a datetime with a timezone offset\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00+02:00'), \n                         datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(120)))\n\n        # Test parsing a datetime with a negative timezone offset\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00-02:00'), \n                         datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(-120)))\n\n        # Test parsing a datetime with a timezone offset that is not a multiple of 60 minutes\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00+01:30'), \n                         datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(90)))\n\n        # Test parsing a datetime with a timezone offset that is not a multiple of 60 minutes and is negative\n        self.assertEqual(parse_datetime('2022-01-01 12:00:00-01:30'), \n                         datetime.datetime(2022, 1, 1, 12, 0, 0, tzinfo=get_fixed_timezone(-90)))\n\n        # Test parsing a datetime with an invalid timezone offset\n        self.assertIsNone(parse_datetime('2022-01-01 12:00:00Z invalid'))\n", "    def test_parse_duration_postgres_interval_format(self):\n        # Test PostgreSQL's day-time interval format, e.g. \"3 days 04:05:06\"\n        self.assertEqual(\n            parse_duration(\"3 days 04:05:06\"),\n            timedelta(days=3, hours=4, minutes=5, seconds=6)\n        )\n        # Test PostgreSQL's day-time interval format with microseconds\n        self.assertEqual(\n            parse_duration(\"3 days 04:05:06.123456\"),\n            timedelta(days=3, hours=4, minutes=5, seconds=6, microseconds=123456)\n        )\n", "    def test_parse_duration_with_only_days(self):\n        self.assertEqual(parse_duration('3 days'), timedelta(days=3))\n", "    def test_parse_date_isoformat_with_leading_zeroes(self):\n        # This test ensures that parse_date can handle date strings with leading zeroes\n        # in the day, month, or year. Django's date.fromisoformat function\n        # returns a date object for these, and we should too.\n        self.assertEqual(parse_date('2022-01-01'), date(2022, 1, 1))\n        self.assertEqual(parse_date('2022-07-25'), date(2022, 7, 25))\n        self.assertEqual(parse_date('2022-07-25-'), date(2022, 7, 25))\n"], "sample_105": ["    def test_get_redirect_url(self):\n        view = views.RedirectView()\n        url = view.get_redirect_url()\n        self.assertIsNone(url)\n", "    def test_redirect_view_with_url(self):\n        view = views.RedirectView(url='/another/url')\n        response = view.get(self.rf.get('/redirect/'))\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/another/url')\n", "    def test_render_to_response(self):\n        view = AboutTemplateView()\n        context = view.get_context_data()\n        response = view.render_to_response(context)\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 200)\n", "    def test_context_mixin(self):\n        context = ContextMixin().get_context_data()\n        self.assertEqual(context, {})\n        context = ContextMixin().get_context_data(extra_context={'foo': 'bar'})\n        self.assertEqual(context, {'view': ContextMixin(), 'foo': 'bar'})\n        context = ContextMixin().get_context_data(view='custom_view')\n        self.assertEqual(context, {'view': 'custom_view'})\n", "    def test_redirect_view(self):\n        view = views.RedirectView()\n        request = self.rf.get('/path/to/url/', {'key': 'value'})\n        response = view.get(request, *[], **{'key': 'value'})\n        self.assertEqual(response.status_code, 301)  # 301 for permanent redirects\n        self.assertEqual(response['Location'], 'path/to/url/?key=value')\n"], "sample_491": ["    def test_bound_field_additional_attributes(self):\n        form = PersonNew(data={\"first_name\": \"John\", \"last_name\": \"Doe\", \"birthday\": \"2022-01-01\"})\n        bound_field = form[\"first_name\"]\n        self.assertEqual(bound_field.as_widget(attrs={\"class\": \"custom-class\"}), bound_field.field.widget.render(name=\"first_name\", value=\"John\", attrs={\"class\": \"custom-class\"}, renderer=get_default_renderer()))\n", "    def test_bound_field_as_textarea(self):\n        form = PersonNew(data={\"first_name\": \"John\", \"last_name\": \"Doe\"})\n        field = form[\"first_name\"]\n        textarea = field.as_textarea()\n        self.assertIn(\"textarea\", textarea)\n        self.assertIn(\"id\", textarea)\n        self.assertIn(\"name\", textarea)\n        self.assertIn(\"value\", textarea)\n        self.assertIn(\"John\", textarea)\n", "    def test_bound_field_hidden_widget(self):\n        # Test that a HiddenInput widget is rendered correctly for a BoundField\n        form = PersonNew(data={\"first_name\": \"John\", \"last_name\": \"Doe\", \"birthday\": \"2020-01-01\"})\n        bound_field = form[\"first_name\"]\n        hidden_widget = bound_field.as_widget(widget=HiddenInput())\n        self.assertEqual(hidden_widget, '<input type=\"hidden\" id=\"first_name_id\" name=\"first_name\" value=\"John\"/>')\n", "    def test_bound_field_attrs(self):\n        form = PersonNew(data={\"first_name\": \"John\", \"last_name\": \"Doe\", \"birthday\": \"2020-01-01\"})\n        bound_field = PersonNew()[\"first_name\"]\n        self.assertEqual(\n            bound_field.build_widget_attrs({\"class\": \"my_class\"}),\n            {\"class\": \"my_class\", \"id\": \"first_name_id\", \"required\": True},\n        )\n        self.assertEqual(\n            bound_field.build_widget_attrs({\"class\": \"my_class\"}, widget=FileInput()),\n            {\"class\": \"my_class\", \"type\": \"file\"},\n        )\n", "    def test_boundfield_build_widget_attrs_disabled_and_required(self):\n        # Test that a field with both disabled and required attributes is rendered correctly\n        form = PersonNew(data={\"first_name\": \"John\", \"last_name\": \"Doe\"})\n        field = form[\"first_name\"]\n        attrs = field.build_widget_attrs({\"class\": \"my-class\"})\n        self.assertIn(\"disabled\", attrs)\n        self.assertIn(\"required\", attrs)\n"], "sample_899": ["    def fit(self, X, y=None):\n        X = check_array(X)\n        return self\n", "    def __init__(self, deprecation_message, *args, **kwargs):\n        self.deprecation_message = deprecation_message\n        self.args = args\n        self.kwargs = kwargs\n", "    def fit(self, X, y):\n        raise ValueError('estimator is not fitted yet')\n", "    def __init__(self):\n        warnings.warn(\"This is a deprecated warning\", DeprecationWarning)\n", "    def __init__(self):\n        self.random_state = None\n"], "sample_1165": ["def test_quaternion_diff():\n    w, x, y, z = symbols('w:z')\n    q = Quaternion(x, y, z, w)\n\n    # Test diff\n    q_diff_x = diff(q, x)\n    assert q_diff_x == Quaternion(1, y, z, w)\n\n    # Test diff with multiple symbols\n    q_diff_xyz = diff(q, (x, y, z))\n    assert q_diff_xyz == Quaternion(1, 1, 1, 1)\n\n    # Test diff with one symbol and multiple symbols\n    q_diff_x_y = diff(q, (x, y))\n    assert q_diff_x_y == Quaternion(1, 1, z, w)\n\n    # Test diff with a non-symbol argument\n    q_diff_2 = diff(q, 2)\n    assert q_diff_2 == Quaternion(0, 0, 0, 0)\n\n    # Test diff with a non-integer argument\n    q_diff_pi = diff(q, pi)\n    assert q_diff_pi == Quaternion(0, 0, 0, 0)\n\n    # Test diff with a non-integer argument and multiple symbols\n    q_diff_pi_xy = diff(q, (pi, y))\n    assert q_diff_pi_xy == Quaternion(0, 0, 1, w)\n", "def test_rotate_point():\n    from sympy import symbols, trigsimp, cos, sin, Matrix\n    w, x, y, z = symbols('w:z')\n    phi = symbols('phi')\n    q = Quaternion(cos(phi/2), 0, 0, sin(phi/2))\n\n    # Test rotation about the origin\n    p = (1, 2, 3)\n    result = q.rotate_point(p)\n    expected_result = (cos(phi)*1 - sin(phi)*2, sin(phi)*1 + cos(phi)*2, 3)\n    assert result == expected_result\n\n    # Test rotation about a point\n    v = (4, 5, 6)\n    result = q.rotate_point(p, (v, phi))\n    expected_result = (cos(phi)*((1-4)*cos(phi) - 2*sin(phi) + 5*sin(phi)) + (1-4)*sin(phi) + 2*cos(phi) + 6,\n                      sin(phi)*((1-4)*cos(phi) - 2*sin(phi) + 5*sin(phi)) - (1-4)*sin(phi) + 2*cos(phi) + 6,\n                      3)\n    assert result == expected_result\n\n    # Test rotation with a quaternion with zero norm\n    q2 = Quaternion(1, 1, 1, 1)\n    raises(ValueError, lambda: q2.rotate_point(p))\n", "def test_pow_non_integer_power():\n    w = symbols('w')\n    q = Quaternion(1, 2, 3, 4)\n    assert q.pow(w).args == (4 + w**2 + 4*w, 4*w + 2*w**2, 3*w + 2*w**2, 4*w + 2*w**2)\n", "def test_pow_cos_sin():\n    w, x, y, z = symbols('w:z')\n    q = Quaternion(w, x, y, z)\n    p = 3\n\n    q2 = q.pow_cos_sin(p)\n\n    # Verify that the q2 is of the correct form\n    assert q2.is_quaternion\n    assert q2.a.is_real\n    assert q2.b.is_real\n    assert q2.c.is_real\n    assert q2.d.is_real\n    assert q2.a.is_number\n    assert q2.b.is_number\n    assert q2.c.is_number\n    assert q2.d.is_number\n\n    # Verify that q2 is equal to q^p in the cos-sin form\n    q3 = q.pow(p)\n    assert trigsimp(q2 - q3) == 0\n", "def test_quaternion_from_axis_angle():\n    # Test with vector (x, y, z) with x = y = z\n    v = (1, 1, 1)\n    angle = 2*pi/3\n    q = Quaternion.from_axis_angle(v, angle)\n    expected_result = Quaternion(0.5, 0.5, 0.5, 0.5)\n    assert q.equals(expected_result)\n\n    # Test with vector (x, y, z) with x = y = 0 and z = 1\n    v = (0, 0, 1)\n    angle = pi/4\n    q = Quaternion.from_axis_angle(v, angle)\n    expected_result = Quaternion(1/2, 0, 0, 1/2)\n    assert q.equals(expected_result)\n\n    # Test with vector (x, y, z) with x = y = z = 0 and angle = 0\n    v = (0, 0, 0)\n    angle = 0\n    q = Quaternion.from_axis_angle(v, angle)\n    expected_result = Quaternion(1, 0, 0, 0)\n    assert q.equals(expected_result)\n\n    # Test with non-zero x, y, z\n    v = (1, 2, 3)\n    angle = 2*pi/3\n    q = Quaternion.from_axis_angle(v, angle)\n    expected_result = Quaternion(0, 1/2, 1/2, 1/2)\n    assert q.equals(expected_result)\n"], "sample_938": ["def test_ManualPageBuilder_init_without_man_pages_config(capsys):\n    # given\n    class TestConfig(Config):\n        project = 'Test Project'\n        master_doc = 'index'\n        author = 'Author'\n        release = '1.0'\n\n    sphinx = Sphinx()\n    app = sphinx.application\n    app.config = TestConfig()\n\n    # when\n    manual_page_builder = ManualPageBuilder(sphinx)\n    manual_page_builder.init()\n\n    # then\n    captured = capsys.readouterr()\n    assert captured.out == \"WARNING: no 'man_pages' config value found; no manual pages will be written\\n\"\n", "def test_man_pages_config_value_empty_list(capsys, tmp_path):\n    confdir = tmp_path / 'conf'\n    confdir.mkdir()\n    conf = confdir / 'conf.py'\n    conf.write_text('''\n        import os\n        import sphinx\n        from sphinx.builders.manpage import setup\n        config = sphinx.Config()\n        setup(config)\n        config.man_pages = []\n        config.master_doc = 'index'\n        config.project = 'test_project'\n        config.release = '1.0'\n        config.author = 'Author'\n    ''')\n\n    # Run Sphinx with the custom configuration\n    args = [str(conf), '-b', 'man', str(tmp_path)]\n    result = sphinx.main(args)\n    captured = capsys.readouterr()\n    assert 'no \"man_pages\" config value found; no manual pages will be written' in captured.out\n    assert result.returncode == 0\n", "def test_manual_page_builder_init(tmp_path, app):\n    builder = ManualPageBuilder(app)\n    assert builder.name == 'man'\n    assert builder.format == 'man'\n    assert builder.epilog == __('The manual pages are in %(outdir)s.')\n", "def test_build_manpage_section_directory(tmp_path, monkeypatch):\n    \"\"\"Test the build process with manpage builder and section directory.\"\"\"\n    # Create a temporary directory for the project\n    project_dir = tmp_path / 'project'\n    project_dir.mkdir()\n    (project_dir / 'conf.py').write_text('''\n        project = 'My Project'\n        author = 'John Doe'\n        master_doc = 'index'\n        release = '1.0'\n    ''')\n\n    # Create the source directory\n    src_dir = project_dir / 'source'\n    src_dir.mkdir()\n    (src_dir / 'index.rst').write_text('''\n        .. toctree::\n           :maxdepth: 2\n\n           index\n    ''')\n\n    # Create the man_pages config value\n    (project_dir / 'conf.py').write_text('''\n        man_pages = [('index', 'index', 'My Project 1.0', ['John Doe'], 1),\n                     ('contributing', 'contributing', 'Contributing', ['Jane Doe'], 3)]\n    ''')\n\n    # Set up the Sphinx app\n    import sphinx\n    app = sphinx.Application()\n    app.add_config_values({\n        'man_make_section_directory': True,\n        'man_pages': [('index', 'index', 'My Project 1.0', ['John Doe'], 1),\n                      ('contributing', 'contributing', 'Contributing', ['Jane Doe'], 3)]\n    })\n    app.builder = sphinx.builders.ManualPageBuilder()\n\n    # Test the write method\n    builder = sphinx.builders.ManualPageBuilder()\n    builder.outdir = tmp_path / 'man'\n    builder.write()\n\n    # Check the output directory structure\n    assert (tmp_path / 'man/1/index').exists()\n    assert (tmp_path / 'man/3/contributing').exists()\n\n    # Check the contents of the man files\n    with open(tmp_path / 'man/1/index.1') as f:\n        contents = f.read()\n        assert 'My Project 1.0' in contents\n        assert 'John Doe' in contents\n    with open(tmp_path / 'man/3/contributing.3') as f:\n        contents = f.read()\n        assert 'Contributing' in contents\n        assert 'Jane Doe' in contents\n", "def test_manual_page_writer(tmpdir, capsys):\n    \"\"\"\n    Test the manual page writer.\n    \"\"\"\n    # create a temporary sphinx project\n    from sphinx import main\n    app = main.main(['-b', 'man', '-d', str(tmpdir), str(tmpdir)])\n    assert app.builder.name == 'man'\n\n    # write a document to the project\n    with open(path.join(tmpdir, 'index.rst'), 'w') as f:\n        f.write('Document title\\n')\n        f.write('================\\n\\n')\n        f.write('This is a document.\\n')\n\n    # set up the config\n    app.config.man_pages = [('index', 'manpage', 'manual page', ['Author'], 1)]\n    app.config.master_doc = 'index'\n    app.config.project = 'testproject'\n    app.config.release = '1.0'\n    app.config.author = 'Test Author'\n\n    # run the build process\n    app.builder.write()\n\n    # check the output\n    output = capsys.readouterr()\n    assert 'testproject.1' in output.out\n    assert 'This is a document.' in output.out\n"], "sample_881": ["def test_top_k_accuracy_score(k):\n    y_true = np.array([0, 1, 2, 2])\n    y_score = np.array([[0.5, 0.2, 0.3], [0.3, 0.4, 0.2], [0.2, 0.4, 0.3], [0.7, 0.2, 0.1]])\n    if k >= y_true.max() + 1:\n        assert top_k_accuracy_score(y_true, y_score, k=k) == 1.0\n    else:\n        expected_output = 1.0 if k == 1 else 0.0\n        assert top_k_accuracy_score(y_true, y_score, k=k) == expected_output\n", "def test_coverage_error():\n    y_true = np.array([[0, 0, 0]])\n    y_score = np.array([[0.1, 0.2, 0.3]])\n    assert_almost_equal(coverage_error(y_true, y_score), 0.0)\n\n", "def test_ndcg_score_ideal():\n    \"\"\"Test that the ideal DCG is 1 when the labels are already\n    sorted in order of relevance.\"\"\"\n\n    # Generate a 2d array of shape (n_samples, n_labels) of\n    # positive labels to ensure that the true labels are already sorted\n    # in order of relevance\n    n_samples = 10\n    n_labels = 5\n    y_true = np.ones((n_samples, n_labels))\n    y_score = np.ones((n_samples, n_labels))\n\n    # The ideal DCG should be 1 when the true labels are already\n    # sorted in order of relevance\n    assert_almost_equal(ndcg_score(y_true, y_score), 1)\n\n    # The ideal DCG should be 0 when the true labels are not sorted\n    # in order of relevance\n    y_true = np.array([[0, 1, 0, 1, 0]])\n    y_score = np.array([[1, 0, 0, 0, 0]])\n    assert_almost_equal(ndcg_score(y_true, y_score), 0)\n\n    # The ideal DCG should be 1 when the true labels are already\n    # sorted in order of relevance with k=2\n    y_true = np.array([[1, 1, 0, 0, 0]])\n    y_score = np.array([[1, 1, 0, 0, 0]])\n    assert_almost_equal(ndcg_score(y_true, y_score, k=2), 1)\n\n    # The ideal DCG should be 0.5 when the true labels are not sorted\n    # in order of relevance with k=2\n    y_true = np.array([[0, 1, 0, 0, 0]])\n    y_score = np.array([[1, 1, 0, 0, 0]])\n    assert_almost_equal(ndcg_score(y_true, y_score, k=2), 0.5)\n\n    # Test with sample weights\n    y_true = np.array([[1, 1, 0, 0, 0]])\n    y_score = np.array([[1, 1, 0, 0, 0]])\n    sample_weight = np.array([1, 1, 1, 1, 1])\n    assert_almost_equal(ndcg_score(y_true, y_score, sample_weight=sample", "def test_ndcg_score_on_multiclass_multioutput():\n    \"\"\"Check correctness of `ndcg_score` with multiclass-multioutput data.\"\"\"\n    X, y = make_multilabel_classification(n_samples=10, n_features=10, random_state=0)\n    y = y.toarray()\n\n    # We can't use the ndcg_score function directly on multioutput data because it\n    # only supports continuous-multioutput and multilabel-indicator data.\n    # So we'll use the _ndcg_sample_scores function directly.\n    y_score = np.random.rand(10, 10)\n    n_labels = y.shape[1]\n    k = 2\n    expected = _ndcg_sample_scores(y, y_score, k=k, ignore_ties=False)\n    result = np.average(expected)\n    assert_array_almost_equal(result, 0.597, decimal=2)\n\n", "def test_dcg_score_vs_ndcg_score(log_base):\n    \"\"\"Test DCG and NDCG computation under different log bases.\"\"\"\n    # Test with 1 label, a perfect ranking and a non-perfect ranking\n    y_true = np.array([[10]])\n    y_score = np.array([[0.5, 0.2, 0.3, 0.4, 0.6]])\n    assert_almost_equal(dcg_score(y_true, y_score, k=5, log_base=log_base), 0.91)\n    assert_almost_equal(ndcg_score(y_true, y_score, k=5), 0.91)\n\n    y_true = np.array([[10, 5]])\n    y_score = np.array([[0.5, 0.2, 0.3, 0.4, 0.6]])\n    assert_almost_equal(dcg_score(y_true, y_score, k=5, log_base=log_base), 0.88)\n    assert_almost_equal(ndcg_score(y_true, y_score, k=5), 0.88)\n\n    # Test with a non-perfect ranking and different log bases\n    y_true = np.array([[5, 10]])\n    y_score = np.array([[0.2, 0.3, 0.4, 0.5, 0.6]])\n    for base in [2, 3, 10]:\n        assert_almost_equal(dcg_score(y_true, y_score, k=5, log_base=base), 0.84)\n        assert_almost_equal(ndcg_score(y_true, y_score, k=5), 0.84)\n\n    # Test with a large number of scores\n    y_true = np.array([[5, 10]])\n    y_score = np.random.rand(1, 100)\n    y_score[:, :10] = np.sort(y_score[:, :10])  # make first 10 scores sorted\n    assert_almost_equal(dcg_score(y_true, y_score, k=5, log_base=log_base), 0.83)\n    assert_almost_equal(ndcg_score(y_true, y_score, k=5), 0.83)\n\n"], "sample_539": ["def test_slider_reset(ax):\n    slider = widgets.Slider(ax, \"label\", 0, 100)\n    slider.set_val(50)\n    assert slider.val == 50\n    slider.reset()\n    assert slider.val == 0\n    assert slider.valinit == 0\n", "def test_SliderBase_disconnected_cid():\n    # Create a SliderBase instance\n    ax = plt.axes()\n    slider = widgets.SliderBase(ax, 'horizontal', 0, 1, 0.5)\n\n    # Connect a function to the 'changed' signal\n        pass\n    cid = slider.on_changed(callback)\n\n    # Disconnet the function\n    slider.disconnect(cid)\n\n    # Verify that the function is no longer connected\n    with pytest.raises(ValueError):\n        slider.on_changed(callback)\n\n    # Verify that the function can be connected again\n    slider.on_changed(callback)\n", "def test_spanselector_init_with_invalid_spancoords_type():\n    with pytest.raises(ValueError):\n        widgets.SpanSelector(ax, lambda *args: None, 'horizontal', spancoords='foo')\n", "def test_SliderBase_init_args(kwargs):\n    s = widgets.SliderBase(ax(), 'vertical', True, False, 0, 100, None, True, 1)\n    assert s.orientation == 'vertical'\n    assert s.closedmin\n    assert not s.closedmax\n    assert s.valmin == 0\n    assert s.valmax == 100\n    assert s.valstep == 1\n    assert s.drag_active is False\n    assert s.valfmt is None\n", "def test_rectangle_selector_init(kwargs, ax):\n    rect = widgets.RectangleSelector(ax, lambda *args: None, **kwargs)\n    assert rect.props == kwargs.get('props', dict(facecolor='red', edgecolor='black', alpha=0.2, fill=True))\n    assert rect.handle_props == kwargs.get('handle_props', dict(markeredgecolor='black'))\n    assert rect.spancoords == kwargs.get('spancoords', 'data')\n    assert rect.button == kwargs.get('button', None)\n"], "sample_1079": ["def test_affine_rank_single_point():\n    p1 = Point(1, 2)\n    assert Point.affine_rank(p1) == 0\n", "def test_Points_equal():\n    x = Symbol('x')\n    y = Symbol('y')\n    p1 = Point(x, y)\n    p2 = Point(x, y)\n    assert p1.equals(p2)\n    p3 = Point(x, x)\n    assert not p1.equals(p3)\n    p4 = Point(1, 2)\n    p5 = Point(2, 3)\n    assert not p4.equals(p5)\n    assert p4.equals(Point(1, 2, 0))\n    p6 = Point2D(x, y)\n    p7 = Point3D(x, y)\n    assert not p6.equals(p7)\n", "def test_point_equals():\n    p = Point(Rational(1, 2), Rational(3, 4))\n    q = Point(Rational(1, 2), Rational(3, 4))\n    assert p.equals(q)\n", "def test_canberra_distance():\n    from sympy.abc import x, y, z\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(3, 4, 5)\n    assert p1.canberra_distance(p2) == (8/9)\n    p3 = Point3D(0, 0, 0)\n    assert p1.canberra_distance(p3) == 6\n    assert p1.canberra_distance(Point3D(0, 0)) == 3\n    assert p1.canberra_distance(Point3D(1, 0, 0)) == 2\n    p4 = Point3D(2, 4, 6)\n    p5 = Point3D(4, 8, 12)\n    assert p4.canberra_distance(p5) == 3\n    p6 = Point3D(x, y, z)\n    p7 = Point3D(x + 2, y + 3, z + 4)\n    assert p6.canberra_distance(p7) == 5\n", "def test_Point_project():\n    from sympy.geometry import Point\n\n    p1 = Point(1, 2)\n    b = Point(3, 4)\n\n    result = Point.project(p1, b)\n\n    # The projected point should be on the line between p1 and b\n    assert Line(p1, b).is_collinear(p1, result)\n\n    # The projected point should be closer to b than p1\n    assert b.distance(result) <= p1.distance(result)\n"], "sample_460": ["    def setUpTestData(cls):\n        cls.site = AdminSite()\n", "    def test_admin_site_default(self):\n        response = self.client.get(reverse(\"admin:index\", current_app=\"admin\"))\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"admin/index.html\")\n", "    def test_get_app_list(self):\n        response = self.client.get(\"/admin/\")\n        app_list = self.site.get_app_list(self.client.request)\n        self.assertEqual(len(app_list), 2)\n        for app in app_list:\n            self.assertIn(app[\"name\"], [\"Application\", \"Auth\"])\n", "    def setUp(self):\n        super().setUp()\n        self.admin_site = site\n        self.admin_site2 = site2\n        self.url = reverse(\"admin:story_changelist\", current_app=\"admin\")\n", "    def test_get_app_list_includes_custom_admin_site(self):\n        response = self.client.get(reverse(\"admin:index\", current_app=\"customadmin\"))\n        self.assertContains(response, \"Custom admin app\")\n"], "sample_671": ["def test_check_xfail_no_run():\n    item = pytest.Item()\n    item.config = pytest.Config()\n    item._store = {}\n    item._store[skipped_by_mark_key] = False\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    item._store[unexpectedsuccess_key] = None\n    config = item.config\n    config.option.runxfail = True\n\n    # Test when xfail(run=False) and runxfail is enabled\n    evalxfail = item._store[evalxfail_key]\n    evalxfail.condition = \"True\"\n    evalxfail.run = False\n    check_xfail_no_run(item)\n    assert evalxfail.istrue()\n\n    # Test when xfail(run=False) and runxfail is not enabled\n    evalxfail.run = True\n    check_xfail_no_run(item)\n    assert not evalxfail.istrue()\n\n    # Test when xfail(run=True) and runxfail is enabled\n    evalxfail.run = True\n    check_xfail_no_run(item)\n    assert evalxfail.istrue()\n\n    # Test when xfail(run=True) and runxfail is not enabled\n    evalxfail.run = False\n    check_xfail_no_run(item)\n    assert evalxfail.istrue()\n", "def test_check_xfail_no_run():\n    # Arrange\n    item = pytest.Function(\"test_function\")\n    item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\")\n    item._store[skipped_by_mark_key] = False\n    item.config.option.runxfail = True\n\n    # Act\n    check_xfail_no_run(item)\n\n    # Assert\n    assert item._store[evalxfail_key].istrue()\n    assert item._store[evalxfail_key].get(\"run\") is False\n    assert xfail(\"[NOTRUN] \" + item._store[evalxfail_key].getexplanation()) == xfail(\"[NOTRUN] \" + item._store[evalxfail_key].getexplanation())\n\n", "def test_evalxfail_key_set_correctly():\n    \"\"\"Test that evalxfail key is set correctly in pytest_runtest_setup\"\"\"\n    # Create a test item with an xfail marker\n    item = pytest.Item(\"test_xfail\")\n    item._markers = [pytest.mark.xfail]\n\n    # Set up the test\n    pytest_runtest_setup(item)\n\n    # Check that the evalxfail key is set correctly\n    assert item._store[evalxfail_key].istrue()\n", "def test_check_strict_xfail_with_different_strict_default():\n    # given\n    config = pytest.Config(\n        plugins=[pytest.mark.pytest_strict_xfail.Plugin()],\n    )\n    config.getini = lambda key: not key == 'xfail_strict'\n    config.option.runxfail = False\n    item = pytest.Item()\n    item.config = config\n    item._store = {}\n\n    # when\n    evalxfail = MarkEvaluator(item, \"xfail\")\n    evalxfail._store[\"strict\"] = True\n    check_strict_xfail(item)\n\n    # then\n    assert item._store.pop(evalxfail_key).get(\"strict\", False) is False\n", "    def test_init(self):\n        \"\"\"Test that MarkEvaluator is correctly initialized\"\"\"\n        item = pytest.Item(\"test_func\", None, None, None, None, None)\n        evaluator = MarkEvaluator(item, \"skip\")\n        assert evaluator.istrue() is False  # default to False\n"], "sample_986": ["def test_scaled_zero_tuple():\n    from sympy import S\n    z = scaled_zero(100)\n    assert z == (0, 1, 100, 1)\n    assert scaled_zero(z) == (0, 1, 100, 1)\n    z = scaled_zero(100, -1)\n    assert scaled_zero(z) == (0, -1, 100, 1)\n    assert scaled_zero(100) == ((0,), 1, 100, 1)\n    assert scaled_zero((0, 1, 100, 1)) == (0, 1, 100, 1)\n    assert scaled_zero(0) == (0, 1, 0, 1)\n    assert scaled_zero(-1) == (1, 1, 0, 1)\n    assert scaled_zero(-100) == (1, 1, 0, 1)\n    try:\n        scaled_zero('a')\n        assert False, \"Expected ValueError\"\n    except ValueError as e:\n        assert str(e) == \"scaled zero expects int or scaled_zero tuple.\"\n", "def test_scaled_zero():\n    assert scaled_zero(100)[0] == ([0], 1, 100, 1)\n    assert scaled_zero((0, 1, 100, 1)) == (0, 1, 100, 1)\n    assert scaled_zero(100, -1) == (-1, 1, 100, 1)\n    assert scaled_zero((0, 1, 100, 1), -1) == (0, 1, 100, 1)\n\n    try:\n        scaled_zero(\"a\")\n    except ValueError as e:\n        assert str(e) == \"scaled zero expects int or scaled_zero tuple.\"\n    try:\n        scaled_zero([1, 2, 3])\n    except ValueError as e:\n        assert str(e) == \"scaled zero expects int or scaled_zero tuple.\"\n    try:\n        scaled_zero(1.0)\n    except ValueError as e:\n        assert str(e) == \"scaled zero expects int or scaled_zero tuple.\"\n", "def test_scaled_zero_return_scaled_zero_tuple():\n    from sympy.core.evalf import scaled_zero\n    assert scaled_zero(100) == (([0], 1, 100, 1), -1)\n    assert scaled_zero(100, -1) == (([0], 1, 100, 1), 1)\n", "def test_evalf_trig():\n    from sympy import cos, sin, asin\n    assert NS(sin(0.5)) == '0.479425538604203'\n    assert NS(cos(-0.5)) == '0.8775825618903728'\n    assert NS(asin(0.5)) == '0.4636476090008065'\n    assert NS(sin(0.5).diff(x)) == '0.8775825618903728/2'\n    assert NS(cos(0.5).diff(x)) == '-0.4338837393793119'\n    assert NS(asin(0.5).diff(x)) == '1/(2*sqrt(1 - x**2))'\n", "def test_evalf_n0():\n    from sympy import Symbol\n    from sympy.core.evalf import evalf\n\n    x = Symbol('x')\n    expr = exp(x)\n    with raises(PrecisionExhausted):\n        evalf(expr, 0)\n"], "sample_1067": ["def test_Mul_as_two_terms():\n    x, y = symbols('x y')\n    assert Mul(x, y).as_two_terms() == (x, y)\n    assert Mul(x, y, x, y).as_two_terms() == (x*y, x*y)\n    assert Mul(x, y, x).as_two_terms() == (x, x*y)\n    assert Mul(x, x*y).as_two_terms() == (x, x*y)\n    assert Mul(x, y, x*x).as_two_terms() == (x, x*y*x)\n    assert Mul(x, y, x, x).as_two_terms() == (x, x*y*x)\n    assert Mul(x, y, x*y).as_two_terms() == (x, x*y*y)\n", "def test_Mul_matches():\n    from sympy import S, Eq, Function, sqrt, sin, symbols\n    x, y = symbols('x y')\n    f = Function('f')\n    m1 = f(x)*x*sin(x)\n    m2 = f(x)*x*sin(x)\n    assert m1.matches(m2) is not None\n    assert m1.matches(m2, old=True) is not None\n    assert m1.matches(m2, old=False) is not None\n    m3 = f(x)*x*sin(y)\n    assert m1.matches(m3) is None\n    assert m1.matches(m3, old=True) is None\n    assert m1.matches(m3, old=False) is None\n    m4 = f(x)*sin(x)\n    assert m1.matches(m4) is None\n    assert m1.matches(m4, old=True) is None\n    assert m1.matches(m4, old=False) is None\n", "def test_Mul_matches():\n    from sympy import abc, symbols\n    x, y = symbols('x y', commutative=False)\n    assert Mul(x, y).matches(x*y) is not None\n    assert Mul(x, y).matches(x*y, old=True) is not None\n    assert Mul(x, y).matches(x*y, old=False) is not None\n    assert Mul(x, y).matches(y*x) is not None\n    assert Mul(x, y).matches(y*x, old=True) is not None\n    assert Mul(x, y).matches(y*x, old=False) is not None\n    assert Mul(x, y).matches(x*y + x*y) is not None\n    assert Mul(x, y).matches(x*y + x*y, old=True) is not None\n    assert Mul(x, y).matches(x*y + x*y, old=False) is not None\n    assert Mul(x, y).matches(x*y + y*x) is not None\n    assert Mul(x, y).matches(x*y + y*x, old=True) is not None\n    assert Mul(x, y).matches(x*y + y*x, old=False) is not None\n    assert Mul(x, y).matches(x**2 + y**2) is None\n    assert Mul(x, y).matches(x**2 + y**2, old=True) is None\n    assert Mul(x, y).matches(x**2 + y**2, old=False) is None\n    assert Mul(x, y).matches(y*x, old=True) is not None\n    assert Mul(x, y).matches(y*x, old=False) is not None\n    assert Mul(x, y).matches(x*y + y*x, old=True) is not None\n    assert Mul(x, y).matches(x*y + y*x, old=False) is not None\n    assert Mul(x, y).matches(x*y + y**2) is not None\n    assert Mul(x, y).matches(x*y + y**2, old=True) is not None\n    assert Mul(x, y).matches(x*y + y**2, old=False) is not None\n    assert Mul(x, y).matches(x*y + y*x + y*x) is not None\n    assert Mul(x, y).matches(x*y + y*x + y*x, old=True) is not None\n    assert", "def test_nc_part_consistency():\n    x, y = symbols('x y')\n    a = Mul(x, y, evaluate=False)\n    b = Mul(x, y)\n    assert a.args_cnc()[1] is b.args_cnc()[1]\n", "def test_associative_property():\n    # Test that A * B * C == (A * B) * C when all three are commutative\n    assert Mul(x, y, z) == Mul(x*y, z)\n    assert Mul(x, y, z) != Mul(y, x, z)\n\n    # Test that A * B * C == A * (B * C) when all three are commutative\n    assert Mul(x, y, z) == Mul(x, y*z)\n\n    # Test that two different ways of creating an expression result in the same\n    # Mul instance.\n    assert Mul(x, y, z) is Mul(*[x, y, z])\n\n    # Test that an expression with multiple Mul instances can be collapsed into\n    # a single Mul instance.\n    assert Mul(x, y, Mul(x, y, z)) == Mul(x, y, z, x, y)\n    assert Mul(x, y, Mul(x, y, z)) == Mul(x, y, x, y, z)\n\n    # Test that non-commutative arguments cannot be reordered.\n    assert Mul(x, y, z) != Mul(y, x, z)\n    assert Mul(x, y, z) != Mul(y, z, x)\n\n    # Test that Mul instances can be created with a variable number of arguments.\n    assert Mul(x, y) == Mul(x, y, 1)\n    assert Mul(x, y, z) == Mul(x, y, z, 1)\n    assert Mul(x, y, z, w) == Mul(x, y, z, w, 1)\n\n    # Test that the associativity property works with commutative arguments.\n    # This test was added because, at the time, the logic in Mul.flatten()\n    # was incorrect and didn't produce the expected result.\n    assert Mul(Mul(x, y), z) == Mul(x, y, z)\n    assert Mul(Mul(x, y), z"], "sample_841": ["def test_ridge_sparse_cg_solver(solver):\n    # Test Ridge regression with sparse data and sparse_cg solver\n    X_sparse = sp.csr_matrix(np.random.rand(100, 100))\n    y_sparse = np.random.rand(100)\n    alpha = 0.1\n    ridge = Ridge(solver=solver, alpha=alpha)\n    ridge.fit(X_sparse, y_sparse)\n    assert ridge.coef_.shape == (100,)\n    assert ridge.n_iter_.shape == ()\n\n    # Test Ridge regression with sparse data and sparse_cg solver, with sample weights\n    X_sparse = sp.csr_matrix(np.random.rand(100, 100))\n    y_sparse = np.random.rand(100)\n    sample_weight = np.random.rand(100)\n    alpha = 0.1\n    ridge = Ridge(solver=solver, alpha=alpha)\n    ridge.fit(X_sparse, y_sparse, sample_weight=sample_weight)\n    assert ridge.coef_.shape == (100,)\n    assert ridge.n_iter_.shape == ()\n\n    # Test Ridge regression with dense data and sparse_cg solver\n    X_dense = np.random.rand(100, 100)\n    y_dense = np.random.rand(100)\n    alpha = 0.1\n    ridge = Ridge(solver=solver, alpha=alpha)\n    ridge.fit(X_dense, y_dense)\n    assert ridge.coef_.shape == (100,)\n    assert ridge.n_iter_ is None\n\n    # Test Ridge regression with dense data and sparse_cg solver, with sample weights\n    X_dense = np.random.rand(100, 100)\n    y_dense = np.random.rand(100)\n    sample_weight = np.random.rand(100)\n    alpha = 0.1\n    ridge = Ridge(solver=solver, alpha=alpha)\n    ridge.fit(X_dense, y_dense, sample_weight=sample_weight)\n    assert ridge.coef_.shape == (100,)\n    assert ridge.n_iter_ is None\n", "def test_ridge_solver_edge_cases(solver):\n    # Test that the solver does not crash with edge cases\n    X = np.ones((100, 10))\n    y = np.ones(100)\n\n    if solver == \"svd\" and sp.issparse(X):\n        pytest.raises(ValueError, Ridge, solver=solver).message\n    else:\n        Ridge(solver=solver).fit(X, y)\n", "def test_ridge_solver(solver):\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n    y = rng.rand(100)\n    ridge = Ridge(solver=solver)\n    ridge.fit(X, y)\n    assert ridge.coef_.shape == (10,)\n    assert ridge.intercept_ == 0.0\n", "def test_ridge_solver(solver):\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([5, 6])\n    alpha = 0.1\n    if solver == \"sparse_cg\":\n        ridge_regression(X, y, alpha, max_iter=10, solver=solver)\n    else:\n        ridge_regression(X, y, alpha, solver=solver)\n\n", "def test_ridge_cv__add_intercept__not_supported(solver):\n    model = RidgeCV(fit_intercept=False, solver=solver)\n    X = np.ones((10, 5))\n    y = np.ones(10)\n    with assert_raises_regex(ValueError, \"In Ridge, only 'sag' solver \"\n                                      \"can directly fit the intercept.\"):\n        model.fit(X, y)\n"], "sample_1169": ["    def test_sorting_above_below_fermi(self):\n        a, b = symbols('a, b', above_fermi=True)\n        i, j = symbols('i, j', below_fermi=True)\n        v = AntiSymmetricTensor('v', (a, i), (b, j))\n        assert v.args[1] == (a, i)\n        assert v.args[2] == (b, j)\n\n        v2 = AntiSymmetricTensor('v', (i, a), (j, b))\n        assert v2.args[1] == (b, i)\n        assert v2.args[2] == (j, a)\n", "def test_NO_create_non_ambiguous_q_operators():\n    from sympy import symbols, Function\n    from sympy.physics.secondquant import NO, F, Fd\n\n    a, b = symbols('a b', above_fermi=True)\n    i, j = symbols('i j', below_fermi=True)\n    p, q = symbols('p q', cls=Dummy)\n\n    no = NO(Fd(a)*F(i)*F(b)*Fd(j))\n    sub_no = no.get_subNO(1)\n    expected = NO(Fd(a)*F(i)*NO(AnnihilateFermion(b)*AnnihilateFermion(j)))\n    assert srepr(sub_no) == srepr(expected)\n\n    no = NO(Fd(p)*F(q)*F(b)*Fd(j))\n    sub_no = no.get_subNO(2)\n    expected = NO(Fd(p)*F(q)*NO(AnnihilateFermion(b)*AnnihilateFermion(j)))\n    assert srepr(sub_no) == srepr(expected)\n\n    no = NO(Fd(a)*Fd(b)*F(q)*Fd(i))\n    sub_no = no.get_subNO(0)\n    expected = NO(NO(AnnihilateFermion(a)*AnnihilateFermion(b))*F(q)*Fd(i))\n    assert srepr(sub_no) == srepr(expected)\n", "def test_Dagger():\n    a = symbols('a', above_fermi=True)\n    i = symbols('i', below_fermi=True)\n    p = symbols('p')\n\n    dagger_B = Dagger(B(a))\n    assert dagger_B == CreateBoson(a)\n\n    dagger_Bd = Dagger(Bd(i))\n    assert dagger_Bd == AnnihilateBoson(i)\n\n    dagger_Bd = Dagger(Bd(p))\n    assert dagger_Bd == AnnihilateBoson(p)\n\n    dagger_B = Dagger(B(p))\n    assert dagger_B == CreateBoson(p)\n\n    dagger_B = Dagger(B(0))\n    assert dagger_B == CreateBoson(0)\n\n    dagger_Bd = Dagger(Bd(0))\n    assert dagger_Bd == AnnihilateBoson(0)\n", "def test_apply_operators_with_multiple_inner_products():\n    # Inner product of a ket and a bra\n    p, q = symbols('p q', cls=Dummy)\n    bra = BBra([1, 2])\n    ket = BKet([1, 2])\n    inner_product = InnerProduct(bra, ket)\n    result = apply_operators(inner_product)\n    assert srepr(result) == 'InnerProduct(Bra([(1, 2)]), Ket([(1, 2)]))'\n\n    # Inner product of a bra and a ket\n    bra = BBra([1, 2])\n    ket = BKet([1, 2])\n    inner_product = InnerProduct(bra, ket)\n    result = apply_operators(inner_product)\n    assert srepr(result) == 'InnerProduct(Bra([(1, 2)]), Ket([(1, 2)]))'\n\n    # Inner product of a bra and a ket with a complex number\n    bra = BBra([1, 2])\n    ket = BKet([1, 2])\n    inner_product = InnerProduct(bra, ket)\n    result = apply_operators(2 * inner_product)\n    assert srepr(result) == '2*InnerProduct(Bra([(1, 2)], ()), Ket([(1, 2)]))'\n\n    # Inner product of a ket and a bra with a symbolic expression\n    p, q = symbols('p q', cls=Dummy)\n    bra = BBra([1, 2])\n    ket = BKet([1, 2])\n    inner_product = InnerProduct(bra, ket)\n    result = apply_operators(p * inner_product)\n    assert srepr(result) == 'p*InnerProduct(Bra([(1, 2)]), Ket([(1, 2)]))'\n\n    # Inner product of a ket and a bra with a list of operators\n    p, q = symbols('p q', cls=Dummy)\n    bra = BBra([1, 2])\n    ket = BKet([1, 2])\n    inner_product = InnerProduct(bra, ket)\n    result = apply_operators([1, 2] + [inner_product, p])\n    assert srepr(result) == 'Add(Add(1, 2), p*InnerProduct(Bra([(1, 2)]), Ket([(1, 2)])))'\n\n    #", "def test_contractions():\n    p, q, r, s = symbols('p,q,r,s', cls=Dummy)\n    i, j, k, l = symbols('i,j,k,l', below_fermi=True, cls=Dummy)\n\n    # no contraction\n    a = F(p)*Fd(q)\n    b = F(r)*Fd(s)\n    c = NO(a*b)\n    assert c == NO(F(p)*Fd(q))*NO(F(r)*Fd(s))\n\n    # contraction of two fermion operators\n    a = F(p)*Fd(i)\n    b = F(j)*Fd(q)\n    c = NO(a*b)\n    assert c == KroneckerDelta(q, i)*NO(F(p)*Fd(j)) + KroneckerDelta(i, j)*NO(F(q)*Fd(p))\n\n    # contraction of two fermion operators (swapped)\n    a = F(j)*Fd(i)\n    b = F(q)*Fd(p)\n    c = NO(a*b)\n    assert c == KroneckerDelta(q, i)*NO(F(p)*Fd(j)) + KroneckerDelta(i, j)*NO(F(q)*Fd(p))\n\n    # contraction of a fermion and a boson\n    a = F(p)*B(q)\n    b = F(r)*Fd(s)\n    c = NO(a*b)\n    assert c == NO(F(p)*B(q))*NO(F(r)*Fd(s))\n\n    # contraction of a fermion and a boson (swapped)\n    a = F(r)*Fd(s)\n    b = F(p)*B(q)\n    c = NO(a*b)\n    assert c == NO(F(r)*Fd(s))*NO(F(p)*B(q))\n\n    # contraction of two bosons\n    a = B(p)*B(q)\n    b = B(r)*B(s)\n    c = NO(a*b)\n    assert c == NO(B(p)*B(q))*NO(B(r)*B(s))\n\n    # contraction of two bosons (swapped)\n    a = B(r)*B(s)\n    b = B(p)*B(q)\n    c = NO(a*b)\n    assert c == NO(B(r)*B(s))*NO(B(p)*B(q))\n\n    # contraction with zero\n    a = B(p)*B(q)\n    b = NO(F(r)*Fd(s))\n    c = NO(a*b)\n    assert c == NO(B(p)*B(q))*NO(F(r)*Fd(s))\n\n   "], "sample_381": ["    def test_add_unique_together(self):\n        # Test adding a unique together constraint\n        before_states = self.make_project_state([\n            self.author_name,\n        ])\n        after_states = self.make_project_state([\n            self.book_foo_together,\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, name='book', unique_together={('author', 'title')})\n", "    def test_generate_added_indexes_with_indexes(self):\n        state1 = self.make_project_state([\n            self.author_with_book,\n        ])\n        state2 = self.make_project_state([\n            self.book_foo_together,\n        ])\n        changes = self.get_changes(state1.models, state2.models)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['AlterModelTable'])\n", "    def test(self):\n        from django.db import models\n        from django.db.migrations.autodetector import MigrationAutodetector\n\n        author_name_nested_deconstructible_1 = ModelState(\n            \"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=[DeconstructibleObject(1), 123])),\n            ]\n        )\n\n        author_name_nested_deconstructible_2 = ModelState(\n            \"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=[DeconstructibleObject(1), 123])),\n            ]\n        )\n\n        before_states = ProjectState()\n        before_states.add_model(author_name_nested_deconstructible_1.clone())\n        after_states = ProjectState()\n        after_states.add_model(author_name_nested_deconstructible_2.clone())\n\n        changes = MigrationAutodetector(before_states, after_states).changes()\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationFieldAttributes(\n            changes, \"testapp\", 0, 0,\n            name=\"name\",\n            old_name=\"name\",\n            field=models.CharField(max_length=200, default=[DeconstructibleObject(1), 123]),\n        )\n", "    def test_altered_managers(self):\n        # State\n        author_managers = [\n            ('objects', models.Manager()),\n        ]\n        new_author_managers = [\n            ('new_objects', models.Manager()),\n            ('objects', models.Manager()),\n        ]\n        other_app_managers = [\n            ('other_app_objects', models.Manager()),\n        ]\n        # Tests\n        old_state = self.make_project_state([\n            self.author_managers,\n            self.author_with_book,\n        ])\n        new_state = self.make_project_state([\n            self.author_with_book,\n            self.author_with_book_order_wrt,\n        ])\n        changes = self.get_changes(old_state, new_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelManagers'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', managers=new_author_managers)\n        # Reversed test\n        old_state = self.make_project_state([\n            self.author_with_book_order_wrt,\n            self.author_with_book,\n        ])\n        new_state = self.make_project_state([\n            self.author_with_book,\n        ])\n        changes = self.get_changes(old_state, new_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelManagers'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='author', managers=author_managers)\n", "    def test_rename_model_options(self):\n        # Define model states\n        new_author = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ], {\n            'permissions': [('can_hire', 'Can hire')],\n            'verbose_name': 'Renamed Author',\n        })\n\n        # Create autodetector\n        autodetector = MigrationAutodetector(self.author_empty, self.make_project_state([new_author]))\n\n        # Check generated operations\n        changes = autodetector._detect_changes()\n        self.assertEqual(len(changes['testapp']), 1)\n        self.assertEqual(changes['testapp'][0].name, 'auto_0001_initial')\n        self.assertEqual(len(changes['testapp'][0].operations), 1)\n        self.assertEqual(changes['testapp'][0].operations[0].__class__.__name__, 'RenameModel')\n        self.assertEqual(changes['testapp'][0].operations[0].old_name, 'Author')\n        self.assertEqual(changes['testapp'][0].operations[0].new_name, 'Author')\n\n        # Add rename model options operation\n        autodetector.add_operation('testapp', operations.RenameModel(\n            old_name='Author',\n            new_name='Renamed Author',\n            options={'permissions': [('can_hire', 'Can hire')]}\n        ), dependencies=[('testapp', 'Author', None, True)])\n\n        # Check generated operations again\n        changes = autodetector._detect_changes()\n        self.assertEqual(len(changes['testapp']), 1)\n        self.assertEqual(changes['testapp'][0].name, 'auto_0001_initial')\n        self.assertEqual(len(changes['testapp'][0].operations), 1)\n        self.assertEqual(changes['testapp'][0].operations[0].__class__.__name__, 'RenameModel')\n        self.assertEqual(changes['testapp'][0].operations[0].old_name, 'Author')\n        self.assertEqual(changes['testapp'][0].operations[0].new_name, 'Renamed Author')\n        self.assertEqual(changes['testapp'][0].operations[0].options, {'permissions': [('can_hire', 'Can hire')]})\n"], "sample_1178": ["def test_aug_assign():\n    x, y, z = symbols('x, y, z')\n    assert aug_assign(x, '+', y) == AddAugmentedAssignment(x, y)\n    assert aug_assign(x, '-', y) == SubAugmentedAssignment(x, y)\n    assert aug_assign(x, '*', y) == MulAugmentedAssignment(x, y)\n    assert aug_assign(x, '/', y) == DivAugmentedAssignment(x, y)\n    assert aug_assign(x, '%', y) == ModAugmentedAssignment(x, y)\n\n    assert aug_assign(x, '+', 1) == AddAugmentedAssignment(x, 1)\n    assert aug_assign(x, '-', 1) == SubAugmentedAssignment(x, 1)\n    assert aug_assign(x, '*', 2) == MulAugmentedAssignment(x, 2)\n    assert aug_assign(x, '/', 2) == DivAugmentedAssignment(x, 2)\n    assert aug_assign(x, '%', 3) == ModAugmentedAssignment(x, 3)\n\n    assert aug_assign(x, '+', y + 1) == AddAugmentedAssignment(x, y + 1)\n    assert aug_assign(x, '-', y + 1) == SubAugmentedAssignment(x, y + 1)\n    assert aug_assign(x, '*', y + 2) == MulAugmentedAssignment(x, y + 2)\n    assert aug_assign(x, '/', y + 3) == DivAugmentedAssignment(x, y + 3)\n    assert aug_assign(x, '%', y + 4) == ModAugmentedAssignment(x, y + 4)\n\n    assert aug_assign(x, '+', x) == AddAugmentedAssignment(x, x)\n    assert aug_assign(x, '-', x) == SubAugmentedAssignment(x, x)\n    assert aug_assign(x, '*', x) == MulAugmentedAssignment(x, x)\n    assert aug_assign(x, '/', x) == DivAugmentedAssignment(x, x)\n    assert aug_assign(x, '%', x) == ModAugmentedAssignment(x, x)\n\n    assert aug_assign(x, '+', x + y) == AddAugmentedAssignment(x, x + y)\n    assert aug_assign(x, '-', x + y) == SubAugmentedAssignment(x, x + y)\n    assert aug_assign(x, '*', x + y) == MulAugmentedAssignment(x, x + y)\n    assert aug_assign(x, '/', x + y) == DivAug", "def test_IntBaseType():\n    from sympy import ccode\n    i = Integer(10)\n    assert ccode(Type.from_expr(i)) == \"intc\"\n    assert ccode(Type.from_expr(i).cast_check(10)) == \"intc\"\n    assert raises(TypeError, Type.from_expr, None)\n    assert raises(ValueError, Type.from_expr, 'foo')\n    assert raises(ValueError, Type.from_expr, nan)\n    assert raises(ValueError, Type.from_expr, oo)\n    assert ccode(Type.from_expr(i)) == \"intc\"\n    assert ccode(Type.from_expr(10)) == \"intc\"\n    assert ccode(Type.from_expr(n)) == \"integer\"\n    assert ccode(Type.from_expr(n).cast_check(10)) == \"int64\"\n    assert ccode(Type.from_expr(n).cast_check(10.5)) == \"int64\"\n    assert ccode(Type.from_expr(n).cast_check(10)) == \"int64\"\n    assert ccode(Type.from_expr(n)) == \"integer\"\n    assert ccode(Type.from_expr(0)) == \"intc\"\n    assert ccode(Type.from_expr(10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(Integer(-10))) == \"intc\"\n    assert ccode(Type.from_expr(-10).cast_check(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10).cast_check(-11)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(n)) == \"integer\"\n    assert ccode(Type.from_expr(10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(n)) == \"integer\"\n    assert ccode(Type.from_expr(n)) == \"integer\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n    assert ccode(Type.from_expr(-10)) == \"intc\"\n\n", "compilation error", "def test_token_eq():\n    from sympy.codegen.ast import Token\n\n    # test eq method on Token\n    t1 = Token()\n    t2 = Token()\n    t3 = Token()\n    t3.args = t1.args\n\n    assert t1 == t2 is False\n    assert t1 == t3 is False\n    t2 = Token()\n    t2.args = t1.args\n    t2.__slots__ = t1.__slots__\n    assert t1 == t2 is True\n    assert t1 == t1 is True\n    assert t1 == t1.__class__() is True\n    assert t1 == Token.__new__(Token) is True\n\n    t3 = Token()\n    t3.__slots__ = t1.__slots__\n    t3.args = (t1.args[0],)\n    assert t1 == t3 is False\n\n    # test __hash__ method on Token\n    hash(t1)\n    assert hash(t1) == hash(t1)\n", "def test_Type_cast_check():\n    # Test casting a value out of bounds\n    with raises(ValueError):\n        float16.cast_check(float(1e3))\n\n    # Test casting a value within bounds\n    float16.cast_check(2.0)\n\n    # Test casting a value close to bounds\n    float16.cast_check(2.0 + 1e-20)\n\n    # Test casting a value outside the range of the type (Float16 in this case)\n    with raises(ValueError):\n        float16.cast_check(2.0 + 1e-15)\n\n    # Test casting a value to a different type\n    assert f32.cast_check(float(2.0)) == 2.0\n\n    # Test casting a Float to a Float\n    assert f32.cast_check(Float(2.0)) == 2.0\n\n    # Test casting a Float to a Float with a different decimal precision\n    assert f64.cast_check(Float(2.0, 10)) == 2.0\n\n    # Test casting a number with many decimal places\n    assert f32.cast_check(Float(123.456789012345678)) == 123.45678901234568\n\n    # Test casting a negative number to an unsigned integer type\n    with raises(ValueError):\n        uint8.cast_check(-1)\n\n    # Test casting a positive number to a signed integer type\n    assert int8.cast_check(127) == 127\n\n    # Test casting a negative number to a signed integer type\n    assert int8.cast_check(-128) == -128\n\n    # Test casting a value too large for an unsigned integer type\n    with raises(ValueError):\n        uint8.cast_check(256)\n\n    # Test casting a value to an integer type with the wrong number of bits\n    with raises(ValueError):\n        int8.cast_check(2**8)\n\n    # Test casting a float to an integer type\n    assert int8.cast_check(1.0) == 1\n\n    # Test casting a float to an integer type with rounding\n    assert int8.cast_check(1.5) == 1\n\n    # Test casting a float to an integer type with rounding up\n    assert int8.cast_check(1.9) == 1\n\n    # Test casting a float to an integer type with rounding down\n    assert int8.cast_check(-1.9) == -2\n\n    # Test casting a float to"], "sample_217": ["    def test_media_property(self):\n        class TestWidget(MultiWidget):\n            media = Media(css={'css1': ['css1.css'], 'css2': ['css2.css']}, js=['js1.js'])\n                super().__init__(attrs, [TextInput()])\n\n        widget = TestWidget()\n        self.assertEqual(widget.media.css, {'css1': ['css1.css'], 'css2': ['css2.css']})\n        self.assertEqual(widget.media.js, ['js1.js'])\n", "    def setUp(self):\n        self.widget = MultiWidget(widgets=[\n            TextInput(),\n            TextInput(),\n            TextInput(),\n        ])\n", "    def test_rendering_with_media_definition(self):\n        class MyWidget(Widget):\n            media = Media(css={'css': 'style.css', 'js': ['script.js']})\n\n        widget = MyWidget()\n        template = Template('''\n            {% load static %}\n            {{ widget.media.css }}\n            {{ widget.media.js }}\n        ''')\n        context = Context({'widget': widget})\n        rendered = template.render(context)\n        self.assertContainsHTML(rendered, '<link href=\"http://media.example.com/static/css/style.css\" rel=\"stylesheet\">')\n        self.assertContainsHTML(rendered, '<script src=\"http://media.example.com/static/js/script.js\"></script>')\n", "    def test_media_merge(self):\n        media = Media()\n        media2 = Media(css={'css1': ['css1.css'], 'css2': ['css2.css']})\n        media = media + media2\n        self.assertEqual(media._css, {'css1': ['css1.css', 'css2.css'], 'css2': ['css2.css']})\n", "    def test_media_widget_rendering(self):\n        # Create a test widget that renders a media widget\n        class TestWidget(MultiWidget):\n            template_name = 'django/forms/widgets/test_media_widget.html'\n            widgets = [TextInput()]\n\n                m = Media()\n                m._css_lists = [self.widgets[0].media._css]\n                m._js_lists = self.widgets[0].media._js\n                return m\n\n        # Render the media widget\n        test_widget = TestWidget()\n        html = test_widget.render('test', 'test')\n        self.assertIn('media.example.com/static/css/test_media_widget.css', html)\n        self.assertIn('media.example.com/static/js/test.js', html)\n"], "sample_806": ["def test_gradient_boosting_classify_toy_iris(presort, loss):\n    # Check classification on iris dataset.\n    iris = datasets.load_iris()\n    X = iris.data[:, :2]  # we only take the first two features.\n    y = iris.target\n\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                    random_state=1, presort=presort)\n\n    assert_raises(ValueError, clf.predict, X)\n\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(X), y)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (150, 10, 3))\n", "def test_gradient_boosting_regressor_toy():\n    # Check regression on a toy dataset.\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    T = [[-1, -1], [2, 2], [3, 2]]\n    true_result = [-1, 1, 1]\n\n    clf = GradientBoostingRegressor(loss='ls', n_estimators=10,\n                                  random_state=1)\n\n    assert_raises(ValueError, clf.predict, T)\n\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    mse_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(mse_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n\n    # Test regression with quantile loss\n    clf = GradientBoostingRegressor(loss='quantile', alpha=0.5,\n                                   n_estimators=10, random_state=1)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(T), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    mse_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(mse_decrease >= 0.0)\n\n    leaves = clf.apply(X)\n    assert_equal(leaves.shape, (6, 10, 1))\n", "def test_staged_predict_proba_output():\n    # Staged predict_proba should output an array of shape (n_samples, n_classes)\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    clf = GradientBoostingClassifier(loss='deviance', n_estimators=2, random_state=1)\n    clf.fit(X, y)\n\n    expected_output = np.array([[0.6321205588, 0.3678794412],\n                               [0.3678794412, 0.6321205588]])\n\n    for pred_proba in clf.staged_predict_proba(X):\n        assert_array_almost_equal(pred_proba, expected_output)\n", "def test_gradient_boosting_regressor_presort(sparse_data):\n    X, y = sparse_data\n    # Check regression on a toy dataset.\n    model = GradientBoostingRegressor(n_estimators=10, random_state=1, presort=presort)\n    model.fit(X, y)\n    assert_array_almost_equal(model.predict(X), y)\n    assert_equal(10, len(model.estimators_))\n    leaves = model.apply(X)\n    assert_equal(leaves.shape, (X.shape[0], 10, 1))\n\n    # Check regression with early stopping.\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    model = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                     validation_fraction=0.2, n_iter_no_change=10)\n    model.fit(X_train, y_train)\n    assert_array_almost_equal(model.predict(X_val), y_val)\n\n    # Check regression with warm start.\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    model = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                     warm_start=True)\n    model.fit(X_train, y_train)\n    assert_array_almost_equal(model.predict(X_val), y_val)\n    model.fit(X_train, y_train)\n    assert_array_almost_equal(model.predict(X_val), y_val)\n\n    # Check regression with different initial estimators.\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    model1 = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                      init=None)\n    model2 = GradientBoostingRegressor(n_estimators=100, random_state=1,\n                                      init='zero')\n    model1.fit(X_train, y_train)\n    model2.fit(X_train, y_train)\n    assert_array_almost_equal(model1.predict(X_val), y_val)\n    assert_array_almost_equal(model2.predict(X_val), y_val)\n", "def test_quantile_huber_loss_predictors(check_random_state(0)):\n    # Test that quantile and huber loss return the correct predictors\n    # for an initial estimator that returns a single value.\n\n    # Test the case with an initial estimator that returns a single value\n    init = ZeroEstimator()\n    X = np.array([[0, 0], [1, 1]])\n    y = np.array([0, 1])\n\n    # Test quantile loss\n    clf = GradientBoostingClassifier(loss='quantile', n_estimators=2, init=init)\n    raw_predictions = clf._raw_predict_init(X)\n    assert_array_almost_equal(raw_predictions, np.array([[0.0, 1.0], [1.0, 0.0]]))\n\n    # Test huber loss\n    clf = GradientBoostingClassifier(loss='huber', n_estimators=2, init=init)\n    raw_predictions = clf._raw_predict_init(X)\n    assert_array_almost_equal(raw_predictions, np.array([[0.0, 1.0], [1.0, 0.0]]))\n"], "sample_765": ["def test_hamming_loss_binary_label_indicators():\n    # Create binary label indicators\n    y_true = np.array([[0, 1], [1, 0]])\n    y_pred = np.array([[0, 1], [0, 0]])\n\n    # Hamming loss should be 0.5\n    hamming_loss_value = hamming_loss(y_true, y_pred)\n    assert_almost_equal(hamming_loss_value, 0.5)\n", "def test_make_prediction(binary):\n    y_true, y_pred, y_prob = make_prediction(binary=binary)\n    assert len(y_true) == len(y_pred) == len(y_prob)\n    assert (y_true >= 0).all()\n    assert (y_pred >= 0).all()\n    assert (y_prob >= 0).all() and (y_prob <= 1).all()\n", "    def test_binary_case(self):\n        # Test the Matthews correlation coefficient for a binary classification\n        # problem.\n        y_true = np.array([1, 1, 0, 0])\n        y_pred = np.array([1, 1, 1, 0])\n        assert_almost_equal(matthews_corrcoef(y_true, y_pred), 0.0)\n", "def test_fbeta_score_samples():\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import fbeta_score\n\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=15,\n                               n_redundant=5, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, weights=None,\n                               flip_y=0.01, class_sep=1.0, random_state=1)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                       random_state=42)\n\n    lr = LogisticRegression()\n    lr.fit(X_train, y_train)\n\n    y_pred = lr.predict(X_test)\n\n    # Test the fbeta_score function with sample_weight\n    sample_weight = np.random.rand(len(y_test))\n\n    # Test the fbeta_score function with beta=0.5\n    fbeta_05 = fbeta_score(y_test, y_pred, beta=0.5, sample_weight=sample_weight)\n    assert_almost_equal(fbeta_05, 0.80612775)\n\n    # Test the fbeta_score function with beta=1.0\n    fbeta_1 = fbeta_score(y_test, y_pred, beta=1.0, sample_weight=sample_weight)\n    assert_almost_equal(fbeta_1, 0.82666667)\n\n    # Test the fbeta_score function with beta=2.0\n    fbeta_2 = fbeta_score(y_test, y_pred, beta=2.0, sample_weight=sample_weight)\n    assert_almost_equal(fbeta_2, 0.80000000)\n", "def test_log_loss_multiclass():\n    \"\"\"Test the multiclass log loss.\n\n    In a multiclass classification problem, when there are more than two classes,\n    the log loss can be computed by treating the problem as a series of\n    binary classification problems, where the probability of the positive class\n    is the probability of each class.\n\n    Test the multiclass log loss with more than two classes and compare it to\n    the multiclass hinge loss.\n    \"\"\"\n    X, y = make_multilabel_classification(n_classes=3, n_samples=100, random_state=0)\n    y_pred = np.random.rand(100, 3)\n    expected_log_loss = log_loss(y, y_pred)\n    expected_hinge_loss = hinge_loss(y, y_pred)\n    # The log loss and hinge loss should be equal in multiclass classification\n    assert_almost_equal(expected_log_loss, expected_hinge_loss)\n\n    # Test the multiclass log loss with more than two classes and different weights\n    X, y = make_multilabel_classification(n_classes=3, n_samples=100, random_state=0)\n    y_pred = np.random.rand(100, 3)\n    sample_weight = np.random.rand(100)\n    expected_log_loss = log_loss(y, y_pred, sample_weight=sample_weight)\n    expected_hinge_loss = hinge_loss(y, y_pred, sample_weight=sample_weight)\n    # The log loss and hinge loss should be equal in multiclass classification\n    assert_almost_equal(expected_log_loss, expected_hinge_loss)\n\n    # Test the multiclass log loss with more than two classes and different labels\n    X, y = make_multilabel_classification(n_classes=3, n_samples=100, random_state=0)\n    y_pred = np.random.rand(100, 3)\n    labels = np.random.randint(0, 3, 100)\n    expected_log_loss = log_loss(y, y_pred, labels=labels)\n    expected_hinge_loss = hinge_loss(y, y_pred, labels=labels)\n    # The log loss and hinge loss should be equal in multiclass classification\n    assert_almost_equal(expected_log_loss, expected_hinge_loss)\n"], "sample_500": ["def test_colorbar_extension_length():\n    # Create a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, extendfrac in enumerate(['both', 'auto', 0.1, None]):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*4 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, extendfrac=extendfrac,\n                     orientation='horizontal', spacing='uniform')\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_extension_length_auto():\n    \"\"\"\n    Produce 12 colorbars with automatic length extensions for either\n    uniform or proportional spacing.\n\n    Helper function for test_colorbar_extension_length_auto.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    fig.subplots_adjust(hspace=.6)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        for j, spacing in enumerate(('uniform', 'proportional')):\n            # Create a subplot.\n            cax = fig.add_subplot(12, 1, i*3 + j + 1)\n            # Generate the colorbar.\n            Colorbar(cax, cmap=cmap, norm=norm,\n                     boundaries=boundaries, values=values,\n                     extend=extension_type, orientation='horizontal',\n                     spacing=spacing, extendfrac='auto')\n            # Turn off text and ticks.\n            cax.tick_params(left=False, labelleft=False,\n                              bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_extension_length_auto_spacing():\n    \"\"\"\n    Test that the extension lengths for a proportional colorbar with 'auto'\n    spacing are correct.\n\n    The length of the extension should be the same as the length of the\n    adjacent box in the colorbar.\n    \"\"\"\n    # Get a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n    # Create a figure and adjust whitespace for subplots.\n    fig = plt.figure()\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        # Get the appropriate norm and use it to get colorbar boundaries.\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        # Create a subplot.\n        cax = fig.add_subplot(4, 1, i + 1)\n        # Generate the colorbar.\n        Colorbar(cax, cmap=cmap, norm=norm,\n                 boundaries=boundaries, values=values,\n                 extend=extension_type, extendfrac='auto',\n                 orientation='horizontal', spacing='proportional')\n        # Turn off text and ticks.\n        cax.tick_params(left=False, labelleft=False,\n                        bottom=False, labelbottom=False)\n    # Return the figure to the caller.\n    return fig\n", "def test_colorbar_ticklabel_update():\n    \"\"\"\n    Test that updating a tick label of the colorbar properly updates\n    the label.\n    \"\"\"\n    # Create a figure with two subplots\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Create a scalar mappable\n    mappable = cm.ScalarMappable(norm=Normalize(vmin=0, vmax=1),\n                                cmap='viridis')\n\n    # Create a colorbar on the first subplot\n    cbar = Colorbar(ax[0], mappable)\n\n    # Set a tick label\n    cbar.set_ticklabels(['label1', 'label2', 'label3'])\n\n    # Update a tick label\n    cbar.set_ticklabels([None, 'new_label', 'label3'])\n\n    # Check that the new tick label has been properly updated\n    assert np.all(cbar.get_ticklabels() == ['None', 'new_label', 'label3'])\n\n    # Create a colorbar on the second subplot\n    cbar = Colorbar(ax[1], mappable)\n\n    # Set a tick label with minor ticks\n    cbar.set_ticklabels(['label1', 'label2', 'label3'], minor=True)\n\n    # Update a tick label with minor ticks\n    cbar.set_ticklabels([None, 'new_label', 'label3'], minor=True)\n\n    # Check that the new tick label has been properly updated\n    assert np.all(cbar.get_ticklabels() == ['None', 'new_label', 'label3'])\n", "def test_colorbar_extension_length_interactive():\n    \"\"\"\n    Test that the extend length of the colorbar is updated when the mappable\n    norm changes, with both uniform and proportional spacing.\n    \"\"\"\n    # Create a colormap and appropriate norms for each extension type.\n    cmap, norms = _get_cmap_norms()\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10), cmap=cmap)\n    im.norm = norms['neither']\n    cbar = Colorbar(ax, im)\n\n    # Create a new norm that includes a minimum extension.\n    new_norm = BoundaryNorm([-10] + norms['neither'].boundaries[1:], len(norms['neither'].boundaries) - 1)\n    im.norm = new_norm\n\n    # Check that the colorbar extension length is updated.\n    cbar.draw_all()\n    assert cbar._extend_lower()\n\n    # Check that the colorbar extension length is updated when the norm changes again.\n    im.norm = BoundaryNorm([-10] + norms['neither'].boundaries[:-1] + [10], len(norms['neither'].boundaries) - 1)\n    cbar.draw_all()\n    assert not cbar._extend_lower()\n\n    # Create a new figure and repeat the test with proportional spacing.\n    plt.close()\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.random.rand(10, 10), cmap=cmap)\n    im.norm = norms['neither']\n    cbar = Colorbar(ax, im, spacing='proportional')\n\n    # Create a new norm that includes a minimum extension.\n    new_norm = BoundaryNorm([-10] + norms['neither'].boundaries[1:], len(norms['neither'].boundaries) - 1)\n    im.norm = new_norm\n\n    # Check that the colorbar extension length is updated.\n    cbar.draw_all()\n    assert cbar._extend_lower()\n\n    # Check that the colorbar extension length is updated when the norm changes again.\n    im.norm = BoundaryNorm([-10] + norms['neither'].boundaries[:-1] + [10], len(norms['neither'].boundaries) - 1)\n    cbar.draw_all()\n    assert not cbar._extend_lower()\n\n    # Check that the colorbar extension length is updated when the norm is changed using the `update_normal` method.\n    im.norm = BoundaryNorm([-"], "sample_740": ["def test_check_symmetric():\n    X = np.array([[1, 2], [3, 4]])\n    assert check_symmetric(X)\n    with pytest.raises(ValueError):\n        check_symmetric(np.array([[1, 2], [3, 5]]))\n\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    assert check_symmetric(X)\n    with pytest.raises(ValueError):\n        check_symmetric(sp.csr_matrix(np.array([[1, 2], [3, 5]])))\n\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError):\n        check_symmetric(X)\n\n    with pytest.raises(ValueError):\n        check_symmetric(np.array([[1, 2], [3]]))\n\n    X = np.array([[1, 2], [3, 4]])\n    with pytest.raises(ValueError):\n        check_symmetric(X, raise_exception=True)\n\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    with pytest.raises(ValueError):\n        check_symmetric(X, raise_exception=True)\n\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    with pytest.raises(ValueError):\n        check_symmetric(X, raise_exception=True)\n\n    with pytest.raises(ValueError):\n        check_symmetric(np.array([[1, 2], [3]]), raise_exception=True)\n\n    X = np.array([[1, 2], [3, 4]])\n    assert check_symmetric(X, tol=1E-5)\n    assert check_symmetric(X, tol=1E-15)\n\n    X = sp.csr_matrix(np.array([[1, 2], [3, 4]]))\n    assert check_symmetric(X, tol=1E-5)\n    assert check_symmetric(X, tol=1E-15)\n", "def test_check_consistent_length_negative_samples():\n    # Test case 1: lists of different lengths\n    lists = [[1, 2, 3], [4, 5, 6, 7, 8]]\n    with pytest.raises(ValueError):\n        check_consistent_length(*lists)\n\n    # Test case 2: lists of equal lengths, but one list contains negative numbers\n    lists = [[1, 2, 3], [-1, -2, -3]]\n    with pytest.raises(ValueError):\n        check_consistent_length(*lists)\n\n    # Test case 3: arrays of different lengths\n    arrays = [np.array([1, 2, 3]), np.array([4, 5, 6, 7, 8])]\n    with pytest.raises(ValueError):\n        check_consistent_length(*arrays)\n\n    # Test case 4: arrays of equal lengths, but one array contains negative numbers\n    arrays = [np.array([1, 2, 3]), np.array([-1, -2, -3])]\n    with pytest.raises(ValueError):\n        check_consistent_length(*arrays)\n", "def test_check_is_fitted():\n    # Test that the estimator is fitted when it has a fit method\n    class MyEstimator:\n            self.coef_ = None\n\n            self.coef_ = np.array([1, 2, 3])\n            return self\n\n    estimator = MyEstimator()\n    check_is_fitted(estimator, ['coef_'])\n\n    # Test that the estimator is not fitted when it has no fit method\n    class MyNonFittedEstimator:\n        pass\n\n    with assert_raises(NotFittedError):\n        check_is_fitted(MyNonFittedEstimator(), ['coef_'])\n\n    # Test that the estimator is not fitted when it has a fit method but no attribute\n    class MyNonFittedEstimator:\n            return self\n\n    with assert_raises(NotFittedError):\n        check_is_fitted(MyNonFittedEstimator(), ['coef_'])\n\n    # Test that the attribute is not None\n    class MyFittedEstimator:\n            self.coef_ = np.array([1, 2, 3])\n\n            return self\n\n    estimator = MyFittedEstimator()\n    check_is_fitted(estimator, ['coef_'])\n\n    # Test that the attribute is not None when using all_or_any=all\n    class MyFittedEstimator:\n            self.coef_ = np.array([1, 2, 3])\n\n            return self\n\n    estimator = MyFittedEstimator()\n    check_is_fitted(estimator, ['coef_'], all_or_any=np.all)\n\n    # Test that the attribute is not None when using all_or_any=any\n    class MyFittedEstimator:\n            self.coef_ = None\n\n            self.coef_ = np.array([1, 2, 3])\n            return self\n\n    estimator = MyFittedEstimator()\n    check_is_fitted(estimator, ['coef_'], all_or_any=np.any)\n\n    # Test that a custom message can be passed\n    class MyEstimator:\n            self.coef_ = None\n\n        def fit", "def test_check_array_dtype():\n    X = np.array([1, 2, 3], dtype=np.float32)\n    check_array(X, dtype=np.float64)\n    assert X.dtype == np.float64\n\n    X = np.array([1, 2, 3], dtype=np.float64)\n    with pytest.raises(TypeError):\n        check_array(X, dtype=np.float32)\n\n    X = np.array([1, 2, 3], dtype=np.float32)\n    with pytest.raises(TypeError):\n        check_array(X, dtype=int)\n\n    X = np.array([1, 2, 3], dtype=np.float64)\n    with pytest.raises(TypeError):\n        check_array(X, dtype=list)\n\n    X = np.array([1, 2, 3], dtype=np.float32)\n    with pytest.raises(TypeError):\n        check_array(X, dtype={\"type\": \"numeric\"})\n\n    X = np.array([1, 2, 3], dtype=np.float64)\n    with pytest.raises(TypeError):\n        check_array(X, dtype=[\"numeric\", int])\n", "compilation error"], "sample_749": ["def test_column_transformer_remainder_fit_transform_passthrough():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    ct = ColumnTransformer(transformers=[\n        ('scale', StandardScaler(), [0, 1]),\n        ('drop', 'drop', [2])\n    ], remainder='passthrough')\n    ct.fit(X)\n    X_t = ct.transform(X)\n    assert_array_equal(X_t, np.array([[ 1.,  2.,  3.], [ 4.,  5.,  6.]]))\n", "def test_ColumnTransformer_get_feature_names():\n    \"\"\"Check that ColumnTransformer returns the correct feature names.\"\"\"\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = make_column_transformer(\n        (['column1'], Trans()),\n        (['column2'], DoubleTrans()),\n    )\n    ct.fit_transform(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['column1__', 'column2__'])\n\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = make_column_transformer(\n        (['column1'], Trans()),\n        (['column2'], DoubleTrans()),\n        remainder='passthrough',\n    )\n    ct.fit_transform(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['column1__', 'column2__', 'column3__'])\n\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = make_column_transformer(\n        (['column1'], Trans()),\n        (['column2'], DoubleTrans()),\n        remainder='passthrough',\n        n_jobs=-1,\n    )\n    ct.fit_transform(X)\n    feature_names = ct.get_feature_names()\n    assert_array_equal(feature_names, ['column1__', 'column2__', 'column3__'])\n\n    with pytest.raises(NotFittedError):\n        ct = make_column_transformer(\n            (['column1'], Trans()),\n            (['column2'], DoubleTrans()),\n        )\n        ct.get_feature_names()\n\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = make_column_transformer(\n        (['column1'], Trans()),\n        (['column2'], DoubleTrans()),\n        remainder='passthrough',\n        n_jobs=-1,\n    )\n    ct._transformers = None\n    with pytest.raises(NotFittedError):\n        ct.get_feature_names()\n\n    X = np.array([[0, 1, 2], [3, 4, 5]])\n    ct = make_column_transformer(\n        (['column1'], Trans()),\n        (['column2'], DoubleTrans()),\n        remainder='passthrough',\n        n_jobs=-1,\n    )\n    ct._transformers_ = []\n    with pytest.raises(NotFittedError):\n        ct.get_feature_names()\n\n    X = np.array", "def test_column_transformer_with_remainder_transformer():\n    # Test case where the remainder transformer is not an instance of ColumnTransformer\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    ct = ColumnTransformer([('drop', StandardScaler(), [0]), ('passthrough', 'passthrough', [1]), ('remainder', TransNo2D(), [2])], remainder=Trans())\n    ct.fit_transform(X)\n    assert_equal(ct.transformer_weights, {})\n    assert_equal(len(ct.transformers_), 3)\n", "def test_set_params():\n    X = np.array([[0., 1., 2., 2.],\n                  [1., 1., 0., 1.]])\n\n    ct = ColumnTransformer([('norm', StandardScaler(), [0, 1]),\n                           ('drop', 'drop', [2, 3])])\n    ct.fit(X)\n\n    # change one of the transformers\n    ct.set_params(transformers=[('norm', StandardScaler(), [0, 2]),\n                               ('drop', 'drop', [1, 3])])\n\n    assert ct.named_transformers_['norm'] is not ct.transformers_[0][1]\n    assert ct.named_transformers_['norm'] is ct.transformers_[0][1]\n\n    # test passing through non-string keys\n    ct = ColumnTransformer([('norm', StandardScaler(), [0, 1]),\n                           ('drop', 'drop', 2)],\n                          remainder='passthrough')\n    ct.set_params(transformers=[('norm', StandardScaler(), 0),\n                               ('drop', 'drop', 2)],\n                  remainder='passthrough')\n\n    # test passing through non-string keys and non-string values\n    ct = ColumnTransformer([('norm', StandardScaler(), [0, 1]),\n                           ('drop', 'drop', 2)],\n                          remainder='passthrough')\n    ct.set_params(transformers=[('norm', StandardScaler(), 0),\n                               ('drop', 'drop', 2)],\n                  remainder='passthrough')\n\n    # test that get_params returns a deep copy\n    ct = ColumnTransformer([('norm', StandardScaler(), [0, 1]),\n                           ('drop', 'drop', [2, 3])])\n    deep_params = ct.get_params(deep=True)\n    shallow_params = ct.get_params(deep=False)\n    ct.set_params(**deep_params)\n    ct.set_params(**shallow_params)\n    assert ct.transformers_ != ct.transformers_\n", "def test_Transformer_in_transformers():\n    \"\"\"Test that a non-compositional transformer in transformers results\n    in a TransformerMixin being appended directly.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n\n    ct = ColumnTransformer([\n        ('double', DoubleTrans(), [0]),\n        ('scaler', StandardScaler(), [1]),\n        ('transformer', Trans(), [0, 1])])\n\n    # Since Trans is not a composition of transformers, it should be\n    # appended directly\n    assert len(ct.named_transformers_) == 3\n    assert isinstance(ct.named_transformers_['transformer'], Trans)\n    ct.fit(X)\n    ct.transform(X)\n\n    # Check that it is being called correctly\n    assert_array_equal(ct.transform(X), [[2., 4.], [6., 8.]])\n"], "sample_1010": ["def test_latex_ModuleImplementedIdeal():\n    from sympy import ModuleImplementedIdeal\n    from sympy.printing.latex import latex\n    from sympy.abc import x, y, z\n    I = ModuleImplementedIdeal(x, [x**2, y**2])\n    assert latex(I) == r\"\\left< x^{2}, y^{2} \\right>\"\n", "def test_LatexPrinter_TensorProduct():\n    from sympy.tensor.tensor import TensorProduct\n    from sympy import symbols, TensorIndex, Matrix, ZeroMatrix, Identity\n    i, j, k, l = symbols('i j k l')\n    i1, j1, k1 = TensorIndex('i1', 'lower'), TensorIndex('j1', 'lower'), TensorIndex('k1', 'lower')\n    i2, j2, k2 = TensorIndex('i2', 'lower'), TensorIndex('j2', 'lower'), TensorIndex('k2', 'lower')\n\n    tp = TensorProduct(i1, j1, k1)\n    tp2 = TensorProduct(i2, j2, k2)\n    print(latex(tp.tensor_product(tp2)))\n\n    tp3 = TensorProduct(i1, j1, k1)\n    tp4 = TensorProduct(j1, k1, i1)\n    print(latex(tp.tensor_product(tp4)))\n", "    def test_hprint_UndefinedFunction(self):\n        printer = LatexPrinter()\n        expr = Expression('f(x, y, z)')\n        self.assertEqual(printer._hprint_UndefinedFunction(expr.func), r'\\operatorname{f}')\n", "def test_latex_KroneckerDelta():\n    assert latex(KroneckerDelta(x, x)) == r'\\delta_{x, x}'\n    assert latex(KroneckerDelta(x, y)) == r'\\delta_{x, y}'\n    assert latex(KroneckerDelta(x)) == r'\\delta_{x}'\n", "def test_LatexPrinter_translated_symbols():\n    L = LatexPrinter({'symbol_names': {x: 'x_i'}})\n    assert L._print(x) == 'x_i'\n    assert L._print(x**2) == 'x_i^2'\n    assert L._print(x**2 + x) == 'x_i^2 + x_i'\n"], "sample_374": ["    def test_prefetch_related_with_prefetch_and_filter(self):\n        \"\"\"\n        Tests prefetch_related() with both Prefetch and filter().\n        \"\"\"\n        # Given\n        reader = Reader.objects.prefetch_related(\n            Prefetch('books_read__authors', queryset=Author.objects.filter(name='Anne')),\n            'books_read__book_review__comment__author'\n        ).get(id=1)\n\n        # When\n        comments = reader.books_read.first().book_review.comment_set.all()\n\n        # Then\n        self.assertEqual(len(comments), 1)\n        self.assertEqual(comments.first().author, comments.first().id)\n", "    def test_prefetch_related_filtering(self):\n        # Test that prefetching with filtering works correctly.\n        reader = Reader.objects.prefetch_related(Prefetch('books_read', queryset=Book.objects.filter(title__startswith='P'))).get()\n        self.assertEqual(reader.books_read.count(), 1)\n        self.assertEqual(reader.books_read.first().title, 'Poems')\n", "    def test_prefetch_related_multiple_lookups(self):\n        qs = Book.objects.select_related('authors').prefetch_related('bookreviews__author')\n        qs = qs.select_related('bookreviews__book__authors')\n        books = list(qs)\n        self.assertEqual(len(books), 4)\n        for book in books:\n            self.assertTrue(isinstance(book.authors.all(), QuerySet))\n            self.assertTrue(isinstance(book.bookreviews.all(), QuerySet))\n            for review in book.bookreviews.all():\n                self.assertTrue(isinstance(review.author, Author))\n                self.assertTrue(isinstance(review.book, Book))\n                self.assertTrue(isinstance(review.book.authors.all(), QuerySet))\n", "    def test_raw_queryset_iterator(self):\n        with CaptureQueriesContext() as captures:\n            raw_queryset = Book.objects.raw(\"SELECT * FROM myapp_book\")\n            list(raw_queryset)\n            self.assertEqual(len(captures.captured_queries), 1)\n            self.assertEqual(captures.captured_queries[0].sql, \"SELECT * FROM myapp_book\")\n", "    def test_prefetching_no_prefetch_related_lookups(self):\n        Author.objects.prefetch_related('books').filter(name='Charlotte').exists()\n        self.assertEqual(connection.queries, [])\n"], "sample_113": ["    def setUp(self):\n        self.superuser = views.create_superuser()\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        This is a docstring\n        with multiple lines\n        and leading/trailing whitespace\n        \"\"\"\n        expected = \"This is a docstring with multiple lines and leading/trailing whitespace\"\n        self.assertEqual(trim_docstring(docstring), expected)\n", "    def setUp(self):\n        self.docstring = \"\"\"", "    def test_trim_docstring(self):\n        docstring = \"\"\"\n        Foo\n        =============\n\n        This is a docstring.\n        \"\"\"\n        result = utils.trim_docstring(docstring)\n        self.assertEqual(result, \"This is a docstring.\")\n"], "sample_884": ["def test_deprecated_decorator_inheritance():\n    class MockClass7(MockClass1):\n        pass\n\n    instance = MockClass7()\n\n    assert _is_deprecated(instance.__init__)\n    assert _is_deprecated(instance.__class__.__init__)\n\n    with pytest.warns(FutureWarning):\n        instance.__init__()\n\n    assert instance.a is None\n    assert isinstance(instance, MockClass1)\n", "def test_inheritance_from_deprecated_class():\n    with pytest.warns(FutureWarning) as record:\n        MockClass7()  # This should issue a warning\n    assert len(record) == 1\n    assert record[0].message.args[0].startswith(\n        \"Class MockClass1 is deprecated\"\n    )\n", "def test_deprecated_decorator_pickling():\n    # Create a MockClass1 instance\n    instance = MockClass1()\n\n    # Pickle the instance\n    pickled_instance = pickle.dumps(instance)\n\n    # Check if the pickled instance can be unpickled\n    unpickled_instance = pickle.loads(pickled_instance)\n\n    # Check if the unpickled instance is still marked as deprecated\n    assert _is_deprecated(unpickled_instance.__init__)\n", "def test_deprecated_decorator_docstring():\n    class MockClass:\n        \"\"\"Deprecated class.\"\"\"\n        @deprecated(\"a message\")\n        pass\n\n    assert \"a message\" in MockClass.__doc__\n\n", "def test_deprecated_init():\n    # Test that a deprecated class instance raises a FutureWarning\n    with pytest.warns(FutureWarning):\n        _ = MockClass3()\n\n    # Test that a non-deprecated class does not raise a FutureWarning\n    with pytest.warns(UserWarning):\n        _ = MockClass4()\n\n    # Test that a deprecated class's child class raises a FutureWarning\n    with pytest.warns(FutureWarning):\n        _ = MockClass5(10)\n\n    # Test that a class that overrides __new__ still raises a FutureWarning\n    with pytest.warns(FutureWarning):\n        _ = MockClass6(10)\n\n"], "sample_770": ["def test_calinski_harabasz_score_empty_labels():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([])\n    with pytest.raises(ValueError):\n        calinski_harabasz_score(X, labels)\n\n", "def test_calinski_harabasz_score_empty_labels():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    labels = np.array([0, 0, 0])\n    with pytest.raises(ValueError) as e:\n        calinski_harabasz_score(X, labels)\n    assert \"Number of labels is\" in str(e.value)\n", "def test_calinski_harabasz_score_handle_one_cluster():\n    X = np.array([[1, 2], [1, 2], [1, 2]])\n    labels = np.array([0, 0, 0])\n    assert_greater(calinski_harabasz_score(X, labels), 0, decimal=6)\n    assert calinski_harabasz_score(X, labels) == calinski_harabaz_score(X, labels)\n", "def test_silhouette_score_precomputed():\n    \"\"\"Test silhouette_score on precomputed distance matrix\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    D = pairwise_distances(X)\n    score = silhouette_score(D, [0, 0, 1])\n    assert_greater(score, -1, 'Silhouette Coefficient should be greater than -1')\n    assert_less(score, 1, 'Silhouette Coefficient should be less than 1')\n", "def test_check_number_of_labels():\n    # Test with 1 label\n    with pytest.raises(ValueError):\n        check_number_of_labels(1, 10)\n\n    # Test with too many labels\n    with pytest.raises(ValueError):\n        check_number_of_labels(11, 10)\n\n    # Test with valid number of labels\n    check_number_of_labels(2, 10)\n\n    # Test with single element in dataset\n    check_number_of_labels(1, 1)\n\n    # Test with empty labels array\n    check_number_of_labels(2, 0)\n"], "sample_151": ["    def test_alter_order_with_respect_to(self):\n        before_states = [\n            self.author_with_book_order_wrt,\n        ]\n        after_states = [\n            self.author_with_book_order_wrt,\n            self.author_with_book,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AlterModelOptions'])\n        self.assertOperationTypes(changes, 'testapp', 1, ['CreateModel', 'AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, options={'order_with_respect_to': 'book'})\n        self.assertOperationAttributes(changes, 'testapp', 1, 0, model_name='Author', fields=[('id', models.AutoField(primary_key=True)), ('name', models.CharField(max_length=200))], options={}, bases=[], managers=[])\n", "    def test_rename_field(self):\n        before_states = [self.author_name, self.author_name_renamed]\n        after_states = [self.author_renamed_with_book]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, unique_together=[(\"author\", \"title\")])\n", "    def test_renamed_field_non_null(self):\n        from django.db.models import CharField\n        from .models import author_name, author_name_renamed\n\n        before = [author_name]\n        after = [author_name_renamed]\n        expected_migrations = {\n            'testapp': [\n                MigrationAutodetector(\"rename_field_name\", \"author_name\", \"name\")\n            ]\n        }\n        self.assertEqual(self.get_changes(before, after), expected_migrations)\n", "    def test_alter_model_options_with_custom_manager(self):\n        \"\"\"\n        Tests that AlterModelOptions correctly handles model options that refer\n        to custom managers.\n        \"\"\"\n        author_unmanaged = ModelState(\"testapp\", \"AuthorUnmanaged\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ])\n        author_managed = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n        ], managers=[(\"unmanaged\", \"object\"), (\"managed\", \"object\")])\n        changes = MigrationAutodetector(\n            self.make_project_state([author_unmanaged]),\n            self.make_project_state([author_managed]),\n        ).changes()\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        migration = changes[\"testapp\"][0]\n        self.assertEqual(migration.name, \"0001_initial\")\n        self.assertIsInstance(migration.operations[0], AlterModelOptions)\n        self.assertEqual(migration.operations[0].options, {\n            'managers': [\n                ('unmanaged', 'object'),\n                ('managed', 'object'),\n            ],\n        })\n", "def test_generate_created_proxies_with_options(self):\n    # Initial models\n    app_label = \"testapp\"\n    initial_state = self.make_project_state([\n        self.author_proxy,\n        self.publisher_with_author,\n        self.book,\n    ])\n    # Modified models with proxy option removed\n    modified_state = self.make_project_state([\n        self.publisher_with_author,\n        self.book,\n    ])\n    # Generate migrations\n    changes = self.get_changes(\n        initial_state.models,\n        modified_state.models,\n    )\n    self.assertEqual(changes.get(app_label, []), [\n        self.make_migration(\n            \"0001_auto_0001_initial\",\n            dependencies=[(app_label, \"__first__\")],\n            operations=[\n                operations.DeleteModel(\n                    name=\"testapp.AuthorProxy\",\n                ),\n            ],\n        ),\n    ])\n    self.assertEqual(changes.get(\"otherapp\", []), [\n        self.make_migration(\n            \"0001_auto_0001_initial\",\n            dependencies=[(app_label, \"0001_auto_0001_initial\")],\n            operations=[\n                operations.CreateModel(\n                    name=\"book\",\n                    fields=[\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                        (\"title\", models.CharField(max_length=200)),\n                    ],\n                    options={\n                        \"indexes\": [models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\")],\n                    },\n                ),\n            ],\n        ),\n    ])\n"], "sample_125": ["def test_set_signed_cookie(self):\n    response = HttpResponse()\n    response.set_signed_cookie('my_cookie', 'hello')\n    signed_value = response.cookies['my_cookie'].value\n    self.assertIn('signed_value', signed_value)\n    response2 = HttpResponse()\n    response2.set_signed_cookie('my_cookie', 'hello', salt='my_salt')\n    signed_value = response2.cookies['my_cookie'].value\n    self.assertIn('my_salt', signed_value)\n    self.assertNotEqual(signed_value, response.cookies['my_cookie'].value)\n", "    def test_set_cookie_expires_date_format(self):\n        response = HttpResponse()\n        expires_date = datetime(2024, 3, 16, tzinfo=utc)\n        response.set_cookie('test_cookie', 'test_value', expires=expires_date)\n        self.assertEqual(response.cookies['test_cookie']['expires'], expires_date.strftime('%a, %d %b %Y %H:%M:%S GMT'))\n", "def test_cookie_domain(self):\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com')\n    self.assertIn('domain=example.com', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path')\n    self.assertIn('path=/path; domain=example.com', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path', secure=True)\n    self.assertIn('path=/path; domain=example.com; secure', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path', secure=True, httponly=True)\n    self.assertIn('path=/path; domain=example.com; secure; httponly', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path', secure=True, httponly=True, samesite='strict')\n    self.assertIn('path=/path; domain=example.com; secure; httponly; samesite=strict', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path', secure=True, httponly=True, samesite='lax')\n    self.assertIn('path=/path; domain=example.com; secure; httponly; samesite=lax', response.cookies['test_cookie'].output_string())\n\n    response = HttpResponse()\n    response.set_cookie('test_cookie', value='test_value', domain='example.com', path='/path', secure=True, httponly=True, samesite='none')\n    self.assertIn('path=/path; domain=example.com; secure; httponly; samesite=None', response.cookies['test_cookie'].output_string())\n", "def test_set_cookie_default_max_age(self):\n    response = HttpResponse()\n    response.set_cookie('test_cookie', 'test_value')\n    self.assertEqual(response.cookies['test_cookie']['max-age'], 31536000)\n", "    def test_header_set(self):\n        response = HttpResponse()\n        response['Set-Cookie'] = 'key=value'\n        self.assertEqual(response['Set-Cookie'], 'key=value')\n        self.assertEqual(response.has_header('Set-Cookie'), True)\n        self.assertEqual(response.get('Set-Cookie'), 'key=value')\n        response['Set-Cookie'] = 'new_value'\n        self.assertEqual(response['Set-Cookie'], 'new_value')\n        del response['Set-Cookie']\n        self.assertEqual(response.has_header('Set-Cookie'), False)\n"], "sample_426": ["    def test_timesince_future_date(self):\n        self.t = datetime.datetime(2024, 7, 26, 14, 30, 0)\n        now = datetime.datetime(2024, 7, 26, 13, 30, 0)\n        self.assertEqual(timesince(self.t, now), '1 minute')\n", "    def test_timesince_zero_delta(self):\n        self.assertEqual(timesince(self.t - self.oneminute, now=self.t), \"0 minutes\")\n", "    def test_zero_time(self):\n        self.assertEqual(timesince(self.t, self.t), \"0 minutes\")\n", "    def test_timesince_falsy_now(self):\n        with self.assertRaises(ValueError):\n            timesince(datetime.datetime(2007, 8, 14, 13, 46, 0), None, reversed=True)\n", "    def test_timesince_with_depth_zero(self):\n        with self.assertRaises(ValueError):\n            timesince(self.t, depth=0)\n"], "sample_35": ["def test_minversion_version_path():\n    # Test minversion with a custom version path\n    assert minversion('astropy', '0.4.4', version_path='version')\n    assert not minversion('astropy', '0.3.3', version_path='version')\n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '0.1.0')\n    with pytest.raises(ValueError):\n        minversion(None, '0.1.0')\n", "def test_minversion_import_name():\n    # Test that minversion raises an ImportError when trying to import a module name that doesn't exist\n    with pytest.raises(ImportError):\n        minversion('nonexistent_module', '1.0')\n\n    # Test that minversion returns False when the module doesn't meet the version requirement\n    with pytest.raises(ImportError):\n        minversion('astropy', '1.0')\n\n    # Test that minversion returns True when the module meets the version requirement\n    assert minversion('astropy', '0.4.4')\n\n    # Test that minversion returns False when the module doesn't meet the version requirement with inclusive=False\n    assert not minversion('astropy', '0.4.3', inclusive=False)\n\n    # Test that minversion returns True when the module meets the version requirement with inclusive=False\n    assert minversion('astropy', '0.4.5', inclusive=False)\n\n    # Test that minversion raises a ValueError when the version is not a valid version string\n    with pytest.raises(ValueError):\n        minversion('astropy', 'invalid_version')\n\n    # Test that minversion raises a ValueError when the version_path is not a string\n    with pytest.raises(ValueError):\n        minversion('astropy', '1.0', version_path=123)\n\n    # Test that minversion raises a ValueError when the version_path is not a dotted attribute path\n    with pytest.raises(ValueError):\n        minversion('astropy', '1.0', version_path='invalid_path')\n\n    # Test that minversion raises a TypeError when the version is not a string\n    with pytest.raises(TypeError):\n        minversion('astropy', 1.0)\n", "def test_find_mod_objs_empty_module():\n    # Test that an empty module is handled correctly\n    class EmptyModule:\n        pass\n\n    localnames, fqnames, objs = introspection.find_mod_objs('astropy.utils.introspection.EmptyModule')\n    assert localnames == []\n    assert fqnames == []\n    assert objs == []\n", "def test_minversion_version_path_error():\n    \"\"\"Test passing a non-string or non-existent version_path raises an error.\"\"\"\n    with pytest.raises(ValueError):\n        minversion('module', '0.12', version_path='__version__._path')\n\n    with pytest.raises(ValueError):\n        minversion('module', '0.12', version_path=1)\n\n    with pytest.raises(ValueError):\n        minversion('module', '0.12', version_path=['__version__._path'])\n\n    with pytest.raises(ValueError):\n        minversion('module', '0.12', version_path=[])\n\n    with pytest.raises(ValueError):\n        minversion('module', '0.12', version_path={'path': 'value'})\n", "def test_minversion__module_string():\n    # Test that a module name can be used instead of a module object\n    assert minversion('astropy', '0.4.4') is True\n    assert minversion('astropy', '0.4.3') is False\n"], "sample_1202": ["def test_Float_creation():\n    # Test creating a Float from a float\n    f = Float(1.0)\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from a string with a minimum precision\n    f = Float('1')\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from a string with a specified precision\n    f = Float('1', 10)\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from a float with a specified precision\n    f = Float(1.0, 10)\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from an integer\n    f = Float(1)\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from a Rational\n    f = Float(Rational(1, 2))\n    assert f == Float(0.5)\n    assert f._mpf_ == Float(0.5)._mpf_\n\n    # Test creating a Float from a decimal\n    d = decimal.Decimal('1.0')\n    f = Float(d)\n    assert f == Float(1.0)\n    assert f._mpf_ == Float(1.0)._mpf_\n\n    # Test creating a Float from a tuple\n    f = Float((1, 5, 0))\n    assert f == Float(5.0)\n    assert f._mpf_ == Float(5.0)._mpf_\n\n    # Test creating a Float from a string that represents infinity\n    f = Float('inf')\n    assert f == S.Infinity\n    assert f._mpf_ == S.Infinity._mpf_\n\n    # Test creating a Float from a string that represents negative infinity\n    f = Float('-inf')\n    assert f == S.NegativeInfinity\n    assert f._mpf_ == S.NegativeInfinity._mpf_\n\n    # Test creating a Float from a string that represents NaN\n    f", "def test_GoldenRatio__int():\n    from sympy import S, GoldenRatio\n    assert isinstance(GoldenRatio.__int__(), int)\n    assert GoldenRatio.__int__() == 1\n    assert S.GoldenRatio.__int__() == 1\n    assert S(1).__int__() == 1\n", "compilation error", "    def test_Rational(self):\n        from sympy import Rational, S\n        from sympy import Integer, Float\n\n        # test with floats\n        self.assertEqual(Rational(1.0, 2.0), Rational(1, 2))\n        self.assertEqual(Rational(3.5), Rational(7, 2))\n        self.assertEqual(Rational(1.5, 2.0), Rational(3, 4))\n        self.assertEqual(Rational(4.0, 2.0), Rational(2, 1))\n\n        # test with strings\n        self.assertEqual(Rational('1.0/2.0'), Rational(1, 2))\n        self.assertEqual(Rational('7/2'), Rational(7, 2))\n        self.assertEqual(Rational('3.5'), Rational(7, 2))\n        self.assertEqual(Rational('1.5/2'), Rational(3, 4))\n        self.assertEqual(Rational('4/2'), Rational(2, 1))\n\n        # test with ints\n        self.assertEqual(Rational(1, 2), Rational(1, 2))\n        self.assertEqual(Rational(7, 2), Rational(7, 2))\n        self.assertEqual(Rational(4, 2), Rational(2, 1))\n\n        # test with sympy ints\n        self.assertEqual(Rational(S(1), 2), Rational(1, 2))\n        self.assertEqual(Rational(S(7), 2), Rational(7, 2))\n        self.assertEqual(Rational(S(4), 2), Rational(2, 1))\n\n        # test with floats from strings\n        self.assertEqual(Rational('0.5'), Rational(1, 2))\n        self.assertEqual(Rational('3.5'), Rational(7, 2))\n\n        # test with strings with scientific notation\n        self.assertEqual(Rational('1.5e-1'), Rational(1.5, 10))\n        self.assertEqual(Rational('3e1'), Rational(30, 1))\n\n        # test with strings with decimal point\n        self.assertEqual(Rational('3.14'), Rational(157, 50))\n        self.assertEqual(Rational('0.5'), Rational(1, 2))\n\n        # test with floats from strings with precision\n        self.assertEqual(Rational('0.5', 5), Rational(251/502, ", "    def test_Float_zero(self):\n        assert float(Float(0)) == 0\n        assert float(Float(0)._mpf_) == 0\n        assert float(_mpf_ninf) == float(_ninf)\n        assert float(_mpf_inf) == float(_inf)\n"], "sample_778": ["def test_nmf_init_with_custom_matrices():\n    # Test that we can initialize NMF with custom matrices W and H\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    W = np.array([[1, 0], [0, 1]])\n    H = np.array([[1, 1], [1, 0]])\n\n    model = NMF(init='custom', W=W, H=H)\n    model.fit(X)\n\n    assert_array_almost_equal(model.components_, H)\n    assert_array_almost_equal(model.transform(X), np.dot(X, H))\n\n    # Test that we can initialize NMF with custom matrices W and H when update_H=False\n    model = NMF(init='custom', W=W, H=H)\n    model.fit(X, update_H=False)\n\n    assert_array_almost_equal(model.components_, H)\n    assert_array_almost_equal(model.transform(X), np.dot(X, H))\n", "def test_compute_regularization():\n    # Test the _compute_regularization function\n    alpha = 1.0\n    l1_ratio = 0.5\n    regularization = 'both'\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(alpha, l1_ratio, regularization)\n    assert l1_reg_W == 0.5\n    assert l1_reg_H == 0.5\n    assert l2_reg_W == 0.5\n    assert l2_reg_H == 0.5\n\n    regularization = 'components'\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(alpha, l1_ratio, regularization)\n    assert l1_reg_W == 1.0\n    assert l1_reg_H == 0.0\n    assert l2_reg_W == 0.0\n    assert l2_reg_H == 0.0\n\n    regularization = 'transformation'\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(alpha, l1_ratio, regularization)\n    assert l1_reg_W == 0.0\n    assert l1_reg_H == 0.0\n    assert l2_reg_W == 1.0\n    assert l2_reg_H == 1.0\n\n    regularization = None\n    l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H = _compute_regularization(alpha, l1_ratio, regularization)\n    assert l1_reg_W == 0.0\n    assert l1_reg_H == 0.0\n    assert l2_reg_W == 0.0\n    assert l2_reg_H == 0.0\n", "def test_nmf_reconstruction_err():\n    # Check that reconstruction error is positive\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    model = NMF(n_components=2, init='random', random_state=0)\n    W = model.fit_transform(X)\n    H = model.components_\n    reconstruction_err = model.reconstruction_err_\n    assert_greater(reconstruction_err, 0)\n\n    # Check that reconstruction error is consistent across multiple runs\n    np.random.seed(0)\n    model = NMF(n_components=2, init='random', random_state=0)\n    W1 = model.fit_transform(X)\n    H1 = model.components_\n    reconstruction_err1 = model.reconstruction_err_\n    np.random.seed(0)\n    model = NMF(n_components=2, init='random', random_state=0)\n    W2 = model.fit_transform(X)\n    H2 = model.components_\n    reconstruction_err2 = model.reconstruction_err_\n    assert_almost_equal(reconstruction_err1, reconstruction_err2)\n\n    # Check that reconstruction error is consistent with np.dot(W, H)\n    model = NMF(n_components=2, init='random', random_state=0)\n    W = model.fit_transform(X)\n    H = model.components_\n    reconstruction_err = model.reconstruction_err_\n    W_check = np.dot(W, H)\n    assert_array_almost_equal(X, W_check)\n\n    # Check that reconstruction error is consistent with np.dot(W, H) with beta_loss = 1\n    model = NMF(n_components=2, init='random', random_state=0, beta_loss='kullback-leibler')\n    W = model.fit_transform(X)\n    H = model.components_\n    reconstruction_err = model.reconstruction_err_\n    W_check = np.dot(W, H)\n    assert_array_almost_equal(X, W_check)\n\n    # Check that reconstruction error is consistent with np.dot(W, H) with beta_loss = 0\n    model = NMF(n_components=2, init='random', random_state=0, beta_loss='itakura-saito')\n    W = model.fit_transform(X)\n    H = model.components_\n    reconstruction_err = model.reconstruction_err_\n    W_check = np.dot(W, H)\n    assert_array_almost_equal(X, W_check)\n\n   ", "def test_non_negative_factorization_shuffle():\n    # Test that the Coordinate Descent solver randomizes the order of coordinates\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    model = NMF(n_components=2, solver='cd', shuffle=True, random_state=0)\n    model.fit(X)\n\n    # The coordinates should be different for each run due to shuffling\n    assert not np.array_equal(model.components_.T, model.components_.T)\n", "def test_nmf_solver_cd_with_negative_values():\n    # Test that the Coordinate Descent solver does not use negative values\n    # in the W or H matrices.\n    X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n    X[1, 1] = -1\n    model = NMF(n_components=2, solver='cd', init='random')\n    W = model.fit_transform(X)\n\n    # Check that W does not contain negative values\n    assert_less_equal(W.min(), 0).sum() == 0\n    # Also check that the product WH does not contain negative values\n    assert_less_equal(np.dot(W, model.components_).min(), 0).sum() == 0\n"], "sample_82": ["    def test_split_datetime_widget_decompress(self):\n        value = datetime.datetime(2022, 1, 1, 12, 30, 0)\n        self.assertEqual(self.widget.decompress(value), [value.date(), value.time()])\n", "    def test_decompress(self):\n        widget = SplitDateTimeWidget(\n            date_format='%Y-%m-%d',\n            time_format='%H:%M',\n        )\n        self.assertEqual(\n            widget.decompress(date(2012, 6, 1)),\n            [date(2012, 6, 1), datetime.time(0, 0)]\n        )\n        self.assertEqual(\n            widget.decompress('2012-06-01 12:00'),\n            [date(2012, 6, 1), datetime.time(12, 0)]\n        )\n", "    def test_select_date_widget_with_months_and_empty_label(self):\n        form = Form({\n            'year': '2007',\n            'month': '01',\n            'day': '01',\n        })\n        rendered = self.widget.render('date', form.data)\n        self.assertIn('<select id=\"id_year\" name=\"year\">', rendered)\n        self.assertIn('<option value=\"2007\"', rendered)\n        self.assertIn('<option value=\"---\" selected>', rendered)\n        self.assertIn('<select id=\"id_month\" name=\"month\">', rendered)\n        self.assertIn('<option value=\"01\"', rendered)\n        self.assertIn('<option value=\"---\" selected>', rendered)\n        self.assertIn('<select id=\"id_day\" name=\"day\">', rendered)\n        self.assertIn('<option value=\"01\"', rendered)\n", "    def test_default_rendering(self):\n        field = DateField()\n        field.widget = self.widget\n        form = Form({field.name: date(2009, 11, 11)})\n        self.assertHTMLEqual(\n            self.widget.render('field', form.cleaned_data['field']),\n            '<input id=\"id_field_0\" name=\"field_0\" type=\"hidden\" type=\"hidden\">'\n            '<input id=\"id_field_1\" name=\"field_1\" type=\"hidden\">',\n        )\n", "    def test_id_for_label(self):\n        # Make sure `id_for_label` returns the first select's ID\n        widget = SelectDateWidget()\n        self.assertEqual(widget.id_for_label('my_id'), 'my_id_month')\n        \n        # Make sure `id_for_label` appends the select name to the ID\n        widget = SelectDateWidget()\n        self.assertEqual(widget.id_for_label('my_id'), 'my_id_month')\n        \n        # Make sure `id_for_label` returns an empty string for an empty ID\n        widget = SelectDateWidget()\n        self.assertEqual(widget.id_for_label(''), '')\n        \n        # Make sure `id_for_label` returns an empty string if add_id_index is False\n        widget = SelectDateWidget(add_id_index=False)\n        self.assertEqual(widget.id_for_label('my_id'), '')\n        \n        # Make sure `id_for_label` returns an empty string if add_id_index is False and the ID is empty\n        widget = SelectDateWidget(add_id_index=False)\n        self.assertEqual(widget.id_for_label(''), '')\n"], "sample_523": ["def test_legend_bbox_transform():\n    with rc_context(rc={'legend.frameon': False, 'legend.loc': 'upper left'}):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3])\n        ax.plot([4, 5, 6], [4, 5, 6])\n        ax.legend(handles=ax.lines, labels=['line1', 'line2'],\n                  bbox_transform=mtransforms.IdentityTransform(),\n                  bbox_to_anchor=(0.5, 0.5, 0.5, 0.5))\n        legend = ax.get_legend()\n        assert legend.get_bbox_to_anchor().transform == mtransforms.IdentityTransform()\n\n        # test transform is inherited from parent\n        ax2 = ax.twinx()\n        ax2.plot([7, 8, 9], [7, 8, 9])\n        legend2 = ax2.legend(handles=ax2.lines, labels=['line3', 'line4'],\n                             bbox_to_anchor=(0.5, 0.5, 0.5, 0.5))\n        assert legend2.get_bbox_to_anchor().transform == ax2.get_xaxis_transform()\n", "def test_Legend__set_bbox_to_anchor__set_bbox_to_anchor_bbox_properties(bbox_to_anchor):\n    with rc_context({\"legend.loc\": \"upper right\"}):\n        fig, ax = plt.subplots()\n        handles, labels = [], []\n        for i in range(5):\n            handles.append(mlines.Line2D([0, 1], [0, 1]))\n            labels.append(f\"Label {i}\")\n        lgd = mlegend.Legend(ax, handles, labels, bbox_to_anchor=bbox_to_anchor)\n        assert lgd.get_bbox_to_anchor().x0 == bbox_to_anchor[0]\n        assert lgd.get_bbox_to_anchor().y0 == bbox_to_anchor[1]\n        if len(bbox_to_anchor) == 4:\n            assert lgd.get_bbox_to_anchor().width == bbox_to_anchor[2]\n            assert lgd.get_bbox_to_anchor().height == bbox_to_anchor[3]\n", "def test_legend_loc(loc, expected_loc):\n    fig, ax = plt.subplots()\n    legend = mlegend.Legend(ax, [mlines.Line2D([0], [0], color='red', lw=2)],\n                            ['Label 1'])\n    legend.set_bbox_to_anchor((0.5, 0.5))\n    legend._loc = loc\n    assert legend.get_loc() == expected_loc\n\n", "def test_legend_draggable_drag_legend():\n    \"\"\"\n    Test that the legend can be dragged when dragged_legends is True.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n\n        fig, axs = plt.subplots()\n        ax1, ax2 = axs[:2]\n\n        ax1.plot([1, 2, 3])\n        ax1.set_title('Title of ax1')\n        ax2.plot([4, 5, 6])\n        ax2.set_title('Title of ax2')\n\n        legend = ax1.legend(\n            handles=[ax1.lines[0], ax2.lines[0]],\n            labels=['Line 1', 'Line 2'],\n            loc='upper right',\n            frameon=True,\n            fancybox=True,\n            shadow=True,\n            title='Legend title',\n            title_fontproperties=FontProperties(size=16),\n            title_fontsize=16,\n            draggable=True,\n            use_blit=True,\n            update='loc',\n        )\n\n        assert legend.get_draggable()\n\n        # Save the figure\n        fig.savefig('legend_draggable.png')\n", "def test_legend_ncol():\n    fig, ax = plt.subplots()\n    ax.plot(np.random.randn(10))\n    ax2 = ax.twinx()\n    ax2.plot(np.random.randn(10))\n    ax.legend(ncols=3)\n    ax2.legend(ncols=2)\n    ax.set_title('NCols')\n    ax2.set_title('NCols')\n    ax.set_ylabel('Y1')\n    ax2.set_ylabel('Y2')\n\n    with rc_context({'legend.loc': 'best'}):\n        fig.legend(ncols=1, bbox_to_anchor=(0, 0.5, 0.5, 0.5),\n                   title='Figure Legend')\n        plt.savefig('test_legend_ncol.png')\n\n    expected = r\"\"\""], "sample_882": ["def test_MLPRegressor_attributes():\n    \"\"\"Check attributes of MLPRegressor.\"\"\"\n    X_reg, y_reg = make_regression(\n        n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7\n    )\n    regr = MLPRegressor(\n        hidden_layer_sizes=(100,),\n        activation=\"relu\",\n        solver=\"adam\",\n        alpha=0.0001,\n        batch_size=\"auto\",\n        learning_rate=\"constant\",\n        learning_rate_init=0.001,\n        power_t=0.5,\n        max_iter=200,\n        shuffle=True,\n        random_state=7,\n        tol=1e-4,\n        verbose=False,\n        warm_start=False,\n        momentum=0.9,\n        nesterovs_momentum=True,\n        early_stopping=False,\n        validation_fraction=0.1,\n        beta_1=0.9,\n        beta_2=0.999,\n        epsilon=1e-8,\n        n_iter_no_change=10,\n        max_fun=15000,\n    )\n    regr.fit(X_reg, y_reg)\n\n    expected_attributes = [\n        \"loss_\",\n        \"best_loss_\",\n        \"loss_curve_\",\n        \"validation_scores_\",\n        \"best_validation_score_\",\n        \"t_\",\n        \"coefs_\",\n        \"intercepts_\",\n        \"n_features_in_\",\n        \"feature_names_in_\",\n        \"n_iter_\",\n        \"n_layers_\",\n        \"n_outputs_\",\n        \"out_activation_\",\n    ]\n    for attr in expected_attributes:\n        assert hasattr(regr, attr)\n        assert not regr._more_tags()\n", "def test_MLPRegressor_max_iter():\n    # Test that max_iter is checked when solver is 'lbfgs'\n    X, y = make_regression(n_samples=200, n_features=10, bias=20.0, noise=100.0, random_state=7)\n    clf = MLPRegressor(solver='lbfgs', max_iter=1)\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n\n    # Test that max_iter is not checked when solver is not 'lbfgs'\n    clf = MLPRegressor(solver='adam', max_iter=1)\n    assert clf.fit(X, y) is not None\n\n    # Test that max_iter is respected when solver is 'lbfgs'\n    clf = MLPRegressor(solver='lbfgs', max_iter=1)\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        clf.fit(X, y)\n    assert clf.n_iter_ == 1\n\n    # Test that max_fun is checked when solver is 'lbfgs'\n    clf = MLPRegressor(solver='lbfgs', max_fun=1)\n    with pytest.raises(ValueError):\n        clf.fit(X, y)\n\n    # Test that max_fun is not checked when solver is not 'lbfgs'\n    clf = MLPRegressor(solver='adam', max_fun=1)\n    assert clf.fit(X, y) is not None\n\n    # Test that max_fun is respected when solver is 'lbfgs'\n    clf = MLPRegressor(solver='lbfgs', max_fun=1)\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n        clf.fit(X, y)\n    assert clf.n_iter_ == 1\n", "def test_MLPClassifier_stochastic():\n    X, y = load_iris()\n    y = LabelBinarizer().fit_transform(y)\n\n    with ignore_warnings(category=ConvergenceWarning):\n        clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, solver=\"adam\")\n        clf.fit(X, y)\n        assert_array_equal(clf.classes_, np.unique(y))\n\n    with ignore_warnings(category=ConvergenceWarning):\n        clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=500, solver=\"lbfgs\")\n        clf.fit(X, y)\n        assert_array_equal(clf.classes_, np.unique(y))\n", "def test_MLPRegressor_partial_fit_multiple_times():\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    clf = MLPRegressor(max_iter=1000, solver='lbfgs')\n    clf.fit(X[:50], y[:50])\n    clf.partial_fit(X[50:], y[50:])\n    assert clf.n_outputs_ == 1\n    assert clf.coefs_ is not None\n    assert clf.intercepts_ is not None\n    assert clf.loss_ > 0\n", "def test_MLPRegressor_single_feature():\n    X = np.array([[1], [2]])\n    y = np.array([[10], [20]])\n\n    mlp_regressor = MLPRegressor()\n    with pytest.raises(ValueError):\n        mlp_regressor.fit(X, y)\n"], "sample_772": ["def test_class_weight_subsample():\n    \"\"\"Check class weight subsample mode.\"\"\"\n    # create a random forest with class weight subsample mode\n    X, y = datasets.make_classification(n_samples=100, n_features=10, n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False, random_state=0)\n    ForestClassifier = FOREST_CLASSIFIERS[\"RandomForestClassifier\"]\n    clf = ForestClassifier(n_estimators=10, bootstrap=True, random_state=1, class_weight=\"balanced_subsample\")\n    clf.fit(X, y)\n    assert_warns_message(UserWarning, clf.fit, X, y)\n\n    # check if the class weights are computed correctly\n    assert_array_equal(clf._validate_y_class_weight(y)[1], clf.class_weight)\n\n    # check if the class weights are computed correctly for multi-output\n    X, y = datasets.make_classification(n_samples=100, n_features=10, n_informative=3, n_redundant=0,\n                                        n_repeated=0, shuffle=False, random_state=0, n_classes=5)\n    ForestClassifier = FOREST_CLASSIFIERS[\"RandomForestClassifier\"]\n    clf = ForestClassifier(n_estimators=10, bootstrap=True, random_state=1, class_weight=\"balanced_subsample\",\n                          n_outputs=3)\n    clf.fit(X, y)\n    assert_warns_message(UserWarning, clf.fit, X, y)\n\n    # check if the class weights are computed correctly for multi-output\n    assert_array_equal(clf._validate_y_class_weight(y)[1], clf.class_weight)\n", "def test_random_trees_embedding_fit_transform_transform():\n    rng = check_random_state(0)\n    X = rng.rand(10, 10)\n    rte = RandomTreesEmbedding(n_estimators=10, max_depth=5, random_state=0)\n    X_transformed = rte.fit_transform(X)\n    X_transformed_2 = rte.transform(X)\n    assert_array_equal(X_transformed, X_transformed_2)\n", "def test_fit_transform_single_sample():\n    \"\"\"Check fit_transform on a single sample.\"\"\"\n    ForestTransformer = FOREST_TRANSFORMERS[\"RandomTreesEmbedding\"]\n\n    X = np.array([[1, 2, 3]])\n    transformer = ForestTransformer(n_estimators=10)\n    X_transformed = transformer.fit_transform(X)\n\n    assert isinstance(X_transformed, csr_matrix)\n    assert X_transformed.shape == (1, 10 * 2)\n    assert X_transformed.toarray().sum(axis=1).all() == 1\n\n    transformer = ForestTransformer(n_estimators=10, sparse_output=False)\n    X_transformed = transformer.fit_transform(X)\n\n    assert isinstance(X_transformed, np.ndarray)\n    assert X_transformed.shape == (1, 10 * 2)\n    assert X_transformed.sum(axis=1).all() == 1\n\n    # test warm_start\n    transformer = ForestTransformer(n_estimators=10, warm_start=True)\n    X_transformed = transformer.fit_transform(X)\n    transformer.fit_transform(X)\n    assert isinstance(X_transformed, csr_matrix)\n    assert X_transformed.shape == (1, 10 * 2)\n    assert X_transformed.toarray().sum(axis=1).all() == 1\n", "def test_classifiers__predict_proba__dtype(self, name):\n    \"\"\"Check predict_proba returns a float32 array.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n    predict_proba = clf.predict_proba(X)\n    assert predict_proba.dtype == np.float32\n\n    if name != 'RandomTreesEmbedding':\n        # Check that predict_proba is not sparse\n        assert not isinstance(predict_proba, csr_matrix)\n        assert not isinstance(predict_proba, csc_matrix)\n        assert not isinstance(predict_proba, coo_matrix)\n", "def test_forest_fit_transform_and_transform(name):\n    \"\"\"Check fit and transform on the toy dataset.\"\"\"\n    if name in FOREST_CLASSIFIERS:\n        Forest = FOREST_CLASSIFIERS[name]\n    elif name in FOREST_REGRESSORS:\n        Forest = FOREST_REGRESSORS[name]\n    else:\n        Forest = FOREST_TRANSFORMERS[name]\n\n    rng = check_random_state(1)\n    X = rng.rand(10, 10)\n\n    forest = Forest(n_estimators=10, random_state=1)\n    X_transformed = forest.fit_transform(X)\n    assert_equal(X_transformed.shape, (10, forest.n_estimators))\n\n    assert_raises(ValueError, forest.fit_transform, X)\n    assert_raises(ValueError, forest.transform, X)\n\n    # check that fit_transform and transform can be called sequentially\n    X_transformed = forest.fit_transform(X)\n    X_transformed2 = forest.transform(X)\n    assert_array_equal(X_transformed, X_transformed2)\n\n    # check that fit_transform and transform can be called with different\n    # parameter values\n    forest.fit(X, y=None)\n    X_transformed = forest.transform(X)\n    assert_array_equal(X_transformed, X_transformed2)\n\n    # check that fit_transform and transform can be called with different\n    # sample weight\n    forest.fit(X, y=None, sample_weight=[1, 1, 1, 0, 0, 0])\n    X_transformed = forest.transform(X)\n    assert_array_equal(X_transformed, X_transformed2)\n\n    # check that fit_transform and transform can be called with a different\n    # number of estimators\n    forest.fit(X, y=None, sample_weight=[1, 1, 1, 0, 0, 0])\n    X_transformed = forest.transform(X)\n    forest.n_estimators = 5\n    X_transformed = forest.transform(X)\n    assert_equal(X_transformed.shape, (10, 5))\n\n    # check that fit_transform and transform can be called with a different\n    # random_state\n    forest.fit(X, y=None, sample_weight=[1, 1, 1, 0, 0, 0])\n    X_transformed = forest.transform(X)\n    forest.random_state = 2\n    X_transformed = forest.transform(X)\n    assert_array_equal(X_transformed, X_transformed2)\n\n    # check"], "sample_112": ["    def test_prepopulated_fields_js_context(self):\n        request = self.request_factory.get(reverse('admin:admin_view'), data={'action': 'add'}, \n                                        user=self.user)\n        admin_view = site.admin_view(request)\n        context = {'adminform': admin_view}\n        prepopulated_fields_js(context)\n        self.assertIn('prepopulated_fields', context)\n        self.assertIn('prepopulated_fields_json', context)\n        self.assertIsInstance(context['prepopulated_fields'], list)\n        self.assertIsInstance(context['prepopulated_fields_json'], str)\n", "    def test_prepopulated_fields_js(self):\n        # Arrange\n        article = Article.objects.create(title='Test', slug='test')\n        inline_admin_formset = self.request_factory.get(reverse('admin:article_article_changelist'))\n        inline_admin_formset._request = inline_admin_formset.request\n        inline_admin_formset.queryset = [article]\n        inline_admin_formset.formset.forms[0].original = None\n        context = {'adminform': article.admin_form, 'inline_admin_formsets': [inline_admin_formset]}\n\n        # Act\n        context = prepopulated_fields_js(context)\n\n        # Assert\n        self.assertIn('prepopulated_fields', context)\n        self.assertIn('prepopulated_fields_json', context)\n        self.assertIsInstance(context['prepopulated_fields'], list)\n        self.assertIsInstance(context['prepopulated_fields_json'], str)\n", "    def test_prepopulated_fields_js(self):\n        # Test prepopulated_fields_js function\n        article_admin = ArticleAdmin(Article, site)\n        request = self.request_factory.get(reverse('admin:articles_article_changelist'))\n        request.user = self.create_superuser()\n        request.user.is_superuser = True\n        request.user.save()\n        request.session = {}\n        prepopulated_fields = [{'field': 'title', 'dependencies': []}, {'field': 'content', 'dependencies': ['title']}]\n        request.context = {'adminform': {'prepopulated_fields': prepopulated_fields}, 'inline_admin_formsets': []}\n\n        ctx = prepopulated_fields_js(request.context)\n        self.assertEqual(ctx['prepopulated_fields'], prepopulated_fields)\n        self.assertEqual(json.loads(ctx['prepopulated_fields_json']), [\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [],\n                \"dependency_list\": [],\n                \"maxLength\": 50,\n                \"allowUnicode\": True\n            },\n            {\n                \"id\": \"#id_content\",\n                \"name\": \"content\",\n                \"dependency_ids\": [\"#id_title\"],\n                \"dependency_list\": [\"title\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": True\n            }\n        ])\n\n        # Test with inline admin formsets\n        inline_admin_formset = [article_admin.get_formset(request)]\n        request.context['inline_admin_formsets'] = [inline_admin_formset]\n        ctx = prepopulated_fields_js(request.context)\n        self.assertEqual(ctx['prepopulated_fields'], prepopulated_fields)\n        self.assertEqual(json.loads(ctx['prepopulated_fields_json']), [\n            {\n                \"id\": \"#id_title\",\n                \"name\": \"title\",\n                \"dependency_ids\": [],\n                \"dependency_list\": [],\n                \"maxLength\": 50,\n                \"allowUnicode\": True\n            },\n            {\n                \"id\": \"#id_content\",\n                \"name\": \"content\",\n                \"dependency_ids\": [\"#id_title\"],\n                \"dependency_list\": [\"title\"],\n                \"maxLength\": 50,\n                \"allowUnicode\": True\n            }\n        ])\n\n        # Test with no prepopulated fields\n        request.context = {'adminform': {'prepopulated_fields': []}, 'inline_admin_formsets': []}\n        ctx = prepopulated_fields_js(request.context)\n       ", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.create(title='Test Article', content='This is a test article.')\n        self.client.force_login(self.superuser)\n        request = self.request_factory.get(reverse('admin:article_article_change', args=(article.id,)))\n        response = self.client.get(reverse('admin:article_article_change', args=(article.id,)), {'_changelist_filters': 'date_hierarchy'})\n        context = response.context\n        context['prepopulated_fields'] = [{'field': article.title, 'dependencies': []}]\n        prepopulated_fields_js = prepopulated_fields_js(context)\n        self.assertIn('prepopulated_fields_json', prepopulated_fields_js)\n        self.assertIn('prepopulated_fields', prepopulated_fields_js)\n", "    def test_prepopulated_fields_js(self):\n        request = self.request_factory.get(reverse('admin:admin_view'))\n        article = Article.objects.create(title='test', slug='test')\n        response = self.client.get(reverse('admin:article_article_changelist'))\n        admin_view = site.admin_view(request, {'article': article})\n        self.assertTrue(admin_view.context_data['prepopulated_fields'])\n\n        # Test if prepopulated_fields_json is correctly formatted\n        prepopulated_fields_json = admin_view.context_data['prepopulated_fields_json']\n        self.assertIsInstance(prepopulated_fields_json, str)\n        self.assertIsNotNone(json.loads(prepopulated_fields_json))\n\n        # Test if cell_count filter works correctly\n        inline_admin_form = admin_view.context_data['inline_admin_form']\n        self.assertIsInstance(inline_admin_form, list)\n        self.assertEqual(self.client.get(reverse('admin:article_article_changelist')).context.get('cell_count'), inline_admin_form.count() + 1)\n\n        # Test if submit_row returns the correct context\n        context = submit_row(admin_view.context_data)\n        self.assertIn('show_delete_link', context)\n        self.assertIn('show_save_as_new', context)\n        self.assertIn('show_save_and_add_another', context)\n        self.assertIn('show_save_and_continue', context)\n        self.assertIn('show_save', context)\n        self.assertIn('show_close', context)\n\n        # Test if change_form_object_tools_tag works correctly\n        context = change_form_object_tools_tag(admin_view.context_data)\n        self.assertEqual(context, admin_view.context_data)\n"], "sample_807": ["def test_calibration_curve_default_bins():\n    X, y = make_classification(n_samples=1000, n_features=10,\n                               n_informative=5, n_redundant=0,\n                               n_repeated=0, n_classes=2, random_state=0)\n    clf = LinearSVC(random_state=0)\n    clf.fit(X, y)\n    y_pred = clf.predict_proba(X)[:, 1]\n    prob_true, prob_pred = calibration_curve(y, y_pred, normalize=True)\n    assert_equal(prob_true.shape[0], 5)\n", "def test_SigmoidCalibration_fit_sample_weights():\n    X = np.array([1., 2., 3.])\n    y = np.array([0, 1, 0])\n    sample_weight = np.array([1., 2., 3.])\n    calibrator = _SigmoidCalibration()\n    calibrator.fit(X, y, sample_weight=sample_weight)\n    assert_almost_equal(calibrator.a_, _sigmoid_calibration(X, y, sample_weight=sample_weight)[0])\n    assert_almost_equal(calibrator.b_, _sigmoid_calibration(X, y, sample_weight=sample_weight)[1])\n", "def test_isotonic_calibration():\n    \"\"\"Test isotonic calibration with different number of samples.\"\"\"\n    X, y = make_classification(n_samples=100, n_features=20, n_informative=10,\n                               n_redundant=0, n_repeated=0, n_classes=2,\n                               n_clusters_per_class=1, weights=None,\n                               flip_y=0.01, random_state=42)\n    clf = LinearSVC(random_state=42)\n    clf.fit(X, y)\n    calibrator = CalibratedClassifierCV(base_estimator=clf, cv=5, method='isotonic')\n    calibrator.fit(X, y)\n    pred_prob = calibrator.predict_proba(X)\n    prob_true, prob_pred = calibration_curve(y, pred_prob[:, 1])\n    assert_array_almost_equal(prob_true, prob_pred)\n", "def test_calibrated_classifier_cv_isotonic():\n    X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n                              n_redundant=0, n_repeated=0, n_classes=2,\n                              n_clusters_per_class=1, weights=None,\n                              random_state=0)\n    y = y.astype(int)\n\n    classifier = CalibratedClassifierCV(base_estimator=LinearSVC(random_state=0),\n                                      cv='prefit', method='isotonic')\n    classifier.base_estimator.fit(X, y)\n    classifier.fit(X, y)\n    proba = classifier.predict_proba(X)\n    assert_array_almost_equal(proba.sum(axis=1), 1)\n\n    clf = RandomForestClassifier(n_estimators=10, random_state=0)\n    classifier = CalibratedClassifierCV(base_estimator=clf, cv='prefit', method='isotonic')\n    classifier.fit(X, y)\n    proba = classifier.predict_proba(X)\n    assert_array_almost_equal(proba.sum(axis=1), 1)\n", "def test_calibration_is_monotonic():\n    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n                              n_redundant=3, n_repeated=0, n_classes=3,\n                              n_clusters_per_class=1, random_state=42)\n    clf = MultinomialNB()\n    clf.fit(X, y)\n    calibrated = CalibratedClassifierCV(clf, cv=5, method='sigmoid')\n    calibrated.fit(X, y)\n    proba = calibrated.predict_proba(X)\n    proba_uni = clf.predict_proba(X)\n    # check that each row is monotonic\n    for i in range(proba.shape[0]):\n        assert_array_almost_equal(proba[i, :], np.sort(proba[i, :]), 1e-3)\n        assert_array_almost_equal(proba[i, :], np.sort(proba_uni[i, :]), 1e-3)\n"], "sample_941": ["def test_restify_special_form():\n    \"\"\"Test restify() with SpecialForm.\"\"\"\n    # Test case for Type\n    assert restify(Type) == ':obj:`typing.Type`'\n\n    # Test case for Union\n    union = Union[int, str]\n    assert restify(union) == ':obj:`Union[int, str]`'\n\n    # Test case for Union with NoneType\n    union = Union[int, None]\n    assert restify(union) == ':obj:`Union[int, None]`'\n\n    # Test case for Union with more than two types\n    union = Union[int, str, float]\n    assert restify(union) == ':obj:`Union[int, str, float]`'\n\n    # Test case for Union with NoneType as the second type\n    union = Union[int, None, str]\n    assert restify(union) == ':obj:`Union[int, Optional[str]]`'\n\n    # Test case for Union with more than two types and NoneType as the second type\n    union = Union[int, None, str, float]\n    assert restify(union) == ':obj:`Union[int, Optional[str]]`'\n\n    # Test case for Union with NoneType as the third type\n    union = Union[int, str, None]\n    assert restify(union) == ':obj:`Union[int, str, Optional[str]]`'\n\n    # Test case for Union with more than three types and NoneType as the second type\n    union = Union[int, None, str, float]\n    assert restify(union) == ':obj:`Union[int, Optional[str], float]`'\n\n    # Test case for Union with more than three types and NoneType as the third type\n    union = Union[int, str, None, float]\n    assert restify(union) == ':obj:`Union[int, str, Optional[float]]`'\n\n    # Test case for Union with NoneType as the last type\n    union = Union[int, str, None]\n    assert restify(union) == ':obj:`Union[int, str, Optional[str]]`'\n\n    # Test case for Union with more than three types and NoneType as the last type\n    union = Union[int, str, None, float]\n    assert restify(union) == ':obj:`Union[int, str, Optional[float]]`'\n\n    # Test case for Union with multiple NoneTypes\n    union = Union[int, str, None,", "def test_restify_with_NewType():\n    # Test with NewType\n    assert restify(MyInt) == ':obj:`MyInt`'\n\n    # Test with NewType and inheritance\n    assert restify(MyInt) == ':obj:`MyInt`'\n\n    # Test with NewType and inheritance with Custom class\n    class MyCustomClass:\n        pass\n\n    MyCustomInt = NewType('MyCustomInt', MyCustomClass)\n    assert restify(MyCustomInt) == ':obj:`MyCustomInt`'\n\n    # Test with NewType and inheritance with Custom class and __qualname__\n    class MyCustomClass2:\n        __qualname__ = 'MyCustomClass2'\n\n    MyCustomInt2 = NewType('MyCustomInt2', MyCustomClass2)\n    assert restify(MyCustomInt2) == ':obj:`MyCustomInt2`'\n\n    # Test with NewType and inheritance with Custom class and __module__\n    class MyCustomClass3:\n        __module__ = 'test_module'\n\n    MyCustomInt3 = NewType('MyCustomInt3', MyCustomClass3)\n    assert restify(MyCustomInt3) == ':obj:`test_module.MyCustomInt3`'\n\n    # Test with NewType and inheritance with Custom class and __module__ and __qualname__\n    class MyCustomClass4:\n        __module__ = 'test_module'\n        __qualname__ = 'MyCustomClass4'\n\n    MyCustomInt4 = NewType('MyCustomInt4', MyCustomClass4)\n    assert restify(MyCustomInt4) == ':obj:`test_module.MyCustomInt4`'\n\n    # Test with NewType and TypeVar\n    MyIntWithT = NewType('MyIntWithT', T)\n    assert restify(MyIntWithT) == ':obj:`MyIntWithT`'\n", "def test_restify_new_type():\n    # Test that NewType is correctly handled\n    assert restify(MyInt) == ':obj:`MyInt`'\n\n    # Test that NewType with a qualified name is correctly handled\n    assert restify(MyInt.__module__ + '.' + MyInt.__name__) == ':obj:`%s.%s`' % (MyInt.__module__, MyInt.__name__)\n\n    # Test that a NewType with a NoneType in its args is correctly handled\n    T = TypeVar('T')\n    MyOptional = NewType('MyOptional', T)\n    assert restify(MyOptional) == ':obj:`MyOptional`'\n\n    # Test that a NewType with a non-TypeVar in its args is correctly handled\n    MyList = NewType('MyList', List[T])\n    assert restify(MyList) == ':obj:`MyList`'\n\n    # Test that a NewType with a TypeVar and a non-TypeVar in its args is correctly handled\n    MyDict = NewType('MyDict', Dict[T, int])\n    assert restify(MyDict) == ':obj:`MyDict`'\n\n    # Test that a NewType with a NewType in its args is correctly handled\n    MyNested = NewType('MyNested', MyInt)\n    assert restify(MyNested) == ':obj:`MyNested`'\n\n    # Test that a NewType with a Union of NewType and a non-TypeVar in its args is correctly handled\n    MyUnion = NewType('MyUnion', Union[MyInt, int])\n    assert restify(MyUnion) == ':obj:`MyUnion`'\n\n    # Test that a NewType with a GenericMeta in its args is correctly handled\n    class MyGeneric(GenericMeta):\n        __args__ = (T,)\n    MyGenericType = NewType('MyGenericType', MyGeneric)\n    assert restify(MyGenericType) == ':obj:`MyGenericType`'\n\n    # Test that a NewType with a NewType that has a __qualname__ in its args is correctly handled\n    class MyGenericNewType(GenericMeta):\n        __args__ = (MyInt,)\n        __qualname__ = 'MyGenericNewType'\n    MyGenericNewType2 = NewType('MyGenericNewType2', MyGenericNewType)\n    assert restify(MyGenericNewType2) == ':obj:`MyGenericNewType2`'\n", "def test_restify_generics_py37():\n    from sphinx.util.typing import restify, restify_py37\n\n    # Test with generic class\n    class MyGen:\n        __origin__ = Generator\n        __args__ = (int, str)\n\n    result = restify_py37(MyGen)\n    assert result == ':class:`MyGen`[[int], str]'\n\n    # Test with generic instance\n    my_gen = MyGen()\n    result = restify_py37(my_gen)\n    assert result == ':class:`MyGen`[[int], str]'\n\n    # Test with generic instance with non-string __args__\n    class MyTupleMeta:\n        __origin__ = Tuple\n        __args__ = (int, str)\n\n    my_tuple = MyTupleMeta()\n    result = restify_py37(my_tuple)\n    assert result == ':class:`MyTupleMeta`[(int, str)]'\n\n    # Test with generic instance with Ellipsis\n    class MyTupleMeta2:\n        __origin__ = Tuple\n        __args__ = (int, ..., str)\n\n    my_tuple2 = MyTupleMeta2()\n    result = restify_py37(my_tuple2)\n    assert result == ':class:`MyTupleMeta2`[(int, ..., str)]'\n\n    # Test with generic instance with __args__ of type list\n    class MyListMeta:\n        __origin__ = List\n        __args__ = [int]\n\n    my_list = MyListMeta()\n    result = restify_py37(my_list)\n    assert result == ':class:`MyListMeta`[int]'\n\n    # Test with generic instance with __args__ of type tuple\n    class MyTupleMeta3:\n        __origin__ = Tuple\n        __args__ = (int, str, float)\n\n    my_tuple3 = MyTupleMeta3()\n    result = restify_py37(my_tuple3)\n    assert result == ':class:`MyTupleMeta3`[(int, str, float)]'\n\n    # Test with generic instance with __args__ containing None\n    class MyTupleMeta4:\n        __origin__ = Tuple\n        __args__ = (int, None, str)\n\n    my_tuple4 = MyTupleMeta4()\n    result = restify_py37(my_tuple4)\n    assert result == ':class:`MyTupleMeta4`[(int, None, str)]'\n\n    # Test with generic instance with __args__ containing another generic type\n", "def test_restify_callable():\n    class MyCallable:\n            pass\n\n        pass\n\n    assert restify(Callable[[Any, int], int]) == ':class:`MyCallable`'\n    assert restify(Callable[[Any, int], None]) == ':class:`MyCallable`'\n    assert restify(my_function) == ':class:`my_function`'\n"], "sample_822": ["def test_euclidean_distances_2D():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    distances = euclidean_distances(X, Y)\n    expected_distances = np.array([[ 5.19615242,  5.19615242],\n                                  [ 5.19615242,  5.19615242]])\n    assert_array_almost_equal(distances, expected_distances)\n", "def test_paired_distances():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n\n    # Test default metric\n    distances = paired_distances(X, Y)\n    expected = np.array([2.8284271247461903, 5.196152422706632])\n    assert_array_almost_equal(distances, expected)\n\n    # Test custom metric\n        return np.sqrt(np.sum((x - y) ** 2))\n    distances = paired_distances(X, Y, metric=custom_metric)\n    expected = np.array([2.8284271247461903, 5.196152422706632])\n    assert_array_almost_equal(distances, expected)\n\n    # Test invalid metric\n    with assert_raises(ValueError):\n        paired_distances(X, Y, metric=\"invalid_metric\")\n\n    # Test metric with keyword arguments\n    distances = paired_distances(X, Y, metric=\"euclidean\", p=3)\n    expected = np.array([3.7416573867739413, 6.324555320336759])\n    assert_array_almost_equal(distances, expected)\n\n    # Test invalid keyword arguments\n    with assert_raises(ValueError):\n        paired_distances(X, Y, metric=\"euclidean\", invalid_kwarg=True)\n", "def test_manhattan_distances():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[1, 2], [2, 3]])\n    result = manhattan_distances(X, Y)\n    expected_result = np.array([[0, 1], [1, 0]])\n    assert_array_equal(result, expected_result)\n\n", "def test_pairwise_distances_chunked():\n    # Test a simple case\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    distances = np.array([[0, 1, 2], [1, 0, 1], [2, 1, 0]])\n    with ignore_warnings():\n        gen = pairwise_distances_chunked(X, reduce_func=lambda x: None)\n        for i, dist in enumerate(gen):\n            assert_array_equal(dist, distances[i])\n\n    # Test a case with n_jobs > 1\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with config_context():\n        with ignore_warnings():\n            gen = pairwise_distances_chunked(X, n_jobs=2, reduce_func=lambda x: None)\n            for i, dist in enumerate(gen):\n                assert_array_equal(dist, distances[i])\n\n    # Test with invalid reduce_func\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    with pytest.raises(TypeError):\n        pairwise_distances_chunked(X, reduce_func='invalid')\n\n    # Test with invalid reduce_func output\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n        return ['invalid']\n    with pytest.raises(TypeError):\n        pairwise_distances_chunked(X, reduce_func=reduce_func)\n\n    # Test with invalid reduce_func output length\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n        return [1]\n    with pytest.raises(ValueError):\n        pairwise_distances_chunked(X, reduce_func=reduce_func)\n\n    # Test with invalid reduce_func output length\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n        return [1, 2, 3]\n    with pytest.raises(ValueError):\n        pairwise_distances_chunked(X, reduce_func=reduce_func)\n\n    # Test with valid reduce_func\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n        return [np.min(dist[start])]\n    gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n    for i, dist in enumerate(gen):\n        assert", "def test_euclidean_distances_chunked():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    D = list(pairwise_distances_chunked(X, Y))\n    D = np.array(D)\n    assert_equal(D.shape, (2, 2))\n    assert_array_almost_equal(D, np.array([[5.196152422706632, 5.477225575051661], [7.483314773386172, 8.485281237811724]])\n"], "sample_1033": ["def test_as_content_primitive_radical():\n    from sympy import sqrt, Rational, sqrt as sympy_sqrt\n\n    assert same_and_same_prec(sympy_sqrt(2).as_content_primitive(radical=True)[0], Rational(1))\n    assert same_and_same_prec(sympy_sqrt(2).as_content_primitive(radical=True)[1], sympy_sqrt(2))\n\n    assert same_and_same_prec(sympy_sqrt(2 + 2*sympy_sqrt(3)).as_content_primitive(radical=True)[0], Rational(1))\n    assert same_and_same_prec(sympy_sqrt(2 + 2*sympy_sqrt(3)).as_content_primitive(radical=True)[1], sympy_sqrt(2 + 2*sympy_sqrt(3)))\n\n    assert same_and_same_prec(sympy_sqrt(2 + 3*sympy_sqrt(2)).as_content_primitive(radical=True)[0], Rational(1))\n    assert same_and_same_prec(sympy_sqrt(2 + 3*sympy_sqrt(2)).as_content_primitive(radical=True)[1], sympy_sqrt(2 + 3*sympy_sqrt(2)))\n\n    assert same_and_same_prec(sympy_sqrt(8).as_content_primitive(radical=True)[0], Rational(2))\n    assert same_and_same_prec(sympy_sqrt(8).as_content_primitive(radical=True)[1], sympy_sqrt(2))\n\n    assert same_and_same_prec(sympy_sqrt(2*2*sympy_sqrt(2)).as_content_primitive(radical=True)[0], Rational(2))\n    assert same_and_same_prec(sympy_sqrt(2*2*sympy_sqrt(2)).as_content_primitive(radical=True)[1], sympy_sqrt(2))\n\n    assert same_and_same_prec(sympy_sqrt(4).as_content_primitive(radical=True)[0], Rational(2))\n    assert same_and_same_prec(sympy_sqrt(4).as_content_primitive(radical=True)[1], sympy_sqrt(2))\n", "def test_as_content_primitive():\n    a, b, c = symbols('a,b,c')\n    assert a.as_content_primitive() == (1, a)\n    assert (2*a + 3*b).as_content_primitive() == (1, 2*a + 3*b)\n    assert (2*a + 3*b).as_content_primitive(clear=False) == (1, 2*a + 3*b)\n    assert (2*a + 2*b).as_content_primitive() == (2, a + b)\n    assert (2*a + 2*b).as_content_primitive(clear=False) == (2, a + b)\n    assert (2*a + 4*b).as_content_primitive() == (2, a + 2*b)\n    assert (2*a + 4*b).as_content_primitive(clear=False) == (2, a + 2*b)\n    assert (2*a + 3*b).as_content_primitive(radical=True) == (1, 2*a + 3*b)\n    assert (2*a + 2*b).as_content_primitive(radical=True) == (2, a + b)\n    assert (2*a + 4*b).as_content_primitive(radical=True) == (2, a + 2*b)\n\n    assert (2*a**3 + 2*b**3).as_content_primitive() == (2, a**3 + b**3)\n    assert (2*a**3 + 4*b**3).as_content_primitive() == (2, a**3 + 2*b**3)\n    assert (2*a**3 + 3*b**3).as_content_primitive() == (1, 2*a**3 + 3*b**3)\n\n    assert (2*a + 2*b + 2*c).as_content_primitive() == (2, a + b + c)\n    assert (2*a + 4*b + 4*c).as_content_primitive() == (2, a + 2*b + 2*c)\n    assert (2*a + 3*b + 4*c).as_content_primitive() == (1, 2*a + 3*b + 4*c)\n\n    assert (2*a + 2*b).as_content_primitive(clear=False, radical=True) == (2, a + b)\n    assert (2*a + 4*b).as_content_primitive(clear=False, radical=True) == (2, a", "def test_Add_primitive():\n    assert Add(Rational(2, 3)*x + Rational(4, 9)*x).primitive() == (Rational(2, 9), Rational(12, 9)*x)\n    assert Add(3*x + 4*y).primitive() == (1, 3*x + 4*y)\n    assert Add(3.0*x + 4.0*y).primitive() == (1, 3.0*x + 4.0*y)\n    assert Add(3 + 4.2).primitive() == (3, 4.2)\n    assert Add(3 + 3*sqrt(2)).primitive() == (3, 3*sqrt(2))\n    assert Add(2*sqrt(2) + 4*sqrt(10)).primitive(radical=True) == (2, sqrt(2)*(1 + 2*sqrt(5)))\n    assert Add(3 + 3*sqrt(2) + 4*sqrt(10)).primitive() == (3, 3*sqrt(2) + 4*sqrt(10))\n    assert Add(3 + 3*sqrt(2) + 4*sqrt(2)).primitive() == (3 + 4*sqrt(2), 1)\n    assert Add(3 + 3*sqrt(2) + 4*sqrt(5)).primitive() == (3, 3*sqrt(2) + 4*sqrt(5))\n", "    def test_as_coefficients_dict__single_term(self):\n        from sympy.abc import a, x\n        assert Add(a*x).as_coefficients_dict() == {x: a}\n        assert Add(a).as_coefficients_dict() == {1: a}\n", "def test_Add__as_content_primitive():\n    a, b = symbols('a b')\n    assert same_and_same_prec((a + 3*b).as_content_primitive()[0], 1)\n    assert same_and_same_prec((3 + b).as_content_primitive()[0], 3)\n    assert same_and_same_prec((a + b + 1).as_content_primitive()[0], 1)\n    assert same_and_same_prec((a + b + 1).as_content_primitive(radical=True)[0], 1)\n    assert same_and_same_prec((a + 3*b + 3).as_content_primitive()[0], 1)\n    assert same_and_same_prec((3 + 3*b).as_content_primitive()[0], 3)\n    assert same_and_same_prec((3 + b + 1).as_content_primitive()[0], 3)\n    assert same_and_same_prec((a + b + 3).as_content_primitive()[0], 1)\n    assert same_and_same_prec((3 + a + b).as_content_primitive()[0], 1)\n    assert same_and_same_prec((3 + b + a).as_content_primitive()[0], 1)\n    assert same_and_same_prec((a + 1 + b + 3).as_content_primitive()[0], 1)\n    assert same_and_same_prec((1 + 3 + a + b).as_content_primitive()[0], 1)\n"], "sample_347": ["def test_make_naive_is_naive(self):\n    \"\"\"Tests that make_naive() returns a naive datetime\"\"\"\n    aware_dt = datetime(2022, 1, 1, tzinfo=pytz.utc)\n    naive_dt = timezone.make_naive(aware_dt)\n    self.assertTrue(timezone.is_naive(naive_dt))\n", "def test_make_naive_on_already_naive_datetime(self):\n    dt_naive = datetime.datetime(2022, 1, 1, tzinfo=None)\n    dt_aware = datetime.datetime(2022, 1, 1, tzinfo=pytz.timezone(\"US/Eastern\"))\n\n    with self.assertRaises(ValueError):\n        timezone.make_naive(dt_naive, timezone=\"US/Eastern\")\n\n    dt_naive_after = timezone.make_naive(dt_aware)\n    self.assertIsNone(dt_naive_after.tzinfo)\n", "    def test_is_naive_datetime_object(self):\n        dt = datetime.datetime.now()\n        self.assertTrue(timezone.is_naive(dt))\n", "    def test_override_timezone_context_manager(self):\n        # Test that the timezone is changed correctly when entering the context\n        # and restored when exiting the context\n        with timezone.override('UTC'):\n            self.assertEqual(timezone.get_current_timezone(), pytz.utc)\n        self.assertEqual(timezone.get_current_timezone(), timezone.get_default_timezone())\n", "    def test_make_naive_datetime_strictly(self):\n        \"\"\"\n        Test that make_naive raises a ValueError when passed a naive datetime.\n        \"\"\"\n        dt = datetime.datetime.now(tz=CET)\n        with self.assertRaises(ValueError):\n            timezone.make_naive(dt)\n"], "sample_109": ["    def test_autocomplete_select_multiple_media(self):\n        form = NotRequiredBandForm()\n        widget = form['band'].widget\n        media = widget.media\n        self.assertIn('admin/js/vendor/jquery/jquery.min.js', media._js)\n        self.assertIn('admin/js/vendor/select2/select2.min.js', media._js)\n        self.assertIn('admin/js/jquery.init.js', media._js)\n        self.assertIn('admin/js/autocomplete.js', media._js)\n        self.assertIn('admin/css/vendor/select2/select2.min.css', media._css)\n        self.assertIn('admin/css/autocomplete.css', media._css)\n", "    def test_autocomplete_widget_template_context(self):\n        with self.settings(ROOT_URLCONF='admin_widgets.urls'):\n            form = AlbumForm()\n            band_field = form.fields['band']\n            featuring_field = form.fields['featuring']\n            band_context = band_field.widget.get_context('band', None, {'class': 'test-class'})['widget']\n            featuring_context = featuring_field.widget.get_context('featuring', None, {'class': 'test-class'})['widget']\n            self.assertIn('data-ajax--cache', band_context['attrs'])\n            self.assertIn('data-ajax--cache', featuring_context['attrs'])\n            self.assertIn('data-ajax--delay', band_context['attrs'])\n            self.assertIn('data-ajax--delay', featuring_context['attrs'])\n            self.assertIn('data-ajax--type', band_context['attrs'])\n            self.assertIn('data-ajax--type', featuring_context['attrs'])\n            self.assertIn('data-ajax--url', band_context['attrs'])\n            self.assertIn('data-ajax--url', featuring_context['attrs'])\n            self.assertIn('data-theme', band_context['attrs'])\n            self.assertIn('data-theme', featuring_context['attrs'])\n            self.assertIn('data-allow-clear', band_context['attrs'])\n            self.assertIn('data-allow-clear', featuring_context['attrs'])\n            self.assertIn('data-placeholder', band_context['attrs'])\n            self.assertIn('data-placeholder', featuring_context['attrs'])\n            self.assertIn('class', band_context['attrs'])\n            self.assertIn('class', featuring_context['attrs'])\n", "    def test_rendering_with_required_field(self):\n        form = RequiredBandForm({'band': '1'})\n        widget = form['band'].field.widget\n        rendered_widget = widget.render('band', '1', {'class': 'my-class'})\n        self.assertIn('class=\"my-class select2-container--open\"', rendered_widget)\n        self.assertIn('data-placeholder=\"Select a Band\"', rendered_widget)\n        self.assertIn('data-theme=\"admin-autocomplete\"', rendered_widget)\n", "    def test_rendering_with_required_option(self):\n        form = AlbumForm(instance=Album())\n        widget = form.fields['band'].widget\n        output = widget.get_context('band', None, {'class': 'my-class'}).get('widget', {})\n        self.assertEqual(output['attrs'], {'class': 'admin-autocomplete my-class'})\n", "    def setUp(self):\n        self.admin_site = admin.AdminSite(name='my-site')\n        self.autocomplete_widget = AutocompleteSelect(Album._meta.get_field('band').remote_field, self.admin_site)\n"], "sample_883": ["def test_BayesianRidge_with_zero_variance_y():\n    \"\"\"Test that BayesianRidge raises an error when the variance of y is zero\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    y = np.zeros(X.shape[0])\n    model = BayesianRidge()\n    with pytest.raises(ValueError):\n        model.fit(X, y)\n\n", "def test_bayesian_ridge_independence_from_X_offset():\n    \"\"\"Check that the BayesianRidge is independent from X_offset_\"\"\"\n    X = np.array([[0, 1], [1, 1], [2, 2]])\n    y = np.array([0, 1, 2])\n    X_offset_ = np.array([1.0, 1.0])\n    X_offset = np.array([-1.0, -1.0])\n    y_offset_ = 0\n    y_offset = 1\n\n    model = BayesianRidge()\n    model.fit(X - X_offset_, y + y_offset_)\n    model.fit(X - X_offset, y + y_offset)\n\n    assert model.coef_ == model.coef_\n    assert model.alpha_ == model.alpha_\n    assert model.lambda_ == model.lambda_\n    assert model.sigma_.shape == model.sigma_.shape\n", "def test_BayesianRidge_fit_intercept():\n    \"\"\"Check that BayesianRidge fit_intercept=True with y centered data.\"\"\"\n    X = np.array([[0, 0], [1, 1], [2, 2]])\n    y = np.array([0, 1, 2])\n    X_centered = X - X.mean(axis=0)\n    y_centered = y - y.mean()\n\n    # Create a BayesianRidge instance with fit_intercept=True\n    br = BayesianRidge(fit_intercept=True)\n\n    # Fit the model to centered data\n    br.fit(X_centered, y_centered)\n\n    # Fit the model to original data with fit_intercept=True\n    br.fit(X, y)\n\n    # Check that coefficients are the same\n    assert_array_almost_equal(br.coef_, br.coef_, decimal=5)\n", "def test_regression_parameters():\n    # Test the regression parameters\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n\n    # BayesianRidge\n    clf = BayesianRidge()\n    clf.fit(X, y)\n    assert clf.alpha_ > 0\n    assert clf.lambda_ > 0\n    assert_array_almost_equal(clf.coef_, np.dot(X.T, y) / np.dot(X.T, X))\n\n    # ARDRegression\n    clf = ARDRegression()\n    clf.fit(X, y)\n    assert clf.alpha_ > 0\n    assert np.all(clf.lambda_ > 0)\n    assert_array_almost_equal(\n        clf.coef_, np.dot(clf.sigma_, np.dot(X.T, y)) / np.dot(X.T, np.dot(clf.sigma_, X))\n    )\n\n    # Check for convergence\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    clf = ARDRegression(max_iter=1)\n    clf.fit(X, y)\n    assert clf.n_iter_ == 1\n    clf = ARDRegression(tol=1e-10)\n    clf.fit(X, y)\n    assert clf.n_iter_ > 1\n\n    # Check for valid max_iter\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    with pytest.raises(ValueError):\n        ARDRegression(max_iter=-1).fit(X, y)\n    with pytest.raises(ValueError):\n        BayesianRidge(max_iter=-1).fit(X, y)\n\n    # Check for valid tol\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([7, 8, 9])\n    with pytest.raises(ValueError):\n        ARDRegression(tol=-1).fit(X, y)\n    with pytest.raises(ValueError):\n        BayesianRidge(tol=-1).fit(X, y)\n\n    # Check for valid alpha_1 and alpha_2\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([", "def test_bayesian_ridge_fit_intercept():\n    # Regression problem\n    X, y = np.array([[1, 2], [3, 4], [5, 6]]), np.array([2, 4, 5])\n    # No intercept\n    br = BayesianRidge(fit_intercept=False)\n    br.fit(X, y)\n    assert_array_almost_equal(br.intercept_, 0)\n    assert_array_almost_equal(br.coef_, [0, 0])\n\n    # With intercept\n    br = BayesianRidge()\n    br.fit(X, y)\n    assert_array_almost_equal(br.intercept_, 0.9796811664411017)\n    assert_array_almost_equal(br.coef_, [0.7521205607323414, 0.9436092018847628])\n"], "sample_992": ["def test_SympyPrinter():\n    # Test printing of a Function with module name not provided\n    expr = sympify(\"sin\")\n    printer = SymPyPrinter()\n    assert printer.doprint(expr) == 'sin()'\n\n    # Test printing of a Function with module name provided\n    expr = sympify(\"numpy.sin\")\n    printer = SymPyPrinter()\n    assert printer.doprint(expr) == 'numpy.sin()'\n\n    # Test printing of Piecewise expression\n    from sympy.functions.elementary.piecewise import Piecewise\n    expr = Piecewise((1, x > 0), (0, x <= 0))\n    printer = NumPyPrinter()\n    assert printer.doprint(expr) == 'numpy.select([x > 0, x <= 0], [1, 0], default=numpy.nan)'\n\n    # Test printing of SparseMatrix\n    from sympy.matrices import SparseMatrix\n    expr = SparseMatrix([[1, 2], [3, 4]])\n    printer = SciPyPrinter()\n    assert printer.doprint(expr) == 'scipy.sparse.coo_matrix([1, 2, 3, 4], ((0, 0), (1, 0), (0, 1), (1, 1)), shape=(2, 2))'\n\n    # Test printing of ImaginaryUnit\n    expr = sympify('I')\n    printer = SymPyPrinter()\n    assert printer.doprint(expr) == 'sympy.I'\n\n    # Test printing of uppergamma function\n    from sympy.functions.special.gamma_functions import uppergamma\n    expr = uppergamma(2, 3)\n    printer = MpmathPrinter()\n    assert printer.doprint(expr) == 'mpmath.gammainc(2, 3, mpmath.inf)'\n", "def test_SciPyPrinter():\n    from sympy import gamma, E, exp\n    from sympy.printing.pycode import SciPyPrinter\n\n    printer = SciPyPrinter()\n    assert printer.doprint(gamma(x)) == \"scipy.special.gamma(x)\"\n    assert printer.doprint(E) == \"scipy.constants.golden_ratio\"\n    assert printer.doprint(exp(x)) == \"sympy.exp(x)\"\n", "def test_mpmath_log1p():\n    x = symbols('x')\n    assert pycode(x.log1p(), printer=MpmathPrinter()) == \"mpmath.log(x+1)\"\n", "def test_PythonCodePrinter_Python2_keywords():\n    x = symbols('x')\n    printer = PythonCodePrinter(settings={'standard': 'python2'})\n    assert printer.doprint(x) == 'x'\n    assert printer.doprint(x+2) == 'x+2'\n    assert printer.doprint(x**2) == 'x**2'\n    assert printer.doprint(x-2) == 'x-2'\n    assert printer.doprint(x*2) == 'x*2'\n    assert printer.doprint(x/2) == 'x/2'\n    assert printer.doprint(x-3) == 'x-3'\n    assert printer.doprint(x+3) == 'x+3'\n    assert printer.doprint(x**-3) == '1/x**3'\n    assert printer.doprint(acos(x)) == 'acos(x)'\n    assert printer.doprint(acos(x).doit()) == 'acos(x)'\n    assert printer.doprint(acos(x).simplify()) == 'acos(x)'\n    assert printer.doprint(x.as_real_imag()) == '(x,0)'\n    assert printer.doprint(x.as_real_imag().args) == '(x, 0)'\n    assert printer.doprint(x.as_real_imag().args[0]) == 'x'\n    assert printer.doprint(x.as_real_imag().args[1]) == '0'\n", "def test_SciPyPrinter():\n    from sympy import sqrt, cos\n    from sympy.codegen import Assignment\n    from sympy.printing.pycode import SciPyPrinter\n    sp = SciPyPrinter()\n\n    expr = sqrt(5)\n    assert sp.doprint(expr) == 'sqrt(5)'\n\n    expr = cos(5)\n    assert sp.doprint(expr) == 'cos(5)'\n\n    expr = Mod(5, 3)\n    assert sp.doprint(expr) == '5 % 3'\n\n    # Test SciPy's special functions\n    expr = SciPyPrinter._print_erf(2)\n    assert sp.doprint(expr) == 'erf(2)'\n\n    # Test SciPy's constants\n    expr = SciPyPrinter._print_golden_ratio()\n    assert sp.doprint(expr) == 'golden_ratio'\n\n    # Test Piecewise function\n    p = And(x>0, x<5)\n    expr = cos(p)\n    assert sp.doprint(expr) == 'cos(x > 0 and x < 5)'\n\n    # Test Matrix printing\n    A = SparseMatrix([[1, 2], [3, 4]])\n    expr = A**2\n    assert sp.doprint(expr) == \"coo_matrix([[(1, 2), (3, 4)], [(1, 2), (3, 4)]], shape=(2, 2), ({0}, {1}), ({0}, {1}))\".format('0', '1')\n\n    # Test _print_MatrixBase\n    A = SparseMatrix([[1, 2], [3, 4]])\n    expr = A\n    assert sp.doprint(expr) == 'coo_matrix([[(1, 2), (3, 4)], [(1, 2), (3, 4)]], shape=(2, 2), ({0}, {1}), ({0}, {1}))'.format('0', '1')\n"], "sample_315": ["    def test_language_fallback_to_default_language(self):\n        # Test that if the language code is not present in the URL and\n        # i18n_patterns_used and prefixed_default_language are both True,\n        # the middleware will redirect to the default language.\n        request = RequestFactory().get('/en-us/path/to/page')\n        middleware = LocaleMiddleware()\n        response = middleware.process_request(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/en-us/path/to/page')\n\n        request = RequestFactory().get('/path/to/page')\n        response = middleware.process_request(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, '/en-us/path/to/page')\n", "    def test_language_from_request(self):\n        \"\"\"\n        Test that the language is correctly determined from the request.\n        \"\"\"\n        request = RequestFactory().post('/')\n        translation.activate('nl')\n        with override_settings(LANGUAGE_CODE='en-us'):\n            response = LocaleMiddleware().process_request(request)\n            self.assertEqual(translation.get_language(), 'nl')\n", "    def test_process_request(self):\n        request = RequestFactory().get('/')\n        request.LANGUAGE_CODE = None\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        self.assertEqual(request.LANGUAGE_CODE, 'en-us')\n", "    def test_process_request(self):\n        # Test if the language from the request is correctly set.\n        request = RequestFactory().get('/')\n        translation.activate('en-us')\n        request.session = {}\n        self.assertEqual(LocaleMiddleware().process_request(request), None)\n        self.assertEqual(request.LANGUAGE_CODE, 'en-us')\n\n        # Test if the default language is used when i18n patterns are used and\n        # the language from the path is not set.\n        request = RequestFactory().get('/nl/')\n        translation.activate('en-us')\n        request.session = {}\n        self.assertEqual(LocaleMiddleware().process_request(request), None)\n        self.assertEqual(request.LANGUAGE_CODE, settings.LANGUAGE_CODE)\n\n        # Test if the language from the path is used when i18n patterns are used.\n        request = RequestFactory().get('/nl/nl/')\n        translation.activate('en-us')\n        request.session = {}\n        self.assertEqual(LocaleMiddleware().process_request(request), None)\n        self.assertEqual(request.LANGUAGE_CODE, 'nl')\n", "    def test_no_language_fallback(self):\n        # Test that the middleware correctly handles cases where\n        # LANGUAGE_CODE is not the default when using language prefixes.\n        urlconf = 'i18n.patterns.urls.default_no_fallback'\n        settings.ROOT_URLCONF = urlconf\n\n        # Test that the middleware doesn't attempt to redirect when LANGUAGE_CODE\n        # isn't the default.\n        request = RequestFactory().get('/en-us/some-view/')\n        response = LocaleMiddleware().process_request(request)\n        self.assertIsNone(response)\n\n        response = LocaleMiddleware().process_response(request, HttpResponse())\n        self.assertNotIn('Accept-Language', response.headers)\n\n        # Test that the middleware attempts to redirect to the language prefix when\n        # LANGUAGE_CODE isn't the default.\n        request = RequestFactory().get('/pt-br/some-view/')\n        response = LocaleMiddleware().process_request(request)\n        self.assertIsNone(response)\n\n        response = LocaleMiddleware().process_response(request, HttpResponse())\n        self.assertIn('Location', response)\n        self.assertEqual(response.status_code, 301)\n        self.assertIn('pt-br/some-view/', response['Location'])\n"], "sample_1179": ["def test_PolyPrinting():\n    p1 = Poly([1, 2, 3], x)\n    p2 = Poly([1, 2, 3], y)\n    p3 = Poly([1, 2, 3, 4, 5], x)\n    p4 = Poly([1, 2, 3, 4, 5], x, y)\n    expected = \"Poly((1, 2, 3), x)\"\n    assert sstr(p1) == expected\n    expected = \"Poly((1, 2, 3), y)\"\n    assert sstr(p2) == expected\n    expected = \"Poly((1, 2, 3, 4, 5), x)\"\n    assert sstr(p3) == expected\n    expected = \"Poly((1, 2, 3, 4, 5), (x, y))\"\n    assert sstr(p4) == expected\n", "def test_WildDot_printer():\n    w = Wild('w')\n    assert sstr(WildDot(w)) == 'w'\n", "def test_str_printer_relational():\n    # Test that relational printers produce the correct output\n    assert sstr(Eq(x, 0)) == 'Eq(x, 0)'\n    assert sstr(And(x > 0, x < 0)) == 'Eq(x, 0)'\n    assert sstr(Or(x > 0, x < 0)) == 'Or(x > 0, x < 0)'\n    assert sstr(Xor(x > 0, x < 0)) == 'Or(x > 0, x < 0)'\n\n    # Test that relational printers handle negation correctly\n    assert sstr(Eq(-x, 0)) == 'Eq(-x, 0)'\n    assert sstr(And(-x > 0, x < 0)) == 'Eq(x, 0)'\n    assert sstr(Or(-x > 0, x < 0)) == 'Or(-x > 0, x < 0)'\n    assert sstr(Xor(-x > 0, x < 0)) == 'Or(-x > 0, x < 0)'\n", "    def test_str_printer_prints_complex_roots(self):\n        printer = StrPrinter(settings={\"sympy_integers\": True})\n        self.assertEqual(printer.doprint(rootof(2, 2)), \"CRootOf(2, 2)\")\n", "def test_printing():\n    # Test printing of different classes\n    assert str(sstr(True)) == 'True'\n    assert str(sstr(False)) == 'False'\n\n    assert str(sstr(Eq(x + y, 0))) == 'Eq(x + y, 0)'\n    assert str(sstr(Rel(x + y, 0))) == 'x + y == 0'\n    assert str(sstr(true)) == 'True'\n    assert str(sstr(false)) == 'False'\n\n    # Test printing of complex numbers\n    assert str(sstr(x + 2*y)) == 'x + 2*y'\n    assert str(sstr(x - 2*y)) == 'x - 2*y'\n    assert str(sstr(x + 2*y + 3*z)) == 'x + 2*y + 3*z'\n    assert str(sstr(2*x - 2*y + 3*z)) == '2*x - 2*y + 3*z'\n\n    # Test printing of Rational\n    assert str(sstr(Rational(1, 2))) == '1/2'\n    assert str(sstr(Rational(1, 2).simplify())) == '1/2'\n\n    # Test printing of Complex\n    assert str(sstr(x + 2*y + 3*I*z)) == 'x + 2*y + 3*I*z'\n    assert str(sstr(x - 2*y + 3*I*z)) == 'x - 2*y + 3*I*z'\n\n    # Test printing of Tuple\n    assert str(sstr(Tuple(x, y, z))) == '(x, y, z)'\n\n    # Test printing of Dict\n    assert str(sstr(Dict({x: 1, y: 2}))) == '{x: 1, y: 2}'\n\n    # Test printing of Relational\n    assert str(sstr(Eq(x, y))) == 'Eq(x, y)'\n    assert str(sstr(Ne(x, y))) == 'x != y'\n\n    # Test printing of Integer and Float\n    assert str(sstr(Integer(5))) == '5'\n    assert str(sstr(Float(5.0))) == '5.0'\n\n    # Test printing of Pow\n    assert str(sstr(x**2))"], "sample_13": ["def test_Latitude_bounds():\n    lat = Latitude([-91, 91] * u.deg)\n    with pytest.raises(ValueError):\n        assert lat.is_within_bounds()\n\n    lat = Latitude([91, -91] * u.deg)\n    with pytest.raises(ValueError):\n        assert lat.is_within_bounds()\n\n    lat = Latitude([-90, 90] * u.deg)\n    assert lat.is_within_bounds()\n\n    lat = Latitude([90, -90] * u.deg)\n    assert lat.is_within_bounds()\n\n    lat = Latitude([-90, 90] * u.deg)\n    assert lat.is_within_bounds(lower=-90 * u.deg, upper=90 * u.deg)\n\n    lat = Latitude([90, -90] * u.deg)\n    assert lat.is_within_bounds(lower=-90 * u.deg, upper=90 * u.deg)\n", "def test_angle_wrap_at():\n    angle = Angle([0, 360, 720, 900] * u.deg)\n    wrapped_angle = angle.wrap_at(360 * u.deg)\n    assert_array_equal(wrapped_angle.degree, [0, 0, 0, 0])\n", "def test_latitude_invalid_angle():\n    with pytest.raises(ValueError):\n        Latitude(91.0 * u.deg)\n", "def test_latitude_convert_to_longitude():\n    lat = Latitude(30 * u.deg)\n    long = lat.to_longitude()\n    assert isinstance(long, Longitude)\n    assert np.allclose(long.value, 30 * u.deg)\n", "def test_latitude_copy():\n    lat = Latitude(['30.0d', '-40.0d'])\n    lat_copy = lat.copy()\n    assert lat is not lat_copy\n    assert_array_equal(lat, lat_copy)\n    lat_copy[0] = 30.5\n    assert not np.array_equal(lat, lat_copy)\n"], "sample_889": ["def test_calibrated_classifiers_params(data):\n    X, y = data\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    clf = CalibratedClassifierCV(LogisticRegression(), method=method, cv=2, ensemble=ensemble)\n    clf.fit(X_train, y_train)\n    assert len(clf.calibrated_classifiers_) == 2\n    assert isinstance(clf.calibrated_classifiers_[0].estimator, LogisticRegression)\n    assert isinstance(clf.calibrated_classifiers_[0].calibrators[0], _SigmoidCalibration) or isinstance(\n        clf.calibrated_classifiers_[0].calibrators[0], IsotonicRegression\n    )\n    assert isinstance(clf.calibrated_classifiers_[1].estimator, LogisticRegression)\n    assert isinstance(clf.calibrated_classifiers_[1].calibrators[0], _SigmoidCalibration) or isinstance(\n        clf.calibrated_classifiers_[1].calibrators[0], IsotonicRegression\n    )\n\n    # Check that the fitted calibrators have the correct parameters\n    for calibrator in [clf.calibrated_classifiers_[0].calibrators[0], clf.calibrated_classifiers_[1].calibrators[0]]:\n        if isinstance(calibrator, _SigmoidCalibration):\n            assert calibrator.a_ is not None\n            assert calibrator.b_ is not None\n        elif isinstance(calibrator, IsotonicRegression):\n            assert calibrator.y_ is not None\n            assert calibrator.x_ is not None\n\n    # Check that the calibrated_classifiers_ attribute is updated correctly when ensemble=False\n    clf = CalibratedClassifierCV(LogisticRegression(), method=method, cv=2, ensemble=False)\n    clf.fit(X_train, y_train)\n    assert len(clf.calibrated_classifiers_) == 1\n    assert isinstance(clf.calibrated_classifiers_[0].estimator, LogisticRegression)\n    assert isinstance(clf.calibrated_classifiers_[0].calibrators[0], _SigmoidCalibration) or isinstance(\n        clf.calibrated_classifiers_[0].calibrators[0], IsotonicRegression\n    )\n\n    # Check that the calibrated_classifiers_ attribute", "def test_calibratedclassifiercv_binary_probabilities():\n    # Test binary classification\n    X, y = make_blobs(n_samples=N_SAMPLES, n_features=4, centers=2, random_state=42)\n    y = y == 0  # binary classification\n    clf = CalibratedClassifierCV(LinearSVC(random_state=42), cv=5, method=\"sigmoid\")\n    clf.fit(X, y)\n    y_pred_proba = clf.predict_proba(X)\n    assert y_pred_proba.shape == (N_SAMPLES, 2)\n    assert_allclose(y_pred_proba.sum(axis=1), np.ones(N_SAMPLES))\n", "def test_calibrated_classifiers_single_classifier_cv(estimator, data):\n    \"\"\"Tests the single calibrated classifier case.\"\"\"\n    X, y = data\n    cv = KFold(n_splits=2, shuffle=True, random_state=42)\n    calibrator = CalibratedClassifierCV(estimator, cv=cv, ensemble=False)\n    calibrator.fit(X, y)\n    assert calibrator.calibrated_classifiers_ == [\n        _CalibratedClassifier(calibrator.estimator, calibrator.calibrators_, method=\"sigmoid\")\n    ]\n    y_pred = calibrator.predict(X)\n    assert len(y_pred) == X.shape[0]\n", "def test_calibrated_classifiers_length():\n    \"\"\"Check that the number of fitted classifier-calibrator pairs is correct.\n\n    The number of fitted classifier-calibrator pairs should be equal to the\n    number of cross-validation folds when `ensemble` is True. When `ensemble`\n    is False, the number should be 1.\n\n    Parameters\n    ----------\n    method : {'sigmoid', 'isotonic'}\n        Method to use for calibration.\n\n    ensemble : bool\n        Whether to use an ensemble of calibrated classifiers.\n\n    X : array-like of shape (n_samples, n_features)\n        The samples.\n\n    y : array-like of shape (n_samples,)\n        The targets.\n\n    cv : int or cv object\n        The cross-validation object.\n\n    \"\"\"\n    X, y = data()\n    estimator = MultinomialNB()\n    for method, ensemble in itertools.product([\"sigmoid\", \"isotonic\"], [True, False]):\n        for cv in [LeaveOneOut(), 5]:\n            clf = CalibratedClassifierCV(\n                estimator,\n                cv=cv,\n                ensemble=ensemble,\n                method=method,\n            )\n            clf.fit(X, y)\n            assert len(clf.calibrated_classifiers_) == (cv.n_splits if hasattr(cv, \"n_splits\") else cv)\n", "def test_calibrated_classifier_cv_binary_classifiers():\n    X, y = make_classification(n_samples=N_SAMPLES, n_classes=2, n_features=6, random_state=42)\n    y = np.where(y == 0, -1, y)\n\n    base_clf = clone(LinearSVC())\n    calibrated_clf = CalibratedClassifierCV(base_clf, cv=3, method=\"sigmoid\")\n    calibrated_clf.fit(X, y)\n    calibrated_proba = calibrated_clf.predict_proba(X)\n\n    # We do not have the ground truth for the calibrated probabilities,\n    # but we can check the shape.\n    assert calibrated_proba.shape == (N_SAMPLES, 2)\n\n    # We can also check that the classifier is well-calibrated.\n    prob_true, prob_pred = calibration_curve(y, calibrated_proba[:, 1])\n    brier_score = brier_score_loss(y, calibrated_proba[:, 1])\n    assert brier_score >= 0\n    assert brier_score <= 0.25  # The Brier score should not be worse than a random classifier.\n\n    # Check that the `predict` method works.\n    assert_array_equal(calibrated_clf.predict(X), y)\n\n    # Check that the `predict_proba` method works.\n    assert_array_almost_equal(calibrated_clf.predict_proba(X), calibrated_proba)\n\n    # Check that the `predict_proba` method returns the correct probabilities\n    # even when the input is a single sample.\n    single_sample = np.array([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]])\n    assert_array_almost_equal(calibrated_clf.predict_proba(single_sample), calibrated_proba[0])\n\n    # Check that a NotFittedError is raised when calling `predict_proba`\n    # before calling `fit`.\n    try:\n        base_clf.predict_proba(X)\n        assert False\n    except NotFittedError:\n        pass\n\n    # Check that a NotFittedError is raised when calling `predict` before\n    # calling `fit`.\n    try:\n        base_clf.predict(X)\n        assert False\n    except NotFittedError:\n        pass\n\n    # Check that a NotFittedError is raised when calling `fit` again after\n    # calling `fit`.\n    try:\n        calibrated_clf.fit(X, y)\n        assert False\n    except NotFittedError:\n        pass\n"], "sample_1109": ["def test_frac_complex():\n    assert frac(1 + 2*I) == S.One + I*frac(2)\n    assert frac(-1 - 2*I) == I*frac(2) - 1\n", "compilation error", "def test_frac_rewrite_as_floor_and_ceiling():\n    x = Symbol('x')\n    y = Symbol('y', real=True)\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n    assert frac(y).rewrite(floor) == y - floor(y)\n    assert frac(y).rewrite(ceiling) == y + ceiling(-y)\n", "def test_fuzz_floored_fractions():\n    from sympy.abc import x\n    from sympy.testing.pytest import XFAIL\n\n    # Test floor of fractional numbers\n    assert floor(frac(x)).simplify() == x - floor(x)\n\n    # Test floor of complex fractions\n    assert floor(frac(x + I*x)).simplify() == I*floor(frac(x)) + floor(x)\n\n    # Test floor of fractions with arguments containing zoo\n    assert floor(frac(zoo)).simplify() == zoo - floor(zoo)\n    assert floor(frac(oo)).simplify() == oo - floor(oo)\n\n    # Test floor of fractions with arguments containing nan\n    assert floor(frac(nan)).simplify() == nan - floor(nan)\n\n    # Test floor of fractions with arguments containing exp\n    assert floor(frac(exp(x))).simplify() == frac(exp(x))\n\n    # Test floor of fractions with arguments containing log\n    assert floor(frac(log(x))).simplify() == frac(log(x))\n\n    # Test floor of fractions with arguments containing sin\n    assert floor(frac(sin(x))).simplify() == frac(sin(x))\n\n    # Test floor of fractions with arguments containing sqrt\n    assert floor(frac(sqrt(x))).simplify() == frac(sqrt(x))\n\n    # Test floor of fractions with arguments containing factorial\n    assert floor(frac(factorial(x))).simplify() == frac(factorial(x))\n\n    # Test floor of fractions with arguments containing ceiling\n    assert floor(frac(ceiling(x))).simplify() == frac(ceiling(x))\n\n    # Test floor of fractions with arguments containing Pi\n    assert floor(frac(pi)).simplify() == frac(pi)\n\n    # Test floor of fractions with arguments containing Rational\n    assert floor(frac(Rational(1,2))).simplify() == frac(Rational(1,2))\n\n    # Test floor of fractions with arguments containing Float\n    assert floor(frac(Float(1.0))).simplify() == frac(Float(1.0))\n\n    # Test floor of fractions with arguments containing I\n    assert floor(frac(I*x)).simplify() == I*floor(frac(x))\n\n    # Test floor of fractions with arguments containing S\n    assert floor(frac(S(1))).simplify() == frac(S(1))\n\n    # Test floor of fractions with arguments containing symbols\n    assert floor(frac(y)).simplify() == 0\n\n", "def test_ceiling_floor_different_types():\n    assert floor(nan) == nan\n    assert floor(zoo) == oo\n    assert ceiling(zoo) == -oo\n    assert floor(E) == 5\n    assert floor(-E) == -6\n    assert ceiling(E) == 6\n    assert ceiling(-E) == 5\n    assert floor(I) == I\n    assert floor(-I) == -I\n    assert ceiling(I) == I\n    assert ceiling(-I) == -I\n    assert floor(S(0.5)) == 0\n    assert floor(S(-0.5)) == -1\n    assert ceiling(S(0.5)) == 1\n    assert ceiling(S(-0.5)) == 0\n    assert floor(S(1.5)) == 1\n    assert floor(S(-1.5)) == -2\n    assert ceiling(S(1.5)) == 2\n    assert ceiling(S(-1.5)) == -1\n    assert floor(2*E) == 5\n    assert floor(-2*E) == -6\n    assert ceiling(2*E) == 6\n    assert ceiling(-2*E) == -5\n    assert frac(pi).is_real\n    assert not frac(pi).is_integer\n    assert frac(pi).is_extended_real\n    assert not frac(pi).is_extended_integer\n    assert frac(pi).is_real\n    assert not frac(pi).is_integer\n    assert frac(pi).is_extended_real\n    assert not frac(pi).is_extended_integer\n    assert frac(S(3)/2).is_integer\n    assert frac(-S(3)/2).is_integer\n    assert frac(S(1)/3).is_integer\n    assert frac(-S(1)/3).is_integer\n    assert frac(pi/2).is_real\n    assert not frac(pi/2).is_integer\n    assert frac(pi/2).is_extended_real\n    assert not frac(pi/2).is_extended_integer\n"], "sample_322": ["    def test_invalid_migration_plan(self):\n        executor = MigrationExecutor(connection=connection)\n        with self.assertRaises(InvalidMigrationPlan):\n            executor.migrate(targets=[(\"migrations\", None)])\n", "    def test_migrate_with_invalid_plan(self):\n        \"\"\"\n        Tests that an InvalidMigrationPlan is raised when trying to migrate\n        with a plan that contains both forwards and backwards migrations.\n        \"\"\"\n        executor = MigrationExecutor(connection=self.connection)\n        with self.assertRaises(InvalidMigrationPlan):\n            executor.migrate([(\"migrations\", \"0001_initial\"), (\"migrations\", \"0002_data\")])\n", "    def test_detect_soft_applied_initial_migration(self):\n        # Create an initial migration that creates a model\n        initial_migration = migrations.RunPython('test_initial_migration')\n        # Apply the initial migration\n        executor = MigrationExecutor(connection)\n        state = executor._create_project_state(with_applied_migrations=True)\n        state.apply_migration(state, initial_migration)\n        # Verify that detect_soft_applied returns True\n        self.assertTrue(executor.detect_soft_applied(None, initial_migration))\n", "    def test_detect_soft_applied_with_initial_migration_replaced(self):\n        from migrations import test_migrations\n        from migrations2 import test_migrations as test_migrations2\n\n        MigrationExecutor.detect_soft_applied = mock.patch.object(MigrationExecutor, 'detect_soft_applied')\n        MigrationExecutor.detect_soft_applied.return_value = (True, None)\n\n        migration_executor = MigrationExecutor(connection=connection)\n        migration = test_migrations2.get_migration('0002_replace_migration')\n        project_state = migration_executor._create_project_state()\n        result = migration_executor.detect_soft_applied(project_state, migration)\n\n        self.assertTrue(result[0])\n        self.assertIsNone(result[1])\n", "    def test_migration_plan_forwards_and_backwards_migrations(self):\n        with self.assertRaises(InvalidMigrationPlan):\n            executor = MigrationExecutor(connection=self.connection)\n            plan = executor.migration_plan([(app_label, \"0001_initial\"), (\"migrations2\", \"0002_backwards\")])\n"], "sample_72": ["    def test_uuid_serializer(self):\n        serializer = serializer_factory(uuid.UUID('12345678-1234-1234-1234-123456789012'))\n        self.assertIsInstance(serializer, UUIDSerializer)\n        expected_imports = {\"import uuid\"}\n        self.assertEqual(serializer.serialize(), (\"uuid.UUID('12345678-1234-1234-1234-123456789012')\", expected_imports))\n", "    def test_serializer_factory_model_field(self):\n        field = models.CharField(max_length=10)\n        serializer = serializer_factory(field)\n        self.assertIsInstance(serializer, ModelFieldSerializer)\n        self.assertEqual(serializer.value, field)\n", "    def test_serializer_factory(self):\n        # Test custom Decimal subclass\n        serializer = serializer_factory(Money(10))\n        self.assertIsInstance(serializer, DecimalSerializer)\n        self.assertEqual(serializer.serialize()[0], 'decimal.Decimal(\"10\")')\n        self.assertEqual(serializer.serialize()[1], {'from decimal import Decimal'})\n\n        # Test SimpleLazyObject\n        lazy_value = SimpleLazyObject(lambda: 10)\n        serializer = serializer_factory(lazy_value)\n        self.assertIsInstance(serializer, BaseSerializer)\n        self.assertEqual(serializer.serialize()[0], '10')\n        self.assertEqual(serializer.serialize()[1], set())\n\n        # Test email validator\n        validator = EmailValidator('email@example.com')\n        serializer = serializer_factory(validator)\n        self.assertIsInstance(serializer, DeconstructableSerializer)\n        self.assertEqual(serializer.serialize()[0], 'EmailValidator(\"email@example.com\")')\n        self.assertEqual(serializer.serialize()[1], set())\n\n        # Test regex validator\n        validator = RegexValidator('regex')\n        serializer = serializer_factory(validator)\n        self.assertIsInstance(serializer, RegexSerializer)\n        self.assertEqual(serializer.serialize()[0], 're.compile(\"regex\")')\n        self.assertEqual(serializer.serialize()[1], {'import re'})\n\n        # Test function with __self__\n            pass\n        func.__self__ = object()\n        serializer = serializer_factory(func)\n        self.assertIsInstance(serializer, FunctionTypeSerializer)\n        self.assertEqual(serializer.serialize()[0], 'object.func')\n        self.assertEqual(serializer.serialize()[1], {'import object'})\n\n        # Test type with __module__\n        class CustomType:\n            pass\n        CustomType.__module__ = 'custom_module'\n        serializer = serializer_factory(CustomType)\n        self.assertIsInstance(serializer, TypeSerializer)\n        self.assertEqual(serializer.serialize()[0], 'custom_module.CustomType')\n        self.assertEqual(serializer.serialize()[1], {'import custom_module'})\n\n        # Test UUID\n        serializer = serializer_factory(uuid.UUID('12345678-1234-1234-1234-123456789012'))\n        self.assertIsInstance(serializer, UUIDSerializer)\n        self.assertEqual(serializer.serialize()[0], 'uuid.UUID(\"12345678-1234-1234-1234-123456789012\")')\n        self.assertEqual(serializer.serialize()[1], {'import uuid'})\n", "    def test_registering_new_serializer(self):\n        class CustomSerializer(BaseSerializer):\n                return \"custom\", {\"import myapp\"}\n\n        Serializer.register(models.CharField, CustomSerializer)\n        self.assertEqual(serializer_factory(models.CharField).serialize(), (\"custom\", {\"import myapp\"}))\n\n        # Test that the new serializer is used in place of the old one\n        self.assertEqual(Serializer._registry[models.CharField], CustomSerializer)\n", "    def test_money(self):\n        value = Money(10)\n        serializer = serializer_factory(value)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, repr(value))\n        self.assertEqual(imports, {\"from decimal import Decimal\"})\n"], "sample_1039": ["def test_print_mathml_GoldenRatio():\n    from sympy.printing.mathml import print_mathml\n    from sympy import GoldenRatio\n    expected_presentation = '<mrow><mi>\u03d5</mi></mrow>'\n    expected_content = '<apply><ci>golden_ratio</ci></apply>'\n    assert print_mathml(GoldenRatio, printer='presentation') == expected_presentation\n    assert print_mathml(GoldenRatio, printer='content') == expected_content\n", "def test_MathMLPrinterBase_doprint():\n    \"\"\"Test MathMLPrinterBase.doprint.\"\"\"\n    from sympy.printing.mathml import MathMLPrinterBase\n    printer = MathMLPrinterBase()\n    expr = Symbol('x') + 1\n    result = printer.doprint(expr)\n    expected_result = '<apply><plus/><ci>x</ci><cn>1</cn></apply>'\n    assert result == expected_result\n", "def test_MathMLPrinter_mathml_tag():\n    # Test mathml_tag method in MathMLContentPrinter\n    mp = MathMLContentPrinter()\n    assert mp.mathml_tag(S.Add) == 'plus'\n    assert mp.mathml_tag(S.Mul) == 'times'\n    assert mp.mathml_tag(S.Derivative) == 'diff'\n    assert mp.mathml_tag(S.Symbol('x')) == 'ci'\n    assert mp.mathml_tag(S.Symbol('x', 1)) == 'ci'\n    assert mp.mathml_tag(S.RandomSymbol('x')) == 'ci'\n    assert mp.mathml_tag(S.sin) == 'sin'\n    assert mp.mathml_tag(S.cos) == 'cos'\n    assert mp.mathml_tag(S.tan) == 'tan'\n    assert mp.mathml_tag(S.cot) == 'cot'\n    assert mp.mathml_tag(S.asin) == 'arcsin'\n    assert mp.mathml_tag(S.asinh) == 'arcsinh'\n    assert mp.mathml_tag(S.acos) == 'arccos'\n    assert mp.mathml_tag(S.acosh) == 'arccosh'\n    assert mp.mathml_tag(S.atan) == 'arctan'\n    assert mp.mathml_tag(S.atanh) == 'arctanh'\n    assert mp.mathml_tag(S.acot) == 'arccot'\n    assert mp.mathml_tag(S.atan2) == 'arctan'\n    assert mp.mathml_tag(S.log) == 'ln'\n    assert mp.mathml_tag(S.Equality) == 'eq'\n    assert mp.mathml_tag(S.Unequality) == 'neq'\n    assert mp.mathml_tag(S.GreaterThan) == 'geq'\n    assert mp.mathml_tag(S.LessThan) == 'leq'\n    assert mp.mathml_tag(S.StrictGreaterThan) == 'gt'\n    assert mp.mathml_tag(S.StrictLessThan) == 'lt'\n\n    # Test that mathml_tag returns the class name for unknown types\n    class Unknown(S.Basic):\n        pass\n    assert mp.mathml_tag(Unknown()) == 'basic'\n", "def test_mathmlprinter_apply_patch():\n    mp = MathMLContentPrinter()\n    mp.apply_patch()\n    assert mp._Element_writexml is not None\n    assert mp._Text_writexml is not None\n", "def test_mathml_rational():\n    from sympy.printing.mathml import mathml\n    from sympy import Rational\n\n    # Create a Rational number with denominator 1\n    r = Rational(2)\n\n    # Test that mathml returns the correct MathML representation\n    assert mathml(r) == '<cn>2</cn>'\n"], "sample_135": ["    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.datetime = datetime(2003, 10, 7, 11, 39)\n        self.date = date(2003, 10, 7)\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.datetime = datetime.now(get_fixed_timezone(0))\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def test_leap_year_detection(self):\n        dt = datetime(2000, 2, 29)\n        df = DateFormat(dt)\n        self.assertTrue(df.L())\n"], "sample_288": ["    def test_key_transform_preprocess_lhs(self):\n        model = models.Model()\n        field = KeyTransform('key')\n        compiler = mock.Mock()\n        connection = mock.Mock()\n        result = field.preprocess_lhs(compiler, connection)\n        self.assertEqual(result[0], model)\n        self.assertEqual(result[1], [])\n        self.assertEqual(result[2], ['key'])\n", "def test_has_any_keys_lookup(self):\n    json_model = JSONModel.objects.create(json_field={'a': 1, 'b': 2})\n    nullable_json_model = NullableJSONModel.objects.create(json_field={'a': 1, 'b': 2})\n\n    # Test valid lookups\n    query = JSONModel.objects.filter(json_field__has_any_keys=['a', 'b', 'c'])\n    self.assertEqual(list(query), [json_model])\n    query = JSONModel.objects.filter(json_field__has_any_keys=['a', 'b'])\n    self.assertEqual(list(query), [json_model])\n    query = JSONModel.objects.filter(json_field__has_any_keys=['a'])\n    self.assertEqual(list(query), [json_model])\n    query = JSONModel.objects.filter(json_field__has_any_keys=['a', 'c'])\n    self.assertEqual(list(query), [])\n\n    # Test invalid lookups\n    with self.assertRaises(ValidationError):\n        JSONModel.objects.filter(json_field__has_any_keys=['a', 'b', 1])\n\n    # Test empty list\n    query = JSONModel.objects.filter(json_field__has_any_keys=[])\n    self.assertEqual(list(query), [])\n\n    # Test NULL JSONField\n    with self.assertRaises(ValidationError):\n        NullableJSONModel.objects.filter(json_field__has_any_keys=['a', 'b'])\n\n    # Test NULL value in the database\n    nullable_json_model.json_field = None\n    nullable_json_model.save()\n    with self.assertRaises(ValidationError):\n        NullableJSONModel.objects.filter(json_field__has_any_keys=['a', 'b'])\n", "    def test_json_decode_error(self):\n        model = JSONModel()\n        field = JSONField()\n        with self.assertRaises(json.JSONDecodeError):\n            field.to_python('not valid json')\n", "    def test_json_field_with_custom_encoder_and_decoder(self):\n        # Test that we can use a custom encoder and decoder with a JSONField.\n        model = JSONModel.objects.create(json_field={'foo': 'bar'})\n\n        class CustomJSONEncoder(json.JSONEncoder):\n                return str(o)\n\n        class CustomJSONDecoder(json.JSONDecoder):\n                super().__init__(*args, **kwargs)\n                self.cache = {}\n\n                if obj in self.cache:\n                    return self.cache[obj]\n                else:\n                    self.cache[obj] = super().object_hook(obj)\n                    return self.cache[obj]\n\n        field = JSONField(encoder=CustomJSONEncoder, decoder=CustomJSONDecoder)\n        self.assertEqual(model.json_field, 'bar')\n\n        # Test that the custom decoder is used correctly.\n        model.json_field = {'foo': 'baz'}\n        model.save()\n        self.assertEqual(model.json_field, {'foo': 'baz'})\n\n        # Test that the custom encoder is used correctly.\n        model.json_field = 'qux'\n        model.save()\n        self.assertEqual(model.json_field, {'foo': 'qux'})\n", "    def test_check_mixin(self):\n        # Test that the mixin allows for data validation\n        json_field = JSONField(encoder=json.JSONEncoder, decoder=CustomJSONDecoder)\n        json_field.clean({\"key\": \"value\"})  # Should not raise an exception\n        with self.assertRaises(ValidationError):\n            json_field.clean({\"key\": 123})  # Should raise an exception\n"], "sample_77": ["    def check_output(self, function, value, output=None):\n        \"\"\"\n        function(value) equals output. If output is None, function(value)\n        equals value.\n        \"\"\"\n        if output is None:\n            output = value\n        self.assertEqual(function(value), output)\n", "def test_linebreaks_autoescape(self):\n    \"\"\"Test linebreaks with autoescape enabled.\"\"\"\n    value = \"<p>First paragraph.</p><p>Second paragraph.</p>\"\n    self.check_output(linebreaks, value, '<p><p>First paragraph.</p><br><p>Second paragraph.</p></p>')\n", "def test_linebreaks_autoescape_true(self):\n    \"\"\"linebreaks(value, autoescape=True)\"\"\"\n    value = \"Hello\\nWorld\\n\\nThis is a new paragraph.\"\n    expected_output = \"<p>Hello</p><p>World</p><p>This is a new paragraph.</p>\"\n    self.check_output(linebreaks, value, expected_output, autoescape=True)\n", "def test_avoid_wrapping(self):\n    # Test with normal spaces\n    self.check_output(avoid_wrapping, \"Hello World\")\n    self.check_output(avoid_wrapping, \"Hello, World!\")\n\n    # Test with different lengths of text\n    self.check_output(avoid_wrapping, \"Short text\")\n    self.check_output(avoid_wrapping, \"Much longer text that wraps\")\n    self.check_output(avoid_wrapping, \"Very, very, very long text that will wrap\")\n\n    # Test with non-breaking spaces\n    self.check_output(avoid_wrapping, \"Hello\u00a0World\")\n    self.check_output(avoid_wrapping, \"Hello, World!\")\n    self.check_output(avoid_wrapping, \"Short text\u00a0\")\n    self.check_output(avoid_wrapping, \"Much longer text that wraps\u00a0\")\n    self.check_output(avoid_wrapping, \"Very, very, very long text that will wrap\u00a0\")\n\n    # Test with no spaces\n    self.check_output(avoid_wrapping, \"HelloWorld\")\n    self.check_output(avoid_wrapping, \"Hello,World!\")\n    self.check_output(avoid_wrapping, \"Shorttext\")\n    self.check_output(avoid_wrapping, \"Muchlongertextthatwraps\")\n    self.check_output(avoid_wrapping, \"Very,very,verylongtextthatwillwrap\")\n\n    # Test with different types of whitespace\n    self.check_output(avoid_wrapping, \"Hello\\rWorld\")\n    self.check_output(avoid_wrapping, \"Hello\\nWorld\")\n    self.check_output(avoid_wrapping, \"Hello\\tWorld\")\n    self.check_output(avoid_wrapping, \"Hello\\fWorld\")\n\n    # Test with non-breaking characters\n    self.check_output(avoid_wrapping, \"Hello\u2014World\")\n    self.check_output(avoid_wrapping, \"Hello\u2014, World!\")\n    self.check_output(avoid_wrapping, \"Short\u2014text\")\n    self.check_output(avoid_wrapping, \"Much\u2014longer text that wraps\u2014\")\n    self.check_output(avoid_wrapping, \"Very, very, very long text that will wrap\u2014\")\n", "def test_strip_tags_with_no_tags(self):\n    self.check_output(strip_tags, \"Hello World\")\n    self.check_output(strip_tags, \"Hello < World >\")\n"], "sample_433": ["    def test_create_model(self):\n        model = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200)),\n            ],\n        )\n        changes = self.get_changes([model], [model])\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, table=f\"testapp_author\", db_table=\"author\"\n        )\n", "    def test_migration_fields(self):\n        migration = Migration(name=\"test_migration\", app_label=\"testapp\")\n        self.assertEqual(migration.operations, [])\n", "    def assertReversibleOperation(self, changes, app_label, position, operation_type):\n        if not changes.get(app_label):\n            self.fail(\n                \"No migrations found for %s\\n%s\"\n                % (app_label, self.repr_changes(changes))\n            )\n        if len(changes[app_label]) < position + 1:\n            self.fail(\n                \"No migration at index %s for %s\\n%s\"\n                % (position, app_label, self.repr_changes(changes))\n            )\n        migration = changes[app_label][position]\n        if not all(operation.reversible for operation in migration.operations):\n            self.fail(\n                \"Not all operations in %s.%s are reversible\\n%s\"\n                % (app_label, migration.name, self.repr_changes(changes))\n            )\n        self.assertOperationTypes(changes, app_label, position, [operation_type])\n", "    def test_atomic_migrations(self):\n        migration = Migration(\n            \"test_migrations\", \"testapp\"\n        )\n        self.assertTrue(migration.atomic)\n\n        migration.atomic = False\n        self.assertFalse(migration.atomic)\n\n        migration.atomic = True\n        self.assertTrue(migration.atomic)\n", "    def test_migration_suggest_name(self):\n        initial_migration = Migration(name=\"initial\", app_label=\"testapp\")\n        self.assertEqual(initial_migration.suggest_name(), \"initial\")\n\n        migration = Migration(\n            name=\"0001_initial\", operations=[], app_label=\"testapp\"\n        )\n        self.assertEqual(migration.suggest_name(), \"0001_initial\")\n\n        migration = Migration(\n            name=\"0001_auto_20210221_2216\", operations=[], app_label=\"testapp\"\n        )\n        self.assertEqual(migration.suggest_name(), \"0001_auto_20210221_2216\")\n\n        migration = Migration(\n            name=\"0001_initial\", operations=[], app_label=\"testapp\"\n        )\n        migration.operations = [\n            migrations.RunSQL(\"CREATE INDEX idx_testapp_author_name ON testapp_author (name);\")\n        ]\n        self.assertEqual(migration.suggest_name(), \"0001_auto_20210221_2216\")\n\n        migration = Migration(\n            name=\"0001_initial\", operations=[], app_label=\"testapp\"\n        )\n        migration.operations = [\n            migrations.RunSQL(\n                \"CREATE INDEX idx_testapp_author_date_of_birth ON testapp_author (date_of_birth);\"\n            )\n        ]\n        self.assertEqual(migration.suggest_name(), \"0001_auto_20210221_2216\")\n"], "sample_1162": ["def test_Derivative_wrt_non_symbol():\n    from sympy import exp, sin, cos, symbols\n    x, y = symbols('x y')\n    F = exp(x*y)\n    deriv = F.diff(y)\n    assert deriv == F.diff(y)\n", "def test_Lambda_evaluate():\n    # Test that lambda functions are properly evaluated when called\n    # with the correct arguments\n    f = Lambda(x, x**2)\n    assert f(3) == 9\n\n    # Test that lambda functions with multiple arguments are properly evaluated\n    # when called with the correct arguments\n    g = Lambda((x, y), x + y)\n    assert g(2, 3) == 5\n\n    # Test that lambda functions are properly evaluated when called with\n    # the correct arguments, even if the arguments are not in the same\n    # order as they were defined\n    g = Lambda((x, y), x + y)\n    assert g(3, 2) == 5\n\n    # Test that lambda functions raise an error when called with the wrong\n    # number of arguments\n    f = Lambda(x, x**2)\n    try:\n        f(1, 2)\n    except BadArgumentsError:\n        pass\n    else:\n        assert False\n\n    # Test that lambda functions raise an error when called with the wrong\n    # type of argument\n    f = Lambda(x, x**2)\n    try:\n        f('a')\n    except BadArgumentsError:\n        pass\n    else:\n        assert False\n\n    # Test that lambda functions can be used to define functions that take\n    # any number of arguments\n    g = Lambda((x,), x)\n    assert g(1) == 1\n    assert g(1, 2) == 1\n    assert g(1, 2, 3) == 1\n\n    # Test that lambda functions can be used to define functions that take\n    # any number of arguments, including zero\n    g = Lambda((x,), x)\n    assert g() == g(1) == 1\n\n    # Test that lambda functions can be used to define functions that take\n    # any number of arguments, and that these functions can be used\n    # in other mathematical operations\n    f = Function('f')\n    g = Lambda((x,), x)\n    assert f(g(1)) == 1\n    assert g(f(1)) == 1\n", "def test_coeff_isneg():\n    # Test basic cases\n    assert _coeff_isneg(S(-3))\n    assert not _coeff_isneg(S(3))\n    assert _coeff_isneg(-oo)\n    assert not _coeff_isneg(oo)\n    assert not _coeff_isneg(S.NegativeOne * Symbol('x', negative=True))\n\n    # Test matrix expressions\n    assert _coeff_isneg(-sin(Symbol('x'))*Symbol('A'))\n    assert not _coeff_isneg(sin(Symbol('x'))*Symbol('A'))\n    assert _coeff_isneg(-exp(-oo)*Symbol('A'))\n    assert not _coeff_isneg(exp(-oo)*Symbol('A'))\n", "def test_Function():\n    # Test that Function correctly handles undefined functions\n    f = Function('f')\n    g = Function('g')\n    assert f(x).is_Function\n    assert g(x).is_Function\n\n    # Test that Function correctly handles undefined function arguments\n    assert f(sin(x)).is_Function\n    assert g(cos(x)).is_Function\n\n    # Test that Function correctly handles string arguments\n    assert Function('sin').is_Function\n    assert Function('cos').is_Function\n    assert Function('exp').is_Function\n\n    # Test that Function correctly handles non-string arguments\n    assert not Function(pi).is_Function\n    assert not Function(symbols('x')).is_Function\n\n    # Test that Function raises a TypeError for invalid arguments\n    try:\n        Function(123)\n        assert False\n    except TypeError as e:\n        assert str(e) == 'expecting string or Symbol for name'\n\n    # Test that Function correctly handles evaluation of undefined functions\n    f = Function('f')\n    g = Function('g')\n    expr = f(x) + g(x)\n    assert expr.is_Function\n    assert expr.diff(x).is_Function\n\n    # Test that Function correctly handles symbols with assumptions\n    x = Symbol('x', positive=True)\n    f = Function('f', positive=True)\n    assert f(x).is_positive\n    assert f(-x).is_negative\n\n    # Test that Function correctly handles undefined functions with different arities\n    f = Function('f', nargs=1)\n    g = Function('f', nargs=2)\n    assert f(x).nargs == S.FiniteSet(1)\n    assert g(x, y).nargs == S.FiniteSet(2)\n\n    # Test that Function correctly handles undefined functions with no arguments\n    f = Function('f')\n    assert f().nargs == S.Naturals0\n", "compilation error"], "sample_839": ["def test_transform_counts():\n    vectorizer = CountVectorizer()\n    X = vectorizer.fit_transform([\"hello world\", \"world\"])\n    assert_array_equal(X.toarray(), np.array([[1, 1, 1], [1, 1, 0]]))\n", "def test_tfidf_transformer_sublinear_tf():\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    vectorizer = TfidfVectorizer(sublinear_tf=True)\n    X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n    # We cannot use assert_array_equal because of float precision issues\n    assert_almost_equal(X.toarray().sum(axis=1), np.ones(X.shape[0]))\n    assert_warns_message(UserWarning, lambda: TfidfVectorizer(sublinear_tf=True))\n", "def test_TfidfVectorizer_clone():\n    vectorizer = TfidfVectorizer(input='filename', encoding='utf-8')\n    X = vectorizer.fit_transform([(\"hello world\", \"hello python\"), (\"foo bar\", \"foo baz\")])\n    X_clone = clone(vectorizer).fit_transform([(\"hello world\", \"hello python\"), (\"foo bar\", \"foo baz\")])\n    assert_array_equal(X.toarray(), X_clone.toarray())\n", "    def test_get_hasher(self):\n        vectorizer = HashingVectorizer()\n        hasher = vectorizer._get_hasher()\n        assert isinstance(hasher, FeatureHasher)\n        assert hasher.n_features == 1048576\n        assert hasher.alternate_sign\n", "def test_hashing_vectorizer_classify():\n    # Test to check if HashingVectorizer is being used as a feature\n    # extractor in a pipeline for classification\n    X_train, X_test, y_train, y_test = train_test_split(\n        ALL_FOOD_DOCS, [0] * 6 + [1] * 4, test_size=0.5, random_state=42)\n    vectorizer = HashingVectorizer(n_features=2**4)\n    clf = LinearSVC()\n    pipeline = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n    pipeline.fit(X_train, y_train)\n    score = cross_val_score(pipeline, ALL_FOOD_DOCS, [0] * 10, cv=2)\n    assert_array_equal(score, [1.0, 1.0])\n    assert_warns_message(ChangedBehaviorWarning, pipeline.fit, X_train, y_train)\n    assert_warns_message(ChangedBehaviorWarning, pipeline.fit, X_train, y_train)\n    # Check if pipeline.predict and pipeline.score raise an error if fit hasn't been called\n    with assert_raise_message(ValueError, \"Has not been fitted\"):\n        pipeline.predict(X_test)\n    with assert_raise_message(ValueError, \"Has not been fitted\"):\n        pipeline.score(X_test, y_test)\n    # Test if HashingVectorizer can be pickled\n    vectorizer_pickled = pickle.dumps(vectorizer)\n    vectorizer_unpickled = pickle.loads(vectorizer_pickled)\n    assert_array_almost_equal(vectorizer_unpickled.transform(ALL_FOOD_DOCS).toarray(),\n                             vectorizer.transform(ALL_FOOD_DOCS).toarray())\n"], "sample_4": ["    def cosmo(self):\n        return Cosmology(Om0=0.3, H0=67.3, Ob0=0.05)\n", "    def cosmo(self):\n        return Cosmology(H0=70 * u.km / (u.s * u.Mpc), Om0=0.3)\n", "    def test_write_html_table_latex_names_default(self, cls):\n        # Create a cosmology with latex names\n        cosmo = cls(H0=67.4 * u.km / (u.s * u.Mpc), Om0=0.31)\n        # Write the cosmology to an HTML table with latex names\n        write_html_table(cosmo, 'test_table.html')\n        # Read the cosmology from the HTML table\n        cosmo_read = read_html_table('test_table.html')\n        # Check that the cosmology parameters are the same\n        assert cosmo_read.H0 == cosmo.H0\n        assert cosmo_read.Om0 == cosmo.Om0\n", "    def test_read_html_table_latex_names_default(self):\n        table = QTable({'H0': [1], '$$H_{0}$$': [1]})\n        table.rename_column('$$H_{0}$$', 'H0')\n        with u.add_enabled_units(cu):\n            table = read_html_table(table, latex_names=True)\n            self.assertEqual(table.H0, 1)\n            self.assertEqual(table['$$H_{0}$$'], None)\n", "    def test_write_html_table_latex_names(self, latex_names, expected_names):\n        # Create a QTable with latex column names\n        table = QTable()\n        for key, value in expected_names.items():\n            table.add_column(key, [value])\n        # Create a Cosmology instance with a None cosmology and a non-default class\n        cosmo = Cosmology(None, class_='MyCosmology')\n        # Write the table to a QTable\n        table = to_table(cosmo, cls=QTable)\n        # Write the table to an html file with latex_names\n        write_html_table(cosmo, \"test.html\", latex_names=latex_names)\n        # Check the html file is written correctly\n        with open(\"test.html\", 'r') as f:\n            html = f.read()\n            assert f'$${expected_names[\"H0\"]}$$' in html\n            assert f'$${expected_names[\"Om0\"]}$$' in html\n            assert f'$${expected_names[\"Ode0\"]}$$' in html\n            assert f'$${expected_names[\"Tcmb0\"]}$$' in html\n            assert f'$${expected_names[\"Neff\"]}$$' in html\n            assert f'$${expected_names[\"m_nu\"]}$$' in html\n            assert f'$${expected_names[\"Ob0\"]}$$' in html\n            assert f'$${expected_names[\"w0\"]}$$' in html\n            assert f'$${expected_names[\"wa\"]}$$' in html\n            assert f'$${expected_names[\"wz\"]}$$' in html\n            assert f'$${expected_names[\"wp\"]}$$' in html\n            assert f'$${expected_names[\"zp\"]}$$' in html\n        # Remove the file\n        import os\n        os.remove(\"test.html\")\n"], "sample_237": ["    def test_check_user_model_is_anonymous_method(self):\n        with self.assertRaisesRegex(\n            AttributeError,\n            r\"'.*is_anonymous must be an attribute or property rather than a method.*'\",\n        ):\n            check_user_model()\n", "    def test_username_field_not_unique(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        error = errors[0]\n        self.assertEqual(error.id, 'auth.E003')\n", "    def test_username_field_unique_check(self):\n        errors = check_user_model()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], checks.Warning)\n        self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_username_field_unique(self):\n        with self.assertChecksEnabled(check_user_model):\n            errors = check_user_model(app_configs=[self.apps.get_app_config('auth_tests')])\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_check_user_model(self):\n        user_model = self.apps.get_model('auth_tests', 'CustomUserNonUniqueUsername')\n        with self.assertChecksRunningPassed():\n            self.assertCheckMessagesEqual(check_user_model(), [\n                checks.Error(\n                    \"'REQUIRED_FIELDS' must be a list or tuple.\",\n                    obj=user_model,\n                    id='auth.E001',\n                ),\n                checks.Error(\n                    \"The field named as the 'USERNAME_FIELD' for a custom user model \"\n                    \"must not be included in 'REQUIRED_FIELDS'.\",\n                    hint=(\n                        \"The 'USERNAME_FIELD' is currently set to 'username', you \"\n                        \"should remove 'username' from the 'REQUIRED_FIELDS'.\"\n                    ),\n                    obj=user_model,\n                    id='auth.E002',\n                ),\n                checks.Error(\n                    \"'username' must be unique because it is named as the 'USERNAME_FIELD'.\",\n                    obj=user_model,\n                    id='auth.E003',\n                ),\n                checks.Critical(\n                    'CustomUserNonUniqueUsername.is_anonymous must be an attribute or property '\n                    'rather than a method. Ignoring this is a security issue as anonymous '\n                    'users will be treated as authenticated!',\n                    obj=user_model,\n                    id='auth.C009',\n                ),\n                checks.Critical(\n                    'CustomUserNonUniqueUsername.is_authenticated must be an attribute or property '\n                    'rather than a method. Ignoring this is a security issue as anonymous '\n                    'users will be treated as authenticated!',\n                    obj=user_model,\n                    id='auth.C010',\n                ),\n            ])\n"], "sample_211": ["    def test_render_to_response(self):\n        view = AboutTemplateView()\n        response = view.render_to_response({})\n        self.assertIsInstance(response, TemplateResponse)\n", "    def test_get_redirect(self):\n        view = views.RedirectView()\n        response = view.get(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n", "    def test_redirect_view_get_request(self):\n        view = views.RedirectView()\n        response = view.get(self.rf.get('/'))\n        self.assertEqual(response.status_code, 302)\n", "    def test_render_to_response(self):\n        view = AboutTemplateView()\n        response = view.get(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_get_redirect_url(self):\n        view = RedirectView()\n        view.url = '/test-url'\n        request = self.rf.get('/test-path')\n        view.request = request\n        self.assertEqual(view.get_redirect_url(), '/test-url')\n"], "sample_457": ["    def test_deferrable_deferred(self):\n        with self.assertRaises(ValueError):\n            UniqueConstraint(\n                fields=['field1', 'field2'],\n                name='unique_constraint',\n                deferrable=Deferrable.IMMEDIATE,\n            )\n", "    def test_deferrable_constraint_sql(self):\n        constraint = UniqueConstraint(\n            name=\"test_deferrable_constraint\",\n            fields=[\"field1\", \"field2\"],\n            deferrable=Deferrable.DEFERRED,\n        )\n        with self.assertWarns(RemovedInDjango60Warning):\n            with connection.schema_editor(using=\"default\") as schema_editor:\n                self.assertEqual(\n                    constraint.constraint_sql(Product, schema_editor),\n                    schema_editor._unique_sql(\n                        Product,\n                        [Product._meta.get_field(field_name) for field_name in constraint.fields],\n                        constraint.name,\n                        deferrable=Deferrable.DEFERRED,\n                    ),\n                )\n", "    def test_deferrable_unique_constraint(self):\n        with self.assertRaises(ValueError):\n            UniqueConstraint(fields=[\"id\"], deferrable=Deferrable.DEFERRED)\n", "    def test_check_constraint_violation_error_message(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    CheckConstraint(check=Q(field=\"test\"), name=\"check_constraint\"),\n                ]\n\n        model = Model(field=\"test\")\n        model.full_clean()\n        self.assertFalse(model._state.adding)\n        with self.assertRaises(ValidationError):\n            model.field = \"test1\"\n            model.full_clean()\n", "    def test_Deferrable_deprecation_warning(self):\n        with self.assertWarns(RemovedInDjango60Warning):\n            Deferrable.DEFERRED\n"], "sample_594": ["def test_limit_lines():\n    s = dedent(\n        \"\"\"\n        This is a long string\n        that spans multiple lines\n        and is intended to be wrapped.\n        \"\"\"\n    )\n    assert limit_lines(s, limit=10) == dedent(\n        \"\"\"\n        This is a long string\n        ...\n        and is intended to be wrapped.\n        \"\"\"\n    )\n    assert limit_lines(s, limit=1) == \"...\"\n    assert limit_lines(s, limit=0) == \"...\"\n    assert limit_lines(\"Hello world\", limit=10) == \"Hello world\"\n    assert limit_lines(\"\", limit=10) == \"\"\n", "def test_format_items():\n    # Test that format_items works as expected for an empty array\n    assert formatting.format_items(np.array([])) == []\n\n    # Test that format_items works as expected for an array of different types\n    array = np.array([1, 2, \"hello\", np.datetime64(\"2022-01-01\"), np.timedelta64(1)])\n    expected = [\"1\", \"2\", \"'hello'\", \"2022-01-01T00:00:00\", \"1 days\"]\n    assert formatting.format_items(array) == expected\n\n    # Test that format_items works as expected for an array of timedelta64 values\n    array = np.array([np.timedelta64(1), np.timedelta64(2)])\n    expected = [\"1 days\", \"2 days\"]\n    assert formatting.format_items(array) == expected\n\n    # Test that format_items works as expected for an array of datetime64 values\n    array = np.array([np.datetime64(\"2022-01-01\"), np.datetime64(\"2022-01-02\")])\n    expected = [\"2022-01-01T00:00:00\", \"2022-01-02T00:00:00\"]\n    assert formatting.format_items(array) == expected\n\n    # Test that format_items works as expected for an array of floats\n    array = np.array([1.123, 2.456])\n    expected = [\"1.1230\", \"2.4560\"]\n    assert formatting.format_items(array) == expected\n\n    # Test that format_items works as expected for an array of strings\n    array = np.array([\"hello\", \"world\"])\n    expected = [\"'hello'\", \"'world'\"]\n    assert formatting.format_items(array) == expected\n", "    def test_diff_dim_summary(self):\n        # Test that diff_dim_summary returns a string when dimensions are different\n        a = xr.Dataset({\"x\": (\"t\", [1])})\n        b = xr.Dataset({\"x\": (\"t\", [2])})\n        assert diff_dim_summary(a, b) == \"Differing dimensions:\\n    (t) != (t)\"\n        \n        # Test that diff_dim_summary returns an empty string when dimensions are the same\n        a = xr.Dataset({\"x\": (\"t\", [1])})\n        b = xr.Dataset({\"x\": (\"t\", [1])})\n        assert diff_dim_summary(a, b) == \"\"\n", "    def test_format_timestamp(self):\n        # Test that dates outside the 1678-2262 range are formatted as strings\n        date = pd.Timestamp(\"2022-01-01\")\n        assert formatting.format_timestamp(date) == \"2022-01-01\"\n        date = pd.Timestamp(\"2525-01-01\")\n        assert formatting.format_timestamp(date) == \"2525-01-01\"\n\n        # Test that NaT is formatted as a string\n        assert formatting.format_timestamp(pd.NaT) == \"NaT\"\n\n        # Test that datetime objects without timezone info are formatted correctly\n        date = pd.Timestamp(\"2022-01-01 12:00:00\")\n        assert formatting.format_timestamp(date) == \"2022-01-01 12:00:00\"\n", "def test_inline_dask_repr():\n    da = xr.DataArray(np.array([1, 2, 3]))\n    da.dask.config.set(scheduler=\"synchronous\")\n    chunked_da = da.chunk({\"x\": 2})\n    assert inline_dask_repr(chunked_da) == \"dask.array<chunksize=(2,)>\"\n\n    with raises_regex(ValueError):\n        inline_dask_repr(np.array([1, 2, 3]))\n\n    with raises_regex(TypeError):\n        inline_dask_repr(\"string\")\n\n"], "sample_246": ["    def test_add_location_full(self):\n        with captured_stdout() as out:\n            output, po_contents = self._run_makemessages(\n                add_location='full',\n                locale=[LOCALE],\n                verbosity=2,\n            )\n        self.assertIn('#: full/path/to/file.html:123\\n', po_contents)\n        self.assertIn('msgid \"example\"\\n', po_contents)\n", "    def testMsgIdNotQuoted(self):\n        path = 'commands/msgid_not_quoted.py'\n        copytree('commands', path)\n        with open(path, 'w', encoding='utf-8') as fp:\n            fp.write('msgid 42\\n')\n        output, po_contents = self._run_makemessages()\n        self.assertMsgIdPlural('42', po_contents, use_quotes=False)\n", "    def test_postprocess_messages_no_templatization(self):\n        # Create a file with a msgid\n        self.write_file('locale/messages.po', 'msgid \"Test\"\\n')\n        # Run makemessages\n        output, po_contents = self._run_makemessages()\n\n        # Check that postprocess_messages didn't alter the message\n        self.assertEqual(po_contents, 'msgid \"Test\"\\n')\n", "    def testMsgmergeWithAddLocation(self):\n        self._run_makemessages(add_location='full')\n\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n\n        self.assertRegex(po_contents, r'^#: .+:\\d+$', 'File location not found in PO file.')\n", "    def test_extract_template_strings(self):\n        self._run_makemessages(domain='django', extensions=['html'])\n\n        self.assertLocationCommentNotPresent('django.po', 'views.py', 'myapp', 'templates', 'my_template.html')\n        self.assertLocationCommentPresent('django.po', 43, 'myapp', 'templates', 'my_template.html')\n\n        self.assertMsgStr(\"Hello, %s!\", 'django.po')\n\n        self.assertMsgIdPlural(\"Hello, %s!\", 'django.po')\n        self.assertMsgIdPlural(\"Hello, %s!\", 'django.po')\n        self.assertMsgStr(\"Hello, %s!\", 'django.po')\n        self.assertMsgStr(\"Hello, %s!\", 'django.po')\n\n        # Test with a string containing a comment\n        self.assertMsgStr(\"Hello, # foo bar!\", 'django.po')\n\n        # Test with a string containing a quote\n        self.assertMsgStr('Hello, \"foo bar\"!', 'django.po')\n\n        # Test with a string containing a quote\n        self.assertMsgStr(\"Hello, 'foo bar'!\", 'django.po')\n\n        # Test with a msgid containing a quote\n        self.assertMsgId(\"Hello, \\\"foo bar\\\"!\", 'django.po')\n        self.assertMsgId(\"Hello, 'foo bar'!\", 'django.po')\n\n        # Test with a string containing a backslash\n        self.assertMsgStr(\"Hello, foo \\\\ bar!\", 'django.po')\n\n        # Test with a msgid containing a backslash\n        self.assertMsgId(\"Hello, foo \\\\\" \"bar\" \"!\", 'django.po')\n\n        # Test with a string containing a newline\n        self.assertMsgStr(\"Hello,\\nfoo bar!\", 'django.po')\n\n        # Test with a msgid containing a newline\n        self.assertMsgId(\"Hello,\\nfoo bar!\", 'django.po')\n"], "sample_170": ["    def test_cleansed_substitute(self):\n        \"\"\"Test that cleansed_substitute is a string of asterisks\"\"\"\n        filter = SafeExceptionReporterFilter()\n        self.assertIsInstance(filter.cleansed_substitute, str)\n        self.assertEqual(len(filter.cleansed_substitute), 20)  # 20 asterisks\n", "    def test_get_safe_settings(self):\n        reporter = SafeExceptionReporterFilter()\n        settings_dict = reporter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        for key in settings_dict.keys():\n            self.assertIsInstance(settings_dict[key], (str, CallableSettingWrapper))\n", "    def test_technical_500_response_text_html(self):\n        request = RequestFactory().get('/')\n\n        # Create a custom exception reporter that will return a string response\n        class CustomExceptionReporter(ExceptionReporter):\n                return 'custom trace'\n\n                return 'custom text'\n\n        with override_settings(DEFAULT_EXCEPTION_REPORTER='views.CustomExceptionReporter'):\n            response = technical_500_response(request, Exception('Test exception'), None, None)\n            self.assertEqual(response.status_code, 500)\n            self.assertEqual(response.content.decode(), 'custom text')\n", "    def test_is_active_debug_true(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertFalse(filter.is_active(RequestFactory().request))\n", "    def test_is_active(self):\n        # Test default behavior\n        reporter = SafeExceptionReporterFilter()\n        self.assertFalse(reporter.is_active(None))\n        self.assertFalse(reporter.is_active({'DEBUG': False}))\n        self.assertTrue(reporter.is_active({'DEBUG': True}))\n        # Test the setting is taken into account\n        reporter = SafeExceptionReporterFilter()\n        reporter.request = {'sensitive_post_parameters': '__ALL__'}\n        self.assertTrue(reporter.is_active(None))\n        reporter.request = {'sensitive_post_parameters': ['foo']}\n        self.assertFalse(reporter.is_active(None))\n"], "sample_561": ["def test_transformed_marker():\n    marker = markers.MarkerStyle('o')\n    transformed_marker = marker.transformed(Affine2D().scale(2))\n    assert isinstance(transformed_marker, markers.MarkerStyle)\n    assert transformed_marker.get_transform() == Affine2D().scale(2)\n    assert transformed_marker.get_path() == marker.get_path()\n", "def test_rotated_marker():\n    # Create a MarkerStyle instance\n    marker = markers.MarkerStyle('o')\n\n    # Rotate the marker by 45 degrees\n    rotated_marker = marker.rotated(deg=45)\n\n    # Check that the path is correctly rotated\n    assert np.allclose(rotated_marker.get_path().vertices, marker.get_path().vertices @ Affine2D().rotate_deg(45))\n\n    # Check that the transform is correctly applied\n    assert rotated_marker.get_transform().angle == 45\n\n    # Check that the get_alt_path and get_alt_transform methods work correctly\n    assert rotated_marker.get_alt_path() is None\n    assert rotated_marker.get_alt_transform() is None\n", "def test_transformed_marker():\n    marker = markers.MarkerStyle('o')\n    marker_transform = marker.get_transform()\n\n    rotated_marker = marker.transformed(Affine2D().rotate_deg(45))\n    assert not np.allclose(marker_transform.transform, rotated_marker.get_transform().transform)\n\n    scaled_marker = marker.transformed(Affine2D().scale(2))\n    assert not np.allclose(marker_transform.transform, scaled_marker.get_transform().transform)\n\n    rotated_scaled_marker = marker.transformed(Affine2D().scale(2).rotate_deg(45))\n    assert not np.allclose(marker_transform.transform, rotated_scaled_marker.get_transform().transform)\n", "def test_custom_marker():\n    # Create a custom marker path\n    path = Path([[0, 0], [1, 0], [1, 1], [0, 1]])\n    marker = markers.MarkerStyle(marker=path)\n\n    # Create a new figure and axis\n    fig, ax = plt.subplots()\n\n    # Plot a line with the custom marker\n    x = np.array([1, 2, 3])\n    y = np.array([1, 2, 3])\n    ax.plot(x, y, marker=marker)\n\n    # Check that the custom marker is displayed correctly\n    assert np.allclose(marker.get_path().vertices, np.array([[0, 0], [1, 0], [1, 1], [0, 1]]))\n\n    # Check that the custom marker transform is applied correctly\n    marker.transformed(Affine2D().scale(2)).get_path()\n    assert np.allclose(marker.transformed(Affine2D().scale(2)).get_path().vertices, np.array([[0, 0], [2, 0], [2, 2], [0, 2]]))\n\n    # Check that the custom marker can be scaled\n    marker_scaled = marker.scaled(2, 3)\n    assert np.allclose(marker_scaled.get_path().vertices, np.array([[0, 0], [2, 0], [2, 3], [0, 3]]))\n\n    # Check that the custom marker can be rotated\n    marker_rotated = marker.rotated(deg=45)\n    assert np.allclose(marker_rotated.get_path().vertices, np.array([[0, 0], [1, -1], [1, 1], [0, 1]]))\n\n    # Check that the custom marker can be transformed\n    marker_transformed = marker.transformed(Affine2D().scale(2).rotate_deg(45))\n    assert np.allclose(marker_transformed.get_path().vertices, np.array([[0, 0], [2, -2], [2, 2], [0, 2]])\n\n    # Check that the custom marker can be used as a fill style\n    marker_filled = marker.scaled(2, 3).filled()\n    assert np.allclose(marker_filled.get_path().vertices, np.array([[0, 0], [2, 0], [2, 3], [0, ", "def test_transformed_transform():\n    marker = markers.MarkerStyle('o')\n    original_transform = marker.get_transform()\n    transformed_marker = marker.transformed(Affine2D().scale(2.0))\n    assert transformed_marker.get_transform() != original_transform\n    assert np.allclose(transformed_marker.get_transform().matrix,\n                      original_transform.matrix * Affine2D().scale(2.0).matrix)\n    assert transformed_marker.get_path() == marker.get_path()\n    transformed_marker_rotated = transformed_marker.transformed(Affine2D().rotate_deg(45))\n    assert np.allclose(transformed_marker_rotated.get_transform().matrix,\n                      (original_transform * Affine2D().scale(2.0) *\n                       Affine2D().rotate_deg(45)).matrix)\n"], "sample_218": ["    def test_trunc_second_with_timezone(self):\n        now = datetime.now(pytz.utc)\n        dt_model = self.create_model(now, now + timedelta(seconds=1))\n        self.assertEqual(\n            TruncSecond(dt_model.start_datetime).value_from_object(dt_model),\n            truncate_to(now, 'second', pytz.timezone('UTC'))\n        )\n", "    def test_trunc(self):\n        dt = datetime(2022, 2, 15, 10, 30, 45)\n        self.assertEqual(Trunc('start_datetime', 'day')(dt).date(), dt.date())\n        self.assertEqual(Trunc('start_datetime', 'week')(dt).date(), dt.date())\n        self.assertEqual(Trunc('start_datetime', 'month')(dt).date(), dt.date())\n        self.assertEqual(Trunc('start_datetime', 'quarter')(dt).date(), dt.date())\n        self.assertEqual(Trunc('start_datetime', 'year')(dt).date(), dt.date())\n\n        self.assertEqual(Trunc('start_datetime', 'hour')(dt).time(), dt.time())\n        self.assertEqual(Trunc('start_datetime', 'minute')(dt).time(), dt.time())\n        self.assertEqual(Trunc('start_datetime', 'second')(dt).time(), dt.time())\n", "    def test_trunc_time(self):\n        start_datetime = datetime(2022, 3, 16, 10, 30)\n        end_datetime = datetime(2022, 3, 16, 14, 30)\n        dt = self.create_model(start_datetime, end_datetime)\n        result = TruncTime('start_time').resolve_expression(dt)\n        self.assertEqual(result.lhs.output_field, TimeField())\n        self.assertEqual(result.lhs.output_field.name, 'start_time')\n        self.assertEqual(result.lhs.output_field.attname, 'start_time')\n", "    def setUp(self):\n        self.tz = pytz.timezone('US/Eastern')\n        self.date = datetime(2022, 1, 1, tzinfo=self.tz)\n", "    def test_trunc_quarter_with_date_field(self):\n        dt = datetime(2024, 3, 15, tzinfo=datetime_timezone('UTC'))\n        dt_model = self.create_model(dt, None)\n        result = TruncQuarter(dt_model.start_datetime) - dt_model.start_datetime\n        self.assertEqual(result, datetime.timedelta(seconds=0))\n"], "sample_937": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_visit_Name():\n    source = \"import math.sin\"\n    expected = \"math.sin\"\n    assert ast.unparse(ast.parse(source).body[0].body[0].name) == expected\n"], "sample_471": ["    def test_to_python(self):\n        field = DateField(input_formats=[\"%Y-%m-%d\"])\n        self.assertEqual(field.to_python(\"2024-07-26\"), datetime.date(2024, 7, 26))\n", "    def test_empty_value(self):\n        field = CharField(empty_value=\"unknown\")\n        self.assertEqual(field.to_python(\"\"), \"unknown\")\n        self.assertEqual(field.to_python(None), \"unknown\")\n        self.assertEqual(field.to_python(123), \"unknown\")\n        self.assertEqual(field.to_python(\"   \"), \"unknown\")\n        self.assertEqual(field.to_python(\"\"), \"unknown\")\n", "    def test_choice_field_choices_are_iterable(self):\n        class MyForm(forms.Form):\n            choices = ChoiceField(choices=[(\"value1\", \"label1\"), (\"value2\", \"label2\")])\n\n        self.assertTrue(isinstance(MyForm().fields[\"choices\"].choices, list))\n", "    def test_empty_value_with_strip(self):\n        field = CharField(empty_value=\"default_value\", strip=True)\n        self.assertEqual(field.to_python(\"\"), \"default_value\")\n        self.assertEqual(field.to_python(\"   \"), \"default_value\")\n        self.assertEqual(field.to_python(None), \"default_value\")\n", "    def test_max_length_validation(self):\n        field = IntegerField(max_length=5)\n        self.field = field\n        self.assertEqual(field.max_length, 5)\n        self.assertEqual(field.default_validators[2], validators.ProhibitNullCharactersValidator())\n        with self.assertRaises(ValidationError):\n            field.clean('123456')\n"], "sample_1053": ["def test_rational_floor():\n    # Test Rational.floor()\n    assert Rational(1, 2).floor() == 0\n    assert Rational(-1, 2).floor() == -1\n    assert Rational(-2, 2).floor() == -1\n    assert Rational(2, 2).floor() == 1\n    assert Rational(-1, 3).floor() == -1\n    assert Rational(-2, 3).floor() == -1\n    assert Rational(-3, 3).floor() == -1\n    assert Rational(1, 3).floor() == 0\n    assert Rational(2, 3).floor() == 0\n    assert Rational(3, 3).floor() == 1\n\n    # Test Integer.floor()\n    assert Integer(1).floor() == 1\n    assert Integer(-1).floor() == -1\n    assert Integer(-2).floor() == -2\n    assert Integer(2).floor() == 2\n    assert Integer(-3).floor() == -3\n    assert Integer(-4).floor() == -4\n    assert Integer(3).floor() == 3\n    assert Integer(4).floor() == 4\n", "    def test_I_imaginary_unit(self):\n        from sympy import I\n        assert I.is_imaginary\n        assert I.is_finite\n        assert I.is_number\n        assert I.is_algebraic\n        assert I.is_transcendental == False\n        assert I.is_real == False\n        assert I.is_positive == False\n        assert I.is_negative == False\n        assert I.is_irrational == False\n        assert I.is_infinite == False\n        assert I.is_zero == False\n        assert I.is_prime == False\n        assert I.as_base_exp() == (S.NegativeOne, S.Half)\n", "def test_nan():\n    assert nan.is_finite == False\n    assert nan.is_infinite == False\n    assert nan.is_nonnegative == False\n    assert nan.is_nonpositive == True\n    assert nan.is_number == True\n    assert nan.is_real == None\n    assert nan.is_irrational == True\n    assert nan.is_positive == False\n    assert nan.is_negative == True\n    assert nan.is_algebraic == None\n    assert nan.is_transcendental == None\n    assert nan.is_integer == None\n    assert nan.is_rational == None\n    assert nan.is_comparable == False\n    assert nan.is_commutative == True\n    assert nan.is_finite == False\n    assert nan.is_zero == None\n    assert nan.is_prime == None\n", "    def test_evalf_floating_point_input(self):\n        a = Float(0.1, 53)\n        assert same_and_same_prec(a.evalf(25), a)\n", "    def test_floor(self):\n        self.assertEqual(-1.5.floor(), -2)\n        self.assertEqual(-2.5.floor(), -3)\n        self.assertEqual(-0.5.floor(), -1)\n        self.assertEqual(0.floor(), 0)\n        self.assertEqual(0.5.floor(), 0)\n        self.assertEqual(1.5.floor(), 1)\n        self.assertEqual(2.5.floor(), 2)\n        self.assertEqual(5.floor(), 5)\n"], "sample_512": ["def test_pause_without_figure():\n    with pytest.raises(RuntimeError):\n        plt.pause(1)\n", "def test_pause():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3])\n    plt.pause(1)  # pause for 1 second\n    plt.show(block=True)\n    plt.pause(5)  # pause for 5 seconds\n    plt.close()\n", "def test_remove_all_subplots():\n    \"\"\"\n    Test that removing all subplots works correctly and does not crash.\n    \"\"\"\n    # Create a figure with two subplots\n    fig, axs = plt.subplots(2, 1)\n    \n    # Check that removing all subplots removes the current axes\n    assert gca() is not None\n    close('all')\n    assert gca() is None\n\n    # Check that removing all subplots does not crash when there are no subplots\n    close('all')\n    assert gca() is None\n\n    # Check that removing all subplots works when there is a single subplot\n    fig, ax = plt.subplots()\n    close('all')\n    assert gca() is None\n", "def test_close_figure():\n    fig = plt.figure()\n    fig.canvas.manager.window.destroy()  # simulate figure window closure\n    plt.close(fig)\n    assert not plt.fignum_exists(fig.number)\n", "def test_xkcd_context_manager():\n    with plt.xkcd(scale=1, length=100, randomness=2):\n        fig, ax = plt.subplots()\n        ax.plot(np.arange(0, 5, 0.1), np.sin(np.arange(0, 5, 0.1)))\n        ax.set_xlabel('Time [s]')\n        ax.set_ylabel('Amplitude')\n        ax.set_title('Sine wave with xkcd style')\n    plt.close()\n"], "sample_227": ["    def get_queryset(self, request):\n        return super().get_queryset(request).filter(year__gte=1900)\n", "    def test_list_filter(self):\n        bookmark1 = Bookmark.objects.create(tag='test_tag')\n        bookmark2 = Bookmark.objects.create(tag='test_tag')\n        admin = BookmarkAdminGenericRelation(self.request_factory.get('/admin/'))\n        admin.list_filter = ['tags']\n        with self.assertRaises(ValueError):\n            admin.get_queryset(self.request_factory.get('/admin/'))\n", "    def test_empty_filter_no_instance(self):\n        admin = BookmarkAdminGenericRelation(self.request_factory.get(url='/admin/'))\n        queryset = admin.get_queryset(self.request_factory.get(url='/admin/'))\n        queryset_list_filter = self.request_factory.get(url='/admin/').META['QUERY_STRING']\n        self.assertEqual(list(admin.list_filter[0].queryset(None, queryset).query), [])\n", "    def lookups(self, request, model_admin):\n        return (\n            ('the 80s', \"the 1980's\"),\n            ('the 90s', \"the 1990's\"),\n        )\n", "    def test_DecadeListFilterParameterEndsWithDoubleUnderscore(self):\n        \"\"\"Test that a parameter ending with '__' and double '__' in the middle is rejected.\"\"\"\n        admin = DecadeFilterBookAdminParameterEndsWith__In\n        with self.assertRaises(ImproperlyConfigured):\n            admin.list_filter = (DecadeListFilterParameterEndsWith__In, 'parameter____in')\n"], "sample_1093": ["def test_SymPyPrinter_Pow():\n    from sympy import symbols, sqrt\n    x = symbols('x')\n    printer = SymPyPrinter()\n    assert printer.doprint(x**2) == 'x**2'\n    assert printer.doprint(x**Rational(1, 2)) == 'x**(1/2)'\n    assert printer.doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert printer.doprint(x**-Rational(1, 2)) == '1/sympy.sqrt(x)'\n    assert printer.doprint(x**-2) == '1/x**2'\n", "def test_PythonCodePrinter_print_SparseMatrix():\n    from sympy import Matrix, MatrixBase, zeros\n    from sympy.codegen import Declaration, Assignment\n    from sympy.codegen.ast import none\n    A = zeros(2, 3)\n    B = Matrix([[1, 2], [3, 4]])\n    A[0, 0] = 1\n    A[1, 1] = 2\n    expr = SparseMatrix(A)\n    code = PythonCodePrinter({'standard':'python3'}).doprint(expr)\n    expected = \"numpy.array([[0, 0, 0], [0, 2, 0]])\"\n    assert code == expected\n", "def test_AbstractPythonCodePrinter_modules():\n    printer = AbstractPythonCodePrinter({'fully_qualified_modules': False})\n    assert printer.modules == set()\n    assert printer.modules == set()\n    printer._declare_module('numpy')\n    assert 'numpy' in printer.modules\n", "def test_PycodePrinter__print_Relational():\n    from sympy import Ne\n    assert PycodePrinter().__print_Relational(Eq(x, y)) == \"(x == y)\"\n    assert PycodePrinter().__print_Relational(Ne(x, y)) == \"(x != y)\"\n    assert PycodePrinter().__print_Relational(x < y) == \"(x < y)\"\n    assert PycodePrinter().__print_Relational(x <= y) == \"(x <= y)\"\n    assert PycodePrinter().__print_Relational(x > y) == \"(x > y)\"\n    assert PycodePrinter().__print_Relational(x >= y) == \"(x >= y)\"\n", "def test_python_code_block():\n    # Test that PythonCodePrinter can handle block matrix printing correctly\n    from sympy import zeros, eye\n    from sympy.printing.pycode import PythonCodePrinter\n    printer = PythonCodePrinter({'standard': 'python3'})\n    mat = eye(3)\n    code = printer._print_BlockMatrix(mat)\n    assert code == 'numpy.block([[1, 0, 0], [0, 1, 0], [0, 0, 1]])'\n\n    # Test that PythonCodePrinter can handle printing of 'None' correctly\n    code = printer._print_NoneToken(None)\n    assert code == 'None'\n\n    # Test that PythonCodePrinter can handle printing of matrices with negative dimensions correctly\n    mat = zeros((5, -3))\n    with raises(IndexError):\n        printer._print_BlockMatrix(mat)\n\n    # Test that PythonCodePrinter can handle printing of matrices with non-integer dimensions correctly\n    mat = zeros((5, 3.5))\n    with raises(ValueError):\n        printer._print_BlockMatrix(mat)\n\n    # Test that PythonCodePrinter can handle printing of codegen arrays correctly\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    arr = CodegenArrayTensorProduct([x, y, z], [[0, 1], [2, 3]])\n    code = printer._print_CodegenArrayTensorProduct(arr)\n    assert code == 'numpy.einsum(\"ij,jk->ik\", x, y, z)'\n\n    # Test that PythonCodePrinter can handle printing of CodegenArrayTensorProduct with subranks\n    arr = CodegenArrayTensorProduct([x, y, z], [[0, 1], [2, 3]], subranks=[2, 2])\n    code = printer._print_CodegenArrayTensorProduct(arr)\n    assert code == 'numpy.einsum(\"ij,jk->ik\", x, y, z)'\n"], "sample_116": ["    def test_make_template_fragment_key_without_vary_on(self):\n        expected_key = TEMPLATE_FRAGMENT_KEY_TEMPLATE % ('template_fragment', hashlib.md5().hexdigest())\n        self.assertEqual(make_template_fragment_key('template_fragment'), expected_key)\n", "    def test_make_template_fragment_key_hashing(self):\n        fragment_name = 'template_fragment'\n        hasher = hashlib.md5()\n        hasher.update(b'template_fragment')\n        expected_key = TEMPLATE_FRAGMENT_KEY_TEMPLATE % (fragment_name, hasher.hexdigest())\n        self.assertEqual(make_template_fragment_key(fragment_name), expected_key)\n", "    def test_make_template_fragment_key_with_no_vary_on(self):\n        # Tests make_template_fragment_key with vary_on=None\n        fragment_name = 'template_fragment'\n        expected_key = TEMPLATE_FRAGMENT_KEY_TEMPLATE % (fragment_name, hashlib.md5().hexdigest())\n        self.assertEqual(make_template_fragment_key(fragment_name, vary_on=None), expected_key)\n", "    def test_make_template_fragment_key_with_no_vary_on(self):\n        self.assertEqual(make_template_fragment_key('fragment_name'), 'template.cache.fragment_name.72d32a942d3c15b6f16f1a67b4b52a6f')\n", "    def test_make_template_fragment_key_with_no_vary_on(self):\n        expected_key = make_template_fragment_key('test_fragment')\n        self.assertEqual(expected_key, 'template.cache.test_fragment.64e86f4b6e693c1d7e5f0d2a6e4269e5')\n"], "sample_977": ["def test_mcode_sum():\n    # Test _print_Sum method\n    s = Sum(x**i for i in range(1, 5))\n    expected_output = \"Hold[Sum[x^1 + x^2 + x^3 + x^4, x]]\"\n    assert mcode(s) == expected_output\n\n    # Test _print_Sum method with multiple variables\n    s = Sum(x**i*y**j for i in range(1, 5) for j in range(1, 3))\n    expected_output = \"Hold[Sum[x^i*y^j, {i, 1, 4}, {j, 1, 2}]]\"\n    assert mcode(s) == expected_output\n\n    # Test _print_Sum method with multiple variables and limits\n    s = Sum(x**i*y**j, (x, 0, 1), (y, 0, 2))\n    expected_output = \"Hold[Sum[x^i*y^j, {i, 0, 1}, {y, 0, 2}]]\"\n    assert mcode(s) == expected_output\n", "def test_printing_Mul_with_non_constant_factors():\n    x, y = symbols('x,y')\n    assert mathematica_code(x**2 + 2*x + 1) == \"x^2 + 2*x + 1\"\n    assert mathematica_code(3*x + 2*y) == \"3*x + 2*y\"\n    assert mathematica_code(2*x*(x + 1)) == \"2*x*(x + 1)\"\n    assert mathematica_code(x**2*(x + 1)) == \"x^2*(x + 1)\"\n    assert mathematica_code(2*x**2*(x + 1)) == \"2*x^2*(x + 1)\"\n", "def test_mathematica_code_with_function_definitions():\n    from sympy import mathematica_code as mcode\n    x = symbols('x')\n    f = Function('f')\n    f_code = mcode(f(x))\n    assert f_code == \"f[x]\"\n    \n    # Test custom function definition\n    custom_functions = {\n        'f': [(lambda x: True, \"MyFunction\")],\n        'g': [(lambda x: True, \"MyOtherFunction\")]\n    }\n    mcode_settings = {\n        'user_functions': custom_functions\n    }\n    g_code = mcode(f(x), **mcode_settings)\n    assert g_code == \"MyFunction[x]\"\n", "def test_print_Pow():\n    assert mcode(x**2) == 'x^2'\n    assert mcode(x**-2) == '1/x^2'\n", "def test_mcode_Tanh_expansion():\n    x = symbols('x')\n    # Test tanh expansion\n    mcode(tanh(x).series(x))\n    # Test asech expansion\n    mcode(asech(x).series(x))\n    # Test acsch expansion\n    mcode(acsch(x).series(x))\n    # Test asinh expansion\n    mcode(asinh(x).series(x))\n    # Test acosh expansion\n    mcode(acosh(x).series(x))\n    # Test atanh expansion\n    mcode(atanh(x).series(x))\n"], "sample_147": ["    def setUp(self):\n        Number.objects.all().delete()\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "    def test_get_or_create_with_auto_primary_key(self):\n        obj = Number.objects.get_or_create(num=5, other_num=5)[0]\n        self.assertEqual(obj.num, 5)\n        self.assertEqual(obj.other_num, 5)\n        self.assertIsNotNone(obj.pk)\n", "    def setUpTestData(cls):\n        from .models import Author, Book\n        Book.objects.bulk_create([\n            Book(title=f'Book {i}', author_id=i) for i in range(10)\n        ])\n        Author.objects.bulk_create([Author(name=f'Author {i}') for i in range(10)])\n", "    def setUpTestData(cls):\n        cls.parent = Parent.objects.bulk_create([Parent(id=i) for i in range(5)])\n        cls.child = Child.objects.bulk_create([Child(id=i, parent_id=i) for i in range(5)])\n"], "sample_215": ["    def test_get_traceback_data(self):\n        reporter = ExceptionReporter(RequestFactory().request, None, None, None)\n        data = reporter.get_traceback_data()\n        self.assertIn('is_email', data)\n        self.assertIn('unicode_hint', data)\n        self.assertIn('frames', data)\n        self.assertIn('request', data)\n        self.assertIn('request_meta', data)\n        self.assertIn('user_str', data)\n        self.assertIn('filtered_POST_items', data)\n        self.assertIn('settings', data)\n        self.assertIn('sys_executable', data)\n        self.assertIn('sys_version_info', data)\n        self.assertIn('server_time', data)\n        self.assertIn('django_version_info', data)\n        self.assertIn('sys_path', data)\n        self.assertIn('template_info', data)\n        self.assertIn('template_does_not_exist', data)\n        self.assertIn('postmortem', data)\n", "    def setUp(self):\n        self.request = RequestFactory().get('/url/')\n        self.exception = Http404()\n", "    def test_get_safe_settings(self):\n        # Test that SafeExceptionReporterFilter doesn't modify non-sensitive settings\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        self.assertEqual(settings_dict['SETTINGS'], 'value')\n        self.assertEqual(settings_dict['SENSITIVE_SETTING'], 'value')\n\n        # Test that SafeExceptionReporterFilter replaces sensitive settings with stars\n        reporter_filter = SafeExceptionReporterFilter()\n        settings_dict = reporter_filter.get_safe_settings()\n        self.assertEqual(settings_dict['SENSITIVE_SETTING'], '*********')\n", "    def test_get_safe_settings(self):\n        reporter = SafeExceptionReporterFilter()\n        settings_dict = reporter.get_safe_settings()\n        self.assertEqual(settings_dict, settings.__dict__)\n        self.assertIsInstance(settings_dict['DEBUG'], CallableSettingWrapper)\n        self.assertIsInstance(settings_dict['MEDIA_ROOT'], CallableSettingWrapper)\n", "    def test_get_safe_settings(self):\n        settings_dict = SafeExceptionReporterFilter().get_safe_settings()\n        self.assertIn('DEBUG', settings_dict)\n        self.assertEqual(settings_dict['DEBUG'], settings.DEBUG)\n"], "sample_1051": ["def test_styleof_example():\n    from sympy.printing.dot import styleof\n    from sympy import Symbol, Basic, Expr\n    styles = [(Basic, {'color': 'blue', 'shape': 'ellipse'}),\n              (Expr,  {'color': 'black'})]\n    basic = Basic(1)\n    expr = Symbol('x')\n    assert styleof(basic, styles) == {'color': 'blue', 'shape': 'ellipse'}\n    assert styleof(expr, styles) == {'color': 'black', 'shape': 'ellipse'}\n", "def test_dotprint_graphstyle_update():\n    style = {}\n    graphstyle = _graphstyle.copy()\n    graphstyle.update(style)\n    graphstyle.update({'rankdir': 'LR', 'ordering': 'TB'})\n    assert dotprint(x+2, labelfunc=str, **graphstyle) == graphprint(x+2, labelfunc=str)\n", "def test_dotnode_with_float():\n    from sympy import Float\n    assert dotnode(Float(3.14)) == '\"Float(3.14)\" [\"color\"=\"black\", \"label\"=\"3.14\", \"shape\"=\"ellipse\"];'\n    assert dotnode(Float(3.14), styles=[(Float, {'color': 'red'})]) == '\"Float(3.14)\" [\"color\"=\"red\", \"label\"=\"3.14\", \"shape\"=\"ellipse\"];'\n", "def test_dotprint_styles_with_kwargs():\n    from sympy.printing.dot import dotprint\n    from sympy.abc import x\n    styles = [(Basic, {'color': 'blue', 'shape': 'ellipse'}),\n              (Expr,  {'color': 'black'})]\n    expected = \"\"\"digraph{\n    # Graph style\n    \"ordering\"=\"out\"\n    \"rankdir\"=\"TD\"\n    \"fontsize\"=\"20\"\n    <BLANKLINE>\n    #########\n    # Nodes #\n    #########\n    <BLANKLINE>\n    \"Add(Integer(2), Symbol('x'))_()\" [\"color\"=\"black\", \"label\"=\"Add\", \"shape\"=\"ellipse\"];\n    \"Integer(2)_(0,)\" [\"color\"=\"black\", \"label\"=\"2\", \"shape\"=\"ellipse\"];\n    \"Symbol('x')_(1,)\" [\"color\"=\"black\", \"label\"=\"x\", \"shape\"=\"ellipse\"];\n    <BLANKLINE>\n    #########\n    # Edges #\n    #########\n    <BLANKLINE>\n    \"Add(Integer(2), Symbol('x'))_()\" -> \"Integer(2)_(0,)\";\n    \"Add(Integer(2), Symbol('x'))_()\" -> \"Symbol('x')_(1,)\";\n    }\n    \"\"\"\n    assert dotprint(x + 2, styles=styles, kwargs={'fontsize': 20}) == expected\n", "def test_styleof_overrides_default_styles():\n    # Test that styleof overrides default styles\n    x = Symbol('x')\n    expr = x + 2\n    style = styleof(expr, default_styles + [(Symbol, {'color': 'red', 'shape': 'box'})])\n    assert style == {'color': 'red', 'shape': 'box', 'label': 'Add'}\n"], "sample_380": ["    def test_Aggregate_str_repr(self):\n        # Test Aggregate's __str__ method\n        aggregate = Aggregate('expression')\n        self.assertIn('expression', str(aggregate))\n        self.assertIn('Aggregate', str(aggregate))\n\n        # Test Aggregate's __repr__ method\n        self.assertIn('expression', repr(aggregate))\n        self.assertIn('Aggregate', repr(aggregate))\n", "    def test_Aggregate_subclass_repr(self):\n        query = Aggregate()\n        self.assertRegex(str(query), r'<Aggregate: Aggregate\\(.*?\\)>')\n\n        query = Avg(F('field'))\n        self.assertRegex(str(query), r'<Avg: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'AVG'})\n\n        query = Count('field')\n        self.assertRegex(str(query), r'<Count: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'COUNT'})\n\n        query = Min('field')\n        self.assertRegex(str(query), r'<Min: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'MIN'})\n\n        query = Max('field')\n        self.assertRegex(str(query), r'<Max: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'MAX'})\n\n        query = Sum('field')\n        self.assertRegex(str(query), r'<Sum: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'SUM'})\n\n        query = StdDev('field')\n        self.assertRegex(str(query), r'<StdDev: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'STDDEV_SAMP', 'sample': True})\n\n        query = Variance('field')\n        self.assertRegex(str(query), r'<Variance: field>')\n        self.assertEqual(query._get_repr_options(), {'function': 'VAR_SAMP', 'sample': True})\n", "    def test_Aggregate_init(self):\n        # Test aggregate function with no filter\n        aggregate = Aggregate(expression=F('id'), distinct=True)\n        self.assertTrue(aggregate.distinct)\n        self.assertIsNone(aggregate.filter)\n\n        # Test aggregate function with filter\n        filter_expr = Q(id__gt=0)\n        aggregate = Aggregate(expression=F('id'), filter=filter_expr, distinct=True)\n        self.assertTrue(aggregate.distinct)\n        self.assertEqual(aggregate.filter, filter_expr)\n\n        # Test aggregate function with default\n        aggregate = Aggregate(expression=F('id'), default=0)\n        self.assertIsNone(aggregate.filter)\n        self.assertEqual(aggregate.default, 0)\n\n        # Test aggregate function with invalid default\n        with self.assertRaises(TypeError):\n            Aggregate(expression=F('id'), default=0, empty_result_set_value='hello')\n", "    def test_Aggregate_distinct(self):\n        # Test that Aggregate allows distinct\n        query = Author.objects.annotate(avg_age=Avg('age'))\n        self.assertEqual(query[0].avg_age, 34)\n        self.assertEqual(query.count(), 9)\n\n        query = Author.objects.annotate(avg_age=Avg('age', distinct=True))\n        self.assertEqual(query[0].avg_age, 34)\n        self.assertEqual(query.count(), 9)\n\n        # Test that Aggregate does not allow distinct if not specified\n        with self.assertRaises(TypeError):\n            Author.objects.annotate(avg_age=Avg('age'))\n", "    def test_Aggregate_resolve_expression(self):\n        b1 = Book.objects.create(name='Test', price=Decimal('10.00'))\n        with self.assertRaises(FieldError):\n            b1.price.avg()\n"], "sample_1083": ["    def test_csch_as_real_imag(self):\n        x = symbols('x')\n        csch_x = csch(x)\n        assert csch_x.as_real_imag(True) == (sinh(x)/cosh(x), 0)\n", "def test_csch__is_extended_real__complex_input():\n    from sympy import I, csch\n    x = Symbol('x', real=True)\n    assert csch(x).is_extended_real\n    assert csch(I).is_extended_real\n", "def test_sech_expansion_term():\n    from sympy.abc import x\n\n    sech_ = sech(x)\n    expansion_term = sech_.expansion_term(4, x, 1/x, -x**2/4)\n\n    assert expansion_term == -17*x**4/96\n", "def test_sinh_atanh():\n    x, y = symbols('x y')\n    assert (sinh(x).as_real_imag(deep=False)[1] == sinh(x)).simplify()\n    assert (sinh(y).as_real_imag(deep=True)[1] == sinh(y)).simplify()\n    raises(ValueError, lambda: sinh(x).as_real_imag(deep=-1))\n    raises(ValueError, lambda: sinh(x).as_real_imag(deep='string'))\n", "def test_acsch_eval():\n    from sympy import asech, pi, sin, cos, sqrt, sinh, cosh, tanh, exp, S\n    from sympy.abc import x, y\n    from sympy import Eq, sin\n\n    assert asech(asech(1)).is_number\n    assert asech(asech(1)).is_real\n\n    assert Eq(asech(asech(2)), 2*pi*S.ImaginaryUnit/3)\n    assert asech(asech(2)).is_real\n\n    assert Eq(asech(asech(exp(1))), S.Zero)\n    assert Eq(asech(asech(exp(-1))), S.Zero)\n    assert asech(asech(exp(1))).is_real\n\n    assert Eq(asech(asech(tanh(1))), 1)\n    assert asech(asech(tanh(1))).is_real\n\n    assert Eq(asech(asech(sinh(1))), S(2))\n    assert asech(asech(sinh(1))).is_real\n\n    assert Eq(asech(asech(cosh(1))), S.Zero)\n    assert asech(asech(cosh(1))).is_real\n\n    assert Eq(asech(asech(x)), 1/asech(x))\n    assert Eq(asech(asech(y)), 1/asech(y))\n\n    assert asech(asech(x + 1)).is_real\n    assert asech(asech(y + 1)).is_real\n\n    assert Eq(asech(asech(x)).diff(x), 1/(x*asech(x)*sqrt(1/x**2 - 1)))\n    assert Eq(asech(asech(y)).diff(y), 1/(y*asech(y)*sqrt(1/y**2 - 1)))\n\n    assert Eq(asech(asech(exp(1))).diff(x), 0)\n    assert Eq(asech(asech(exp(-1))).diff(x), 0)\n\n    assert Eq(asech(asech(tanh(1))).diff(x), 1/(tanh(1)*asech(tanh(1))*sqrt(1/tanh(1)**2 - 1)))\n    assert Eq(asech(asech(sinh(1))).diff(x), 1/(sinh(1)*"], "sample_782": ["def test_ColumnTransformer_sparse_output_as_dense():\n    \"\"\"Ensure that ColumnTransformer returns dense output even if the\n    sparse threshold is set to 0.\"\"\"\n    transformer = make_column_transformer(\n        (StandardScaler(), ['numerical_column']),\n        n_jobs=1,\n        sparse_threshold=0.0\n    )\n    X = np.array([[1, 2], [3, 4]])\n    Xsparse = sparse.csr_matrix([[1, 2], [3, 4]])\n    assert_array_equal(transformer.fit_transform(X).toarray(),\n                      transformer.fit_transform(Xsparse).toarray())\n", "def test_ColumnTransformer_sparse_threshold():\n    X = np.array([[1, 2], [3, 4]])\n    ct = ColumnTransformer([('scaler', StandardScaler(), [0])], sparse_threshold=1.0)\n    X_sparse = sparse.csr_matrix(X)\n    X_sparse[0, 0] = 1e9\n    X_sparse[1, 1] = 1e9\n    ct.fit(X)\n    X_t_sparse = ct.transform(X_sparse)\n    X_t_dense = ct.transform(X)\n    assert_array_equal(X_t_sparse.toarray(), X_t_dense)\n", "def test_ColumnTransformer_transformers_without_output_of_correct_type():\n    X = np.array([[1, 2, 3], [4, 5, 6]])\n    ct = make_column_transformer(\n        (StandardScaler(), [0]),\n        (Normalizer(norm='l1'), slice(1, 2)),\n        (DoubleTrans(), [2])\n    )\n\n    with pytest.raises(ValueError):\n        ct.fit_transform(X)\n", "def test_fit_transform_sparse_output():\n    \"\"\"Tests that ColumnTransformer can handle sparse matrices correctly.\"\"\"\n    X = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n\n    estimator = make_column_transformer(\n        (StandardScaler(), [0, 1], n_jobs=-1), sparse_threshold=0)\n    estimator.fit_transform(X)\n\n    assert estimator.sparse_output_ is True\n\n    X = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    estimator = make_column_transformer(\n        (StandardScaler(), [0, 1], n_jobs=-1), sparse_threshold=0.5)\n    estimator.fit_transform(X)\n\n    assert estimator.sparse_output_ is False\n\n    X = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n    estimator = make_column_transformer(\n        (StandardScaler(), [0, 1], n_jobs=-1), sparse_threshold=0.2)\n    estimator.fit_transform(X)\n\n    assert estimator.sparse_output_ is True\n\n", "def test_ColumnTransformer_sparse_threshold():\n    \"\"\"Test that ColumnTransformer uses sparse_threshold correctly\"\"\"\n    X = sparse.csr_matrix(np.array([[1, 2], [3, 4]]))\n    X_non_sparse = np.array([[1, 2], [3, 4]])\n    transformer = StandardScaler()\n    transformer.fit_transform = lambda X, y=None: sparse.eye(X.shape[0]) if sparse.issparse(X) else X\n    transformer.transform = transformer.fit_transform\n\n    ct = make_column_transformer((('scaler', transformer, [0]),), sparse_threshold=0.1)\n    ct.fit_transform(X)\n    result = ct.transform(X_non_sparse)\n    assert sparse.isspmatrix(result)  # Test that result is a sparse matrix\n\n    ct = make_column_transformer((('scaler', transformer, [0]),), sparse_threshold=0.5)\n    ct.fit_transform(X)\n    result = ct.transform(X_non_sparse)\n    assert not sparse.isspmatrix(result)  # Test that result is a dense matrix\n"], "sample_685": ["    def test_log_cli_level(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.INFO)\n            logger.info(\"info message\")\n            logger.warning(\"warning message\")\n            \"\"\"\n        )\n        result = testdir.runpytest(\n            \"-s\", \"--log-cli-level\", \"WARNING\", \"--junit-xml=live.log.xml\"\n        )\n        result.assert_outcomes(1)\n        assert result.stderr_str() == \"warning message\"\n", "def test_log_auto_indent(caplog):\n    caplog.set_level(logging.INFO, logger=\"root\")\n\n    with caplog.at_level(logging.INFO, logger=\"root\"):\n        logger.info(\"Multiline message\")\n        logger.info(\"This is the second line of a multiline message.\")\n\n    assert len(caplog.records) == 1\n    assert caplog.records[0].message == \"Multiline message\\nThis is the second line of a multiline message.\"\n\n    with caplog.at_level(logging.INFO, logger=\"root\"):\n        logger.info(\"Single line message\")\n\n    assert len(caplog.records) == 2\n    assert caplog.records[1].message == \"Single line message\"\n\n    with caplog.at_level(logging.INFO, logger=\"root\"):\n        logger.info(\"Multiline message with autoindent\")\n        logger.info(\"This is the second line of a multiline message.\")\n\n    assert len(caplog.records) == 2\n    assert caplog.records[1].message.startswith(\n        \"  This is the second line of a multiline message.\"\n    )\n", "def test_log_file_level_set():\n    \"\"\"Tests the log level for the file log handler.\"\"\"\n    testdir = Testdir()\n    testdir.makepyfile(\n        \"\"\"\n            logging.getLogger(__name__).warning(\"this is a warning\")\n            logging.getLogger(__name__).error(\"this is an error\")\n        \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file = test.log\n        log_file_level = WARNING\n        \"\"\"\n    )\n    testdir.runpytest(\"-vv\")\n    testdir.runpytest(\"--log-file\", \"--log-file-level\", \"ERROR\")\n    with open(\"test.log\") as f:\n        content = f.read()\n        assert \"this is an error\" in content\n        assert \"this is a warning\" not in content\n", "def test_catch_logs_with_level(caplog):\n    caplog.handler.setLevel(logging.DEBUG)\n    logger.debug(\"Debug message\")\n    assert len(caplog.records) == 1\n    assert caplog.records[0].levelname == \"DEBUG\"\n    assert caplog.text == \"DEBUG __main__: __main__: Debug message\\n\"\n    assert caplog.records[0].name == \"root\"\n    assert caplog.messages == [\"Debug message\"]\n", "def test_get_log_level_for_setting(tmp_path):\n    # Test that `get_log_level_for_setting` correctly extracts log level from option and ini\n    config = Config({\"rootdir\": str(tmp_path)})\n    assert get_log_level_for_setting(config, \"log_level\") is None\n    assert get_log_level_for_setting(config, \"log_level\", \"log_level\") == logging.INFO\n    config.parse(args=[\"--log-level=INFO\"])\n    assert get_log_level_for_setting(config, \"log_level\") == logging.INFO\n    config._ini[\"log_level\"] = \"WARNING\"\n    assert get_log_level_for_setting(config, \"log_level\") == logging.WARNING\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(config, \"non_existent_log_level\")\n    with pytest.raises(pytest.UsageError):\n        get_log_level_for_setting(config, \"log_level\", \"non_existent_log_level\")\n"], "sample_653": ["def test_get_actual_log_level():\n    # test case for get_actual_log_level function\n    config = pytest.config\n    assert get_actual_log_level(config, \"nonexistent\") is None\n\n    config.setoption(\"log_level\", \"DEBUG\")\n    assert get_actual_log_level(config, \"log_level\") == logging.DEBUG\n\n    config.setoption(\"log_level\", \"NonExistentLogLevel\")\n    with pytest.raises(pytest.UsageError):\n        get_actual_log_level(config, \"log_level\")\n\n    config.setoption(\"log_level\", 5)\n    assert get_actual_log_level(config, \"log_level\") == 5\n\n    config.setoption(\"log_level\", \"123\")\n    with pytest.raises(ValueError):\n        get_actual_log_level(config, \"log_level\")\n\n    config.setoption(\"log_level\", \"DEBUG\")\n    config.setoption(\"log_file_level\", \"WARNING\")\n    assert get_actual_log_level(config, \"log_level\") == logging.DEBUG\n    assert get_actual_log_level(config, \"log_file_level\") == logging.WARNING\n", "def test_get_actual_log_level():\n    # Test that get_actual_log_level returns the correct value for valid logging levels\n    # and that it raises a UsageError for invalid logging levels.\n    config = pytest.config\n    # Test valid logging levels\n    assert get_actual_log_level(config, \"log_level\") == logging.DEBUG\n    assert get_actual_log_level(config, \"log_level\") == logging.INFO\n    assert get_actual_log_level(config, \"log_level\") == logging.WARN\n    assert get_actual_log_level(config, \"log_level\") == logging.ERROR\n    assert get_actual_log_level(config, \"log_level\") == logging.CRITICAL\n\n    # Test invalid logging levels\n    with pytest.raises(pytest.UsageError):\n        get_actual_log_level(config, \"log_level\", \"invalid_level\")\n    with pytest.raises(pytest.UsageError):\n        get_actual_log_level(config, \"log_level\", \"InvalidLevel\")\n\n    # Test that get_actual_log_level returns None when the option is not set\n    assert get_actual_log_level(config) is None\n\n    # Test that get_actual_log_level returns None when the option is set to None\n    assert get_actual_log_level(config, \"log_level\", None) is None\n\n    # Test that get_actual_log_level converts string to integer\n    assert get_actual_log_level(config, \"log_level\", \"DEBUG\") == logging.DEBUG\n    assert get_actual_log_level(config, \"log_level\", \"INFO\") == logging.INFO\n    assert get_actual_log_level(config, \"log_level\", \"WARN\") == logging.WARN\n    assert get_actual_log_level(config, \"log_level\", \"ERROR\") == logging.ERROR\n    assert get_actual_log_level(config, \"log_level\", \"CRITICAL\") == logging.CRITICAL\n\n    # Test that get_actual_log_level raises an error when the level is not found\n    with pytest.raises(ValueError):\n        get_actual_log_level(config, \"log_level\", \"xyz\")\n", "def test_get_actual_log_level():\n    \"\"\"Test that get_actual_log_level() returns None when no option is set.\"\"\"\n    config = pytest.config\n    assert get_actual_log_level(config) is None\n", "def test_get_option_ini():\n    # Test cases from pytest.ini\n    config = pytest.config\n    assert get_option_ini(config, \"log_print\") is None\n    assert get_option_ini(config, \"log_print\") == config.getini(\"log_print\")\n    assert get_option_ini(config, \"log_print\") == config.getoption(\"log_print\")\n    assert get_option_ini(config, \"log_print\", \"default\") == config.getoption(\"log_print\")\n    assert get_option_ini(config, \"log_print\", \"default\") == config.getini(\"log_print\")\n    assert get_option_ini(config, \"non_existent_option\") is None\n    assert get_option_ini(config, \"non_existent_option\", \"default\") == \"default\"\n\n    # Test cases from command-line arguments\n    config = pytest.config\n    assert get_option_ini(config, \"log_print\") is None\n    with pytest.raises(pytest.UsageError):\n        get_option_ini(config, \"log_print\")\n    assert get_option_ini(config, \"log_print\") == config.getoption(\"log_print\")\n    assert get_option_ini(config, \"log_print\", \"default\") == config.getoption(\"log_print\")\n    assert get_option_ini(config, \"log_print\", \"default\") == config.getoption(\"log_print\")\n    assert get_option_ini(config, \"non_existent_option\") is None\n    assert get_option_ini(config, \"non_existent_option\", \"default\") == \"default\"\n\n    # Test case when option is specified multiple times\n    config = pytest.config\n    config.option.log_print = \"false\"\n    assert get_option_ini(config, \"log_print\") == \"false\"\n    assert get_option_ini(config, \"log_print\", \"default\") == \"false\"\n\n    # Test case when option is not specified in the command line or in pytest.ini\n    config = pytest.config\n    config.option.log_print = None\n    assert get_option_ini(config, \"log_print\") is None\n    assert get_option_ini(config, \"log_print\", \"default\") == \"default\"\n", "def test_log_capture_fixture_lifecycle(caplog):\n    \"\"\"Test that log_capture_fixture works as expected.\"\"\"\n    # Test that the fixture does not interfere with the root logger's log level\n    original_log_level = logging.getLogger().level\n    caplog.set_level(logging.INFO)\n    assert logging.getLogger().level == original_log_level\n\n    # Test that the fixture stores and restores the original log levels\n    original_log_level = logging.getLogger().level\n    caplog.set_level(logging.INFO)\n    caplog._finalize()\n    assert logging.getLogger().level == original_log_level\n\n    # Test that the fixture captures log records correctly\n    caplog.clear()\n    logging.getLogger(\"test_logger\").info(\"Test info message\")\n    logging.getLogger(\"test_logger\").warning(\"Test warning message\")\n    assert len(caplog.records) == 2\n    assert caplog.record_tuples == [(\"test_logger\", 20, \"Test info message\")]\n\n    # Test that the fixture captures log records from different phases\n    caplog.clear()\n    with caplog.at_level(logging.INFO):\n        logging.getLogger(\"test_logger\").info(\"Test info message\")\n        logging.getLogger(\"test_logger\").warning(\"Test warning message\")\n    assert len(caplog.records) == 1\n    assert caplog.record_tuples == [(\"test_logger\", 20, \"Test info message\")]\n\n    # Test that the fixture captures log records from different loggers\n    caplog.clear()\n    logging.getLogger(\"test_logger1\").info(\"Test info message 1\")\n    logging.getLogger(\"test_logger2\").info(\"Test info message 2\")\n    assert len(caplog.records) == 2\n    assert caplog.record_tuples == [(\"test_logger1\", 20, \"Test info message 1\"), (\"test_logger2\", 20, \"Test info message 2\")]\n"], "sample_897": ["def test_partial_dependence_display_from_estimator_2way_kind_average(diabetes, clf_diabetes):\n    X, y = diabetes.data, diabetes.target\n    features = [(0, 1)]\n    subsample = 1000\n    pdp = PartialDependenceDisplay.from_estimator(\n        clf_diabetes, X, features, kind=kind, grid_resolution=grid_resolution\n    )\n    pdp.plot()\n    assert pdp.bounding_ax_ is not None\n    assert pdp.axes_.shape == (1, 1)\n    assert pdp.lines_.shape == (1, 1, 1)\n    assert pdp.contours_.shape == (1, 1)\n    assert pdp.deciles_vlines_.shape == (1, 1)\n    assert pdp.deciles_hlines_.shape == (1, 1)\n    assert pdp.contours_[0, 0].levels.size == 8\n    assert pdp.contours_[0, 0].alpha == 0.75\n    assert pdp.deciles_vlines_[0, 0].lines[0].get_visible()\n    assert pdp.deciles_hlines_[0, 0].lines[0].get_visible()\n    assert pdp.bars_.shape == (0,)\n    assert pdp.heatmaps_.shape == (0,)\n", "def test_partial_dependence_display_pred_values_shape(estimator, X, features, expected_shape):\n    pd_display = PartialDependenceDisplay.from_estimator(estimator, X, features)\n    assert pd_display.pd_results[0].grid_values.shape == expected_shape\n", "def test_partial_dependence_display_plot_two_way_categorical():\n    X = np.array([[1, 0, 2], [2, 1, 0], [0, 2, 1]])\n    X = _convert_container(X, dtype=np.int32)\n    X_cat = OneHotEncoder().fit_transform(X).toarray()\n    X_cont = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n    X_cont = _convert_container(X_cont, dtype=np.float64)\n    X = np.concatenate((X_cont, X_cat), axis=1)\n    estimator = GradientBoostingClassifier(n_estimators=10, random_state=1).fit(X, np.zeros(3))\n    display = PartialDependenceDisplay.from_estimator(\n        estimator,\n        X,\n        [(0, 1)],\n        categorical_features=[1],\n        feature_names=[\"feature0\", \"feature1\", \"feature2\"],\n        kind=\"average\",\n    )\n    assert display.kind == \"average\"\n    assert display.pd_results[0].grid_values.shape == (10, 10)\n    display.plot()\n    # Check that a 2-way PDP plot for categorical features is drawn.\n", "def test_from_estimator_multiclass(clf_diabetes, target_idx, expected_target_idx):\n    display = PartialDependenceDisplay.from_estimator(\n        clf_diabetes, diabetes.data, [0], target=target_idx\n    )\n    assert display.target_idx == expected_target_idx\n", "def test_partial_dependence_display_from_estimator_kind_individualize_auto(\n    n_cols, clf_diabetes, monkeypatch"], "sample_1166": ["def test_Monomial_pow():\n    M = Monomial((3, 4, 5))\n    assert M**2 == Monomial((6, 8, 10))\n    assert M**3 == Monomial((9, 12, 15))\n    with raises(ExactQuotientFailed):\n        M**-1\n    M = Monomial((3, 4, 5), gens=[x, y, z])\n    assert M**2 == Monomial((6, 8, 10), gens=[x, y, z])\n", "def test_Monomial_hash():\n    M = Monomial(x**3*y**4*z**5)\n    assert hash(M) == hash((3, 4, 5))\n    assert M == Monomial(x**3*y**4*z**5)\n    assert M == Monomial((3, 4, 5))\n    assert M != Monomial(x**3*y**4*z**6)\n    assert M != Monomial((3, 4, 7))\n", "def test_Monomial_rebuild():\n    M = Monomial((2, 3))\n    M2 = Monomial((4, 5))\n    M3 = M.rebuild((6, 7))\n    assert M3 == Monomial((6, 7))\n    assert M3.gens == M.gens\n", "def test_Monomial_pow():\n    M = Monomial((3, 4, 5), (x, y, z))\n    assert M**2 == Monomial((6, 8, 10), (x, y, z))\n    assert M**-2 == Monomial((0, 0, 0), (x, y, z))\n    assert M**-1 == Monomial((-3, -4, -5), (x, y, z))\n    assert M**0 == Monomial((0, 0, 0), (x, y, z))\n    assert M**1 == Monomial((3, 4, 5), (x, y, z))\n    assert M**10 == Monomial((30, 40, 50), (x, y, z))\n    assert M**-10 == Monomial((0, 0, 0), (x, y, z))\n\n    with raises(NotImplementedError):\n        M**3.14\n", "def test_Monomial_pow():\n    monom = Monomial((3, 4))\n    assert monom**2 == Monomial((6, 8))\n    assert monom**-1 == Monomial((0, 0))\n    assert monom**3 == Monomial((9, 12))\n    assert monom**0 == Monomial((0, 0))\n    assert monom**-2 == Monomial((0, 0))\n\n    with raises(ZeroDivisionError):\n        monom ** (1/2)\n\n    with raises(ValueError):\n        monom ** -2.5\n\n    monom = Monomial((3, 4, 5))\n    assert monom**2 == Monomial((6, 8, 10))\n\n    monom = Monomial()\n    assert monom**0 == Monomial()\n    with raises(ZeroDivisionError):\n        monom ** (1/2)\n    with raises(ValueError):\n        monom ** -2.5\n"], "sample_553": ["def test_html_movie():\n    from matplotlib import rcParams\n    rcParams['animation.html'] = 'html5'\n    rcParams['animation.writer'] = 'ffmpeg'\n\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n    ax.set_xlim(0, 10)\n    ax.set_ylim(-1, 1)\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    ani = animation.FuncAnimation(fig, animate, init_func=init, frames=10)\n    html = ani.to_html5_video()\n    assert html.startswith('<video ')\n\n    # Test with embed limit\n    rcParams['animation.embed_limit'] = 10\n    html = ani.to_html5_video()\n    assert html == 'Video too large to embed.'\n", "def test_movie_writer_no_blit():\n    # Generate an animation without blit\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    ani = animation.FuncAnimation(fig, animate, init_func=init, blit=False)\n    writer = animation.MovieWriter(fps=5)\n\n    # Save the animation without blit\n    with animation.PillowWriter().saving(fig, \"test.mp4\", dpi=100):\n        ani.save(\"test.mp4\", writer=writer)\n\n    # Check if the animation was saved successfully\n    assert os.path.exists(\"test.mp4\")\n\n    # Clean up\n    os.remove(\"test.mp4\")\n", "def test_file_movie_writer(tmp_path):\n    \"\"\"Test that saving a movie file works using the file movie writer.\"\"\"\n    # Create a figure and save a movie file using the file movie writer.\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n    ax.set_xlim(0, 10)\n    ax.set_ylim(-1, 1)\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    # Create a file movie writer and save a movie file.\n    outfile = tmp_path / \"movie.mp4\"\n    writer = animation.MovieWriter(outfile=outfile)\n    anim = animation.FuncAnimation(fig, animate, init_func=init, frames=5,\n                                  blit=False)\n    anim.save(outfile, writer=writer)\n\n    # Check that the movie file was created.\n    assert outfile.exists()\n\n    # Check that the movie file has the correct frame count.\n    # The movie file should have 5 frames (one for each frame of the animation).\n    command = [\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"stream=nb_frames\",\n               \"-of\", \"default=noprint_wrappers=1:nokey=1\", str(outfile)]\n    output = subprocess.check_output(command).decode(\"utf-8\").strip()\n    assert int(output) == 5\n", "def test_mpl_animation_movie_writer_saving_with_blit():\n    \"\"\"\n    Test MovieWriter's saving method with blit turned on.\n    \"\"\"\n    from matplotlib import pyplot as plt\n    from matplotlib import animation\n    from matplotlib import rcParams\n\n    # Change the backend to ensure blitting is supported\n    rcParams['animation.ffmpeg_path'] = 'ffmpeg'\n    rcParams['animation.ffmpeg_args'] = []\n\n    # Create a simple animation\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init, blit=True, interval=50)\n\n    # Change the output filename to something that will be deleted quickly\n    filename = 'test.m4v'\n    # Create a temporary directory to store the movie file\n    with TemporaryDirectory() as tmpdir:\n        Path(tmpdir).joinpath(filename).write_bytes(b'')\n        # Set up a MovieWriter to save the animation\n        writer = animation.MovieWriter(fps=30, codec='h264')\n        # Save the animation\n        with writer.saving(fig, Path(tmpdir).joinpath(filename)):\n            anim._draw_next_frame(anim._framedata[0], blit=True)\n        # Check that grab_frame was called only once\n        assert writer._count == 1\n\n    # Check that the output file exists\n    assert Path(tmpdir).joinpath(filename).exists()\n    # Check that the output file has the correct size\n    assert Path(tmpdir).joinpath(filename).stat().st_size > 0\n", "    def __init__(self, fig):\n        self.fig = fig\n        self.x = 0\n        self.y = 0\n\n        self.ax = fig.add_subplot(111)\n        self.ax.set_xlim(-1, 1)\n        self.ax.set_ylim(-1, 1)\n\n        self.line, = self.ax.plot([], [], 'o')\n\n        self._init()\n"], "sample_650": ["def test_log_capture_fixture_finalizes_logs(caplog):\n    caplog.clear()\n    caplog.set_level(logging.WARNING)\n\n    # Log some records\n    logging.getLogger(\"test_logger\").warning(\"Warning message\")\n    logging.getLogger(\"test_logger\").error(\"Error message\")\n\n    # Check that the fixture finalized logs\n    assert caplog.handler.stream.getvalue() == \"\"\n    assert caplog.text == \"\"\n", "def test_catching_logs(caplog: FixtureRequest) -> None:\n    \"\"\"Ensure catching_logs works.\"\"\"\n    # Arrange\n    caplog.handler.setLevel(logging.INFO)\n    caplog.handler.setFormatter(logging.Formatter(fmt=\"%(levelname)s - %(message)s\"))\n\n    # Act\n    logging.basicConfig(handlers=[caplog.handler])\n    logging.info(\"test\")\n\n    # Assert\n    assert \"INFO - test\" in caplog.text\n", "def test_log_capture_levels():\n    # Test that log levels are correctly set on log capture handler.\n    with Pytester().pytestinstance():\n        caplog_handler = caplog.handler\n        assert caplog_handler.level == logging.INFO\n\n        caplog.set_level(logging.DEBUG, \"my_logger\")\n        assert caplog_handler.level == logging.DEBUG\n\n        caplog.set_level(logging.ERROR, logger=\"my_logger\")\n        assert caplog_handler.level == logging.ERROR\n", "def test_log_capture_fixtures(tmp_path, capsys):\n    # Create a test file\n    test_file = tmp_path / \"test.py\"\n    test_file.write_text(\n        \"\"\"import logging", "def test_logging_plugin_registering():\n    \"\"\"Test that logging plugin is registered.\"\"\"\n    # TODO: Need a way to check if a plugin is registered.\n    # Maybe this could be done in the pytest_configure function.\n    # https://github.com/pytest-dev/pytest/issues/6011\n    assert True\n"], "sample_185": ["    def test_iter_format_modules(self):\n        # Test that iter_format_modules returns the correct modules\n        modules = list(iter_format_modules('de'))\n        self.assertEqual(len(modules), 2)\n        self.assertIn('path.to.custom.formats.de', [module.__name__ for module in modules])\n", "    def test_number_format_with_decimal_pos(self):\n        # Test number_format with decimal position argument\n        value = decimal.Decimal('123.456')\n        self.assertEqual(number_format(value, decimal_pos=1), '12 345.46')\n        self.assertEqual(number_format(value, decimal_pos=0), '123.456')\n        self.assertEqual(number_format(value, decimal_pos=2), '12 345,46')\n", "    def test_reset_format_cache(self):\n        from django.conf import settings\n        lang = 'de'\n        settings.LANGUAGE_CODE = lang\n        initial_cache = _format_cache.copy()\n        reset_format_cache()\n        self.assertNotEqual(initial_cache, _format_cache)\n        self.assertEqual(_format_cache, {})\n", "    def test_get_format_input_formats(self):\n        # Test that get_format returns input formats from format modules\n        # when USE_L10N is True.\n        self.reset_settings({\n            'USE_L10N': True,\n            'FORMAT_MODULE_PATH': ['django.conf.locale.de']\n        })\n        self.assertEqual(get_format('DATE_INPUT_FORMATS'), ['%d.%m.%Y', '%d/%m/%Y', '%d %B %Y'])\n\n        # Test that get_format returns the default input formats when USE_L10N is False.\n        self.reset_settings({\n            'USE_L10N': False,\n            'FORMAT_MODULE_PATH': ['django.conf.locale.de']\n        })\n        self.assertEqual(get_format('DATE_INPUT_FORMATS'), ['%Y-%m-%d'])\n\n        # Test that get_format returns the format from settings when the format is not\n        # found in any format module.\n        self.reset_settings({\n            'USE_L10N': True,\n            'FORMAT_MODULE_PATH': ['django.conf.locale.de'],\n            'DATE_INPUT_FORMATS': ['%Y-%m-%d']\n        })\n        self.assertEqual(get_format('DATE_INPUT_FORMATS'), ['%Y-%m-%d'])\n", "    def test_date_format(self):\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', use_l10n=False)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang=None, use_l10n=False)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', use_l10n=True, lang=None)\n\n        self.assertEqual(get_format('DATE_FORMAT'), 'DATE_FORMAT')\n\n        self.assertEqual(get_format('DATE_FORMAT', 'en-us'), 'DATE_FORMAT')\n\n        self.assertEqual(get_format('DATE_FORMAT', 'en-us', use_l10n=False), 'DATE_FORMAT')\n\n        self.assertEqual(get_format('DATE_FORMAT', use_l10n=False), 'DATE_FORMAT')\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang='en-us', use_l10n=True)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang=None, use_l10n=False, reverse=True)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang=None, use_l10n=False, reverse=False)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang='en-us', use_l10n=True, reverse=True)\n\n        with self.assertRaises(ValueError):\n            get_format('DATE_FORMAT', lang='en-us', use_l10n=True, reverse=False)\n\n        reset_format_cache()\n\n        with self.assertRaises(KeyError):\n            get_format('DATE_FORMAT', lang='en-us')\n\n        get_format('DATE_FORMAT', 'en-us')\n\n        self.assertEqual(get_format('DATE_FORMAT', 'en-us'), get_format('DATE_FORMAT', 'en-us'))\n\n        self.assertEqual(get_format('DATE_FORMAT', lang='en-us'), get_format('DATE_FORMAT', 'en-us'))\n\n        self.assertEqual(get_format('DATE_FORMAT', use_l10n=True), get_format('DATE_FORMAT', 'en-us'))\n\n        self.assertEqual(get_format('DATE_FORMAT', lang='en-us', use_l10n=True), get_format('DATE_FORMAT', 'en-us'))\n\n        get_format('DATE_FORMAT', use_l10n=True)\n\n        self.assertEqual(get_format('DATE_FORMAT'), get_format('DATE_FORMAT'))\n\n        get_format('DATE_FORMAT')\n\n        reset_format_cache()\n\n        self.assertEqual(get_format('DATE_FORMAT'), 'DATE_FORMAT')\n\n        get_format('DATE_FORMAT')\n\n        reset_format_cache()\n\n        get_format('DATE_FORMAT')\n"], "sample_684": ["def test_filter_traceback_entry_from_pluggy():\n    # Test that a TracebackEntry from pluggy is filtered out\n    # from appearing in the traceback.\n    with mock.patch(\"pluggy.__file__\") as fake_pluggy_file:\n        fake_pluggy_file.return_value = \"/path/to/pluggy/__init__.py\"\n        _PLUGGY_DIR = py.path.local(\"/path/to/pluggy\")\n        entry = TracebackEntry(\n            TracebackType(\"/path/to/pluggy/__init__.py\", 0, 0, 1)\n        )\n        assert filter_traceback(entry)\n", "def test_filter_traceback_plugin_exclude():\n    from _pytest._code import filter_traceback\n    from _pytest._code.code import TracebackEntry\n    from _pytest._code.code import filter_traceback\n\n    # Create a fake plugin path\n    plugin_path = py.path.local('pytest_plugin.py')\n\n    # Create a fake traceback entry that points to the plugin file\n    entry = TracebackEntry(\n        rawentry=FrameType.from_frame_data(\n            filename=plugin_path,\n            lineno=1,\n            f_code=Code('lambda: pass').raw\n        ),\n        excinfo=None\n    )\n\n    # Test that the entry is excluded from the traceback\n    assert not filter_traceback(entry)\n", "    def test_init(self):\n        code_obj = Code(rawcode=\"def foo(): pass\")\n        assert code_obj.filename == \"builtins\"\n        assert code_obj.firstlineno == 0\n        assert code_obj.name == \"foo\"\n        assert code_obj.raw.co_filename == \"builtins\"\n        assert code_obj.raw.co_firstlineno == 1\n        assert code_obj.raw.co_name == \"foo\"\n", "    def test_repr(self):\n        tb = TracebackType()\n        entry = TracebackEntry(tb)\n        assert repr(entry) == \"<TracebackEntry %s:%d>\" % (entry.frame.code.path, entry.lineno + 1)\n", "def test_FormattedExcinfo_repr_locals():\n    \"\"\"Test that repr_locals() does not fail when locals contain unhashable values\"\"\"\n    # Using a mock frame to bypass the need to create a real frame\n    frame = Frame(FrameType(1, {}, {}, Code(\"x\"), 1, 2, \"s\", 3, 4, 5))\n    frame.f_locals[\"foo\"] = [1, 2, 3]  # List is unhashable\n    frame.f_locals[\"bar\"] = {\"a\": 1, \"b\": 2}  # Dict is unhashable\n    formatted_excinfo = FormattedExcinfo()\n    repr_locals = formatted_excinfo.repr_locals(frame.f_locals)\n    assert repr_locals is not None\n    assert isinstance(repr_locals, ReprLocals)\n    assert repr_locals.lines  # Ensure the repr_locals instance has a lines attribute\n"], "sample_1113": ["def test_blockmatrix_row_blocksizes_property():\n    M = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n    assert M.rowblocksizes == [l, n]\n", "def test_BlockMatrix_equals():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n    E = MatrixSymbol('E', 2, 2)\n    F = MatrixSymbol('F', 2, 2)\n    \n    block_A = BlockMatrix([[A, B], [C, D]])\n    block_B = BlockMatrix([[A, B], [C, D]])\n    block_C = BlockMatrix([[A, B], [D, C]])\n    block_D = BlockMatrix([[B, A], [C, D]])\n    block_E = BlockMatrix([[A, B, C], [D, E, F]])\n\n    assert block_A.equals(block_B)\n    assert block_A.structurally_equal(block_B)\n    assert not block_A.equals(block_C)\n    assert not block_A.structurally_equal(block_C)\n    assert not block_A.equals(block_D)\n    assert not block_A.structurally_equal(block_D)\n    assert not block_A.structurally_equal(block_E)\n", "def test_BlockDiagMatrix_as_real_imag():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    diag = BlockDiagMatrix(X, Y, Z)\n    real_matrices, im_matrices = diag.as_real_imag()\n    assert real_matrices == BlockDiagMatrix(X, ZeroMatrix(2, 2), ZeroMatrix(2, 2))\n    assert im_matrices == BlockDiagMatrix(ZeroMatrix(2, 2), ZeroMatrix(2, 2), Z)\n", "def test_block_matrix_unevaluated_block():\n    k, l, m, n = symbols('k l m n', integer=True)\n    X1 = MatrixSymbol('X1', m, m)\n    X2 = MatrixSymbol('X2', m, m)\n    X3 = MatrixSymbol('X3', m, m)\n    X4 = MatrixSymbol('X4', m, m)\n\n    B = BlockMatrix([[X1, X2], [X3, X4]])\n    B1 = BlockMatrix([[X1, X2], [X3, X4]])\n\n    assert B.is_BlockMatrix\n    assert B.unevaluated_block(0, 0) == X1\n    assert B.unevaluated_block(1, 0) == X2\n    assert B.unevaluated_block(0, 1) == X3\n    assert B.unevaluated_block(1, 1) == X4\n\n    # Check if accessing a block that is out of bounds returns unevaluated\n    assert B.unevaluated_block(2, 2) == MatrixElement(B, 2, 2)\n\n    # Check if accessing a block with a block that is not a block matrix\n    # returns unevaluated\n    assert B.unevaluated_block(0, 2) == MatrixElement(B, 0, 2)\n", "def test_blockmatrix_creation_with_row_blocksizes_mismatch():\n    A = MatrixSymbol('A', 3, 2)\n    B = MatrixSymbol('B', 2, 3)\n    C = MatrixSymbol('C', 3, 2)\n\n    # Test case: mismatched row block sizes\n    with raises(ValueError):\n        BlockMatrix([[A, B], [C]])\n\n    # Test case: row block sizes are correct, but col block sizes are not\n    D = MatrixSymbol('D', 2, 3)\n    with raises(ValueError):\n        BlockMatrix([[A, D, B]])\n\n    # Test case: block matrix creation with correct row and col block sizes\n    E = MatrixSymbol('E', 2, 3)\n    block_matrix = BlockMatrix([[A, E], [C, B]])\n    assert block_matrix.is_BlockMatrix\n"], "sample_1131": ["def test_print_pow():\n    # Testing Pow with rational exponent in SymPyPrinter\n    printer = SymPyPrinter()\n    assert printer._print(Pow(x, Rational(1, 2))) == 'x**(1/2)'\n    # Testing Pow with non-rational exponent in SymPyPrinter\n    assert printer._print(Pow(x, Rational(1, 3))) == 'x**(1/3)'\n    # Testing Pow with integer exponent in NumPyPrinter\n    printer = NumPyPrinter()\n    assert printer._print(Pow(x, -3)) == 'x**(-3)'\n    # Testing Pow with non-integer exponent in NumPyPrinter\n    assert printer._print(Pow(x, 2.5)) == 'x**(2.5)'\n    # Testing Pow with rational exponent in MpmathPrinter\n    printer = MpmathPrinter()\n    assert printer._print(Pow(x, Rational(1, 2))) == 'mpmath.sqrt(x)'\n    # Testing Pow with non-rational exponent in MpmathPrinter\n    assert printer._print(Pow(x, Rational(1, 3))) == 'x**(1/3)'\n    # Testing Pow with integer exponent in SciPyPrinter\n    printer = SciPyPrinter()\n    assert printer._print(Pow(x, -3)) == 'x**(-3)'\n    # Testing Pow with non-integer exponent in SciPyPrinter\n    assert printer._print(Pow(x, 2.5)) == 'x**(2.5)'\n    # Testing Pow with rational exponent in PythonCodePrinter\n    printer = PythonCodePrinter()\n    assert printer._print(Pow(x, Rational(1, 2))) == 'math.sqrt(x)'\n    # Testing Pow with non-rational exponent in PythonCodePrinter\n    assert printer._print(Pow(x, Rational(1, 3))) == 'x**(1/3)'\n", "    def test_PiecewiseWithDefault(self):\n        piecewise = Piecewise((z, z > 0), (Rational(1, 2), True))\n        assert NumPyPrinter().doprint(piecewise) == 'numpy.select([True, True], [z, 1/2], default=None)'\n", "def test_print_rational_pow():\n    from sympy import sqrt, Rational, Pow, symbols\n    x, y = symbols('x y')\n    assert PythonCodePrinter().doprint(sqrt(x)) == 'math.sqrt(x)'\n    assert MpmathPrinter().doprint(sqrt(x)) == 'mpmath.sqrt(x)'\n    assert NumPyPrinter().doprint(sqrt(x)) == 'numpy.sqrt(x)'\n    assert SciPyPrinter().doprint(sqrt(x)) == 'scipy.special._ufuncs.sqrt(x)'\n    assert SymPyPrinter().doprint(sqrt(x)) == 'sympy.sqrt(x)'\n    assert PythonCodePrinter().doprint(sqrt(x).subs(sqrt(x), Rational(1, 2))) == '1/math.sqrt(x)'\n    assert MpmathPrinter().doprint(sqrt(x).subs(sqrt(x), Rational(1, 2))) == 'mpmath.sqrt(x)*mpmath.sqrt(x)'\n    assert NumPyPrinter().doprint(sqrt(x).subs(sqrt(x), Rational(1, 2))) == 'numpy.sqrt(x)*numpy.sqrt(x)'\n    assert SciPyPrinter().doprint(sqrt(x).subs(sqrt(x), Rational(1, 2))) == 'scipy.special._ufuncs.sqrt(x)*scipy.special._ufuncs.sqrt(x)'\n    assert SymPyPrinter().doprint(sqrt(x).subs(sqrt(x), Rational(1, 2))) == 'sympy.sqrt(x)*sympy.sqrt(x)'\n    assert PythonCodePrinter().doprint(Pow(x, Rational(1, 2))) == 'x**(1/2)'\n    assert MpmathPrinter().doprint(Pow(x, Rational(1, 2))) == 'x**(mpmath.mpf(1)/mpmath.mpf(2))'\n    assert NumPyPrinter().doprint(Pow(x, Rational(1, 2))) == 'numpy.sqrt(x)'\n    assert SciPyPrinter().doprint(Pow(x, Rational(1, 2))) == 'scipy.special._ufuncs.sqrt(x)'\n    assert SymPyPrinter().doprint(Pow(x, Rational(1, 2))) == 'sympy.sqrt(x)'\n", "def test_SparseMatrix():\n    A = SparseMatrix(((1, (0, 1)), (2, (0, 2))))\n    printer = SciPyPrinter()\n    assert printer.doprint(A) == 'scipy.sparse.coo_matrix(([1, 2], ([0, 0], [1, 2])), shape=(3, 3))'\n", "def test_ScipyPrinter():\n    from sympy.testing.pytest import raises\n    from sympy import besselj, sin, S, oo, sqrt, exp, Matrix, Eq, cos\n    from sympy.codegen import Assignment\n    from sympy.printing.pycode import SciPyPrinter\n\n    printer = SciPyPrinter()\n    assert printer.doprint(Eq(besselj(0, sin(1)), cos(1))) == \"scipy.special.jv(0, sin(1)) == cos(1)\"\n    assert printer.doprint(Eq(sqrt(oo), 1)) == \"scipy.special.gammaln(oo) == 1\"\n    assert printer.doprint(Eq(exp(1), 1)) == \"scipy.special.gamma(1) * scipy.special.exp(1) == 1\"\n    assert printer.doprint(Eq(cos(1), sin(1))) == \"scipy.special.gammaln(oo) == scipy.special.gammaln(oo)\"\n    assert printer.doprint(Eq(cos(1), cos(1))) == \"scipy.special.gammaln(oo) == scipy.special.gammaln(oo)\"\n\n    assert printer.doprint(Assignment('x', exp(1))) == \"x = scipy.special.exp(1)\"\n    assert printer.doprint(Assignment('y', oo)) == \"y = scipy.special.gammaln(oo)\"\n    assert printer.doprint(Assignment('z', sqrt(1))) == \"z = math.sqrt(1)\"\n    assert printer.doprint(Assignment('w', sqrt(oo))) == \"w = scipy.special.gammaln(oo)\"\n\n    assert printer.doprint(Matrix([1, 2, 3])) == \"scipy.sparse.coo_matrix([(1, (0, 0)), (2, (0, 1)), (3, (0, 2))], shape=(1, 3))\"\n    assert printer.doprint(Matrix([[1, 2], [3, 4]])) == \"scipy.sparse.coo_matrix([(1, (0, 0)), (2, (0, 1)), (3, (1, 0)), (4, (1, 1))], shape=(2, 2))\"\n\n    assert printer.doprint(MatrixSolve(Matrix([[1, 2], [3, 4]]),"], "sample_290": ["def test_migration_suggest_name_reversible_migration(self):\n    migration = Migration(name='001', app_label='testapp')\n    migration.reversible = True\n    self.assertEqual(migration.suggest_name(), '001')\n", "    def test_operation_types(self):\n        author_initial = ModelState(\"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))])\n        author_added_field = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200)),\n        ])\n        changes = self.get_changes([author_initial], [author_added_field])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RunPython\", \"RunSQL\"])\n", "    def test_mutation_of_project_state(self):\n        project_state = self.make_project_state([self.author_empty, self.author_name])\n        migration = Migration('0001_initial', 'testapp')\n        migration.operations = [\n            operations.CreateModel(\n                name='Author',\n                fields=[\n                    ('id', models.AutoField(primary_key=True)),\n                    ('name', models.CharField(max_length=200)),\n                ]\n            )\n        ]\n        new_state = migration.mutate_state(project_state, preserve=False)\n        self.assertEqual(new_state.models['testapp.Author'].fields, [\n            ('id', models.AutoField(primary_key=True)),\n            ('name', models.CharField(max_length=200)),\n        ])\n", "def test_destructible_object_migration_attributes(self):\n    changes = {}\n    for state in [\n        self.author_name_deconstructible_1,\n        self.author_name_deconstructible_2,\n        self.author_name_deconstructible_3,\n        self.author_name_deconstructible_4,\n        self.author_name_deconstructible_list_1,\n        self.author_name_deconstructible_list_2,\n        self.author_name_deconstructible_list_3,\n        self.author_name_deconstructible_tuple_1,\n        self.author_name_deconstructible_tuple_2,\n        self.author_name_deconstructible_tuple_3,\n        self.author_name_deconstructible_dict_1,\n        self.author_name_deconstructible_dict_2,\n        self.author_name_deconstructible_dict_3,\n        self.author_name_nested_deconstructible_1,\n        self.author_name_nested_deconstructible_2,\n        self.author_name_nested_deconstructible_changed_arg,\n        self.author_name_nested_deconstructible_extra_arg,\n        self.author_name_nested_deconstructible_changed_kwarg,\n        self.author_name_nested_deconstructible_extra_kwarg,\n    ]:\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, defaults=state.model_state.model.__dict__, field_defaults=state.model_state.model.__dict__)\n        changes = {}\n    for state in [\n        self.author_name_deconstructible_list_1,\n        self.author_name_deconstructible_list_2,\n        self.author_name_deconstructible_list_3,\n        self.author_name_deconstructible_tuple_1,\n        self.author_name_deconstructible_tuple_2,\n        self.author_name_deconstructible_tuple_3,\n        self.author_name_deconstructible_dict_1,\n        self.author_name_deconstructible_dict_2,\n        self.author_name_deconstructible_dict_3,\n    ]:\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default_value=state.model_state.model.__dict__['name'])\n        changes = {}\n", "    def test_reversible_operations_mutate_state(self):\n        migration = Migration('0001_initial', 'testapp')\n        project_state = self.make_project_state([self.author_empty])\n        operation = mock.Mock(spec=models.AddField, reversible=True)\n        operation.state_forwards = mock.Mock(return_value=None)\n        operation.database_forwards = mock.Mock(return_value=None)\n        operation.database_backwards = mock.Mock(return_value=None)\n        migration.operations = [operation]\n        new_state = migration.mutate_state(project_state, preserve=False)\n        self.assertEqual(new_state, project_state)\n"], "sample_475": ["    def test_exclude_is_none(self):\n        class ValidationTestModelAdmin(admin.ModelAdmin):\n            exclude = None\n\n        self.assertIsValid(ValidationTestModelAdmin, ValidationTestModel)\n", "    def test_prepopulated_fields_key_valid(self):\n        admin = admin.ModelAdmin()\n        admin.prepopulated_fields = {\"name\": \"full_name\"}\n        self.assertEqual(admin.check(), [])\n", "    def test_fieldsets_item_is_a_list_or_tuple(self):\n        class InvalidAdmin(admin.ModelAdmin):\n            fieldsets = [[1, 2]]\n\n        self.assertIsInvalid(\n            InvalidAdmin,\n            Song,\n            \"'fieldsets' must be a list or tuple.\",\n            id=\"admin.E008\",\n        )\n", "    def test_prepopulated_fields_with_duplicate_fields(self):\n        model_admin = admin.ModelAdmin\n        model = ValidationTestModel\n        msg = \"The value of 'prepopulated_fields' contains duplicate field(s).\"\n        id = \"admin.E015\"\n        self.assertIsInvalid(model_admin, model, msg, id)\n", "    def test_prepopulated_fields_allow_allowed_field_types(self):\n        class TestModelAdmin(admin.ModelAdmin):\n            model = ValidationTestModel\n            prepopulated_fields = {\n                \"title\": (\"artist\",)\n            }\n\n        self.assertIsValid(TestModelAdmin, ValidationTestModel)\n"], "sample_1112": ["def test_digits_with_zero_input():\n    # Test that the function raises an error when the input is 0\n    raises(ValueError, lambda: digits(0))\n", "def test_digits_negative_input():\n    \"\"\"Test that the digits function handles negative input correctly.\"\"\"\n    # Test that digits returns the correct result for a negative number\n    assert digits(-123) == [-10, 2, 3]\n    # Test that digits returns the correct result for a negative number in a specific base\n    assert digits(-123, b=2) == [-10, 1, 1, 1, 1, 1]\n    # Test that digits raises an error when the base is not greater than 1\n    with raises(ValueError):\n        digits(-123, b=1)\n    # Test that digits returns the correct result with a specified number of digits\n    assert digits(-123, digits=3) == [-10, 0, 0, 2, 3]\n", "def test_digits_negative_base():\n    # Test that digits() handles negative bases correctly\n    with raises(ValueError):\n        digits(10, -5)\n    with raises(ValueError):\n        digits(10, 1)\n", "def test_count_digits_zero():\n    assert count_digits(0) == {0: 1}\n    assert count_digits(0, 2) == {0: 1}\n    assert count_digits(0, 16) == {0: 1}\n    ", "def test_digits_with_large_b():\n    # Test with large base\n    assert digits(35, b=100) == [100, 3, 5]\n"], "sample_733": ["def test_TfidfVectorizer_fit_transform_equal_to_fit_and_transform():\n    \"\"\"Test TfidfVectorizer.fit_transform() is equivalent to\n    TfidfVectorizer.fit() followed by TfidfVectorizer.transform()\"\"\"\n    vectorizer = TfidfVectorizer()\n    docs = [\"the pizza\", \"the salad\", \"the burger\"]\n    expected_X = vectorizer.fit_transform(docs).toarray()\n    vectorizer.fit(docs)\n    X = vectorizer.transform(docs).toarray()\n    assert_array_equal(expected_X, X)\n", "def test_hashing_vectorizer_dtype():\n    X = HashingVectorizer(dtype=np.float32).fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.float32\n\n    with pytest.raises(TypeError):\n        HashingVectorizer(dtype='int')\n\n    with pytest.raises(TypeError):\n        HashingVectorizer(dtype=np.object)\n\n    # test with 64bit integers\n    X = HashingVectorizer(dtype=np.int64).fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.int64\n\n    # test that the non-negative flag has no effect\n    # on the dtype of the output\n    X = HashingVectorizer(non_negative=True).fit_transform(JUNK_FOOD_DOCS)\n    assert X.dtype == np.int32\n", "def test_hashing_vectorizer():\n    X = HashingVectorizer().fit_transform([\"Hello world\"])\n    assert X.shape == (1, 2**18)  # or some other fixed size\n", "def test_transformer_mixin_fit_transform():\n    from sklearn.feature_extraction.text import VectorizerMixin\n    vectorizer = VectorizerMixin()\n    assert_equal(vectorizer.fit_transform([]), None)\n    assert_equal(vectorizer.partial_fit([]), vectorizer)\n    assert_raises(ValueError, vectorizer.fit, \"a string\")\n    assert_raises(ValueError, vectorizer.partial_fit, \"a string\")\n", "def test_hashing_vectorizer_ngram_range():\n    \"\"\"Test HashingVectorizer with ngram_range parameter.\"\"\"\n    vectorizer = HashingVectorizer(ngram_range=(2, 3))\n    assert vectorizer.ngram_range == (2, 3)\n    vectorizer = HashingVectorizer(ngram_range=[2, 3])\n    assert vectorizer.ngram_range == (2, 3)\n\n    # Test ValueError when min_n > max_n\n    with assert_raises(ValueError):\n        HashingVectorizer(ngram_range=(3, 2))\n\n    # Test ValueError when min_n and max_n are not integers\n    with assert_raises(ValueError):\n        HashingVectorizer(ngram_range=(2.5, 3))\n\n    # Test that build_analyzer returns the correct function\n    vectorizer = HashingVectorizer(analyzer='char')\n    analyzer = vectorizer.build_analyzer()\n    assert analyzer is not None\n    assert analyzer == vectorizer.build_analyzer()\n\n    # Test that _get_hasher returns the correct hasher\n    hasher = vectorizer._get_hasher()\n    assert hasher is not None\n    assert hasher == vectorizer._get_hasher()\n\n    # Test that transform works with an iterable of strings\n    vectorizer = HashingVectorizer()\n    docs = ['test', 'test2', 'test3']\n    X = vectorizer.transform(docs)\n    assert X.shape == (3, 17408)\n\n    # Test that transform works with a numpy array of strings\n    import numpy as np\n    vectorizer = HashingVectorizer()\n    docs = np.array(['test', 'test2', 'test3'])\n    X = vectorizer.transform(docs)\n    assert X.shape == (3, 17408)\n"], "sample_543": ["def test_slider_tight_bounds():\n    fig, ax = plt.subplots()\n    slider = widgets.Slider(ax, 'slider', -1, 1)\n    slider.valmin = -2\n    slider.valmax = 2\n    slider.valstep = 0.5\n    slider.set_val(-1)\n    assert slider.valmin == -2\n    assert slider.valmax == 2\n    assert slider.valstep == 0.5\n    assert slider.val == -1\n    slider.set_val(1)\n    assert slider.val == 1\n    slider.set_val(-0.25)\n    assert slider.val == -0.25\n    slider.set_val(1.75)\n    assert slider.val == 1.75\n", "    def test_click(self, ax):\n            pass\n\n        button = widgets.Button(ax, 'Click me')\n        assert button.get_active()\n        assert not button.get_visible()\n        button.set_visible(True)\n        assert button.get_visible()\n        button.on_clicked(on_clicked)\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'button_press_event', {'button': 1, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'button_press_event', {'button': 2, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'button_press_event', {'button': 1, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'button_release_event', {'button': 1, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'motion_notify_event', {'button': None, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'motion_notify_event', {'button': 1, 'inaxes': button.ax, 'x': 0.5, 'y': 0.5, 'xdata': 0.5, 'ydata': 0.5})\n        assert button.get_active()\n        assert button.get_visible()\n\n        do_event(button.ax, 'motion_notify", "def test_spanselector_interactive_minspanx():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2 * np.pi, 400)\n    ax.plot(x, np.sin(x))\n        assert_allclose(min, 0.0)\n        assert_allclose(max, 0.0)\n    span = widgets.SpanSelector(ax, onselect, direction='horizontal', minspanx=1.0)\n    ax.set_xlim(-1, 2)\n    click_and_drag(span, ax, (0, 0), (0.5, 0))\n    assert_allclose(span.extents, (0.5, 1.5, 0, 0))\n    click_and_drag(span, ax, (0.5, 0), (0.5, 0))\n    assert span.extents is None\n    click_and_drag(span, ax, (0, 0), (1.0, 0))\n    assert span.extents is None\n    assert span._selection_completed is False\n    span.onselect = lambda min, max: None\n    click_and_drag(span, ax, (0, 0), (0.5, 0))\n    assert span.extents is None\n    assert span._selection_completed is False\n", "def test_lasso_selector():\n    fig, ax = plt.subplots()\n    x = np.linspace(0, 2*np.pi, 100)\n    y = np.sin(x)\n    ax.plot(x, y)\n\n        assert_allclose(verts, [(1.7, 0.4), (2.7, 0.4), (3.3, 1.3)])\n\n    lasso = widgets.LassoSelector(ax, onselect)\n\n        assert_allclose(lasso.verts, [(1.7, 0.4), (2.7, 0.4), (3.3, 1.3)])\n\n    click_and_drag(lasso, ax, start=(0.5, 0.5), end=(3, 3), verify=verify_selection)\n", "def test_button():\n    fig, ax = plt.subplots()\n    ax.set_xlim(-1, 1)\n    ax.set_ylim(-1, 1)\n    b = widgets.Button(ax, 'click me')\n    b.on_clicked(noop)\n    assert b.active is True\n\n    # Test button is not visible until eventson is enabled\n    b.eventson = False\n    assert b.ax.get_window_extent().get_extents().x0 > 0\n\n    # Test button is visible after eventson is enabled\n    b.eventson = True\n    assert b.ax.get_window_extent().get_extents().x0 < 0\n\n    # Test label is not visible until eventson is enabled\n    b.label.set_visible = mock.Mock()\n    b.label.set_visible.return_value = None\n    b.eventson = False\n    b.ax.get_window_extent()\n    b.label.set_visible.assert_not_called()\n\n    # Test label is visible after eventson is enabled\n    b.eventson = True\n    b.ax.get_window_extent()\n    b.label.set_visible.assert_called_once()\n\n    # Test hover color is set correctly\n    b.hovercolor = 'red'\n    click_and_drag(b.ax, b.ax.transAxes.inverted().transform((0, 1)))\n    assert mcolors.to_rgba(b.ax.get_facecolor()) == mcolors.to_rgba('red')\n\n    # Test button is not drawn if drawon is disabled\n    b.drawon = False\n    click_and_drag(b.ax, b.ax.transAxes.inverted().transform((0, 1)))\n    b.ax.draw_artist.assert_not_called()\n\n    # Test button is drawn if drawon is enabled\n    b.drawon = True\n    click_and_drag(b.ax, b.ax.transAxes.inverted().transform((0, 1)))\n    b.ax.draw_artist.assert_called_once()\n"], "sample_725": ["def test_check_array_with_object_dtype_converts_to_float64():\n    X = np.array([1, 2, 3], dtype=object)\n    with pytest.warns(UserWarning) as rec:\n        check_array(X)\n    assert rec.list[0].message.args[0].startswith(\n        \"Beginning in version 0.22, arrays of strings will be interpreted as \"\n        \"decimal numbers if parameter 'dtype' is 'numeric'. It is recommended \"\n        \"that you convert the array to type np.float64 before passing it to \"\n        \"check_array.\")\n\n    X = np.array([1, 2, 3], dtype=object)\n    X = check_array(X, dtype=\"numeric\")\n    assert X.dtype == np.float64\n", "def test_check_array_large_integers():\n    # Test that large integers are allowed when copy is True\n    array = np.array([1e20], dtype=np.int64)\n    assert_equal(check_array(array, copy=True).dtype, np.int64)\n    # Test that large integers are not allowed when copy is False\n    with assert_raises_regex(ValueError, \"overflow\"):\n        check_array(array, copy=False)\n\n    # Test that large integers are allowed when dtype is None\n    array = np.array([1e20], dtype=np.int64)\n    assert_equal(check_array(array, dtype=None).dtype, np.int64)\n    # Test that large integers are not allowed when dtype is 'numeric'\n    with assert_raises_regex(ValueError, \"overflow\"):\n        check_array(array, dtype='numeric')\n", "def test_check_array_list_input():\n    # Test check_array with a list input\n    X = [[1, 2, 3], [4, 5, 6]]\n    expected_output = np.array([[1, 2, 3], [4, 5, 6]])\n    assert_array_equal(check_array(X, ensure_2d=True), expected_output)\n\n    # Test check_array with a list of floats\n    X = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n    assert_array_equal(check_array(X, ensure_2d=True), expected_output)\n\n    # Test check_array with a list of strings\n    with pytest.raises(ValueError):\n        check_array([\"a\", \"b\", \"c\"], ensure_2d=True)\n\n    # Test check_array with a list of lists of different lengths\n    with pytest.raises(ValueError):\n        check_array([[1, 2], [3], [4, 5, 6]], ensure_2d=True)\n\n    # Test check_array with a list of lists and ensure_min_samples\n    with pytest.raises(ValueError):\n        check_array([[1, 2], [3], [4, 5, 6]], ensure_2d=True, ensure_min_samples=2)\n\n    # Test check_array with a list of lists and ensure_min_features\n    with pytest.raises(ValueError):\n        check_array([[1, 2], [3], [4, 5, 6]], ensure_2d=True, ensure_min_features=3)\n", "def test_has_fit_parameter():\n    \"\"\"Check has_fit_parameter function\"\"\"\n    # Test with an estimator that has the parameter\n    model = ARDRegression()\n    assert has_fit_parameter(model, 'n_iter')\n    assert has_fit_parameter(model, 'alpha_init')\n\n    # Test with an estimator that does not have the parameter\n    model = KNeighborsClassifier()\n    assert not has_fit_parameter(model, 'n_iter')\n    assert not has_fit_parameter(model, 'alpha_init')\n\n    # Test with an estimator that has a different parameter name\n    model = RandomForestRegressor()\n    assert not has_fit_parameter(model, 'n_jobs')\n\n    # Test with an estimator that has the parameter with a different type\n    model = SVR()\n    assert has_fit_parameter(model, 'kernel')\n\n    # Test with an estimator that has the parameter but it is not a keyword argument\n    model = KNeighborsClassifier()\n    assert not has_fit_parameter(model, 'max_depth')\n    assert has_fit_parameter(model, 'max_depth', accept_non_kwargs=True)\n    assert not has_fit_parameter(model, 'max_depth', accept_non_kwargs=False)\n", "def test_check_is_fitted_with_mock_estimator():\n    class MockEstimator:\n            pass\n\n            raise NotFittedError(\"Mock estimator not fitted\")\n\n    estimator = MockEstimator()\n    with pytest.raises(NotFittedError):\n        check_is_fitted(estimator, \"predict_\")\n\n    estimator.fit(None, None)\n    check_is_fitted(estimator, \"predict_\", all_or_any=lambda x: all(x))\n"], "sample_247": ["    def setUpTestData(cls):\n        cls.a1 = Author.objects.create(name='Adrian Holovaty', age=34)\n        cls.b1 = Book.objects.create(\n            isbn='159059725', name='The Definitive Guide to Django: Web Development Done Right',\n            pages=447, rating=4.5, price=Decimal('30.00'), contact=cls.a1, publisher=Publisher.objects.create(),\n            pubdate=datetime.date(2007, 12, 6)\n        )\n", "    def test_join_promotion(self):\n        from django.db.models import OuterRef\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        class Store(models.Model):\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n\n        q = Book.objects.raw('SELECT * FROM books_book')\n        q = q.join(Store, 'book')\n        self.assertEqual(len(q.alias_map), 2)\n        self.assertEqual(q.alias_map[q.get_initial_alias()], q.alias_map['book'])\n\n        q = Book.objects.raw('SELECT * FROM books_book')\n        q = q.join(OuterRef('id'), 'id')\n        self.assertEqual(len(q.alias_map), 1)\n        self.assertEqual(q.alias_map[q.get_initial_alias()], q.alias_map['id'])\n\n        q = Book.objects.raw('SELECT * FROM books_book')\n        q = q.join('book__title', 'id')\n        self.assertEqual(len(q.alias_map), 1)\n        self.assertEqual(q.alias_map[q.get_initial_alias()], q.alias_map['id'])\n\n        q = Book.objects.raw('SELECT * FROM books_book')\n        q = q.join(OuterRef('id'), 'id__title')\n        self.assertEqual(len(q.alias_map), 1)\n        self.assertEqual(q.alias_map[q.get_initial_alias()], q.alias_map['id'])\n", "    def setUpTestData(cls):\n        cls.model1 = Book.objects.create(isbn='123456789', name='Test Book', pages=400)\n        cls.model2 = Book.objects.create(isbn='987654321', name='Test Book 2', pages=500)\n", "    def test_add_annotation_with_summarize(self):\n        q = Author.objects.annotate(total_books=Count('book_set'))\n        self.assertEqual(q.query.annotation_select, {'total_books': q.query.annotate.total_books})\n", "    def setUpTestData(cls):\n        cls.author = Author.objects.create(name='John', age=30)\n        cls.born = datetime.date(1970, 1, 1)\n        cls.born = datetime.datetime(1970, 1, 1).date()\n        cls.dob = datetime.date(1970, 1, 1)\n        cls.now = datetime.date.today()\n        cls.datediff = datetime.timedelta(days=10)\n        cls.current_year = datetime.date.today().year\n"], "sample_160": ["    def test_format_integer_with_sign(self):\n        self.assertEqual(format(123, decimal_sep='.').strip(), '123')\n        self.assertEqual(format(-123, decimal_sep='.').strip(), '-123')\n", "    def test_force_grouping_with_sequence(self):\n        self.assertEqual(\n            format(12345678901234567890, decimal_sep='', decimal_pos=None, \n                   grouping=(3, 2, 0), thousand_sep='', force_grouping=True),\n            '12,345,678,901,234,567,890'\n        )\n", "    def test_decimal_sep_with_decimal_pos(self):\n        # Test case for decimal separator with a specified decimal position\n        self.assertEqual(format(123.456, '.', 4), '123.4560')\n", "    def test_format_integer_negative(self):\n        self.assertEqual(format(-123, decimal_sep='.', decimal_pos=2), '-123.00')\n", "    def test_force_grouping_with_invalid_grouping(self):\n        # Test that force_grouping=True should ignore invalid grouping\n        self.assertEqual(format(12345.67, '.', 2, grouping='abc', force_grouping=True, use_l10n=False),\n                         '123,45.67')\n        self.assertEqual(format(12345.67, '.', 2, grouping='abc', force_grouping=True, use_l10n=False),\n                         '123,45.67')\n"], "sample_104": ["    def test_path_error(self):\n        with self.assertRaises(ImproperlyConfigured):\n            storage.staticfiles_storage.path('foo.css')\n", "    def test_manifest_static_files_storage(self):\n        # Test ManifestStaticFilesStorage uses the correct storage backend\n        self.assertEqual(get_storage_class(settings.STATICFILES_STORAGE), ManifestStaticFilesStorage)\n", "    def test_post_process_manifest_strict(self):\n        storage = ManifestStaticFilesStorage()\n        with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            files = {'static.css': ContentFile('body { background-color: #fff; }', 'css')}\n            storage.post_process(files)\n            storage.manifest_strict = False\n            storage.post_process(files)\n            self.assertIn('static.css', storage.hashed_files)\n            with self.assertRaises(ValueError):\n                storage.stored_name('static.css')\n", "    def test_manifest_static_files_storage(self):\n        # Test ManifestStaticFilesStorage class\n        storage = ManifestStaticFilesStorage()\n\n        # Test ManifestStaticFilesStorage post_process method\n        with tempfile.TemporaryDirectory() as temp_dir:\n            storage.location = temp_dir\n            self.assertEqual(storage.read_manifest(), None)\n\n            storage.save_manifest()\n            self.assertIn('paths', storage.read_manifest())\n\n            # Test that saving manifest updates the hashed files cache\n            self.assertEqual(storage.read_manifest()['paths'], storage.hashed_files)\n\n            # Test ManifestStaticFilesStorage stored_name method\n            hashed_files = storage.read_manifest()['paths']\n            self.assertEqual(storage.stored_name('file.js'), hashed_files['file.js'])\n\n            # Test ManifestStaticFilesStorage load_manifest method\n            storage.delete('manifest.json')\n            self.assertEqual(storage.load_manifest(), {})\n\n            # Test ManifestStaticFilesStorage save_manifest method\n            storage.save_manifest()\n            self.assertIn('paths', storage.read_manifest())\n\n        # Test ManifestStaticFilesStorage with keep_intermediate_files set to False\n        with tempfile.TemporaryDirectory() as temp_dir:\n            storage.location = temp_dir\n            storage.keep_intermediate_files = False\n            self.assertEqual(storage.read_manifest(), None)\n\n            storage.save_manifest()\n            self.assertNotIn('paths', storage.read_manifest())\n", "    def test_manifest_files_are_recreated_after_post_process_passes_exceeded(self):\n        with tempfile.TemporaryDirectory() as root:\n            self._create_test_staticfiles_storage(root)\n            staticfiles_storage = storage.staticfiles_storage\n\n            # Save a file to the storage, then post-process it. This should create a manifest.\n            staticfiles_storage.save('test.txt', ContentFile('Hello, world!'))\n\n            # Set max post-process passes to 1, which is not enough to\n            # process the files.\n            staticfiles_storage.max_post_process_passes = 1\n\n            # Post-process the files. This should create a new manifest with\n            # a hashed name for the file.\n            for name, hashed_name, processed in staticfiles_storage.post_process(dry_run=False):\n                pass\n\n            # Set max post-process passes to 0, which should cause an\n            # exception.\n            staticfiles_storage.max_post_process_passes = 0\n\n            # Post-process the files. This should raise a RuntimeError.\n            with self.assertRaises(RuntimeError):\n                for name, hashed_name, processed in staticfiles_storage.post_process(dry_run=False):\n                    pass\n\n            # The manifest should still be recreated.\n            self.assertTrue(staticfiles_storage.exists(staticfiles_storage.manifest_name))\n"], "sample_810": ["def test_Pipeline_fit_transform():\n    \"\"\"Test fit_transform of a pipeline.\"\"\"\n    X, y = load_iris(return_X_y=True)\n    scaler = StandardScaler()\n    selector = SelectKBest(f_classif, k=2)\n    clf = SVC()\n    pipe = Pipeline(steps=[('scaler', scaler), ('selector', selector), ('clf', clf)])\n    X_transformed = pipe.fit_transform(X, y)\n    assert_equal(X_transformed.shape, (150, 2))\n    assert isinstance(X_transformed, np.ndarray)\n", "    def test_pipeline_fit_transform(self):\n        X = np.random.rand(5, 5)\n        y = np.random.randint(0, 2, size=5)\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        Xt, _ = pipe.fit_transform(X, y)\n        assert Xt.shape == (5, 5)\n", "    def test_fit_params(self):\n        no_fit = NoFit()\n        assert_equal(no_fit.fit_params(None, None), {})\n", "    def test_pipeline_slicing(self):\n        \"\"\"Test slicing of a pipeline\"\"\"\n        X, y = load_iris(return_X_y=True)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        pipeline = make_pipeline(StandardScaler(), SVC())\n        pipeline.fit(X_train, y_train)\n        sub_pipeline = pipeline[:1]\n        self.assertEqual(sub_pipeline.named_steps, pipeline.named_steps)\n        self.assertEqual(sub_pipeline.steps, pipeline.steps[:1])\n        self.assertTrue(isinstance(sub_pipeline, Pipeline))\n", "    def test_score_with_sample_weight(self):\n        # Test if score() method passes sample_weight to the final estimator\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n        sample_weight = np.array([0.5, 0.5])\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        pipe.score(X, y, sample_weight=sample_weight)\n        assert pipe.named_steps['logisticregression'].score(X, y, sample_weight=sample_weight)\n"], "sample_518": ["    def test_boxstyle(self):\n        # Create a new figure and a set of subplots.\n        fig, axs = plt.subplots(1, 2)\n        fig.set_size_inches(8, 4)\n\n        # Add a FancyBboxPatch to each subplot.\n        axs[0].add_patch(BoxStyle(\"round\", pad=0.2)(0, 0, 1, 1))\n        axs[1].add_patch(BoxStyle(\"round\", pad=0.2, rounding_size=0.1)(0, 0, 1, 1))\n\n        # Set the x and y limits for each subplot.\n        axs[0].set_xlim(0, 1)\n        axs[0].set_ylim(0, 1)\n        axs[1].set_xlim(0, 1)\n        axs[1].set_ylim(0, 1)\n\n        # Show the plot.\n        plt.show()\n", "def test_FancyArrowPatch_arrowstyle():\n    # Test FancyArrowPatch arrowstyle property\n    fig, ax = plt.subplots()\n    patch = FancyArrowPatch(posA=(0.1, 0.1), posB=(0.5, 0.5))\n    ax.add_patch(patch)\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"simple\"\n\n    patch.arrowstyle = \"Fancy,head_length=0.2\"\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"Fancy\"\n    assert patch.get_arrowstyle().head_length == 0.2\n\n    patch.arrowstyle = \"Arc3,rad=0.2\"\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"Arc3\"\n    assert patch.get_arrowstyle().rad == 0.2\n\n    patch.arrowstyle = \"CurveA\"\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"CurveA\"\n\n    patch.arrowstyle = \"Simple\"\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"Simple\"\n\n    # Test FancyArrowPatch set_arrowstyle method\n    patch.set_arrowstyle(\"CurveB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"CurveB\"\n\n    patch.set_arrowstyle(\"CurveAB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"CurveAB\"\n\n    patch.set_arrowstyle(\"FancyFilledA\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"FancyFilledA\"\n\n    patch.set_arrowstyle(\"FancyFilledAB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"FancyFilledAB\"\n\n    patch.set_arrowstyle(\"BracketA\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"BracketA\"\n\n    patch.set_arrowstyle(\"BracketB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"BracketB\"\n\n    patch.set_arrowstyle(\"BracketAB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"BracketAB\"\n\n    patch.set_arrowstyle(\"BarAB\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"BarAB\"\n\n    patch.set_arrowstyle(\"BracketCurve\")\n    fig.canvas.draw()\n    assert patch.get_arrowstyle().name == \"BracketCurve\"\n\n    patch.set_arrowstyle(\"CurveBracket\")\n    fig.canvas.draw()\n    assert patch.get_arrow", "def test_fancybboxpatch_mutationscale():\n    fig, axs = plt.subplots(1, 1, figsize=(8, 6))\n    ax = axs\n    ax.axis('off')\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n    boxstyle = BoxStyle.Round(pad=0.1)\n    patch = FancyBboxPatch((1, 1), 1, 1, boxstyle=boxstyle,\n                           mutation_scale=0.5, mutation_aspect=2)\n    ax.add_patch(patch)\n    ax.plot([0, 2], [0, 2], 'k--')\n    fig.canvas.draw()\n    bbox = patch.get_window_extent(fig.canvas.get_renderer())\n    assert bbox.width == 2.5\n    assert bbox.height == 2\n", "compilation error", "def test_connectionpatch():\n    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n    ax1, ax2 = axs\n\n    xyA = (0.1, 0.1)\n    xyB = (0.7, 0.7)\n    annotation = ConnectionPatch(xyA, xyB, \"data\",\n                                coordsB=\"data\",\n                                axesA=ax1, axesB=ax2,\n                                arrowstyle=\"->\",\n                                connectionstyle=\"arc3\",\n                                shrinkA=2,\n                                shrinkB=2,\n                                mutation_scale=10.,\n                                mutation_aspect=1,\n                                clip_on=True,\n                                color='red')\n    annotation.set_annotation_clip(True)\n\n    ax1.add_patch(annotation)\n    ax1.set_xlim(-0.1, 1.1)\n    ax1.set_ylim(-0.1, 1.1)\n    ax1.set_title(\"Before drawing\")\n\n    annotation.draw(ax2.transData)\n\n    ax2.set_xlim(-0.1, 1.1)\n    ax2.set_ylim(-0.1, 1.1)\n    ax2.set_title(\"After drawing\")\n\n    # Set up the figure so that it will be saved correctly\n    # when using the image_comparison decorator.\n    fig.tight_layout()\n\n    yield check_figures_equal, fig, \"check_connectionpatch_before_after.png\"\n"], "sample_936": ["def test_stringify_TypeVar():\n    assert stringify(T) == 'T'\n    assert stringify(TypeVar('T')) == 'T'\n    assert stringify(TypeVar('X', bound=int)) == 'int'\n", "def test_stringify_typevar():\n    T = TypeVar('T')\n    assert stringify(T) == 'T'\n    assert stringify(T.__origin__) == 'TypeVar'\n", "    def __call__(self, *args, **kwargs):\n        pass\n\n", "def test_stringify_TypeVar():\n    T = TypeVar('T')\n    assert stringify(T) == 'T'\n", "def test_stringify_List():\n    # Test List[int] type annotation\n    assert stringify(List[int]) == 'List[int]'\n\n    # Test List with multiple type parameters\n    assert stringify(List[Tuple[int, str, float]]) == 'List[Tuple[int, str, float]]'\n\n    # Test List with Ellipsis\n    assert stringify(List[Ellipsis]) == 'List[...]'\n\n    # Test List with None in type parameters\n    assert stringify(List[Optional[int]]) == 'List[Optional[int]]'\n\n    # Test List with non-type as type parameter\n    class MyType:\n        pass\n    assert stringify(List[MyType]) == 'List[MyType]'\n\n    # Test List with user-defined class as type parameter\n    assert stringify(List[MyClass1]) == 'List[MyClass1]'\n\n    # Test List with nested type parameter\n    class MyNestedClass:\n        pass\n    assert stringify(List[MyNestedClass]) == 'List[MyNestedClass]'\n\n    # Test List with advanced user-defined class as type parameter\n    class MyNestedClass(MyClass2):\n        pass\n    assert stringify(List[MyNestedClass]) == 'List[MyNestedClass]'\n\n    # Test List with broken type as type parameter\n    with pytest.raises(TypeError):\n        stringify(List[BrokenType])\n\n    # Test List with Any as type parameter\n    assert stringify(List[Any]) == 'List[Any]'\n\n    # Test List with None as type parameter\n    assert stringify(List[None]) == 'List[None]'\n"], "sample_352": ["    def test_and_node(self):\n        node = WhereNode([Q(id=1), Q(name='foo')])\n        self.assertEqual(node.connector, 'AND')\n        self.assertEqual(node.children, [Q(id=1), Q(name='foo')])\n        self.assertEqual(node.negated, False)\n", "    def test_clone(self):\n        node = WhereNode([WhereNode([Q(a=1), Q(b=2)], connector='AND', negated=False)])\n        clone = node.clone()\n        self.assertNotEqual(node, clone)\n        self.assertEqual(node.connector, clone.connector)\n        self.assertEqual(node.negated, clone.negated)\n        self.assertEqual(node.children, clone.children)\n", "    def test_split_having(self):\n        # Test that split_having works as expected\n        # Test with AND connector\n        where_node = WhereNode([F('num') > 1, F('num') == 2], connector=AND)\n        where_node_2, having_node = where_node.split_having()\n        self.assertIsInstance(where_node_2, WhereNode)\n        self.assertIsInstance(having_node, WhereNode)\n        self.assertEqual(len(where_node_2.children), 2)\n        self.assertEqual(len(having_node.children), 0)\n\n        # Test with OR connector\n        where_node = WhereNode([F('num') > 1, F('num') == 2], connector=OR)\n        where_node_2, having_node = where_node.split_having()\n        self.assertIsInstance(where_node_2, WhereNode)\n        self.assertIsInstance(having_node, WhereNode)\n        self.assertEqual(len(where_node_2.children), 0)\n        self.assertEqual(len(having_node.children), 2)\n\n        # Test with both aggregate and non-aggregate children\n        where_node = WhereNode([F('num') > 1, Count('id') > 2], connector=AND)\n        where_node_2, having_node = where_node.split_having()\n        self.assertIsInstance(where_node_2, WhereNode)\n        self.assertIsInstance(having_node, WhereNode)\n        self.assertEqual(len(where_node_2.children), 1)\n        self.assertEqual(len(having_node.children), 1)\n\n        # Test with negated node\n        where_node = WhereNode([F('num') > 1, F('num') == 2], connector=AND, negated=True)\n        where_node_2, having_node = where_node.split_having()\n        self.assertIsInstance(where_node_2, WhereNode)\n        self.assertIsInstance(having_node, WhereNode)\n        self.assertEqual(len(where_node_2.children), 2)\n        self.assertEqual(len(having_node.children), 0)\n", "    def setUpTestData(cls):\n        cls.c1 = Classroom.objects.create(name='class1')\n        cls.c2 = Classroom.objects.create(name='class2')\n        cls.c3 = Classroom.objects.create(name='class3')\n\n        cls.t1 = Teacher.objects.create(name='t1', classroom=cls.c1)\n        cls.t2 = Teacher.objects.create(name='t2', classroom=cls.c2)\n        cls.t3 = Teacher.objects.create(name='t3', classroom=cls.c3)\n\n        cls.s1 = Student.objects.create(name='s1', teacher=cls.t1)\n        cls.s2 = Student.objects.create(name='s2', teacher=cls.t2)\n        cls.s3 = Student.objects.create(name='s3', teacher=cls.t3)\n", "    def test_where_distinct(self):\n        # Ensure that using distinct in the where clause is supported\n        # and does not prevent ordering\n        query = BaseA.objects.filter(distinct=True).order_by('id')\n        self.assertEqual(list(query), [1, 2, 3])\n"], "sample_843": ["def test_RationalQuadratic_diag():\n    r\"\"\"Test the diagonal of the RationalQuadratic kernel.\"\"\"\n    # Test cases for isotropic variant\n    kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)\n    X = np.atleast_2d(np.random.RandomState(0).normal(0, 1, (10, 2)))\n    K_diag = kernel.diag(X)\n    assert_array_equal(K_diag, kernel(X).diagonal())\n\n    # Test case for anisotropic variant\n    kernel = RationalQuadratic(length_scale=[1.0, 2.0], alpha=1.5)\n    X = np.atleast_2d(np.random.RandomState(0).normal(0, 1, (10, 2)))\n    K_diag = kernel.diag(X)\n    assert_array_equal(K_diag, kernel(X).diagonal())\n\n    # Test case with fixed alpha\n    kernel = RationalQuadratic(length_scale=1.0, alpha=1.5,\n                              alpha_bounds=\"fixed\")\n    X = np.atleast_2d(np.random.RandomState(0).normal(0, 1, (10, 2)))\n    K_diag = kernel.diag(X)\n    assert_array_equal(K_diag, kernel(X).diagonal())\n\n    # Test case with fixed length scale\n    kernel = RationalQuadratic(length_scale=1.0, alpha=1.5,\n                              length_scale_bounds=\"fixed\")\n    X = np.atleast_2d(np.random.RandomState(0).normal(0, 1, (10, 2)))\n    K_diag = kernel.diag(X)\n    assert_array_equal(K_diag, kernel(X).diagonal())\n\n    # Test case with fixed alpha and length scale\n    kernel = RationalQuadratic(length_scale=1.0, alpha=1.5,\n                              alpha_bounds=\"fixed\", length_scale_bounds=\"fixed\")\n    X = np.atleast_2d(np.random.RandomState(0).normal(0, 1, (10, 2)))\n    K_diag = kernel.diag(X)\n    assert_array_equal(K_diag, kernel(X).diagonal())\n", "def test_clone_with_theta():\n    kernel = RBF(length_scale=2.0, length_scale_bounds=(0.1, 10.0))\n    cloned_kernel = clone(kernel)\n    cloned_kernel.theta = kernel.theta\n    assert_almost_equal(cloned_kernel.theta, kernel.theta)\n    cloned_kernel.theta += 1.0\n    assert_not_almost_equal(cloned_kernel.theta, kernel.theta)\n    assert_almost_equal(kernel.theta, np.exp(np.array([])))\n\n    kernel = ConstantKernel(constant_value=2.0)\n    cloned_kernel = clone(kernel)\n    cloned_kernel.theta = kernel.theta\n    assert_almost_equal(cloned_kernel.theta, kernel.theta)\n    cloned_kernel.theta += 1.0\n    assert_not_almost_equal(cloned_kernel.theta, kernel.theta)\n    assert_almost_equal(kernel.theta, 0.0)\n\n    kernel = PairwiseKernel(gamma=2.0, metric='linear')\n    cloned_kernel = clone(kernel)\n    cloned_kernel.theta = kernel.theta\n    assert_almost_equal(cloned_kernel.theta, kernel.theta)\n    cloned_kernel.theta += 1.0\n    assert_not_almost_equal(cloned_kernel.theta, kernel.theta)\n    assert_almost_equal(kernel.theta, np.log(np.array([2.0])))\n", "def test_Kernel__nested_kernel_creation():\n    \"\"\"Test if nested kernel creation works as expected.\"\"\"\n    # Create a new kernel with a nested kernel\n    k = Exponentiation(RBF(length_scale=1.0), 2.0)\n    # Check that the kernel is correctly represented\n    assert isinstance(k, Exponentiation)\n    assert isinstance(k.kernel, RBF)\n    assert k.kernel.length_scale == 1.0\n    # Check that the hyperparameters are correctly represented\n    assert k.hyperparameters[0].name == \"kernel__length_scale\"\n    assert k.hyperparameters[0].value_type == \"numeric\"\n    assert k.hyperparameters[0].bounds == (1e-5, 1e5)\n    # Check that the bounds are correctly applied\n    k.theta = np.array([0.5])\n    assert k.bounds[0, 0] == np.log(0.5)\n    assert k.bounds[0, 1] == np.log(1.5)\n    # Check that the kernel can be cloned correctly\n    k_clone = clone(k)\n    assert k_clone.theta == k.theta\n    assert k_clone.bounds == k.bounds\n    # Check that the kernel can be evaluated correctly\n    X = np.array([[1, 2]])\n    Y = np.array([[3, 4]])\n    K = k(X, Y)\n    assert K.shape == (1, 1)\n    # Check that the kernel can be evaluated with gradient correctly\n    K_grad = k(X, Y, eval_gradient=True)[1]\n    assert K_grad.shape == (1, 1, 1)\n", "    def test_clone(self, kernel):\n        clone_kernel = clone(kernel)\n        assert isinstance(clone_kernel, Kernel)\n        for k in kernel.hyperparameters:\n            assert isinstance(clone_kernel.hyperparameter_ + k.name, Hyperparameter)\n        assert clone_kernel.theta.size == kernel.theta.size\n        assert_array_almost_equal(clone_kernel.bounds, kernel.bounds)\n        assert clone_kernel.__class__ == kernel.__class__\n        assert clone_kernel.diag(X).size == kernel.diag(X).size\n        if hasattr(kernel, \"is_stationary\"):\n            assert clone_kernel.is_stationary() == kernel.is_stationary()\n        if hasattr(kernel, \"diag\"):\n            assert clone_kernel.diag(X).size == kernel.diag(X).size\n", "def test_kernel_gradient():\n    # Check that the gradient of a kernel with respect to its hyperparameters is\n    # evaluated correctly.\n    X = np.atleast_2d(np.array([[1.0, 2.0], [3.0, 4.0]]))\n\n    # Test RBF\n    kernel = RBF(length_scale=1.0)\n    K, K_gradient = kernel(X, eval_gradient=True)\n    expected_gradient = -2 * (K * (X[:, np.newaxis] - X) ** 2)[:, :, np.newaxis]\n    assert_array_almost_equal(K_gradient.sum(-1)[:, :, 0], expected_gradient.sum(-1)[:, :, 0])\n\n    # Test Matern\n    kernel = Matern(length_scale=1.0)\n    K, K_gradient = kernel(X, eval_gradient=True)\n    expected_gradient = (2.0 * K * np.sqrt(2 * kernel.nu) *\n                         (X[:, np.newaxis] - X) * (X[:, np.newaxis, :] - X[:, :, np.newaxis]) ** 2)[:, :, np.newaxis]\n    assert_array_almost_equal(K_gradient.sum(-1)[:, :, 0], expected_gradient.sum(-1)[:, :, 0])\n\n    # Test RationalQuadratic\n    kernel = RationalQuadratic(length_scale=1.0, alpha=1.5)\n    K, K_gradient = kernel(X, eval_gradient=True)\n    alpha_gradient = K * (-1.5 * np.log(1 + np.linalg.norm(X - X[:, np.newaxis], axis=2)**2 / (2 * kernel.alpha * kernel.length_scale**2)) / kernel.length_scale**2 + np.linalg.norm(X - X[:, np.newaxis], axis=2)**2 / (2 * kernel.alpha * kernel.length_scale**2) * 1 / (1 + np.linalg.norm(X - X[:, np.newaxis], axis=2)**2 / (2 * kernel.alpha * kernel.length_scale**2))**2)\n    length_scale_gradient = K * (2 * np.linalg.norm(X - X[:, np.newaxis], axis=2)**2 / (2 * kernel.alpha * kernel.length_scale**2)**2) / kernel.length_scale\n    expected_gradient = np.dstack((alpha_gradient, length_scale_gradient))\n    assert_array_almost_equal(K_gradient.sum(-1)[:, :, 0], expected_gradient.sum(-1)[:, :, 0])\n\n    # Test ExpSineSquared\n"], "sample_1158": ["def test_cant_sympify_with_custom_class_subclassing():\n    class CustomClass(CantSympify):\n        pass\n\n    with raises(SympifyError):\n        sympify(CustomClass())\n", "def test_sympify_numpy_datatype():\n    from sympy import sympify\n    from sympy.core.sympify import _convert_numpy_types\n    assert isinstance(_convert_numpy_types(1.0), Float)\n    assert isinstance(_convert_numpy_types(1.5), Float)\n    assert isinstance(_convert_numpy_types(1), Integer)\n    assert isinstance(_convert_numpy_types(1.5 + 1j), Float)\n    assert isinstance(_convert_numpy_types(complex(1, 2)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array(1.0)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array(1.5 + 1j)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j])), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.float32)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.float64)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.int32)), Integer)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.int64)), Integer)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.complex64)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.complex128)), Float)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.uint32)), Integer)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.uint64)), Integer)\n    assert isinstance(_convert_numpy_types(numpy.array([1.0, 1.5, 1.5 + 1j], dtype=numpy.int8)), Integer)\n    assert isinstance(_convert", "def test_sympify_numpy_array_with_ndim():\n    # Test that sympify treats numpy arrays with ndim > 0 as non-convertible\n    import numpy as np\n    a = np.array([[1, 2], [3, 4]])\n    with raises(SympifyError):\n        sympify(a)\n\n", "def test_sympify_real_numpy_array():\n    # Issue #17370: numpy real arrays should be sympified as reals\n    from numpy import array, real\n    a = real(array([1, 2]))\n    assert sympify(a).is_real  # do not raise an error\n\n", "    def __iter__(self):\n        yield 1\n        yield 2\n"], "sample_587": ["    def test_merge_variables_priority_vars(self):\n        ds1 = create_test_data()\n        ds2 = create_test_data()\n        priority_var = xr.DataArray([1, 2, 3], dims=['x'])\n        priority_var.name = 'priority_var'\n        merged_variables, _, _ = merge_variables([ds1.variables, ds2.variables], priority_vars={priority_var.name: priority_var})\n        assert merged_variables['priority_var'].values.tolist() == [1, 2, 3]\n", "def test_merge_core_explicit_coords():\n    # Test that explicit_coords parameter works as expected\n    # Create two datasets with different coordinate names\n    ds1 = xr.Dataset({\"var1\": (\"x\", [1, 2, 3]), \"var2\": (\"y\", [4, 5, 6])})\n    ds2 = xr.Dataset({\"var1\": (\"x\", [1, 2, 3]), \"var3\": (\"y\", [7, 8, 9])})\n\n    # Define explicit coordinates\n    explicit_coords = {\"var1\"}\n\n    # Merge the datasets\n    variables, coord_names, dims = merge_core([ds1, ds2], priority_arg=None, explicit_coords=explicit_coords)\n\n    # Check that var1 is a coordinate\n    assert \"var1\" in coord_names\n\n    # Check that var3 is not a coordinate\n    assert \"var3\" not in coord_names\n\n    # Check that var2 is a non-coordinate\n    assert \"var2\" not in coord_names\n", "def test_merge_variables_with_priority_vars_and_conflicting_dimensions():\n    # Given\n    x = xr.Dataset({\"a\": ((1, 2, 3), np.array([1, 2, 3]))})\n    y = xr.Dataset({\"a\": ((1, 2, 3, 4), np.array([4, 5, 6, 7]))})\n    priority_vars = {\"a\": x[\"a\"]}\n\n    # When\n    with pytest.raises(xr.MergeError):\n        merge_variables(\n            [x[\"a\"].to_dict(), y[\"a\"].to_dict()], priority_vars=priority_vars\n        )\n", "def test_merge_core_prioritizes_priority_arg(self):\n    da1 = xr.DataArray([1, 2, 3], dims=['x'])\n    da2 = xr.DataArray([4, 5, 6], dims=['x'])\n    da3 = xr.DataArray([7, 8, 9], dims=['x'])\n\n    data_vars = {\n        'x': da1,\n        'y': xr.DataArray([1, 2, 3], dims=['y'])\n    }\n    other_data_vars = {\n        'x': da3,\n        'y': xr.DataArray([7, 8, 9], dims=['y'])\n    }\n\n    with raises_regex('conflicting values'):\n        merge_core([data_vars, other_data_vars], priority_arg=1)\n\n    with raises_regex('conflicting values'):\n        merge_core([data_vars, other_data_vars], priority_arg=2)\n\n    merged_vars, _, _ = merge_core([data_vars, other_data_vars], priority_arg=0)\n    assert merged_vars['x'].values.tolist() == [1, 2, 3]\n    assert merged_vars['y'].values.tolist() == [1, 2, 3]\n", "def test_merge_core_preserves_indexes(self):\n    ds1 = xr.Dataset({\"a\": (\"x\", [1, 2, 3])})\n    ds2 = xr.Dataset({\"a\": (\"x\", [4, 5, 6]), \"b\": (\"y\", [10, 20])})\n    ds3 = xr.Dataset({\"a\": (\"x\", [7, 8, 9]), \"c\": (\"z\", [100, 200])})\n\n    variables, coord_names, dims = merge_core([ds1, ds2, ds3], priority_arg=1)\n    assert set(variables.keys()) == {\"a\", \"b\", \"c\"}\n    assert set(variables[\"a\"].dims) == {\"x\"}\n    assert variables[\"b\"].dims == (\"y\",)\n    assert variables[\"c\"].dims == (\"z\",)\n    assert set(dims.keys()) == {\"x\", \"y\", \"z\"}\n    assert dims[\"x\"] == 3\n    assert dims[\"y\"] == 2\n    assert dims[\"z\"] == 2\n"], "sample_970": ["def test_getslots():\n    class MyClass:\n        __slots__ = ['a', 'b']\n\n    class MySubclass(MyClass):\n        __slots__ = ['c']\n\n    assert getslots(MyClass) == {'a': None, 'b': None}\n    assert getslots(MySubclass) == {'a': None, 'b': None, 'c': None}\n    assert getslots(object) is None\n    with pytest.raises(TypeError):\n        getslots(123)\n    with pytest.raises(ValueError):\n        class MyClass:\n            __slots__ = 123\n", "def test_getargspec():\n    class TestFunction:\n            pass\n\n    sig = inspect.getargspec(TestFunction.test_func)\n    assert sig.args == ['a', 'b']\n    assert sig.varargs is None\n    assert sig.varkw is None\n    assert sig.defaults == (None,)\n    assert sig.kwonlyargs == ['c']\n    assert sig.kw_defaults == {'c': None}\n    assert sig.annotations == {}\n", "def test_is_singledispatch_method():\n    class A:\n        @singledispatch\n            return x\n\n        @f.register(int)\n            return f'integer {x}'\n\n    assert inspect.is_singledispatch_method(A().f)\n    assert not inspect.is_singledispatch_method(functools.singledispatch(lambda x: x)(int))\n", "def test_getmro_safely():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    class C(B):\n        pass\n\n    class D(C):\n        pass\n\n    obj = D()\n    mro = inspect.getmro(obj)\n    assert mro == (D, C, B, A, object)\n", "def test_getannotations():\n    class TestClass:\n        __annotations__ = {'a': int, 'b': str}\n\n    assert inspect.getannotations(TestClass) == {'a': int, 'b': str}\n"], "sample_150": ["    def test_create_parser(self):\n        command = BaseCommand()\n        parser = command.create_parser('django-admin', 'test_command')\n        self.assertIsInstance(parser, CommandParser)\n        self.assertEqual(parser.prog, 'django-admin test_command')\n        self.assertEqual(parser.description, command.help)\n        self.assertEqual(parser.formatter_class, DjangoHelpFormatter)\n        self.assertIn('--version', parser._option_string_actions)\n", "    def test_handle_app_config_no_app_config(self):\n        class TestAppCommand(AppCommand):\n                return 'Output for app_config: %s' % app_config.name\n        parser = TestAppCommand()\n        with self.assertRaises(CommandError):\n            parser.handle()\n", "    def test_system_check(self):\n        check_result = check_database_backends(connections)\n        self.assertEqual(check_result[0], 0)\n        self.assertEqual(check_result[1], [])\n", "    def test_style_variables(self, no_style_mock, style_mock):\n        cmd = BaseCommand()\n        cmd.style = style_mock\n        cmd.style_func = lambda x: x\n        cmd.stdout = mock.Mock()\n        cmd.stderr = mock.Mock()\n        cmd.output_transaction = True\n        cmd.execute(verbosity=0, database='default')\n        style_mock.SQL_KEYWORD.assert_called_once_with('BEGIN')\n        style_mock.SQL_KEYWORD.assert_called_twice_with('COMMIT')\n", "    def setUp(self):\n        self.label_command = LabelCommand()\n        self.app_label = 'myapp'\n"], "sample_972": ["def test_stringify_typing_Any():\n    assert stringify(typing.Any) == '~typing.Any'\n", "def test_stringify_special_form():\n    from sphinx.util.typing import stringify\n\n    assert stringify(typing.Any) == '~typing.Any'\n    assert stringify(typing.NoReturn) == '~typing.NoReturn'\n    assert stringify(typing.Union) == '~typing.Union'\n    assert stringify(typing.GenericMeta) == '~typing.GenericMeta'\n    assert stringify(typing.NewTypeMeta) == '~typing.NewTypeMeta'\n    assert stringify(typing.TypeVarMeta) == '~typing.TypeVarMeta'\n    assert stringify(typing.ParamSpecMeta) == '~typing.ParamSpecMeta'\n    assert stringify(typing.LiteralMeta) == '~typing.LiteralMeta'\n    assert stringify(typing.AnnotatedMeta) == '~typing.AnnotatedMeta'\n    assert stringify(typing.ConcatenateMeta) == '~typing.ConcatenateMeta'\n    assert stringify(typing.OptionalMeta) == '~typing.OptionalMeta'\n    assert stringify(typing.UnionMeta) == '~typing.UnionMeta'\n    assert stringify(typing.CallableMeta) == '~typing.CallableMeta'\n    assert stringify(typing.GeneratorMeta) == '~typing.GeneratorMeta'\n    assert stringify(typing.ParamSpec) == '~typing.ParamSpec'\n    assert stringify(typing.TypeAlias) == '~typing.TypeAlias'\n", "def test_stringify_SystemExit():\n    \"\"\"Test that the stringify function can handle SystemExit.\"\"\"\n    from typing import Union\n    from typing import SystemExit\n\n    assert stringify(SystemExit, mode='fully-qualified-except-typing') == 'builtins.SystemExit'\n", "def test_restify_newtype():\n    # Test with NewType\n    assert restify(MyInt) == ':py:obj:`~typing.NewType`'\n\n    # Test with NewType and string\n    assert restify('MyInt') == 'MyInt'\n\n    # Test with NewType and class\n    class MyInt2(MyInt):\n        pass\n    assert restify(MyInt2) == ':py:obj:`~typing.NewType`'\n\n    # Test with NewType and None\n    assert restify(None) == 'None'\n\n    # Test with NewType and Ellipsis\n    assert restify(...) == '...'\n\n    # Test with NewType and instance\n    assert restify(MyInt()) == ':py:obj:`~typing.NewType`'\n", "    def test_get_type_hints(self):\n        obj1 = MyClass1()\n        assert get_type_hints(obj1) == {}\n        assert get_type_hints(obj1, localns={'T': TypeVar('T')}) == {'__annotations__': {}}\n        assert get_type_hints(obj1, globalns={'T': TypeVar('T')}) == {'__annotations__': {}}\n"], "sample_1105": ["def test_factor_in_front():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 3)\n    X = MatMul(A, B)\n    assert factor_in_front(X) == X\n    assert factor_in_front(MatMul(2, A, B)) == MatMul(2, A, B)\n    assert factor_in_front(MatMul(2, 2, A, B)) == MatMul(4, A, B)\n    assert factor_in_front(MatMul(2, A, 2, B)) == MatMul(4, A, B)\n\n    # Test with non-matrix arguments\n    X = MatMul(A, x, B)\n    assert factor_in_front(X) == X\n    X = MatMul(2, A, x, B)\n    assert factor_in_front(X) == MatMul(2, A, x, B)\n    X = MatMul(2, 2, A, x, B)\n    assert factor_in_front(X) == MatMul(4, A, x, B)\n    X = MatMul(2, A, 2, x, B)\n    assert factor_in_front(X) == MatMul(4, A, x, B)\n", "def test_refine_MatMul_with_unitary_matrix():\n    from sympy import MatrixSymbol, Q, assuming, refine\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    expr = X * Y * Z\n    with assuming(Q.unitary(X)):\n        result = refine(expr)\n        expected = X.conjugate().transpose() * Z\n        assert result.equals(expected), f\"Expected {expected}, got {result}\"\n", "def test_only_squares():\n    n = Symbol('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    X = MatMul(A, B, C)\n    result = only_squares(X)\n    assert len(result) == 1\n    assert result[0].doit() == MatMul(A, B, C).doit()\n", "def test_remove_ids():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    I = eye(2)\n    X = MatMul(A, I, B, C, I)\n    assert remove_ids(X) == MatMul(A, B, C)\n", "def test_matmul_valid_shapes():\n    n = 3\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n\n    # Test MatMul with 2 matrices\n    matmul_2 = A*B\n    assert matmul_2.is_MatMul\n    assert matmul_2.args[0].is_Matrix\n    assert matmul_2.args[1].is_Matrix\n    assert matmul_2.shape == (n, n)\n\n    # Test MatMul with 3 matrices\n    matmul_3 = A*B*C\n    assert matmul_3.is_MatMul\n    assert matmul_3.args[0].is_Matrix\n    assert matmul_3.args[1].is_Matrix\n    assert matmul_3.args[2].is_Matrix\n    assert matmul_3.shape == (n, n)\n\n    # Test MatMul with 0 matrices\n    matmul_0 = A*B*C\n    try:\n        matmul_0 = matmul_0.subs({A: Identity(n)})\n        assert matmul_0 == B*C\n    except ShapeError:\n        pass\n\n    # Test MatMul with invalid shapes\n    matmul_invalid = A*C\n    try:\n        matmul_invalid = matmul_invalid.doit()\n        assert False\n    except ShapeError:\n        pass\n\n    # Test MatMul with a matrix and a scalar\n    matmul_scalar = A*2\n    assert matmul_scalar.is_MatMul\n    assert matmul_scalar.args[0].is_Matrix\n    assert matmul_scalar.args[1].is_Number\n    assert matmul_scalar.shape == (n, m)\n\n    # Test MatMul with a scalar and a matrix\n    matmul_scalar_mat = 2*A\n    assert matmul_scalar_mat.is_MatMul\n    assert matmul_scalar_mat.args[0].is_Number\n    assert matmul_scalar_mat.args[1].is_Matrix\n    assert matmul_scalar_mat.shape == (n, m)\n\n    # Test MatMul with Identity matrix\n    matmul_identity = A*Identity(n)\n    assert matmul_identity.is_MatMul\n    assert matmul_identity.args[0].is_Matrix\n    assert matmul_identity.args[1].is_Matrix\n    assert"], "sample_916": ["def test_parse_expression_fallback() -> None:\n    class Config:\n        c_id_attributes = [\"id_attr\"]\n        c_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"int x = 5;\", location=None, config=Config())\n    parser.allowFallbackExpressionParsing = True\n    ast = parser.parse_declaration('function', 'function')\n    parser.assert_end()\n    # the expression should have been parsed by the fallback parser\n    assert isinstance(ast.declaration, ASTFallbackExpr)\n    assert ast.declaration.expr == \"5\"\n", "def test_parse_function_name():\n    # a simple function\n    ast = parse('function', 'void foo(int a, int b)')\n    res = str(ast)\n    if res != 'void foo(int a, int b)':\n        print(\"\")\n        print(\"Input:    \", 'void foo(int a, int b)')\n        print(\"Result:   \", res)\n        print(\"Expected: \", 'void foo(int a, int b)')\n        raise DefinitionError(\"\")\n\n    rootSymbol = Symbol(None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\")\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature('void foo(int a, int b)', '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n\n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        try:\n            id = ast.get_id(version=i)\n            assert id is not None\n            idActual.append(id[len(_id_prefix[i]):])\n        except NoOldIdError:\n            idActual.append(None)\n\n    res = [True]\n    for i in range(1, _max_id + 1):\n        res.append(idExpected[i] == idActual[i])\n\n    if not all(res):\n        print(\"input:    %s\" % 'void foo(int a, int b)'.rjust(20))\n        for i in range(1, _max_id + 1):\n            if res[i]:\n                continue\n            print(\"Error in id version %d.\" % i)\n            print(\"result:   %s\" % idActual[i])\n            print(\"expected: %s\" % idExpected[i])\n        print(rootSymbol.dump(0))\n        raise DefinitionError(\"\")\n\n    # a function with template parameters\n    ast = parse('function', 'template <typename T> void foo(T a)')\n    res = str(ast)\n    if res != 'template <typename T> void foo(T a)':\n        print(\"\")\n        print(\"Input:    \", 'template <typename T> void foo(T a", "def test_definition_parse_type_with_init():\n    input = \"typedef int foo(int);\"\n    idDict = {1: 'c.1'}\n    output = \"typedef int foo(int);\"\n    check('typedef', input, idDict, output)\n", "def test_parse_pointer():\n    input = \"int *ptr;\"\n    idDict = {1: \"ptr_1\", 2: \"ptr_2\", 3: \"ptr_3\", 4: \"ptr_4\", 5: \"ptr_5\"}\n    output = \"int*ptr\"\n    check('member', input, idDict, output)\n", "def test_unscoped_enumerator():\n    input = \"enum E { X }\"\n    idDict = {3: \"E.X\"}\n    output = \"enum E { X };\"\n    check(\"enum\", input, idDict, output)\n"], "sample_320": ["    def test_alter_model_table(self):\n        initial_sql = self.execute_migration(\n            \"001\", \"002\", test_apps=[\"tests.test_app\"]\n        )\n        self.assertIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            initial_sql,\n        )\n        self.assertIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            initial_sql,\n        )\n        self.execute_migration(\"002\", \"003\", test_apps=[\"tests.test_app\"])\n        self.assertIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            self.execute_migration(\"003\", \"004\", test_apps=[\"tests.test_app\"]),\n        )\n        self.assertIn(\n            \"ALTER TABLE tests_testapp_unicodemodel\",\n            self.execute_migration(\"004\", \"005\", test_apps=[\"tests.test_app\"]),\n        )\n        self.execute_migration(\"005\", \"006\", test_apps=[\"tests.test_app\"])\n        final_sql = self.execute_migration(\n            \"006\", \"007\", test_apps=[\"tests.test_app\"]\n        )\n        self.assertNotIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            final_sql,\n        )\n        self.assertIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            final_sql,\n        )\n        self.assertIn(\n            \"CREATE TABLE tests_testapp_unicodemodel (\",\n            final_sql,\n        )\n        self.assertIn(\n            \"ALTER TABLE tests_testapp_unicodemodel\",\n            final_sql,\n        )\n", "    def test_add_index(self):\n        # Create table\n        migration = Migration(\"tests\", \"0001_initial\", project_state=ProjectState())\n        migration.operations.append(CreateModel(\"unicode\", ( (\"name\", models.CharField(max_length=255)), (\"description\", models.TextField()), (\"price\", models.DecimalField(max_digits=8, decimal_places=2)), (\"tags\", models.ManyToManyField(\"unicode\")) ), options={\"indexes\": []}, managers=[(\"objects\", models.Manager())]))\n        migration.state_after.apply()\n        \n        # Add index\n        migration = Migration(\"tests\", \"0002_add_index\", project_state=ProjectState())\n        migration.operations.append(AddIndex(\"unicode\", models.Index(fields=[\"price\"], name=\"price_index\")))\n        migration.state_after.apply()\n        \n        # Verify state\n        self.assertEqual(migration.state_after.models[\"tests.unicode\"].indexes[0].fields, [\"price\"])\n        self.assertEqual(migration.state_after.models[\"tests.unicode\"].indexes[0].name, \"price_index\")\n        \n        # Verify database\n        with CaptureQueriesContext() as queries:\n            with transaction.atomic():\n                cursor = connection.cursor()\n                query = \"SHOW INDEX FROM tests_unicode\"\n                cursor.execute(query)\n                results = cursor.fetchall()\n                self.assertEqual(len(results), 4)\n                self.assertEqual(results[0][5], \"price_index\")\n        \n        # Remove index\n        migration = Migration(\"tests\", \"0003_remove_index\", project_state=ProjectState())\n        migration.operations.append(RemoveIndex(\"unicode\", \"price_index\"))\n        migration.state_after.apply()\n        \n        # Verify state\n        self.assertEqual(migration.state_after.models[\"tests.unicode\"].indexes, [])\n        \n        # Verify database\n        with CaptureQueriesContext() as queries:\n            with transaction.atomic():\n                cursor = connection.cursor()\n                query = \"SHOW INDEX FROM tests_unicode\"\n                cursor.execute(query)\n                results = cursor.fetchall()\n                self.assertEqual(len(results), 3)\n", "    def test_rename_index_fields(self):\n        class MyModel(models.Model):\n            a = models.IntegerField()\n            b = models.IntegerField()\n\n        with transaction.atomic():\n            Migration.from_state(\n                ProjectState(\n                    apps=[],\n                    operations=[\n                        migrations.CreateModel(\n                            name='MyModel',\n                            fields=[\n                                (\"a\", models.IntegerField()),\n                                (\"b\", models.IntegerField()),\n                            ],\n                        ),\n                        migrations.RunPython(self.create_index, self.delete_index),\n                    ],\n                )\n            ).apply(self.migration_plan)\n\n            with connection.schema_editor() as schema_editor:\n                self.assertEqual(\n                    schema_editor.get_indexes(\"myapp_mymodel\"),\n                    [\n                        (\n                            'myapp_mymodel_a',\n                            'myapp_mymodel_b',\n                            [],\n                            [],\n                            (),\n                        ),\n                        (\n                            'myapp_mymodel',\n                            'myapp_mymodel_a',\n                            [],\n                            [],\n                            (),\n                        ),\n                    ],\n                )\n\n        class RenameIndexFieldOperation(migrations.RenameIndex):\n            old_fields = ['myapp_my_model__a']\n\n        with transaction.atomic():\n            Migration.from_state(\n                ProjectState(\n                    apps=[],\n                    operations=[\n                        migrations.CreateModel(\n                            name='MyModel',\n                            fields=[\n                                (\"a\", models.IntegerField()),\n                                (\"b\", models.IntegerField()),\n                            ],\n                        ),\n                        migrations.RunPython(self.create_index, self.delete_index),\n                        RenameIndexFieldOperation(\n                            model_name='MyModel',\n                            new_name='myapp_my_model__c',\n                            old_name='myapp_my_model__a',\n                            old_fields='myapp_my_model__a',\n                        ),\n                    ],\n                )\n            ).apply(self.migration_plan)\n\n            with connection.schema_editor() as schema_editor:\n                self.assertEqual(\n                    schema_editor.get_indexes(\"myapp_mymodel\"),\n                    [\n                        (\n                            'myapp_my_model__a',\n                            'myapp_my_model__b',\n                            [],\n                            [],\n                            (),\n                        ),\n                        (\n                            'myapp_my_model',\n                            'myapp_my_model__c',\n                            [],\n                            [],\n                            (),\n                        ),\n                    ],\n                )\n", "    def test_add_index(self):\n        with CaptureQueriesContext() as queries:\n            migration = Migration(\"tests\", \"0005_auto_20221226_1203\")\n            operation = AddIndex(\n                model_name=\"unicodemodel\",\n                index=models.Index(fields=[\"name\"]),\n            )\n            operation.state_forwards(\"tests\", ModelState([], {\"tests\": {}}))\n            self.assertEqual(\n                queries.queries,\n                [\n                    {\n                        \"sql\": \"CREATE UNIQUE INDEX tests_unicodemodel_name_3fbd03e_uniq ON tests_unicodemodel (name)\",\n                        \"params\": (),\n                        \"using\": \"btree\",\n                    }\n                ],\n            )\n            self.assertEqual(\n                ModelState(\n                    app_label=\"tests\",\n                    name=\"unicodemodel\",\n                    fields=[(\"id\", models.AutoField()), (\"name\", models.CharField(max_length=255))],\n                    options={\"db_table\": \"tests_unicodemodel\"},\n                    bases=(models.Model,),\n                    managers=[],\n                ),\n                migration.state,\n            )\n", "    def test_alter_table(self):\n        # We need a model with a custom table name to test this\n        with self.assertRaises(ValueError):\n            AlterModelTable('food', table='nonexistent_table').deconstruct()\n\n        # Test simple table renaming\n        with self.assertRaises(IntegrityError):\n            with transaction.atomic():\n                with atomic():\n                    Migration.from_state(\n                        initial=True,\n                        app_label='test',\n                        name='0001_initial',\n                        state=ModelState(\n                            'test',\n                            'food',\n                            [('id', models.AutoField(primary_key=True)),\n                            ('name', models.CharField(max_length=255))],\n                            {'db_table': 'fooo'},\n                            (models.Model,),\n                            [],\n                        ),\n                    ).apply ProjectState(None, None, None)\n                self.assertEqual(connection.schema_editor._constraint_names('test', column_names=['name'], index=False), {'test_food_id_key'})\n\n        food = UnicodeModel.objects.create(name='Bread')\n\n        operation = AlterModelTable('food', table='foods')\n        with transaction.atomic():\n            with atomic():\n                Migration.from_state(\n                    initial=True,\n                    app_label='test',\n                    name='0001_initial',\n                    state=ModelState(\n                        'test',\n                        'food',\n                        [('id', models.AutoField(primary_key=True)),\n                         ('name', models.CharField(max_length=255))],\n                        {'db_table': 'fooo'},\n                        (models.Model,),\n                        [],\n                    ),\n                ).apply(ProjectState(None, None, None))\n                operation.database_forwards('test', connection.schema_editor, ModelState('test', 'food', [('id', models.AutoField(primary_key=True)), ('name', models.CharField(max_length=255))], {'db_table': 'fooo'}, (models.Model,), []), ModelState('test', 'food', [('id', models.AutoField(primary_key=True)), ('name', models.CharField(max_length=255))], {'db_table': 'foods'}, (models.Model,), []))\n                self.assertEqual(connection.schema_editor._constraint_names('test', column_names=['name'], index=False), {'test_foods_id_key'})\n\n                # Test that we can add an index\n                connection.schema_editor.create_index(UnicodeModel, ['name'])\n                self.assertEqual(connection.schema_editor._constraint_names('test', column_names=['name'], index=True), {'test_foods_name_idx'})\n\n                # Test that we can add a unique constraint\n                connection.schema_editor.create_unique(UnicodeModel, ['name'])\n                self.assertEqual(connection"], "sample_1157": ["def test_rationalize_single_number():\n    # Test that rationalize correctly handles single numbers\n    assert parse_expr('1', transformations=(rationalize,)) == (NAME, 'Rational', (NUMBER, '1'))\n    assert parse_expr('-2', transformations=(rationalize,)) == (NAME, 'Rational', (NUMBER, '-2'))\n    assert parse_expr('3.14', transformations=(rationalize,)) == (NAME, 'Float')\n    assert parse_expr('3.14', transformations=(rationalize, auto_number)) == (NAME, 'Rational', (NUMBER, '3078/985'))\n", "    def test_function_exponentiation(self):\n        result = parse_expr('sin**2(x)', local_dict={})\n        expected = Function('sin')(x)**2\n        assert result == expected\n", "def test_rationalize():\n    # Test conversion of floats to Rationals\n    assert parse_expr('2.5').is_Rational\n    assert parse_expr('1.0/3').is_Rational\n    assert parse_expr('-0.5').is_Rational\n    assert parse_expr('0.5').is_Rational\n\n    # Test edge cases\n    assert parse_expr('-0').is_Rational\n    assert parse_expr('0').is_Rational\n\n    # Test conversion of floats to Rationals with implicit multiplication\n    assert parse_expr('2.5*x').is_Rational\n    assert parse_expr('1.0/3*x').is_Rational\n    assert parse_expr('-0.5*x').is_Rational\n    assert parse_expr('0.5*x').is_Rational\n\n    # Test conversion of floats to Rationals with function calls\n    assert parse_expr('sin(2.5)').is_Rational\n    assert parse_expr('sin(1.0/3)').is_Rational\n    assert parse_expr('sin(-0.5)').is_Rational\n    assert parse_expr('sin(0.5)').is_Rational\n\n    # Test conversion of floats to Rationals with implicit multiplication and function calls\n    assert parse_expr('sin(2.5*x)').is_Rational\n    assert parse_expr('sin(1.0/3*x)').is_Rational\n    assert parse_expr('sin(-0.5*x)').is_Rational\n    assert parse_expr('sin(0.5*x)').is_Rational\n\n    # Test conversion of floats to Rationals with function calls and exponents\n    assert parse_expr('sin(2.5)**2').is_Rational\n    assert parse_expr('(1.0/3)**2').is_Rational\n    assert parse_expr('(-0.5)**2').is_Rational\n    assert parse_expr('(0.5)**2').is_Rational\n\n    # Test conversion of floats to Rationals with implicit multiplication, function calls, and exponents\n    assert parse_expr('sin(2.5)*x**2').is_Rational\n    assert parse_expr('(1.0/3)*x**2').is_Rational\n    assert parse_expr('(-0.5)*x**2').is_Rational\n    assert parse_expr('(0.5)*x**2').is_Rational\n", "def test_split_symbols_with_keyword():\n    from sympy import symbols\n    x, y = symbols('x y')\n    assert parse_expr('x**2*y**2', transformations=(split_symbols,)) == [x, '**', 2, '*', y, '**', 2]\n    assert parse_expr('x**2*y**2', transformations=(split_symbols,)) == [x, '**', 2, '*', y, '**', 2]\n", "def test_parse_expr_error_message():\n    from sympy import sympify\n    from sympy.parsing.sympy_parser import parse_expr\n    # test error messages for the cases where parsing fails\n    # all of the following should raise a ValueError\n    assertRaises(ValueError, parse_expr, \"1 + \")\n    assertRaises(ValueError, parse_expr, \"1 +\")\n    assertRaises(ValueError, parse_expr, \"(1 +\")\n    assertRaises(ValueError, parse_expr, \"1 +]\")\n\n    # test specific error message\n    try:\n        parse_expr(\"x+y\")\n    except ValueError as e:\n        assert str(e) == \"Error from parse_expr with transformed code: 'x+y'\"\n\n    # test raise a TokenError on invalid syntax\n    assertRaises(TokenError, parse_expr, \"1+2+\")\n    assertRaises(TokenError, parse_expr, \"1+-2+\")\n    assertRaises(TokenError, parse_expr, \"1+2+-\")\n\n    # test that TokenError is raised on invalid syntax\n    assertRaises(TokenError, parse_expr, \"1+2-\")\n    assertRaises(TokenError, parse_expr, \"1+2/\")\n    assertRaises(TokenError, parse_expr, \"1+2**\")\n    assertRaises(TokenError, parse_expr, \"1+2^\")\n    assertRaises(TokenError, parse_expr, \"1+2%\")\n\n    # test that sympy's parser raises an error\n    assertRaises(sympify.SympifyError, parse_expr, \"-\")\n\n    # test when the string is empty\n    assertRaises(ValueError, parse_expr, \"\")\n"], "sample_947": ["compilation error", "def test_parse_nested_name():\n    parser = DefinitionParser(\"\", location=None, config=None)\n    ast = parser.parse_declaration(\"function\", \"function\")\n    assert ast.declaration.name.names == ['f', 'n', 'ested', 'n', 'a', 'm', 'e']\n", "def test_nested_name():\n    ast = parse('member', 'struct foo { int bar; };')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n    ast = parse('function', 'void foo(int bar);')\n    assert ast.name.names == ['foo']\n    assert ast.name.rooted == False\n    ast = parse('type', 'typedef int foo;')\n    assert ast.name.names == ['foo']\n    assert ast.name.rooted == False\n    ast = parse('struct', 'struct foo { int bar; };')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n    ast = parse('union', 'union foo { int bar; };')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n    ast = parse('enum', 'enum foo { bar };')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n    ast = parse('member', 'struct foo { struct bar { int baz; }; };')\n    assert ast.name.names == ['foo', 'bar', 'baz']\n    assert ast.name.rooted == False\n    ast = parse('function', 'void foo(int bar(int baz));')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n    ast = parse('type', 'typedef void (*foo)(int);')\n    assert ast.name.names == ['foo']\n    assert ast.name.rooted == False\n    ast = parse('member', 'struct foo { struct bar { struct baz { int qux; }; }; };')\n    assert ast.name.names == ['foo', 'bar', 'baz', 'qux']\n    assert ast.name.rooted == False\n    ast = parse('function', 'void foo(int bar(int baz(int qux)));')\n    assert ast.name.names == ['foo', 'bar']\n    assert ast.name.rooted == False\n", "compilation error", "compilation error"], "sample_874": ["    def test_get_indices(self):\n        \"\"\"Test `get_indices` method in BiclusterMixin.\"\"\"\n        bicluster = BiclusterMixin()\n        with pytest.raises(NotImplementedError):\n            bicluster.get_indices(0)\n", "def test_BaseEstimator_set_params_fails_with_invalid_input():\n    # Test that set_params fails with invalid input\n    class InvalidEstimator(BaseEstimator):\n            pass\n\n    with pytest.raises(ValueError) as e:\n        InvalidEstimator().set_params(non_existent_param=1)\n    assert \"Invalid parameter 'non_existent_param' for estimator\" in str(e.value)\n\n    class InvalidEstimator(BaseEstimator):\n            self.valid_param = None\n\n    with pytest.raises(ValueError) as e:\n        InvalidEstimator().set_params(non_existent_param=1)\n    assert \"Invalid parameter 'non_existent_param' for estimator\" in str(e.value)\n\n    class InvalidEstimator(BaseEstimator):\n            self.valid_param = None\n\n            return [\"valid_param\"]\n\n    with pytest.raises(ValueError) as e:\n        InvalidEstimator().set_params(non_existent_param=1)\n    assert \"Invalid parameter 'non_existent_param' for estimator\" in str(e.value)\n\n    class InvalidEstimator(BaseEstimator):\n            self.valid_param = None\n\n            return [\"valid_param\"]\n\n            self.valid_param = params[\"valid_param\"]\n            return self\n\n    with pytest.raises(ValueError) as e:\n        InvalidEstimator().set_params(non_existent_param=1)\n    assert \"Invalid parameter 'non_existent_param' for estimator\" in str(e.value)\n", "    def test_clone(self):\n        estimator = StepSelector(step=2)\n        clone = clone(estimator)\n        assert isinstance(clone, StepSelector)\n        assert clone.step == 2\n", "def test_BaseEstimator_clone():\n    \"\"\"Test BaseEstimator.clone()\"\"\"\n    est = StepSelector(step=3)\n    clone = est.clone()\n    assert clone.__class__ == est.__class__\n    assert clone.step == 3\n    assert clone.n_features_in_ is None\n    assert clone.feature_names_in_ is None\n    clone = clone.set_params(step=2)\n    assert clone.step == 2\n", "def test_validate_data_no_reset():\n    \"\"\"Test that validate_data doesn't set n_features_in_ when not necessary.\"\"\"\n    selector = StepSelector(step=2)\n    X = np.arange(10).reshape(2, 5)\n    selector._validate_data(X, reset=False)\n    assert selector.n_features_in_ is None\n\n    selector = StepSelector(step=2)\n    selector._validate_data(X, reset=True)\n    assert selector.n_features_in_ == 5\n"], "sample_1005": ["def test_latex_finitefield():\n    # Test FiniteField\n    f = FiniteField(mod=7)\n    assert latex(f) == r'\\mathbb{F}_{7}'\n\n    # Test FiniteField with modulus as a symbol\n    f = FiniteField(mod=k)\n    assert latex(f) == r'\\mathbb{F}_{%s}' % latex(k)\n\n    # Test FiniteField with multiple generators\n    f = FiniteField(mod=7, n=2)\n    assert latex(f) == r'\\mathbb{F}_{7}^{2}'\n\n    # Test FiniteField with multiple generators and modulus as a symbol\n    f = FiniteField(mod=k, n=2)\n    assert latex(f) == r'\\mathbb{F}_{%s}^{2}' % latex(k)\n\n    # Test FiniteField with multiple generators and modulus as a symbol\n    # with multiple generators and modulus as a symbol\n    f = FiniteField(mod=k, n=2, gen=[a, b])\n    assert latex(f) == r'\\mathbb{F}_{%s}^{2}\\left[%s, %s\\right]' % \\\n        (latex(k), latex(a), latex(b))\n\n    # Test FiniteField with multiple generators and modulus as a symbol\n    # with multiple generators and modulus as a symbol and itex\n    f = FiniteField(mod=k, n=2, gen=[a, b], itex=True)\n    assert latex(f, itex=True) == r'$$\\mathbb{F}_{%s}^{2}\\left[%s, %s\\right]$$' % \\\n        (latex(k), latex(a), latex(b))\n\n    # Test FiniteField with multiple generators and modulus as a symbol and\n    # mode\n    f = FiniteField(mod=k, n=2, gen=[a, b], mode='inline')\n    assert latex(f, mode='inline') == r'$\\mathbb{F}_{%s}^{2}\\left[%s, %s\\right]}' % \\\n        (latex(k), latex(a), latex(b))\n", "    def test_invert_trigonometric_functions_latex(self):\n        printer = LatexPrinter()\n        assert printer._hprint_Function('asin') == r'as'\n        assert printer._hprint_Function('acos') == r'ac'\n        assert printer._hprint_Function('atan') == r'at'\n        assert printer._hprint_Function('acot') == r'acot'\n", "def test_MatMul():\n    M = Matrix([[1, 2], [3, 4]])\n    latex_expr = latex(M)\n    assert latex_expr == r'\\begin{pmatrix}1 & 2\\\\3 & 4\\end{pmatrix}'\n\n    M2 = Matrix([[5, 6], [7, 8]])\n    M_latex = latex(M * M2)\n    assert M_latex == r'\\begin{pmatrix}19 & 22\\\\43 & 50\\end{pmatrix}'\n\n    M3 = Matrix([[9, 10], [11, 12]])\n    M_latex = latex(M * M2 * M3)\n    assert M_latex == r'\\begin{pmatrix}247 & 286\\\\559 & 646\\end{pmatrix}'\n\n    assert latex(M + M2) == r'1\\cdot\\begin{pmatrix}1 & 2\\\\3 & 4\\end{pmatrix} + 1\\cdot\\begin{pmatrix}5 & 6\\\\7 & 8\\end{pmatrix}'\n    latex_printer = LatexPrinter()\n    latex_expr = latex_printer._print_MatAdd(M + M2)\n    assert latex_expr == r'  + \\begin{pmatrix}5 & 6\\\\7 & 8\\end{pmatrix}'\n\n    latex_expr = latex(M * M2 * M3)\n    assert latex_expr == r'\\begin{pmatrix}247 & 286\\\\559 & 646\\end{pmatrix}'\n\n    latex_expr = latex(M + M2 * M3)\n    assert latex_expr == r'\\begin{pmatrix}1 & 2\\\\3 & 4\\end{pmatrix} + 1\\cdot\\begin{pmatrix}69 & 82\\\\155 & 184\\end{pmatrix}'\n\n    latex_expr = latex(M2 * M3 + M)\n    assert latex_expr == r'1\\cdot\\begin{pmatrix}69 & 82\\\\155 & 184\\end{pmatrix} + 1\\cdot\\begin{pmatrix", "def test_latex():\n    assert latex(Tensor([1, 2, 3])) == r\"\\left[ \\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\end{array} \\right]\"\n    assert latex(Matrix([[1, 2], [3, 4]])) == r\"\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\end{array} \\right]\"\n    assert latex(Matrix([[1, 2], [3, 4]]).transpose()) == r\"\\left[ \\begin{array}{cc} 1 & 3 \\\\ 2 & 4 \\end{array} \\right]\"\n    assert latex(Matrix([[1, 2], [3, 4]]).T) == r\"\\left[ \\begin{array}{cc} 1 & 3 \\\\ 2 & 4 \\end{array} \\right]\"\n    assert latex(Tensor([1, 2, 3])) == r\"\\left[ \\begin{array}{c} 1 \\\\ 2 \\\\ 3 \\end{array} \\right]\"\n    assert latex(Operator(\"A\")) == r\"A\"\n    assert latex(TensorProductForm(Operator(\"A\"), Operator(\"B\"))) == r\"A \\otimes B\"\n    assert latex(TensorProductForm(Operator(\"A\"), Operator(\"B\"), Operator(\"C\"))) == r\"A \\otimes B \\otimes", "def test_latex():\n    # Test with a custom latex name\n    assert latex(Operator('op')) == r'\\operatorname{op}'\n    \n    # Test with a recognized latex name\n    assert latex(Chi('x')) == r'\\operatorname{Chi}\\left(x\\right)'\n    \n    # Test with a non-recognized latex name\n    assert latex(Function('fun')) == r'\\operatorname{fun}'\n\n    # Test with a MatrixSymbol\n    assert latex(MatrixSymbol('M', 2, 2)) == r'M'\n\n    # Test with a Quaternion\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q) == r'1 + 2 i + 3 j + 4 k'\n\n    # Test with a Quaternion with a specific form\n    q = Quaternion(1, 2, 3, 4)\n    assert latex(q, fold_func_brackets = True) == r'1 + 2 i + 3 j + 4 k'\n\n    # Test with a Custom latex name\n    class CustomFunc(Function):\n        func = lambda x: x + 1\n    assert latex(CustomFunc('f')) == r'\\operatorname{f}'\n\n    # Test with an IndexedBase\n    assert latex(IndexedBase('x', (1, 2))) == r'x_{1, 2}'\n\n    # Test with a LambertW\n    assert latex(LambertW()) == r'\\operatorname{W}'\n\n    # Test with a LambertW with custom name\n    assert latex(LambertW('W')) == r'\\operatorname{W}'\n\n    # Test with a Subs\n    assert latex(Subs('x', 'y', 'z')) == r'\\left. z \\right|_{\\substack{ y = z }}'\n\n    # Test with a MatrixAdd\n    m1 = Matrix([[1, 2], [3, 4]])\n    m2 = Matrix([[5, 6], [7, 8]])\n    assert latex(m1 + m2) == r'6 8\\\\8 10'\n\n    # Test with a Mul with negative number\n    assert latex(-2) == r'-2'\n"], "sample_1153": ["def test_unpolarify_periodic_argument():\n    x = symbols('x')\n    arg = periodic_argument(exp_polar(2*pi*x), 2*pi)\n    assert unpolarify(arg) == x\n", "def test_polarify():\n    x, y = symbols('x y')\n    expr1 = exp_polar(x) + y\n    result1, subs1 = polarify(expr1)\n    assert result1 == (exp_polar(x) + y, {x: x})\n    assert subs1 == {x: x}\n\n    expr2 = exp_polar(x) * y\n    result2, subs2 = polarify(expr2)\n    assert result2 == (exp_polar(x) * y, {x: x})\n    assert subs2 == {x: x}\n\n    expr3 = exp_polar(x) / y\n    result3, subs3 = polarify(expr3)\n    assert result3 == (exp_polar(x) / y, {x: x})\n    assert subs3 == {x: x}\n\n    expr4 = exp(x) + y\n    result4, subs4 = polarify(expr4)\n    assert result4 == (exp(x) + y, {x: x})\n    assert subs4 == {x: x}\n\n    expr5 = exp_polar(3*x + 2*y)\n    result5, subs5 = polarify(expr5)\n    assert result5 == (exp_polar(3*x + 2*y), {x: x, y: y})\n    assert subs5 == {x: x, y: y}\n", "def test_abs_unpolarify():\n    from sympy import unpolarify\n    x = symbols('x')\n    assert unpolarify(Abs(x + I)) == Abs(x + I)\n    assert unpolarify(Abs(x + I).expand()) == Abs(x + I)\n    assert unpolarify(Abs(x + I).as_real_imag()[0]) == Abs(x + I).as_real_imag()[0]\n    assert unpolarify(Abs(x + I).as_real_imag()[1]) == Abs(x + I).as_real_imag()[1]\n    assert unpolarify(Abs(x + I) + I) == Abs(x + I) + I\n    assert unpolarify(Abs(x + I) * x) == Abs(x + I) * x\n    assert unpolarify(Abs(x + I) * (x + I)) == Abs(x + I) * (x + I)\n    assert unpolarify(Abs(x + I) * (x + I) * x) == Abs(x + I) * (x + I) * x\n    assert unpolarify(Abs(x + I) * (x + I) * x**2) == Abs(x + I) * (x + I) * x**2\n    assert unpolarify(Abs(x + I) * (x + I) * x**3) == Abs(x + I) * (x + I) * x**3\n    assert unpolarify(Abs(x + I) * x + I) == Abs(x + I) * x + I\n    assert unpolarify(x * Abs(x + I)) == x * Abs(x + I)\n    assert unpolarify(I * Abs(x + I)) == I * Abs(x + I)\n    assert unpolarify(Abs(x + I) * x * I) == Abs(x + I) * x * I\n    assert unpolarify(Abs(x + I) * (x + I) * I) == Abs(x + I) * (x + I) * I\n    assert unpolarify(x * Abs(x + I) * I) == x * Abs(x + I) * I\n    assert unpolarify(x * I * Abs(x + I)) == x * I * Abs(x + I)\n    assert unpolarify(Abs(x + I) * I * x) == Abs(x + I) * I", "def test_abs():\n    x = symbols('x', real=True)\n    assert N_equals(Abs(-x), x)\n    assert N_equals(Abs(x**2), x**2)\n    assert N_equals(Abs(-x**2), x**2)\n    assert N_equals(Abs(4*x + 2*I), sqrt(16*x**2 + 4))\n    assert N_equals(Abs(I), 1)\n    assert N_equals(Abs(-I), 1)\n    assert N_equals(Abs(x + 2*I), sqrt(x**2 + 4))\n    assert N_equals(Abs(2*I), 2)\n    assert N_equals(Abs(I + 2), sqrt(5))\n    assert N_equals(Abs(-I - 2), sqrt(5))\n    assert N_equals(Abs(2 - I), sqrt(5))\n    assert N_equals(Abs(-2 + I), sqrt(5))\n", "def test_unpolarify_polar_lift_with_expr():\n    x = Symbol('x', polar=True)\n    # Test that polar_lift is not applied to other expressions\n    assert unpolarify(2*x) == 2*x\n    assert unpolarify(2*x + 3) == 2*x + 3\n    assert unpolarify(exp(x)) == exp(x)\n    assert unpolarify(x**2) == x**2\n    assert unpolarify(sin(x)) == sin(x)\n"], "sample_924": ["def check_type_object(name, input, idDict, output=None, key=None, asTextOutput=None):\n    if output is None:\n        output = input\n    # First, check without semicolon\n    _check(name, input, idDict, output, key, asTextOutput)\n    # Second, check with semicolon\n    _check(name, input + ' ;', idDict, output + ';', key,\n           asTextOutput + ';' if asTextOutput is not None else None)\n\n", "def test_class_declaration():\n    idDict = {1: \"C1\", 2: \"C2\"}\n    check(\"class\", r\"::class Class\",\n          idDict, r\"::class Class C2\")\n    idDict = {1: \"C1\", 2: \"C2\", 3: \"C3\"}\n    check(\"class\", r\"::class Class\",\n          idDict, r\"::class Class C3\")\n", "def test_param_pack() -> None:\n    idDict = {\n        3: 't',\n        4: 'E',\n    }\n    input = r'template <template <T> void f(T x);'\n    output = r'template <template <T> void f(T x)'\n    check('templateParam', input, idDict, output)\n", "def test_template_parameter_list():\n    input = \"template <typename T>\"\n    output = \"template<template<1>\"\n    idDict = {1: '1'}\n    _check('templateParam', input, idDict, output, key='T', asTextOutput=None)\n", "compilation error"], "sample_308": ["    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.datetime_obj = datetime(2003, 10, 7, 11, 39, tzinfo=timezone.get_fixed_timezone(3600))\n        self.time_obj = datetime(11, 39, 0, tzinfo=timezone.get_fixed_timezone(3600))\n        self.date_obj = date(2003, 10, 7)\n", "    def setUp(self):\n        self.timezone = get_default_timezone()\n        self.datetime_obj = datetime.now(self.timezone)\n        self.time_obj = self.datetime_obj.time()\n        self.date_obj = self.datetime_obj.date()\n", "    def setUp(self):\n        self._orig_lang = translation.get_language()\n        translation.activate('en-us')\n        self.now = datetime.now()\n        self.date = date.today()\n        self.dt = make_aware(self.now, timezone=get_default_timezone())\n", "    def test_time_format_a(self):\n        dt = datetime.now(tz=get_default_timezone())\n        tf = TimeFormat(dt)\n        self.assertEqual(tf.a(), 'a.m.')\n\n        # Test pm\n        dt = datetime.now(tz=get_default_timezone())\n        dt.hour = 13\n        tf = TimeFormat(dt)\n        self.assertEqual(tf.a(), 'p.m.')\n"], "sample_232": ["    def test_deconstruct(self):\n        field = JSONField()\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(name, 'json_field')\n        self.assertEqual(path, 'django.db.models.fields.json.JSONField')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {})\n", "    def test_deconstruct(self):\n        field = JSONField(encoder=json.dumps, decoder=CustomJSONDecoder)\n        name, path, args, kwargs = field.deconstruct()\n        self.assertEqual(name, 'json_field')\n        self.assertEqual(path, 'models.fields.json.JSONField')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {\n            'encoder': json.dumps,\n            'decoder': CustomJSONDecoder,\n        })\n", "    def test_invalid_json_value(self):\n        json_field = JSONModel.json_field\n        with self.assertRaises(ValidationError):\n            json_field.validate('not a valid json')\n", "    def test_key_transform_preprocess_lhs(self):\n        json_field = JSONModel()\n        field_value = json_field.json_value = {'key': 'value'}\n        field_value.save()\n        key_transform = KeyTransform('key', json_field)\n\n        # Test case when key is an integer\n        key_transform_with_int_key = KeyTransform(0, json_field)\n        self.assertEqual(key_transform_with_int_key.preprocess_lhs(self.connection, None)[0], 'json_value')\n\n        # Test case when key is a string\n        key_transform_with_str_key = KeyTransform('key', json_field)\n        self.assertEqual(key_transform_with_str_key.preprocess_lhs(self.connection, None)[0], 'json_value')\n", "    def test_custom_decoder(self):\n        model = JSONModel(json_data='{\"key\": \"value\"}')\n        model.save()\n        with CaptureQueriesContext() as queries:\n            json_object = model.json_data\n        self.assertEqual(json_object, {\"key\": \"value\"})\n        self.assertEqual(len(queries), 1)\n"], "sample_610": ["def test_parse_iso8601_like_hour_minute(string, expected):\n    result = parse_iso8601_like(string)\n    assert result == expected\n", "def test_parse_iso8601_like_fractions(string, expected):\n    date_type = cftime.datetime\n    parsed = parse_iso8601_like(string)\n    assert parsed == expected\n", "def test_to_datetimeindex_non_standard_calendar(self):\n    if not has_cftime:\n        pytest.skip(\"cftime not installed\")\n\n    year = cftime.datetime(2000, 1, 1, calendar=\"noleap\")\n    index = CFTimeIndex([year, year + cftime.timedelta(days=1)], name=\"test_index\")\n    assert isinstance(index.to_datetimeindex(), pd.DatetimeIndex)\n    assert index.to_datetimeindex().dtype == \"datetime64[ns]\"\n    assert index.to_datetimeindex().name == \"test_index\"\n\n    # test with an index that cannot be converted\n    with pytest.raises(ValueError):\n        index = CFTimeIndex([cftime.datetime(2000, 13, 1)], name=\"test_index\")\n        index.to_datetimeindex()\n\n    # test with an index that cannot be converted because of out-of-bounds\n    with pytest.raises(ValueError):\n        index = CFTimeIndex([cftime.datetime(1970, 1, 1, calendar=\"noleap\"), cftime.datetime(1969, 12, 31)], name=\"test_index\")\n        index.to_datetimeindex()\n", "def test_parse_iso8601_like_multiple_matches():\n    \"\"\"Test that the parse_iso8601_like function returns one of the first matches.\"\"\"\n    # Test that we only return the first match if there are multiple matches\n    string = \"1999-01-01T12:34:56Z, 1999-02-01T12:34:56Z\"\n    with pytest.raises(ValueError):\n        parse_iso8601_like(string)\n\n    # Test that we return a match for a string with multiple, consecutive matches\n    string = \"1999-01-01, 1999-01-01, 1999-02-01\"\n    match = re.match(_EXTENDED_PATTERN, string)\n    assert match.groupdict() == {\"year\": \"1999\", \"month\": \"01\", \"day\": \"01\"}\n\n    # Test that we return a match for a string with a single match\n    string = \"1999-01-01 12:34:56\"\n    match = re.match(_EXTENDED_PATTERN, string)\n    assert match.groupdict() == {\"year\": \"1999\", \"month\": \"01\", \"day\": \"01\"}\n", "def test_parse_iso8601_like(date_like_string, expected):\n    # Use a custom datetime type to test against a non-standard calendar\n    if has_cftime:\n        from cftime import DatetimeNoLeap\n\n        date_type = DatetimeNoLeap\n    else:\n        date_type = datetime\n\n    assert_equal(\n        parse_iso8601_like(date_like_string),\n        expected,\n        err_msg=f\"Failed to parse {date_like_string} with expected date type {type(expected)}\",\n    )\n"], "sample_455": ["    def test_create_and_remove_sql(self):\n        with self.assertRaises(NotImplementedError):\n            UniqueConstraint(fields=[\"field1\", \"field2\"], name=\"test_constraint\").create_sql(models.Product, None)\n        with self.assertRaises(NotImplementedError):\n            UniqueConstraint(fields=[\"field1\", \"field2\"], name=\"test_constraint\").remove_sql(models.Product, None)\n", "    def test_check_constraint_deconstruct(self):\n        constraint = CheckConstraint(check=Q(a=1), name='my_constraint')\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.CheckConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'check': Q(a=1), 'name': 'my_constraint'})\n", "    def test_check_constraint_create_sql(self):\n        with mock.patch('django.db.models.sql.compiler.SQLCompiler') as mock_compiler:\n            model = models.Model()\n            constraint = CheckConstraint(check=Q(field__gt=1), name='test_constraint')\n            constraint.create_sql(model, mock_compiler.return_value)\n            mock_compiler.return_value.connection.execute.assert_called_once_with(mock.ANY)\n", "    def test_unique_constraint_with_condition(self):\n        constraint = UniqueConstraint(\n            name=\"test_constraint\",\n            condition=Q(price__gt=10) | Q(price__lt=10),\n            fields=[\"price\"],\n        )\n        with self.assertRaises(ValidationError):\n            UniqueConstraintConditionProduct.objects.create(price=10)\n", "    def test_check_constraint_invalid_instance(self):\n        product = Product(name='Test', price=10.99)\n        product.save()\n        constraint = CheckConstraint(\n            check=Q(name='Test'),\n            name='test_constraint',\n            violation_error_code='invalid_instance',\n            violation_error_message='Instance name cannot be Test.'\n        )\n        with self.assertRaises(ValidationError):\n            constraint.validate(Product, product)\n"], "sample_576": ["    def test_pair_empty(self):\n        p = Plot()\n        p.pair(x=[\"x1\", \"x2\"], y=[\"y1\", \"y2\"])\n        expected_spec = {\n            \"variables\": {\"x0\": \"x1\", \"x1\": \"x2\", \"y0\": \"y1\", \"y1\": \"y2\"},\n            \"structure\": {\"x\": [\"x0\", \"x1\"], \"y\": [\"y0\", \"y1\"]},\n            \"cross\": True,\n            \"wrap\": None,\n        }\n        assert p._pair_spec == expected_spec\n", "    def test_pair_empty(self):\n        p = Plot()\n        p.pair()\n        assert p._pair_spec == {\"variables\": {}, \"structure\": {}, \"cross\": True, \"wrap\": None}\n", "    def test_add_no_transforms(self):\n        mark = Mark()\n        p = Plot().add(mark)\n        assert isinstance(p, Plot)\n        assert mark in p._layers\n", "    def test_add_layer_multiple_layers(self):\n\n        mark = Mark()\n        mark._grouping_props = [\"x\", \"y\"]\n\n        p = Plot(data=pd.DataFrame({\"x\": [1, 2], \"y\": [3, 4]}))\n        p = p.add(mark).add(mark)\n\n        assert len(p._layers) == 2\n", "    def test_init_defaults(self):\n        p = Plot()\n        with theme_context({\"figure.figsize\": (1, 1)}):\n            p.theme()\n        assert p._theme == {\"figure.figsize\": (1, 1)}\n"], "sample_724": ["def test_empty_array():\n    \"\"\"Test imputation for empty arrays.\"\"\"\n\n    X = np.empty((5, 5))\n    imputer = Imputer(strategy=\"mean\", missing_values=0, axis=0)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans, X)\n\n    imputer = Imputer(strategy=\"median\", missing_values=0, axis=1)\n    assert_raises(ValueError, imputer.fit_transform, X.copy().transpose())\n\n    imputer = Imputer(strategy=\"most_frequent\", missing_values=0, axis=0)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans, X)\n\n    # Test with sparse array\n    X = sparse.csr_matrix(np.ones((5, 5)))\n    imputer = Imputer(strategy=\"mean\", missing_values=0, axis=0)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans.toarray(), X.toarray())\n\n    imputer = Imputer(strategy=\"median\", missing_values=0, axis=1)\n    assert_raises(ValueError, imputer.fit_transform, X.copy().transpose())\n\n    imputer = Imputer(strategy=\"most_frequent\", missing_values=0, axis=0)\n    X_trans = imputer.fit_transform(X)\n    assert_array_equal(X_trans.toarray(), X.toarray())\n\n    # Test with empty array and axis 1\n    X = np.empty((5, 5))\n    imputer = Imputer(strategy=\"mean\", missing_values=0, axis=1)\n    assert_raises(ValueError, imputer.fit_transform, X)\n\n    imputer = Imputer(strategy=\"median\", missing_values=0, axis=1)\n    assert_raises(ValueError, imputer.fit_transform, X.copy())\n\n    imputer = Imputer(strategy=\"most_frequent\", missing_values=0, axis=1)\n    assert_raises(ValueError, imputer.fit_transform, X)\n", "def test_statistics_with_zeros_and_nans():\n    \"\"\"Test that the imputer is able to impute from a mix of zeros and nans\"\"\"\n    X = np.array([[1, np.nan, 3],\n                  [0, 0, 0],\n                  [np.nan, 2, 4]])\n    X_true = np.array([[1, 0, 3],\n                       [0, 0, 0],\n                       [np.nan, 2, 4]])\n    statistics = np.array([1, 0, 3])\n\n    _check_statistics(X, X_true, \"mean\", statistics, 0)\n    _check_statistics(X, X_true, \"median\", statistics, 0)\n    _check_statistics(X, X_true, \"most_frequent\", statistics, 0)\n\n    X = np.array([[1, np.nan, 3],\n                  [0, 0, 0],\n                  [np.nan, 2, 4]])\n    X_true = np.array([[1, 0, 3],\n                       [0, 0, 0],\n                       [np.nan, 2, 4]])\n    statistics = np.array([1, 0, 3])\n\n    _check_statistics(X, X_true, \"mean\", statistics, \"NaN\")\n    _check_statistics(X, X_true, \"median\", statistics, \"NaN\")\n    _check_statistics(X, X_true, \"most_frequent\", statistics, \"NaN\")\n", "def test_imputer_column_elimination():\n    \"\"\"Test the elimination of columns without observed values.\"\"\"\n    rng = np.random.RandomState(0)\n    X = np.array([[1, np.nan, 4, np.nan],\n                  [2, 2, 2, 2],\n                  [np.nan, np.nan, 4, 4]])\n\n    imputer = Imputer(strategy=\"mean\", missing_values=np.nan, axis=0)\n    imputer.fit(X)\n    X_imputed = imputer.transform(X)\n\n    expected_X = np.array([[2, 2, 4, 4],\n                          [2, 2, 2, 2],\n                          [2, 2, 2, 2]])\n    assert_array_equal(X_imputed, expected_X)\n", "def test_imputer_axis_out_of_range():\n    \"\"\"Test what happens when the axis is out of range.\n\n    Check that:\n        - it raises a ValueError\"\"\"\n\n    X = np.array([[1, 2, np.nan], [np.nan, 4, 5]])\n    err_msg = \"Can only impute missing values on axis 0 and 1, got axis = {0}\"\n\n    # Negative axis\n    imputer = Imputer(strategy=\"mean\", axis=-1)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.transform, X)\n\n    # Axis greater than 1\n    imputer = Imputer(strategy=\"mean\", axis=2)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.transform, X)\n\n    # Axis less than 0\n    imputer = Imputer(strategy=\"mean\", axis=-2)\n    assert_raises(ValueError, imputer.fit, X)\n    assert_raises(ValueError, imputer.transform, X)\n", "def test_statistics_with_zero_statistics():\n    \"\"\"Test the case when statistics_ contains only zeros.\n\n    This test checks the behavior of imputation with a dataset that has only\n    zeros in the statistics_ attribute.\"\"\"\n\n    X = np.array([[1., 2., 0.], [0., 4., 5.]])\n    X_true = np.array([[1., 2., 2.]])\n    strategy = \"mean\"\n    missing_values = 0\n    statistics = np.array([0., 0., 0.])\n\n    _check_statistics(X, X_true, strategy, statistics, missing_values)\n"], "sample_242": ["    def test_year_lookup_bounds_for_datetime_field(self):\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field.return_value = ('2010', '2020')\n        lookup = YearExact(Lookup(), Value(datetime(2010, 1, 1)))\n        self.assertEqual(lookup.year_lookup_bounds(connection, 2010), ('2010', '2020'))\n", "    def test_YearLookup_get_bound_params(self):\n        lookup = YearExact(lhs=DateTimeField(), rhs=2022)\n        year_lookup = YearExact(lhs=lookup.lhs, rhs=lookup.rhs)\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field = lambda year: (1, 2)\n        with self.assertRaises(NotImplementedError):\n            year_lookup.get_bound_params(*year_lookup.year_lookup_bounds(connection, lookup.rhs))\n", "    def test_transform__bilateral_transform(self):\n        field = DateTimeField()\n        lookup = Transform(field, datetime.now())\n        self.assertIsInstance(lookup.bilateral_transforms[0], type(lookup))\n", "    def test_year_lookup_bounds(self):\n        year = 2022\n        lookup = YearExact(lhs=DateTimeField(), rhs=year)\n        connection = mock.Mock()\n        connection.ops.year_lookup_bounds_for_datetime_field.return_value = ('start', 'finish')\n        result = lookup.as_sql(None, connection)\n        connection.ops.year_lookup_bounds_for_datetime_field.assert_called_once_with(year)\n        self.assertEqual(result, ('start finish', ['start', 'finish']))\n", "    def test_exact_lookup_with_non_boolean_rhs(self):\n        from django.db.models import Value\n        from .models import CustomLookup\n        lookup = CustomLookup(lhs=DateTimeField(), rhs=Value('2022-01-01 12:00:00'))\n        with self.assertRaises(ValueError):\n            lookup.as_sql(mock.Mock(), mock.Mock())\n"], "sample_842": ["def test_clone(estimator):\n    estimator_copy = clone(estimator)\n    assert isinstance(estimator_copy, type(estimator))\n    assert estimator_copy.get_params(deep=True) == estimator.get_params(deep=True)\n", "def test_MetaEstimatorMixin():\n    class DummyMetaEstimator(MetaEstimatorMixin):\n        pass\n\n    class DummyEstimator:\n        pass\n\n    estimator = DummyEstimator()\n    with pytest.raises(TypeError):\n        MetaEstimatorMixin._required_parameters  # type: ignore\n\n    meta_estimator = DummyMetaEstimator()\n    with pytest.raises(TypeError):\n        meta_estimator._required_parameters  # type: ignore\n\n    clone(estimator)\n    clone(meta_estimator)\n\n    meta_estimator.estimator = estimator\n    assert MetaEstimatorMixin._required_parameters == [\"estimator\"]\n\n    with pytest.raises(TypeError):\n        clone(meta_estimator).estimator\n\n    # Test that the required parameters are not set on the estimator\n    new_estimator = clone(estimator).set_params(estimator=clone(estimator))\n    assert estimator is not new_estimator\n    assert new_estimator.estimator is clone(estimator)\n", "def test_kernel_clone(kernel):\n    # Check if the cloned kernel is equal to the original kernel\n    cloned_kernel = clone(kernel)\n    assert isinstance(cloned_kernel, type(kernel))\n    assert hasattr(cloned_kernel, \"get_params\")\n    assert hasattr(cloned_kernel, \"set_params\")\n    assert hasattr(cloned_kernel, \"get_support\")\n    assert hasattr(cloned_kernel, \"support\")\n    assert hasattr(cloned_kernel, \"n_features_in\")\n    assert hasattr(cloned_kernel, \"n_features_out\")\n    assert hasattr(cloned_kernel, \"_n_features\")\n    assert hasattr(cloned_kernel, \"_n_outputs\")\n    assert hasattr(cloned_kernel, \"_get_tag\")\n    assert hasattr(cloned_kernel, \"_more_tags\")\n    assert hasattr(cloned_kernel, \"scores\")\n    assert hasattr(cloned_kernel, \"decision_function\")\n    assert hasattr(cloned_kernel, \"predict\")\n    assert hasattr(cloned_kernel, \"predict_log_proba\")\n    assert hasattr(cloned_kernel, \"predict_proba\")\n    assert hasattr(cloned_kernel, \"transform\")\n    assert hasattr(cloned_kernel, \"inverse_transform\")\n    assert hasattr(cloned_kernel, \"fit\")\n    assert hasattr(cloned_kernel, \"fit_transform\")\n    assert hasattr(cloned_kernel, \"get_params\")\n    assert hasattr(cloned_kernel, \"set_params\")\n\n    # Check if the cloned kernel has the same parameters as the original kernel\n    params = kernel.get_params()\n    cloned_params = cloned_kernel.get_params()\n    assert params == cloned_params\n\n    # Check if the cloned kernel has the same score function as the original kernel\n    assert callable(cloned_kernel.score)\n    assert callable(kernel.score)\n\n    # Check if the cloned kernel has the same predict function as the original kernel\n    assert callable(cloned_kernel.predict)\n    assert callable(kernel.predict)\n\n    # Check if the cloned kernel has the same fit function as the original kernel\n    assert callable(cloned_kernel.fit)\n    assert callable(kernel.fit)\n", "def test_ClusterMixin_fit_predict():\n    \"\"\"Test ClusterMixin's fit_predict\"\"\"\n    from sklearn.cluster import KMeans\n    km = KMeans(n_clusters=2, random_state=0)\n    X = np.array([[1, 2], [1, 4], [1, 5], [10, 2], [10, 4], [10, 5]])\n    labels = km.fit_predict(X)\n    assert_array_equal(labels, [0, 0, 0, 1, 1, 1])\n    km.fit_predict(X, np.array([0, 0, 0, 1, 1, 1]))  # Test with y argument\n", "def test_clone_estimator():\n    # Test that the clone function works correctly for various types of estimators\n    # including those with nested objects and parameters with default values.\n    from sklearn.linear_model import LogisticRegression, Ridge\n    from sklearn.preprocessing import StandardScaler\n\n    # Test 1: Clone an estimator with a simple object\n    estimator = LogisticRegression()\n    cloned = clone(estimator)\n    assert isinstance(cloned, LogisticRegression)\n    assert estimator == cloned\n\n    # Test 2: Clone an estimator with a nested object\n    estimator = LogisticRegression()\n    estimator.set_params(solver='lbfgs')\n    cloned = clone(estimator)\n    assert isinstance(cloned, LogisticRegression)\n    assert estimator == cloned\n\n    # Test 3: Clone an estimator with a parameter that has a default value\n    estimator = LogisticRegression()\n    estimator.set_params(solver='lbfgs')\n    cloned = clone(estimator)\n    assert isinstance(cloned, LogisticRegression)\n    assert estimator == cloned\n\n    # Test 4: Clone an estimator with a list or tuple of estimators\n    estimator = (LogisticRegression(), Ridge())\n    cloned = clone(estimator)\n    assert isinstance(cloned, tuple)\n    assert all(isinstance(c, LogisticRegression) or isinstance(c, Ridge) for c in cloned)\n\n    # Test 5: Clone an estimator with a set of estimators\n    estimator = {LogisticRegression(), Ridge()}\n    with pytest.raises(TypeError):\n        clone(estimator)\n\n    # Test 6: Clone an estimator with a frozenset of estimators\n    estimator = frozenset([LogisticRegression(), Ridge()])\n    cloned = clone(estimator)\n    assert isinstance(cloned, frozenset)\n    assert all(isinstance(c, LogisticRegression) or isinstance(c, Ridge) for c in cloned)\n\n    # Test 7: Clone an estimator with a dictionary\n    with pytest.raises(TypeError):\n        clone({'a': LogisticRegression()})\n"], "sample_1026": ["def test_imp_namespace_dict():\n    from sympy.abc import x\n    from sympy.utilities.lambdify import implemented_function, _imp_namespace\n\n    f = implemented_function('f', lambda x: x+1)\n    g = implemented_function('g', lambda x: x*10)\n\n    expr = {f(x): g(x), 'non_function': 1}\n    namespace = {}\n    _imp_namespace(expr, namespace)\n\n    assert 'f' in namespace\n    assert 'g' in namespace\n    assert namespace['f'] == lambda x: x+1\n    assert namespace['g'] == lambda x: x*10\n", "def test_EvaluatorPrinter_print_unpacking():\n    from sympy import Lambda, symbols\n    from sympy.utilities.lambdify import _EvaluatorPrinter\n\n        printer = _EvaluatorPrinter()\n        funcname = 'test_func'\n        args = (expr,)\n        funcstr = printer.doprint(funcname, args, expr)\n        assert expected in funcstr\n\n    # Test basic case\n    w, x, y, z = symbols('w,x,y,z')\n    expr = Lambda((x, y), x + y)\n    expected = 'x = _0, y = _1; return (x + y)'\n    test_print_unpacking(expr, expected)\n\n    # Test nested tuples\n    expr = Lambda(((x, y), (z, w)), x + y + z + w)\n    expected = 'x = _0, y = _1; z = _2, w = _3; return (x + y + z + w)'\n    test_print_unpacking(expr, expected)\n\n    # Test with multiple arguments\n    expr = Lambda((x, y, z), x + y + z)\n    expected = 'x = _0, y = _1, z = _2; return (x + y + z)'\n    test_print_unpacking(expr, expected)\n\n    # Test with duplicate arguments\n    expr = Lambda((x, y, y), x + y)\n    expected = 'x = _0, y = _1; return (x + y)'\n    test_print_unpacking(expr, expected)\n\n    # Test with DeferredVector\n    from sympy.matrices import DeferredVector\n    expr = Lambda(DeferredVector([x, y]), x + y)\n    expected = 'x = _0, y = _1; return (x + y)'\n    test_print_unpacking(expr, expected)\n", "def testEvaluatorPrinter_preprocess():\n    printer = LambdaPrinter()\n    evp = _EvaluatorPrinter(printer, dummify=True)\n    dvector = DeferredVector([sin(x), cos(x)])\n    argstrs, expr = evp._preprocess(dvector, None)\n    assert argstrs == ['arg_0']\n    assert expr == 'sin(x)'\n", "def test_lambdastr_args_not_iterable():\n    from sympy.abc import x, y, z\n    from sympy import sin\n    assert lambdastr(x, sin(x)) == \"lambda x: sin(x)\"\n    assert lambdastr(x, sin(x**2)) == \"lambda x: sin(x**2)\"\n    assert lambdastr(x, sin(x + y)) == \"lambda x: sin(x + y)\"\n    assert lambdastr(x, sin(x + y + z)) == \"lambda x: sin(x + y + z)\"\n    assert lambdastr(x, sin(x + y + z), dummify=True) == \"lambda x: sin(_0 + _1 + _2)\"\n", "def test_max_min():\n    from sympy import symbols, lambdify, Max, Min\n    from sympy.abc import x, y\n    f = lambdify((x, y), Max(x, y))\n    g = lambdify((x, y), Min(x, y))\n    assert f(1, 2) == 2\n    assert g(1, 2) == 1\n"], "sample_153": ["    def test_check_database_backends_with_custom_databases(self, check):\n        check.return_value = []\n        with self.assertRaises(ValidationError):\n            check_database_backends([{'ENGINE': 'django.db.backends.sqlite3', 'NAME': 'db.sqlite3'}, \n                                   {'ENGINE': 'django.db.backends.sqlite3', 'NAME': 'db.sqlite3'}])\n", "    def test_check_database_backends_with_non_supported_databases(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            mock_check.return_value = []\n            # Introduce a non-supported database\n            settings.DATABASES['non-supported'] = {\n                'ENGINE': 'non-supported-engine',\n            }\n            with self.assertRaises(ValueError):\n                check_database_backends(databases=['default', 'non-supported'])\n            self.assertEqual(mock_check.call_count, 2)\n", "    def test_check_model_proxy(self, mock_model_check, mock_check):\n        # Test that _check_model method is called when checking a model\n        mock_check.return_value = []\n        class TestModel(ModelBase):\n            class Meta:\n                proxy = True\n\n        TestModel.check()\n        mock_model_check.assert_called_once()\n", "    def test_check_swappable(self, mock_pg_check, mock_sqlite_check, mock_mysql_check, mock_base_check):\n        # Test if the check_swappable function raises an error when the swapped model does not exist\n        model = models.Model\n        model._meta.swapped = 'nonexistent_model'\n        with self.assertRaises(LookupError):\n            model._meta._check_swappable()\n\n        # Test if the check_swappable function raises an error when the swapped model is abstract\n        model._meta.swapped = 'app_label.abstract_model'\n        with self.assertRaises(LookupError):\n            model._meta._check_swappable()\n\n        # Test if the check_swappable function raises an error when the swapped model is not in the correct format\n        model._meta.swapped = 'invalid_swapped_model'\n        with self.assertRaises(ValueError):\n            model._meta._check_swappable()\n\n        # Test if the check_swappable function returns an error when the swapped model is not installed\n        model._meta.swapped = 'app_label.installed_model'\n        mock_installed = mock.Mock()\n        apps.get_model.return_value = mock_installed\n        with self.assertRaises(LookupError):\n            model._meta._check_swappable()\n\n        apps.get_model.return_value = None\n        model._meta.swapped = 'app_label.installed_model'\n        with self.assertRaises(LookupError):\n            model._meta._check_swappable()\n", "    def test_check_model_name_db_lookup_clashes(self):\n        # Test for E023: The model name cannot start or end with an underscore\n        # as it collides with the query lookup syntax.\n        with self.assertWarns(DeprecationWarning):\n            check_database_backends(['sqlite3'], {})\n\n        # Test for E024: The model name cannot contain double underscores as it\n        # collides with the query lookup syntax.\n        with self.assertWarns(DeprecationWarning):\n            check_database_backends(['sqlite3'], {})\n\n        # Test for non-Django backends\n        with self.assertWarns(DeprecationWarning):\n            check_database_backends(['oracle'], {})\n"], "sample_1056": ["def test_lambda_printer_pow():\n    x, y = symbols('x,y')\n    expr = x**2\n    expected = '(x**2)'\n    assert lambdarepr(expr) == expected\n", "def test_numexpr_lambda_replacement():\n    # Test that numexpr printer replaces functions correctly\n    expr1 = sin(x)\n    assert lambdarepr(expr1, printer='numexpr') == \"evaluate('sin(x)', truediv=True)\"\n    expr2 = asin(x)\n    assert lambdarepr(expr2, printer='numexpr') == \"evaluate('arcsin(x)', truediv=True)\"\n    expr3 = exp(x)\n    assert lambdarepr(expr3, printer='numexpr') == \"evaluate('exp(x)', truediv=True)\"\n    expr4 = sqrt(x)\n    assert lambdarepr(expr4, printer='numexpr') == \"evaluate('sqrt(x)', truediv=True)\"\n\n    # Test that numexpr printer raises an error for unsupported functions\n    expr5 = Matrix([x, y])\n    with raises(TypeError):\n        lambdarepr(expr5, printer='numexpr')\n    expr6 = Interval(0, 1)\n    with raises(TypeError):\n        lambdarepr(expr6, printer='numexpr')\n    expr7 = Piecewise((x, x > 0), (y, x <= 0))\n    with raises(TypeError):\n        lambdarepr(expr7, printer='numexpr')\n    expr8 = Sum(x**2, (i, 0, 10))\n    with raises(TypeError):\n        lambdarepr(expr8, printer='numexpr')\n\n    # Test that numexpr printer replaces multiple arguments correctly\n    expr9 = sin(x) + cos(y)\n    assert lambdarepr(expr9, printer='numexpr') == \"evaluate('(sin(x) + cos(y))', truediv=True)\"\n    expr10 = sin(x) + cos(y) + tan(z)\n    assert lambdarepr(expr10, printer='numexpr') == \"evaluate('(sin(x) + cos(y) + tan(z))', truediv=True)\"\n\n    # Test that numexpr printer replaces functions with multiple arguments\n    expr11 = asin(x)\n    assert lambdarepr(expr11, printer='numexpr') == \"evaluate('arcsin(x)', truediv=True)\"\n    expr12 = atan2(x, y)\n    assert lambdarepr(expr12, printer='numexpr') == \"evaluate('arctan2(x, y)', truediv=True)\"\n", "def test_print_Not():\n    # Test printing a single Not expression\n    assert lambdarepr(~x) == \"(not (x))\"\n    assert lambdarepr(~i) == \"(not (i))\"\n\n    # Test printing multiple Not expressions\n    assert raises(TypeError, lambda: lambdarepr(x & ~x))\n    assert raises(TypeError, lambda: lambdarepr(x | ~x))\n", "def test_lambdarepr_function_with_many_args():\n    from sympy import sin, cos, tan, log, exp, sqrt\n    x, y = symbols(\"x,y\")\n    func_str = lambdarepr(sin(x) + cos(y) + tan(x) + log(x) + exp(y) + sqrt(x))\n    assert func_str == \"evaluate('sin(x) + cos(y) + tan(x) + log(x) + exp(y) + sqrt(x)', truediv=True)\"\n", "def test_LambdaPrinter_floating_point_conversion():\n    from sympy import sin, sqrt, exp\n    x = symbols('x')\n    result = lambdarepr(sqrt(5) + sin(3*x))\n    expected = 'math.sqrt(5) + math.sin(3*x)'\n    assert result == expected\n"], "sample_1076": ["def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 2, 2)\n    b = Matrix([1, 2])\n    expected = 'numpy.linalg.solve(A, b)'\n    printer = NumPyPrinter({'standard': 'python3'})\n    assert printer.doprint(MatrixSolve(A, b)) == expected\n", "def test_print_Relational():\n    assert PyCodePrinter().doprint(Equal(x, 1)) == 'x == 1'\n    assert PyCodePrinter().doprint(Eq(x, 1)) == 'x == 1'\n    assert PyCodePrinter().doprint(Eq(x, y)) == '(x == y)'\n    assert PyCodePrinter().doprint(Equality(x, y)) == '(x == y)'\n    assert PyCodePrinter().doprint(Gt(x, 1)) == 'x > 1'\n    assert PyCodePrinter().doprint(Ge(x, 1)) == 'x >= 1'\n    assert PyCodePrinter().doprint(Lt(x, 1)) == 'x < 1'\n    assert PyCodePrinter().doprint(Le(x, 1)) == 'x <= 1'\n    assert PyCodePrinter().doprint(Ne(x, 1)) == '(x != 1)'\n    assert PyCodePrinter().doprint(Unequality(x, 1)) == '(x != 1)'\n    assert PyCodePrinter().doprint(Less(x, 1)) == '(x < 1)'\n    assert PyCodePrinter().doprint(Greater(x, 1)) == '(x > 1)'\n    assert PyCodePrinter().doprint(Equal(x, y, evaluate=False)) == 'x == y'\n    assert PyCodePrinter().doprint(Eq(x, y, evaluate=False)) == 'x == y'\n", "def test_NaN_and_Infinity():\n    assert MpmathPrinter()._print_NaN(oo) == 'float(\"nan\")'\n    assert SciPyPrinter()._print_NaN(oo) == 'numpy.nan'\n    assert NumPyPrinter()._print_NaN(oo) == 'numpy.nan'\n\n    assert MpmathPrinter()._print_NegativeInfinity(oo) == 'float(\"-inf\")'\n    assert SciPyPrinter()._print_NegativeInfinity(oo) == 'numpy.NINF'\n    assert NumPyPrinter()._print_NegativeInfinity(oo) == 'numpy.NINF'\n\n    assert MpmathPrinter()._print_NaN(zoo) == 'float(\"nan\")'\n    assert SciPyPrinter()._print_NaN(zoo) == 'numpy.nan'\n    assert NumPyPrinter()._print_NaN(zoo) == 'numpy.nan'\n\n    assert MpmathPrinter()._print_NegativeInfinity(zoo) == 'float(\"-inf\")'\n    assert SciPyPrinter()._print_NegativeInfinity(zoo) == 'numpy.NINF'\n    assert NumPyPrinter()._print_NegativeInfinity(zoo) == 'numpy.NINF'\n\n    assert MpmathPrinter()._print_ComplexInfinity(zoo) == 'float(\"nan\")'\n    assert SciPyPrinter()._print_ComplexInfinity(zoo) == 'numpy.nan'\n    assert NumPyPrinter()._print_ComplexInfinity(zoo) == 'numpy.nan'\n", "def test_NumPyPrinter_MatrixSolve():\n    # Test NumPyPrinter's MatrixSolve printer\n    M = MatrixSymbol('M', 2, 2)\n    b = Matrix([[1, 2], [3, 4]])\n    expr = MatrixSolve(M, b)\n    printer = NumPyPrinter()\n    assert printer._print(expr) == 'numpy.linalg.solve(M, b)'\n", "def test_SciPyPrinter():\n    from sympy import Rational, sqrt, exp, oo\n    printer = SciPyPrinter()\n    assert printer.doprint(exp(sqrt(x)*oo) + Rational(1, 2)) == 'scipy.special.exp(x**2*scipy.constants.inf) + 0.5'\n    assert printer.doprint(Piecewise((1, x < 0), (x, True))) == 'scipy.special.select([(-oo < x, x < oo), (x >= 0, x)], [scipy.special.nan, x])'\n    assert printer.doprint(And(x > 0, x < 1)) == 'scipy.special.logical_and(x > 0, x < 1)'\n    assert printer.doprint(sqrt(x)) == 'scipy.special.sqrt(x)'\n    assert printer.doprint(sqrt(x) + 1) == 'scipy.special.sqrt(x) + 1'\n"], "sample_1057": ["def test_render_as_module_standard_not_supported(self):\n    # Test that rendering fails when the standard is not supported\n    with self.assertRaises(ValueError):\n        render_as_module(content=None, standard='unsupported_standard')\n", "def test_render_as_module_fully_qualified_modules():\n    # Test when fully_qualified_modules is True\n    content = Print('print(\"Hello, World!\")')\n    printer = PythonCodePrinter({'standard': 'python3', 'fully_qualified_modules': True})\n    rendered = render_as_module(content, 'python3')\n    assert 'import' not in rendered\n    assert 'from' not in rendered\n    assert 'print' in rendered\n", "def test_render_as_module_fully_qualified_modules():\n    content = Print('x')\n    standard = 'python2'\n    expected_imports = ['import sympy.core.printing']\n    expected_code = 'import sympy.core.printing\\n\\nfrom sympy.core.printing import x'\n    result = render_as_module(content, standard)\n    assert 'import' in result\n    assert all(imp in result for imp in expected_imports)\n    assert 'from' in result\n    assert result == expected_code\n", "def test_render_as_module_fully_qualified_modules():\n    content = Print('Hello, World!')\n    result = render_as_module(content)\n    expected = 'from sympy.codegen.ast import Print\\n\\nPrint(\"Hello, World!\")'\n    assert result == expected\n", "def test_render_as_module_with_custom_printing_settings():\n    # Create a custom printing settings with a custom 'standard'\n    settings = {'standard': 'custom'}\n    printer = PythonCodePrinter(settings)\n    \n    # Test rendering with a simple expression\n    content = sympify('x**2')\n    expected_output = \"import sympy\\n\\nx**2\"\n    assert render_as_module(content, 'custom') == expected_output\n    \n    # Test rendering with a more complex expression\n    content = sympify('x**2 + 3*x + 4')\n    expected_output = \"import sympy\\n\\nx**2 + 3*x + 4\"\n    assert render_as_module(content, 'custom') == expected_output\n    \n    # Test rendering with module imports\n    content = sympify('sin(x)')\n    expected_output = \"import sympy\\nfrom sympy import sin\\n\\nsin(x)\"\n    assert render_as_module(content, 'custom') == expected_output\n    \n    # Test rendering with fully qualified module imports\n    printer._settings['fully_qualified_modules'] = True\n    content = sympify('sin(x)')\n    expected_output = \"import sympy\\n\\nsympy.sin(x)\"\n    assert render_as_module(content, 'custom') == expected_output\n    \n    # Test rendering with a complex expression and module imports\n    content = sympify('sin(x) + cos(x)')\n    expected_output = \"import sympy\\nfrom sympy import cos, sin\\n\\ncos(x) + sin(x)\"\n    assert render_as_module(content, 'custom') == expected_output\n    \n    # Test rendering with a complex expression and fully qualified module imports\n    printer._settings['fully_qualified_modules'] = True\n    content = sympify('sin(x) + cos(x)')\n    expected_output = \"import sympy\\n\\nsympy.cos(x) + sympy.sin(x)\"\n    assert render_as_module(content, 'custom') == expected_output\n"], "sample_196": ["    def test_date_extract_sql(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        with self.assertRaises(NotImplementedError) as cm:\n            ops.date_extract_sql('year', 'field_name')\n        self.assertIn(self.may_require_msg % 'date_extract_sql', str(cm.exception))\n", "    def test_autopk_value_validation(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(ops.validate_autopk_value(1), 1)\n        with self.assertRaises(ValueError):\n            ops.validate_autopk_value(0)\n", "    def test_no_limit_value(self):\n        with self.assertRaises(NotImplementedError):\n            self.ops.no_limit_value()\n", "    def test_execute_sql_flush_no_flush(self):\n        # Test that execute_sql_flush does not raise any exceptions when no SQL\n        # needs to be flushed.\n        self.ops.execute_sql_flush([])\n", "    def test_no_limit_value(self):\n        \"\"\"\n        Tests the no_limit_value() method.\n        \"\"\"\n        self.assertRaises(NotImplementedError, self.ops.no_limit_value)\n\n"], "sample_1106": ["def test_canonicalize_zero_matrix():\n    A = ZeroMatrix(2, 2)\n    assert canonicalize(A) == A\n    assert canonicalize(A + A) == A * 2\n", "def test_MatAdd_transpose():\n    n, m, l = symbols('n m l', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    X = MatAdd(A, B)\n    X_t = X.transpose()\n    assert X_t.is_MatAdd\n    assert X_t.args[0].is_Transpose\n    assert X_t.args[0].args[0].is_MatAdd\n    assert X_t.args[1] == B\n", "def test_MatAdd_derivative():\n    n, m, l = symbols('n m l', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, n)\n    X = MatrixSymbol('X', m, n)\n    \n    # Test the derivative of MatAdd with respect to MatrixSymbol\n    expr = MatAdd(A, B, X)\n    diff = expr.diff(X)\n    assert diff == Matrix([[1, 0]])\n    \n    # Test the derivative of MatAdd with respect to a constant Matrix\n    expr = MatAdd(A, B, 2*eye(n))\n    diff = expr.diff(eye(n))\n    assert diff == Matrix([[0, 0]])\n    \n    # Test the derivative of MatAdd with respect to a Matrix that is not present in the expression\n    expr = MatAdd(A, B, X)\n    diff = expr.diff(C)\n    assert diff == Matrix([[0, 0]])\n", "def test_matadd_shapes():\n    n, m, l, k = symbols('n m l k', integer=True)\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', m, l)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    E = MatrixSymbol('E', m, n)\n\n    # Test cases for the same shape\n    assert A.shape == (n, m)\n    assert B.shape == (m, l)\n    assert C.shape == (n, n)\n    assert D.shape == (n, n)\n    assert E.shape == (m, n)\n\n    # Test cases for different shapes\n    with XFAIL:\n        assert MatAdd(A, B).shape == (n, m)\n        assert MatAdd(B, C).shape == (m, l)\n        assert MatAdd(C, D).shape == (n, n)\n        assert MatAdd(D, E).shape == (n, n)\n\n    # Test cases with zero matrix\n    assert (MatAdd(A, ZeroMatrix(n, m)).shape == (n, m))\n    assert (MatAdd(ZeroMatrix(m, l), B).shape == (m, l))\n    assert (MatAdd(C, ZeroMatrix(n, n)).shape == (n, n))\n    assert (MatAdd(ZeroMatrix(n, n), D).shape == (n, n))\n\n    # Test cases with Identity matrix\n    assert (MatAdd(C, Identity(n)).shape == (n, n))\n    assert (MatAdd(Identity(n), D).shape == (n, n))\n    assert (MatAdd(C, Identity(n)).shape == (n, n))\n    assert (MatAdd(Identity(n), D).shape == (n, n))\n\n    # Test cases with all zeros\n    assert (MatAdd(C, GenericZeroMatrix()).shape == (n, n))\n    assert (MatAdd(D, GenericZeroMatrix()).shape == (n, n))\n    assert (MatAdd(C, GenericZeroMatrix()).shape == (n, n))\n    assert (MatAdd(D, GenericZeroMatrix()).shape == (n, n))\n\n", "def test_mat_add_validation():\n    # Test that MatAdd validation raises a TypeError when a scalar is mixed with a Matrix\n    assert_raises(TypeError, MatAdd, 1, Matrix([[1, 2]]))\n    assert_raises(TypeError, MatAdd, Matrix([[1, 2]]), 1)\n    # Test that MatAdd validation raises a ShapeError when matrices have different shapes\n    assert_raises(ShapeError, MatAdd, Matrix([[1, 2]]), Matrix([[1, 2, 3]]))\n    # Test that MatAdd validation does not raise an error when matrices have the same shape\n    MatAdd(Matrix([[1, 2]]), Matrix([[3, 4]]))\n"], "sample_1088": ["def test_symmetrize_zero():\n    from sympy.abc import x, y, z\n    F = x**2 + y**2 + z**2\n    result = symmetrize(F)\n    assert result[0].as_expr() == (x + y + z)**2\n    assert result[1].as_expr() == 0\n", "def test_rational_interpolate_values():\n    from sympy import Rational, symbols\n    x = symbols('x')\n\n    # test with integral coefficients\n    data = [(1, 2), (3, 6), (5, 10)]\n    expected = Rational(2)\n    assert rational_interpolate(data, 0, x) == expected\n\n    # test with non-integral coefficients\n    data = [(1, 2.5), (3, 7.5), (5, 12.5)]\n    expected = Rational(2.5)\n    assert rational_interpolate(data, 0, x) == expected\n\n    # test with non-integer degree\n    data = [(1, 2), (3, 6), (5, 10)]\n    expected = Rational(10)\n    assert rational_interpolate(data, 1.5, x) == expected\n\n    # test with large data points\n    data = [(100, 200), (300, 600), (500, 1000)]\n    expected = Rational(200)\n    assert rational_interpolate(data, 0, x) == expected\n\n    # test with negative data points\n    data = [(-1, -2), (-3, -6), (-5, -10)]\n    expected = Rational(2)\n    assert rational_interpolate(data, 0, x) == expected\n\n    # test with single data point\n    data = [(1, 2)]\n    expected = Rational(2)\n    assert rational_interpolate(data, 0, x) == expected\n\n    # test with no data points\n    data = []\n    expected = 0\n    assert rational_interpolate(data, 0, x) == expected\n", "def test_symmetrize_formal_symbols():\n    x, y = symbols('x y')\n    F = x**2 + y**2\n    polys, symbols = symmetrize(F, formal=True)\n    assert polys == [(symbols[0], x + y), (symbols[1], x*y)]\n    assert polys[0][1].is_Add\n    assert polys[1][1].is_Mul\n", "def test_rational_interpolate_degree_too_high():\n    # Test case where the degree of the numerator is too high\n    data = [(1, -210), (2, -35), (3, 105), (4, 231), (5, 350), (6, 465)]\n    degnum = 10  # Degree in the numerator is too high\n    X = symbols('x')\n\n    expected_error = OptionError(\"Too few values for the required degree.\")\n    with raises(OptionError, match=expected_error):\n        rational_interpolate(data, degnum, X)\n", "def test_symmetrize_formal():\n    a, b, c = symbols('a:b')\n    f = a**2 + b**2 + c**2 + 2*a*b + 2*b*c + 2*a*c\n    result, polys = symmetrize(f, formal=True)\n    expected = ((S(3)*a*b*c + (a + b + c)**2, 0),)\n    assert result == expected\n"], "sample_1068": ["def test_octave_code_zero_dimensional_matrices():\n    A = MatrixSymbol('A', 0, 0)\n    B = MatrixSymbol('B', 1, 0)\n    C = MatrixSymbol('C', 0, 1)\n    E = eye(0)\n    F = eye(1)\n    G = eye(0)\n    assert octave_code(A) == 'A = []'\n    assert octave_code(B) == 'B = []'\n    assert octave_code(C) == 'C = []'\n    assert octave_code(E) == 'E = eye(0)'\n    assert octave_code(F) == 'F = eye(1)'\n    assert octave_code(G) == 'G = eye(0)'\n    assert octave_code(Matrix([[A, B], [C, E]])) == 'A = []; B = []; C = []; E = eye(0); Matrix([[A, B], [C, E]]) = [A, B; C, E]'\n    assert octave_code(Matrix([[A, B], [C, F]])) == 'A = []; B = []; C = []; F = eye(1); Matrix([[A, B], [C, F]]) = [A, B; C, F]'\n    assert octave_code(Matrix([[A, B], [C, G]])) == 'A = []; B = []; C = []; G = eye(0); Matrix([[A, B], [C, G]]) = [A, B; C, G]'\n", "def test_octave_code_contract_and_inline():\n    from sympy import MatrixSymbol, IndexedBase, Idx\n    from sympy import Eq, indexed, cos\n    A = MatrixSymbol('A', 3, 3)\n    i = Idx('i', 3)\n    j = Idx('j', 3)\n    A_con = IndexedBase('A_con', shape=(3, 3))\n    e = Eq(A_con[i, j], cos(i)*cos(j))\n    code = octave_code(e.rhs, assign_to=e.lhs, contract=True, inline=True)\n    expected = 'A_con(i, j) = cos(i).*cos(j);'\n    assert code == expected\n", "def test_octave_code_mat_pow():\n    from sympy import MatrixSymbol, Matrix\n    A = MatrixSymbol('A', 2, 2)\n    assert octave_code(A**3) == '(A^3)'\n", "def test_octave_code_user_functions():\n    from sympy import octave_code, Function, sin, cos\n\n    f = Function('f')\n\n    # Test that custom function with one argument is correctly handled\n    assert octave_code(sin(x)) == 'sin(x)'\n    assert octave_code(f(x)) == 'f(x)'\n\n    # Test that custom function with two arguments is correctly handled\n    assert octave_code(sin(x, y)) == 'sin(x, y)'\n\n    # Test that custom function with no arguments is correctly handled\n    assert octave_code(f()) == 'f()'\n\n    # Test that custom function with a mix of arguments is correctly handled\n    assert octave_code(sin(x, f(y))) == 'sin(x, f(y))'\n\n    # Test that a list of custom functions is correctly handled\n    custom_functions = {\n        \"sin\": \"custom_sin\",\n        \"f\": \"custom_f\"\n    }\n    assert octave_code(sin(x), user_functions=custom_functions) == 'custom_sin(x)'\n    assert octave_code(f(x), user_functions=custom_functions) == 'custom_f(x)'\n", "def test_octave_code_piecewise_with_defaults():\n    from sympy import Piecewise\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    code = octave_code(pw)\n    assert code == \"x > 0; x = x + 1; else; x;\"\n\n"], "sample_973": ["def test_getglobals():\n    module = types.ModuleType('module')\n    obj = types.FunctionType(lambda: None, module.__dict__)\n    assert inspect.getglobals(obj) == {}\n\n    obj = types.FunctionType(lambda: None, globals())\n    assert inspect.getglobals(obj) == globals()\n", "def test_getannotations():\n    class A:\n        __annotations__ = {'x': int}\n\n    class B:\n        __annotations__ = {'x': int, 'y': str}\n\n    assert getannotations(A) == {'x': int}\n    assert getannotations(B) == {'x': int, 'y': str}\n\n        pass\n    assert getannotations(func) == {}\n\n    class C:\n        pass\n\n    assert getannotations(C) == {}\n\n    assert getannotations(None) == {}\n    assert getannotations('hello') == {}\n    assert getannotations(123) == {}\n    assert getannotations(True) == {}\n    assert getannotations(datetime.datetime) == {}\n    assert getannotations(enum.Enum) == {}\n    assert getannotations(functools.partial) == {}\n    assert getannotations(types.FunctionType) == {}\n    assert getannotations(types.ModuleType) == {}\n    assert getannotations(types.MethodType) == {}\n    assert getannotations(types.SimpleNamespace) == {}\n    assert getannotations(types.CellType) == {}\n    assert getannotations(types.BuiltinFunctionType) == {}\n    assert getannotations(types.BuiltinMethodType) == {}\n    assert getannotations(types.CodeType) == {}\n    assert getannotations(types.FrameType) == {}\n    assert getannotations(types.FunctionType) == {}\n    assert getannotations(types.GeneratorType) == {}\n    assert getannotations(types.GetSetDescriptorType) == {}\n    assert getannotations(types.MemberDescriptorType) == {}\n    assert getannotations(types.MethodDescriptorType) == {}\n    assert getannotations(types.ModuleType) == {}\n    assert getannotations(types.NoneType) == {}\n    assert getannotations(types.TracebackType) == {}\n    assert getannotations(types.TypeType) == {}\n    assert getannotations(types.UnboundMethodType) == {}\n    assert getannotations(types.WrapperDescriptorType) == {}\n", "def test_getannotations():\n    class TestClass:\n        __annotations__ = {'a': int, 'b': str}\n            pass\n            pass\n\n    assert inspect.getannotations(TestClass) == {'a': int, 'b': str}\n    assert inspect.getannotations(TestClass()) == {'a': int, 'b': str}\n    assert inspect.getannotations(TestClass.method) == {}\n    assert inspect.getannotations(5) == {}\n    assert inspect.getannotations(\"hello\") == {}\n", "def test_getall_invalid_type():\n    with pytest.raises(ValueError):\n        inspect.getall([1, 2, 3])\n", "def test_TypeAliasNamespace():\n    namespace = TypeAliasNamespace({\n        'module1': 'module1',\n        'module2': 'module2',\n        'class1': 'class1',\n        'function1': 'function1'\n    })\n    assert namespace['module1'] == 'module1'\n    assert isinstance(namespace['module1'], TypeAliasForwardRef)\n    assert namespace['module2'] == 'module2'\n    assert isinstance(namespace['module2'], TypeAliasForwardRef)\n    assert namespace['class1'] == 'class1'\n    assert isinstance(namespace['class1'], TypeAliasForwardRef)\n    assert namespace['function1'] == 'function1'\n    assert isinstance(namespace['function1'], TypeAliasForwardRef)\n\n    with pytest.raises(KeyError):\n        namespace['non_existent_module']\n"], "sample_1154": ["def test_linsolve_nullspace():\n    # Test that the nullspace is correctly computed\n    eqs = [Eq(x + 2*y, 0), Eq(x - 2*y, 0)]\n    sol = _linsolve(eqs, [x, y])\n    expected_sol = {x: -2*y, y: y}\n    assert sol == expected_sol\n\n    # Test that the nullspace is correctly computed for a system with multiple variables\n    eqs = [Eq(x + 2*y + 3*z, 0), Eq(x - 2*y + z, 0), Eq(x + y + z, 0)]\n    sol = _linsolve(eqs, [x, y, z])\n    expected_sol = {x: y - 2*z, y: -z, z: z}\n    assert sol == expected_sol\n\n    # Test that the nullspace is correctly computed for a system with complex coefficients\n    eqs = [Eq(x + 2*I*y, 0), Eq(x - 2*I*y, 0)]\n    sol = _linsolve(eqs, [x, y])\n    expected_sol = {x: -2*I*y, y: y}\n    assert sol == expected_sol\n\n    # Test that the nullspace is correctly computed for a system with rational coefficients\n    eqs = [Eq(x + 2/3*y, 0), Eq(x - 2/3*y, 0)]\n    sol = _linsolve(eqs, [x, y])\n    expected_sol = {x: -2/3*y, y: y}\n    assert sol == expected_sol\n", "def test_linsolve_real_field():\n    from sympy import Eq, I\n    from sympy.abc import x, y\n\n    eqs = [Eq(x + y, 1), Eq(x - y, 2)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 3/2, y: -1/2}\n", "def test_linsolve_rref_with_zero_row():\n    from sympy.polys.matrices.linsolve import _linsolve\n    from sympy import symbols, Eq\n    x, y, z = symbols('x y z')\n    eqs = [Eq(x + y, 0), Eq(x - y, 2), Eq(0, 0)]\n    result = _linsolve(eqs, [x, y])\n    expected = {x: 0, y: 0}\n    assert result == expected\n", "def test_linsolve_complex_domain():\n    x, y = S.symbols('x y', real=False)\n    eqs = [Eq(x + y, 1 + 2*I), Eq(x - y, 1 - 2*I)]\n    result = _linsolve(eqs, [x, y])\n    assert result == {x: 3/2, y: -1/2}\n", "def test_linsolve_with_complex_equations():\n    # Test that _linsolve works with complex equations\n    from sympy import I\n    eqs = [Eq(x + 2*I*y, 1), Eq(2*x - y + I*z, 0)]\n    sol = _linsolve(eqs, [x, y, z])\n    assert sol == {x: -2*I, y: 1/2, z: 0}\n"], "sample_1119": ["def test_inv_mod():\n    A = MatrixSymbol('A', 2, 2)\n    m = 3\n    assert _inv_mod(A, m) == (A ** (m - 1)) * (-1)\n", "    def test_non_square_matrix_error(self):\n        with raises(NonSquareMatrixError):\n            Matrix(3, 2, [1, 2, 3, 4])\n", "def test_matrix_echelon_form():\n    m = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 10]])\n    echelon = m.echelon_form()\n    assert echelon[0, 2] == 3\n    assert echelon[1, 2] == 6\n    assert echelon[2, 2] == 10\n", "    def test_jordan_cells(self):\n        from sympy import Matrix\n        # Test that jordan_cells correctly returns the blocks of a jordan matrix\n        m = Matrix.jordan_block(2, 1)\n        p, j = m.jordan_cells()\n        self.assertEqual(p, eye(2))\n        self.assertEqual(j, [[1, 0], [0, 1]])\n\n        # Test that jordan_cells correctly returns the blocks of a more complex jordan matrix\n        m = Matrix.jordan_block(3, 2)\n        p, j = m.jordan_cells()\n        self.assertEqual(p, eye(3))\n        self.assertEqual(j, [[2, 1, 0], [0, 2, 0], [0, 0, 2]])\n\n        # Test that jordan_cells correctly returns the blocks of a jordan matrix with repeated eigenvalues\n        m = Matrix.jordan_block(2, 1).multiply(Matrix.jordan_block(2, 1))\n        p, j = m.jordan_cells()\n        self.assertEqual(p, eye(4))\n        self.assertEqual(j, [[1, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 1], [0, 0, 0, 1]])\n", "def test_inv():\n    assert isinstance(inv(C), MatPow)\n    assert inv(D).args[0] == D\n    assert inv(D).args[1] == -1\n    assert inv(C + D) == inv(C + D)\n    assert (A + B).inv() == (A.inv() + B.inv())\n    assert (A - B).inv() == (A.inv() - B.inv())\n    assert (A * B).inv() == B.inv() * A.inv()\n    assert (A / B) == (A * B.inv())\n    assert (A / B).inv() == (B.inv() * A.inv())\n    assert (A.inv() + B.inv()) == (B + A).inv()\n    assert (A.inv() - B.inv()) == (B - A).inv()\n    assert (A.inv() * B.inv()) == (A * B).inv()\n"], "sample_1036": ["def test_Mul_as_two_terms():\n    from sympy.core import symbols, I\n    from sympy.functions import sympify\n    from sympy.core.testing import testing\n    from sympy import Mul\n    # Test normal case\n    x, y = symbols('x y')\n    result = Mul(x, y).as_two_terms()\n    testing.assert_equal(result, (x, y))\n    # Test single argument case\n    result = Mul(x).as_two_terms()\n    testing.assert_equal(result, (1, x))\n    # Test no arguments\n    result = Mul().as_two_terms()\n    testing.assert_equal(result, (1, 1))\n", "    def test_as_two_terms(self):\n        from sympy import Mul\n        assert Mul(2, 3, 4).as_two_terms() == (24, Mul(3, 4))\n        assert Mul(2, Mul(3, 4)).as_two_terms() == (2, Mul(3, 4))\n        assert Mul().as_two_terms() == (1, Mul())\n", "def test_Mul_to_finite():\n    assert Mul(_fuzzy_group([1, 2, 3, 4, 5]).is_finite)\n    assert Mul(_fuzzy_group([1, 2, 3, I, 5]).is_finite)\n    assert Mul(_fuzzy_group([1, 2, 3, S.Infinity]).is_infinite)\n    assert Mul(_fuzzy_group([1, 2, S.Infinity]).is_infinite)\n    assert Mul(_fuzzy_group([1, 2, S.NegativeInfinity]).is_infinite)\n    assert Mul(_fuzzy_group([1, 2, S.NaN]).is_nan)\n    assert Mul(_fuzzy_group([1, S.NaN, 2]).is_nan)\n", "    def test_flatten(self):\n        from sympy.core import S, sqrt, I\n        from sympy.abc import x, y\n        M = Mul(2, x, S(3), sqrt(x**2 + 1))\n        assert M.args == [2, x, S(3), sqrt(x**2 + 1)]\n\n        M = Mul(Mul(x, y), S(2))\n        assert M.args == [x*y, 2]\n\n        M = Mul(x**2 + 1, x + y)\n        assert M.args == [x**2 + 1, x + y]\n", "def test_Mul_matches():\n    assert Eq(Mul(2, 3, 4).matches('2*a'), {a: 12})\n    assert Eq(Mul(2, 3, 4).matches('4*a'), {a: 24})\n    assert Eq(Mul(2, 3, 4).matches('5*a'), None)\n    assert Eq(Mul(2, 3, 4).matches('a*b'), {a: 12, b: 4})\n    assert Eq(Mul(2, 3, 4).matches('a*b*c'), {a: 24, b: 3, c: 4})\n    assert Eq(Mul(2, 3, 4).matches('2*a*b*c'), {a: 12, b: 4, c: 1})\n    assert Eq(Mul(2, 3, 4).matches('3*a*b*c'), {a: 8, b: 4, c: 1})\n    assert Eq(Mul(2, 3, 4).matches('4*a*b*c'), {a: 24, b: 3, c: 1})\n\n    assert Eq(Mul(x, x).matches('x**2'), {x: x})\n    assert Eq(Mul(x, x).matches('y**2'), None)\n\n    # Negative number\n    assert Eq(Mul(-2, 3, 4).matches('-2*a'), {a: 12})\n    assert Eq(Mul(-2, 3, 4).matches('4*a'), {a: -24})\n\n    # Check that the match dictionary does not have extraneous keys\n    assert Eq(Mul(x, 2*x, x).matches('x**3'), {x: x**2})\n    assert Eq(Mul(x, 2*x, x).matches('x*y'), {x: 2*x**2, y: x})\n\n    # Test that it handles `1` properly\n    assert Eq(Mul(2).matches('a'), {a: 2})\n\n    # Test that it does not attempt to match a non-Mul with a Mul\n    with raises(TypeError):\n        Mul(x).matches(2)\n"], "sample_927": ["compilation error", "def test_ast_fold_expr():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser(\"(a + 1)\", location=None, config=Config())\n    ast = parser.parse_expression()\n    assert isinstance(ast, ASTFoldExpr)\n    assert ast.leftExpr.get_id(1) == \"a\"\n    assert ast.op == \"+\"\n    assert ast.rightExpr.get_id(1) == \"1\"\n", "compilation error", "def test_qualified_name():\n    input = r\"\"\"\n    namespace my::namespace {\n        class MyClass;\n        class MyOtherClass;\n    }\n    \"\"\"\n    output = r\"\"\"\n    namespace my::namespace\n    MyClass\n    MyOtherClass\n    \"\"\"\n    idDict = {3: \"my::namespace_MyClass_E\", 4: \"my::namespace_MyOtherClass_E\"}\n    check(\"namespace\", input, idDict, output)\n", "compilation error"], "sample_588": ["    def test_tile_ids_from_nested_list_single_level(self):\n        \"\"\"Check that tile IDs can be extracted from a single level list.\"\"\"\n        datasets = [Dataset(), Dataset()]\n        tile_ids = list(_infer_tile_ids_from_nested_list(datasets, ()))\n        assert len(tile_ids) == 2\n        assert tile_ids[0] == (0,)\n        assert tile_ids[1] == (1,)\n", "def test_combine_nested_with_empty_datasets():\n    # Test that an empty list of datasets returns an empty Dataset\n    datasets = []\n    combined = combine_nested(datasets, ['x', 'y'])\n    assert combined == Dataset()\n\n    # Test that a list containing a single dataset returns that dataset\n    datasets = [create_test_data()]\n    combined = combine_nested(datasets, ['x', 'y'])\n    assert_identical(combined, datasets[0])\n\n    # Test that a list of datasets with no common variables raises an error\n    datasets = [create_test_data('x', 'y'), create_test_data('z', 'w')]\n    with raises_regex(ValueError, 'The supplied objects do not form a hypercube'):\n        combine_nested(datasets, ['x', 'y'])\n\n    # Test that a list of datasets with different variable names returns an error\n    datasets = [create_test_data('x', 'y'), create_test_data('x', 'z')]\n    with raises_regex(ValueError, 'encountered unexpected variable'):\n        combine_nested(datasets, ['x', 'y'])\n\n    # Test that a list of datasets with a variable that is an NA in one dataset\n    # but not in another raises an error\n    datasets = [create_test_data('x', 'y'), create_test_data('x', 'y', fill_value=dtypes.NA)]\n    with raises_regex(ValueError, 'The supplied objects do not form a hypercube'):\n        combine_nested(datasets, ['x', 'y'])\n\n    # Test that a list of datasets with no common coordinates raises an error\n    datasets = [create_test_data('x', 'y', coords={'x': [1, 2, 3]}), create_test_data('x', 'y')]\n    with raises_regex(ValueError, 'Could not find any dimension coordinates'):\n        combine_nested(datasets, ['x', 'y'])\n", "def test_tile_ids_from_nested_list_empty_list():\n    empty_list = []\n    combined_ids = _infer_concat_order_from_positions([empty_list])\n    assert_equal(combined_ids, OrderedDict([]))\n\n", "def test_combine_by_coords_with_monotonic_decreasing_indexes():\n    # Create four datasets with monotonic decreasing indexes\n    t = [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]\n    d = [\"2021-12-01\", \"2021-12-02\", \"2021-12-03\"]\n    d2 = [\"2021-11-01\", \"2021-11-02\", \"2021-11-03\"]\n    d3 = [\"2021-10-01\", \"2021-10-02\", \"2021-10-03\"]\n\n    coords = OrderedDict()\n    coords[\"time\"] = pd.to_datetime(t)\n    coords[\"day\"] = pd.to_datetime(d)\n    coords[\"day2\"] = pd.to_datetime(d2)\n    coords[\"day3\"] = pd.to_datetime(d3)\n\n    da = DataArray(np.arange(27), dims=[\"time\", \"day\", \"day2\", \"day3\"])\n    ds = Dataset(coords=coords, data_vars={\"a\": da})\n\n    ds2 = Dataset(coords=coords, data_vars={\"a\": da.shift(time=-1)})\n    ds3 = Dataset(coords=coords, data_vars={\"a\": da.shift(time=-2)})\n    ds4 = Dataset(coords=coords, data_vars={\"a\": da.shift(time=-3)})\n\n    # Combine the datasets along their common dimension coordinates\n    combined = combine_by_coords([ds, ds2, ds3, ds4])\n\n    # Check the combined dataset has the expected shape\n    assert combined.shape == (3, 3, 3, 3)\n\n    # Check the combined dataset has the expected data\n    assert np.all(combined.data == np.array([24, 21, 18, 15, 12, 9, 6, 3, 0, 27, 24, 21, 18, 15, 12, 9, 6, 3, 0, 27, 24, 21, 18, 15, 12, 9, 6]))\n\n    # Check the combined dataset has the expected coordinates\n    assert combined.coords[\"time\"].equals(pd.to_datetime([\"2022-01-01\", \"2022-01-02\", \"2022-01-03\"]))\n    assert combined.coords[\"day\"].equals(pd.to_datetime([\"2021-12", "def test_combine_nested_tile_ids_shape():\n    # create some test data\n    coords = [\"a\", \"b\", \"c\"]\n    data = np.random.rand(3, 3, 3)\n    da = DataArray(data, dims=(\"x\", \"y\", \"z\"))\n    da.coords[\"foo\"] = coords\n\n    # create a 3D list of datasets\n    datasets = [\n        [Dataset({\"a\": da}), Dataset({\"b\": da})],\n        [Dataset({\"c\": da}), Dataset({\"d\": da})],\n    ]\n    datasets = [[datasets, datasets]]\n\n    # manually specify the tile IDs\n    ids = [(0, 0, 0), (0, 0, 1), (1, 0, 0), (1, 0, 1)]\n\n    # use combine_nested to combine the datasets\n    combined = combine_nested(datasets, concat_dims=[\"x\", \"y\", \"z\"], ids=ids)\n\n    # check that the tile IDs match the specified IDs\n    combined_ids = OrderedDict(zip(ids, [Dataset({\"e\": combined[\"z\"]}) for _ in range(4)]))\n    assert_combined_tile_ids_equal(combined_ids, _infer_concat_order_from_positions([combined]))\n\n    # check that the shape of the tile IDs is correct\n    _check_shape_tile_ids(combined_ids)\n"], "sample_430": ["    def test_deconstructible_field(self):\n        model_state = ModelState(\n            \"testapp\",\n            \"Author\",\n            [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=DeconstructibleObject())),\n            ],\n        )\n        changes = self.get_changes([model_state], [model_state])\n        self.assertEqual(len(changes.get(\"testapp\", [])), 1)\n        self.assertEqual(len(changes[\"testapp\"][0].operations), 1)\n        self.assertIsInstance(changes[\"testapp\"][0].operations[0], operations.AddField)\n        self.assertEqual(changes[\"testapp\"][0].operations[0].name, \"Author\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].field.name, \"name\")\n        self.assertEqual(changes[\"testapp\"][0].operations[0].field.db_column, \"name\")\n        self.assertIsInstance(changes[\"testapp\"][0].operations[0].field.default, DeconstructibleObject)\n", "    def test_renamed_field_destruct(self):\n        project_state = self.make_project_state([\n            self.author_name_renamed,\n            self.author_name,\n        ])\n        changes = self.get_changes(project_state.models, project_state.models)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['RenameField'])\n", "    def test_model_renamed_with_custom_primary_key(self):\n        \"\"\"\n        A model is renamed and the primary key is custom.\n        \"\"\"\n        before_states = self.make_project_state([\n            self.author_custom_pk.clone()\n        ])\n        after_states = self.make_project_state([\n            self.author_custom_pk.clone(name='NewAuthor'),\n        ])\n        changes = self.get_changes(before_states, after_states)\n        self.assertEqual(len(changes.get('testapp', [])), 1)\n        self.assertEqual(len(changes.get('testapp', [0]).dependencies), 1)\n        self.assertEqual(changes.get('testapp', [0]).operations[0].__class__.__name__, 'RenameModel')\n        self.assertEqual(changes.get('testapp', [0]).operations[0].old_name, 'Author')\n        self.assertEqual(changes.get('testapp', [0]).operations[0].new_name, 'NewAuthor')\n        self.assertEqual(len(changes.get('testapp', [0]).operations[0].dependencies), 1)\n        self.assertEqual(changes.get('testapp', [0]).operations[0].dependencies[0], ('testapp', '__first__'))\n", "    def test_alter_field_alter_with_no_default_value(self):\n        from django.db.migrations.operations import AlterField\n\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n        )\n        self.assertEqual(len(changes.get(\"testapp\", [])), 0)\n\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [(\"id\", models.AutoField(primary_key=True)), (\"name\", models.CharField())],\n                )\n            ],\n        )\n        self.assertEqual(len(changes.get(\"testapp\", [])), 1)\n        self.assertIsInstance(changes[\"testapp\"][0].operations[0], AlterField)\n\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"name\", models.CharField(max_length=50)),\n                    ],\n                )\n            ],\n        )\n        self.assertEqual(len(changes.get(\"testapp\", [])), 1)\n        self.assertIsInstance(changes[\"testapp\"][0].operations[0], AlterField)\n\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp\",\n                    \"Author\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\n                            \"name\",\n                            models.CharField(\n                                max_length=200,\n                                validators=[RegexValidator(regex=\"^[a-zA-Z]+$\")],\n                            ),\n                        ),\n                    ],\n                )\n            ],\n        )\n        self.assertEqual(len(changes.get(\"testapp\", [])), 1)\n        self.assertIsInstance(changes[\"testapp\"][0].operations[0], AlterField)\n\n        changes = self.get_changes(\n            [\n                ModelState(\n                    \"testapp\", \"Author\", [(\"id\", models.AutoField(primary_key=True))]\n                )\n            ],\n            [\n                ModelState(\n                    \"testapp", "    def test_add_operations_with_dependencies_in_order(self):\n        state1 = ProjectState()\n        state2 = ProjectState()\n        state2.add_model(\n            ModelState(\n                \"testapp\", \"Model\", [(\"id\", models.AutoField(primary_key=True))]\n            )\n        )\n        state2.add_model(\n            ModelState(\n                \"testapp\",\n                \"Model\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                ],\n            )\n        )\n        state2.add_model(\n            ModelState(\n                \"testapp\", \"Model2\", [(\"id\", models.AutoField(primary_key=True))]\n            )\n        )\n        changes = self.get_changes(\n            [state1], [state2], questioner=mock.Mock()\n        )\n        self.assertEqual(\n            self.repr_changes(changes, include_dependencies=True),\n            \"\"\"\\n  testapp:\\n    0001_initial\\n      Dependencies: None\\n    0002_add_name\\n      Dependencies: (testapp, 0001_initial)\\n    0003_add_model2\\n      Dependencies: (testapp, 0002_add_name)\\n\"\"\",\n        )\n        self.assertEqual(\n            changes[\"testapp\"],\n            [\n                operations.Operation(name=\"0001_initial\", dependencies=[]),\n                operations.Operation(name=\"0002_add_name\", dependencies=[\"testapp.0001_initial\"]),\n                operations.Operation(name=\"0003_add_model2\", dependencies=[\"testapp.0002_add_name\"]),\n            ],\n        )\n"], "sample_959": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_template_declaration_prefix():\n    # test template parameter lists\n    input = '::namespace::std'\n    output = '::namespace::std'\n    check('namespace', input, {3: 'N'}, output)\n\n    input = '::namespace::std::template<int, float>'\n    output = '::namespace::std::template<int,float>'\n    check('namespace', input, {3: 'N', 4: 'I' + 'I' + 'E'})\n\n    input = '::namespace::std::template<int, float, template<class T> class U>'\n    output = '::namespace::std::template<int,float,template<class T> class U>'\n    check('namespace', input, {3: 'N', 4: 'I' + 'I' + 'E', 5: 'I' + 'I' + 'I' + 'I' + 'E'})\n\n    # test template introductions\n    input = '::namespace::std::template<int, float, template<class T> class U> concept { };'\n    output = '::namespace::std::template<int,float,template<class T> class U> concept{ }'\n    check('namespace', input, {3: 'N', 4: 'I' + 'I' + 'E', 5: 'I' + 'I' + 'I' + 'I' + 'E'})\n\n    # test that we do the correct thing when there is a qualified name after a template parameter list\n    input = '::namespace::std::template<int> A<int> x;'\n    output = '::namespace::std::template<int>A<int> x;'\n    check('namespace', input, {3: 'N', 4: 'I' + 'I' + 'E'})\n\n    # test that we do the correct thing when there is a qualified name after a template introduction\n    input = '::namespace::std::template<int, float> concept { }; A<int> x;'\n    output = '::namespace::std::template<int,float> concept{ }A<int> x;'\n    check('namespace', input, {3: 'N', 4: 'I' + 'I' + 'E'})\n\n    # test that we do the correct thing when there is a qualified name after a template introduction\n    # and a template parameter list after that\n    input = '::namespace::std::template<int, float, template<class T> class U> concept { }; A<int"], "sample_1118": ["def test_MatPow_evaluate_to_power_of_negative_one():\n    # Test power of negative one\n    A = MatrixSymbol('A', 2, 2)\n    assert (MatPow(A, -1).doit() == Inverse(A).doit())\n", "def test_matpow_negative_non_square():\n    from sympy import Eq, MatPow\n    A = MatrixSymbol('A', 2, 3)\n    with raises(NonSquareMatrixError):\n        MatPow(A, -1)\n", "def test_ZeroMatrix_power():\n    A = ZeroMatrix(2, 2)\n    assert raises(NonInvertibleMatrixError, lambda: A**-1)\n", "def test_MatPow_evaluate_positive_integer_power_of_inverse():\n    from sympy import symbols, I\n    from sympy.matrices import Matrix\n\n    A = MatrixSymbol('A', 2, 2)\n    x, y = symbols('x y')\n    A_expr = Matrix([[x, y], [y, x]])\n    A_expr = A_expr.subs(x, 2).subs(y, 3)\n\n    A_inv = A_expr.inv()\n    MatPow_test = A_inv**3\n    MatPow_result = MatPow(A_expr, 3)\n    assert MatPow_test == MatPow_result\n", "def test_MatPow_NonInvertibleMatrixError():\n    from sympy import Matrix, Symbol\n    n = Symbol('n')\n    A = Matrix([[1, 2], [3, 4]])\n    assert raises(NonInvertibleMatrixError, MatPow, A, 0)\n    assert raises(NonInvertibleMatrixError, MatPow, A, -1)\n    assert raises(NonInvertibleMatrixError, MatPow, A, -2)\n"], "sample_969": ["def test_restify_SystemExit():\n    \"\"\"Test restify with SystemExit.\"\"\"\n    import sys\n    assert restify(sys.SystemExit) == ':py:obj:`~sys.SystemExit`'\n", "def test_restify_string():\n    \"\"\"Test restify() for string type.\"\"\"\n    assert restify(str) == ':py:obj:`str`'\n    assert restify('str') == 'str'\n    assert restify('...') == '...'\n    assert restify('int') == ':py:obj:`int`'\n", "def test_restify():\n    # Test restify with NoneType\n    assert restify(None) == ':py:obj:`None`'\n\n    # Test restify with NewType\n    class MyInt(NewType('MyInt', int)):\n        pass\n    assert restify(MyInt) == ':py:obj:`~sphinx.util.typing.MyInt`'\n\n    # Test restify with UnionType\n    class UnionTypeTest(Union[int, str]):\n        pass\n    assert restify(UnionTypeTest) == ':py:obj:`~sphinx.util.typing.Union` [int, str]'\n\n    # Test restify with Optional\n    class OptionalTypeTest(Optional[int]):\n        pass\n    assert restify(OptionalTypeTest) == ':py:obj:`~sphinx.util.typing.Optional` [int]'\n\n    # Test restify with Callable\n    class CallableTypeTest(Callable[[int, int], int]):\n        pass\n    assert restify(CallableTypeTest) == ':py:obj:`~sphinx.util.typing.Callable` [[int], int]'\n\n    # Test restify with Union and Optional\n    class UnionOptionalTypeTest(Union[Optional[int], int]):\n        pass\n    assert restify(UnionOptionalTypeTest) == ':py:obj:`~sphinx.util.typing.Optional` [int | :py:obj:`~sphinx.util.typing.Union` [int]]'\n\n    # Test restify with Union and None\n    class UnionNoneTypeTest(Union[int, None]):\n        pass\n    assert restify(UnionNoneTypeTest) == ':py:obj:`~sphinx.util.typing.Union` [int, None]'\n\n    # Test restify with None in Union\n    class UnionNoneTest(Union[int, None, None]):\n        pass\n    assert restify(UnionNoneTest) == ':py:obj:`~sphinx.util.typing.Union` [int, None]'\n\n    # Test restify with Literal\n    class LiteralTypeTest(Literal[int, 1, 2, 3]):\n        pass\n    assert restify(LiteralTypeTest) == ':py:obj:`~sphinx.util.typing.Literal` [1, 2, 3]'\n\n    # Test restify with Annotated\n    class AnnotatedTypeTest(Annotated[int, 'my-annotation']):\n        pass\n    assert restify(AnnotatedTypeTest) == 'my-", "def test_restify_UnionType():\n    \"\"\"\n    Test restify() function with UnionType.\n    \"\"\"\n    union_type = Union[int, str]\n    assert restify(union_type) == ':py:obj:`~typing.Union` [int, str]'\n\n    union_type = Union[int, str, None]\n    assert restify(union_type) == ':py:obj:`~typing.Union` [int, str, None]'\n\n    union_type = Union[int, str, int]\n    assert restify(union_type) == ':py:obj:`~typing.Union` [int, str, int]'\n\n    union_type = Union[int, str, None, int]\n    assert restify(union_type) == ':py:obj:`~typing.Union` [int, str, None, int]'\n\n    # Test with empty args\n    union_type = Union[...]\n    assert restify(union_type) == '...'\n\n    # Test with UnionType in py310+\n    UnionType = Union[int, str]\n    assert restify(UnionType) == ':py:obj:`~types.Union` [int, str]'\n", "def test_stringify_newtype():\n    class MyType(NewType('MyType', int)):\n        pass\n\n    assert stringify(MyType) == '~typing.MyType'\n"], "sample_1141": ["def test_sympify_method_args():\n    from sympy.core import SympifyError\n    class TestClass:\n        @sympify_method_args\n            return x\n    try:\n        TestClass().func(3)\n        assert False\n    except SympifyError:\n        assert True\n", "def test_nseries_number_result():\n    from sympy import sin, pi, oo\n    from sympy.abc import x\n    expr = sin(x)\n    with raises(SympifyError):\n        expr._eval_nseries(x, n=oo)\n", "compilation error", "def test_as_leading_term():\n    from sympy.core import Expr, Function\n    x = symbols('x')\n    expr = Function('f')(x)\n    assert expr.as_leading_term(x) == expr\n", "def test_Eq_is_equal():\n    from sympy import Eq\n    a, b = symbols('a b')\n    assert Eq(a, b) == Eq(a, b)\n    assert Eq(a, b) != Eq(b, a)\n    assert Eq(a, b) != Eq(a, a)\n    assert Eq(a, b) != Eq(a, a + 1)\n"], "sample_1174": ["def test_re():\n    # Test real part of zero\n    assert re(0) == 0\n    # Test real part of negative\n    assert re(-1) == -1\n    # Test real part of positive\n    assert re(1) == 1\n    # Test real part of imaginary\n    assert re(2*I) == 0\n    # Test real part of complex\n    assert N_equals(re(3 + 4*I), 3)\n    # Test real part of real number with square root\n    assert re(sqrt(4)) == 2\n    # Test real part of complex expression\n    x = Symbol('x')\n    assert re(x + x*I) == x\n", "def test_re_unpolarify():\n    x = Symbol('x', polar=True)\n    expr = re(2*x)\n    unpolar_expr = unpolarify(expr)\n    assert unpolar_expr == 2*x\n", "def test_unpolarify_polar_lift():\n    x = Symbol('x')\n    assert unpolarify(polar_lift(x)) == x\n    assert unpolarify(polar_lift(x**2)) == x**2\n    assert unpolarify(polar_lift(x**2 + 3)) == x**2 + 3\n    assert unpolarify(polar_lift(x**2 + 3)**2) == (x**2 + 3)**2\n    assert unpolarify(2*polar_lift(x) + 3) == 2*x + 3\n    assert unpolarify(2*polar_lift(x) + polar_lift(3)) == 2*x + 3\n    assert unpolarify(polar_lift(x) + polar_lift(3)) == x + 3\n    assert unpolarify(polar_lift(3)*polar_lift(x)) == 3*x\n    assert unpolarify(polar_lift(3)*x) == 3*x\n", "    def test_abs_of_number(self):\n        x = Symbol('x')\n        assert N_equals(Abs(x**2).simplify(), Abs(x**2).simplify())\n        assert N_equals(Abs(x**2 + 1).simplify(), Abs(x**2 + 1).simplify())\n", "def test_ArgumentIndexError():\n    x = symbols('x')\n    f = exp(x)\n    assert raises(ArgumentIndexError, lambda: f.diff(x, 1, 2))\n"], "sample_133": ["    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.locale = 'de'\n", "    def setUp(self):\n        self.factory = RequestFactory()\n        self.locale_dir = path.dirname(locale_dir('en'))\n", "    def test_get_paths(self):\n        # Test get_paths method returns the correct list of paths\n        app_configs = [\n            apps.get_app_config('django.contrib.admin'),\n            apps.get_app_config('django.contrib.auth'),\n            apps.get_app_config('django.contrib.contenttypes'),\n            apps.get_app_config('django.contrib.sessions'),\n            apps.get_app_config('django.contrib.messages'),\n            apps.get_app_config('django.contrib.staticfiles'),\n        ]\n        paths = JavaScriptCatalog().get_paths(['django.contrib.admin', 'django.contrib.auth'])\n        self.assertEqual(paths, [app.path for app in app_configs[:2]])\n\n        # Test get_paths raises ValueError when invalid package is provided\n        with self.assertRaises(ValueError):\n            JavaScriptCatalog().get_paths(['django.contrib.admin', 'invalid_package'])\n", "    def test_get_paths(self):\n        \"\"\"Test get_paths method.\"\"\"\n        request = RequestFactory.get('/')\n        view = JavaScriptCatalog()\n        view.request = request\n        # Test default values\n        self.assertEqual(view.get_paths(''), [])\n        self.assertEqual(view.get_paths(None), [])\n        # Test paths for single package\n        view.packages = 'django.conf'\n        self.assertEqual(view.get_paths('django.conf'), [path.join(settings.BASE_DIR, 'conf/locale')])\n        # Test paths for multiple packages\n        view.packages = 'django.conf+django.contrib.admin'\n        self.assertEqual(view.get_paths('django.conf+django.contrib.admin'),\n                         [path.join(settings.BASE_DIR, 'conf/locale'), path.join(settings.BASE_DIR, 'contrib/admin/locale')])\n        # Test invalid package\n        view.packages = 'django.conf+invalid'\n        with self.assertRaises(ValueError):\n            view.get_paths('django.conf+invalid')\n", "    def test_get_paths(self):\n        \"\"\"JavaScriptCatalog.get_paths() method should return a list of paths for the given packages.\"\"\"\n        request = RequestFactory().get('/')\n        view = JavaScriptCatalog()\n        packages = ['app1', 'app2']\n        expected_paths = [path.join(app_config.path, 'locale') for app_config in apps.get_app_configs() if app_config.name in packages]\n        self.assertEqual(view.get_paths(packages), expected_paths)\n"], "sample_1058": ["def test_print_Relational_with_SparseMatrix():\n    matrix = SparseMatrix([[1, 2], [3, 4]])\n    assert SciPyPrinter().doprint(Eq(matrix, 1)) == 'scipy.sparse.coo_matrix([[1, 2], [3, 4]]) == 1'\n    assert NumPyPrinter().doprint(Eq(matrix, 1)) == 'numpy.coo_matrix([[1, 2], [3, 4]]) == 1'\n", "def test_PythonCodePrinter_print_MatrixBase():\n    printer = PythonCodePrinter({'standard': 'python3'})\n    m = MatrixSymbol('M', 2, 2)\n    assert printer._print_MatrixBase(m) == 'M'\n", "def test_SciPyPrinter_and_Piecewise():\n    from sympy import Piecewise, sqrt, sin\n    from sympy.printing.pycode import SciPyPrinter\n\n    sp = SciPyPrinter()\n    assert sp.doprint(Piecewise((sin(x), x > 0), (0, True))) == \"scipy.special.select([x > 0, True], (sin(x), [0])\"\n", "def test_print_piecewise():\n    assert (\n        NumPyPrinter().doprint(Piecewise((Eq(x, 1), (x > 0), (x < 0, 2*x)))\n        ) == \"numpy.select([x > 0, x < 0], [1, 2*x], default=numpy.nan)\"\n    )\n    assert (\n        SciPyPrinter().doprint(Piecewise((Eq(x, 1), (x > 0), (x < 0, 2*x)))\n        ) == \"scipy.special.select([x > 0, x < 0], [1, 2*x], default=numpy.nan)\"\n    )\n    assert (\n        SymPyPrinter().doprint(Piecewise((Eq(x, 1), (x > 0), (x < 0, 2*x)))\n        ) == \"sympy.select([x > 0, x < 0], [1, 2*x], default=numpy.nan)\"\n    )\n", "def test_num_poly_print():\n    from sympy.core import Poly\n    from sympy.codegen.ast import none\n    expr = Poly([1, 2, 3, 4, 5], x)\n    printer = NumPyPrinter()\n    expected = \"numpy.array([1, 2, 3, 4, 5])\"\n    assert printer.doprint(expr) == expected\n"], "sample_828": ["def test_pairwise_distances_sparse_X_numpy_Y():\n    X = csr_matrix(np.array([[1, 2], [3, 4]]))\n    Y = np.array([[5, 6], [7, 8]])\n    distances = pairwise_distances(X, Y)\n    expected_distances = np.array([[3, 5], [5, 7]])\n    assert_array_almost_equal(distances.toarray(), expected_distances)\n", "def test_pairwise_distances_chunked_chunked():\n    X = np.array([[1, 2], [3, 4]])\n\n        return D[start:].sum(axis=1)\n\n    with pytest.raises(TypeError):\n        list(pairwise_distances_chunked(X, reduce_func=reduce_func, working_memory=0))\n    with pytest.raises(TypeError):\n        list(pairwise_distances_chunked(X, reduce_func=reduce_func, working_memory=\"0\"))\n\n        return [D[start:].sum(axis=1)]\n\n    result = list(pairwise_distances_chunked(X, reduce_func=reduce_func))\n    expected = [6.0, 12.0]\n    assert_array_almost_equal(result[0], expected)\n\n    result = list(pairwise_distances_chunked(X, reduce_func=reduce_func, working_memory=0))\n    expected = [[6.0], [12.0]]\n    assert_array_equal(result[0], expected)\n\n        return (D[start:].sum(axis=1), D[start:].mean(axis=1))\n\n    result = list(pairwise_distances_chunked(X, reduce_func=reduce_func))\n    expected = ([6.0, 12.0], [3.0, 6.0])\n    assert_array_almost_equal(result[0], expected[0])\n    assert_array_almost_equal(result[1], expected[1])\n\n    result = list(pairwise_distances_chunked(X, reduce_func=reduce_func, working_memory=0))\n    expected = ([6.0], [3.0])\n    assert_array_equal(result[0][0], expected[0])\n    assert_array_equal(result[1][0], expected[1])\n", "def test_pairwise_distances_argmin_min_subset():\n    # Test pairwise_distances_argmin_min with subset of points\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    Y = np.array([[7, 8], [9, 10]])\n    argmin, dist = pairwise_distances_argmin_min(X, Y, axis=1)\n    assert_equal(argmin, [1, 0])\n    assert_array_almost_equal(dist, [np.linalg.norm(X[1] - Y[0]),\n                                    np.linalg.norm(X[0] - Y[1])])\n", "def test_check_pairwise_arrays_not_sparse():\n    X = np.array([[1, 2], [3, 4]])\n    Y = np.array([[5, 6], [7, 8]])\n    with pytest.raises(ValueError) as excinfo:\n        check_pairwise_arrays(X, Y, precomputed=False)\n    assert \"Input contains non-numeric values and cannot be converted to a numpy array\" in str(excinfo.value)\n\n", "def test_pairwise_distances_chunked_invalid_metric():\n    X = np.random.RandomState(0).rand(5, 3)\n    with pytest.raises(ValueError):\n        next(pairwise_distances_chunked(X, metric=\"non_existent_metric\"))\n"], "sample_827": ["def test_csc_median_axis_0():\n    rs = RandomState(0)\n    n_samples, n_features = 100, 10\n    X = sp.csc_matrix(np.dot(rs.rand(n_samples, n_features), rs.rand(n_features, n_features)), dtype=np.float64)\n    median = csc_median_axis_0(X)\n    assert_array_almost_equal(median, np.median(X.toarray(), axis=0))\n", "def test_csr_mean_variance_axis():\n    # Test that mean and variance are correctly computed along axis 0\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    mean, var = mean_variance_axis(X, axis=0)\n    assert_array_almost_equal(mean, np.array([4., 5., 6.]))\n    assert_array_almost_equal(var, np.array([5., 5., 5.]))\n\n    # Test that mean and variance are correctly computed along axis 1\n    X = sp.csr_matrix(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n    mean, var = mean_variance_axis(X.T, axis=0)\n    assert_array_almost_equal(mean, np.array([3., 4., 5.]))\n\n    # Test that _raise_error_wrong_axis raises a ValueError\n    assert_raises(ValueError, mean_variance_axis, X, axis=2)\n\n    # Test that _raise_typeerror raises a TypeError\n    assert_raises(TypeError, _raise_typeerror, X)\n", "def test_incr_mean_variance_axis():\n    # Test case 1: Basic test\n    X = sp.csr_matrix([[1, 0, 2], [0, 3, 0]])\n    last_mean = np.array([0, 0])\n    last_var = np.array([0, 0])\n    last_n = 0\n    result = incr_mean_variance_axis(X, 0, last_mean, last_var, last_n)\n    expected_mean = np.array([0.5, 1.5])\n    expected_var = np.array([1, 1])\n    expected_n = 2\n    assert_array_almost_equal(result[0], expected_mean)\n    assert_array_almost_equal(result[1], expected_var)\n    assert_equal(result[2], expected_n)\n\n    # Test case 2: Incrementing existing means and variances\n    last_mean = np.array([0.5, 1.5])\n    last_var = np.array([1, 1])\n    last_n = 2\n    result = incr_mean_variance_axis(X, 0, last_mean, last_var, last_n)\n    expected_mean = np.array([0.5, 1.5])\n    expected_var = np.array([1, 1])\n    expected_n = 4\n    assert_array_almost_equal(result[0], expected_mean)\n    assert_array_almost_equal(result[1], expected_var)\n    assert_equal(result[2], expected_n)\n\n    # Test case 3: Test with NaN values\n    X = sp.csr_matrix([[1, np.nan, 2], [0, 3, 0]])\n    last_mean = np.array([0, 0])\n    last_var = np.array([0, 0])\n    last_n = 0\n    result = incr_mean_variance_axis(X, 0, last_mean, last_var, last_n)\n    expected_mean = np.array([0.5, 1.5])\n    expected_var = np.array([1, 1])\n    expected_n = 2\n    assert_array_almost_equal(result[0], expected_mean)\n    assert_array_almost_equal(result[1], expected_var)\n    assert_equal(result[2], expected_n)\n\n    # Test case 4: Test with negative values\n    X = sp.csr_matrix([[-1, 0, 2], [0, 3, 0]])\n    last_mean = np.array([0,", "def test_csc_median_axis_0():\n    rng = RandomState(0)\n    X = sp.csc_matrix(rng.rand(10, 5), dtype=np.float64)\n\n    median = csc_median_axis_0(X)\n    assert_array_almost_equal(median, np.median(X.toarray(), axis=0))\n\n    # Test with all zeros\n    X = sp.csc_matrix(np.zeros((10, 5)), dtype=np.float64)\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.zeros(5))\n\n    # Test with all negative numbers\n    X = sp.csc_matrix(-rng.rand(10, 5), dtype=np.float64)\n    median = csc_median_axis_0(X)\n    assert_array_almost_equal(median, np.median(X.toarray(), axis=0))\n\n    # Test with NaN values\n    X = sp.csc_matrix(rng.rand(10, 5))\n    X.data[0] = np.nan\n    median = csc_median_axis_0(X)\n    assert_array_almost_equal(median, np.nan)\n\n    # Test with too few samples\n    X = sp.csc_matrix(rng.rand(2, 5), dtype=np.float64)\n    median = csc_median_axis_0(X)\n    assert_array_almost_equal(median, np.median(X.toarray(), axis=0))\n\n    # Test with an empty matrix\n    X = sp.csc_matrix((0, 5), dtype=np.float64)\n    median = csc_median_axis_0(X)\n    assert_array_equal(median, np.zeros(5))\n", "def test_csc_median_axis_0_empty():\n    \"\"\"Tests that csc_median_axis_0 raises an error when the matrix is empty.\"\"\"\n    X = sp.csc_matrix((0, 5))\n    with pytest.raises(ValueError):\n        csc_median_axis_0(X)\n\n"], "sample_154": ["    def test_database_check_raises_error_when_database_does_not_exist(self, mock_check):\n        with self.assertRaises(KeyError):\n            check_database_backends(databases=['non_existent'])\n", "    def test_empty_databases_list(self, mock_check):\n        mock_check.return_value = []\n        result = check_database_backends(databases=[])\n        self.assertEqual(result, [])\n", "    def test_check_database_backends_with_empty_databases(self, mock_check):\n        issues = check_database_backends(databases={'default', 'other'})\n        mock_check.assert_called_once_with(connection=connection)\n        self.assertEqual(issues, [])\n", "    def test_check_database_backends_return_empty_list_if_no_databases(self, mock_check):\n        # Given\n        databases = {}\n        mock_check.return_value = []\n\n        # When\n        result = check_database_backends(databases)\n\n        # Then\n        self.assertEqual(result, [])\n", "def test_check_database_backends_empty_database(self):\n    # Test that an empty databases list returns an empty list\n    result = check_database_backends()\n    self.assertEqual(result, [])\n"], "sample_319": ["    def test_custom_user_destructible_object_default(self):\n        initial_state = self.make_project_state([\n            self.custom_user,\n        ])\n        final_state = self.make_project_state([\n            self.custom_user_no_inherit,\n        ])\n        changes = self.get_changes(initial_state, final_state)\n        self.assertNumberMigrations(changes, \"thirdapp\", 1)\n        self.assertOperationTypes(changes, \"thirdapp\", 0, [\"DeleteModel\"])\n        self.assertOperationAttributes(\n            changes, \"thirdapp\", 0, 0, name=\"CustomUser\", managers=[]\n        )\n", "    def test_altered_options(self):\n        author_with_options = self.make_project_state([self.author_with_options])\n        before_states = [ProjectState()]\n        after_states = [self.make_project_state([self.author_with_options])]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n", "    def test_generate_renamed_fields_simple(self):\n        author_name_renamed = self.author_name_renamed.clone()\n        author_name_renamed.name = \"NewAuthor\"\n        author_name_renamed.fields[1] = (\"names\", models.CharField(max_length=200))\n        before_states = [self.author_name, author_name_renamed]\n        after_states = [self.author_name_renamed]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 0, ((\"__app__\", \"__first__\",),))\n        self.assertMigrationDependencies(changes, \"testapp\", 1, ((\"__app__\", \"Author\",),))\n        self.assertOperationTypes(changes, \"testapp\", 0, ('RenameModel',))\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, old_name=\"Author\", new_name=\"NewAuthor\")\n        self.assertOperationTypes(changes, \"testapp\", 1, ('RenameField',))\n        self.assertOperationAttributes(\n            changes, \"testapp\", 1, 0, old_name=\"name\", new_name=\"names\"\n        )\n", "    def test_alter_model_options(self):\n        before_states = [self.author_with_options, self.author_with_db_table_comment]\n        after_states = [\n            self.author_with_options,\n            self.author_with_db_table_options,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, options={\"verbose_name\": \"Authi\"}\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, options={\"db_table\": \"author_one\"}\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, options={\"db_table\": \"author_two\"}\n        )\n", "    def test_altered_index_together(self):\n        # Test that AlterIndexTogether is used when unique_together changes.\n        from django.db.migrations.operations import AddConstraint\n\n        before_states = [\n            self.author_dates_of_birth_auto_now,\n            self.contract,\n            self.publisher,\n        ]\n        after_states = [\n            self.author_dates_of_birth_auto_now,\n            self.contract,\n            self.publisher,\n            self.book_unique_together,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterIndexTogether\"])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelOptions\"])\n        self.assertMigrationDependencies(\n            changes, \"otherapp\", 0, [(\"testapp\", \"__first__\")]\n        )\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\")\n"], "sample_415": ["    def test_check_constraint_deconstruction(self):\n        constraint = CheckConstraint(check=Q(a=1), name='test_check_constraint')\n        self.assertEqual(constraint.deconstruct(), (\n            \"django.db.models.constraints.CheckConstraint\",\n            (),\n            {\"name\": 'test_check_constraint', \"check\": Q(a=1)}\n        ))\n", "    def test_create_sql_deferrable(self):\n        product = Product(name=\"Test Product\", price=10.99)\n        product.save()\n        constraint = UniqueConstraint(\n            model=Product,\n            fields=[\"name\"],\n            name=\"test_unique_constraint\",\n            deferrable=Deferrable.DEFERRED,\n        )\n        with self.subTest(deferrable=True):\n            with atomic():\n                with connection.schema_editor(using=\"default\"):\n                    sql = constraint.create_sql(product, connection.schema_editor())\n                    self.assertIn(\"DEFERRABLE\", sql)\n                    self.assertIn(\"INITIALLY DEFERRED\", sql)\n", "    def test_unique_constraint_deferrable_immediate(self):\n        with atomic():\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintDeferrable.create()\n", "    def test_check_constraint_with_invalid_check(self):\n        with self.assertRaises(TypeError):\n            CheckConstraint(check=True, name='test_constraint')\n", "    def test_deconstruct(self):\n        check = Q(field__gt=1)\n        constraint = CheckConstraint(check=check, name=\"test_constraint\")\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.constraints.CheckConstraint\")\n        self.assertEqual(args, ())\n        self.assertEqual(\n            kwargs,\n            {\n                \"name\": \"test_constraint\",\n                \"check\": check,\n            },\n        )\n"], "sample_826": ["def test_OneHotEncoder_handle_unknown_error_with_sparse_input():\n    \"\"\"Tests that OneHotEncoder raises a ValueError with handle_unknown='error' when given sparse input.\"\"\"\n    \n    X = sparse.csr_matrix([[0, 1], [1, 0]])\n    encoder = OneHotEncoder(handle_unknown='error')\n    with assert_raises_regex(ValueError, \"Found unknown categories\"):\n        encoder.fit_transform(X)\n", "def test_OneHotEncoder_handle_unknown_error_with_drop():\n    \"\"\"Test OneHotEncoder handle_unknown='error' with drop.\"\"\"\n    enc = OneHotEncoder(handle_unknown='error', drop='first')\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    enc.fit(X)\n    X_new = [['Male', 4], ['Female', 3]]\n    with pytest.raises(ValueError):\n        enc.transform(X_new)\n", "def test_ordinal_encoder_with_unknown_category():\n    # Test that ordinal encoder handles unknown category correctly\n    X = [['Male', 1], ['Female', 3], ['Female', 2], ['Unknown', 1]]\n    encoder = OrdinalEncoder()\n    with pytest.raises(NotFittedError):\n        encoder.transform(X)\n\n    encoder.fit(X)\n    X_transformed = encoder.transform(X)\n    X_inverse = encoder.inverse_transform(X_transformed)\n\n    assert_array_equal(X_inverse, X)\n", "def test_onehot_encoder_categorical_features_boolean():\n    X = np.array([[True, False], [False, True], [True, True]])\n    encoder = OneHotEncoder(categorical_features=[0], dtype=np.float64)\n    encoder.fit(X)\n    X_onehot = encoder.transform(X)\n    X_expected = np.array([[1., 0.],\n                          [0., 1.],\n                          [1., 1.]])\n    assert_array_equal(toarray(X_onehot), X_expected)\n    assert_array_equal(encoder.get_feature_names(), np.array(['x0_T', 'x0_F', 'x1_F', 'x1_T']))\n    assert_array_equal(encoder.inverse_transform(X_onehot), X)\n", "def test_ordinal_encoder_fit_transform():\n    encoder = OrdinalEncoder()\n    X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    encoder.fit(X)\n    X_transformed = encoder.transform(X)\n    assert_array_equal(X_transformed, np.array([[0., 2.], [1., 0.]]))\n    assert_array_equal(encoder.inverse_transform(X_transformed), np.array([['Male', 1], ['Female', 2]]))\n"], "sample_781": ["def test_feature_importances(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    rng = check_random_state(0)\n    X = rng.rand(100, 10)\n    y = 3 * X[:, 0] + 2 * X[:, 1] + rng.randn(100)\n\n    for cls in [ForestClassifier, ForestRegressor]:\n        forest = cls(n_estimators=10, random_state=0)\n        forest.fit(X, y)\n        assert_greater(forest.feature_importances_.sum(), 0)\n        assert_less(forest.feature_importances_.max(), 1)\n\n        forest = cls(n_estimators=10, random_state=0, max_features=1)\n        forest.fit(X, y)\n        assert_greater(forest.feature_importances_.sum(), 0)\n        assert_less(forest.feature_importances_.max(), 1)\n\n        forest = cls(n_estimators=10, random_state=0, max_features=2)\n        forest.fit(X, y)\n        assert_greater(forest.feature_importances_.sum(), 0)\n        assert_less(forest.feature_importances_.max(), 1)\n", "def test_random_trees_embedding_sparse_output():\n    \"\"\"Check RandomTreesEmbedding with sparse output.\"\"\"\n    X = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    forest = RandomTreesEmbedding(n_estimators=10, sparse_output=True)\n    X_embedded = forest.fit_transform(X)\n    assert isinstance(X_embedded, csr_matrix)\n    assert X_embedded.getformat() == 'csr'\n\n    forest = RandomTreesEmbedding(n_estimators=10, sparse_output=False)\n    X_embedded = forest.fit_transform(X)\n    assert isinstance(X_embedded, np.ndarray)\n    assert X_embedded.dtype == np.float32\n", "def test_random_forest_regressor_zero_estimators():\n    with pytest.raises(ValueError):\n        RandomForestRegressor(n_estimators=0).fit(X=boston.data, y=boston.target)\n", "def test_fit_transform():\n    \"\"\"Check fit_transform behavior for RandomTreesEmbedding.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.rand(100, 10)\n\n    # Test that the code does not crash\n    transformer = RandomTreesEmbedding(n_estimators=10, random_state=0)\n    transformer.fit_transform(X)\n\n    # Test that the result is correct\n    transformer = RandomTreesEmbedding(n_estimators=10, random_state=0)\n    X_embedded = transformer.fit_transform(X)\n    assert X_embedded.shape == (100, transformer.n_features_)\n\n    # Test that the fit_transform method can handle sparse matrices\n    X_sparse = csr_matrix(X)\n    transformer = RandomTreesEmbedding(n_estimators=10, random_state=0)\n    X_embedded_sparse = transformer.fit_transform(X_sparse)\n    assert X_embedded_sparse.shape == (100, transformer.n_features_)\n\n    # Test that the fit method can handle sparse matrices\n    X_sparse = csr_matrix(X)\n    transformer = RandomTreesEmbedding(n_estimators=10, random_state=0)\n    transformer.fit(X_sparse)\n    assert transformer.transform(X_sparse).shape == (100, transformer.n_features_)\n", "def test_forest_regressor_sanity(name):\n    \"\"\"Test forest regressor.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n    \n    # toy sample\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1]]\n    y = [-1, -1, -1, 1, 1, 1]\n    true_result = [-2.6, -1.6, -2.6, 0.6, 0.6, 1.6]\n\n    # Test regression with default parameters\n    clf = ForestRegressor(n_estimators=10, random_state=1)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and n_jobs=-1\n    with parallel_backend('multiprocessing', n_jobs=-1):\n        clf = ForestRegressor(n_estimators=10, random_state=1)\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and n_jobs=4\n    with parallel_backend('multiprocessing', n_jobs=4):\n        clf = ForestRegressor(n_estimators=10, random_state=1)\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and max_depth=5\n    clf = ForestRegressor(n_estimators=10, max_depth=5, random_state=1)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and max_features=None\n    clf = ForestRegressor(n_estimators=10, max_features=None, random_state=1)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and min_samples_split=5\n    clf = ForestRegressor(n_estimators=10, min_samples_split=5, random_state=1)\n    clf.fit(X, y)\n    assert_array_almost_equal(clf.predict(X), true_result, 2)\n\n    # Test regression with default parameters and min_samples_leaf=5\n    clf = ForestRegressor(n_estimators=10, min_samples"], "sample_195": ["    def test_date_extract_sql(self):\n        self.assertEqual(self.ops.date_extract_sql('year', 'my_date_field'), \"django_date_extract('year', 'my_date_field')\")\n", "    def test_datetime_trunc_sql_with_tzname(self):\n        ops = DatabaseOperations(connection=connection)\n        self.assertEqual(ops.datetime_trunc_sql('second', 'field_name'), \"django_datetime_trunc('second', 'field_name', NULL, NULL)\")\n", "    def setUp(self):\n        self.ops = BaseDatabaseOperations(connection=connection)\n", "    def test_combine_duration_expression(self):\n        ops = BaseDatabaseOperations(connection=connection)\n        self.assertEqual(ops.combine_duration_expression('+', ['1 day', '2 days']), \"django_format_dtdelta('+', '1 day', '2 days')\")\n        self.assertEqual(ops.combine_duration_expression('-', ['1 day', '2 days']), \"django_format_dtdelta('-', '1 day', '2 days')\")\n\n        with self.assertRaises(ValueError):\n            ops.combine_duration_expression('*', ['1 day', '2 days'])\n\n        with self.assertRaises(ValueError):\n            ops.combine_duration_expression('+', ['1 day', '2 days', '3 days'])\n", "    def test_bulk_batch_size(self):\n        # Test bulk_batch_size returns 999 when more than one field is given\n        ops = DatabaseOperations(connection=connection)\n        self.assertEqual(ops.bulk_batch_size(('field1', 'field2'), ['obj1', 'obj2']), 999)\n\n        # Test bulk_batch_size returns 500 when one field is given\n        self.assertEqual(ops.bulk_batch_size(('field1',), ['obj1', 'obj2']), 500)\n\n        # Test bulk_batch_size returns the length of the list when no fields are given\n        self.assertEqual(ops.bulk_batch_size((), ['obj1', 'obj2']), 2)\n\n        # Test bulk_batch_size raises an error when fields and objs have different lengths\n        with self.assertRaises(ValueError):\n            ops.bulk_batch_size(('field1', 'field2', 'field3'), ['obj1', 'obj2'])\n\n        # Test bulk_batch_size returns the correct value when objs is an empty list\n        self.assertEqual(ops.bulk_batch_size(('field1', 'field2'), []), 0)\n"], "sample_1152": ["def test_powsimp_dict():\n    from sympy.abc import x, y, z\n    from sympy import powsimp, exp, log\n\n    # Test powsimp with a dictionary input\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4})\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and combine='all'\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, combine='all')\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and combine='exp'\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, combine='exp')\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and combine='base'\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, combine='base')\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and force=True\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, force=True)\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and deep=True\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, deep=True)\n    assert expr == {'a': x**2, 'b': y**3, 'c': z**4}\n\n    # Test powsimp with a dictionary input and measure=count_ops\n    expr = powsimp({'a': x**2, 'b': y**3, 'c': z**4}, measure=count_ops)\n    assert expr == {'a': x**2, 'b': y**", "def test_powsimp_polar():\n    from sympy import powsimp, PolarFunction, polarify, unpolarify\n    a = symbols('a', polar=True)\n    b = symbols('b', polar=False)\n    assert powsimp((a**a)**b) == a**b\n    assert powsimp((b**a)**b) == b**(a*b)\n    assert powsimp((a**a)**a) == a**(a**2)\n    assert powsimp((b**a)**a) == b**(a**2)\n", "def test_powsimp_with_polar_numbers():\n    from sympy import powsimp, sqrt, exp, exp_polar, log, symbols\n    x, y = symbols('x y', real=True)\n    p = symbols('p', polar=True)\n    assert powsimp((p**2)**(x/3)) == (p**(2*x/3))\n    assert powsimp(p**x * p**(y)) == p**(x+y)\n    assert powsimp(exp_polar(x)*exp_polar(y)) == exp_polar(x+y)\n    assert powsimp(sqrt(p)**x * sqrt(p)**y) == (sqrt(p))**(x+y)\n    assert powsimp((p**x)**y) == p**(x*y)\n", "def test_powsimp_radical_distributive():\n    from sympy.abc import m, x, y\n    x, y = symbols('x y', positive=True)\n    assert powsimp(root(x, 4)**(y)) == root(x, 4)**y\n    assert powsimp(root(x**2, 3)**(y)) == root(x**2, 3)**y\n    assert powsimp((x**2)**(y/2)) == (x**y)**(1/2)\n    assert powsimp(root(x, 2)**(y/2)) == (x**(y/2))\n    assert powsimp(root(x**3, 6)**(2*y)) == (x**(y/3))**(2)\n", "def test_powsimp_noncommutative():\n    from sympy.core.compatibility import ordering\n    x, y = symbols('x y', commutative=False)\n    eq = (x + y)**4\n    assert not powsimp(eq).is_Atom\n    assert powsimp(eq).is_Mul\n    assert powsimp(eq).args[0] == x + y\n    assert powsimp(eq).args[1] == 4\n    # Test that powsimp combines commuting and non-commuting parts separately\n    eq = (x + y)*x\n    expected = Mul(x + y, x)\n    assert powsimp(eq) == expected\n    # Test that powsimp combines non-commuting parts first\n    eq = (y + x)*y\n    expected = y*(y + x)\n    assert powsimp(eq) == expected\n    # Test that powsimp handles powsimp of non-commuting parts correctly\n    eq = (y + x)**3\n    expected = y**3 + 3*y**2*x + 3*y*x**2 + x**3\n    assert powsimp(eq) == expected\n"], "sample_934": ["def test_c_ptr_to_ptr(self):\n    input = \"void *ptr;\"\n    idDict = {\n        1: \"c.ptr.1\",\n        2: \"c.ptr.2\",\n    }\n    output = \"void *ptr;\"\n    key = None\n    asTextOutput = None\n    _check('member', input, idDict, output, key, asTextOutput)\n\n    input = \"void (*ptr)();\"\n    idDict = {\n        1: \"c.ptr.1\",\n        2: \"c.ptr.2\",\n    }\n    output = \"void (*ptr)(void);\"\n    key = None\n    asTextOutput = None\n    _check('function', input, idDict, output, key, asTextOutput)\n", "def test_declaration_object_type():\n    input = \"int x;\"\n    idDict = {2: \"x_2\", 1: \"x\"}\n    output = \"int x;\"\n    key = None\n    asTextOutput = \"int x;\"\n    _check('int', input, idDict, output, key, asTextOutput)\n", "def test_ASTIdentifier_is_anon():\n    ast = ASTIdentifier(identifier='@anonymous')\n    assert ast.is_anon() is True\n    ast = ASTIdentifier(identifier='not_anonymous')\n    assert ast.is_anon() is False\n", "def test_attribute_parsing():\n    # Test that we can parse attributes correctly.\n    input = 'int foo __attribute__((not an attribute)) = 42;'\n    output = 'int foo __attribute__((not an attribute)) = 42;'\n    idDict = {1: 'c.int.foo', 2: 'c.int.foo', 3: 'c.int.foo'}\n    _check('member', input, idDict, output)\n\n    # Test that we can parse attribute names with commas correctly.\n    input = 'int foo __attribute__((one, two, three)) = 42;'\n    output = 'int foo __attribute__((one, two, three)) = 42;'\n    idDict = {1: 'c.int.foo', 2: 'c.int.foo', 3: 'c.int.foo'}\n    _check('member', input, idDict, output)\n\n    # Test that we can parse attribute names with parentheses correctly.\n    input = 'int foo __attribute__((one(one, two))) = 42;'\n    output = 'int foo __attribute__((one(one, two))) = 42;'\n    idDict = {1: 'c.int.foo', 2: 'c.int.foo', 3: 'c.int.foo'}\n    _check('member', input, idDict, output)\n\n    # Test that we can parse attribute names with quotes correctly.\n    input = 'int foo __attribute__((\\\"one\\\", \\\"two\\\")) = 42;'\n    output = 'int foo __attribute__((\\\"one\\\", \\\"two\\\")) = 42;'\n    idDict = {1: 'c.int.foo', 2: 'c.int.foo', 3: 'c.int.foo'}\n    _check('member', input, idDict, output)\n\n    # Test that we can parse attribute names with double quotes correctly.\n    input = 'int foo __attribute__((\\\"one\\\" \\\"two\\\")) = 42;'\n    output = 'int foo __attribute__((\\\"one\\\" \\\"two\\\")) = 42;'\n    idDict = {1: 'c.int.foo', 2: 'c.int.foo', 3: 'c.int.foo'}\n    _check('member', input, idDict, output)\n\n    # Test that we can parse attribute names with backticks correctly.\n    input = 'int foo __attribute__((`one`, `two`)) = 42;'\n    output = 'int foo __attribute__((`one`, `", "def test_declarator():\n    # TODO: this needs to be in test_domains.py, but we don't have a test for that\n    #       case\n    check('member', 'int* foo',\n          {1: 'foo'}, 'int* foo')\n    check('member', 'int** foo',\n          {1: 'foo'}, 'int** foo')\n    check('member', 'int *** foo',\n          {1: 'foo'}, 'int *** foo')\n    check('member', 'int * foo[]',\n          {1: 'foo'}, 'int * foo[]')\n    check('member', 'int ** foo[]',\n          {1: 'foo'}, 'int ** foo[]')\n    check('member', 'int *** foo[]',\n          {1: 'foo'}, 'int *** foo[]')\n    check('member', 'int * foo[10]',\n          {1: 'foo'}, 'int * foo[10]')\n    check('member', 'int ** foo[10]',\n          {1: 'foo'}, 'int ** foo[10]')\n    check('member', 'int *** foo[10]',\n          {1: 'foo'}, 'int *** foo[10]')\n    check('member', 'int* const foo',\n          {1: 'foo'}, 'int* const foo')\n    check('member', 'int* volatile foo',\n          {1: 'foo'}, 'int* volatile foo')\n    check('member', 'int* restrict foo',\n          {1: 'foo'}, 'int* restrict foo')\n    check('member', 'int* const volatile foo',\n          {1: 'foo'}, 'int* const volatile foo')\n    check('member', 'int* const restrict foo',\n          {1: 'foo'}, 'int* const restrict foo')\n    check('member', 'int* volatile restrict foo',\n          {1: 'foo'}, 'int* volatile restrict foo')\n    check('member', 'int* const volatile restrict foo',\n          {1: 'foo'}, 'int* const volatile restrict foo')\n    check('member', 'int* foo()',\n          {1: 'foo'}, 'int* foo()')\n    check('member', 'int* (*foo())',\n          {1: 'foo'}, 'int* (*foo())')\n    check('member', 'int* (*foo())()',\n          {1: 'foo'}, 'int* (*foo())()')\n    check('"], "sample_132": ["    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n        self.assertIn('DEBUG', settings_dict)\n        self.assertEqual(settings_dict['DEBUG'], '********************')\n", "    def test_get_safe_settings(self):\n        filter = SafeExceptionReporterFilter()\n        settings_dict = filter.get_safe_settings()\n        self.assertIsInstance(settings_dict, dict)\n\n        # Test a non-sensitive setting\n        self.assertEqual(settings_dict.get('DEBUG'), settings.DEBUG)\n\n        # Test a sensitive setting\n        self.assertEqual(settings_dict.get('SECRET_KEY'), SafeExceptionReporterFilter.cleansed_substitute)\n", "    def test_sensitive_variables_decorator(self):\n        # Test that variables decorated with sensitive_variables are redacted\n        # in the traceback frame variables.\n        with override_settings(DEBUG=True):\n            class MyClass:\n                @sensitive_variables('__all__')\n                    self.arg1 = 'sensitive'\n                    self.arg2 = 'not sensitive'\n\n            instance = MyClass()\n            reporter = get_exception_reporter_filter(self.request)\n            reporter.filter = SafeExceptionReporterFilter()\n            frame = {\n                'f_locals': {'self': instance, 'args': (), 'kwargs': {}},\n                'f_back': inspect.currentframe().f_back,\n            }\n            self.assertEqual(reporter.filter.get_traceback_frame_variables(self.request, frame)['arg1'], '*********')\n            self.assertEqual(reporter.filter.get_traceback_frame_variables(self.request, frame)['arg2'], 'not sensitive')\n", "    def test_cleanses_setting_key(self):\n        filter = SafeExceptionReporterFilter()\n        self.assertEqual(filter.cleanse_setting('API_KEY', 'my_api_key'), '********************')\n        self.assertEqual(filter.cleanse_setting('MY_SETTING', 'value'), 'value')\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 123), 123)\n        self.assertEqual(filter.cleanse_setting('SENSITIVE_VAR', [1, 2, 3]), [1, 2, 3])\n        self.assertEqual(filter.cleanse_setting('SENSITIVE_DICT', {'key': 'value'}), {'key': 'value'})\n", "    def test_cleansing_setting_value(self):\n        filter = SafeExceptionReporterFilter()\n        setting = 'SOME_SETTING'\n        setting_value = {'API_KEY': 'secret'}\n        cleaned_value = filter.cleanse_setting(setting, setting_value)\n        self.assertEqual(cleaned_value, {'API_KEY': filter.cleansed_substitute})\n"], "sample_731": ["def test_california_housing_feature_names_order():\n    \"\"\"Check if the feature names are in the correct order.\"\"\"\n    data = fetch()\n    assert np.array_equal(data.feature_names, [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\",\n                                            \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"])\n\n", "def test_california_housing_return_X_y_shape():\n    data, target = fetch_california_housing(return_X_y=True)\n    assert data.shape == (20640, 8)\n    assert target.shape == (20640,)\n", "def test_fetch_california_housing_missing_data_home():\n    from tempfile import NamedTemporaryFile\n    import os\n    # Create a temporary directory\n    data_home = NamedTemporaryFile().name\n    # Remove the directory to test the download\n    os.rmdir(data_home)\n    try:\n        fetch_california_housing(data_home=data_home, download_if_missing=True)\n        assert False, \"Expected IOError\"\n    except IOError:\n        pass\n", "def test_fetch_california_housing_data_home_creation():\n    # Test that data_home is correctly created when it doesn't exist\n    import os\n    import shutil\n    import tempfile\n    import unittest\n    from sklearn.datasets import fetch_california_housing\n\n    # Create a temporary directory\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Make sure the directory is not already a data_home\n        os.environ['SCIKIT_LEARN_DATA_HOME'] = None\n\n        # Fetch the data\n        data = fetch_california_housing(data_home=tmpdir)\n\n        # Check that data_home was created\n        assert os.path.exists(tmpdir)\n        assert os.path.isdir(tmpdir)\n\n        # Check that the data was stored in the data_home\n        assert os.path.exists(os.path.join(tmpdir, 'california_housing.pklz'))\n        shutil.rmtree(tmpdir)\n", "def test_fetch_california_housing_data_home_not_exists(self):\n    import tempfile\n    data_home = tempfile.mkdtemp()\n    with tempfile.NamedTemporaryFile(mode='w') as f:\n        f.write(str(data_home))\n        f.flush()\n        os.environ['SCIKIT_LEARN_DATA'] = f.name\n        try:\n            fetch_california_housing(data_home=data_home, download_if_missing=False)\n            self.fail(\"Expected IOError\")\n        except IOError as e:\n            self.assertEqual(str(e), \"Data not found and `download_if_missing` is False\")\n"], "sample_603": ["def test_obj_repr(obj_type, obj, expected):\n    assert repr(obj) == expected\n", "def test_summarize_attrs(ds, expected_attrs):\n    attrs_dl = summarize_attrs(ds.attrs)\n    attrs_dl = attrs_dl.replace(\"\\n\", \"\")\n    expected_attrs = \"\\n\".join(expected_attrs)\n    assert attrs_dl == expected_attrs\n", "def test_collapsible_section_expanded_and_collapsed():\n    text = collapsible_section(\n        \"Test section\",\n        inline_details=\"Some details\",\n        details=\"More details\",\n        n_items=10,\n        enabled=True,\n        collapsed=False,\n    )\n    assert \"<input\" in text\n    assert \"<label\" in text\n    assert \"Some details\" in text\n    assert \"<div\" in text\n\n    text = collapsible_section(\n        \"Test section\",\n        inline_details=\"Some details\",\n        details=\"More details\",\n        n_items=10,\n        enabled=True,\n        collapsed=True,\n    )\n    assert \"<input\" in text\n    assert \"<label\" in text\n    assert \"Some details\" in text\n    assert \"<div\" in text\n    assert \"checked\" not in text\n", "def test_dataset_repr_html():\n    # Test a dataset with multiple data variables and attributes\n    ds = dataset()\n    html_repr = dataset_repr(ds)\n    assert 'xarray.Dataset' in html_repr\n    assert \"tmin\" in html_repr\n    assert \"tmax\" in html_repr\n    assert \"description\" in html_repr\n    assert \"Test data.\" in html_repr\n\n    # Test a dataset with a multi-dimensional data variable\n    ds = xr.Dataset({\"v\": ([\"x\", \"y\"], np.random.rand(2, 2))})\n    html_repr = dataset_repr(ds)\n    assert 'xarray.Dataset' in html_repr\n    assert \"v\" in html_repr\n\n    # Test a dataset with no data variables\n    ds = xr.Dataset(attrs={\"description\": \"Test data.\"})\n    html_repr = dataset_repr(ds)\n    assert 'xarray.Dataset' in html_repr\n    assert \"description\" in html_repr\n    assert \"Test data.\" in html_repr\n\n    # Test a dataset with a large number of attributes\n    ds = xr.Dataset(attrs={str(i): i for i in range(10)})\n    html_repr = dataset_repr(ds)\n    assert 'xarray.Dataset' in html_repr\n    assert \"10\" in html_repr\n", "def test_collapsible_section():\n    sections = [\n        collapsible_section(\"Section 1\", inline_details=\"Details 1\", details=\"More details 1\"),\n        collapsible_section(\"Section 2\", inline_details=\"Details 2\", details=\"More details 2\", n_items=10),\n        collapsible_section(\"Section 3\", inline_details=\"Details 3\", details=\"More details 3\", collapsed=True),\n        collapsible_section(\"Section 4\", inline_details=\"Details 4\", details=\"More details 4\", enabled=False),\n    ]\n\n    assert len(sections) == 4\n    assert sections[0] == (\n        \"<input id='section-4f1f9507b3c24a388544c0c7e2a8ed8f' \"\n        \"class='xr-section-summary-in' type='checkbox'>\"\n        \"<label for='section-4f1f9507b3c24a388544c0c7e2a8ed8f' class='xr-section-summary' \"\n        \"title='Expand/collapse section'>\"\n        \"Section 1:</label>\"\n        \"<div class='xr-section-inline-details'>Details 1</div>\"\n        \"<div class='xr-section-details'>More details 1</div>\"\n    )\n    assert sections[1] == (\n        \"<input id='section-9f9e9507b3c24a388544c0c7e2a8ed8f' \"\n        \"class='xr-section-summary-in' type='checkbox' disabled>\"\n        \"<label for='section-9f9e9507b3c24a388544c0c7e2a8ed8f' class='xr-section-summary' \"\n        \"title='Expand/collapse section'>\"\n        \"Section 2:</label>\"\n        \"<div class='xr-section-inline-details'>Details 2</div>\"\n        \"<div class='xr-section-details'>More details 2</div>\"\n        \"<span>(10)</span>\"\n    )\n    assert sections[2] == (\n        \"<input id='section-4f1f9507b3c24a388544c0c7e2a8ed8f' \"\n        \"class='xr-section-summary-in' type='checkbox' checked>\"\n        \"<label for='section-4f1f9507b3c24a388544c0c7e2a"], "sample_935": ["def test_fold_expression():\n    input = \"int a = 0; a = 1;\"\n    idDict = {\n        1: \"A\",\n        2: \"Aa\",\n        3: \"Aaa\",\n        4: \"Aaaa\"\n    }\n    output = \"int a = 0; a = 1;\"\n    key = None\n    asTextOutput = None\n    _check('fold-expression', input, idDict, output, key, asTextOutput)\n", "def test_alias_object():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser('::std::foo', location=None, config=Config())\n    ast = parser.parse_declaration('class', 'class')\n    parser.assert_end()\n    assert ast.objectType == 'class'\n    assert ast.directiveType == 'class'\n    assert ast.visibility is None\n    assert ast.templatePrefix is None\n    assert ast.requiresClause is None\n    assert ast.declaration.objectType == 'class'\n    assert ast.declaration.directiveType == 'class'\n    assert ast.declaration.visibility is None\n    assert ast.declaration.templatePrefix is None\n    assert ast.declaration.requiresClause is None\n    assert ast.declaration.name == ast\n    assert ast.declaration.function_params is None\n    assert ast.declaration.get_newest_id() == 'std::fooCPPv3E'\n    assert ast.declaration.get_id(1) == 'std::foo'\n    assert ast.declaration.get_id(2) == 'Ststd::fooE'\n\n    ast = parse('member', '::std::foo')\n    assert ast.objectType == 'member'\n    assert ast.directiveType == 'member'\n    assert ast.visibility is None\n    assert ast.templatePrefix is None\n    assert ast.requiresClause is None\n    assert ast.declaration.objectType == 'member'\n    assert ast.declaration.directiveType == 'member'\n    assert ast.declaration.visibility is None\n    assert ast.declaration.templatePrefix is None\n    assert ast.declaration.requiresClause is None\n    assert ast.declaration.name == ast\n    assert ast.declaration.function_params is None\n    assert ast.declaration.get_newest_id() == 'std::fooCPPv3E'\n    assert ast.declaration.get_id(1) == 'std::foo'\n    assert ast.declaration.get_id(2) == 'Ststd::fooE'\n\n    ast = parse('type', '::std::foo')\n    assert ast.objectType == 'type'\n    assert ast.directiveType == 'type'\n    assert ast.visibility is None\n    assert ast.templatePrefix is None\n    assert ast.requiresClause is None\n    assert ast.declaration.objectType == 'type'\n    assert ast.declaration.directiveType == 'type'\n    assert ast.declaration.visibility is None\n    assert ast.declaration.templatePrefix is None\n    assert ast.declaration.requiresClause is None\n", "def test_lookup():\n    root = Symbol(None, None, None, None, None, None)\n    root2 = Symbol(root, None, None, None, None, None)\n    root3 = Symbol(root2, None, None, None, None, None)\n    root4 = Symbol(root3, None, None, None, None, None)\n\n    symbol = root.add_declaration(ASTDeclaration('function', 'function', None,\n                                                 None, None, None, None),\n                                  docname=\"TestDoc\")\n\n    assert symbol.siblingAbove is None\n    assert symbol.siblingBelow is None\n\n    symbol2 = root2.add_declaration(ASTDeclaration('function', 'function', None,\n                                                    None, None, None, None),\n                                     docname=\"TestDoc2\")\n\n    symbol3 = root3.add_declaration(ASTDeclaration('function', 'function', None,\n                                                    None, None, None, None),\n                                     docname=\"TestDoc3\")\n\n    symbol4 = root4.add_declaration(ASTDeclaration('function', 'function', None,\n                                                    None, None, None, None),\n                                     docname=\"TestDoc4\")\n\n    assert symbol.siblingBelow is symbol2\n    assert symbol2.siblingAbove is symbol\n    assert symbol2.siblingBelow is symbol3\n    assert symbol3.siblingAbove is symbol2\n    assert symbol3.siblingBelow is symbol4\n    assert symbol4.siblingAbove is symbol3\n    assert symbol4.siblingBelow is None\n\n    # Test lookup in root symbol\n    assert root.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"a\"), None)],\n                                        [False], rooted=True),\n                          [], 'any', True, True, True)\n    assert root.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"a\"), None)],\n                                        [False], rooted=True),\n                          [], 'any', True, True, False)\n    assert root.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"a\"), None)],\n                                        [False], rooted=True),\n                          [], 'any', False, True, True)\n    assert root.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"a\"), None)],\n                                        [False], rooted=True),\n                          [], 'any', False, True, False)\n\n    # Test lookup in a symbol with sibling\n    assert symbol.find_name(ASTNestedName([ASTNestedNameElement(ASTIdentifier(\"a\"), None)],\n                                          [False", "compilation error", "compilation error"], "sample_923": ["compilation error", "def test_parse_type_0():\n    # Test parsing of type declaration with 'void'\n    idDict = {1: 'type_void', 2: 'type_void', 3: 'type_void'}\n    check('type', 'void', idDict, 'void')\n", "def test_c_declarator_name_suffix():\n    \"\"\"Test parse of declarator-name-suffix.\"\"\"\n    idDict = {}\n    check('member', 'typedef int foo::*x;',\n          idDict, 'typedef int x;')\n    check('member', 'typedef int foo::*x[10];',\n          idDict, 'typedef int x[10];')\n    check('member', 'typedef int foo::*x[10][20];',\n          idDict, 'typedef int x[20][10];')\n    check('member', 'typedef int *foo::*x;',\n          idDict, 'typedef int *x;')\n    check('member', 'typedef int *foo::*x[10];',\n          idDict, 'typedef int *x[10];')\n    check('member', 'typedef int *foo::*x[10][20];',\n          idDict, 'typedef int *x[20][10];')\n    check('member', 'typedef int foo::*x(int);',\n          idDict, 'typedef int x(int);')\n    check('member', 'typedef int foo::*x(int)[10];',\n          idDict, 'typedef int x(int)[10];')\n    check('member', 'typedef int *foo::*x(int);',\n          idDict, 'typedef int *x(int);')\n    check('member', 'typedef int *foo::*x(int)[10];',\n          idDict, 'typedef int *x(int)[10];')\n    check('member', 'typedef int foo::*x::*y;',\n          idDict, 'typedef int *y;')\n    check('member', 'typedef int *foo::*x::*y;',\n          idDict, 'typedef int *y;')\n    check('member', 'typedef int *foo::*x::*y::*z;',\n          idDict, 'typedef int *z;')\n    check('member', 'typedef int foo::*x::*y::*z::*w;',\n          idDict, 'typedef int *w;')\n    check('member', 'typedef int *foo::*x::*y::*z::*w;',\n          idDict, 'typedef int *w;')\n    check('member', 'typedef int foo::*x::*y::*z::*w::*v;',\n          idDict, 'typedef int *v;')\n    check('member', 'typedef int *foo::*x::*y::*z::*w::*v;',\n          idDict, 'typedef int *v;", "def test_CTypeObject_parse_type_with_init():\n    input = \"type name\"\n    idDict = {1: 'type name'}\n    expected = \"name\"\n\n    _check('type', input, idDict, expected)\n\n", "def test_symbol_table_nested_structs():\n    check('struct', \"\"\"\n    struct Foo {\n        struct Bar {\n            int x;\n            struct Baz {\n                int y;\n            } z;\n        };\n        int z;\n    };\n    \"\"\",\n          {1: 'Foo', 2: 'Foo::Bar', 3: 'Foo::Bar::Baz', 4: 'Foo::Bar::Baz::x',\n           5: 'Foo::z', 6: 'Foo'},\n          \"\"\"\n    struct Foo;\n    struct Foo::Bar;\n    struct Foo::Bar::Baz;\n    int Foo::Bar::Baz::x;\n    int Foo::z;\n    struct Foo\n    \"\"\")\n"], "sample_302": ["    def setUp(self):\n        self.settings_dict = {\n            'ENGINE': 'django.db.backends.postgresql',\n            'NAME': 'test_database',\n            'USER': 'test_user',\n            'PASSWORD': 'test_password',\n            'HOST': 'localhost',\n            'PORT': '5432',\n            'OPTIONS': {\n                'passfile': '/path/to/passfile',\n                'service': 'test_service',\n                'sslmode': 'require',\n                'sslrootcert': '/path/to/sslrootcert',\n                'sslcert': '/path/to/sslcert',\n                'sslkey': '/path/to/sslkey'\n            }\n        }\n        self.client = DatabaseClient(connection=connection)\n", "    def test_empty_options(self):\n        settings_dict = {\n            'ENGINE': 'django.db.backends.postgresql',\n            'HOST': 'localhost',\n            'NAME': 'test_db',\n            'USER': 'test_user',\n            'PASSWORD': 'test_password',\n        }\n        options = {}\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, ['--help'])\n        self.assertEqual(args, ['psql', '-h', 'localhost', '-U', 'test_user', 'test_db', '--help'])\n        self.assertIsNone(env)\n", "    def test_settings_to_cmd_args_env_empty_settings(self):\n        settings_dict = {}\n        parameters = []\n        with mock.patch.object(BaseDatabaseClient, 'executable_name') as mock_executable_name:\n            mock_executable_name.return_value = 'psql'\n            args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n            self.assertEqual(args, ['psql'])\n            self.assertIsNone(env)\n", "    def test_settings_to_cmd_args_env_with_default_database(self):\n        connection.settings_dict = {\n            'ENGINE': 'django.db.backends.postgresql',\n            'HOST': 'localhost',\n            'PORT': '5432',\n            'NAME': None,\n            'USER': 'postgres',\n            'PASSWORD': 'password',\n            'OPTIONS': {\n                'service': 'service',\n                'sslmode': 'require',\n                'sslrootcert': 'path/to/ssl/root/cert',\n                'sslcert': 'path/to/ssl/cert',\n                'sslkey': 'path/to/ssl/key',\n                'passfile': 'path/to/passfile'\n            }\n        }\n        args, env = DatabaseClient.settings_to_cmd_args_env(connection.settings_dict, [])\n        self.assertEqual(args, ['-psql', '-h', 'localhost', '-p', '5432', 'postgres', '-U', 'postgres', '-P', 'password', '-s', 'service', '-G', 'require', '-r', 'path/to/ssl/root/cert', '-c', 'path/to/ssl/cert', '-k', 'path/to/ssl/key', '-f', 'path/to/passfile'])\n        self.assertEqual(env, {'PGPASSWORD': 'password', 'PGSERVICE': 'service', 'PGSSLMODE': 'require', 'PGSSLROOTCERT': 'path/to/ssl/root/cert', 'PGSSLCERT': 'path/to/ssl/cert', 'PGSSLKEY': 'path/to/ssl/key', 'PGPASSFILE': 'path/to/passfile'})\n", "    def test_empty_settings_dict_returns_default_values(self):\n        settings_dict = {}\n        parameters = []\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, ['psql'])\n        self.assertIsNone(env)\n"], "sample_732": ["def test_fetch_kddcup99_subset_SA_shuffle_false_return_X_y_false():\n    \"\"\"Test fetch_kddcup99 with subset='SA', shuffle=False, return_X_y=False\"\"\"\n    X, y = fetch_kddcup99(subset='SA', shuffle=False, return_X_y=True)\n    assert isinstance(X, np.ndarray)\n    assert isinstance(y, np.ndarray)\n    assert X.shape == (976158, 41)\n    assert y.shape == (976158,)\n", "def test_fetch_kddcup99_subset_return_X_y(self):\n    # Test that fetch_kddcup99 returns the data and target when return_X_y=True\n    # for the 'SA' subset.\n    kdd = fetch_kddcup99(subset='SA', return_X_y=True)\n    self.assertIsInstance(kdd, tuple)\n    self.assertEqual(len(kdd), 2)\n    self.assertIsInstance(kdd[0], np.ndarray)\n    self.assertIsInstance(kdd[1], np.ndarray)\n    self.assertEqual(kdd[0].shape, (976158, 41))\n    self.assertEqual(kdd[1].shape, (976158,))\n\n    # Test that fetch_kddcup99 returns the data and target when return_X_y=True\n    # for the 'SF' subset.\n    kdd = fetch_kddcup99(subset='SF', return_X_y=True)\n    self.assertIsInstance(kdd, tuple)\n    self.assertEqual(len(kdd), 2)\n    self.assertIsInstance(kdd[0], np.ndarray)\n    self.assertIsInstance(kdd[1], np.ndarray)\n    self.assertEqual(kdd[0].shape, (699691, 4))\n    self.assertEqual(kdd[1].shape, (699691,))\n\n    # Test that fetch_kddcup99 returns the data and target when return_X_y=True\n    # for the 'http' subset.\n    kdd = fetch_kddcup99(subset='http', return_X_y=True)\n    self.assertIsInstance(kdd, tuple)\n    self.assertEqual(len(kdd), 2)\n    self.assertIsInstance(kdd[0], np.ndarray)\n    self.assertIsInstance(kdd[1], np.ndarray)\n    self.assertEqual(kdd[0].shape, (619052, 3))\n    self.assertEqual(kdd[1].shape, (619052,))\n\n    # Test that fetch_kddcup99 returns the data and target when return_X_y=True\n    # for the 'smtp' subset.\n    kdd = fetch_kddcup99(subset='smtp', return_X_y=True)\n    self.assertIsInstance(kdd, tuple)\n    self.assertEqual(len(kdd), 2)\n    self.assertIsInstance(kdd[0], np.ndarray)\n    self.assertIsInstance(kdd[1], np.ndarray)\n    self.assertEqual(kdd[0].shape, (95373, 3))\n    self.assertEqual(kdd[1].shape, (95373,))\n", "    def test_fetch_kddcup99_subset(self):\n        # Test fetching different subsets\n        for subset in ['SA', 'SF', 'http', 'smtp']:\n            data = fetch_kddcup99(subset=subset, percent10=True)\n            assert_equal(data.target.dtype, np.object)\n            if subset == 'SA':\n                assert_equal(data.data.shape[1], 41)\n                assert_equal(data.target.shape[0], 976158)\n            elif subset in ['http', 'smtp']:\n                assert_equal(data.data.shape[1], 3)\n                assert_equal(data.target.shape[0], 619052 if subset == 'http' else 95373)\n            elif subset == 'SF':\n                assert_equal(data.data.shape[1], 4)\n                assert_equal(data.target.shape[0], 699691)\n", "def test_fetch_kddcup99_shuffle():\n    \"\"\"Check that the data and target are shuffled correctly.\"\"\"\n    X, y = fetch_kddcup99(return_X_y=True)\n    assert len(X) == len(y)\n    assert all(X[i] != X[i-1] for i in range(1, len(X)))\n    assert all(y[i] != y[i-1] for i in range(1, len(y)))\n", "def test_fetch_kddcup99_subset():\n    # Test fetch_kddcup99 with different subsets\n    # Only 'SA', 'SF', 'http', 'smtp' are tested, as 'None' is tested in test_fetch_kddcup99\n    for subset in ['SA', 'SF', 'http', 'smtp']:\n        data = fetch_kddcup99(subset=subset)\n        assert_equal(data.target.dtype, object)\n        if subset == 'SA':\n            assert_equal(data.data.shape, (976158, 41))\n        elif subset in ['SF', 'http', 'smtp']:\n            if subset == 'SF':\n                assert_equal(data.data.shape, (699691, 4))\n            elif subset == 'http':\n                assert_equal(data.data.shape, (619052, 3))\n            elif subset == 'smtp':\n                assert_equal(data.data.shape, (95373, 3))\n"], "sample_575": ["    def test_log_transform(self, x):\n        s = Continuous(trans=\"log\")._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 1)\n        assert_array_equal(a.major.formatter.locator(), np.logspace(-1, 1, 10))\n", "def test_continuous_label_default_formatter(self, x):\n    s = Continuous().label()._setup(x, Coordinate())\n    a = PseudoAxis(s._matplotlib_scale)\n    a.set_view_interval(0, 1)\n    a.set_major_formatter(mpl.ticker.FuncFormatter(lambda x: f\"{x:.2f}\"))\n    a.set_units(x.dtype)\n    assert_array_equal(a.major.formatter.format(0.5), [\"0.50\"])\n    assert_array_equal(a.major.formatter.format(0.75), [\"0.75\"])\n", "    def test_label_formatter_with_base(self, x):\n        s = Continuous().label(base=10)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 1)\n        formatter = a.major.formatter\n        assert isinstance(formatter, mpl.ticker.LogFormatterSciNotation)\n        assert formatter.base == 10\n", "    def x(self):\n        return pd.Series([pd.Timestamp(\"2020-01-01\"), pd.Timestamp(\"2020-01-15\"), pd.Timestamp(\"2020-01-30\")], name=\"x\")\n", "    def test_tick_transform(self, x, trans):\n\n        s = Continuous(trans=trans)._setup(x, Coordinate())\n        a = PseudoAxis(s._matplotlib_scale)\n        a.set_view_interval(0, 1)\n        tick_locs = a.major.locator()\n\n        # Note: it's difficult to check the tick locations with the current\n        # implementation of the log/symlog locator, as it chooses the locations\n        # automatically based on the data range. However, we can check that the\n        # locations are not empty.\n        assert tick_locs.size > 0\n\n        # Check that the labels are correctly formatted\n        labels = a.major.formatter.format_ticks(tick_locs)\n        if trans.startswith(\"log\"):\n            assert re.match(r\"^\\d+\\.\\d+$\", labels[0])\n        elif trans == \"symlog\":\n            assert re.match(r\"^\\+?\\d+\\.\\d+[e\\+\\-]?\\d+$\", labels[0])\n        elif trans == \"sqrt\":\n            assert re.match(r\"^\\d+\\.\\d+$\", labels[0])\n        elif trans == \"logit\":\n            assert re.match(r\"^\\d+\\.\\d+$\", labels[0])\n        elif trans == \"pow\":\n            assert re.match(r\"^\\d+\\.\\d+$\", labels[0])\n"], "sample_926": ["def test_parse_type_like_declaration():\n    input = \"int i;\"\n    idDict = {2: \"c.i\", 1: \"c.i\", 3: \"c.i\"}\n    output = \"int i;\"\n    key = None\n    asTextOutput = None\n    _check('type', input, idDict, output, key, asTextOutput)\n", "def test_CEnumeratorObject_merge_with(self):\n    ast = parse(\"enumerator\", \"{} = 5;\")\n    idDict = {1: 'e1'}\n    rootSymbol = Symbol(None, None, None, None)\n    symbol1 = rootSymbol.add_declaration(ast, docname=\"TestDoc1\")\n    symbol2 = Symbol(None, None, None, None)\n    ast = parse(\"enumerator\", \"{} = 5;\")\n    ast.symbol.isRedeclaration = True\n    symbol2.add_declaration(ast, docname=\"TestDoc2\")\n    symbol2.merge_with(symbol1, [\"TestDoc1\", \"TestDoc2\"], self.env)\n    res = symbol2.get_all_symbols()\n    assert len(list(res)) == 2\n    assert list(res)[0].declaration.objectType == \"enumerator\"\n    assert list(res)[1].declaration.objectType == \"enumerator\"\n", "compilation error", "compilation error", "compilation error"], "sample_279": ["    def test_deferrable_immutable(self):\n        constraint = UniqueConstraint(fields=['field1'], name='test_constraint', deferrable=Deferrable.IMMEDIATE)\n        self.assertEqual(constraint.deferrable, Deferrable.IMMEDIATE)\n        self.assertNotEqual(constraint.deferrable, Deferrable.DEFERRED)\n        with self.assertRaises(ValueError):\n            constraint.deferrable = Deferrable.DEFERRED\n", "    def test_deferrable_immediate(self):\n        # Test that deferrable can be set to immediate\n        constraint = UniqueConstraint(\n            fields=['name', 'price'],\n            name='test_constraint',\n            deferrable=Deferrable.IMMEDIATE\n        )\n        self.assertEqual(constraint.deferrable, Deferrable.IMMEDIATE)\n", "    def test_check_constraint_invalid_check(self):\n        with self.assertRaises(TypeError):\n            CheckConstraint(check='not a Q instance', name='test_check')\n", "    def test_create_sql_with_deferrable(self):\n        constraint = UniqueConstraint(\n            fields=['field1', 'field2'],\n            name='test_constraint',\n            deferrable=Deferrable.DEFERRED,\n        )\n        with mock.patch('django.db.models.sql.query.Query') as mock_query:\n            mock_query.return_value.get_compiler.return_value.as_sql.return_value = ('%(sql)s', [])\n            model = models.Model()\n            schema_editor = mock.Mock()\n            schema_editor.quote_value.return_value = 'value'\n            sql = constraint.create_sql(model, schema_editor)\n            mock_query.assert_called_once_with(model=model, alias_cols=False)\n            self.assertEqual(sql, 'CREATE UNIQUE INDEX test_constraint ON \"model\" (\"field1\", \"field2\") DEFERRABLE DEFERRED')\n", "    def test_create_and_remove_check_constraint(self):\n        # Create a test model with a check constraint\n        class TestModel(models.Model):\n            field = models.CharField(max_length=255)\n\n            class Meta:\n                constraints = [\n                    CheckConstraint(check=models.Q(field__len__gt=0), name='test_constraint')\n                ]\n\n        # Get the constraint name\n        constraint_name = TestModel._meta.constraints[0].name\n\n        # Create the constraint\n        with connection.schema_editor(using='default') as schema_editor:\n            schema_editor.create_model(TestModel)\n            constraint_sql = TestModel._meta.constraints[0].create_sql(TestModel, schema_editor)\n            self.assertIn(constraint_name, constraint_sql)\n\n        # Remove the constraint\n        with connection.schema_editor(using='default') as schema_editor:\n            schema_editor.delete_model(TestModel)\n            constraint_sql = TestModel._meta.constraints[0].remove_sql(TestModel, schema_editor)\n            self.assertIn(constraint_name, constraint_sql)\n"], "sample_611": ["compilation error", "def test_to_cftime_datetime(date_str, calendar, expected_date):\n    date = to_cftime_datetime(date", "def test_cftime_range_closed_start_end(calendar, start, end, expected_result):\n    result = cftime_range(start=start, end=end, periods=None, freq=None, calendar=calendar)\n    assert result.equals(expected_result)\n\n", "def test_cftime_range_frequency_length(calendar):\n    offset = to_offset(\"MS\")\n    start = to_cftime_datetime(\"2020-01-01\", calendar)\n    end = to_cftime_datetime(\"2020-01-31\", calendar)\n    dates = _generate_range(start, end, None, offset)\n    assert len(dates) == expected\n", "def test_date_range_like_calendar(calendar, use_cftime):\n    date_range_like(\n        date_range(\n            start=\"2000-01-01\",\n            end=\"2000-12-31\",\n            freq=\"AS\",\n            use_cftime=True,\n            calendar=\"standard\",\n        ),\n        calendar,\n        use_cftime=use_cftime,\n    )\n"], "sample_1064": ["def _compare_tensorflow_matrix_add(variables, expr):\n    f = lambdify(variables, expr, 'tensorflow')\n    random_matrices = [randMatrix(v.rows, v.cols) for v in variables]\n\n    graph = tf.Graph()\n    r = None\n    with graph.as_default():\n        tf_rvs = [eval(tensorflow_code(i)) for i in random_matrices]\n        session = tf.compat.v1.Session(graph=graph)\n        r = session.run(f(*tf_rvs))\n\n    e = expr.subs({k: v for k, v in zip(variables, random_matrices)})\n    e = e.doit()\n    if e.is_Matrix:\n        if not isinstance(e, MatrixBase):\n            e = e.as_explicit()\n        e = e.tolist()\n\n    assert (r + e).all() == (r + e).all()\n", "def _compare_tensorflow_piecewise(variables, expr):\n    f = lambdify(variables, expr, 'tensorflow')\n    rvs = [random.randint(0, 10) for v in variables]\n\n    graph = tf.Graph()\n    r = None\n    with graph.as_default():\n        tf_rvs = [eval(tensorflow_code(i)) for i in rvs]\n        session = tf.compat.v1.Session(graph=graph)\n        r = session.run(f(*tf_rvs))\n\n    e = expr.subs({k: v for k, v in zip(variables, rvs)}).doit()\n    assert r == e\n", "def test_tensorflow_atanh():\n    x = symbols('x')\n    f = lambdify(x, atanh(x), 'tensorflow')\n    graph = tf.Graph()\n    with graph.as_default():\n        tf_x = eval(tensorflow_code(x))\n        session = tf.compat.v1.Session(graph=graph)\n        r = session.run(f(tf_x))\n    assert abs(r - atanh(x)) < 10**-6\n", "def test_tensorflow_code_matrix_inverse():\n    M = MatrixSymbol(\"M\", 3, 3)\n    inverse = Inverse(M)\n\n    f = lambdify([M], inverse, 'tensorflow')\n    graph = tf.Graph()\n    with graph.as_default():\n        session = tf.compat.v1.Session(graph=graph)\n        r = session.run(f(M))\n\n    e = inverse.doit()\n    if e.is_Matrix:\n        e = e.as_explicit()\n    r = r.eval().tolist()\n    assert (r == e).all()\n\n", "def test_tensorflow_codepiecewise_with_conditionals():\n    x, y = symbols('x y')\n    a, b, c = symbols('a b c')\n    expr = Piecewise((a, x < 0), (b, x >= 0), (c, x == 0))\n    f = lambdify(x, expr, 'tensorflow')\n    session = tf.compat.v1.Session()\n    r = session.run(f(1))\n    assert r == b\n    r = session.run(f(-1))\n    assert r == a\n    r = session.run(f(0))\n    assert r == c\n"], "sample_948": ["def test_id_fundamental_v2() -> None:\n    \"\"\"\n    Test id_fundamental_v2 for fundamental types\n    \"\"\"\n    idDict = {'void': 'v', 'bool': 'b', 'char': 'c',\n              'signed char': 'a', 'unsigned char': 'h',\n              'wchar_t': 'w', 'char32_t': 'Di', 'char16_t': 'Ds',\n              'short': 's', 'short int': 's', 'signed short': 's',\n              'signed short int': 's', 'unsigned short': 't',\n              'unsigned short int': 't', 'int': 'i', 'signed': 'i',\n              'signed int': 'i', 'unsigned': 'j', 'unsigned int': 'j',\n              'long': 'l', 'long int': 'l', 'signed long': 'l',\n              'signed long int': 'l', 'unsigned long': 'm',\n              'unsigned long int': 'm', 'long long': 'x', 'long long int': 'x',\n              'signed long long': 'x', 'signed long long int': 'x',\n              'unsigned long long': 'y', 'unsigned long long int': 'y',\n              'float': 'f', 'double': 'd', 'long double': 'e',\n              'auto': 'Da', 'decltype(auto)': 'Dc', 'std::nullptr_t': 'Dn'}\n\n    check('type', 'void', idDict, 'v')\n    check('type', 'bool', idDict, 'b')\n    check('type', 'char', idDict, 'c')\n    check('type', 'signed char', idDict, 'a')\n    check('type', 'unsigned char', idDict, 'h')\n    check('type', 'wchar_t', idDict, 'w')\n    check('type', 'char32_t', idDict, 'Di')\n    check('type', 'char16_t', idDict, 'Ds')\n    check('type', 'short', idDict, 's')\n    check('type', 'short int', idDict, 's')\n    check('type', 'signed short', idDict, 's')\n    check('type', 'signed short int', idDict, 's')\n    check('type', 'unsigned short', idDict, 't')\n    check('type',", "def test_ASTTemplateParamTemplateType() -> None:\n    # test if ASTTemplateParamTemplateType is properly constructed\n    data = ASTTemplateKeyParamPackIdDefault('key', ASTIdentifier('identifier'), True, None)\n    nestedParams = ASTTemplateParams([ASTTemplateParamType(data)])\n    ast = ASTTemplateParamTemplateType(nestedParams, data)\n    assert str(ast) == \"key 0 identifier\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to string\n    assert str(ast.get_identifier()) == \"identifier\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id\n    assert ast.get_id(version=2) == \"0X\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with prefixed=True\n    assert ast.get_id(version=2, prefixed=True) == \"_0X\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with prefixed=False\n    assert ast.get_id(version=2, prefixed=False) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with prefixed=None\n    assert ast.get_id(version=2, prefixed=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"function\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"type\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"class\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"union\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"enum\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"enumerator\", symbol=None) == \"_0\"\n\n    # test if ASTTemplateParamTemplateType is properly converted to id with objectType\n    assert ast.get_id(version=2, objectType=\"namespace\", symbol=None) == \"_0", "def test_parse_template_argument_list():\n    # Test normal template arguments\n    input = \"template <int, int> class Test;\"\n    idDict = {2: '2', 3: '3'}\n    output = \"template <int, int>\"\n    check('function', input, idDict, output)\n    # Test template arguments with no parameters\n    input = \"template <> class Test;\"\n    idDict = {}\n    output = \"template <>\"\n    check('function', input, idDict, output)\n    # Test template arguments with just a type\n    input = \"template <int> class Test;\"\n    idDict = {2: '2'}\n    output = \"template <int>\"\n    check('function', input, idDict, output)\n    # Test template arguments with a type and a non-type argument\n    input = \"template <int, int> class Test;\"\n    idDict = {2: '2', 3: '3'}\n    output = \"template <int, int>\"\n    check('function', input, idDict, output)\n    # Test template arguments with a type and an expression\n    input = \"template <int, 5> class Test;\"\n    idDict = {2: '2', 3: '5'}\n    output = \"template <int, 5>\"\n    check('function', input, idDict, output)\n    # Test template arguments with a type, an expression, and a type\n    input = \"template <int, 5, int> class Test;\"\n    idDict = {2: '2', 3: '5', 4: '4'}\n    output = \"template <int, 5, int>\"\n    check('function', input, idDict, output)\n    # Test template arguments with a type, an expression, a type, and a non-type argument\n    input = \"template <int, 5, int, int> class Test;\"\n    idDict = {2: '2', 3: '5', 4: '4', 5: '5'}\n    output = \"template <int, 5, int, int>\"\n    check('function', input, idDict, output)\n    # Test template arguments with a pack expansion\n    input = \"template <int, ...> class Test;\"\n    idDict = {2: '2', 3", "def test_alias_object_without_noroot_option():\n    input = '.. cpp:alias:: foo bar baz\\n    :maxdepth: 1\\n    :noroot: False\\n    :cpp:aliasOptions: {\"noroot\": False}'\n    output = \"\"\".. _cpp:foo_bar_baz:\\n\\n``cpp:foo_bar_baz``_", "def test_nested_name_with_operator_and_template_args():\n    input = \"\"\"::operator++<int, float>\"\"\"\n    output = \"\"\"::operator++<int, float>\"\"\"\n    idDict = {2: 'op++', 3: 'op++_I', 4: 'op++_IIf', 5: 'op++_IIfE', 6: 'op++_IIfIE', 7: 'op++_IIfIEE'}\n    check('nestedName', input, idDict, output)\n"], "sample_1069": ["def test_glsl_code_Piecewise_with_default_term():\n    tau = symbols('tau')\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    glsl_code(expr, tau)\n    glsl_code(expr, tau, allow_unknown_functions=True)\n    glsl_code(expr, tau, allow_unknown_functions=True, human=False)\n    glsl_code(expr, tau, use_operators=False)\n    glsl_code(expr, tau, use_operators=False, human=False)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=True)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False, precision=10)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False, precision=10, glsl_types=False)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False, precision=10, glsl_types=False, mat_nested=True)\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False, precision=10, glsl_types=False, mat_nested=True, mat_separator=',')\n    glsl_code(expr, tau, use_operators=False, human=False, contract=False, precision=10, glsl_types=False, mat_nested=True, mat_separator=',', mat_transpose=True)\n", "def test_piecewise_with_default():\n    tau, x = symbols('tau, x')\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    result = glsl_code(expr, tau)\n    assert result == \"if (x > 0) { tau = x + 1; } else { tau = x; }\"\n", "def test_glsl_code_with_matrix():\n    A = Matrix([[x**2, x**3], [x**4, x**5]])\n    B = Matrix([y**2, y**3])\n    C = Matrix([z**2, z**3])\n\n    result = glsl_code(A + B)\n    expected = \"mat2(2*x**2, 3*x**2, 2*x**3, 3*x**3)\"\n    assert result == expected\n\n    result = glsl_code(A + B + C)\n    expected = \"mat2(2*x**2 + 2*y**2, 3*x**2 + 3*y**2, 2*x**3 + 2*y**3, 3*x**3 + 3*y**3)\"\n    assert result == expected\n\n    result = glsl_code(A + B, use_operators=False)\n    expected = \"mul(add(mat2(2*x**2, 3*x**2, 2*x**3, 3*x**3), mat2(y**2, y**3, y**2, y**3)))\"\n    assert result == expected\n\n    result = glsl_code(A*B)\n    expected = \"mat2(mul(2*x**2, y**2), mul(3*x**2, y**3), mul(2*x**3, y**2), mul(3*x**3, y**3))\"\n    assert result == expected\n\n    result = glsl_code(A*B + B*C)\n    expected = \"mat2(mul(2*x**2, y**2) + mul(y**2, z**2), mul(3*x**2, y**3) + mul(y**3, z**3), mul(2*x**3, y**2) + mul(y**2, z**3), mul(3*x**3, y**3) + mul(y**3, z**3))\"\n    assert result == expected\n\n    result = glsl_code(A*B, use_operators=False)\n    expected = \"mul(mat2(mul(2*x**2, y**2), mul(3*x**2, y**3), mul(2*x**3, y**2), mul(3*x**3, y**3)), mat2(y**2, y**3,", "def test_glsl_code_Piecewise_with_assign_to():\n    x = symbols('x')\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    result = glsl_code(expr, assign_to=x)\n    expected = \"if (x > 0) {\\n   x = x + 1;\\n} else {\\n   x = x;\\n}\"\n    assert result == expected\n", "def test_mat_separator():\n    from sympy import glsl_code\n    mat = Matrix([[1, 2], [3, 4]])\n    glsl_code(mat, mat_separator=',')\n    glsl_code(mat, mat_separator=',\\n')\n    glsl_code(mat, mat_separator=';')\n    glsl_code(mat, mat_separator='')\n    # Test that a ValueError is raised for invalid separator\n    with raises(ValueError):\n        glsl_code(mat, mat_separator='invalid_separator')\n"], "sample_1125": ["def test_UnitaryOperator_methods():\n    from sympy import I, eye, Matrix, symbols, conjugate, sqrt\n\n    x = symbols('x')\n    U = UnitaryOperator('U')\n    U_matrix = eye(3)\n    U_matrix[0, 1] = I\n    U_matrix[1, 0] = -I\n    U_matrix = U_matrix.as_real_imag()[0] + 1j*U_matrix.as_real_imag()[1]\n\n    U_matrix = U_matrix * (1 / sqrt(2))\n\n    U = Matrix(U_matrix).inv()\n    U = U.as_real_imag()[0] + 1j * U.as_real_imag()[1]\n    U_inv = UnitaryOperator('U_inv')\n    assert U_inv._eval_adjoint() == U\n", "def test_HermitianOperator_UnitaryOperator():\n    from sympy.physics.quantum import Dagger, HermitianOperator, UnitaryOperator\n    from sympy.physics.quantum.operator import Operator\n    from sympy.abc import x\n\n    # Test HermitianOperator\n    H = HermitianOperator('H')\n    assert H.is_hermitian\n    assert Dagger(H) == H\n\n    # Test UnitaryOperator\n    U = UnitaryOperator('U')\n    assert U.is_unitary\n    assert Dagger(U)*U == IdentityOperator()\n\n    # Test HermitianOperator and UnitaryOperator inheritance\n    assert issubclass(HermitianOperator, Operator)\n    assert issubclass(UnitaryOperator, Operator)\n\n    # Test power of HermitianOperator\n    H = HermitianOperator('H')\n    assert H**2 == H * H\n\n    # Test power of UnitaryOperator\n    U = UnitaryOperator('U')\n    assert U**(-1) == Dagger(U)\n\n    # Test power of UnitaryOperator with negative exponent\n    assert U**(-2) == Dagger(U)**2\n", "def test_OutterProduct_properties():\n    from sympy.physics.quantum.state import Ket, Bra, outer\n\n    k = Ket('k')\n    b = Bra('b')\n    op = OuterProduct(k, b)\n    assert op.ket == k\n    assert op.bra == b\n    assert op.hilbert_space == k.hilbert_space\n    assert op.is_commutative == False\n", "def test_OuternProduct_rearrangement():\n    from sympy import symbols, Matrix, IdentityMatrix, conjugate\n    from sympy.physics.quantum import Operator, Dagger, OuterProduct, Ket, Bra\n\n    x, y = symbols('x y')\n\n    # Test the following cases\n    ket = Ket('ket')\n    bra = Bra('bra')\n    op1 = OuterProduct(ket, bra)\n    op2 = OuterProduct(bra, ket)\n\n    # Test if the ket and bra are correctly swapped when there is no Dagger in the expression\n    assert op1.ket == ket\n    assert op1.bra == bra\n    assert op2.ket == bra\n    assert op2.bra == ket\n\n    # Test the following cases with Dagger\n    Dagger_op1 = Dagger(op1)\n    Dagger_op2 = Dagger(op2)\n\n    # Test if the ket and bra are correctly swapped when there is Dagger in the expression\n    assert Dagger_op1.ket == bra\n    assert Dagger_op1.bra == ket\n    assert Dagger_op2.ket == ket\n    assert Dagger_op2.bra == bra\n", "def test_outer_product():\n    x, y = symbols('x y')\n\n    # Test creation and properties\n    op = OuterProduct(Expr(x), Expr(y))\n    assert op.ket == x\n    assert op.bra == y\n    assert op.hilbert_space is None  # TODO: make sure this is set correctly\n    assert op.is_commutative is False\n\n    # Test creation from ket and bra\n    ket = Expr(x)\n    bra = Expr(y)\n    op = OuterProduct(ket, bra)\n    assert op.ket == ket\n    assert op.bra == bra\n    assert op.hilbert_space is None  # TODO: make sure this is set correctly\n    assert op.is_commutative is False\n\n    # Test adjoint\n    op = OuterProduct(Expr(x), Expr(y))\n    assert op._eval_adjoint().ket == Expr(y)\n    assert op._eval_adjoint().bra == Expr(x)\n\n    # Test printing\n    assert str(op) == '|x><y|'\n    assert op._print_pretty().as.sympystr() == '|x><y|'\n\n    # Test representation\n    # Note: This will fail without a working representation method\n    # for OuterProduct and its components\n    assert op._represent() == x*y\n\n    # Test trace\n    op = OuterProduct(Expr(x), Expr(y))\n    assert op._eval_trace() == x\n"], "sample_723": ["def _check_statistics_empty_array(X, X_true, strategy, missing_values):\n    \"\"\"Utility function for testing imputation on an empty array.\n\n    Test:\n        - with dense and sparse arrays\n\n    Check that:\n        - the statistics (mean, median, mode) are correct\n        - the missing values are imputed correctly\"\"\"\n\n    err_msg = \"Parameters: strategy = %s, missing_values = %s, \" \\\n              \"axis = {0}, sparse = {1}\" % (strategy, missing_values)\n\n    # Empty matrix, axis = 0\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n    imputer.fit(X)\n    X_trans = imputer.transform(X.copy())\n    if strategy == \"mean\":\n        assert_array_equal(imputer.statistics_, np.nan,\n                          err_msg=err_msg.format(0, False))\n    elif strategy in [\"median\", \"most_frequent\"]:\n        assert_array_equal(imputer.statistics_, np.nan,\n                          err_msg=err_msg.format(0, False))\n    assert_array_equal(X_trans, X_true,\n                      err_msg=err_msg.format(0, False))\n\n    # Empty matrix, axis = 1\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=1)\n    imputer.fit(X.transpose())\n    if np.isnan(np.nan).any():\n        assert_raises(ValueError, imputer.transform, X.copy().transpose())\n    else:\n        X_trans = imputer.transform(X.copy().transpose())\n        assert_array_equal(X_trans, X_true.transpose(),\n                          err_msg=err_msg.format(1, False))\n\n    # Sparse empty matrix, axis = 0\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n    imputer.fit(sparse.csc_matrix(X))\n    X_trans = imputer.transform(sparse.csc_matrix(X.copy()))\n\n    if sparse.issparse(X_trans):\n        X_trans = X_trans.toarray()\n\n    if strategy == \"mean\":\n        assert_array_equal(imputer.statistics_, np.nan,\n                          err_msg=err_msg.format(0, True))\n    elif strategy in [\"median\", \"most_frequent\"]:\n        assert_array_equal(imputer.statistics_, np.nan,\n                          err_msg=err_msg.format(0, True))\n    assert_array_equal(X_trans, X_true,\n                      err_msg=err_msg.format(0, True))\n\n    # Sparse empty matrix, axis = 1\n   ", "def test_simple_imputer_with_custom_initial_statistics():\n    \"\"\"Test if imputation works correctly with custom initial statistics.\"\"\"\n    \n    # Test case for axis=0\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan]])\n    X_true = np.array([[1, 2, 1.5], [3, 4, 1.5]])\n    imputer = SimpleImputer(missing_values=np.nan, axis=0)\n    imputer.statistics_ = np.array([1.5, 2, 1.5])  # Custom statistics\n    X_trans = imputer.fit(X).transform(X)\n    assert_array_equal(imputer.statistics_, np.array([1.5, 2, 1.5]))\n    assert_array_almost_equal(X_trans, X_true)\n\n    # Test case for axis=1\n    X = np.array([[1, 2, np.nan], [3, 4, np.nan]])\n    X_true = np.array([[1, 2, 1.5], [3, 4, 1.5]])\n    imputer = SimpleImputer(missing_values=np.nan, axis=1)\n    imputer.statistics_ = np.array([1.5, 2, 1.5])  # Custom statistics\n    imputer.fit(X.transpose())\n    X_trans = imputer.transform(X.transpose())\n    assert_array_equal(imputer.statistics_, np.array([1.5, 2, 1.5]))\n    assert_array_almost_equal(X_trans, X_true.transpose())\n\n    # Test case for sparse matrix with axis=0\n    X_sparse = sparse.csc_matrix([[1, 2, np.nan], [3, 4, np.nan]])\n    X_true = np.array([[1, 2, 1.5], [3, 4, 1.5]])\n    imputer = SimpleImputer(missing_values=np.nan, axis=0)\n    imputer.statistics_ = np.array([1.5, 2, 1.5])  # Custom statistics\n    imputer.fit(X_sparse)\n    X_trans = imputer.transform(X_sparse)\n    assert_array_equal(imputer.statistics_, np.array([1.5, 2, 1.5]))\n    assert_array_almost_equal(X_trans.toarray(), X_true)\n\n    # Test case for sparse matrix with axis=1\n   ", "def test_simple_imputer_strategy_most_frequent_empty_array():\n    \"\"\"Test strategy=\"most_frequent\" with an array containing only missing values.\"\"\"\n    # Create a test case with only missing values\n    X = np.array([np.nan, np.nan, np.nan])\n    X_true = np.array([0.0, 0.0, 0.0])\n\n    # Test with normal matrix, axis = 0\n    imputer = SimpleImputer(strategy=\"most_frequent\", axis=0)\n    imputer.fit(X)\n    assert_array_equal(imputer.statistics_, [0.0])\n    assert_array_almost_equal(imputer.transform(X), X_true)\n\n    # Test with normal matrix, axis = 1\n    imputer = SimpleImputer(strategy=\"most_frequent\", axis=1)\n    imputer.fit(X.reshape(1, -1))\n    assert_array_equal(imputer.statistics_, [0.0])\n    try:\n        imputer.transform(X.reshape(1, -1))\n        assert False\n    except ValueError as e:\n        assert \"Some rows only contain \" in str(e)\n\n    # Test with sparse matrix, axis = 0\n    X_sparse = sparse.csc_matrix(X)\n    imputer = SimpleImputer(strategy=\"most_frequent\", axis=0)\n    imputer.fit(X_sparse)\n    assert_array_equal(imputer.statistics_, [0.0])\n    assert_array_almost_equal(imputer.transform(X_sparse).toarray(), X_true)\n\n    # Test with sparse matrix, axis = 1\n    imputer = SimpleImputer(strategy=\"most_frequent\", axis=1)\n    imputer.fit(sparse.csc_matrix(X.reshape(1, -1)))\n    assert_array_equal(imputer.statistics_, [0.0])\n    try:\n        imputer.transform(sparse.csc_matrix(X.reshape(1, -1)))\n        assert False\n    except ValueError as e:\n        assert \"Some rows only contain \" in str(e)\n", "def _check_axis_imputation_error(X, missing_values):\n    \"\"\"Utility function for testing axis imputation error.\n\n    Check that:\n        - when axis = 1, the imputer raises a ValueError\n          if it is not possible to fill in the missing values\"\"\"\n\n    missing_X = X.copy()\n    missing_X[0, 0] = missing_values\n\n    err_msg = \"Parameters: missing_values = %s, axis = {0}\" \\\n              % (missing_values)\n\n    imputer = SimpleImputer(missing_values, strategy=\"mean\", axis=1)\n    assert_raises(ValueError, imputer.fit, missing_X)\n    assert_raises(ValueError, imputer.transform, missing_X)\n\n    imputer = SimpleImputer(missing_values, strategy=\"median\", axis=1)\n    assert_raises(ValueError, imputer.fit, missing_X)\n    assert_raises(ValueError, imputer.transform, missing_X)\n\n    imputer = SimpleImputer(missing_values, strategy=\"most_frequent\", axis=1)\n    assert_raises(ValueError, imputer.fit, missing_X)\n    assert_raises(ValueError, imputer.transform, missing_X)\n\n    imputer = SimpleImputer(missing_values, strategy=\"mean\", axis=0)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(X_trans.shape, X.shape)\n\n    imputer = SimpleImputer(missing_values, strategy=\"median\", axis=0)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(X_trans.shape, X.shape)\n\n    imputer = SimpleImputer(missing_values, strategy=\"most_frequent\", axis=0)\n    imputer.fit(X)\n    X_trans = imputer.transform(X)\n    assert_equal(X_trans.shape, X.shape)\n\n", "def test_imputer_weighted_mean():\n    \"\"\"Test that the weighted mean imputation strategy works correctly.\n\n    Check that:\n        - the mean values are computed correctly\n        - the missing values are imputed correctly\n    \"\"\"\n    X = np.array([[1, 2, 3], [4, np.nan, 6], [np.nan, 7, 8]])\n    X_true = np.array([[1, 2, 3], [4, 4, 6], [4, 7, 8]])\n\n    statistics = np.array([1, 5, 3])\n    missing_values = np.nan\n    strategy = \"mean\"\n\n    _check_statistics(X, X_true, strategy, statistics, missing_values)\n\n    # Test imputation of weighted mean\n    imputer = SimpleImputer(missing_values, strategy=strategy, axis=0)\n    imputer.fit(X)\n\n    # Set some weights\n    weights = np.array([0.3, 0.2, 0.5])\n    X_trans = imputer.transform(X.copy())\n    assert_array_almost_equal(X_trans, X_true, err_msg=\"Weighted mean imputation\")\n\n    # Check the statistics are not the same as the mean\n    assert_not_equal(imputer.statistics_, statistics,\n                     err_msg=\"Statistics should be different from mean\")\n\n"], "sample_1142": ["def test_matrix_symbol_derivative():\n    from sympy import MatrixSymbol, diff, sqrt\n    A = MatrixSymbol('A', 3, 3)\n    x = symbols('x')\n    f = sqrt(A + x)\n    # For the derivative of sqrt(A+x) with respect to x\n    assert diff(f, x) == sqrt(A + x)/2/(2*sqrt(A + x))\n", "def test_matrix_element_doit():\n    from sympy import MatrixSymbol, MatrixElement\n    M = MatrixSymbol('M', 2, 2)\n    E = MatrixElement(M, 1, 1)\n    assert E.doit() == M[1, 1]\n", "def test_matrix_symbol_indexing():\n    from sympy import MatrixSymbol, MatrixElement\n    A = MatrixSymbol('A', 2, 3)\n    m = A[0, 0]\n    assert isinstance(m, MatrixElement)\n    assert m.args[0] is A\n    assert m.args[1] == 0\n    assert m.args[2] == 0\n\n    A = MatrixSymbol('A', 2, 3)\n    m = A[1, 1]\n    assert isinstance(m, MatrixElement)\n    assert m.args[0] is A\n    assert m.args[1] == 1\n    assert m.args[2] == 1\n\n    A = MatrixSymbol('A', 3, 3)\n    m = A[1, 2]\n    assert isinstance(m, MatrixElement)\n    assert m.args[0] is A\n    assert m.args[1] == 1\n    assert m.args[2] == 2\n\n    A = MatrixSymbol('A', 3, 3)\n    m = A[1, 2]\n    n = A[2, 0]\n    assert isinstance(m, MatrixElement)\n    assert m.args[0] is A\n    assert m.args[1] == 1\n    assert m.args[2] == 2\n    assert isinstance(n, MatrixElement)\n    assert n.args[0] is A\n    assert n.args[1] == 2\n    assert n.args[2] == 0\n", "def test_matrix_expr_equality():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 3, 3)\n    C = MatrixSymbol('C', 3, 3)\n\n    assert A.equals(A)\n    assert B.equals(B)\n    assert C.equals(C)\n\n    assert not A.equals(B)\n    assert not B.equals(A)\n\n    assert A.equals(A + 0)\n    assert not A.equals(A + 1)\n    assert A.equals(A - 0)\n\n    assert A.equals(A.as_explicit())\n    assert A.equals(A.as_mutable())\n\n    assert A.equals(Matrix(3, 3, [1, 2, 3, 4, 5, 6, 7, 8, 9]))\n    assert not A.equals(Matrix(3, 3, [1, 2, 3, 4, 5, 6, 7, 8, 10]))\n", "def test_MatMul_commutative():\n    from sympy import sin, cos\n    A = MatrixSymbol('A', 3, 3)\n    assert not A.is_commutative  # Test if is_commutative is False\n    assert not (A * sin(x)).is_commutative  # Test if is_commutative is False for expr * sin(x)\n    assert not ((A * cos(x)) * sin(x)).is_commutative  # Test if is_commutative is False for expr1 * expr2 * expr3\n"], "sample_309": ["    def test_parse_http_date_rfc1123(self):\n        date_str = 'Wed, 21 Jan 2015 07:28:00 GMT'\n        self.assertEqual(parse_http_date(date_str), 1421795280)\n", "    def test_parse_http_date_rfc1123(self):\n        # Test the RFC1123 date format\n        date_str = 'Wed, 21 Oct 2020 07:28:00 GMT'\n        expected_timestamp = datetime(2020, 10, 21, 7, 28, 0, tzinfo=timezone.utc).timestamp()\n        self.assertEqual(parse_http_date(date_str), expected_timestamp)\n", "    def test_urlencode_doseq_on_iterable(self):\n        data = {'a': ['1', '2', '3']}\n        encoded = urlencode(data, doseq=True)\n        self.assertEqual(encoded, 'a=1&a=2&a=3')\n", "    def test_parse_http_date_rfc1123(self):\n        date_str = 'Wed, 21 Oct 2015 07:28:00 GMT'\n        self.assertEqual(parse_http_date(date_str), 1445384880)\n", "    def test_base36_to_int(self):\n        self.assertEqual(base36_to_int('0'), 0)\n        self.assertEqual(base36_to_int('a'), 10)\n        self.assertEqual(base36_to_int('z'), 35)\n        self.assertEqual(base36_to_int('1z'), 35 * 36 + 1)\n        self.assertEqual(base36_to_int('1a'), 36 + 10)\n"], "sample_1038": ["def test_matrix_symbols():\n    A = MatrixSymbol('A', 3, 4)\n    B = MatrixSymbol('B', 4, 5)\n    assert set(matrix_symbols(A)) == {A}\n    assert set(matrix_symbols(A + B)) == {A, B}\n    assert set(matrix_symbols(A * B)) == set()\n    assert set(matrix_symbols(A + A)) == {A}\n    assert set(matrix_symbols(A + 1)) == {A}\n", "def test_as_explicit():\n    A = MatrixSymbol('A', 3, 3)\n    assert A.as_explicit().shape == (3, 3)\n    assert A.as_explicit().__class__.__name__ == 'ImmutableDenseMatrix'\n\n    # Test with a non-square matrix\n    B = MatrixSymbol('B', 4, 3)\n    assert B.as_explicit().shape == (4, 3)\n\n    # Test with a square matrix\n    C = MatrixSymbol('C', 5, 5)\n    assert C.as_explicit().shape == (5, 5)\n", "def test_matrix_symbol_shape():\n    A = MatrixSymbol('A', 3, 4)\n    assert A.shape == (3, 4)\n", "    def test___new__(self):\n        from sympy import MatrixSymbol, MatrixExpr\n        mat = MatrixSymbol('A', 3, 3)\n        assert isinstance(mat, MatrixExpr)\n        assert isinstance(mat, MatrixSymbol)\n        assert mat.is_MatrixExpr\n        assert mat.is_Matrix\n        assert mat.is_symbol\n\n        mat = MatrixSymbol('A', 3, 3)\n        assert mat.rows == 3\n        assert mat.cols == 3\n        assert mat.shape == (3, 3)\n        assert mat.is_square\n\n        try:\n            mat = MatrixSymbol('A', 3, 'a')\n            assert False\n        except TypeError:\n            assert True\n\n        try:\n            mat = MatrixSymbol('A', 'a', 3)\n            assert False\n        except TypeError:\n            assert True\n", "def test_MatrixSymbol_indexing():\n    A = MatrixSymbol('A', 3, 4)\n    A1 = A[1]\n    assert isinstance(A1, MatrixElement)\n    assert A1.parent == A\n    assert A1.i == 1\n    assert A1.j == None\n\n    A2 = A[1, 2]\n    assert isinstance(A2, MatrixElement)\n    assert A2.parent == A\n    assert A2.i == 1\n    assert A2.j == 2\n\n    assert raises(IndexError, \"A[10]\")\n    assert raises(IndexError, \"A[1, 'a']\")\n\n    B = MatrixSymbol('B', 4, 5)\n    AB = A*B\n    AB_21 = AB[2, 1]\n    assert isinstance(AB_21, MatrixElement)\n    assert AB_21.parent == AB\n    assert AB_21.i == 2\n    assert AB_21.j == 1\n    assert AB_21.args[0].shape == (3, 5)\n\n    assert raises(IndexError, \"AB[1, 2, 3]\")\n\n    C = MatrixSymbol('C', 3, 3)\n    C2 = C[1]\n    assert isinstance(C2, MatrixElement)\n    assert C2.parent == C\n    assert C2.i == 1\n    assert C2.j == None\n\n    D2 = C[1, 1]\n    assert isinstance(D2, MatrixElement)\n    assert D2.parent == C\n    assert D2.i == 1\n    assert D2.j == 1\n\n    assert raises(IndexError, \"C[10]\")\n    assert raises(IndexError, \"C[1, 'a']\")\n    assert raises(IndexError, \"C[1, 2, 3]\")\n\n    # Generic identity and generic zero matrix\n    I = GenericIdentity()\n    Z = GenericZeroMatrix()\n    assert raises(TypeError, \"I[0]\")\n    assert raises(TypeError, \"Z[0]\")\n\n    # MatMul with identity\n    I = Identity(5)\n    AB = A*I\n    assert AB[0, 0] == A[0, 0]\n    assert AB[1, 1] == A[1, 1]\n    assert AB[2, 2] == A[2, 2]\n    assert AB[0, 1"], "sample_431": ["    def test_state_attributes(self):\n        article = Article.objects.create(title=\"test\", content=\"test\")\n        self.assertTrue(article._state.db)\n        self.assertTrue(article._state.adding)\n", "    def test_state_instance_is_mutable(self):\n        instance = models.Model()\n        with self.assertRaises(AttributeError):\n            instance._state = None\n", "    def test_create_from_db(self):\n        # Test creating a model instance from the database\n        article = Article.objects.create(title=\"Test Article\", content=\"Test content\")\n        new_article = models.Model.from_db(\"default\", [\"id\", \"title\"], [1, \"New Title\"])\n        self.assertEqual(new_article.id, article.id)\n        self.assertEqual(new_article.title, \"New Title\")\n", "    def setUp(self):\n        self.db = self.settings.DATABASES['default']['NAME']\n", "    def test_get_state_methods(self):\n        instance = Article.objects.create()\n        self.assertEqual(instance._state.db, DEFAULT_DB_ALIAS)\n        self.assertTrue(instance._state.adding)\n        self.assertEqual(instance._state.fields_cache, {})\n        self.assertEqual(instance._state.refresh_state, {})\n\n        instance.save()\n        self.assertEqual(instance._state.db, DEFAULT_DB_ALIAS)\n        self.assertFalse(instance._state.adding)\n"], "sample_604": ["def test_array_repr_variable():\n    da = xr.DataArray(np.array([1, 2, 3]))\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray([1, 2, 3])>\n        [1 2 3]\"\"\"\n    )\n    assert formatting.array_repr(da) == expected\n\n    da = xr.DataArray(np.array([[1, 2], [3, 4]]))\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray([[1, 2], [3, 4]]> (2, 2)\n        [[1 2]\n         [3 4]]\"\"\"\n    )\n    assert formatting.array_repr(da) == expected\n\n    da = xr.DataArray(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray([[[1, 2], [3, 4]], [[5, 6], [7, 8]]> ((2, 2), (2, 2))\n        [[[1 2]\n          [3 4]]\n         [[5 6]\n          [7 8]]]\"\"\"\n    )\n    assert formatting.array_repr(da) == expected\n\n    da = xr.DataArray(np.array([1, 2, 3]), dims=[\"a\"])\n    expected = dedent(\n        \"\"\"\\\n        <xarray.DataArray([1, 2, 3] (a))>\n        [1 2 3]\"\"\"\n    )\n    assert formatting.array_repr(da) == expected\n", "    def test_inline_variable_array_repr(self):\n        da = xr.DataArray([1, 2, 3], dims=['x'])\n        assert formatting.inline_variable_array_repr(da, 10) == \"1.0 2.0 3.0\"\n\n        da = xr.DataArray([1, 2, 3], dims=['x'], coords={'x': [1, 2, 3]})\n        assert formatting.inline_variable_array_repr(da, 10) == \"1.0 2.0 3.0\"\n\n        da = xr.DataArray([1, 2, 3], dims=['x'], coords={'x': [1, 2, 3]}, attrs={'a': 1})\n        assert formatting.inline_variable_array_repr(da, 10) == \"1.0 2.0 3.0\"\n\n        da = xr.DataArray(np.array([1, 2, 3]), dims=['x'], coords={'x': [1, 2, 3]})\n        assert formatting.inline_variable_array_repr(da, 10) == \"[3 values with dtype=int64]\"\n\n        da = xr.DataArray([1, 2, 3], dims=['x'], coords={'x': [1, 2, 3]}, encoding={'chunksizes': [1, 3]})\n        assert formatting.inline_variable_array_repr(da, 10) == \"dask.array<chunksize=(1, 3)>\"\n\n        da = xr.DataArray([1, 2, 3], dims=['x'], coords={'x': [1, 2, 3]}, encoding={'dtype': 'int64'})\n        assert formatting.inline_variable_array_repr(da, 10) == \"[3 values with dtype=int64]\"\n\n        da = xr.DataArray(np.array([1, 2, 3]), dims=['x'], coords={'x': [1, 2, 3]}, encoding={'fill_value': 0})\n        assert formatting.inline_variable_array_repr(da, 10) == \"[3 values with dtype=int64]\"\n\n        da = xr.DataArray([1, 2, 3], dims=['x'], coords={'x': [1, 2, 3]}, encoding={'dtype': 'int64', 'fill_value': 0})\n        assert formatting.inline_variable_array_repr(da, 10) == \"<int64: nnz=3, fill_value=0>\"\n\n   ", "    def test_format_timestamp(self):\n        t = pd.Timestamp('2022-01-01 12:00:00')\n        assert formatting.format_timestamp(t) == \"2022-01-01T12:00:00\"\n\n        t = np.datetime64('2022-01-01')\n        assert formatting.format_timestamp(t) == \"2022-01-01\"\n\n        t = None\n        assert formatting.format_timestamp(t) == str(t)\n\n        t = pd.NaT\n        assert formatting.format_timestamp(t) == str(t)\n", "def test_inline_variable_array_repr_dask_array():\n    da = xr.DataArray(np.random.rand(10), dims=[\"x\"])\n    da = da.chunk({\"x\": 5})\n    max_width = 20\n    expected_output = f\"dask.array<chunksize={(5,)}>\"\n    assert formatting.inline_variable_array_repr(da, max_width) == expected_output\n", "def test_format_timestamp():\n    # Test date before the min valid date\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(np.datetime64(\"0001-01-01\"))\n\n    # Test date after the max valid date\n    with pytest.raises(OutOfBoundsDatetime):\n        formatting.format_timestamp(np.datetime64(\"2262-04-11 00:00:00\"))\n\n    # Test NaT\n    assert formatting.format_timestamp(np.datetime64(\"NaT\")) == \"NaT\"\n\n    # Test date\n    assert formatting.format_timestamp(np.datetime64(\"2020-01-01\")) == \"2020-01-01\"\n\n    # Test time\n    assert formatting.format_timestamp(np.datetime64(\"2020-01-01T12:00:00\")) == \"2020-01-01T12:00:00\"\n\n    # Test datetime\n    assert formatting.format_timestamp(datetime(2020, 1, 1, 12, 0, 0)) == \"2020-01-01T12:00:00\"\n\n    # Test timestamp with precision\n    assert formatting.format_timestamp(pd.Timestamp(\"2020-01-01 12:00:00\")) == \"2020-01-01 12:00:00\"\n\n    # Test pandas datetime\n    assert formatting.format_timestamp(pd.Timestamp(\"2020-01-01\")) == \"2020-01-01 00:00:00\"\n"], "sample_917": ["def test_fundamental_types():\n    check('type', \"void\", {'1': 'c', '2': 'v'},\n          'void')\n\n    check('type', \"bool\", {'1': 'b', '2': 'b'},\n          'bool')\n\n    check('type', \"char\", {'1': 'c', '2': 'c'},\n          'char')\n\n    check('type', \"wchar_t\", {'1': 'w', '2': 'w'},\n          'wchar_t')\n\n    check('type', \"char16_t\", {'1': 'Ds', '2': 'Ds'},\n          'char16_t')\n\n    check('type', \"char32_t\", {'1': 'Di', '2': 'Di'},\n          'char32_t')\n\n    check('type', \"int\", {'1': 'i', '2': 'i'},\n          'int')\n\n    check('type', \"float\", {'1': None, '2': 'f'},\n          'float')\n\n    check('type', \"double\", {'1': None, '2': 'd'},\n          'double')\n\n    check('type', \"auto\", {'1': 'Da', '2': 'Da'},\n          'auto')\n\n    check('type', \"long double\", {'1': None, '2': 'e'},\n          'long double')\n", "compilation error", "def test_keyword_overload_resolution():\n    input = 'int add(int, int); int add(double, double);'\n    check('function', input, {1: '1', 2: '2', 3: '3'})\n", "compilation error", "def test_IdentifierNameWithPrefix():\n    ast = parse('class', \"class MyName::Name;\")\n    res = str(ast.name)\n    expected = \"MyName::Name\"\n    assert res == expected\n\n"], "sample_1159": ["    def test_nonstandard_real(self):\n        from sympy import S\n        assert isinstance(InconsistentAssumptions('real'), InconsistentAssumptions)\n", "def test_check_assumptions_with_wildcard():\n    x = Symbol('x')\n    assert check_assumptions(x**2 + x, x)\n    assert not check_assumptions(x**2 + x, x, positive=True)\n    assert check_assumptions(x**2 + x, x, integer=True)\n", "def test_common_assumptions():\n    x = Symbol('x', real=True)\n    y = Symbol('y', imaginary=True)\n    z = Symbol('z')\n    s1 = Symbol('s1', extended_negative=True)\n    s2 = Symbol('s2', extended_positive=True)\n    s3 = Symbol('s3', extended_real=True)\n    assert common_assumptions([x, y]) == {'commutative': True, 'finite': True}\n    assert common_assumptions([s1, s2]) == {'commutative': True, 'extended_real': True}\n    assert common_assumptions([s1, s3]) == {'commutative': True}\n    assert common_assumptions([s2, s3]) == {'commutative': True}\n    assert common_assumptions([x, s1]) == {}\n    assert common_assumptions([y, s2]) == {}\n    assert common_assumptions([x, s3]) == {}\n    assert common_assumptions([y, s3]) == {}\n    assert common_assumptions([s1, s2]) == {'extended_real': True}\n", "def test_common_assumptions_nonempty():\n    from sympy.core.assumptions import common_assumptions\n    assert common_assumptions([1, 2]) == {}\n    assert common_assumptions([1, 2], check=['integer']) == {'integer': True}\n    assert common_assumptions([1, 3]) == {}\n    assert common_assumptions([1, 3], check=['odd']) == {'odd': True}\n", "def test_check_assumptions_with_wildcard():\n    w = Wild('x')\n    assert check_assumptions(w, real=True, nonnegative=True)\n    assert check_assumptions(w, real=True)\n    assert check_assumptions(w, positive=True, nonnegative=True)\n    assert not check_assumptions(w, positive=True, negative=True)\n    assert not check_assumptions(w, positive=True)\n"], "sample_1173": ["def test_parse_expr_invalid_token():\n    assert parse_expr('x ^ y') == \"x**y\"\n    assert parse_expr('x ^ y', evaluate=False) == \"x**y\"\n", "def test_auto_symbol():\n    # Test that undefined symbols are properly converted to SymPy symbols\n    code = \"x + y\"\n    expected = \"x + y\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n\n    # Test that undefined symbols with underscores are properly converted to SymPy symbols\n    code = \"x_y + y\"\n    expected = \"x_y + y\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n\n    # Test that undefined symbols with Greek letters are not split\n    code = \"theta + y\"\n    expected = \"theta + y\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n\n    # Test that undefined symbols with Greek letters are properly converted to SymPy symbols\n    code = \"theta + y\"\n    expected = \"theta + y\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n\n    # Test that undefined symbols with 'sin' or 'cos' prefix are properly converted to SymPy symbols\n    code = \"sin(x) + y\"\n    expected = \"sin(x) + y\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n\n    # Test that defined symbols are not converted to SymPy symbols\n    code = \"x = 1\"\n    expected = \"Eq(x, 1)\"\n    result = parse_expr(code, evaluate=False)\n    assert str(result) == expected\n    result = parse_expr(code, evaluate=False, transformations=(convert_equals_signs,))\n    assert str(result) == expected\n\n    # Test that undefined symbols with leading digits are properly converted to SymPy symbols\n    code = \"3x + y\"\n    expected = \"3*x + y\"\n    result = parse_expr(code, evaluate=False, transformations=(implicit_multiplication_application,))\n    assert str(result) == expected\n\n    # Test that undefined symbols with trailing digits are properly converted to SymPy symbols\n    code = \"x3 + y\"\n    expected = \"x3 + y\"\n    result = parse_expr(code, evaluate=False, transformations=(implicit_multiplication_application,))\n    assert str(result) == expected\n\n    # Test that undefined symbols with underscores and trailing digits are properly converted to SymPy symbols\n    code = \"x_3 + y\"\n    expected = \"x_3 + y\"\n    result = parse_expr(code, evaluate=False,", "def test_split_symbols_custom():\n    from sympy import Symbol\n\n        return not _token_splittable(name)\n    s = split_symbols_custom(is_splittable)\n\n    x = Symbol('x')\n    y = Symbol('y')\n    z = Symbol('z')\n\n    result = implicit_multiplication_application('x y z', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y * z', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y z *', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y  z', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y ', {}, {})\n    assert result == ['x', '*', 'y', '*']\n\n    result = implicit_multiplication_application('x y', {}, {})\n    assert result == ['x', '*', 'y']\n\n    result = implicit_multiplication_application(' x y', {}, {})\n    assert result == ['x', '*', 'y']\n\n    result = implicit_multiplication_application('  x y', {}, {})\n    assert result == ['x', '*', 'y']\n\n    result = implicit_multiplication_application('x  y', {}, {})\n    assert result == ['x', '*', 'y']\n\n    result = implicit_multiplication_application('x y z ', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y  z ', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y z  ', {}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y z', {'x': 1, 'y': 2}, {})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y z', {'x': 1}, {'y': 2})\n    assert result == ['x', '*', 'y', '*', 'z']\n\n    result = implicit_multiplication_application('x y z', {'x': 1}, {'y':", "def test_auto_symbol_on_keyword():\n    from sympy.core import Symbol\n\n    # sympy uses keywords like True, False, None which should not be symbols\n    s = 'True + 2'\n    x = parse_expr(s)\n    assert x == 2 + True\n\n    s = 'None + 2'\n    x = parse_expr(s)\n    assert x == 2 + None\n\n    s = 'False + 2'\n    x = parse_expr(s)\n    assert x == 2 + False\n\n    s = 'xyz + 2'\n    x = parse_expr(s)\n    assert x == 2 + Symbol('xyz')\n\n    s = '123 + 2'\n    x = parse_expr(s)\n    assert x == 2 + Integer(123)\n\n    s = '1.23 + 2'\n    x = parse_expr(s)\n    assert x == 2 + Float('1.23')\n\n    s = '1/2 + 2'\n    x = parse_expr(s)\n    assert x == 2 + Rational(1, 2)\n\n    s = '1j + 2'\n    x = parse_expr(s)\n    assert x == 2 + I\n\n    s = 'sympy + 2'\n    x = parse_expr(s)\n    assert x == 2 + Symbol('sympy')\n", "def test_parse_expr_function_exponentiation():\n    from sympy import sin\n\n    result = parse_expr('sin**2(x)')\n    assert isinstance(result, Mul)\n    assert result.args == (sin(x)**2,)\n"], "sample_1034": ["def return_one_on_all(qubits):\n    return qubits.nqubits == qubits.nqubits\n\n", "def return_one_on_three(qubits):\n    return qubits == IntQubit(3, nqubits=qubits.nqubits)\n", "def test_grover_algorithm_multiple_iterations():\n    # Check if the state is still a superposition after multiple iterations\n    oracle = lambda qubits: qubits == IntQubit(2)\n    nqubits = 2\n    iterated = apply_grover(oracle, nqubits, iterations=10)\n    basis_states = superposition_basis(nqubits)\n    assert qapply(iterated) == qapply(basis_states), \"State is not a superposition after multiple iterations\"\n\n    # Check if the apply_grover function returns a QuantumExpr\n    assert isinstance(apply_grover(oracle, nqubits), qapply(basis_states)), \"apply_grover should return a QuantumExpr\"\n\n    # Check if the apply_grover function works for an empty oracle\n    empty_oracle = lambda qubits: False\n    iterated = apply_grover(empty_oracle, nqubits)\n    assert iterated == superposition_basis(nqubits), \"apply_grover with an empty oracle should return the initial superposition\"\n\n    # Check if the apply_grover function works for a non-integer number of qubits\n    non_int_oracle = lambda qubits: qubits == IntQubit(2.5, nqubits=nqubits)\n    try:\n        apply_grover(non_int_oracle, nqubits)\n        assert False, \"apply_grover should raise an error for non-integer number of qubits\"\n    except QuantumError:\n        pass\n\n    # Check if the apply_grover function works for a negative number of qubits\n    negative_oracle = lambda qubits: qubits == IntQubit(2)\n    try:\n        apply_grover(negative_oracle, -1)\n        assert False, \"apply_grover should raise an error for negative number of qubits\"\n    except QuantumError:\n        pass\n\n    # Check if the apply_grover function works for zero number of qubits\n    zero_oracle = lambda qubits: qubits == IntQubit(2)\n    try:\n        apply_grover(zero_oracle, 0)\n        assert False, \"apply_grover should raise an error for zero number of qubits\"\n    except QuantumError:\n        pass\n", "def test_superposition_basis_nqubits_0():\n    \"\"\"Test that superposition_basis returns zero for nqubits=0.\"\"\"\n    # Test that superposition_basis returns 0 for nqubits=0\n    assert superposition_basis(0) == 0\n", "def test_grover_iteration():\n    # Test oracle gate\n    f = lambda qubits: qubits == IntQubit(3)\n    v = OracleGate(2, f)\n    qstate = superposition_basis(2)\n    # Apply oracle gate directly\n    expected_result = qapply(-qstate)\n    result = v._apply_operator_Qubit(qstate)\n    assert qapply(result) == qapply(expected_result)\n\n    # Test W gate\n    wgate = WGate(2)\n    qstate = superposition_basis(2)\n    result = wgate._apply_operator_Qubit(qstate)\n    expected_result = qapply((2/sqrt(4))*(superposition_basis(2)) - qstate)\n    assert qapply(result) == qapply(expected_result)\n\n    # Test grover_iteration\n    v = OracleGate(2, f)\n    result = grover_iteration(superposition_basis(2), v)\n    # The expected result is |2> after one iteration\n    expected_result = qapply(IntQubit(2))\n    assert qapply(result) == qapply(expected_result)\n\n"], "sample_437": ["    def test_set_autocommit_inside_atomic_block(self):\n        # Create a connection\n        connection = connections[DEFAULT_DB_ALIAS]\n\n        # Create a transaction\n        with connection.cursor() as cursor:\n            cursor.execute(\"BEGIN\")\n\n            # Set autocommit\n            connection.set_autocommit(True)\n\n            # Try to set autocommit again\n            with self.assertRaises(TransactionManagementError):\n                connection.set_autocommit(True)\n\n            # Commit the transaction\n            cursor.execute(\"COMMIT\")\n\n            # Try to set autocommit again\n            with self.assertRaises(TransactionManagementError):\n                connection.set_autocommit(True)\n", "    def test_get_autocommit(self):\n        \"\"\"Test get_autocommit method\"\"\"\n        conn = connection\n        self.assertEqual(conn.get_autocommit(), conn.settings_dict[\"AUTOCOMMIT\"])\n", "    def test_commit_on_exit(self):\n        with self.settings(DATABASES={\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\"}}):\n            conn = connection\n            self.assertFalse(conn.commit_on_exit)\n            conn.set_autocommit(False)\n            conn.commit_on_exit = True\n            with self.assertRaises(TransactionManagementError):\n                conn.commit()\n            conn.commit_on_exit = False\n            conn.commit()\n            self.assertFalse(conn.commit_on_exit)\n            self.assertEqual(conn.atomic_blocks, [])\n", "    def test_copy_alias(self):\n        # Ensure that a copy of the database connection with a custom alias is created correctly.\n        connection_copy = connection.copy(alias=\"test_alias\")\n        self.assertNotEqual(connection.alias, connection_copy.alias)\n        self.assertEqual(connection.settings_dict, connection_copy.settings_dict)\n", "    def test_atomic_block_in_transaction(self):\n        \"\"\"\n        Atomic blocks should be activated when entering an atomic transaction.\n        \"\"\"\n        connection = connections['default']\n        connection.cursor().execute('CREATE TABLE test_table (id INTEGER PRIMARY KEY)')\n        with connection.cursor() as cursor:\n            cursor.execute('INSERT INTO test_table (id) VALUES (1)')\n            connection.atomic():\n                cursor.execute('INSERT INTO test_table (id) VALUES (2)')\n                self.assertEqual(connection.atomic_blocks, [True])\n            cursor.execute('INSERT INTO test_table (id) VALUES (3)')\n        self.assertEqual(connection.atomic_blocks, [True, False])\n        cursor.execute('SELECT * FROM test_table')\n        self.assertEqual(cursor.fetchall(), [(1,), (2,), (3,)])\n"], "sample_1155": ["def test_construct_domain_fraction_negative_powers():\n    from sympy import symbols\n    from sympy.polys.constructor import construct_domain\n\n    x, y = symbols('x y')\n\n    expressions = [y/x, x/(1 - y)]\n    domain, result = construct_domain(expressions)\n\n    assert isinstance(domain, ZZ)\n    assert result == [y/x, -x/(y - 1)]\n", "def test_construct_algebraic_field():\n    # Test construction with a single algebraic number\n    domain, result = construct_domain([sqrt(2)])\n    assert domain == QQ.algebraic_field((sqrt(2))\n                                     ).dtype and len(result) == 1\n    assert result[0] == QQ.algebraic_field((sqrt(2)).dtype.from_list([1, 0], sqrt(2), QQ)\n\n    # Test construction with multiple algebraic numbers\n    domain, result = construct_domain([sqrt(2), sqrt(3)])\n    assert domain == QQ.algebraic_field((sqrt(2), sqrt(3)).dtype and len(result) == 2\n    assert result[0] == QQ.algebraic_field((sqrt(2), sqrt(3)).dtype.from_list([1, 0], sqrt(2), QQ)\n    assert result[1] == QQ.algebraic_field((sqrt(2), sqrt(3)).dtype.from_list([0, 1], sqrt(3), QQ)\n\n    # Test construction with complex algebraic numbers\n    domain, result = construct_domain([sqrt(2) + 3, sqrt(3) + 4*I])\n    assert domain == ComplexField(prec=53) and len(result) == 2\n    assert result[0] == ComplexField(prec=53).from_sympy(sqrt(2) + 3)\n    assert result[1] == ComplexField(prec=53).from_sympy(sqrt(3) + 4*I)\n\n    # Test construction with rational and algebraic numbers\n    domain, result = construct_domain([Rational(1, 2), sqrt(2)])\n    assert domain == QQ.algebraic_field((sqrt(2)).dtype and len(result) == 2\n    assert result[0] == QQ.algebraic_field((sqrt(2)).dtype.from_list([1, 0], sqrt(2), QQ)\n    assert result[1] == QQ.algebraic_field((sqrt(2)).dtype.from_list([1, 0], sqrt(2), QQ)\n", "def test_default_domain():\n    from sympy import construct_domain, S\n    expressions = [S(2), S(3), S(4)]\n    K, elements = construct_domain(expressions)\n    assert K == ZZ\n    assert elements == [2, 3, 4]\n", "def test_construct_domain_with_rational_and_algebraic_numbers():\n    from sympy import Rational, sqrt, S\n\n    exprs = [Rational(1, 2), sqrt(2)]\n    domain, elements = construct_domain(exprs)\n\n    assert domain == QQ\n    assert elements == [S(1)/2]\n\n    # test with extension=True\n    exprs = [sqrt(2)]\n    domain, elements = construct_domain(exprs, extension=True)\n\n    assert domain == QQ.algebraic_field((S(2).sqrt, S(2)))\n    assert elements == [QQ.algebraic_field((S(2).sqrt, S(2))).from_algebraic(expr) for expr in exprs]\n", "def test_construct_domain_extension_with_complex_coefficients():\n    # Test that when there are complex coefficients, the domain is set to CC\n    domain, _ = construct_domain([1 + I, 2 + 3*I])\n    assert domain.is_ComplexField\n    assert domain == CC\n"], "sample_1037": ["def test_only_squares():\n    # Test normal case\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    result = only_squares(A, B, C)\n    assert result == MatMul(A, B, C)\n\n    # Test non-square matrices\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 3)\n    with raises(ValueError):\n        only_squares(A, B)\n\n    # Test no matrices\n    A = MatrixSymbol('A', 2, 2)\n    result = only_squares(A)\n    assert result == A\n\n    # Test empty tuple\n    result = only_squares()\n    assert result == ()\n\n    # Test single matrix\n    A = MatrixSymbol('A', 2, 2)\n    result = only_squares(A)\n    assert result == A\n\n    # Test multiple non-square matrices\n    A = MatrixSymbol('A', 2, 3)\n    B = MatrixSymbol('B', 3, 2)\n    C = MatrixSymbol('C', 2, 2)\n    with raises(ValueError):\n        only_squares(A, B, C)\n\n    # Test matrices with different number of rows\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 3, 2)\n    with raises(ValueError):\n        only_squares(A, B)\n\n    # Test matrices with different number of columns\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 3)\n    with raises(ValueError):\n        only_squares(A, B)\n", "def test_refine_MatMul_orthogonal_matrices():\n    from sympy import MatrixSymbol, Q, assuming, refine, Identity\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    with assuming(Q.orthogonal(X)):\n        result = refine(MatMul(X, Y, Y.T))\n        assert result == Identity(2)\n    with assuming(Q.orthogonal(Y)):\n        result = refine(MatMul(X, X.T, Y))\n        assert result == Identity(2)\n", "def test_refine_MatMul_multiple_inverses():\n    from sympy.assumptions.ask import ask, Q\n    from sympy.assumptions.refine import handlers_dict\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = X * X.T * Y * Y.T\n    assert refine_MatMul(expr, {}) == Identity(2)\n    assert refine_MatMul(X * X.T * Y * Y.T, {'orthogonal': [Q.orthogonal(X), Q.orthogonal(Y)]}) == Identity(2)\n    assert refine_MatMul(X * X.T * Y * Y.T, {'orthogonal': [Q.orthogonal(X), Q.orthogonal(Y)], 'unitary': [Q.unitary(Y)]}) == Identity(2)\n", "def test_shapes_of_invalid_matmul():\n    A = MatrixSymbol('A', 3, 4)\n    B = MatrixSymbol('B', 5, 3)\n    C = MatrixSymbol('C', 3, 6)\n\n    # Check when the first two matrices are not compatible\n    with raises(ShapeError):\n        MatMul(A, B, C)\n\n    # Check when the last two matrices are not compatible\n    with raises(ShapeError):\n        MatMul(A, B, C).doit()\n\n    # Check when the first and last matrices are not compatible\n    with raises(ShapeError):\n        MatMul(C, B, A).doit()\n\n    # Check when the second and third matrices are not compatible\n    with raises(ShapeError):\n        MatMul(B, A, C).doit()\n\n    # Check when the first matrix has more columns than the second matrix has rows\n    with raises(ShapeError):\n        MatMul(A, B).doit()\n\n    # Check when the second matrix has more rows than the third matrix has columns\n    with raises(ShapeError):\n        MatMul(B, C).doit()\n\n    # Check when the first matrix has more columns than the second matrix has rows\n    with raises(ShapeError):\n        MatMul(C, B).doit()\n\n    # Check when the second matrix has more rows than the third matrix has columns\n    with raises(ShapeError):\n        MatMul(B, C).doit()\n\n    # Check when the first and second matrices are compatible, but the second and third are not\n    with raises(ShapeError):\n        MatMul(B, A, C).doit()\n", "    def test_matmul_with_zeros(self):\n        A = MatMul(Matrix([[1, 0], [0, 1]]), Matrix([[0, 0], [0, 0]]))\n        assert simplify(A) == Matrix([[0, 0], [0, 0]])\n"], "sample_1063": ["def test_lambdastr_dummify_false():\n    from sympy.abc import x, y, z\n    from sympy.utilities.lambdify import lambdastr\n    expr = x**2 + y + z\n    args = (x, y, z)\n    expected_str = \"lambda x,y,z: x**2 + y + z\"\n    assert lambdastr(args, expr, dummify=False) == expected_str\n", "def test_lambdify_lambdastr_transforms_complex_literal():\n    from sympy.abc import x\n    from sympy import lambdify, Lambda, sin, cos\n\n    # Test that lambdastr correctly transforms complex literals\n    expr = sin(x) + 2 + 3j\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0) + 2 + 3j)\n\n    # Test that lambdastr correctly transforms complex literals with negative sign\n    expr = sin(x) + -2 - 3j\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0) + -2 - 3j)\n\n    # Test that lambdastr correctly transforms complex literals when the imaginary part is zero\n    expr = sin(x) + 2 + 0j\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0) + 2)\n\n    # Test that lambdastr correctly transforms complex literals when the real part is zero\n    expr = sin(x) + 0 + 3j\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0) + 3j)\n\n    # Test that lambdastr correctly transforms complex literals when both the real and imaginary parts are zero\n    expr = sin(x) + 0 + 0j\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0))\n\n    # Test that lambdastr correctly transforms complex literals when the function is a lambda function\n    expr = Lambda(x, sin(x) + 2 + 3j)\n    f = lambdify(x, expr)\n    assert f(0) == (sin(0) + 2 + 3j)\n", "def test_lambdify_replaces_S(self):\n    from sympy import S, sin, lambdify, sympify\n    x = S(1)\n    expr = sin(x)\n    func = lambdify(x, expr, 'numpy')\n    self.assertEqual(func(1), 0.8414709848078965)\n    self.assertEqual(func(sympify(1)), 0.8414709848078965)\n", "def test_lambdify_modules():\n    from sympy import sin, cos, tan, exp, Piecewise, E, Integral, oo, I, Abs, Function, true, false, And, Or, Not, ITE, Min, Max, floor, diff, IndexedBase, Sum, DotProduct, Eq, Dummy, sinc, erf, erfc, factorial, gamma, loggamma, digamma, RisingFactorial, besselj, bessely, besseli, besselk, S, beta, MatrixSymbol, fresnelc, fresnels\n    from sympy.functions.elementary.complexes import re, im, Abs, arg\n    from sympy.functions.special.polynomials import chebyshevt, chebyshevu, legendre, hermite, laguerre, gegenbauer, assoc_legendre, assoc_laguerre, jacobi\n    from sympy.printing.pycode import NumPyPrinter\n\n    # Test with various modules\n    modules = [\n        'math', 'mpmath', 'numpy', 'scipy', 'tensorflow', 'sympy', 'numexpr'\n    ]\n\n    for module in modules:\n        print(f'Testing with module {module}')\n        try:\n            _import(module)\n        except NameError as e:\n            print(f'Skipping {module}: {e}')\n            continue\n\n        x, y, z = symbols('x,y,z')\n\n        # Test lambdastr with various arguments\n        args = x, y, z\n        expr = x + y + z\n        f = lambdify(args, expr, module)\n        assert f(1, 2, 3) == 6\n\n        args = (x, y, z)\n        expr = x + y + z\n        f = lambdify(args, expr, module)\n        assert f(1, 2, 3) == 6\n\n        args = x, y\n        expr = sin(x) * cos(y)\n        f = lambdify(args, expr, module)\n        assert f(1, 2) == math.cos(2) * math.sin(1)\n\n        # Test lambdastr with various modules\n        for module in modules:\n            try:\n                _import(module)\n            except NameError as e:\n                print(f'Skipping {module}: {e}')\n                continue\n\n            args", "def test_lambdify_with_tf_float32():\n    from sympy.abc import x\n    from sympy import Float, sin, tan, exp\n    from sympy.utilities.lambdify import lambdify\n\n    # Test that lambdify can handle tf.float32\n    # Note: This test requires tensorflow to be installed\n    if not tensorflow:\n        skip(\"TensorFlow not installed\")\n\n    x = Float(0, 32)  # tf.float32\n    expr = sin(x) + tan(x) + exp(x)\n    f = lambdify(x, expr, 'tensorflow')\n    result = f(tf.constant(0.0))\n    assert result.dtype == tf.float32\n    assert result.numpy() == 1.0\n\n    # Test that lambdify can handle tensor flows of different shapes\n    tensor = tf.constant([[0.0, 1.0], [2.0, 3.0]])\n    result = f(tensor)\n    assert result.dtype == tf.float32\n    assert result.numpy().tolist() == [[1.0, 1.704244, 3.2941073], [4.3818116, 6.1961524]]\n"], "sample_586": ["def test_concat_different_coords():\n    da1 = DataArray([1, 2, 3], dims=['x'])\n    da2 = DataArray([4, 5, 6], dims=['x'])\n    coords1 = {'x': pd.Index([1, 2, 3], name='x')}\n    coords2 = {'x': pd.Index([1, 2, 3], name='x'), 'y': pd.Index([4, 5, 6], name='y')}\n\n    ds1 = Dataset({'a': da1}, coords=coords1)\n    ds2 = Dataset({'b': da2}, coords=coords2)\n\n    ds = concat([ds1, ds2], dim='y', coords='different')\n\n    assert_equal(ds.coords['y'], pd.Index([4, 5, 6], name='y'))\n    assert_equal(ds.coords['x'], pd.Index([1, 2, 3], name='x'))\n    assert_equal(ds.data_vars, ['a', 'b'])\n", "def test_concat_compat_equals_no_conflicts():\n    da1 = DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = DataArray([4, 5, 6], dims=[\"x\"])\n    da3 = DataArray([7, 8, 9], dims=[\"x\"])\n\n    # Test with data_vars=\"minimal\"\n    ds = concat([da1.to_dataset(), da2.to_dataset(), da3.to_dataset()], \"x\", data_vars=\"minimal\", compat=\"equals\", fill_value=np.nan)\n    assert ds[\"x\"].values.tolist() == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert ds.attrs == {}\n\n    # Test with data_vars=\"all\"\n    ds = concat([da1.to_dataset(), da2.to_dataset(), da3.to_dataset()], \"x\", data_vars=\"all\", compat=\"equals\", fill_value=np.nan)\n    assert ds[\"x\"].values.tolist() == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert ds.attrs == {}\n\n    # Test with data_vars=[\"x\"]\n    ds = concat([da1.to_dataset(), da2.to_dataset(), da3.to_dataset()], \"x\", data_vars=[\"x\"], compat=\"equals\", fill_value=np.nan)\n    assert ds[\"x\"].values.tolist() == [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    assert ds.attrs == {}\n\n    # Test with data_vars=[\"y\"]\n    with pytest.raises(ValueError):\n        concat([da1.to_dataset(), da2.to_dataset(), da3.to_dataset()], \"x\", data_vars=[\"y\"], compat=\"equals\", fill_value=np.nan)\n", "def test_concat_dataset_with_valid_compat_equals():\n    data_vars = {'x': ('t', np.arange(4)), 'y': ('t', np.arange(4))}\n\n    ds1 = Dataset(data_vars, coords={'time': ('t', np.arange(2))})\n    ds2 = Dataset(data_vars, coords={'time': ('t', np.arange(2))})\n\n    ds = concat([ds1, ds2], 'time', data_vars='different', compat='equals')\n    expected = Dataset(data_vars, coords={'time': ('time', np.arange(4))})\n    assert_equal(ds, expected)\n", "def test_concat_compat_overrides_coord():\n    # Create two datasets with a common coordinate but different values\n    ds1 = Dataset(\n        coords={\"x\": np.arange(10), \"y\": np.linspace(0, 1, 10)},\n        data_vars={\"var1\": ((\"x\", \"y\"), np.random.rand(10, 10))},\n    )\n    ds2 = Dataset(\n        coords={\"x\": np.arange(10), \"y\": np.linspace(0, 1, 10)},\n        data_vars={\"var1\": ((\"x\", \"y\"), np.random.rand(10, 10))},\n    )\n    ds2[\"var1\"] = ds2[\"var1\"].assign_attrs({\"_FillValue\": 1.0})\n\n    # Test that compat='override' ignores the coordinate values and uses the first dataset's value\n    result = concat([ds1, ds2], dim=\"y\", compat=\"override\")\n    assert_identical(ds1, result)\n\n    # Test that compat='override' also works when the coordinate is a DataArray\n    da1 = DataArray(np.arange(10), dims=[\"x\"], coords={\"x\": np.arange(10)})\n    ds1 = Dataset({\"x\": da1, \"y\": np.linspace(0, 1, 10)})\n    da2 = DataArray(np.arange(10), dims=[\"x\"], coords={\"x\": np.arange(10)})\n    ds2 = Dataset({\"x\": da2, \"y\": np.linspace(0, 1, 10)})\n    ds2[\"x\"] = ds2[\"x\"].assign_attrs({\"_FillValue\": 1.0})\n    result = concat([ds1, ds2], dim=\"y\", compat=\"override\")\n    assert_identical(ds1, result)\n", "def test_concat_all_different_equals():\n    # test data\n    da1 = DataArray([1, 2, 3], dims=[\"x\"])\n    da2 = DataArray([4, 5, 6], dims=[\"x\"])\n    da3 = DataArray([7, 8, 9], dims=[\"x\"])\n\n    # test that 'different' option does not raise an error\n    result = concat([da1, da2, da3], dim=\"x\", data_vars=\"different\", coords=\"all\")\n    assert isinstance(result, DataArray)\n\n    # test that variables are correctly merged\n    assert np.array_equal(result.data, np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]))\n\n    # test that attrs are correctly merged\n    result = concat([da1, da2, da3], dim=\"x\", data_vars=\"different\", coords=\"all\")\n    da1.attrs[\"a\"] = \"b\"\n    da2.attrs[\"b\"] = \"c\"\n    da3.attrs[\"c\"] = \"d\"\n    result = concat([da1, da2, da3], dim=\"x\", data_vars=\"different\", coords=\"all\")\n    assert result.attrs == {\"a\": \"b\", \"b\": \"c\", \"c\": \"d\"}\n\n    # test that encoding is correctly merged\n    da1.encoding = {\"encoding\": \"encoding1\"}\n    da2.encoding = {\"encoding\": \"encoding2\"}\n    da3.encoding = {\"encoding\": \"encoding3\"}\n    result = concat([da1, da2, da3], dim=\"x\", data_vars=\"different\", coords=\"all\")\n    assert result.encoding == {\"encoding\": \"encoding3\"}\n\n    # test that dim_coords are correctly merged\n    da1.coords[\"dim\"] = np.array([1])\n    da2.coords[\"dim\"] = np.array([2])\n    da3.coords[\"dim\"] = np.array([3])\n    result = concat([da1, da2, da3], dim=\"x\", data_vars=\"different\", coords=\"all\")\n    assert_array_equal(result.coords[\"dim\"], np.array([1, 2, 3]))\n"], "sample_780": ["def test_latentdirichletallocation__partial_fit_not_fitted__raises_notfittederror():\n    # Set up the LDA model with default parameters\n    lda = LatentDirichletAllocation(n_components=10)\n\n    # Check if partial_fit raises NotFittedError when the model is not fitted\n    with assert_raises_regexp(NotFittedError, \"call `fit` first\"):\n        lda.partial_fit(np.random.rand(100, 100))\n", "def test_partial_fit_with_zero_batch_size():\n    n_components, X = _build_sparse_mtx()\n    lda = LatentDirichletAllocation(n_components=n_components)\n    lda.partial_fit(X)\n    assert_equal(lda.n_batch_iter_, 1)\n    assert_equal(lda.n_iter_, 1)\n    assert_array_almost_equal(lda.components_.sum(1), 1)\n    assert lda.bound_ is None\n", "def test_check_params_fails_on_invalid_learning_method(learning_method):\n    \"\"\"Check that setting an invalid learning method raises an error.\"\"\"\n    lda = LatentDirichletAllocation(learning_method=learning_method)\n    with assert_raises_regexp(ValueError, \"Invalid 'learning_method' parameter: '%s'\" % learning_method):\n        lda._check_params()\n", "def test_partial_fit_cover_all_docs(total_samples):\n    # Test that calling partial_fit on multiple sets of data cover all data\n    # points.\n    n_components = 3\n    X = np.random.randint(0, 2, (100, n_components))\n    lda = LatentDirichletAllocation(n_components=n_components)\n    lda.fit(X)\n\n    n = len(lda.partial_fit(X[:10]))\n    n_total = len(lda.partial_fit(X[:20]))\n    n_total_10 = len(lda.partial_fit(X[:10]))\n    n_total_20 = len(lda.partial_fit(X[:20]))\n    assert_equal(n, 1)\n    assert_equal(n_total, 2)\n    assert_equal(n_total_10, 1)\n    assert_equal(n_total_20, 2)\n\n", "def test_perplexity():\n    # Test that the perplexity function is monotonically decreasing in the\n    # number of components\n    n_components = 3\n    topic_word_prior = 1.0\n    doc_topic_prior = 1.0\n    X, matrix = _build_sparse_mtx()\n\n    lda = LatentDirichletAllocation(n_components, topic_word_prior,\n                                    doc_topic_prior, learning_method='batch')\n    lda.fit(matrix)\n\n    perps = []\n    for n_components in [1, 2, 3, 4, 5, 6, 7, 8, 9]:\n        lda.n_components = n_components\n        perps.append(lda.perplexity(matrix))\n\n    assert_array_almost_equal(perps[:-1], perps[1:], decimal=1)\n"], "sample_1075": ["def test_beta_function_eval_rewrite_as_gamma():\n    x, y = Symbol('x'), Symbol('y')\n    beta_func = beta(x, y)\n    gamma_func = gamma(x)*gamma(y) / gamma(x + y)\n    assert beta_func._eval_rewrite_as_gamma(x, y) == gamma_func\n    assert beta_func._eval_rewrite_as_gamma(x, y, **{'x': 2}) == gamma(2)*gamma(y) / gamma(2 + y)\n", "def test_beta_eval():\n    x, y = Symbol('x'), Symbol('y')\n    beta_func = beta(x, y)\n    assert beta_func.expand().args == (gamma(x), gamma(y), gamma(x + y))\n", "def test_beta_is_real():\n    x, y = Symbol('x'), Symbol('y')\n    beta_function = beta(x, y)\n    assert beta_function._eval_is_real()\n", "def test_beta_func_symmetry():\n    x, y = Symbol('x'), Symbol('y')\n    beta_func = beta(x, y)\n    # Test mirror symmetry\n    assert beta_func == beta(y, x)\n    # Test mirror symmetry with conjugate\n    assert beta_func.conjugate() == beta_func.conjugate()\n", "def test_beta_function_expansion():\n    x, y = Symbol('x'), Symbol('y')\n    beta_func = beta(x, y)\n    expanded = expand_func(beta_func)\n    assert expanded == beta(x, y), \"Beta function expansion is incorrect\"\n"], "sample_906": ["def check_macro(name, input, idDict, output, key=None, asTextOutput=None):\n    if key is None:\n        key = name\n    key += ' '\n    if name == \"macro\":\n        inputActual = input\n        outputAst = output\n        outputAsText = output\n    else:\n        inputActual = input.format(key='')\n        outputAst = output.format(key='')\n        outputAsText = output.format(key=key)\n    if asTextOutput is not None:\n        outputAsText = asTextOutput\n\n    # first a simple check of the AST\n    ast = parse(name, inputActual)\n    res = str(ast)\n    if res != outputAst:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"Result:   \", res)\n        print(\"Expected: \", outputAst)\n        raise DefinitionError(\"\")\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature(input, '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    resAsText = parentNode.astext()\n    if resAsText != outputAsText:\n        print(\"\")\n        print(\"Input:    \", input)\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", outputAsText)\n        print(\"Node:\", parentNode)\n        raise DefinitionError(\"\")\n\n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        try:\n            id = ast.get_id(version=i)\n            assert id is not None\n            idActual.append(id[len(_id_prefix[i]):])\n        except NoOldIdError:\n            idActual.append(None)\n\n    res = [True]\n    for i in range(1, _max_id + 1):\n        res.append(idExpected[i] == idActual[i])\n\n    if not all(res):\n        print(\"input:    %s\" % input.rjust(20))\n        for i in range(1, _max_id + 1):\n            if", "def test_parse_type_definition_c_string_type():\n    ast = parse('type', 'c_string_type')\n    expectedAst = ASTTrailingTypeSpecName(prefix=None, nestedName=ASTNestedName([ASTIdentifier('c_string_type')], rooted=False))\n    res = str(ast)\n    if res != str(expectedAst):\n        print(\"\")\n        print(\"Input:    \", 'c_string_type')\n        print(\"Result:   \", res)\n        print(\"Expected: \", str(expectedAst))\n        raise DefinitionError(\"\")\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    symbol = rootSymbol.add_declaration(ast, docname=\"TestDoc\", line=42)\n    parentNode = addnodes.desc()\n    signode = addnodes.desc_signature('c_string_type', '')\n    parentNode += signode\n    ast.describe_signature(signode, 'lastIsName', symbol, options={})\n    resAsText = parentNode.astext()\n    if resAsText != 'c_string_type':\n        print(\"\")\n        print(\"Input:    \", 'c_string_type')\n        print(\"astext(): \", resAsText)\n        print(\"Expected: \", 'c_string_type')\n        print(\"Node:\", parentNode)\n        raise DefinitionError(\"\")\n    idExpected = [None]\n    for i in range(1, _max_id + 1):\n        if i in idDict:\n            idExpected.append(idDict[i])\n        else:\n            idExpected.append(idExpected[i - 1])\n    idActual = [None]\n    for i in range(1, _max_id + 1):\n        try:\n            id = ast.get_id(version=i)\n            assert id is not None\n            idActual.append(id[len(_id_prefix[i]):])\n        except NoOldIdError:\n            idActual.append(None)\n\n    res = [True]\n    for i in range(1, _max_id + 1):\n        res.append(idExpected[i] == idActual[i])\n\n    if not all(res):\n        print(\"input:    %s\" % 'c_string_type'.rjust(20))\n        for i in range(1, _max_id + 1):\n            if res[i]:\n                continue\n            print(\"Error in id version %d.\" % i)\n            print(\"result:   %s\" % idActual[i])\n            print(\"expected: %s\" % idExpected[i])\n        print(rootSymbol.dump(0))\n        raise DefinitionError(\"\")\n", "def check_member(name, input, idDict, output=None, key=None, asTextOutput=None):\n    if output is None:\n        output = input\n    # First, check without semicolon\n    _check(name, input, idDict, output, key, asTextOutput)\n    # Second, check with semicolon\n    _check(name, input + ' ;', idDict, output + ';', key,\n           asTextOutput + ';' if asTextOutput is not None else None)\n", "def test_c_array_declarator():\n        check(name, input, idDict, output, key, asTextOutput)\n\n    check_array_declarator('member', 'int {a, b}[3];', {1: 'int{a,b}[3]'})\n    check_array_declarator('member', 'int {a, b}*; ', {1: 'int{a,b}*'})\n    check_array_declarator('member', 'int {a, b}*[3];', {1: 'int{a,b}*[3]'})\n    check_array_declarator('member', 'int {a, b}*[3][4];', {1: 'int{a,b}*[3][4]'})\n    check_array_declarator('member', 'int {a, b}*[3][4][5];', {1: 'int{a,b}*[3][4][5]'})\n    check_array_declarator('member', 'int {a, b}[3][4];', {1: 'int{a,b}[3][4]'})\n    check_array_declarator('member', 'int {a, b}[3][4][5];', {1: 'int{a,b}[3][4][5]'})\n\n    check_array_declarator('function', 'int foo(int {a, b}[3]);', {1: 'int foo(int {a,b}[3])'})\n    check_array_declarator('function', 'int foo(int {a, b}*);', {1: 'int foo(int {a,b}* )'})\n    check_array_declarator('function', 'int foo(int {a, b}*[3]);', {1: 'int foo(int {a,b}*[3])'})\n    check_array_declarator('function', 'int foo(int {a, b}*[3][4]);', {1: 'int foo(int {a,b}*[3][4])'})\n    check_array_declarator('function', 'int foo(int {a, b}*[3][4][5]);', {1: 'int foo(int {a,b}*[3][4][5])'})\n    check_array_declarator('function', 'int foo(int {a, b}[3][4", "def test_parse_parameters():\n    input = \"void foo(int x, int y, ...);\"\n    output = \"(int x, int y, ...)\"\n    idDict = {}\n    _check('function', input, idDict, output)\n"], "sample_825": ["def test_PLSSVD_fit_transform():\n    rng = check_random_state(0)\n    X = rng.rand(10, 5)\n    Y = rng.rand(10, 3)\n    pls = PLSSVD(n_components=2)\n    pls.fit(X, Y)\n    Xr, Yr = pls.transform(X, Y)\n    assert_equal(Xr.shape, (10, 2))\n    assert_equal(Yr.shape, (10, 2))\n", "def test_PLSSVD_fit_transform():\n    \"\"\"Test the fit_transform method of PLSSVD\"\"\"\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls = PLSSVD(n_components=2)\n    pls.fit(X, Y)\n    X_t, Y_t = pls.fit_transform(X, Y)\n    assert_equal(X_t.shape, (4, 2))\n    assert_equal(Y_t.shape, (4, 2))\n    assert_array_almost_equal(np.dot(X_t, pls.x_weights_.T),\n                            np.dot(X, pls.x_weights_.T))\n    assert_array_almost_equal(np.dot(Y_t, pls.y_weights_.T),\n                            np.dot(Y, pls.y_weights_.T))\n", "def test_PLSRegression_max_iter():\n    # Test max_iter with a convergent case\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls = PLSRegression(n_components=1, max_iter=10)\n    pls.fit(X, Y)\n    assert pls.n_iter_[0] == 1\n\n    # Test max_iter with a non-convergent case\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls = PLSRegression(n_components=1, max_iter=10)\n    with pytest.warns(ConvergenceWarning):\n        pls.fit(X, Y)\n\n    # Test max_iter with a convergent case and svd algorithm\n    X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\n    Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\n    pls = PLSRegression(n_components=1, algorithm='svd', max_iter=10)\n    pls.fit(X, Y)\n    assert pls.n_iter_[0] == 1\n", "def test_PLSSVD():\n    rng = check_random_state(42)\n    X = rng.randn(10, 3)\n    Y = rng.randn(10, 2)\n    pls = PLSSVD(n_components=1)\n    pls.fit(X, Y)\n    transformed_X, transformed_Y = pls.transform(X, Y)\n    assert_equal(transformed_X.shape[1], 1)\n    assert_equal(transformed_Y.shape[1], 1)\n    assert_equal(transformed_X.shape[0], 10)\n", "def test_PLSRegression():\n    X, y = load_linnerud(return_X_y=True)\n    X = StandardScaler().fit_transform(X)\n\n    pls = PLSRegression(n_components=1)\n    pls.fit(X, y)\n    X_scores, y_scores = pls.transform(X, y)\n\n    # Check if the number of components is correct\n    assert pls.n_components == 1\n    # Check if the shape of the scores are correct\n    assert X_scores.shape == (185, 1)\n    assert y_scores.shape == (185, 1)\n\n    # Check if the fit_transform method produces the same results\n    X_scores_ft, y_scores_ft = pls.fit_transform(X, y)\n    assert_array_almost_equal(X_scores, X_scores_ft)\n    assert_array_almost_equal(y_scores, y_scores_ft)\n\n    # Check if the predict method produces the same results\n    y_pred = pls.predict(X)\n    assert_array_almost_equal(y_pred, y_scores)\n"], "sample_1004": ["def test_free_symbols():\n    c = ConditionSet(x, x < 2, Interval(0, 4))\n    assert c.free_symbols == set([x])\n    c = ConditionSet(x, x < 2, S.Integers)\n    assert c.free_symbols == set([x])\n    c = ConditionSet(x, x < 2, ConditionSet(y, y < 3, S.Reals))\n    assert c.free_symbols == set([x, y])\n", "def test_condition_set_interval():\n    x = Symbol('x')\n    condition = Eq(x, 0)\n    base_set = Interval(0, 1)\n    condition_set = ConditionSet(x, condition, base_set)\n    assert condition_set.contains(0)  # 0 in [0, 1]\n    assert not condition_set.contains(1)  # 1 not in [0, 1]\n", "def test_dummy_substitution():\n    from sympy import S, Symbol, ConditionSet, Interval\n    from sympy.abc import x, y\n\n    c = ConditionSet(x, x < 1, {x, y})\n    assert c.sym == x\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, y})\n\n    c = ConditionSet(x, x < 1, {y})\n    assert c.sym == x\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y})\n\n    # test dummy symbol substitution with different base sets\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.base_set == Interval(0, 2)\n    assert c.subs(x, y) == ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.base_set == Interval(0, 2)\n\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.subs(y, 1) == ConditionSet(x, x < 1, Interval(0, 2))\n\n    c = ConditionSet(x, x < 1, Interval(0, 2))\n    assert c.subs(y, x) == ConditionSet(x, x < 1, Interval(0, 2))\n", "def test_condition_set_base_set_universal_set():\n    from sympy import S, ConditionSet, pi, sin, Eq\n    x = Symbol('x')\n    cond = Eq(sin(x), 0)\n    assert ConditionSet(x, cond).base_set == S.UniversalSet()\n    assert ConditionSet(x, cond).base_set.contains(S(5))\n    assert ConditionSet(x, cond).base_set.contains(5)\n", "def test_condition_set_substitution():\n    # Define symbols\n    y = Symbol('y')\n    w = Symbol('w')\n    n = Symbol('n', negative=True)\n    \n    # Create a ConditionSet\n    c = ConditionSet(n, n > 0, S.Integers)\n    \n    # Substitute the base set\n    c = c.subs(n, w)\n    \n    # Try to substitute the dummy symbol\n    c = c.subs(c.sym, y)\n    \n    # The dummy symbol should still be w\n    assert c.sym == w\n    \n    # Now substitute w with a new value\n    c = c.subs(w, 5)\n    \n    # The base set should have been updated\n    assert c.base_set == S.Integers\n    \n    # But the dummy symbol should still be y\n    assert c.sym == y\n    \n    # Check that contains returns False\n    assert not c.contains(5)\n    "], "sample_958": ["def test_ast_type_with_init():\n    ast = parse('type', \"int f(int x) {}\")\n    assert ast.declSpecs.outer is None\n    assert ast.declSpecs.leftSpecs.storage is None\n    assert ast.declSpecs.leftSpecs.threadLocal is None\n    assert ast.declSpecs.leftSpecs.inline is None\n    assert not ast.declSpecs.leftSpecs.restrict\n    assert not ast.declSpecs.leftSpecs.volatile\n    assert not ast.declSpecs.leftSpecs.const\n    assert ast.declSpecs.leftSpecs.attrs == []\n    assert ast.declSpecs.rightSpecs.storage is None\n    assert ast.declSpecs.rightSpecs.threadLocal is None\n    assert ast.declSpecs.rightSpecs.inline is None\n    assert not ast.declSpecs.rightSpecs.restrict\n    assert not ast.declSpecs.rightSpecs.volatile\n    assert not ast.declSpecs.rightSpecs.const\n    assert ast.declSpecs.rightSpecs.attrs == []\n    assert isinstance(ast.decl, ASTDeclaratorNameParam)\n    assert ast.decl.declId is not None\n    assert ast.decl.declId.name == \"f\"\n    assert ast.decl.arrayOps == []\n    assert isinstance(ast.decl.param, ASTParameters)\n    assert ast.decl.param.args == [ASTFunctionParameter(ASTType(ASTDeclSpecsSimple(None, None, None, None, None, None, []), ASTDeclaratorNameParam(None, [], None)))]  # noqa: E501\n    assert ast.decl.param.attrs == []\n", "def test_member_function_parameter_id_with_macro_args():\n    idDict = {1: 'a', 2: 'b'}\n    check('member', 'int foo(int a, int b) { return a + b; }',\n          idDict, output='int foo(int ,int ) { return  + ; }',\n          key='foo', asTextOutput='int foo(int ,int ) { return  + ; }')\n", "def test_CAliasObject_check_xref_role_role_type_vs_object_type():\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    domain = cppDomain.CDomain()\n    env = cppDomain.Symbol(None, None, None, None, None, None, None)\n    env.config = Config()\n    aliasObj = domain.directives['alias'](None, None, env, 'test:alias', 'test')\n    parser = DefinitionParser('test:alias::test', None, env.config)\n    try:\n        name = parser.parse_xref_object()\n    except DefinitionError as e:\n        logger.warning(e, location=None)\n        name = None\n\n    if name is None:\n        return\n\n    rootSymbol: Symbol = domain.data['root_symbol']\n    parentSymbol: Symbol = rootSymbol.direct_lookup(domain.get_objtype('alias').lookup_key)\n    assert parentSymbol  # should be there\n    s = parentSymbol.find_declaration(name, 'alias',\n                                      matchSelf=True, recurseInAnon=True)\n    if s is None or s.declaration is None:\n        return\n\n    # TODO: check role type vs. object type\n    declaration = s.declaration\n    displayName = name.get_display_string()\n    assert declaration.objectType == 'alias'\n", "def test_parse_declaration_Cpp():\n    # TODO: https://en.cppreference.com/w/cpp/language/c++11#Context\n    idDict = {1: 'cpp::std::tuple', 2: 'cpp::std::vector'}\n\n    check('type', 'type name(tuple, T...)', idDict, 'name(tuple, T...)')\n    check('type', 'type name(std::tuple, T...)', idDict, 'name(std::tuple, T...)')\n    check('type', 'type name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)')\n    check('type', 'type name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)', key='std::__tuple')\n\n    check('function', 'function name(std::tuple, T...)', idDict, 'name(std::tuple, T...)')\n    check('function', 'function name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)')\n    check('function', 'function name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)', key='std::__tuple')\n\n    check('member', 'member name(std::tuple, T...)', idDict, 'name(std::tuple, T...)')\n    check('member', 'member name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)')\n    check('member', 'member name(std::__tuple, T...)', idDict, 'name(std::__tuple, T...)', key='std::__tuple')\n\n    check('functionParam', 'function name(std::tuple, T...) param', idDict,\n            output='name(std::tuple, T...) param')\n    check('functionParam', 'function name(std::__tuple, T...) param', idDict,\n            output='name(std::__tuple, T...) param')\n    check('functionParam', 'function name(std::__tuple, T...) param', idDict,\n            output='name(std::__tuple, T...) param', key='std::__tuple')\n", "def test_CAliasObject_all_aliases_and_noroot():\n    # Test that a C alias works when it expands into multiple signatures.\n    class Config:\n        cpp_id_attributes = [\"id_attr\"]\n        cpp_paren_attributes = [\"paren_attr\"]\n    parser = DefinitionParser('namespace::x::y', location=None, config=Config())\n    root = parser.parse_namespace_object()\n    parser.assert_end()\n    rootSymbol = Symbol(None, None, None, None, None, None, None)\n    symbol = rootSymbol.add_name(root)\n    document = nodes.document()\n    aliasOptions = {\n        'maxdepth': 2,\n        'noroot': True,\n    }\n    node = addnodes.desc()\n    node.document = document\n    node['domain'] = 'c'\n    # 'desctype' is a backwards compatible attribute\n    node['objtype'] = node['desctype'] = 'alias'\n    node['noindex'] = True\n\n    alias = CAliasObject('alias:func::x:y::z', node)\n    alias.options = aliasOptions\n    alias.names = ['alias:func::x:y::z']\n    assert len(alias.get_signatures()) == 2\n    assert len(alias.node.children) == 2\n    assert len(symbol._children) == 2\n    assert alias.node is not None\n    assert alias.node['domain'] == 'c'\n    assert alias.node['objtype'] == 'alias'\n    assert alias.node['noindex'] == True\n    assert len(alias.node['ids']) == 0\n\n    # Test that a C alias works when it expands into multiple signatures and noroot is True.\n    alias = CAliasObject('alias:func::x:y::z', node)\n    alias.options = aliasOptions\n    alias.names = ['alias:func::x:y::z']\n    assert len(alias.get_signatures()) == 2\n    assert len(alias.node.children) == 2\n    assert len(symbol._children) == 2\n    assert alias.node is not None\n    assert alias.node['domain'] == 'c'\n    assert alias.node['objtype'] == 'alias'\n    assert alias.node['noindex'] == True\n    assert len(alias.node['ids']) == 0\n\n    # Test that a C alias has no ids when noroot is True.\n    aliasOptions = {\n        'maxdepth': 2,\n        'noroot': True,\n    }\n"], "sample_303": ["    def test_settings_to_cmd_args_env_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            BaseDatabaseClient(settings_dict={}, parameters={}).settings_to_cmd_args_env(\n                settings_dict={}, parameters={}\n            )\n", "    def test_settings_to_cmd_args_env_called_with_settings_dict_and_parameters(self, mock_settings_to_cmd_args_env):\n        client = BaseDatabaseClient(connection=connection)\n        settings_dict = {'key1': 'value1', 'key2': 'value2'}\n        parameters = {'param1': 'value1', 'param2': 'value2'}\n        mock_settings_to_cmd_args_env.return_value = ('args', {'env_var1': 'env_value1'})\n        result = client.settings_to_cmd_args_env(settings_dict, parameters)\n        mock_settings_to_cmd_args_env.assert_called_once_with(settings_dict, parameters)\n        self.assertEqual(result, ('args', {'env_var1': 'env_value1'}))\n", "    def test_settings_to_cmd_args_env_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            BaseDatabaseClient(settings_dict={}, parameters={}).settings_to_cmd_args_env(settings_dict={}, parameters={})\n", "    def test_runshell_with_env_variables(self):\n        # Arrange\n        settings_dict = {'DB_USER': 'test_user', 'DB_PASSWORD': 'test_password'}\n        parameters = []\n        with mock.patch.object(os, 'environ', {'DB_USER': 'wrong_user'}):\n            with self.assertRaises(subprocess.CalledProcessError):\n                self.client.runshell(parameters)\n", "def test_settings_to_cmd_args_env_fails_if_subclass_does_not_implement(self):\n    with self.assertRaises(NotImplementedError):\n        BaseDatabaseClient(settings_dict={'key': 'value'}, parameters={'param': 'value'})\n"], "sample_1126": ["def test_dagger_complex_conjugate_and_transpose():\n    x = symbols('x')\n    A = Operator(x**2 + 3*x + 2)\n    result = Dagger(A)\n    assert result == Dagger(Operator(x**2 + 3*x + 2)), f\"Expected Dagger(A) to be Dagger(A) but got {result}\"\n    assert result.args == [Dagger(A)], f\"Expected result.args to be [Dagger(A)] but got {result.args}\"\n    assert result.is_Dagger, f\"Expected result.is_Dagger to be True but got {result.is_Dagger}\"\n", "def test_dagger_on_matrix_with_multiple_elements():\n    # Create a matrix with multiple elements\n    x, y, z, w = symbols('x y z w')\n    matrix = Matrix([[x + I*y, z + w*I], [x - I*y, z - w*I]])\n\n    # Take the dagger of the matrix\n    result = Dagger(matrix)\n\n    # The result should be the conjugate and transpose of the original matrix\n    expected_result = Matrix([[x - I*y, x + I*y], [z - w*I, z + w*I]])\n\n    assert result.equals(expected_result)\n", "def test_dagger_with_non_numberic_matrix():\n    # Test with non-numeric matrix\n    m = Matrix([[symbols('a', real=True), symbols('b', real=True)],\n               [symbols('c', real=True), symbols('d', real=True)]])\n    dag_m = Dagger(m)\n    # Expected result of dagger operation on the non-numeric matrix\n    expected_result = Matrix([[symbols('a', real=True), symbols('c', real=True)],\n                            [symbols('b', real=True), symbols('d', real=True)]])\n    assert dag_m == expected_result\n", "def test_dagger_on_matrix_with_real_elements():\n    \"\"\"Test that the dagger of a matrix with real elements is correctly\n    calculated.\"\"\"\n    m = Matrix([[1, 2], [3, 4]])\n    dagger_m = Dagger(m)\n    expected_dagger_m = Matrix([[1, 3], [2, 4]])\n    assert dagger_m.equals(expected_dagger_m)\n", "def test_dagger_complex_conjugate_and_transpose_of_Mul():\n    # Create a test expression as a product of multiple terms\n    A = Operator('A')\n    B = Operator('B')\n    x = symbols('x')\n    expr = (A*x + B).simplify()\n\n    # Test that the dagger operation applies correctly\n    result = Dagger(expr)\n    assert result == Dagger(B) * Dagger(A)*x + Dagger(B)\n"], "sample_1117": ["def test_orthogonal_matrix_transpose():\n    X = MatrixSymbol('X', 2, 2)\n    assert AskSymmetricHandler.Transpose(X, [Q.orthogonal(X)])\n", "def test_AskDiagonalHandler():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 3, 3)\n    Z = MatrixSymbol('Z', 2, 2)\n    A1x1 = MatrixSymbol('A1x1', 1, 1)\n    B1x1 = MatrixSymbol('B1x1', 1, 1)\n    C0x0 = MatrixSymbol('C0x0', 0, 0)\n    V1 = MatrixSymbol('V1', 2, 1)\n    V2 = MatrixSymbol('V2', 2, 1)\n    W = MatrixSymbol('W', 2, 2)\n\n    # Test diagonal matrices\n    assert AskDiagonalHandler.DiagonalMatrix(X, Q.true)\n    assert AskDiagonalHandler.DiagonalMatrix(A1x1, Q.true)\n    assert AskDiagonalHandler.DiagonalMatrix(B1x1, Q.true)\n    assert AskDiagonalHandler.DiagonalMatrix(C0x0, Q.true)\n\n    # Test non-diagonal matrices\n    assert not AskDiagonalHandler.DiagonalMatrix(Y, Q.true)\n    assert not AskDiagonalHandler.DiagonalMatrix(Z, Q.true)\n    assert not AskDiagonalHandler.DiagonalMatrix(V1, Q.true)\n    assert not AskDiagonalHandler.DiagonalMatrix(V2, Q.true)\n    assert not AskDiagonalHandler.DiagonalMatrix(W, Q.true)\n\n    # Test Identity matrix\n    assert AskDiagonalHandler.Identity(X, Q.true)\n    assert AskDiagonalHandler.Identity(A1x1, Q.true)\n    assert AskDiagonalHandler.Identity(B1x1, Q.true)\n    assert AskDiagonalHandler.Identity(C0x0, Q.true)\n\n    # Test non-Identity matrices\n    assert not AskDiagonalHandler.Identity(Y, Q.true)\n    assert not AskDiagonalHandler.Identity(Z, Q.true)\n    assert not AskDiagonalHandler.Identity(V1, Q.true)\n    assert not AskDiagonalHandler.Identity(V2, Q.true)\n    assert not AskDiagonalHandler.Identity(W, Q.true)\n\n    # Test DiagMatrix\n    assert AskDiagonalHandler.DiagMatrix(X, Q.true)\n    assert AskDiagonalHandler.DiagMatrix(A1x1, Q.true)\n    assert AskDiagonalHandler.DiagMatrix(B1x1, Q.true)\n    assert AskDi", "def test_AskSymmetricHandler_MatrixSlice_diagonal_matrix():\n    X = DiagonalMatrix([[1, 0], [0, 1]])\n    assert AskSymmetricHandler.MatrixSlice(X, {}) == True\n", "def test_AskSymmetricHandler():\n    # Check that the MatMul method returns True when all matrix arguments are symmetric\n    assert AskSymmetricHandler.MatMul(MatrixMul(X, X), {}) == True\n\n    # Check that the MatMul method returns False when at least one matrix argument is not symmetric\n    assert AskSymmetricHandler.MatMul(MatrixMul(X, Y), {}) == False\n\n    # Check that the MatPow method returns True when the base matrix is symmetric and the exponent is non-negative\n    assert AskSymmetricHandler.MatPow(MatrixPow(X, 2), {}) == True\n\n    # Check that the MatPow method returns None when the base matrix is not symmetric\n    assert AskSymmetricHandler.MatPow(MatrixPow(X, -1), {}) == None\n\n    # Check that the MatAdd method returns True when all matrix arguments are symmetric\n    assert AskSymmetricHandler.MatAdd(X + Y) == True\n\n    # Check that the MatrixSymbol method returns True when the matrix is symmetric\n    assert AskSymmetricHandler.MatrixSymbol(X, {}) == True\n\n    # Check that the MatrixSymbol method returns False when the matrix is not symmetric\n    assert AskSymmetricHandler.MatrixSymbol(Y, {}) == False\n\n    # Check that the ZeroMatrix method returns True when the matrix is a zero matrix\n    assert AskSymmetricHandler.ZeroMatrix(ZeroMatrix(2, 2), {}) == True\n\n    # Check that the OneMatrix method returns True when the matrix is a 1x1 matrix\n    assert AskSymmetricHandler.OneMatrix(OneMatrix(1, 1), {}) == True\n\n    # Check that the Transpose method returns True when the matrix is symmetric\n    assert AskSymmetricHandler.Transpose(Transpose(X), {}) == True\n\n    # Check that the Inverse method returns True when the matrix is symmetric and invertible\n    assert AskSymmetricHandler.Inverse(Inverse(X), {}) == True\n\n    # Check that the MatrixSlice method returns True when the matrix slice is symmetric\n    assert AskSymmetricHandler.MatrixSlice(MatrixSlice(X, (1, 1)), {}) == True\n", "def test_AskMatrixSlice():\n    # Test if MatrixSlice of a diagonal matrix is diagonal\n    M = DiagonalMatrix([[1, 2], [3, 4]])\n    assert ask(Q.diagonal(MatrixSlice(M[1, 1]), None), None)\n    assert ask(Q.diagonal(MatrixSlice(M, (1, 1))), None)\n    assert ask(Q.diagonal(MatrixSlice(M, (0, 0))), None)\n\n    # Test if MatrixSlice of an upper triangular matrix is upper triangular\n    M = MatrixSymbol('M', 3, 3)\n    M = M.upper_triangular()\n    assert ask(Q.upper_triangular(MatrixSlice(M, (0, 1))), None)\n    assert ask(Q.upper_triangular(MatrixSlice(M, (0, 0))), None)\n\n    # Test if MatrixSlice of a lower triangular matrix is lower triangular\n    M = MatrixSymbol('M', 3, 3)\n    M = M.lower_triangular()\n    assert ask(Q.lower_triangular(MatrixSlice(M, (0, 1))), None)\n    assert ask(Q.lower_triangular(MatrixSlice(M, (0, 0))), None)\n\n    # Test if MatrixSlice of a matrix with integer elements is diagonal\n    M = DiagMatrix([1, 2, 3])\n    assert ask(Q.diagonal(MatrixSlice(M, (0, 0))), None)\n    assert ask(Q.diagonal(MatrixSlice(M, (1, 1))), None)\n\n    # Test if MatrixSlice of a matrix with real elements is diagonal\n    M = DiagMatrix([1, 2, 3])\n    assert ask(Q.diagonal(MatrixSlice(M, (0, 0))), None)\n    assert ask(Q.diagonal(MatrixSlice(M, (1, 1))), None)\n\n    # Test if MatrixSlice of a matrix with complex elements is diagonal\n    M = DiagMatrix([1, 2, 3])\n    assert ask(Q.diagonal(MatrixSlice(M, (0, 0))), None)\n    assert ask(Q.diagonal(MatrixSlice(M, (1, 1))), None)\n"], "sample_1035": ["def test_measure_all_qubit():\n    qubit = IntQubit(1)\n    result = measure_all(qubit)\n    expected_result = [(IntQubit(1), 1)]\n    assert result == expected_result\n\n", "def test_measure_partial_oneshot():\n    # Test with 2 qubits\n    qubit = IntQubit(3) + IntQubit(2)\n    bits = (0,)\n    result = measure_partial_oneshot(qubit, bits)\n    expected = (IntQubit(3),)\n    assert result == expected\n\n    # Test with 3 qubits\n    qubit = IntQubit(7) + IntQubit(4)\n    bits = (1, 2)\n    result = measure_partial_oneshot(qubit, bits)\n    expected = (IntQubit(7) + IntQubit(4),)\n    assert result == expected\n\n    # Test with 4 qubits\n    qubit = IntQubit(15) + IntQubit(8)\n    bits = (3,)\n    result = measure_partial_oneshot(qubit, bits)\n    expected = (IntQubit(15) + IntQubit(8),)\n    assert result == expected\n", "def test_measure_all():\n    from sympy.physics.quantum.qubit import Qubit, IntQubit\n    from sympy.physics.quantum.gate import X\n    c = X(0)*X(1)*Qubit('00')\n    c = c.expand()\n    result = measure_all(c)\n    assert len(result) == 4\n    expected_result = [(Qubit(IntQubit(0, 2)), 1/4), (Qubit(IntQubit(1, 2)), 1/4),\n                      (Qubit(IntQubit(2, 2)), 1/4), (Qubit(IntQubit(3, 2)), 1/4)]\n    assert result == expected_result\n", "def test_measure_partial_oneshot_one_bit():\n    from sympy.physics.quantum.qubit import Qubit\n    from sympy.physics.quantum.gate import H\n    q = H(0)*Qubit('0')\n    m = qapply(q)\n    result = measure_partial_oneshot(m, (0,))\n    assert result.qubit_values == (1, 0)\n    ", "def test_measure_partial():\n    from sympy import Matrix\n    from sympy.physics.quantum.qubit import Qubit\n    from sympy.physics.quantum.gate import H, X, Y, Z\n    from sympy.physics.quantum.represent import represent\n    from sympy.physics.quantum.qapply import qapply\n    from sympy.physics.quantum.qubit import measure_partial\n\n    # Simple case: measure one qubit\n    q = qapply(H(0)*H(1)*Qubit('00'))\n    result = measure_partial(q, (0,))\n    expected = [(sqrt(2)*Qubit('00')+sqrt(2)*Qubit('10'))/2, (sqrt(2)*Qubit('01')+sqrt(2)*Qubit('11'))/2]\n    assert result == expected\n\n    # Partial measurement on a qubit in a superposition\n    q = qapply(X(0)*Qubit('00'))\n    result = measure_partial(q, (0,))\n    expected = [(Qubit('00')/2, 1/2), (Qubit('10')/2, 1/2)]\n    assert result == expected\n\n    # Partial measurement on two qubits\n    q = qapply(H(0)*H(1)*Qubit('00'))\n    result = measure_partial(q, (0, 1))\n    expected = [(Qubit('00')/2, 1/4), (Qubit('10')/2, 1/4),\n                (Qubit('01')/2, 1/4), (Qubit('11')/2, 1/4)]\n    assert result == expected\n\n    # Partial measurement on a qubit in a superposition, with two qubits to measure\n    q = qapply(X(0)*Qubit('00'))\n    result = measure_partial(q, (0, 1))\n    expected = [(Qubit('00')/2, 1/2), (Qubit('10')/2, 1/2)]\n    assert result == expected\n"], "sample_1116": ["def test_refine_Inverse():\n    # Test orthogonal matrix\n    with assuming(Q.orthogonal(D)):\n        result = refine(D.I)\n        assert result == D.T\n\n    # Test unitary matrix\n    with assuming(Q.unitary(D)):\n        result = refine(D.I)\n        assert result == D.conjugate()\n\n    # Test singular matrix\n    with assuming(Q.singular(D)):\n        with raises(ValueError):\n            refine(D.I)\n\n    # Test non-square matrix\n    with assuming(Q.singular(E)):\n        with raises(NonSquareMatrixError):\n            refine(E.I)\n\n    # Test non-invertible matrix\n    with assuming(Q.orthogonal(E)):\n        with raises(NonInvertibleMatrixError):\n            refine(E.I)\n\n    # Test matrix that is neither orthogonal, unitary nor singular\n    result = refine(D.I)\n    assert result == D\n", "def test_Inverse_refine():\n    from sympy import MatrixSymbol, Q, assuming, refine\n    X = MatrixSymbol('X', 3, 3)\n    with assuming(Q.orthogonal(X)):\n        result = refine(Inverse(X))\n        assert result == X.T\n    with assuming(Q.unitary(X)):\n        result = refine(Inverse(X))\n        assert result == X.conjugate()\n    with assuming(Q.singular(X)):\n        raises(ValueError, lambda: refine(Inverse(X)))\n    with assuming(Q.symmetric_pos_definite(X)):\n        result = refine(Inverse(X))\n        assert result == X.conjugate().T\n    with assuming(Q.symmetric_neg_definite(X)):\n        result = refine(Inverse(X))\n        assert result == -X.conjugate().T\n", "def test_inverse_non_square_matrix():\n    # Test that Inverse raises NonSquareMatrixError for non-square matrix\n    A = MatrixSymbol('A', 2, 3)\n    with raises(NonSquareMatrixError):\n        Inverse(A)\n\n    # Test that Inverse raises TypeError for non-matrix\n    with raises(TypeError):\n        Inverse(1)\n\n    # Test that Inverse refines correctly for orthogonal matrix\n    X = MatrixSymbol('X', 2, 2)\n    with assuming(Q.orthogonal(X)):\n        assert refine(Inverse(X)) == X.T\n\n    # Test that Inverse refines correctly for unitary matrix\n    with assuming(Q.unitary(X)):\n        assert refine(Inverse(X)) == X.conjugate()\n\n    # Test that Inverse refines correctly for singular matrix\n    with assuming(Q.singular(X)):\n        with raises(ValueError):\n            refine(Inverse(X))\n\n    # Test that Inverse refines correctly for non-square matrix that is not singular\n    A = MatrixSymbol('A', 2, 3)\n    with assuming(Q.nonsingular(A)):\n        assert refine(Inverse(A)) == A\n", "def test_refine_Inverse():\n    # Test that the refine method returns the identity matrix when the matrix is orthogonal\n    with assuming(Q.orthogonal(C)):\n        assert refine(Inverse(C)) == C.T\n\n    # Test that the refine method returns the conjugate of the matrix when the matrix is unitary\n    with assuming(Q.unitary(C)):\n        assert refine(Inverse(C)) == C.conjugate()\n\n    # Test that the refine method raises an exception when the matrix is singular\n    with assuming(Q.singular(C)):\n        with raises(NonInvertibleMatrixError):\n            refine(Inverse(C))\n\n    # Test that the refine method returns the original matrix when no assumptions are applied\n    assert refine(Inverse(C)) == Inverse(C)\n", "def test_Inverse_determinant():\n    from sympy import MatrixSymbol, eye\n    from sympy.matrices.expressions.determinant import det\n\n    n = 3\n    A = MatrixSymbol('A', n, n)\n    inv_A = Inverse(A)\n\n    # Test case 1: Inverse of a non-square matrix raises NonSquareMatrixError\n    inv_A_det = det(inv_A)\n    assert inv_A_det.equals(1 / det(A))\n    assert inv_A_det.is_Number\n\n    # Test case 2: Inverse of an invertible matrix with a specific determinant\n    A_det = det(A)\n    assert A_det.equals(2)  # assuming A is an invertible matrix with det(A) = 2\n    inv_A_det = det(inv_A)\n    assert inv_A_det.equals(1 / 2)\n\n    # Test case 3: Inverse of a matrix with an expression as determinant\n    A_det = MatrixSymbol('det_A', 1, 1)\n    inv_A_det = det(inv_A)\n    assert inv_A_det.equals(1 / A_det)\n\n    # Test case 4: Inverse of a singular matrix raises ValueError\n    D = MatrixSymbol('D', n, n)\n    refine(D, {Q.singular: True})\n    inv_D = Inverse(D)\n    assert inv_D.equals(D)  # D is singular, its inverse is not defined\n"], "sample_779": ["    def test_get_params(self):\n        # Test that get_params returns the same thing before and after set_params\n        # with some fuzz\n        est = LinearRegression()\n        params = est.get_params(deep=False)\n        est.set_params(**params)\n        curr_params = est.get_params(deep=False)\n        assert_equal(set(params.keys()), set(curr_params.keys()))\n        for k, v in curr_params.items():\n            assert params[k] is v\n\n        # some fuzz values\n        test_values = [-np.inf, np.inf, None]\n        test_params = deepcopy(params)\n        for param_name in params.keys():\n            default_value = params[param_name]\n            for value in test_values:\n                test_params[param_name] = value\n                try:\n                    est.set_params(**test_params)\n                except (TypeError, ValueError) as e:\n                    e_type = e.__class__.__name__\n                    warnings.warn(\"{0} occurred during set_params of param {1} on \"\n                                  \"{2}. It is recommended to delay parameter \"\n                                  \"validation until fit.\".format(e_type,\n                                                                     param_name,\n                                                                     type(est)))\n                    change_warning_msg = \"Estimator's parameters changed after \" \\\n                                         \"set_params raised {}\".format(e_type)\n                    params_before_exception = curr_params\n                    curr_params = est.get_params(deep=False)\n                    try:\n                        assert_equal(set(params_before_exception.keys()),\n                                    set(curr_params.keys()))\n                        for k, v in curr_params.items():\n                            assert params_before_exception[k] is v\n                    except AssertionError:\n                        warnings.warn(change_warning_msg)\n                else:\n                    curr_params = est.get_params(deep=False)\n                    assert_equal(set(test_params.keys()),\n                                set(curr_params.keys()),\n                                msg=\"get_params result does not match what was passed to set_params\")\n                    for k, v in curr_params.items():\n                        assert test_params[k] is v, msg=\"parameter %s has changed value after set_params\" % k\n                test_params[param_name] = default_value\n", "    def __init__(self, solver=None):\n        self.solver = solver\n", "    def __init__(self, rng=None):\n        self.rng = rng\n", "    def fit(self, X, y=None):\n        if X.shape[0] == 0:\n            raise ValueError(\"empty data\")\n        return self\n", "    def __init__(self):\n        self.wrong_attribute1 = 0\n        self.wrong_attribute2 = 0\n        self.wrong_attribute3 = 0\n        self.wrong_attribute4 = 0\n        self.wrong_attribute5 = 0\n"], "sample_454": ["    def test_exclusion_constraint_creation_with_gist_index_type(self):\n        constraint = ExclusionConstraint(\n            name=\"test_exclusion_constraint\",\n            expressions=[(\"price\", \">\"), (\"quantity\", \">\")],\n            index_type=\"GIST\",\n        )\n        self.assertEqual(constraint.index_type, \"GIST\")\n        self.assertEqual(constraint.expressions, [(\"price\", \">\"), (\"quantity\", \">\")])\n        self.assertIsInstance(constraint.condition, type(None))\n", "    def test_exclusion_constraint_deconstruct(self):\n        constraint = ExclusionConstraint(\n            name=\"test_exclusion\",\n            expressions=[(\"field1\", \"gt\"), (\"field2\", \"lt\")],\n            condition=F(\"field3\") == 1,\n            index_type=\"GIST\",\n            deferrable=Deferrable.DEFERRED,\n            include=[\"field1\", \"field2\"],\n            violation_error_code=\"code\",\n            violation_error_message=\"message\",\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.exclusion\")\n        self.assertEqual(args, [])\n        self.assertEqual(\n            kwargs,\n            {\n                \"name\": \"test_exclusion\",\n                \"expressions\": [(\"field1\", \"gt\"), (\"field2\", \"lt\")],\n                \"condition\": F(\"field3\") == 1,\n                \"index_type\": \"GIST\",\n                \"deferrable\": Deferrable.DEFERRED,\n                \"include\": [\"field1\", \"field2\"],\n                \"violation_error_code\": \"code\",\n                \"violation_error_message\": \"message\",\n            },\n        )\n", "    def test_exclusion_constraint_with_gist_index(self):\n        constraint = ExclusionConstraint(\n            name=\"test_exclusion_constraint\",\n            expressions=[(\"field1\", \"<\"), (\"field2\", \">\")],\n            condition=None,\n        )\n        self.assertEqual(constraint.index_type, \"GIST\")\n", "    def test_exclusion_constraint_deconstruct(self):\n        constraint = ExclusionConstraint(\n            name=\"exclusion_constraint\",\n            expressions=[(\"field1\", \"<>\"), (\"field2\", \"<>\")],\n            condition=Q(field1=1),\n            deferrable=True,\n            index_type=\"gist\",\n            include=[\"field3\"],\n            violation_error_code=1,\n            violation_error_message=\"error message\",\n        )\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, \"django.db.models.constraints.ExclusionConstraint\")\n        self.assertEqual(args, ())\n        self.assertEqual(\n            kwargs,\n            {\n                \"name\": \"exclusion_constraint\",\n                \"expressions\": [(\"field1\", \"<>\"), (\"field2\", \"<>\")],\n                \"condition\": constraint.condition,\n                \"index_type\": \"gist\",\n                \"deferrable\": constraint.deferrable,\n                \"include\": [\"field3\"],\n                \"violation_error_code\": 1,\n                \"violation_error_message\": \"error message\",\n            },\n        )\n", "    def test_exclusion_constraint_create_sql(self):\n        # Create a test model with an exclusion constraint\n        class TestModel(models.Model):\n            name = models.CharField(max_length=255)\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.ExclusionConstraint(\n                        name=\"exclusion_constraint\",\n                        expressions=[\n                            (\"name\", \">\"),\n                            (\"value\", \"<\"),\n                        ],\n                        index_type=\"GIST\",\n                    )\n                ]\n\n        # Test the create SQL generated by the exclusion constraint\n        model = TestModel()\n        with self.subTest(\"with expressions and index type\"):\n            expected_sql = (\n                \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s EXCLUDE USING GIST \"\n                \"(%(expressions)s)%(include)s%(where)s%(deferrable)s\"\n            )\n            self.assertRegex(\n                TestModel._meta.db_table,\n                expected_sql,\n                TestModel._meta.db_table,\n                name=\"exclusion_constraint\",\n                expressions=\"%(expressions)s\",\n                include=\"%(include)s\",\n                where=\"%(where)s\",\n                deferrable=\"%(deferrable)s\",\n            )\n"], "sample_1087": ["def test_f_polys():\n    f_0, f_1, f_2, f_3, f_4, f_5, f_6 = f_polys()\n    assert isinstance(f_0, Poly)\n    assert isinstance(f_1, Poly)\n    assert isinstance(f_2, Poly)\n    assert isinstance(f_3, Poly)\n    assert isinstance(f_4, Poly)\n    assert isinstance(f_5, Poly)\n    assert isinstance(f_6, Poly)\n", "def test_dmp_fateman_poly_F_1():\n    # Test dmp_fateman_poly_F_1 function with different values of n\n    for n in range(1, 10):\n        F, G, H = dmp_fateman_poly_F_1(n)\n        assert isinstance(F, DMP)\n        assert isinstance(G, DMP)\n        assert isinstance(H, DMP)\n\n    # Test dmp_fateman_poly_F_1 function with n=0\n    with raises(ValueError):\n        dmp_fateman_poly_F_1(0)\n", "def test_fateman_polys_properties():\n    # Test Poly properties\n    F, G, H = fateman_poly_F_1(4)\n    assert F.is_Poly and G.is_Poly and H.is_Poly\n    assert F.as_expr() == ((x + 1)*(x + 2)).as_expr()\n    assert G.as_expr() == ((y_0**2 + Add(*[y**2 for y in Y[1:]]))*(-3*y_1*y_0**2 + y_1**2 - 1)).as_expr()\n\n    F, G, H = fateman_poly_F_2(3)\n    assert F.is_Poly and G.is_Poly and H.is_Poly\n    assert F.as_expr() == ((y_0 - Add(*[y for y in Y[1:]] - 2)**2).as_expr()\n    assert G.as_expr() == ((y_0 + Add(*[y for y in Y[1:]] + 2)**2).as_expr()\n\n    F, G, H = fateman_poly_F_3(2)\n    assert F.is_Poly and G.is_Poly and H.is_Poly\n    assert F.as_expr() == ((y_0**(n + 1) + Add(*[y**(n + 1) for y in Y[1:]]) + 1)**2).as_expr()\n    assert G.as_expr() == ((y_0**(n + 1) - Add(*[y**(n + 1) for y in Y[1:]] - 2)**2).as_expr()\n\n    F, G, H = dmp_fateman_poly_F_1(4, ZZ)\n    assert F.is_DMP and G.is_DMP and H.is_DMP\n    assert F.as_expr() == ((x + 1)*(x + 2)).as_expr()\n    assert G.as_expr() == ((y_0**2 + Add(*[y**2 for y in Y[1:]]))*(-3*y_1*y_0**2 + y_1**2 - 1)).as_expr()\n\n    F, G, H = dmp_fateman_poly_F_2(3, ZZ)\n    assert F.is_DMP and G.is_DMP and H.is_DMP\n    assert F.as_expr() == ((y_0 - Add(*[y for", "def test_wang_polys():\n    from sympy.polys.specialpolys import f_polys, w_polys\n\n    # Test if all the functions return polynomials\n    polys = f_polys()\n    for poly in polys:\n        assert isinstance(poly, Poly)\n    polys = w_polys()\n    for poly in polys:\n        assert isinstance(poly, Poly)\n\n    # Test if all the functions work with the correct number of variables\n    R, x, y, z, t = symbols('x,y,z,t')\n    f_0 = f_polys()[0]\n    assert len(f_0.as_dict().keys()) == 3\n    f_1 = f_polys()[1]\n    assert len(f_1.as_dict().keys()) == 3\n    f_2 = f_polys()[2]\n    assert len(f_2.as_dict().keys()) == 3\n    f_3 = f_polys()[3]\n    assert len(f_3.as_dict().keys()) == 4\n    f_4 = f_polys()[4]\n    assert len(f_4.as_dict().keys()) == 4\n    f_5 = f_polys()[5]\n    assert len(f_5.as_dict().keys()) == 3\n    f_6 = f_polys()[6]\n    assert len(f_6.as_dict().keys()) == 4\n    w_1 = w_polys()[0]\n    assert len(w_1.as_dict().keys()) == 3\n    w_2 = w_polys()[1]\n    assert len(w_2.as_dict().keys()) == 2\n", "def test_f_polys():\n    \"\"\"Test the f_polys() function\"\"\"\n    polys = f_polys()\n    for i in range(len(polys)):\n        f, g, h = polys[i]\n        assert isinstance(f, Poly)\n        assert isinstance(g, Poly)\n        assert isinstance(h, Poly)\n"], "sample_243": ["def test_group_by(self):\n    q = Query(model=Author)\n    q.add_ordering('name')\n    q.set_group_by()\n    expected_group_by = ('name', )\n    self.assertEqual(q.group_by, expected_group_by)\n    q.set_group_by(allow_aliases=False)\n    self.assertEqual(q.group_by, ())\n", "    def test_resolve_lookup_value(self):\n        q = Query(model=Author)\n        value = F('name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), F('name'))\n\n        value = 'string'\n        self.assertEqual(q.resolve_lookup_value(value, None, False), 'string')\n\n        value = [1, 2, 3]\n        self.assertEqual(q.resolve_lookup_value(value, None, False), [1, 2, 3])\n\n        value = (1, 2, 3)\n        self.assertEqual(q.resolve_lookup_value(value, None, False), (1, 2, 3))\n\n        value = 123\n        self.assertEqual(q.resolve_lookup_value(value, None, False), 123)\n\n        class TestValue:\n            pass\n\n        value = TestValue()\n        self.assertEqual(q.resolve_lookup_value(value, None, False), TestValue())\n\n        value = Q()\n        self.assertEqual(q.resolve_lookup_value(value, None, False), Q())\n\n        value = Lower('name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), Lower('name'))\n\n        value = Exact(Lower('name'), 'lower_name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), Exact(Lower('name'), 'lower_name'))\n\n        value = GreaterThan(1, 'name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), GreaterThan(1, 'name'))\n\n        value = LessThan(1, 'name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), LessThan(1, 'name'))\n\n        value = IsNull('name')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), IsNull('name'))\n\n        value = Q(name='John', age=30)\n        self.assertEqual(q.resolve_lookup_value(value, None, False), Q(name='John', age=30))\n\n        value = Q(name='John')\n        self.assertEqual(q.resolve_lookup_value(value, None, False), Q(name='John'))\n\n        value = {'name': 'John', 'age': 30}\n        with self.assertRaises(FieldError):\n            q.resolve_lookup_value(value, None, False)\n\n        value = Lower('name')\n        self.assertEqual(q.resolve_lookup_value(value, None, True), Lower('name'))\n\n        value = 'string'\n        self.assertEqual(q.resolve_lookup_value(value, None, True), 'string')\n\n        value = [1, 2, 3", "    def test_query_existence(self):\n        \"\"\"\n        Test existence query.\n        \"\"\"\n        author = Author.objects.get(id=1)\n        query = Query(Author, where=WhereNode)\n        exists = query.exists()\n        self.assertTrue(exists.has_results(using='default'))\n", "def test_join_promotion_with_filtered_relation(self):\n    q = Query(Ranking.objects.all())\n    q.add_filter(Q(rating__in=(1, 2)))\n    q.add_filtered_relation(\n        FilteredRelation(\n            relation_name='author__name',\n            condition=Q(name='John')\n        ),\n        alias='author'\n    )\n    q.clear_ordering(True)\n    q.set_limits(high=2)\n    compiler = q.get_compiler(using='default')\n    query = compiler.execute_sql(OR)\n    self.assertEqual(len(query), 3)\n    self.assertEqual(set(query), {1, 2, 3})\n", "    def test_annotation_and_ordering(self):\n        q = Query(ObjectC.objects.model)\n        q.add_annotation(Lower(Col('pk')), 'lower_pk')\n        q.add_ordering('lower_pk')\n        self.assertEqual(q.sql_with_params(), \"SELECT lower(pk) AS lower_lower_pk FROM test_objectc ORDER BY lower(lower_pk)\")\n        q = Query(ObjectC.objects.model)\n        q.add_annotation(Col('pk'), 'pk')\n        q.add_ordering('-pk')\n        self.assertEqual(q.sql_with_params(), \"SELECT pk AS pk FROM test_objectc ORDER BY -pk\")\n"], "sample_1025": ["def test_print_MatrixBase():\n    A = MatrixSymbol('A', 2, 2)\n    A_val = Matrix([[1, 2], [3, 4]])\n    printer = PythonCodePrinter()\n    assert printer._print_MatrixBase(A) == 'Matrix([[1, 2], [3, 4]])'\n    assert printer._print_MatrixBase(A_val) == 'Matrix([[1, 2], [3, 4]])'\n", "def test_print_MpmathPrinter():\n    # Test printing with MpmathPrinter\n    mp = MpmathPrinter(settings={'user_functions': {'sin': 'sinp'}})\n    assert mp.doprint(sin(x)) == \"mpmath.sinp(x)\"\n    assert mp.doprint(sin(pi)) == \"mpmath.sinp(pi)\"\n    assert mp.doprint(sin(Rational(1, 2))) == \"mpmath.sinp(Rational(1, 2))\"\n", "def test_print_Piecewise_with_default_value():\n    from sympy.printing.pycode import NumPyPrinter\n\n    pw = Piecewise((1, x < 0), (x, True))\n    result = NumPyPrinter().doprint(pw)\n    assert result == 'numpy.select([x < 0, True], [1, x], default=numpy.nan)'\n", "def test_PythonCodePrinter_print_Return():\n    printer = PythonCodePrinter()\n    expr = Return(Rational(5))\n    result = printer.doprint(expr)\n    assert result == 'return 5'\n\n", "def test_print_FuncWithMultipleArguments():\n    from sympy import sin\n    expr = sin(x, y)\n    printer = PythonCodePrinter({'user_functions': {'sin': 'math.sin'}})\n    assert printer.doprint(expr) == \"math.sin(x, y)\"\n"], "sample_976": ["def test_symbols_seq():\n    x = symbols('x', seq=True)\n    assert len(x) == 1\n    assert x[0].name == 'x'\n", "def test_Symbol_new_method():\n    # Test the Symbol constructor\n    s1 = Symbol('x')\n    s2 = Symbol('x')\n\n    assert s1 is s2\n\n    s3 = Symbol('y', commutative=False)\n    s4 = Symbol('y', commutative=False)\n\n    assert s3 is not s4\n\n    # Test the __new_stage2__ method\n    s5 = Symbol.__new_stage2__('z')\n    assert s5.name == 'z'\n    assert s5._assumptions.is_commutative == True\n    assert s5.is_commutative == True\n\n    s6 = Symbol.__new_stage2__('w', commutative=True)\n    assert s6.name == 'w'\n    assert s6._assumptions.is_commutative == True\n    assert s6.is_commutative == True\n\n    s7 = Symbol.__new_stage2__('v', commutative=False)\n    assert s7.name == 'v'\n    assert s7._assumptions.is_commutative == False\n    assert s7.is_commutative == False\n\n    # Test the __xnew__ method\n    s8 = Symbol.__xnew__('u')\n    assert s8.name == 'u'\n    assert s8._assumptions.is_commutative == True\n    assert s8.is_commutative == True\n\n    s9 = Symbol.__xnew__('t', commutative=True)\n    assert s9.name == 't'\n    assert s9._assumptions.is_commutative == True\n    assert s9.is_commutative == True\n\n    s10 = Symbol.__xnew__('s', commutative=False)\n    assert s10.name == 's'\n    assert s10._assumptions.is_commutative == False\n    assert s10.is_commutative == False\n\n    # Test the __xnew_cached_ method\n    s11 = Symbol.__xnew_cached__('r')\n    assert s11.name == 'r'\n    assert s11._assumptions.is_commutative == True\n    assert s11.is_commutative == True\n\n    s12 = Symbol.__xnew_cached__('q', commutative=True)\n    assert s12.name == 'q'\n    assert s12._assumptions.is_commutative == True\n    assert s12.is_commutative == True\n\n    s13 = Symbol.__", "def test_symbols():\n    # Test symbols() function with a single symbol\n    assert symbols('x') == Symbol('x')\n    assert isinstance(symbols('x'), Symbol)\n\n    # Test symbols() function with a comma-separated string of symbols\n    x, y, z = symbols('x, y, z')\n    assert isinstance(x, Symbol)\n    assert isinstance(y, Symbol)\n    assert isinstance(z, Symbol)\n    assert x == Symbol('x')\n    assert y == Symbol('y')\n    assert z == Symbol('z')\n\n    # Test symbols() function with a list of symbols\n    symbols_list = symbols(['x', 'y', 'z'])\n    assert isinstance(symbols_list, list)\n    assert isinstance(symbols_list[0], Symbol)\n    assert isinstance(symbols_list[1], Symbol)\n    assert isinstance(symbols_list[2], Symbol)\n    assert symbols_list[0] == Symbol('x')\n    assert symbols_list[1] == Symbol('y')\n    assert symbols_list[2] == Symbol('z')\n\n    # Test symbols() function with a set of symbols\n    symbols_set = symbols({'x', 'y', 'z'})\n    assert isinstance(symbols_set, set)\n    assert isinstance(symbols_set.pop(), Symbol)\n    assert isinstance(symbols_set.pop(), Symbol)\n    assert isinstance(symbols_set.pop(), Symbol)\n\n    # Test symbols() function with a string that contains a colon\n    symbols_colon = symbols('x:10')\n    assert isinstance(symbols_colon, tuple)\n    assert len(symbols_colon) == 10\n    assert symbols_colon[0] == Symbol('x0')\n    assert symbols_colon[-1] == Symbol('x9')\n\n    # Test symbols() function with a string that contains a colon and a letter\n    symbols_colon_letter = symbols('x:z')\n    assert isinstance(symbols_colon_letter, tuple)\n    assert len(symbols_colon_letter) == 3\n    assert symbols_colon_letter[0] == Symbol('x')\n    assert symbols_colon_letter[1] == Symbol('y')\n    assert symbols_colon_letter[2] == Symbol('z')\n\n    # Test symbols() function with a string that contains a colon and a number\n    symbols_colon_number = symbols('x:5')\n    assert isinstance(symbols_colon_number, tuple)\n    assert len(symbols_colon_number) == 5\n    assert symbols_colon_number[", "def test_symbols_empty_string():\n    with raises(ValueError):\n        symbols('')\n", "def test_symbols_range_parsing():\n    # Test with single range\n    x = symbols('x:5')\n    assert len(x) == 5\n    for i in range(5):\n        assert x[i] == 'x' + str(i)\n\n    # Test with multiple ranges\n    x, y = symbols('x:5, y:3')\n    assert len(x) == 5\n    for i in range(5):\n        assert x[i] == 'x' + str(i)\n    assert len(y) == 3\n    for i in range(3):\n        assert y[i] == 'y' + str(i)\n\n    # Test with letter range\n    x = symbols('x:a:c')\n    assert len(x) == 3\n    assert x[0] == 'x_a'\n    assert x[1] == 'x_b'\n    assert x[2] == 'x_c'\n\n    # Test with empty range\n    x = symbols('x:z:')\n    assert len(x) == 0\n\n    # Test with null range\n    x = symbols('x:')\n    assert len(x) == 0\n\n    # Test with multiple ranges with null ranges\n    x, y = symbols('x:, y:z:')\n    assert len(x) == 0\n    assert len(y) == 0\n\n    # Test with escaped characters\n    x = symbols('x\\:y')\n    assert len(x) == 2\n    assert x[0] == 'x:y'\n    assert x[1] == 'y'\n\n    # Test with parentheses\n    x = symbols('x((a:b))')\n    assert len(x) == 2\n    assert x[0] == 'x(a)'\n    assert x[1] == 'x(b)'\n\n    # Test with single range with parentheses\n    x = symbols('x:(a:b)')\n    assert len(x) == 1\n    assert x[0] == 'x(a)'\n\n    # Test with multiple ranges with parentheses\n    x, y = symbols('x:(a:b), y:(c:d)')\n    assert len(x) == 1\n    assert x[0] == 'x(a)'\n    assert len(y) == 1\n    assert y[0] == 'y(c)'\n"], "sample_907": ["compilation error", "def test_decl_specs_simple() -> None:\n    # simple decl-specs\n    check('simple', 'const auto i = 42;',\n          {1: 'Si'},\n          'const auto i = 42;')\n\n    # simple decl-specs with const\n    check('simple', 'const auto i = 42;',\n          {1: 'Si'},\n          'const auto i = 42;')\n\n    # simple decl-specs with volatile\n    check('simple', 'volatile auto i = 42;',\n          {1: 'Vi'},\n          'volatile auto i = 42;')\n\n    # simple decl-specs with const and volatile\n    check('simple', 'const volatile auto i = 42;',\n          {1: 'Ci'},\n          'const volatile auto i = 42;')\n\n    # simple decl-specs with storage-class-specifier\n    check('simple', 'static auto i = 42;',\n          {1: 'Si'},\n          'static auto i = 42;')\n\n    # simple decl-specs with storage-class-specifier and const\n    check('simple', 'static const auto i = 42;',\n          {1: 'Sc'},\n          'static const auto i = 42;')\n\n    # simple decl-specs with storage-class-specifier and volatile\n    check('simple', 'static volatile auto i = 42;',\n          {1: 'Sv'},\n          'static volatile auto i = 42;')\n\n    # simple decl-specs with storage-class-specifier and const and volatile\n    check('simple', 'static const volatile auto i = 42;',\n          {1: 'Ss'},\n          'static const volatile auto i = 42;')\n\n    # simple decl-specs with function-specifier\n    check('simple', 'auto i() = 42;',\n          {1: 'Fi'},\n          'auto i() = 42;')\n\n    # simple decl-specs with function-specifier and const\n    check('simple', 'const auto i() = 42;',\n          {1: 'Ci'},\n          'const auto i() = 42;')\n\n    # simple decl-specs with function-specifier and virtual\n    check('simple', 'virtual auto i() = 42;',\n          {1: 'Vi'},\n          'virtual auto i() = 42;')\n\n    # simple decl-specs with function-specifier and explicit\n    check('simple', 'explicit auto i() = ", "compilation error", "compilation error", "def test_no_old_id_error():\n    ast = parse('member', 'int x = 5;')\n    with pytest.raises(NoOldIdError):\n        ast.get_id(1)\n"], "sample_873": ["def test_get_feature_names_out_input_features_array_like():\n    \"\"\"Test get_feature_names_out when input_features is array-like.\"\"\"\n    selector = StepSelector(step=2)\n    selector.fit(X)\n\n    # Test when feature_names_in_ is defined\n    selector.feature_names_in_ = feature_names\n    input_features = np.array(feature_names)\n    expected_output = np.array(feature_names_t)\n    assert_array_equal(selector.get_feature_names_out(input_features), expected_output)\n\n    # Test when feature_names_in_ is not defined\n    selector.feature_names_in_ = None\n    input_features = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\"])\n    expected_output = np.array([\"a\", \"c\", \"e\", \"g\", \"i\"])\n    assert_array_equal(selector.get_feature_names_out(input_features), expected_output)\n\n    # Test when input_features has different length than feature_names_in_\n    selector.feature_names_in_ = feature_names\n    input_features = np.array([\"a\", \"b\", \"c\", \"d\"])\n    with pytest.raises(ValueError):\n        selector.get_feature_names_out(input_features)\n", "def test_get_support_mask():\n    selector = StepSelector(step=4)\n    assert_array_equal(selector._get_support_mask(), np.array([True, False, True, False, True, False, True, False, True, False]))\n", "def test_get_support_indices():\n    \"\"\"Test that get_support returns an integer array when indices=True\"\"\"\n    selector = StepSelector(step=2)\n    selector.fit(X)\n\n    mask = selector._get_support_mask()\n    expected_support = np.array([True, False, True, False, True, False, True, False, True, False])\n\n    assert_array_equal(selector.get_support(indices=True), np.where(expected_support)[0])\n    assert_array_equal(selector.get_support(indices=True), support_inds)\n\n", "def test_inverse_transform_sparse_matrix():\n    selector = StepSelector(step=2)\n    selector.fit(X)\n    Xt = selector.transform(X)\n    X_inv = selector.inverse_transform(Xt)\n    assert_array_equal(X_inv, Xinv)\n", "def test_get_feature_names_out_empty_feature_names():\n    \"\"\"Test that `get_feature_names_out` returns an empty list when no feature names are specified and there are no features.\"\"\"\n    selector = StepSelector()\n    assert_array_equal(selector.get_feature_names_out(), [\"x0\", \"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\", \"x7\", \"x8\", \"x9\"])\n    selector.fit(X)\n    assert_array_equal(selector.get_feature_names_out(), feature_names)\n    selector = StepSelector(step=0)\n    assert_array_equal(selector.get_feature_names_out(), [])\n    assert_array_equal(selector.get_feature_names_out([\"x0\", \"x1\", \"x2\"]), [\"x0\"])\n    assert_array_equal(selector.get_feature_names_out([\"x3\", \"x4\", \"x5\"]), [])\n"]}, "raw_scored_generations": {"sample_0": [false, false, false, false, false], "sample_3": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_2": [true, false, false, false, false], "sample_4": [false, false, true, true, true], "sample_5": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_7": [false, true, false, false, false], "sample_8": [false, false, false, false, true], "sample_9": [true, true, true, false, true], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [false, false, false, false, false], "sample_13": [false, false, true, false, false], "sample_14": [true, false, true, false, true], "sample_15": [true, true, true, true, true], "sample_16": [false, false, true, false, false], "sample_17": [true, false, false, false, false], "sample_18": [false, false, false, false, false], "sample_19": [false, false, false, false, false], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, true], "sample_24": [true, false, false, true, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, false, true, false, false], "sample_29": [false, false, false, false, false], "sample_30": [false, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [true, true, true, false, true], "sample_33": [true, false, false, false, true], "sample_34": [true, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [false, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, true, false, false, false], "sample_41": [false, false, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, false, false, false, false], "sample_44": [false, false, false, false, false], "sample_45": [true, false, false, false, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, false, false, false], "sample_48": [true, false, false, false, false], "sample_49": [false, false, false, false, false], "sample_50": [false, false, false, false, false], "sample_51": [false, false, true, false, false], "sample_52": [false, false, false, true, false], "sample_54": [false, false, false, false, false], "sample_53": [false, false, false, false, false], "sample_55": [false, false, false, false, false], "sample_58": [false, false, false, false, false], "sample_56": [false, false, false, false, false], "sample_57": [true, false, true, false, false], "sample_59": [false, false, false, false, false], "sample_60": [false, false, false, false, false], "sample_61": [false, false, false, false, false], "sample_62": [false, false, false, false, false], "sample_63": [true, false, false, false, false], "sample_64": [true, false, false, false, false], "sample_65": [false, true, false, false, false], "sample_67": [false, false, false, false, true], "sample_66": [false, false, false, false, false], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, false, false, false, false], "sample_72": [false, false, false, false, false], "sample_73": [true, true, true, true, true], "sample_75": [false, false, false, false, false], "sample_74": [true, false, false, false, false], "sample_76": [false, true, true, false, false], "sample_77": [true, false, false, false, false], "sample_78": [false, false, true, false, false], "sample_79": [true, true, true, true, false], "sample_80": [false, false, true, false, false], "sample_82": [false, false, false, false, false], "sample_81": [false, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, false], "sample_84": [true, true, true, true, false], "sample_86": [false, false, false, false, false], "sample_88": [false, false, false, false, false], "sample_87": [true, false, false, true, false], "sample_89": [false, false, false, false, true], "sample_90": [false, false, false, false, false], "sample_91": [false, false, false, false, false], "sample_92": [false, false, false, false, false], "sample_93": [false, false, false, false, false], "sample_94": [true, false, false, false, true], "sample_95": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_97": [false, false, false, false, false], "sample_100": [false, false, false, true, false], "sample_102": [false, false, false, false, false], "sample_101": [false, false, true, false, false], "sample_103": [false, false, false, false, true], "sample_104": [true, true, true, true, true], "sample_107": [false, false, false, false, false], "sample_106": [true, false, false, false, false], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, true, false, false, false], "sample_111": [false, false, false, false, false], "sample_110": [false, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, true], "sample_115": [false, false, true, false, false], "sample_116": [false, false, false, false, false], "sample_117": [true, false, false, false, true], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [false, false, false, false, false], "sample_122": [true, false, true, false, false], "sample_123": [true, true, true, false, true], "sample_124": [false, false, true, true, false], "sample_125": [false, false, false, false, true], "sample_126": [true, true, true, true, false], "sample_127": [false, false, false, false, false], "sample_128": [true, false, false, false, false], "sample_129": [false, false, false, false, false], "sample_130": [false, false, false, false, false], "sample_131": [true, false, false, false, false], "sample_132": [false, false, false, false, true], "sample_133": [false, false, false, false, false], "sample_135": [false, false, false, false, false], "sample_134": [false, false, false, false, false], "sample_136": [false, false, false, false, false], "sample_139": [false, false, false, false, false], "sample_137": [false, false, true, true, false], "sample_138": [true, true, false, true, true], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_142": [false, false, false, false, false], "sample_143": [true, false, false, false, false], "sample_144": [false, false, false, false, false], "sample_145": [false, false, false, false, false], "sample_146": [true, false, false, false, false], "sample_147": [false, false, true, false, false], "sample_148": [false, false, false, false, false], "sample_151": [false, false, false, false, true], "sample_149": [false, false, false, false, false], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, true, false, true, false], "sample_155": [false, false, false, false, false], "sample_156": [false, false, false, false, false], "sample_157": [false, false, false, false, true], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false], "sample_160": [false, false, false, false, false], "sample_161": [false, false, false, false, false], "sample_162": [false, true, true, true, true], "sample_163": [false, false, false, false, false], "sample_164": [false, false, true, false, false], "sample_165": [false, false, false, false, false], "sample_166": [false, false, false, false, true], "sample_167": [true, false, true, false, false], "sample_168": [false, false, true, true, false], "sample_171": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_170": [true, false, false, false, false], "sample_172": [false, false, false, false, false], "sample_173": [false, true, true, true, false], "sample_174": [false, false, false, false, true], "sample_175": [false, false, false, false, false], "sample_176": [false, true, false, false, false], "sample_177": [false, false, false, false, false], "sample_178": [false, false, false, false, false], "sample_180": [false, false, true, false, false], "sample_179": [false, false, false, false, false], "sample_182": [false, false, false, false, false], "sample_181": [true, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [false, false, false, false, false], "sample_185": [false, false, false, false, false], "sample_186": [false, false, false, false, false], "sample_187": [true, false, false, false, false], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, false], "sample_190": [false, false, false, false, false], "sample_191": [false, false, false, false, false], "sample_192": [false, true, false, false, false], "sample_193": [false, false, false, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, false, false, false, false], "sample_196": [true, false, true, false, true], "sample_198": [false, false, false, false, false], "sample_197": [true, true, true, true, false], "sample_199": [false, false, false, false, false], "sample_201": [false, false, false, false, false], "sample_200": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, false, false, false, false], "sample_204": [false, false, false, false, false], "sample_205": [true, false, false, true, false], "sample_206": [false, false, false, false, false], "sample_207": [false, false, false, false, false], "sample_208": [false, false, false, false, false], "sample_209": [false, false, false, false, false], "sample_210": [false, true, false, false, false], "sample_211": [false, false, false, false, true], "sample_213": [false, false, true, false, true], "sample_212": [false, false, true, false, false], "sample_214": [false, true, false, false, false], "sample_215": [false, false, false, false, false], "sample_216": [false, false, true, true, false], "sample_217": [false, false, false, false, false], "sample_218": [false, false, false, false, false], "sample_219": [false, false, false, false, false], "sample_220": [false, false, false, false, false], "sample_221": [false, false, false, false, false], "sample_222": [false, false, true, false, false], "sample_223": [false, false, false, false, false], "sample_224": [false, false, false, false, false], "sample_225": [false, false, false, false, false], "sample_226": [true, false, false, false, false], "sample_227": [false, false, false, false, false], "sample_228": [false, false, false, true, false], "sample_229": [true, true, false, false, false], "sample_230": [false, false, false, false, false], "sample_231": [false, false, false, true, false], "sample_232": [false, false, false, false, false], "sample_233": [true, true, false, false, true], "sample_234": [true, false, false, true, false], "sample_235": [false, false, false, false, false], "sample_236": [false, false, false, false, false], "sample_237": [false, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [true, false, false, false, false], "sample_240": [true, false, false, false, true], "sample_241": [false, false, false, false, false], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, false, false], "sample_245": [true, true, true, false, false], "sample_246": [true, true, true, true, true], "sample_247": [false, false, false, false, false], "sample_248": [true, false, false, false, false], "sample_249": [false, false, false, false, false], "sample_250": [false, false, false, false, false], "sample_251": [false, false, false, false, false], "sample_252": [false, false, false, false, false], "sample_253": [false, false, false, false, false], "sample_254": [false, false, false, false, false], "sample_256": [false, false, true, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, false], "sample_258": [false, false, true, false, true], "sample_259": [false, false, false, false, false], "sample_260": [false, false, false, false, false], "sample_261": [true, true, true, true, false], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, true, false], "sample_264": [false, false, false, false, false], "sample_265": [false, false, false, true, true], "sample_266": [false, true, false, false, false], "sample_267": [false, false, false, false, false], "sample_268": [true, false, false, false, false], "sample_269": [false, false, false, false, false], "sample_270": [true, false, false, false, false], "sample_271": [false, true, false, false, false], "sample_272": [false, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, false, false], "sample_275": [true, true, true, true, true], "sample_276": [false, false, false, false, false], "sample_277": [false, true, false, false, false], "sample_278": [false, false, false, false, false], "sample_279": [false, false, false, false, false], "sample_280": [false, false, false, true, false], "sample_281": [true, false, false, true, false], "sample_282": [false, false, false, false, false], "sample_283": [false, false, false, false, false], "sample_284": [true, true, true, false, true], "sample_285": [false, false, true, true, false], "sample_286": [false, false, false, false, false], "sample_287": [false, false, false, false, false], "sample_288": [false, false, false, false, false], "sample_289": [true, true, true, true, true], "sample_290": [true, false, false, true, false], "sample_291": [false, true, false, false, false], "sample_292": [true, false, true, true, true], "sample_293": [false, false, true, false, false], "sample_294": [false, true, true, true, true], "sample_295": [false, true, false, false, false], "sample_296": [false, false, false, false, false], "sample_297": [false, true, false, false, false], "sample_298": [true, false, false, false, false], "sample_299": [false, true, false, false, false], "sample_300": [false, false, false, false, false], "sample_301": [false, false, false, false, false], "sample_302": [false, false, false, false, false], "sample_303": [false, false, false, false, true], "sample_304": [false, false, false, false, false], "sample_305": [false, false, false, false, false], "sample_306": [false, false, false, false, false], "sample_307": [false, false, false, false, false], "sample_308": [false, false, false, false, false], "sample_309": [false, true, true, false, false], "sample_310": [false, false, false, false, false], "sample_312": [false, true, true, false, true], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [false, false, false, false, false], "sample_315": [false, false, false, false, false], "sample_316": [false, false, false, false, false], "sample_317": [false, false, false, false, false], "sample_318": [true, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [false, false, false, false, false], "sample_321": [true, true, true, true, true], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [true, true, false, false, true], "sample_325": [false, false, false, false, false], "sample_326": [false, true, false, true, true], "sample_327": [false, false, false, false, true], "sample_328": [true, false, false, false, false], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [true, false, true, true, false], "sample_332": [false, false, false, false, false], "sample_333": [false, false, false, true, false], "sample_334": [false, false, true, true, true], "sample_335": [false, false, true, false, false], "sample_336": [false, false, false, false, false], "sample_337": [false, false, false, false, false], "sample_338": [false, true, true, false, true], "sample_339": [false, false, false, false, false], "sample_340": [false, false, false, false, false], "sample_341": [true, true, false, true, false], "sample_342": [false, true, false, true, false], "sample_343": [false, false, false, false, true], "sample_344": [false, false, false, false, false], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [false, false, true, true, false], "sample_348": [false, false, false, false, false], "sample_349": [false, false, false, false, false], "sample_350": [false, false, false, false, false], "sample_351": [false, false, false, true, false], "sample_352": [true, false, false, false, false], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, true, false], "sample_355": [false, true, false, false, false], "sample_356": [false, false, false, false, false], "sample_357": [false, false, true, false, false], "sample_358": [false, true, false, false, false], "sample_359": [false, false, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [true, false, false, false, true], "sample_362": [false, false, false, false, false], "sample_363": [false, false, false, false, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [true, false, true, true, false], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_370": [true, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, false, false, false], "sample_374": [false, false, false, false, true], "sample_375": [false, false, false, false, false], "sample_376": [false, false, false, false, false], "sample_377": [false, false, true, false, false], "sample_378": [false, false, false, false, false], "sample_379": [false, false, true, false, true], "sample_380": [false, false, false, false, false], "sample_381": [false, false, false, false, false], "sample_382": [false, false, false, false, false], "sample_383": [false, false, false, false, false], "sample_384": [false, false, false, false, false], "sample_385": [true, false, false, false, false], "sample_386": [false, false, false, true, false], "sample_387": [false, false, false, false, false], "sample_388": [false, false, false, false, false], "sample_389": [false, false, false, false, false], "sample_390": [false, false, true, true, false], "sample_391": [false, false, false, false, false], "sample_392": [false, false, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [false, false, false, false, false], "sample_397": [true, false, false, false, false], "sample_398": [false, false, true, false, true], "sample_399": [false, false, false, false, false], "sample_400": [true, true, false, false, false], "sample_401": [false, false, false, false, false], "sample_402": [false, false, false, false, false], "sample_403": [false, false, false, false, false], "sample_404": [true, false, true, true, true], "sample_405": [false, false, false, false, false], "sample_406": [false, false, false, false, false], "sample_407": [false, false, false, false, false], "sample_408": [false, false, false, false, false], "sample_409": [false, false, false, false, false], "sample_410": [false, false, true, false, true], "sample_411": [false, false, true, false, false], "sample_412": [false, false, false, true, false], "sample_413": [false, false, false, false, false], "sample_414": [false, false, false, true, false], "sample_415": [false, false, false, false, false], "sample_416": [true, false, false, false, false], "sample_417": [false, false, false, false, true], "sample_418": [false, false, false, false, false], "sample_419": [false, true, false, false, false], "sample_420": [false, false, false, true, false], "sample_421": [false, false, false, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, false, false, false], "sample_424": [false, false, false, false, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, false, true], "sample_427": [true, false, false, false, true], "sample_428": [false, false, false, false, false], "sample_429": [false, false, false, false, false], "sample_430": [false, false, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [false, false, false, false, false], "sample_433": [false, false, true, false, false], "sample_434": [false, false, false, false, false], "sample_435": [true, false, false, false, false], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, true, false], "sample_438": [false, false, false, false, false], "sample_439": [false, true, true, false, false], "sample_440": [false, false, false, false, false], "sample_441": [false, true, true, false, true], "sample_442": [false, false, false, false, false], "sample_443": [false, false, false, false, false], "sample_444": [true, true, true, true, true], "sample_445": [false, true, true, true, true], "sample_446": [false, false, false, false, false], "sample_447": [true, false, false, false, false], "sample_448": [false, false, false, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, true, true, true, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [false, false, false, true, false], "sample_457": [false, false, false, false, false], "sample_458": [false, false, false, false, false], "sample_459": [false, false, true, false, false], "sample_460": [false, true, false, false, false], "sample_461": [false, false, false, false, false], "sample_462": [false, false, false, false, false], "sample_463": [false, false, false, false, false], "sample_464": [true, false, false, false, false], "sample_465": [true, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [true, false, false, false, false], "sample_469": [false, false, false, false, false], "sample_468": [false, false, false, false, false], "sample_470": [false, false, false, false, true], "sample_471": [false, false, false, false, false], "sample_472": [false, false, false, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [true, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [false, false, false, false, false], "sample_478": [false, false, true, true, false], "sample_479": [false, false, false, false, false], "sample_480": [false, false, false, true, false], "sample_481": [false, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, false, false, false], "sample_484": [false, false, false, false, false], "sample_485": [true, true, false, false, false], "sample_486": [false, true, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [false, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, false, false, false, false], "sample_491": [false, false, false, false, false], "sample_492": [false, false, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [false, false, false, false, true], "sample_496": [false, false, false, false, false], "sample_497": [false, false, false, false, false], "sample_498": [false, false, false, false, false], "sample_499": [false, false, false, false, false], "sample_500": [false, false, false, false, false], "sample_501": [false, false, false, false, false], "sample_502": [false, false, false, false, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, false], "sample_506": [false, true, false, false, false], "sample_507": [false, false, false, false, false], "sample_508": [false, false, false, false, false], "sample_509": [false, true, false, false, false], "sample_510": [true, false, false, false, false], "sample_511": [false, false, false, false, true], "sample_512": [false, false, false, false, false], "sample_513": [false, false, false, false, true], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [false, false, false, false, false], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, false, false, false, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, false], "sample_526": [false, false, true, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, false, false, false, false], "sample_529": [false, false, false, false, false], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [false, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [true, false, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [false, false, false, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [true, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, true], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [true, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, false, false, false], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, false, false], "sample_572": [false, false, true, false, false], "sample_573": [false, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [true, false, true, true, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [false, false, false, false, false], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [true, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [false, false, false, false, false], "sample_594": [false, false, false, false, false], "sample_595": [false, true, false, false, false], "sample_596": [false, false, false, false, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, false, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, false, false, false], "sample_601": [false, false, false, false, false], "sample_602": [false, false, false, false, false], "sample_603": [false, false, false, false, false], "sample_604": [false, false, false, false, false], "sample_605": [false, false, false, false, false], "sample_606": [false, false, false, false, false], "sample_607": [false, false, false, false, false], "sample_608": [false, false, false, false, false], "sample_609": [false, false, false, false, false], "sample_610": [true, true, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, false, false, false, false], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, true, false], "sample_619": [false, false, false, false, false], "sample_620": [false, false, false, true, false], "sample_621": [false, true, false, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, true], "sample_625": [false, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [true, false, false, false, false], "sample_628": [false, true, false, false, false], "sample_629": [false, false, false, true, false], "sample_630": [false, false, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, false, false], "sample_639": [false, false, false, false, false], "sample_640": [false, true, false, true, false], "sample_641": [false, false, false, false, false], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [false, false, false, false, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, true], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, true, false], "sample_656": [false, false, false, false, true], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, false], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, false, false, false, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, false, false], "sample_670": [true, false, false, false, true], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [true, true, true, true, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [true, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, true, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, false, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [true, true, false, false, false], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [false, true, true, false, false], "sample_704": [false, false, false, false, false], "sample_705": [false, false, false, false, false], "sample_706": [true, false, false, true, true], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, true, false, false], "sample_714": [false, true, false, false, false], "sample_715": [false, false, false, false, false], "sample_716": [false, false, false, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, true, false, false], "sample_720": [true, false, false, true, false], "sample_721": [false, false, false, false, false], "sample_722": [false, false, false, false, false], "sample_723": [true, false, false, true, false], "sample_724": [false, false, false, false, false], "sample_725": [false, false, true, false, false], "sample_726": [false, true, false, false, false], "sample_727": [false, true, false, false, false], "sample_728": [false, false, false, true, false], "sample_729": [false, false, false, false, false], "sample_730": [false, false, false, false, false], "sample_731": [false, true, false, false, false], "sample_732": [false, false, false, false, false], "sample_733": [true, false, false, false, false], "sample_734": [false, false, false, true, false], "sample_735": [false, false, false, false, false], "sample_736": [false, false, false, false, false], "sample_737": [true, false, true, false, false], "sample_738": [false, true, false, false, false], "sample_739": [false, false, false, false, false], "sample_740": [false, false, false, false, false], "sample_741": [false, false, true, false, false], "sample_742": [false, false, false, false, false], "sample_743": [false, false, false, false, false], "sample_744": [false, false, false, false, false], "sample_745": [false, false, false, false, false], "sample_746": [false, false, false, false, false], "sample_747": [false, false, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, false, false], "sample_751": [false, false, false, false, false], "sample_752": [true, false, false, false, false], "sample_753": [false, false, false, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, true, false], "sample_756": [true, false, false, false, false], "sample_757": [true, false, false, false, false], "sample_758": [true, false, false, false, false], "sample_759": [false, false, false, false, true], "sample_760": [true, false, false, false, false], "sample_761": [false, false, false, false, false], "sample_762": [true, false, false, false, false], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [false, false, false, false, false], "sample_766": [false, false, false, false, false], "sample_767": [false, false, false, false, false], "sample_768": [true, false, false, false, false], "sample_769": [false, false, false, false, false], "sample_770": [true, true, false, false, false], "sample_771": [true, false, false, false, false], "sample_772": [false, false, false, false, false], "sample_773": [false, false, false, false, false], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, true, false, false], "sample_777": [false, false, false, false, false], "sample_778": [false, false, false, false, false], "sample_779": [false, false, false, true, false], "sample_780": [false, false, false, false, false], "sample_781": [false, false, false, false, false], "sample_782": [false, false, false, false, false], "sample_783": [false, false, false, false, false], "sample_784": [false, false, false, false, false], "sample_785": [false, false, false, false, false], "sample_786": [false, false, false, false, false], "sample_787": [false, false, false, true, false], "sample_788": [false, false, false, false, false], "sample_789": [false, true, true, false, false], "sample_790": [false, false, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [false, false, false, false, false], "sample_793": [false, false, false, false, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [true, false, true, true, false], "sample_797": [false, false, false, false, false], "sample_798": [false, false, false, false, false], "sample_799": [false, false, false, false, false], "sample_800": [false, false, false, false, true], "sample_801": [false, false, false, true, false], "sample_802": [false, false, true, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, true, false, false, false], "sample_805": [false, false, false, false, true], "sample_806": [false, false, false, false, false], "sample_807": [false, true, false, false, false], "sample_808": [false, true, true, false, true], "sample_809": [false, false, false, false, false], "sample_810": [false, false, false, false, false], "sample_811": [true, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, false, false, false, false], "sample_814": [false, false, false, false, false], "sample_815": [false, false, false, false, true], "sample_816": [false, true, false, false, false], "sample_817": [true, false, false, true, true], "sample_818": [false, false, false, false, false], "sample_819": [false, false, false, false, false], "sample_820": [false, true, false, true, false], "sample_821": [false, false, false, false, false], "sample_822": [false, false, false, false, false], "sample_823": [false, false, false, false, false], "sample_824": [false, false, false, false, false], "sample_825": [false, false, false, false, false], "sample_826": [false, true, false, false, false], "sample_827": [true, false, false, false, false], "sample_828": [false, false, false, false, true], "sample_829": [false, false, true, false, false], "sample_830": [false, false, false, false, false], "sample_831": [false, false, false, false, false], "sample_832": [false, true, false, false, false], "sample_833": [false, false, false, false, false], "sample_834": [false, true, false, true, false], "sample_835": [false, false, false, true, false], "sample_836": [false, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, true, false, false], "sample_839": [false, false, false, false, false], "sample_840": [false, false, false, false, false], "sample_841": [false, false, false, false, false], "sample_842": [false, false, false, false, false], "sample_843": [false, false, false, false, false], "sample_844": [false, false, false, false, false], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [false, false, false, false, false], "sample_848": [false, false, false, false, false], "sample_849": [false, false, false, false, false], "sample_850": [false, false, false, false, false], "sample_851": [false, false, false, false, false], "sample_852": [false, false, false, false, false], "sample_853": [false, false, false, false, false], "sample_854": [false, true, false, true, false], "sample_855": [false, false, false, false, false], "sample_856": [false, false, false, false, false], "sample_857": [true, false, true, false, false], "sample_858": [false, false, true, true, false], "sample_859": [false, false, true, false, false], "sample_860": [false, false, false, false, false], "sample_861": [false, false, false, false, false], "sample_862": [false, false, true, false, false], "sample_863": [false, false, false, true, false], "sample_864": [false, false, false, true, true], "sample_865": [false, false, false, false, false], "sample_866": [true, false, false, false, false], "sample_867": [false, false, false, false, false], "sample_868": [true, false, true, false, false], "sample_869": [true, false, false, false, false], "sample_870": [false, false, false, false, false], "sample_871": [false, false, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [true, false, true, true, false], "sample_874": [false, false, false, false, false], "sample_875": [false, true, false, false, false], "sample_876": [true, false, false, false, false], "sample_877": [true, false, true, true, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, true, false, false], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, true, false], "sample_883": [false, false, true, false, false], "sample_884": [false, false, false, false, false], "sample_885": [false, false, false, false, false], "sample_886": [false, true, false, false, false], "sample_887": [false, false, false, false, false], "sample_888": [false, false, false, true, true], "sample_889": [false, false, false, false, false], "sample_890": [false, false, false, true, false], "sample_891": [false, false, false, false, false], "sample_892": [false, false, false, false, false], "sample_893": [false, true, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, true, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, false, false, false], "sample_903": [false, false, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [false, false, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, false, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, false, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, true, false], "sample_937": [false, false, false, false, false], "sample_938": [false, false, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, false, false], "sample_941": [false, false, false, false, false], "sample_942": [false, false, false, false, false], "sample_943": [false, false, false, false, false], "sample_944": [true, false, false, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, true], "sample_952": [false, false, false, false, false], "sample_953": [false, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, true, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [false, false, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [true, false, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [true, true, true, true, true], "sample_968": [false, false, false, false, false], "sample_969": [false, false, false, false, false], "sample_970": [false, false, false, false, false], "sample_971": [false, false, false, false, false], "sample_972": [false, false, false, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, false, false, false, false], "sample_975": [true, true, true, true, true], "sample_976": [false, false, false, true, false], "sample_977": [false, false, false, false, false], "sample_978": [false, false, false, false, true], "sample_979": [false, false, true, false, false], "sample_980": [false, true, false, false, false], "sample_981": [false, true, false, false, false], "sample_982": [false, false, false, false, true], "sample_983": [false, false, true, false, false], "sample_984": [false, false, false, false, true], "sample_985": [true, false, false, false, false], "sample_986": [false, false, false, false, false], "sample_987": [false, false, false, true, false], "sample_988": [false, false, false, false, true], "sample_989": [false, false, false, false, false], "sample_990": [false, false, false, false, false], "sample_991": [false, false, false, false, false], "sample_992": [false, false, false, false, false], "sample_993": [false, false, false, false, false], "sample_994": [false, false, false, false, false], "sample_995": [false, false, true, false, false], "sample_996": [false, false, false, false, false], "sample_997": [false, false, false, true, false], "sample_998": [false, false, false, false, false], "sample_999": [false, false, false, false, false], "sample_1000": [true, false, false, false, false], "sample_1001": [false, false, false, false, false], "sample_1002": [false, false, false, false, false], "sample_1003": [false, false, true, false, false], "sample_1004": [false, true, false, false, false], "sample_1005": [false, false, false, false, false], "sample_1006": [false, true, true, false, false], "sample_1007": [false, false, true, false, false], "sample_1008": [true, false, false, false, false], "sample_1009": [false, true, false, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, false, false, false, false], "sample_1012": [false, false, false, false, false], "sample_1013": [false, false, true, false, false], "sample_1014": [false, false, false, false, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, false, true, false, false], "sample_1017": [false, false, false, false, false], "sample_1018": [false, false, false, false, false], "sample_1019": [false, false, false, false, false], "sample_1020": [false, true, false, false, false], "sample_1021": [false, false, false, true, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [false, false, false, false, false], "sample_1025": [false, false, false, false, false], "sample_1026": [false, false, true, false, true], "sample_1027": [false, false, true, false, false], "sample_1028": [false, false, false, true, true], "sample_1029": [false, true, false, false, false], "sample_1030": [false, false, false, false, true], "sample_1031": [false, false, false, false, false], "sample_1032": [true, false, false, false, false], "sample_1033": [false, false, false, true, false], "sample_1034": [false, false, false, false, false], "sample_1035": [false, false, false, false, false], "sample_1036": [false, false, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [false, true, true, false, false], "sample_1039": [false, false, false, false, true], "sample_1040": [false, false, false, false, false], "sample_1041": [true, true, false, false, true], "sample_1042": [false, false, false, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [true, false, true, false, true], "sample_1046": [false, false, false, false, false], "sample_1047": [false, false, false, false, false], "sample_1048": [false, true, true, false, false], "sample_1049": [false, true, false, false, false], "sample_1050": [false, false, false, false, true], "sample_1051": [true, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [true, true, false, true, false], "sample_1054": [false, false, false, false, false], "sample_1055": [false, false, false, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, false, false], "sample_1060": [false, false, false, false, false], "sample_1061": [false, false, false, false, false], "sample_1062": [false, false, false, false, false], "sample_1063": [false, false, false, true, true], "sample_1064": [true, true, false, false, false], "sample_1065": [false, true, false, true, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, true, false, false, false], "sample_1068": [false, false, false, false, false], "sample_1069": [false, false, false, false, false], "sample_1070": [false, false, false, false, false], "sample_1071": [false, false, false, false, false], "sample_1072": [false, false, false, false, false], "sample_1073": [false, false, false, false, false], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [false, true, false, false, false], "sample_1078": [false, false, false, false, false], "sample_1079": [true, false, true, false, false], "sample_1080": [true, true, false, true, false], "sample_1081": [true, false, true, false, true], "sample_1082": [false, false, false, false, false], "sample_1083": [false, false, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [false, false, false, false, true], "sample_1086": [false, false, false, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, false, false, false], "sample_1089": [false, false, false, false, false], "sample_1090": [false, false, false, false, false], "sample_1091": [false, false, false, false, false], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [false, false, false, false, false], "sample_1095": [false, false, false, false, false], "sample_1096": [false, false, true, false, false], "sample_1097": [false, false, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, false, true, false, false], "sample_1101": [true, false, false, false, false], "sample_1102": [false, true, false, false, false], "sample_1103": [false, true, false, true, false], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, true, true, false], "sample_1106": [false, false, false, false, false], "sample_1107": [false, false, false, false, false], "sample_1108": [false, true, false, false, false], "sample_1109": [false, false, true, false, false], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, true, false, false], "sample_1112": [false, false, true, true, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, false, false, false, false], "sample_1115": [true, false, false, false, false], "sample_1116": [false, false, false, false, false], "sample_1117": [false, false, false, false, false], "sample_1118": [true, false, false, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [false, false, false, false, false], "sample_1121": [false, false, false, true, false], "sample_1122": [false, false, false, false, false], "sample_1123": [false, false, false, false, false], "sample_1124": [false, false, false, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, false, true, true, false], "sample_1127": [false, false, false, false, false], "sample_1128": [false, false, false, false, true], "sample_1129": [false, false, false, false, false], "sample_1130": [true, false, false, false, false], "sample_1131": [false, false, false, false, false], "sample_1132": [true, false, false, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, false, false, false, false], "sample_1135": [false, false, false, false, true], "sample_1136": [false, false, false, false, false], "sample_1137": [false, true, false, false, false], "sample_1138": [false, false, false, false, false], "sample_1139": [true, false, false, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [false, false, false, true, true], "sample_1142": [false, false, false, false, true], "sample_1143": [false, false, false, false, false], "sample_1144": [false, false, false, false, false], "sample_1145": [false, false, false, true, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, false, false, false], "sample_1148": [false, false, false, false, false], "sample_1149": [false, false, false, false, false], "sample_1150": [false, false, true, false, true], "sample_1151": [false, false, false, true, false], "sample_1152": [false, false, true, false, false], "sample_1153": [false, false, false, false, false], "sample_1154": [false, true, false, false, false], "sample_1155": [false, false, true, false, false], "sample_1156": [false, false, false, false, false], "sample_1157": [false, false, false, false, false], "sample_1158": [true, false, false, false, false], "sample_1159": [false, false, false, false, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, false, false, false, false], "sample_1162": [true, false, false, false, false], "sample_1163": [false, false, false, false, false], "sample_1164": [false, false, false, false, false], "sample_1165": [false, false, false, false, false], "sample_1166": [false, false, true, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [false, false, true, false, false], "sample_1170": [false, false, false, false, false], "sample_1171": [false, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [false, false, false, false, false], "sample_1174": [false, false, false, true, false], "sample_1175": [false, false, false, false, false], "sample_1176": [false, true, false, false, true], "sample_1177": [false, false, false, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, false, false, false], "sample_1180": [false, false, false, true, false], "sample_1181": [false, false, false, false, false], "sample_1182": [false, true, false, false, false], "sample_1183": [false, true, false, false, false], "sample_1184": [false, true, false, false, false], "sample_1185": [false, false, true, false, true], "sample_1186": [true, true, false, false, false], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [true, true, false, false, false], "sample_1190": [false, false, false, false, false], "sample_1191": [false, false, true, false, false], "sample_1192": [true, false, true, false, false], "sample_1193": [false, false, false, false, false], "sample_1194": [true, false, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, true, false, true, true], "sample_1199": [false, true, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [false, true, false, false, true], "sample_1203": [false, false, false, false, false], "sample_1204": [false, false, false, false, false], "sample_1205": [false, false, true, true, true], "sample_1206": [false, false, false, true, false], "sample_1207": [false, false, false, false, false], "sample_1208": [false, false, false, false, false], "sample_1209": [false, false, false, false, false]}}