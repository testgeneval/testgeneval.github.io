{"raw_generations": {"sample_792": ["    def test_complement_nb_partial_fit():\n        \"\"\"Test the partial_fit functionality of ComplementNB.\"\"\"\n        clf = ComplementNB()\n        # Check if partial fit works with initial data\n        clf.partial_fit(X2[:3], y2[:3])\n        assert clf.classes_.tolist() == [1, 2, 3]\n        # Check if it keeps track of classes not seen in initial data\n        clf.partial_fit(X2[3:], y2[3:])\n\n        # Check if predict works after partial fit\n        assert_array_equal(clf.predict(X2), y2)\n\n", "    def test_gaussian_nb_fit_sparse_input(self):\n        # Test fit method with sparse input\n        X_train_sparse = scipy.sparse.csr_matrix(X1)\n        clf = GaussianNB()\n        clf.fit(X_train_sparse, y1)\n        assert_array_equal(clf.predict(X_train_sparse), clf.predict(X1))\n", "    def test_gaussian_nb_partial_fit():\n        # Test that partial_fit works correctly\n\n        gnb = GaussianNB()\n        gnb.partial_fit(X1[:5], y1[:5], classes=np.unique(y1))\n        gnb.partial_fit(X1[5:], y1[5:])\n        y_pred = gnb.predict(X1)\n        assert_array_equal(y_pred, y1)\n", "    def test_gaussian_nb_predict_proba(self):\n        clf = GaussianNB()\n        clf.fit(X, y)\n        y_proba = clf.predict_proba(X)\n        assert_array_almost_equal(y_proba.sum(axis=1), np.ones(len(X)), 5)\n        assert_almost_equal(y_proba[:3, 0].mean(), 1)\n        assert_almost_equal(y_proba[3:, 1].mean(), 1)\n", "def test_gaussian_nb_partial_fit():\n    # Test that partial_fit works as expected for GaussianNB\n    clf = GaussianNB()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n    # First partial fit\n    clf.partial_fit(X_train[:2], y_train[:2])\n    # Second partial fit\n    clf.partial_fit(X_train[2:], y_train[2:])\n    assert_array_equal(clf.predict(X_test), GaussianNB().fit(X_train,\n                                                           y_train).predict(\n                                                               X_test))\n\n"], "sample_275": ["    def test_delete_with_select_related(self):\n        p1 = Person.objects.create(name='John')\n        p2 = Person.objects.create(name='Jane')\n\n        Award.objects.create(name='Nobel', recipient=p1)\n        Award.objects.create(name='Pulitzer', recipient=p2)\n        AwardNote.objects.create(award=Award.objects.get(name='Nobel'), note='Great job!')\n\n        with transaction.atomic():\n            with self.conn2.cursor() as cursor:\n                cursor.execute('SELECT id FROM delete_regress_person WHERE id = %s', [p1.id])\n                p1_id = cursor.fetchone()[0]\n                cursor.execute('DELETE FROM delete_regress_person WHERE id = %s', [p1_id])\n            with self.assertRaises(Award.DoesNotExist):\n                # This should raise an error because p1 has already been deleted\n                Award.objects.select_related('recipient').get(name='Nobel')\n\n", "    def test_delete_locking(self):\n        \"\"\"\n        Tests that delete statements are correctly locked.\n\n        This test case assumes the database supports a transaction isolation level\n        that prevents deadlocks from occurring on concurrent delete statements.\n        \"\"\"\n        # Create a book and some related items in a transaction\n        transaction.managed(True)\n        book = Book.objects.create(title=\"The Hitchhiker's Guide to the Galaxy\")\n        Toy.objects.create(name=\"Plush Towel\", book=book)\n        Toy.objects.create(name=\"Babel Fish\", book=book)\n        transaction.commit()\n\n        # Start two transactions that will try to delete the book, but from\n        # different connections\n        with transaction.atomic():\n            cursor = self.conn2.cursor()\n            cursor.execute(\"DELETE FROM delete_regress_book WHERE id = %s\", [book.id])\n\n        # Try to delete the book from the primary connection. This should block\n        # until the other transaction completes.\n        with self.assertRaises(DatabaseError):\n            Book.objects.get(id=book.id).delete()\n\n        # Now commit the second transaction\n        self.conn2.commit()\n        # The delete from the primary connection should now succeed\n        Book.objects.get(id=book.id).delete()\n\n", "    def test_delete_locking(self):\n        with transaction.atomic():\n            p1 = Person.objects.create(name='Alice')\n            p2 = Person.objects.create(name='Bob')\n\n            c1 = Contact.objects.create(person=p1, email='alice@example.com')\n            c2 = Contact.objects.create(person=p2, email='bob@example.com')\n\n        # In a separate transaction, delete p1 (and its related Contact).\n        transaction.on_commit(lambda: self.conn2.execute(\"DELETE FROM delete_regress_person WHERE id = %s\", [p1.id]))\n        transaction.on_commit(lambda: self.conn2.execute(\"DELETE FROM delete_regress_contact WHERE id = %s\", [c1.id]))\n        with transaction.atomic():\n            # This should not raise an error, even though p1 is already\n            # deleted in the other connection.\n            for contact in Contact.objects.filter(person__name='Alice'):\n                contact.delete()\n\n        # Check that 'Bob's' contact still exists\n        self.assertEqual(Contact.objects.filter(person__name='Bob').count(), 1)\n", "    def test_delete_locking(self):\n        \"\"\"\n        Test that deleted objects are properly locked to prevent race conditions.\n        \"\"\"\n        with transaction.atomic(using='default'):\n            # Create an object in the default database\n            person = Person.objects.create(name='John Doe')\n\n            # Start a new transaction on the second connection\n            with transaction.atomic(using=self.conn2.alias):\n                # Try to delete the same object with a different connection\n\n                Person.objects.using(self.conn2.alias).filter(id=person.id).delete()\n\n                # Rollback the second transaction\n                self.conn2.rollback()\n\n            # Commit the first transaction\n            transaction.commit()\n\n\n\n", "    def test_delete_locking(self):\n        # Create a Person object\n        p = Person.objects.create(first_name=\"John\", last_name=\"Doe\")\n        \n        # Start a transaction on connection 1\n        with transaction.atomic():\n            # Delete the Person object on connection 1\n            Person.objects.filter(pk=p.pk).delete()\n        \n        # Check if the Person object doesn't exist on connection 2\n        with self.assertRaises(ObjectDoesNotExist):\n            Person.objects.using('default').get(pk=p.pk)\n"], "sample_408": ["    def test_remove_field_with_related_model_deletion(self):\n        before = [self.author_with_book, self.book]\n        after = [self.author_without_book]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, name=\"book\", model_name=\"author\"\n        )\n\n", "    def test_rename_field_with_default(self):\n        before = [\n            self.author_with_name,\n        ]\n        after = [\n            self.author_with_name_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"n\",\n                                       old_name=\"name\")\n\n", "    def test_add_field_with_default_to_existing_model(self):\n        before_states = [\n            self.author_no_bio,\n        ]\n        after_states = [\n            self.author_with_bio,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField'])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='biography', field_type='TextField'\n        )\n", "    def test_unique_together_remove_field(self):\n        before = self.make_project_state(\n            [\n                self.book_unique_together,\n            ]\n        )\n        after = self.make_project_state(\n            [\n                ModelState(\n                    \"otherapp\",\n                    \"Book\",\n                    [\n                        (\"id\", models.AutoField(primary_key=True)),\n                        (\"author\", models.ForeignKey(\"testapp.Author\", models.CASCADE)),\n                        (\"title\", models.CharField(max_length=200)),\n                    ],\n                    {\"unique_together\": {(\"title\",)}},\n                ),\n            ]\n        )\n        changes = self.get_changes(before, after)\n        self.assertMigrationDependencies(changes, 'otherapp', 0, [])\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n\n", "    def test_rename_field_with_default(self):\n        before = self.make_project_state(\n            [\n                self.author_with_name,\n            ]\n        )\n        after = self.make_project_state(\n            [\n                self.author_with_renamed_field_with_default,\n            ]\n        )\n        changes = self.get_changes(before, after)\n        self.assertEqual(len(changes.items()), 1)\n\n"], "sample_1130": ["def test_set_pos_nested():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    p1.set_pos(p2, 10*N.x)\n    p2.set_pos(p3, 5*N.y)\n    assert p1.pos_from(p3) == 10*N.x + 5*N.y\n\n", "def test_point_vel_cyclic():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    p1.set_pos(p2, N.x)\n    p2.set_pos(p3, N.y)\n    p3.set_pos(p1, N.z)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")  \n        p1.vel(N)\n\n", "def test_point_vel_autocalc_cyclic():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = Point('p2')\n    p3 = Point('p3')\n    p1.set_vel(N, 10*N.x)\n    p2.set_pos(p1, 5*N.y)\n    p3.set_pos(p2, 5*N.z)\n    p1.set_pos(p3, -5*N.z)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        p2_vel = p2.vel(N)\n    assert p2_vel == 10*N.x\n", "def test_locatenew():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = p1.locatenew('p2', 10*N.x)\n    assert p2.pos_from(p1) == 10*N.x\n", "def test_point_a2pt_theory_with_non_inertial_fixed_frame():\n    N = ReferenceFrame('N')\n    B = N.orientnew('B', 'Axis', [dynamicsymbols('q'), N.z])\n    O = Point('O')\n    P = O.locatenew('P', 10 * B.x)\n    O.set_vel(N, 5 * N.x)\n    P.a2pt_theory(O, N, B)\n    assert P.acc(N) == 10*B.y*dynamicsymbols('q').diff(dynamicsymbols('t'))**2 + 10*dynamicsymbols('q').diff(dynamicsymbols('t'), 2)*B.y\n"], "sample_766": ["    def test_sparse_coder_split_sign(self):\n        coder = SparseCoder(dictionary=self.dictionary_,\n                            transform_algorithm='omp',\n                            split_sign=True)\n        code = coder.transform(X)\n        assert code.shape[1] == 2 * self.dictionary_.shape[0]\n", "    def test_dict_learning_warm_start(self):\n        # Test warm starting with a pre-initialized dictionary\n        dict_init = rng_global.randn(4, n_features)\n        model = DictionaryLearning(n_components=4, random_state=0,\n                                   dict_init=dict_init)\n        model.fit(X)\n        assert_array_equal(model.components_, model.components_)\n\n        # Test warm starting with a partial_fit\n        model_partial = DictionaryLearning(n_components=4, random_state=0)\n        model_partial.partial_fit(X[:5])\n        model_partial.partial_fit(X[5:])\n        assert_array_almost_equal(model.components_, model_partial.components_,\n                                  decimal=2)\n", "    def test_dict_learning_online_n_iter_warning(self):\n        # Test that a warning is raised if n_iter is too small\n\n        with pytest.warns(ConvergenceWarning, match=\"did not converge\"):\n            MiniBatchDictionaryLearning(n_iter=1, random_state=0).fit(X)\n\n", "    def test_transform_n_nonzero_coefs_omp(self):\n        dl = DictionaryLearning(n_components=5, transform_algorithm='omp',\n                                transform_n_nonzero_coefs=2)\n        dl.fit(X)\n        code = dl.transform(X)\n        assert_equal(np.count_nonzero(code, axis=1).max(), 2)\n", "    def test_partial_fit_warm_start(self):\n        # Test that partial_fit works correctly with warm start\n        # using a previously trained dictionary.\n\n        # Initial dictionary learning\n        dico = DictionaryLearning(n_components=2, random_state=0)\n        dico.fit(X)\n\n        # Partial fit with warm start\n        dico2 = DictionaryLearning(n_components=2, random_state=0)\n        dico2.partial_fit(X[:5, :], iter_offset=0)\n        dico2.partial_fit(X[5:, :], iter_offset=1)\n        assert_array_almost_equal(dico.components_, dico2.components_)\n\n"], "sample_803": ["    def test_roc_auc_score_multiclass_ovo():\n\n        # test roc_auc_score for multiclass classification with 'ovo' strategy\n\n        y_true = np.array([0, 1, 2, 0, 1, 2])\n        y_score = np.array([[0.2, 0.5, 0.3],\n                            [0.8, 0.1, 0.1],\n                            [0.1, 0.2, 0.7],\n                            [0.3, 0.6, 0.1],\n                            [0.6, 0.3, 0.1],\n                            [0.1, 0.1, 0.8]])\n        \n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UndefinedMetricWarning)\n            auc_ovo = roc_auc_score(y_true, y_score, multi_class='ovo')\n\n        # calculate expected auc using one-vs-one strategy\n        expected_auc = 0.5  # Placeholder for expected AUC value\n\n        assert_almost_equal(auc_ovo, expected_auc, decimal=2)\n\n\n\n", "def test_label_ranking_average_precision_score_multilabel():\n    n_samples = 10\n    n_labels = 5\n\n    y_true = np.zeros((n_samples, n_labels))\n    y_true[0, 1] = 1\n    y_true[1, 0] = 1\n    y_true[2, 3] = 1\n    y_true[3, 2] = 1\n    y_true[4, 4] = 1\n    y_true[5, 1] = 1\n    y_true[6, 0] = 1\n    y_true[7, 3] = 1\n    y_true[8, 2] = 1\n    y_true[9, 4] = 1\n\n    # Perfect ranking\n    y_score = np.zeros((n_samples, n_labels))\n    y_score[0, 1] = 1\n    y_score[1, 0] = 1\n    y_score[2, 3] = 1\n    y_score[3, 2] = 1\n    y_score[4, 4] = 1\n    y_score[5, 1] = 1\n    y_score[6, 0] = 1\n    y_score[7, 3] = 1\n    y_score[8, 2] = 1\n    y_score[9, 4] = 1\n    assert_almost_equal(label_ranking_average_precision_score(y_true, y_score), 1.0)\n\n    # Reverse ranking\n    y_score = np.zeros((n_samples, n_labels))\n    y_score[0, 0] = 1\n    y_score[1, 1] = 1\n    y_score[2, 2] = 1\n    y_score[3, 3] = 1\n    y_score[4, 4] = 1\n    y_score[5, 0] = 1\n    y_score[6, 1] = 1\n    y", "    def test_roc_auc_score_multiclass_empty_classes(self):\n        # Check that roc_auc_score handles empty classes correctly.\n\n        y_true = np.array([0, 1, 2, 0, 1, 2])\n        y_score = np.array([[0.1, 0.2, 0.7],\n                            [0.3, 0.6, 0.1],\n                            [0.8, 0.1, 0.1],\n                            [0.1, 0.2, 0.7],\n                            [0.3, 0.6, 0.1],\n                            [0.8, 0.1, 0.1]])\n\n        for average in ['micro', 'macro']:\n            with pytest.warns(UndefinedMetricWarning):\n                score = roc_auc_score(y_true, y_score, average=average)\n                assert np.isnan(score)\n", "def test_average_precision_score_empty_y_true():\n    y_true = np.array([])\n    y_score = np.array([])\n    assert_raises(ValueError, average_precision_score, y_true, y_score)\n", "    def test_roc_auc_score_multiclass_micro(self):\n        y_true = [0, 1, 2, 0, 1, 2]\n        y_score = [[0.2, 0.5, 0.3],\n                   [0.9, 0.6, 0.1],\n                   [0.1, 0.2, 0.7],\n                   [0.6, 0.2, 0.2],\n                   [0.3, 0.8, 0.1],\n                   [0.1, 0.1, 0.8]]\n        assert_almost_equal(roc_auc_score(y_true, y_score,\n                                         average='micro'),\n                            0.833333333333, 7)\n\n"], "sample_257": ["    def test_transform_isnull(self):\n        with self.assertRaises(ValueError):\n            KeyTransform(None, 'key')\n\n\n", "    def test_key_transform_isnull_with_has_key(self):\n        json_data = [\n            {'name': 'John', 'age': 30},\n            {'name': 'Jane', 'age': 25},\n            {'name': 'Peter', 'age': None},\n        ]\n        objects = [JSONModel.objects.create(data=d) for d in json_data]\n\n        # Test for key existing and IS NULL\n        isnull_objects = JSONModel.objects.filter(data__name__isnull=True)\n        self.assertQuerysetEqual(isnull_objects, [])\n\n        # Test for key existing and NOT IS NULL\n        not_isnull_objects = JSONModel.objects.filter(\n            data__name__isnull=False,\n        )\n        self.assertQuerysetEqual(not_isnull_objects, objects)\n\n        # Test for key not existing and IS NULL\n\n        isnull_objects = JSONModel.objects.filter(data__nonexistent__isnull=True)\n        self.assertQuerysetEqual(isnull_objects, objects)\n\n", "    def test_key_transform_in_with_subquery(self):\n        with connection.cursor() as cursor:\n            cursor.execute('CREATE TABLE \"relatedjsonmodel_related\" (\"id\" SERIAL PRIMARY KEY)')\n            cursor.execute('ALTER TABLE \"relatedjsonmodel_related\" ADD COLUMN \"value\" TEXT')\n            cursor.execute('INSERT INTO \"relatedjsonmodel_related\" (\"value\") VALUES (%s)', ('foo',))\n            cursor.execute('INSERT INTO \"relatedjsonmodel_related\" (\"value\") VALUES (%s)', ('bar',))\n            cursor.execute('ALTER TABLE \"relatedjsonmodel\" ADD COLUMN \"related_id\" INTEGER REFERENCES \"relatedjsonmodel_related\" (\"id\")')\n        RelatedJSONModel.objects.create(related_id=1, data={'key': 'value'})\n        RelatedJSONModel.objects.create(related_id=2, data={'other_key': 'value'})\n\n        subquery = RelatedJSONModel.objects.filter(\n            data__related_id__in=RelatedJSONModel.objects.filter(data__key='value').values('related_id'),\n        )\n        self.assertQuerysetEqual(subquery, [\n            RelatedJSONModel.objects.get(\n                related_id=1, data={'key': 'value'}\n            ),\n        ])\n\n\n", "    def test_key_transform_isnull(self):\n        obj = JSONModel.objects.create(data={'key': 'value'})\n        self.assertTrue(\n            JSONModel.objects.filter(data__key__isnull=False).exists()\n        )\n        self.assertFalse(\n            JSONModel.objects.filter(data__key__isnull=True).exists()\n        )\n        obj.data = {}\n        obj.save()\n        self.assertFalse(\n            JSONModel.objects.filter(data__key__isnull=False).exists()\n        )\n        self.assertTrue(\n            JSONModel.objects.filter(data__key__isnull=True).exists()\n        )\n", "    def test_key_transform_isnull_with_null_key(self):\n        model_data = [\n            {'key': {'a': 1, 'b': None}},\n            {'key': {'a': 2}},\n        ]\n        self.create_json_objects(JSONModel, model_data)\n\n        # Check for a key that doesn't exist\n        qs = JSONModel.objects.filter(key__isnull=True)\n        self.assertQuerysetEqual(qs, [])\n\n        # Check for a key that exists but has a null value\n        qs = JSONModel.objects.filter(key__b__isnull=True)\n        self.assertQuerysetEqual(qs, [JSONModel.objects.get(pk=1)])\n\n"], "sample_1206": ["def test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(2) == 1\n    assert fibonacci(3) == 2\n    assert fibonacci(4) == 3\n    assert fibonacci(5) == 5\n    assert fibonacci(6) == 8\n    assert fibonacci(7) == 13\n    assert fibonacci(8) == 21\n    assert fibonacci(9) == 34\n", "def test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(2) == 1\n    assert fibonacci(3) == 2\n    assert fibonacci(4) == 3\n    assert fibonacci(5) == 5\n    assert fibonacci(6) == 8\n    assert fibonacci(7) == 13\n\n", "    def test_catalan_rewrite():\n        assert S.Catalan._eval_rewrite_as_Sum() == Sum(S.NegativeOne**k / (2*k+1)**2, (k, 0, S.Infinity))\n\n", "    def test_sympify_mpmath_complex():\n        a = mpmath.mpc(1, 2)\n        assert sympify(a) == 1 + 2*S.ImaginaryUnit\n", "    def test_isqrt():\n        assert isqrt(4) == 2\n        assert isqrt(16) == 4\n        assert isqrt(15) == 3\n        assert isqrt(0) == 0\n\n        assert isqrt(-1) == S.ImaginaryUnit\n        assert isqrt(-4) == 2*S.ImaginaryUnit\n"], "sample_1161": ["def test_print_Tr():\n    assert sstr(Tr(Matrix([[1, 2], [3, 4]]))) == 'Tr(Matrix([[1, 2], [3, 4]]))'\n", "def test_printmethod_sympyrepr():\n    class Foo(Expr):\n            return \"MyFoo()\"\n    f = Foo()\n    assert sstr(f, printmethod='_sympyrepr') == \"MyFoo()\"\n\n", "def test_print_MatrixSlice():\n    M = MatrixSymbol('M', 3, 3)\n    assert sstr(M[1, 1]) == 'M[1, 1]'\n    assert sstr(M[1, :]) == 'M[1, :]'\n    assert sstr(M[:, 1]) == 'M[:, 1]'\n", "    def test_print_RootSum(self):\n        assert sstr(RootSum(x**2 - 1, x)) == 'RootSum(x**2 - 1, x)'\n        assert sstr(RootSum(x**2 - 1, S.IdentityFunction)) == 'RootSum(x**2 - 1)'\n", "    def test_sstrrepr_MatrixSlice():\n        A = MatrixSymbol('A', 3, 3)\n        assert sstrrepr(A[:, 1]) == \"MatrixSlice(A, (Slice(None, None, None), 1))\"\n"], "sample_1120": ["def test_MatrixElement_doit_nested():\n    M = MatrixSymbol('M', 3, 3)\n    x = Symbol('x')\n    M[x, x] = x**2\n\n    assert M[x, x].doit() == x**2\n\n    M[x, x] = M[x, x]\n\n    assert M[x, x].doit() == x**2\n\n", "    def test_identity_zero_multiply():\n        assert Identity(n)*ZeroMatrix(n, m) == ZeroMatrix(n, m)\n        assert ZeroMatrix(n, m)*Identity(m) == ZeroMatrix(n, m)\n\n", "def test_matrix_element_subs():\n    M = MatrixSymbol('M', 2, 2)\n    expr = M[0, 1]\n    assert expr.subs(M, ImmutableMatrix([[1, 2], [3, 4]])) == 2\n\n\n", "def test_ZeroMatrix():\n    assert ZeroMatrix(m, n) is ZeroMatrix(m, n)\n    assert ZeroMatrix(m, n) != ZeroMatrix(n, m)\n\n    # Test zero matrices with symbolic dimensions\n    x, y = symbols('x y', integer=True)\n    assert ZeroMatrix(x, y) is ZeroMatrix(x, y)\n    assert ZeroMatrix(x, y) != ZeroMatrix(y, x)\n", "def test_ZeroMatrix_add():\n    Z1 = ZeroMatrix(2, 2)\n    Z2 = ZeroMatrix(2, 2)\n    assert Z1 + Z2 == ZeroMatrix(2, 2)\n    assert Z1 + 1 == Matrix(2, 2, [1]*4)\n    assert 1 + Z1 == Matrix(2, 2, [1]*4)\n"], "sample_1153": ["def test_polarify_subs():\n    x = Symbol('x')\n    y = Symbol('y')\n    expr = x*y\n\n    # Test with subs=True\n    result, subs_dict = polarify(expr, subs=True)\n    assert isinstance(result, Expr)\n    assert all(isinstance(s, Dummy) for s in result.free_symbols)\n\n    # Test with subs=False\n    result = polarify(expr, subs=False)\n    assert result == expr\n\n    # Test with lift=True\n    result = polarify(expr, lift=True)\n    assert isinstance(result, Expr)\n    assert all(\n        isinstance(s, polar_lift) for s in result.free_symbols\n        if s.is_Symbol\n    )\n", "    def test_periodic_argument():\n        z = symbols('z', real=True)\n        # Test the basic functionality\n        assert periodic_argument(exp_polar(I*2*pi), 2*pi) == 0\n        assert periodic_argument(exp_polar(I*5*pi), 2*pi) == pi\n        assert periodic_argument(exp_polar(I*(-2)*pi), 2*pi) == 0\n        assert periodic_argument(exp_polar(I*pi/2), 2*pi) == pi/2\n        assert periodic_argument(exp_polar(I*(-pi/2)), 2*pi) == 3*pi/2\n        # Test handling of complex numbers\n        assert periodic_argument(2 + 3*I, 2*pi) == atan2(3, 2)\n        assert periodic_argument(\n            exp_polar(I*pi/4)*2*exp_polar(I*pi/2), pi) == pi/4\n        # Test non-standard periods\n        assert periodic_argument(exp_polar(I*pi), 3*pi) == pi\n        assert periodic_argument(exp_polar(I*pi), 4*pi) == pi\n        assert periodic_argument(exp_polar(I*pi), pi) == pi\n        # Test with a symbolic period\n        p = Symbol('p', positive=True)\n        assert isinstance(periodic_argument(exp_polar(I*pi), p),\n                                  periodic_argument)\n        assert periodic_argument(exp_polar(I*2*pi), p) == 0\n", "def test_polar_lift_rewrite():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert polar_lift(x).rewrite('exp') == exp_polar(0)*x\n    assert polar_lift(x*y).rewrite('exp') == exp_polar(0)*x*y\n    assert polar_lift(-x).rewrite('exp') == exp_polar(I*pi)*x\n    assert polar_lift(1j).rewrite('exp') == exp_polar(I*pi/2)\n    assert polar_lift(-1j).rewrite('exp') == exp_polar(-I*pi/2)\n", "def test_polarify_integration():\n    x, y = symbols('x y')\n    expr = Integral(x**2*sin(y), (y, 0, pi))\n    polar_expr = polarify(expr)[0]\n    assert polar_expr == Integral(\n        x**2*sin(_y), (_y, 0, pi)).subs({x: _x, y: _y})\n\n", "    def test_unpolarify_subs():\n        x, y = symbols('x y', polar=True)\n        e = (x*y)**2\n        assert unpolarify(e, subs={x: _x, y: _y}) == (_x*_y)**2\n"], "sample_610": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_569": ["    def test_residplot(self):\n        with pytest.raises(TypeError):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df, x_partial=\"d\")\n        with pytest.raises(ValueError):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df, order=5)\n        ax = lm.residplot(x=\"x\", y=\"y\", data=self.df)\n        assert isinstance(ax, mpl.axes.Axes)\n        ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, lowess=True)\n        assert isinstance(ax, mpl.axes.Axes)\n        ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, robust=True)\n        assert isinstance(ax, mpl.axes.Axes)\n        ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, ax=plt.axes())\n        assert isinstance(ax, mpl.axes.Axes)\n", "    def test_residplot(self):\n        with pytest.warns(UserWarning):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df)\n        with pytest.warns(UserWarning):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df, lowess=True)\n        with pytest.warns(UserWarning):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df, robust=True)\n", "    def test_residplot_robust(self):\n        ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, robust=True)\n        assert len(ax.lines) == 2  # Residual scatterplot and zero line\n", "    def test_residplot(self):\n        with sns.axes_style(\"whitegrid\"):\n            ax = lm.residplot(x=\"x\", y=\"y\", data=self.df.loc[:30],\n                              lowess=True)\n            assert len(ax.lines) == 2\n            assert isinstance(ax.lines[0], mpl.lines.Line2D)\n\n            ax = lm.residplot(\"x\", \"y\", self.df.loc[:30], robust=True,\n                              scatter_kws={\"marker\": \"s\"})\n            assert len(ax.collections) == 1\n            assert ax.collections[0].get_offsets().shape[0] == 31\n            \n            ax = lm.residplot(\"x\", \"y\", self.df,\n                              x_partial=\"d\", color=\"m\", label=\"my label\")\n            assert ax.lines[0].get_color() == \"m\"\n\n", "    def test_residplot(self):\n\n        # Basic residplot\n        ax = lm.residplot('x', 'y', self.df)\n        assert isinstance(ax, mpl.axes.Axes)\n\n        # With lowess\n        ax = lm.residplot('x', 'y', self.df, lowess=True)\n        assert isinstance(ax, mpl.axes.Axes)\n\n        # With partial\n        ax = lm.residplot('x', 'y', self.df, x_partial='d')\n        assert isinstance(ax, mpl.axes.Axes)\n\n        # With robust fit\n        ax = lm.residplot('x', 'y', self.df, robust=True)\n        assert isinstance(ax, mpl.axes.Axes)\n\n        # With custom color\n        ax = lm.residplot('x', 'y', self.df, color='red')\n        assert isinstance(ax, mpl.axes.Axes)\n\n\n"], "sample_1009": ["def test_vector_to_matrix():\n    N = ReferenceFrame('N')\n    v = x*N.x + y*N.y + z*N.z\n    assert v.to_matrix(N) == Matrix([x, y, z]).reshape(3,1)\n", "def test_vector_dot_product_with_symbol():\n    q1 = symbols('q1')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.x])\n\n    v1 = A.x\n    v2 = N.y\n\n    assert dot(v1, v2) == sin(q1)\n", "def test_vector_diff_var_not_in_dcm():\n    q1 = dynamicsymbols('q1')\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [q1, N.y])\n    u1, u2 = dynamicsymbols('u1, u2')\n    v = u1 * A.x + u2 * N.y\n    res = v.diff(u2, N, var_in_dcm=False)\n    assert res == N.y\n", "def test_vector_dot_product_with_vector_with_same_frame():\n    N = ReferenceFrame('N')\n    v1 = 2*N.x + 3*N.y\n    v2 = N.x + 4*N.y\n\n    assert dot(v1, v2) == 2*N.x | N.x + 8*N.x | N.y + 12*N.y | N.y\n    assert dot(v1, v2).simplify() == 2 + 12 == 14\n", "    def test_zero_vector():\n        v = Vector(0)\n        assert v.args == []\n        assert v == 0\n\n"], "sample_854": ["    def test_SVC_decision_function_shape_ovr(self):\n        # Test the decision_function output shape when decision_function_shape='ovr'\n        clf = svm.SVC(kernel='linear', decision_function_shape='ovr')\n        clf.fit(iris.data[:100], iris.target[:100])\n        assert clf.decision_function(iris.data[:10]).shape == (10, 3)\n", "    def test_svc_predict_proba_ovr():\n        # Test predict_proba with ovr strategy\n        svm = svm.SVC(probability=True, decision_function_shape='ovr')\n        svm.fit(X, Y)\n        probs = svm.predict_proba(T)\n        assert_array_almost_equal(\n            probs,\n            np.array([[0.7135, 0.2865], [0.2865, 0.7135], [0.2865, 0.7135]])\n        )\n\n", "    def test_SVC_decision_function_shape_ovr(self):\n        # Test decision_function_shape='ovr' for SVC\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2, random_state=42)\n        clf = svm.SVC(decision_function_shape='ovr', kernel='linear',\n                      probability=True, random_state=42)\n        clf.fit(X_train, y_train)\n        dec_func = clf.decision_function(X_test)\n        assert dec_func.shape[1] == 3  # 3 classes in iris dataset\n\n\n", "    def test_svc_decision_function_shape_ovr(self):\n        # Check that decision_function shape is correct for OneVsRest\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2, random_state=42)\n\n        clf = svm.SVC(decision_function_shape='ovr', kernel='linear')\n        clf.fit(X_train, y_train)\n        dec = clf.decision_function(X_test)\n        assert dec.shape == (len(X_test), len(set(y_train)))\n\n", "    def test_svc_predict_proba_raises_error_when_probability_is_false():\n        clf = svm.SVC(probability=False)\n        clf.fit(iris.data[:100], iris.target[:100])\n        with pytest.raises(NotFittedError):\n            clf.predict_proba(iris.data[100:])\n\n"], "sample_178": ["    def test_formset_factory_absolute_max(self):\n        class MyForm(Form):\n            field = CharField()\n\n        MyFormSet = formset_factory(MyForm, absolute_max=3, max_num=4)\n        formset = MyFormSet(data={\n            'form-0-field': ['a'],\n            'form-1-field': ['b'],\n            'form-2-field': ['c'],\n            'form-3-field': ['d'],\n            'form-TOTAL_FORMS': '5',\n        })\n        self.assertEqual(formset.errors, [])\n\n        formset = MyFormSet(data={\n            'form-0-field': ['a'],\n            'form-1-field': ['b'],\n            'form-2-field': ['c'],\n            'form-3-field': ['d'],\n            'form-4-field': ['e'],\n            'form-TOTAL_FORMS': '5',\n        })\n        self.assertIn('too_many_forms', formset.non_form_errors())\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(Choice, absolute_max=5)\n\n        formset = formset_class({'choices-TOTAL_FORMS': '6'}, data={'choices-0-choice': 'A'})\n        self.assertEqual(formset.errors, [{'choice': ['Ensure this field has no more than 5 character(s).']}])\n        self.assertEqual(formset.total_form_count(), 5)\n\n        formset = formset_class({'choices-TOTAL_FORMS': '2'})\n        self.assertEqual(formset.total_form_count(), 2)\n", "    def test_formset_factory_with_custom_kwargs(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1)\n        formset = CustomKwargFormSet(data={'form-0-name': 'foo'}, custom_kwarg='bar')\n        self.assertEqual(formset[0].custom_kwarg, 'bar')\n", "    def test_formset_factory_absolute_max(self):\n        class MyForm(Form):\n            name = CharField()\n\n        # Create a FormSet with an absolute_max of 3\n        MyFormSet = formset_factory(MyForm, absolute_max=3)\n\n        # Creating a FormSet with 4 forms should raise a ValidationError\n        with self.assertRaises(ValidationError):\n            MyFormSet(data={f'form-{i}-name': 'test' for i in range(4)})\n\n", "    def test_formset_factory_absolute_max_validation(self):\n        # Test that absolute_max parameter in formset_factory raises a ValueError\n        # when max_num exceeds it.\n\n        with self.assertRaises(ValueError):\n            formset_factory(Choice, absolute_max=5, max_num=10)\n\n"], "sample_189": ["    def test_cache_key_warnings(self):\n        with self.assertWarnsRegex(CacheKeyWarning, KEY_ERRORS_WITH_MEMCACHED_MSG):\n            cache.set('invalid key', 1)\n\n", "    def test_cache_key_warnings(self):\n        with self.assertWarns(CacheKeyWarning) as w:\n            cache.set('a long key that will be truncated', 1)\n        self.assertIn(KEY_ERRORS_WITH_MEMCACHED_MSG % 'a long key that will be truncated', str(w.warning))\n", "    def test_incr_decr_version(self):\n        cache.set('key', 'value', version=1)\n        self.assertEqual(cache.incr_version('key', version=1), 2)\n        self.assertEqual(cache.get('key', version=1), None)\n        self.assertEqual(cache.get('key', version=2), 'value')\n        self.assertEqual(cache.decr_version('key', version=2), 1)\n        self.assertEqual(cache.get('key', version=2), None)\n        self.assertEqual(cache.get('key', version=1), 'value')\n", "    def test_cache_key_warnings(self):\n        with warnings.catch_warnings(record=True) as w:\n            cache.set('a' * 251, 1)\n            self.assertEqual(len(w), 1)\n            self.assertEqual(str(w[0].message), KEY_ERRORS_WITH_MEMCACHED_MSG % ('a' * 251))\n", "    def test_make_template_fragment_key(self):\n        key = make_template_fragment_key('my_fragment', {'foo': 'bar'}, None)\n        self.assertEqual(\n            key, 'django.template.fragments:my_fragment__foo=bar',\n        )\n\n"], "sample_1172": ["    def test_solve_biquadratic_complex():\n        x, y = symbols('x, y')\n        a = Poly(y**2 - 4 + x, y, x)\n        b = Poly(y*2 + 3*x - 7, y, x)\n        sol = solve_biquadratic(a, b, Options((x, y), {'domain': 'ZZ'}))\n        assert all(isinstance(v, (Integer, Rational)) for sol_tuple in sol for v in sol_tuple)\n\n", "def test_solve_biquadratic_non_ground_gcd():\n    # Test case for when the GCD of the polynomials in the\n    # Groebner basis is not ground, i.e., the system is not\n    # zero-dimensional\n\n    a = Poly(x**2 - y**2, x, y)\n    b = Poly(x**2 + y**2 - 1, x, y)\n    with raises(SolveFailed):\n        solve_biquadratic(a, b, Options((x, y), {'domain': 'ZZ'}))\n", "def test_solve_biquadratic_complex():\n    x, y = symbols('x y')\n    a = Poly(y**2 - 4 + x, y, x, domain='ZZ')\n    b = Poly(y*2 + 3*x - 7, y, x, domain='ZZ')\n    NewOption = Options((x, y), {'domain': 'ZZ'})\n    sol = solve_biquadratic(a, b, NewOption)\n    assert set(sol) == {(1/3, 3), (41/27, 11/9)}\n", "    def test_solve_biquadratic_solvefailed():\n        f = Poly(x**2 + y**2 - 1, x, y)\n        g = Poly(x + y - 1, x, y)\n        with raises(SolveFailed):\n            solve_biquadratic(f, g, Options((x, y), {'domain': 'ZZ'}))\n", "    def test_solve_biquadratic_complex_roots():\n        a = Poly(x**2 - 2*y**2 - 1, x, y, domain='ZZ')\n        b = Poly(x*y - 1, x, y, domain='ZZ')\n        f, g = a, b\n        opt = Options((x, y), {'domain': 'ZZ'})\n\n        solution = solve_biquadratic(f, g, opt)\n        assert solution == [\n            (sqrt(5)/2, sqrt(5)/2),\n            (-sqrt(5)/2, -sqrt(5)/2)\n        ] or solution == [\n            (-sqrt(5)/2, sqrt(5)/2),\n            (sqrt(5)/2, -sqrt(5)/2)\n        ]\n"], "sample_564": ["def test_add_collection3d():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    # Test LineCollection\n    verts = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2]])\n    lines = LineCollection(verts, colors='red')\n    ax.add_collection(lines)\n\n    # Test PolyCollection\n    vertices = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    faces = [[0, 1, 2, 3]]\n    poly = PolyCollection(\n        [vertices[face] for face in faces], facecolors=['blue'])\n    ax.add_collection(poly)\n", "    def test_stem3D(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        x = np.arange(10)\n        y = np.arange(10)\n        z = np.arange(10)\n        ax.stem(x, y, z, linefmt='--', markerfmt='o', basefmt='r-')\n        # ax.stem(x, y, z, linefmt='C3-', markerfmt='x', basefmt='C1--')\n        # ax.stem(x, y, z, orientation='y')\n\n", "    def test_stem3d(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        x, y, z = get_test_data(delta=0.2)\n        ax.stem(x, y, z)\n        ax.set_title('3D Stem Plot')\n", "def test_quiver3D(fig):\n    ax = fig.add_subplot(projection='3d')\n    X, Y, Z = np.meshgrid(np.arange(-0.8, 1, 0.2),\n                         np.arange(-0.8, 1, 0.2),\n                         np.arange(-0.8, 1, 0.2))\n    U = np.sin(np.pi * X) * np.cos(np.pi * Y) * np.cos(np.pi * Z)\n    V = -np.cos(np.pi * X) * np.sin(np.pi * Y) * np.cos(np.pi * Z)\n    W = (np.sqrt(2.0 / 3.0) * np.cos(np.pi * X) * np.cos(np.pi * Y) *\n         np.sin(np.pi * Z))\n    ax.quiver(X, Y, Z, U, V, W, length=0.1)\n", "def test_axes3d_quiver():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    x, y, z = np.meshgrid(np.arange(-0.8, 1, 0.2),\n                          np.arange(-0.8, 1, 0.2),\n                          np.arange(-0.8, 1, 0.2))\n    u = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z)\n    v = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z)\n    w = (np.sqrt(2.0 / 3.0) * np.cos(np.pi * x) * np.cos(np.pi * y) *\n         np.sin(np.pi * z))\n\n    ax.quiver(x, y, z, u, v, w, length=0.1)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    return fig\n"], "sample_900": ["    def test_mlp_regressor_partial_fit_sparse(self):\n        # Test if MLPRegressor supports partial_fit with sparse data\n        rng = np.random.RandomState(0)\n        X = csr_matrix(rng.randn(100, 10))\n        y = rng.randn(100)\n\n        mlp = MLPRegressor(hidden_layer_sizes=(5,), activation='relu',\n                           solver='adam', batch_size=20, max_iter=5)\n        mlp.partial_fit(X, y)\n        mlp.partial_fit(X, y)\n\n        # Check if the model is fitted\n        assert hasattr(mlp, 'coefs_')\n\n", "    def test_mlp_regressor_multi_label(self):\n        # Check if the Multi-Label Classification works with MLPRegressor\n        X, y = make_multilabel_classification(n_samples=100, n_features=20,\n                                                 n_classes=5, n_labels=2,\n                                                 random_state=0)\n        clf = MLPRegressor(solver='lbfgs', random_state=0,\n                           max_iter=1000)\n        clf.fit(X, y)\n\n", "    def test_mlp_regressor_partial_fit_earlystopping(self):\n        # Test that MLPRegressor works with partial_fit and early stopping\n        random_state = np.random.RandomState(42)\n        X = random_state.rand(100, 10)\n        y = random_state.rand(100, 1)\n        mlp = MLPRegressor(\n            hidden_layer_sizes=(5,),\n            activation=\"relu\",\n            solver=\"sgd\",\n            max_iter=10,\n            early_stopping=True,\n            random_state=42\n        )\n        for i in range(5):\n            mlp.partial_fit(X[i * 20:(i + 1) * 20], y[i * 20:(i + 1) * 20])\n", "    def test_mlp_regressor_sparse_input(self):\n        X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n        X_sparse = csr_matrix(X)\n        regressor = MLPRegressor(random_state=0)\n        regressor.fit(X_sparse, y)\n        assert_almost_equal(regressor.predict(X_sparse), regressor.predict(X))\n\n\n\n", "    def test_mlp_classifier_partial_fit_warm_start():\n        # Test partial_fit with warm_start for classification\n        mlp = MLPClassifier(hidden_layer_sizes=(10,), activation=\"relu\",\n                            solver='sgd', learning_rate='constant',\n                            learning_rate_init=0.01, warm_start=True)\n        mlp.partial_fit(X_digits_binary[:50], y_digits_binary[:50],\n                        classes=[0, 1])\n        mlp.partial_fit(X_digits_binary[50:100], y_digits_binary[50:100])\n        mlp_2 = MLPClassifier(hidden_layer_sizes=(10,), activation=\"relu\",\n                              solver='sgd', learning_rate='constant',\n                              learning_rate_init=0.01, warm_start=True)\n        mlp_2.partial_fit(X_digits_binary[:100], y_digits_binary[:100],\n                          classes=[0, 1])\n\n        assert_almost_equal(mlp.loss_, mlp_2.loss_, decimal=4)\n\n\n\n"], "sample_73": ["    def test_manifest_storage_double_hashing(self):\n        \"\"\"\n        Test that ManifestStaticFilesStorage doesn't double-hash filenames.\n        \"\"\"\n\n        self.add_css('style.css', 'body{color:red;}', 'css/')\n        self.add_js('script.js', 'console.log(\"Hello world!\");', 'js/')\n        call_command('collectstatic', interactive=False, verbosity=0)\n\n        # Get the hashed file names from the manifest.\n        manifest_data = storage.staticfiles_storage.read_manifest()\n        manifest = json.loads(manifest_data)\n        hashed_style_css = manifest['paths']['css/style.css']\n        hashed_script_js = manifest['paths']['js/script.js']\n\n        # Ensure the file names are only hashed once.\n        self.assertEqual(\n            hashed_style_css.count('.'), 1\n        )\n        self.assertEqual(\n            hashed_script_js.count('.'), 1\n        )\n        self.assertNotIn(\n            '.', hashed_style_css.split('/')[-1].split('.')[0],\n        )\n        self.assertNotIn(\n            '.', hashed_script_js.split('/')[-1].split('.')[0],\n        )\n", "    def test_manifest_strict_mode(self):\n        \"\"\"\n        Test that ManifestStaticFilesStorage raises an error when a file is\n        referenced in a CSS file but not found in the manifest.\n        \"\"\"\n        self.create_file('style.css', 'body { background: url(\"missing.jpg\"); }')\n        with self.assertRaises(ValueError) as e:\n            call_command('collectstatic', interactive=False, verbosity=0)\n        self.assertIn(\"Missing staticfiles manifest entry for 'missing.jpg'\", str(e.exception))\n\n        # Verify the manifest has only the expected entry\n        manifest = self.storage.read_manifest()\n        self.assertEqual(manifest.get('paths', {}).get('style.css'),\n                         self.hashed_file_path('style.css'))\n\n", "    def test_hashed_files_with_absolute_paths_in_css(self):\n        \"\"\"\n        Test that hashed files are correctly generated for CSS with absolute\n        paths.\n        \"\"\"\n        # Create CSS file with absolute path\n        css_content = \"\"\"\n            @font-face {\n                src: url(/fonts/myfont.woff2) format('woff2');\n            }\n        \"\"\"\n        self.create_file('style.css', css_content)\n\n        # Create font file\n        self.create_file('fonts/myfont.woff2', b'font data')\n\n        # Collect static files\n        call_command('collectstatic', interactive=False, verbosity=0)\n\n        # Assert hashed file exists\n        hashed_css_path = self.hashed_file_path('style.css')\n        self.assertTrue(os.path.isfile(os.path.join(settings.STATIC_ROOT, hashed_css_path)))\n\n        # Assert that the CSS file references the hashed font file\n        with open(os.path.join(settings.STATIC_ROOT, hashed_css_path), 'r') as f:\n            css_content = f.read()\n            \n        # Since the path is absolute, the hashed path shouldn't change\n        self.assertIn(f'\"{settings.STATIC_URL}fonts/myfont.woff2\"', css_content)\n\n", "    def test_hashed_files_strict_manifest_missing_entry(self):\n        \"\"\"\n        Test that an error is raised when a file is referenced in a CSS\n        file but is missing from the manifest.\n        \"\"\"\n        with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            # Create a CSS file that references a nonexistent JS file.\n            css_path = os.path.join(TEST_ROOT, 'test.css')\n            with open(css_path, 'w') as f:\n                f.write('@import url(\"%s\");' % os.path.join(settings.STATIC_URL, 'nonexistent.js'))\n\n            # Create a manifest file\n            with open(os.path.join(TEST_ROOT, storage.ManifestFilesMixin.manifest_name), 'w') as f:\n                json.dump({'paths': {'test.css': 'test.css' }, 'version': '1.0'}, f)\n\n            with self.assertRaises(ValueError) as e:\n                call_command('collectstatic', verbosity=0, interactive=False)\n            self.assertEqual(str(e.exception), \"Missing staticfiles manifest entry for 'nonexistent.js'\")\n", "    def test_manifest_staticfiles_storage_hash_collision(self):\n        \"\"\"\n        Test that ManifestStaticFilesStorage handles hash collisions gracefully.\n\n        This test creates two files with the same hash but different content,\n        then verifies that the storage can correctly differentiate them and\n        store them with unique hashed names.\n\n        \"\"\"\n        # Create two files with the same hash but different content\n        content1 = b'This is the first file.'\n        content2 = b'This is the second file.'\n        path1 = os.path.join(self.test_dir, 'colliding_file1.css')\n        path2 = os.path.join(self.test_dir, 'colliding_file2.css')\n        with open(path1, 'wb') as f:\n            f.write(content1)\n        with open(path2, 'wb') as f:\n            f.write(content2)\n\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n\n        # Collect static files\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        # Verify that the storage correctly hashed the files\n        self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, self.hashed_file_path('colliding_file1.css'))))\n        self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, self.hashed_file_path('colliding_file2.css'))))\n\n        # Assert that the hashed filenames are different\n        hashed_name1 = self.hashed_file_path('colliding_file1.css')\n        hashed_name2 = self.hashed_file_path('colliding_file2.css')\n        self.assertNotEqual(hashed_name1, hashed_name2)\n\n        self.assertPostCondition()\n"], "sample_1169": ["def test_simplify_index_permutations():\n\n    p, q, r, s = symbols('p q r s')\n    f = Function('f')\n    g = Function('g')\n\n    PermutList = [PermutationOperator(p,q),PermutationOperator(r,s)]\n    expr = f(p,r)*g(q,s) - f(q,r)*g(p,s) + f(q,s)*g(p,r) - f(p,s)*g(q,r)\n    assert simplify_index_permutations(expr,PermutList) == f(p, r)*g(q, s)*PermutationOperator(p, q)*PermutationOperator(r, s)\n\n", "def test_simplify_index_permutations_2():\n    p,q,r,s = symbols('p,q,r,s')\n    f = Function('f')\n    g = Function('g')\n    expr = f(p,q)*g(r,s) + f(q,p)*g(r,s)\n    PermutList = [PermutationOperator(p,q)]\n    simplifiedexpr = simplify_index_permutations(expr,PermutList)\n    assert simplifiedexpr == f(p, q)*g(r, s) + f(q, p)*g(r, s)*PermutationOperator(p, q)\n", "    def test_simplify_index_permutations_issue_23225():\n        from sympy import symbols\n        p, q, r = symbols('p q r')\n        f = Function(\"f\")\n        expr = f(p, r) * f(q, r) - f(q, r) * f(p, r)\n        permutations = [PermutationOperator(p, q)]\n        result = simplify_index_permutations(expr, permutations)\n        assert result == f(p, r) * f(q, r) * PermutationOperator(p, q) - f(q, r) * f(p, r)\n\n\n\n", "def test_simplify_index_permutations():\n    p, q, r, s = symbols('p q r s')\n    f = Function('f')\n    g = Function('g')\n\n    expr = f(p)*g(q) - f(q)*g(p)\n    assert simplify_index_permutations(expr,[PermutationOperator(p,q)]) ==\\\n           f(p)*g(q)*PermutationOperator(p, q)\n\n    PermutList = [PermutationOperator(p,q),PermutationOperator(r,s)]\n    expr = f(p,r)*g(q,s) - f(q,r)*g(p,s) + f(q,s)*g(p,r) - f(p,s)*g(q,r)\n    assert simplify_index_permutations(expr,PermutList) ==\\\n           f(p, r)*g(q, s)*PermutationOperator(p, q)*PermutationOperator(r, s)\n\n", "def test_simplify_index_permutations():\n    p,q,r,s = symbols('p,q,r,s')\n    f = Function('f')\n    g = Function('g')\n\n    expr = f(p)*g(q) - f(q)*g(p)\n\n    PermutList = [PermutationOperator(p,q)]\n\n    assert simplify_index_permutations(expr,PermutList) == \\\n        f(p)*g(q)*PermutationOperator(p, q)\n\n"], "sample_304": ["    def test_prohibit_null_characters_validator_with_message(self):\n        validator = ProhibitNullCharactersValidator(message='Null characters are not allowed.')\n        with self.assertRaisesMessage(ValidationError, 'Null characters are not allowed.'):\n            validator('\\x00something')\n\n", "    def test_prohibit_null_characters_validator_with_message(self):\n        validator = ProhibitNullCharactersValidator(message='Null characters are not allowed in filenames.')\n        with self.assertRaisesMessage(ValidationError, 'Null characters are not allowed in filenames.'):\n            validator('\\x00something')\n\n", "    def test_validate_ipv46_address_with_empty_string(self):\n        validator = validate_ipv46_address\n        with self.assertRaises(ValidationError):\n            validator('')\n", "    def test_validate_image_file_extension_with_pillow_not_installed(self):\n        with mock.patch.object(types, 'SimpleNamespace', return_value=None):\n            with self.assertRaises(ValidationError):\n                validate_image_file_extension(ContentFile('contents', name='file.jpg'))\n", "    def test_prohibit_null_characters(self):\n        validator = ProhibitNullCharactersValidator()\n        with self.assertRaisesRegex(ValidationError, 'Null characters are not allowed.'):\n            validator(None, null_characters='\\x00')\n        \n\n"], "sample_423": ["    def test_knight_rabbit_unique_together(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        with pytest.raises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_circular_foreign_key_model(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        with self.assertRaises(models.IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_unique_together_with_circular_fk(self):\n        with self.assertRaisesRegex(\n            exceptions.ValidationError, \".*must be unique.*\"\n        ):\n            rabbit = Rabbit(knight_id=1, parent_id=1)\n            rabbit.full_clean()\n", "    def test_rabbit_circular_fk_unique_together(self):\n        knight = Knight.objects.create()\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight.rabbit_set.create(knight=knight))\n"], "sample_898": ["    def test_label_ranking_average_precision_score_multilabel(self):\n        # Test label ranking average precision score with multilabel data\n        n_samples = 10\n        n_labels = 5\n\n        # Generate random binary labels\n        y_true = np.random.randint(0, 2, size=(n_samples, n_labels))\n        # Generate random scores\n        y_score = np.random.rand(n_samples, n_labels)\n\n        # Compute label ranking average precision score\n        lr_ap_score = label_ranking_average_precision_score(y_true, y_score)\n\n        # Check that the score is between 0 and 1\n        assert_greater(lr_ap_score, 0)\n        assert_less_equal(lr_ap_score, 1)\n\n", "    def test_average_precision_score_empty_prediction(self):\n        y_true = np.array([0, 0, 1, 1])\n        y_score = np.array([])\n        with pytest.raises(ValueError):\n            average_precision_score(y_true, y_score)\n", "    def test_label_ranking_average_precision_score_ties(self):\n        y_true = np.array([[1, 0, 0], [0, 1, 0]])\n        y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n        expected_score = 0.5\n        score = label_ranking_average_precision_score(y_true, y_score)\n        assert_almost_equal(score, expected_score)\n", "    def test_sparse_multilabel_classification_metrics(self):\n        random_state = check_random_state(42)\n        n_samples = 50\n        n_labels = 10\n        X = sp.csr_matrix(random_state.rand(n_samples, n_labels))\n        y = LabelBinarizer().fit_transform(random_state.randint(0, 2,\n                                                                size=(n_samples,\n                                                                      n_labels)))\n\n        # Test multilabel metrics with sparse input\n        for metric_name in MULTILABELS_METRICS:\n            metric = ALL_METRICS[metric_name]\n            if metric_name in METRICS_WITH_POS_LABEL :\n                # TODO: Fix pos_label support in multilabel metrics\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    score = metric(y, y, pos_label=1)\n            else:\n                score = metric(y, y)\n            assert_true(np.isfinite(score))\n", "    def test_label_ranking_average_precision_score_empty_labels(self):\n        y_true = np.zeros((0, 3))\n        y_score = np.zeros((0, 3))\n        score = label_ranking_average_precision_score(y_true, y_score)\n        assert_equal(score, 0.0)\n"], "sample_996": ["def test_product_factorial_general():\n    assert product(k, (k, 1, n)) == factorial(n)\n", "    def test_product_empty_product():\n        assert product(1, (n, 5, 4)) == 1\n", "    def test_product_with_symbolic_limits():\n        # Test product with symbolic limits\n        p = Product(x, (k, n, m))\n\n        assert p.doit(deep=False) == Product(x, (k, n, m))\n        assert simplify(p.doit()) == x**(m - n)  \n\n", "def test_product_rewrite_as_Sum():\n    assert Product(exp(x), (x, 1, 5))._eval_rewrite_as_Sum() == exp(Sum(log(exp(x)), (x, 1, 5)))\n", "def test_product_empty():\n    assert product(1, (k, 5, 4)) == 1\n    assert product(k, (k, 5, 4)) == 1\n"], "sample_681": ["    def test_log_cli_level_overrides_log_level(\n        self, testdir: Testdir, capsys: CaptureFixture", "    def test_log_level_setting_with_ini_and_cli(\n        self, testdir: Testdir, capsys: CaptureFixture", "    def test_log_cli_level_overrides_log_level(self, testdir: Testdir) -> None:\n        p = testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n            \n            logging.basicConfig(level=logging.DEBUG)\n            \n                logging.info(\"This is an info log\")\n                \n            pytest.main([\"-v\", \"--log-cli-level=WARNING\", __file__])\n        \"\"\"\n        )\n        result = testdir.runpytest(p)\n\n        assert result.ret == ExitCode.OK\n        assert result.stdout.lines == [\n            \"*collecting...\",\n            \"*collected 1 item*\",\n            \"*test_function* PASSED*\",\n        ]\n\n        assert \"This is an info log\" not in result.stdout.str()\n\n", "    def test_log_level_not_string(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.debug(\"This is a debug message\")\n        \"\"\"\n        )\n        # Set a log level that is not a string\n        testdir.runpytest(\"--log-level\", \"30\")\n        assert testdir.stderr.str().find(\"not recognized as a logging level\") > 0\n\n\n\n", "    def test_log_cli_level_overrides_log_level(\n        self, testdir: Testdir, caplog: CaptureFixture"], "sample_20": ["    def test_read_table_fits_empty_table(self, tmpdir):\n        \"\"\"Test reading an empty table.\"\"\"\n        # Create an empty table\n        table = Table()\n\n        # Write table to a FITS file\n        filename = str(tmpdir / \"empty_table.fits\")\n        write_table_fits(table, filename)\n\n        # Read the table back in\n        read_table = read_table_fits(filename)\n\n        # Assert that the tables are the same\n        assert read_table.colnames == []\n        assert len(read_table) == 0\n\n\n\n", "    def test_write_table_fits_append_existing(self, tmpdir):\n        filename = str(tmpdir.join(\"test.fits\"))\n        hdu = table_to_hdu(Table(self.data))\n        hdu.writeto(filename)\n\n        # Append to the existing file\n        new_data = np.array(\n            list(zip([5, 6], [\"e\", \"f\"], [10.1, 12.3])),\n            dtype=[(\"a\", int), (\"b\", \"U1\"), (\"c\", float)],\n        )\n\n        new_table = Table(new_data)\n        write_table_fits(new_table, filename, append=True)\n\n        # Read back the appended file\n        with fits.open(filename) as hdul:\n            assert len(hdul) == 2\n            assert equal_data(hdul[1].data, new_data)", "    def test_write_read_masked_column(self):\n        t = Table(\n            {\n                \"a\": [1, 2, 3, 4],\n                \"b\": [\"a\", \"b\", \"c\", \"d\"],\n                \"c\": [2.3, 4.5, 6.7, 8.9],\n                \"d\": [True, False, True, True],\n            },\n            masked=True,\n        )\n        t[\"c\"][1] = np.ma.masked\n\n        hdulist = fits.open(get_pkg_data_filename(\"data/mask.fits\"))\n        table = hdulist[1]\n\n        t[\"c\"].mask = table.data[\"c\"].mask\n        hdulist.close()\n\n        tmp = get_temp_file()\n        t.write(tmp, format=\"fits\", overwrite=True)\n        t2 = Table.read(tmp, format=\"fits\")\n\n        assert_array_equal(t[\"a\"], t2[\"a\"])\n        assert_array_equal(t[\"b\"], t2[\"b\"])\n        assert_array_equal(t[\"d\"], t2[\"d\"])\n        assert_array_equal(t[\"c\"].filled(), t2[\"c\"].filled())\n        assert_array_equal(t[\"c\"].mask, t2[\"c\"].mask)\n\n        os.remove(tmp)\n\n", "    def test_write_read_table_with_masked_columns(self):\n        \"\"\"Test that masked columns are correctly handled when reading and writing\n        tables.\n        \"\"\"\n        # Create a table with a masked column\n        t = Table(self.data)\n        t[\"c\"][1] = np.nan\n\n        # Write the table to a FITS file\n        tmpfile = get_pkg_data_filename(\"temp.fits\")\n        write_table_fits(t, tmpfile)\n\n        # Read the table back in\n        t2 = read_table_fits(tmpfile)\n\n        # Check that the data is the same\n        assert_array_equal(t[\"a\"], t2[\"a\"])\n        assert_array_equal(t[\"b\"], t2[\"b\"])\n\n        # Check that the masked column is the same\n        assert np.isnan(t2[\"c\"][1])\n        assert_allclose(t[\"c\"], t2[\"c\"], equal_nan=True)\n\n        # Clean up the temporary file\n        os.remove(tmpfile)\n", "    def test_write_table_fits_mask_invalid(self, tmpdir):\n        # Regression test for issue #8991\n        table = Table(self.data, masked=True)\n        table[\"c\"][1] = np.nan\n        filename = str(tmpdir / \"test.fits\")\n        write_table_fits(table, filename)\n        table_out = read_table_fits(filename)\n        assert_array_equal(table_out[\"a\"], self.data[\"a\"])\n        assert_array_equal(table_out[\"b\"], self.data[\"b\"])\n        assert_array_equal(table_out[\"c\"].copy(), self.data[\"c\"])\n"], "sample_168": ["    def test_remove_stale_content_types_interactive_no(self):\n        \"\"\"\n        Test that stale content types are not deleted when interactive is True\n        and the user answers 'no' to the confirmation prompt.\n        \"\"\"\n        with captured_stdout() as stdout:\n            call_command(\n                'remove_stale_contenttypes',\n                interactive=True,\n                verbosity=2\n            )\n        self.assertIn(\"Stale content types remain.\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n", "    def test_remove_stale_contenttypes_interactive_no(self):\n        self.app_config.models = {}  # Simulate the app being uninstalled\n        with captured_stdout() as stdout, mock.patch('builtins.input', return_value='no'):\n            call_command('removestale', database='default', interactive=True, verbosity=2)\n        self.assertIn(\"Stale content types remain.\", stdout.getvalue())\n        self.assertEqual(ContentType.objects.count(), self.before_count + 1)\n\n", "    def test_delete_stale_content_types_when_interactive(self):\n        # Mock user input to simulate 'yes' response.\n        with mock.patch('builtins.input', return_value='yes'):\n            with captured_stdout() as stdout:\n                call_command('remove_stale_contenttypes', interactive=True)\n            self.assertIn('Deleting stale content type', stdout.getvalue())\n            self.assertEqual(ContentType.objects.count(), self.before_count)\n\n", "    def test_remove_stale_content_types_interactive(self):\n        # Create a stale content type from an uninstalled app.\n        with mock.patch('django.apps.apps.get_app_configs', return_value=self.available_apps[:-1]):\n            stale_content_type = ContentType.objects.create(\n                app_label='no_models',\n                model='Fake',\n            )\n\n        with captured_stdout() as stdout, mock.patch('builtins.input', return_value='yes'):\n            call_command('removestalecontenttypes', database='default', interactive=True,\n                         include_stale_apps=True, verbosity=2)\n            output = stdout.getvalue()\n\n        self.assertIn('Some content types in your database are stale and can be deleted.', output)\n        self.assertIn(f'    - Content type for no_models.Fake', output)\n        self.assertNotIn(stale_content_type.pk, [ct.pk for ct in ContentType.objects.all()])\n\n", "    def test_remove_stale_content_types_interactive(self, mock_stdout):\n        \"\"\"\n        Test the interactive removal of stale content types.\n        \"\"\"\n        self.app_config = apps.get_app_config('empty_models')\n        mock_input = mock.MagicMock()\n        mock_input.return_value = 'yes'\n        with captured_stdout() as out, mock.patch('builtins.input', mock_input):\n            call_command('remove_stale_contenttypes', database='default', interactive=True, verbosity=0)\n        self.assertEqual(ContentType.objects.count(), self.before_count)\n        output = out.getvalue()\n        self.assertIn(\"Some content types in your database are stale and can be deleted.\", output)\n\n"], "sample_401": ["    def test_formset_factory_absolute_max(self):\n        formset_cls = formset_factory(\n            FavoriteDrinkForm,\n            extra=5,\n            absolute_max=3,\n        )\n        formset = formset_cls(data={\"form-0-name\": \"Coke\", \"form-1-name\": \"Pepsi\"})\n        self.assertEqual(formset.errors, [])\n        self.assertTrue(formset.is_valid())\n\n        formset = formset_cls(data={\"form-0-name\": \"Coke\"})\n        self.assertEqual(formset.errors, [])\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 1)\n\n        formset = formset_cls(data={\"form-0-name\": \"Coke\", \"form-1-name\": \"Pepsi\", \"form-2-name\": \"Sprite\", \"form-3-name\": \"Fanta\"})\n        self.assertEqual(formset.errors, [])\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 4)\n\n        formset = formset_cls(\n            data={\n                \"form-0-name\": \"Coke\",\n                \"form-1-name\": \"Pepsi\",\n                \"form-2-name\": \"Sprite\",\n                \"form-3-name\": \"Fanta\",\n            }\n        )\n        self.assertEqual(len(formset.errors), 1)\n        self.assertEqual(formset.errors[0].message(), \"Please submit at most 3 forms.\")\n", "    def test_formset_with_non_form_error(self):\n        formset = ChoiceFormsetWithNonFormError({\n            'choices-TOTAL_FORMS': '1',\n            'choices-INITIAL_FORMS': '0',\n            'choices-0-choice': 'Choice 1',\n            'choices-0-votes': '1',\n        })\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.non_form_errors()), 1)\n        self.assertEqual(formset.non_form_errors()[0].message, 'non-form error')\n", "    def test_formset_with_non_form_errors(self):\n        formset = ChoiceFormsetWithNonFormError({\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-choice\": \"test\",\n            \"form-0-votes\": \"1\",\n        })\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.non_form_errors().as_ul(), '<ul class=\"errorlist\"><li>non-form error</li></ul>')\n", "    def test_formset_factory_with_custom_kwargs(self):\n        formset = formset_factory(CustomKwargForm, extra=1)\n        instance = formset(\n            {},\n            custom_kwarg='test',\n        )\n        self.assertEqual(instance.forms[0].custom_kwarg, 'test')\n", "    def test_formset_factory_with_absolute_max(self):\n        class MyForm(Form):\n            field = CharField()\n\n        formset = formset_factory(\n            MyForm,\n            max_num=10,\n            absolute_max=5,\n        )\n\n        formset_instance = formset(data={\n            'form-0-field': 'foo',\n            'form-1-field': 'bar',\n            'form-2-field': 'baz',\n            'form-3-field': 'qux',\n            'form-4-field': 'quux',\n            'form-5-field': 'corge',\n\n            TOTAL_FORM_COUNT: 6,\n        })\n        self.assertFalse(formset_instance.is_valid())\n\n        self.assertEqual(formset_instance.errors, [\n            ErrorList([ValidationError('Please submit at most 5 forms.', code='too_many_forms')]),\n\n        ])\n\n\n\n"], "sample_1059": ["def test_laguerre_poly():\n    n = Symbol('n', integer=True)\n    assert laguerre_poly(n, x).subs(n, 0) == 1\n    assert laguerre_poly(n, x).subs(n, 1) == 1 - x\n    assert laguerre_poly(n, x).subs(n, 2) == x**2/2 - 2*x + 1\n    assert laguerre_poly(n, x).subs(n, 3) == -x**3/6 + 3*x**2/2 - 3*x + 1\n\n    assert diff(laguerre_poly(n, x), x) == -assoc_laguerre(n - 1, 1, x)\n    assert diff(laguerre_poly(n, x), x).subs(n, 0) == 0\n\n    # Test for negative values of n\n    assert laguerre_poly(-1, x) == exp(x) * laguerre_poly(0, -x)\n\n    # Test for symbolic n\n    assert laguerre_poly(n, 0) == binom(n, 0)\n\n", "    def test_assoc_laguerre_derivative_alpha():\n        n = Symbol('n', integer=True)\n        alpha = Symbol('alpha')\n        assert diff(assoc_laguerre(n, alpha, x), alpha) == \\\n            Sum(assoc_laguerre(_k, alpha, x)/(-alpha + n), (_k, 0, n - 1))\n", "def test_assoc_laguerre_diff_alpha():\n    n, alpha, x = Symbol('n'), Symbol('alpha'), Symbol('x')\n    assert diff(assoc_laguerre(n, alpha, x), alpha) == Sum(assoc_laguerre(k, alpha, x)/(n - alpha), (k, 0, n - 1))\n", "def test_assoc_laguerre_diff():\n    n = Symbol('n', integer=True)\n    alpha = Symbol('alpha')\n    assert diff(assoc_laguerre(n, alpha, x), x) == -assoc_laguerre(n - 1, alpha + 1, x)\n\n", "def test_assoc_laguerre_diff_alpha():\n    n, alpha = symbols('n alpha')\n    assert diff(assoc_laguerre(n, alpha, x), alpha) == Sum(assoc_laguerre(k, alpha, x)/(n - alpha), (k, 0, n - 1))\n    assert diff(assoc_laguerre(2, alpha, x), alpha) == assoc_laguerre(0, alpha, x)/(2 - alpha) + assoc_laguerre(1, alpha, x)/(1 - alpha)\n\n"], "sample_420": ["    def test_custom_error_message(self):\n        form = CustomErrorMessageForm(data={'name1': 'invalid'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['name1'], ['Form custom error message.']\n        )\n", "    def test_custom_error_messages(self):\n        form = CustomErrorMessageForm({'name1': 'invalid'})\n        self.assertEqual(\n            form.errors['name1'], ['Form custom error message.']\n        )\n", "    def test_modelform_with_media(self):\n        form = ModelFormWithMedia()\n        self.assertEqual(form.Media.js, (\"/some/form/javascript\",))\n        self.assertEqual(form.Media.css, {\"all\": (\"/some/form/css\",)})\n", "    def test_invalid_model_field_exclusion(self):\n        with self.assertRaises(ImproperlyConfigured):\n            class InvalidExclusionForm(forms.ModelForm):\n                class Meta:\n                    model = CustomFieldForExclusionModel\n                    exclude = ['does_not_exist']\n", "    def test_modelform_with_custom_error_message(self):\n        form = CustomErrorMessageForm({'name1': 'invalid-data'})\n        self.assertEqual(form.errors, {'name1': ['Form custom error message.']})\n"], "sample_10": ["    def test_set_column_slice(self):\n        t = self.t\n        t['a'][0:1] = [10]\n        assert_array_equal(t['a'], np.array([10, 2, 3]))\n        t['a'][-1:] = [11]\n        assert_array_equal(t['a'], np.array([10, 2, 11]))\n\n", "    def test_assign_column_with_same_name(self):\n        t = self.t.copy()\n        t['a'] = self.c\n        assert_array_equal(t['a'], self.c)\n        assert t['a'].info.name == 'a'\n\n\n\n", "    def test_set_column_with_index(self):\n        t = self.t\n        t['e'] = self.c\n        assert t.colnames == ['a', 'b', 'e']\n\n\n", "    def test_set_column_masked_copy_indices(self):\n        t = self.t.copy()\n        old_indices = t['a'].info.indices.copy()\n        t['a'] = self.a.copy(copy_indices=False)\n        assert t['a'].info.indices is old_indices\n", "    def test_set_column_copy(self):\n        t = self.t.copy()\n        t['a'] = self.a.copy()\n        assert t['a'] is not self.a\n        t['a'][:] = self.a[:]\n        assert t['a'] is not self.a\n\n"], "sample_53": ["    def test_empty_value_with_required_and_not_required(self):\n        required_form = RequiredBandForm()\n        self.assertHTMLEqual(\n            str(required_form['band']),\n            f'<select name=\"band\" required class=\"form-control autocomplete\" required=\"\">{self.empty_option}<option value=\"1\">Band A</option><option value=\"2\">Band B</option></select>',\n        )\n        not_required_form = NotRequiredBandForm()\n        self.assertHTMLEqual(\n            str(not_required_form['band']),\n            f'<select name=\"band\" class=\"form-control autocomplete\" >{self.empty_option}<option value=\"1\">Band A</option><option value=\"2\">Band B</option></select>',\n        )\n", "    def test_required_field_with_no_value(self):\n        form = RequiredBandForm()\n        self.assertEqual(\n            form.as_p(),\n            '<p><label for=\"id_band\">Band:</label> <select name=\"band\" id=\"id_band\" required=\"required\"></select></p>'\n        )\n\n", "    def test_empty_select_with_empty_label(self):\n        form = NotRequiredBandForm()\n        expected_html = (\n            '<select name=\"band\" id=\"id_band\" class=\"my-class\">'\n            self.empty_option\n            '</select>'\n        )\n        self.assertHTMLEqual(form.as_p(), expected_html)\n\n", "    def test_required_field_autocomplete_select_widget_renders_required_attribute(self):\n        form = RequiredBandForm()\n        html = form.as_p()\n        self.assertIn('<select required', html)\n\n", "    def test_autocomplete_select_required_field(self):\n        form = RequiredBandForm()\n        self.assertEqual(str(form), f'<form method=\"post\"><div><label for=\"id_band\">Band:</label><select name=\"band\" id=\"id_band\" required class=\"autocomplete select2\"><option value=\"\"></option></select></div><button type=\"submit\">Submit</button></form>')\n        form.data = {'band': ''}\n        self.assertFalse(form.is_valid())\n"], "sample_1110": ["    def test_print_Identity(self):\n        a = Identity(2)\n        for printer in (PythonCodePrinter, NumPyPrinter, SciPyPrinter):  \n            self.assertEqual(printer().doprint(a), (\n                'numpy.eye(2)' if isinstance(printer, NumPyPrinter) or isinstance(printer, SciPyPrinter) \n                else 'sympy.Identity(2)'\n            ))\n", "    def test_sympy_matrix_solve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        x = MatrixSymbol('x', 2, 1)\n        eq = Eq(A*x, b)\n        sol = MatrixSolve(eq, x)\n        code = SymPyPrinter().doprint(sol)\n        assert code == 'solve((A*x), x, dict=True)[x]'\n", "    def test_IndexedBase(self):\n        p = IndexedBase(\"p\")\n        self.assertEqual(str(PythonCodePrinter().doprint(p[0])), \"p[0]\")\n", "    def test_Piecewise_evalf(self):\n        a = symbols('a')\n        expr = Piecewise((1, a < 0), (2, True))\n        code = SciPyPrinter().doprint(expr.evalf(subs={a:-1}))\n        assert code == '1'\n", "    def test_MatrixSymbol(self):\n        A = MatrixSymbol(\"A\", 2, 2)\n        self.assertEqual(pycode(A), \"MatrixSymbol('A', 2, 2)\")\n\n        # TODO: Add tests for other matrix types and properties\n"], "sample_923": ["def test_function_id_attributes():\n    check(\n        'function',\n        'void myfunc(int a{id_attr} = 5, int b[10]{paren_attr})',\n        {1: 'myfunc-int-int'},\n        key='myfunc',\n        asTextOutput='myfunc(int a{id_attr} = 5, int b[10]{paren_attr})',\n    )\n", "def test_function_template():\n    check(\n        'function',\n        'template<typename T> void foo(T t);',\n        {1: 'function:c:foo'},\n        'void foo(T t);',\n    )\n", "    def test_namespace_resolve(self):\n        rootSymbol = Symbol(None, None, None, None)\n        rootSymbol.add_name(cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"A\")]))\n        rootSymbol.add_name(cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"B\")]))\n        aSymbol = rootSymbol.direct_lookup(cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"A\")]).get_lookup_key())\n        assert aSymbol\n        bSymbol = rootSymbol.direct_lookup(cppDomain.ASTNestedName([cppDomain.ASTIdentifier(\"B\")]).get_lookup_key())\n        assert bSymbol\n        assert aSymbol != bSymbol\n        aSymbol.add_declaration(cppDomain.ASTDeclaration(\"function\", cppDomain.ASTIdentifier(\"a_func\"), \"void a_func()\", None),\n                                docname=\"TestDoc\")\n        bSymbol.add_declaration(cppDomain.ASTDeclaration(\"function\", cppDomain.ASTIdentifier(\"b_func\"), \"void b_func()\", None),\n                                docname=\"TestDoc\")\n\n        env = docutils.new_document()\n        env.temp_data['c:parent_symbol'] = rootSymbol\n        env.ref_context['c:parent_key'] = rootSymbol.get_lookup_key()\n\n        # Test resolving A::a_func\n        retnode, objtype = cppDomain._resolve_xref_inner(env, \"TestDoc\", None,\n                                                      'any', \"A::a_func\", None, None)\n", "compilation error", "def test_cpp_enum_scoped():\n    check(\n        'enum',\n        'enum MyEnum {{ VALUE_A, VALUE_B }};',\n        {1: 'myenum', 2: 'myenum'},\n        'enum MyEnum { VALUE_A, VALUE_B };',\n    )\n"], "sample_1031": ["def test_quantity_arithmetic():\n    assert (2*m).units == m\n    assert (2*m + 3*m).units == m\n    assert (2*m - 3*m).units == m\n    assert (2*m * 3).units == m\n\n    assert (2*kg).units == kg\n    assert (2*kg + 3*kg).units == kg\n    assert (2*kg - 3*kg).units == kg\n    assert (2*kg * 3).units == kg\n\n    assert (2*s).units == s\n    assert (2*s + 3*s).units == s\n    assert (2*s - 3*s).units == s\n    assert (2*s * 3).units == s\n\n    assert (10 * m/s).units == velocity\n\n    assert (m*kg/s**2).units == velocity * mass / time\n\n    assert (4*m/s + 5*m/s).units == velocity\n    assert (4*m/s - 5*m/s).units == velocity\n\n", "def test_quantity_units():\n    assert Quantity('meter') == m\n    assert Quantity('kilogram').scale_factor == Rational(1, 1)\n    assert Quantity('second').dimension == time\n    assert Quantity('meter/second').dimension == velocity\n    assert Quantity('meter').abbreviation == 'm'\n    assert Quantity('kilometer').name == 'kilometer'\n    assert Quantity('kilogram').symbol_name == 'kg'\n    assert Quantity('meter').get_unit_system() is None\n    assert Quantity('meter', UnitSystem(\"my_system\")).get_unit_system() == UnitSystem(\"my_system\")\n\n", "def test_quantity_dimension():\n    assert Quantity(\"meter\").dimension == length\n    assert Quantity(\"kilogram\").dimension == mass\n    assert Quantity(\"second\").dimension == time\n    assert Quantity(\"meter/second\").dimension == velocity\n    assert Quantity(\"meter*kilogram/second**2\").dimension == force\n\n", "def test_quantity_dimension():\n    assert Quantity('meter').dimension == length\n    assert Quantity('kilogram').dimension == mass\n    assert Quantity('second').dimension == time\n    assert Quantity('ampere').dimension == current\n\n    assert Quantity('meter').get_dimension_system() is None\n    assert Quantity('meter', dimsys=UnitSystem.SI).get_dimension_system() is UnitSystem.SI\n    assert Quantity('meter/second').dimension == velocity\n\n    assert Quantity('meter**2').dimension == length**2\n\n    assert Quantity('meter*second').dimension == length * time\n", "    def test_quantity_addition():\n        x = Quantity('1 meter')\n        y = Quantity('2 meters')\n        assert x + y == Quantity('3 meters')\n\n        with warns_deprecated_sympy():\n            x = Quantity('1 m')\n            y = Quantity('2 m')\n            assert x + y == Quantity('3 m')\n"], "sample_284": ["    def test_url_converter_with_relative_path(self):\n        \"\"\"\n        Test that url_converter correctly handles relative paths within the\n        hashed file.\n        \"\"\"\n        with self.settings(STATIC_URL='/static/'):\n            finders.finders.append(finders.AppDirectoriesFinder(self.apps))\n            test_path = os.path.join('my_app', 'css', 'style.css')\n            self.copy_file(\n                os.path.join('my_app', 'css', 'style.css'),\n                os.path.join(TEST_ROOT, 'my_app', 'css', 'style.css'),\n            )\n            self.copy_file(\n                os.path.join('my_app', 'css', 'fonts', 'myfont.woff'),\n                os.path.join(TEST_ROOT, 'my_app', 'css', 'fonts', 'myfont.woff'),\n            )\n\n            # Add a relative path reference within the CSS file\n            with open(\n                os.path.join(TEST_ROOT, 'my_app', 'css', 'style.css'), 'a'\n            ) as f:\n                f.write(f\"\"\"@font-face {{", "    def test_manifest_strict_mode_missing_entry(self):\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n        settings.STATICFILES_DIRS = [TEST_ROOT / 'staticfiles_test']\n\n        with override_settings(DEBUG=False):\n            with self.assertRaises(ValueError) as cm:\n                call_command('collectstatic', interactive=False, verbosity=0)\n            self.assertEqual(\n                str(cm.exception),\n                \"Missing staticfiles manifest entry for 'css/style.css'\",\n            )\n\n", "    def test_hashed_file_processing_with_multiple_patterns(self):\n        \"\"\"Ensure multiple patterns are processed correctly.\"\"\"\n        # Create test files with different extensions and patterns.\n        css_file = os.path.join(self.static_dir, 'style.css')\n        js_file = os.path.join(self.static_dir, 'script.js')\n        with open(css_file, 'w') as f:\n            f.write(\"\"\"\n                @import url(\"subfolder/my-font.woff2\");\n                body {\n                    background: url(\"image.png\") no-repeat;\n                }\n                \"\"\")\n        with open(js_file, 'w') as f:\n            f.write(\"\"\"\n                const img = new Image();\n                img.src = 'image.png';\n            \"\"\")\n\n        # Mock the 'finders' module to return our test files.\n        with mock.patch('django.contrib.staticfiles.finders.find') as mocked_find:\n            mocked_find.return_value = [css_file, js_file]\n            # Run collectstatic\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n        # Verify the hashed files exist.\n        hashed_css_file = self.hashed_file_path('style.css')\n        hashed_js_file = self.hashed_file_path('script.js')\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir, hashed_css_file)))\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir, hashed_js_file)))\n\n        # Verify that the CSS import and image URL are hashed correctly.\n        with open(os.path.join(self.output_dir, hashed_css_file), 'r') as f:\n            css_content = f.read()\n            self.assertIn(\n                'url(\"%s\")' % self.hashed_file_path('subfolder/my-font.woff2'),\n                css_content\n            )\n            self.assertIn(\n                'url(\"%s\")' % self.hashed_file_path('image.png'),\n                css_content", "    def test_hashed_file_with_fragment(self):\n        path = 'css/style.css#my-fragment'\n        self.write_file(path, 'body { color: red; }')\n        call_command('collectstatic', interactive=False, verbosity=0, exclude=['another_app'])\n        hashed_path = self.hashed_file_path(path)\n        self.assertEqual(\n            self.read_file(hashed_path), 'body { color: red; }',\n        )\n        self.assertTrue(hashed_path.endswith('#my-fragment'))\n        self.assertPostCondition()\n", "    def test_hashed_file_with_absolute_url_in_css(self):\n        content = \"\"\"\n        @import url(\"https://example.com/style.css\");\n        \"\"\"\n        self.create_static_file(\n            'style.css', content\n        )\n        self.assertFileExists('style.css')\n        output = StringIO()\n        with override_settings(\n            STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage',\n            STATIC_URL='/static/',\n        ):\n            call_command('collectstatic', stdout=output)\n        output.seek(0)\n        self.assertIn('Processing style.css', output.read())\n        self.assertFileExists('style.css')\n\n"], "sample_426": ["    def test_timesince_past_date_with_timezone(self):\n        with translation.override('en'):\n            d = timezone.make_aware(datetime.datetime(2007, 8, 10, 12, 0, 0), timezone.utc)\n            now = timezone.make_aware(datetime.datetime(2007, 8, 14, 12, 0, 0), timezone.utc)\n            self.assertEqual(timesince(d, now), \"4 days\")\n", "    def test_timesince_across_daylight_saving(self):\n        # Ensure timesince handles datetime objects with different timezones correctly\n        with translation.override('en'):\n            with timezone.override('US/Eastern'):\n                d1 = datetime.datetime(2023, 11, 5, 1, 0, 0)\n                d2 = datetime.datetime(2023, 11, 5, 2, 0, 0)\n                self.assertEqual(timesince(d1, d2), \"1 hour\")\n", "    def test_timesince_future(self):\n        # Test that timesince outputs \"0 minutes\" when the future date is passed\n        future = self.t + self.oneyear\n        self.assertEqual(timesince(future, self.t), gettext(\"0 minutes\"))\n\n", "    def test_timesince_with_timezone_aware_dates(self):\n        with translation.override(\"en-us\"):\n            now = timezone.now()\n            past = now - self.oneday\n            self.assertEqual(timesince(past), \"1 day\")\n            self.assertEqual(timesince(past, now=now.replace(tzinfo=None)), \"1 day\")\n            past = past.replace(tzinfo=timezone.utc)\n            self.assertEqual(timesince(past, now=now.replace(tzinfo=timezone.utc)), \"1 day\")\n", "    def test_timesince_across_years_tz(self):\n        with translation.override('en', deactivate=True):\n            with timezone.override(timezone.get_current_timezone()):\n                d = datetime.datetime(2007, 8, 14, 13, 46, 0, tzinfo=timezone.utc)\n                now = timezone.now()\n                self.assertEqual(timesince(d, now), '1 year')\n"], "sample_938": ["    def test_man_pages_config(app, status, warning):\n        app.config.man_pages = [\n            ('index', 'myprogram', 'My Program', ['Me'], 1),\n            ('contents', 'myprogram-contents', 'My Program Contents', [], 8),\n        ]\n        app.build()\n        assert len(app.env.all_docs) == 2\n", "def test_man_pages_config(app, status, warning):\n    config = app.config\n    assert config.man_pages == default_man_pages(config)\n    assert not config.man_show_urls\n    assert not config.man_make_section_directory\n", "    def test_build_manpage_default(self, app, status, warning):\n        app.config.man_pages = default_man_pages(app.config)\n        app.build()\n        assert status.success\n        assert len(warning) == 0\n", "    def test_default_man_pages(app: Sphinx, status: Sphinx.BuildStatus) -> None:\n        config = Config()\n        config.master_doc = 'index'\n        config.project = 'Test Project'\n        config.release = '1.0'\n        config.author = 'Test Author'\n        default_man_pages(config)\n        assert config.man_pages == [\n            ('index', 'Test Project', 'Test Project 1.0', ['Test Author'], 1)\n        ]\n", "    def test_manpage_builder(app_tmpdir, status, config):\n        config.man_pages = default_man_pages(config)\n        app = app_tmpdir.sphinx(buildername='man', srcdir='test_proj', doctreedir='test_proj', confoverrides={'man_show_urls': True})\n        app.build()\n        with open(path.join(app.outdir, \"manpage.html\"), encoding='utf-8') as f:\n            content = f.read()\n        assert content.startswith(\"<html\")\n\n"], "sample_639": ["    def test_create_message_definition_from_tuple(self):\n        checker = OtherBasicChecker()\n        msgid = \"W0001\"\n        msg_tuple = (\n            \"Basic checker has an example.\",\n            \"basic-checker-example\",\n            \"Used nowhere and serves no purpose.\",\n        )\n        message_definition = checker.create_message_definition_from_tuple(msgid, msg_tuple)\n        self.assertEqual(message_definition.msgid, msgid)\n        self.assertEqual(message_definition.msg, msg_tuple[0])\n        self.assertEqual(message_definition.symbol, msg_tuple[1])\n        self.assertEqual(message_definition.description, msg_tuple[2])\n", "    def test_get_full_documentation(self):\n        checker = OtherBasicChecker()\n        expected_doc = (\n            \".. _basic:\\n\\n\"\n            \"Basic checker checker\\n\"\n            \"Verbatim name of the checker is `basic`.\\n\\n\"\n            \".. _Basic checker Documentation\\n\"\n            '\"Basic checker has an example.\"\\n\\n'\n            \".. _Basic checker Options\\n\"\n            \"\\n\"\n            \".. _Basic checker Messages\\n\"\n            \":W0001: Basic checker has an example. (basic-checker-example)\\n\"\n            \"Used nowhere and serves no purpose.\\n\\n\\n\"\n        )\n        self.assertEqual(checker.get_full_documentation(checker.msgs, checker.options_and_values(), checker.reports, doc='Basic checker has an example.', module='basic'), expected_doc)\n\n", "    def test_get_full_documentation_with_module(self):\n        checker = OtherBasicChecker()\n        doc = \"This checker is used for testing get_full_documentation.\"\n        expected_result = (\n            \".. _other_basic_checker:\\n\\n\"\n            \"Basic checker checker\\n\"\n            \"This checker is provided by ``pylint.checkers``.\\n\\n\"\n            \"Verbatim name of the checker is ``basic``.\\n\\n\"\n            \".. _Basic checker Documentation:\\n\"\n            f\"{cleandoc(doc)}\\n\\n\"\n            \".. _Basic checker Options:\\n\"\n            f\"{get_rst_section(None, [])}\\n\"\n            \".. _Basic checker Messages:\\n\"\n            \":W0001: Basic checker has an example. (basic-checker-example)\\n\\n\\n\"\n        )\n        self.assertEqual(\n            checker.get_full_documentation(\n                msgs=checker.msgs,\n                options=checker.options_and_values(),\n                reports=checker.reports,\n                doc=doc,\n                module=\"pylint.checkers\"\n            ),\n            expected_result,\n        )\n\n", "    def test_consistency(self):\n        checker = OtherBasicChecker()\n        checker.check_consistency()\n        checker.msgs[\"W0002\"] = (\n            \"Another message\",\n            \"basic-checker-example2\",\n            \"This one is different.\",\n        )\n        with self.assertRaises(InvalidMessageError) as context:\n            checker.check_consistency()\n        self.assertIn(\"Inconsistent checker part in message id\", str(context.exception))\n\n", "    def test_get_full_documentation(self):\n        checker = OtherBasicChecker()\n        doc = checker.get_full_documentation(\n            msgs=checker.msgs, options=checker.options_and_values(), reports=checker.reports\n        )\n        self.assertIn(\"Basic checker has an example.\", doc)\n        self.assertIn(\"Verbatim name of the checker is ``basic``.\", doc)\n        self.assertIn(\"'example-args'\", doc)\n        self.assertIn(\"``42``\", doc)\n"], "sample_1064": ["    def test_tensorflow_piecewise():\n        expr = Piecewise((x, x < 1), (x**2, True))\n        _compare_tensorflow_scalar([x], expr)\n\n        expr = Piecewise((x**2, x < 1), (sin(x), x < 2), (x**3, True))\n        _compare_tensorflow_scalar([x], expr)\n\n\n\n", "def test_tensorflow_pow():\n    x = symbols('x')\n    _compare_tensorflow_scalar([x], x**2)\n    _compare_tensorflow_scalar([x], x**3)\n    _compare_tensorflow_scalar([x], x**-1)\n", "    def test_tensorflow_piecewise():\n        x = Symbol('x')\n        expr = Piecewise((x**2, x > 0), (x, True))\n        f = lambdify(x, expr, 'tensorflow')\n        graph = tf.Graph()\n        with graph.as_default():\n            session = tf.compat.v1.Session(graph=graph)\n            r = session.run(f(2))\n            assert r == 4\n            r = session.run(f(-2))\n            assert r == -2\n\n\n\n", "def test_tensorflow_Piecewise():\n    x = Symbol('x')\n    pw = Piecewise((x**2, x < 0), (x, x >= 0))\n    _compare_tensorflow_scalar([x], pw)\n\n\n\n", "def test_tensorflow_code_piecewise():\n    expr = Piecewise((x, x < 0), (x**2, x >= 0))\n    f = lambdify(x, expr, 'tensorflow')\n    graph = tf.Graph()\n    with graph.as_default():\n        r = f(-1)\n        session = tf.compat.v1.Session(graph=graph)\n        assert session.run(r) == -1\n        r = f(1)\n        assert session.run(r) == 1\n"], "sample_316": ["    def test_get_image_dimensions_web(self):\n        if not Image:\n            self.skipTest('Pillow not installed')\n        if not HAS_WEBP:\n            self.skipTest('WEBP support not installed in Pillow')\n        with NamedTemporaryFile(suffix='.webp') as f:\n            f.write(b'RIFF\\x89WEBP\\x00\\x00\\x00\\x00VP8L')\n            f.seek(0)\n            width, height = images.get_image_dimensions(f)\n            self.assertEqual(width, 0)\n            self.assertEqual(height, 0)\n\n", "    def test_image_dimensions(self):\n        if Image is None:\n            return\n\n        with NamedTemporaryFile(suffix=\".jpg\") as temp:\n            temp.write(b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01...\\xff\\xd9\")\n            temp.seek(0)\n            file = File(temp)\n            img = images.ImageFile(file)\n            self.assertEqual(img.width, 100)\n            self.assertEqual(img.height, 100)\n", "    def test_image_file_dimensions(self):\n        if Image is None:\n            return\n\n        with NamedTemporaryFile(delete=True, suffix='.png') as tf:\n            im = Image.new('RGB', (100, 200))\n            im.save(tf.name)\n            tf.seek(0)\n            file = File(tf)\n            self.assertEqual(file.width, 100)\n            self.assertEqual(file.height, 200)\n", "    def test_image_file_dimensions(self):\n        if Image is None:\n            return\n\n        with NamedTemporaryFile(suffix='.jpg') as f:\n            img = Image.new('RGB', (100, 200))\n            img.save(f.name)\n            f.seek(0)\n\n            file = File(f)\n            self.assertEqual(file.width, 100)\n            self.assertEqual(file.height, 200)\n", "    def test_get_image_dimensions_with_zlib_error(self):\n        if Image is None:\n            return\n        with NamedTemporaryFile(suffix=\".png\") as f:\n            # Create a PNG file with a truncated stream\n            f.write(b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x01\\x08\\x06\\x00\\x00\\x00\\x9c\\x8b\\xd8')\n            f.flush()\n            f.seek(0)\n            width, height = images.get_image_dimensions(f)\n            self.assertEqual(width, None)\n            self.assertEqual(height, None)\n"], "sample_404": ["    def test_variable_with_empty_string_in_kwargs(self):\n        t = self._engine().from_string(\n            '{% filter upper %}{{ \"foo\" }}</% filter %}'\n        )\n        output = t.render(Context({}))\n        self.assertEqual(output, 'FOO')\n", "    def test_variable_with_filter(self):\n        t = Template(\"{% load my_filters %}{{ my_var|my_filter }}\")\n        c = Context({'my_var': 'test', 'my_filter': lambda v: v.upper()})\n        self.assertEqual(t.render(c), \"TEST\")\n", "    def test_variable_with_filters_in_for_loop(self):\n        engine = self._engine()\n        template_string = \"\"\"\n        {% for item in [1, 2, 3] %}\n            {{ item|add:1 }}\n        {% endfor %}\n        \"\"\"\n        template = engine.from_string(template_string)\n        context = Context({\"item\": 1})\n        output = template.render(context)\n        self.assertEqual(output, \"234\")\n", "    def test_variable_attribute_lookup(self):\n        t = Template('{{ foo.bar.baz }}')\n        c = {'foo': {'bar': {'baz': 'Hello'}}}\n        self.assertEqual(t.render(Context(c)), 'Hello')\n\n", "    def test_variable_node(self):\n        t = Template('{% if foo %}Hello, {{ foo }}!{% endif %}')\n        c = Context({'foo': 'World'})\n        self.assertEqual(t.render(c), 'Hello, World!')\n\n"], "sample_727": ["def test_imputer_sparse_fit_axis_1_all_missing():\n    X = sparse.csr_matrix([[np.nan, 0], [np.nan, np.nan]])\n    imputer = Imputer(strategy=\"mean\", axis=1)\n    assert_raises(ValueError, imputer.fit, X)\n", "    def test_most_frequent_sparse_csr(self):\n        # Check that most_frequent works correctly with sparse CSR matrices\n\n        rng = np.random.RandomState(0)\n        n_samples, n_features = 10, 5\n        X = sparse.csr_matrix(rng.randint(0, 20, size=(n_samples, n_features)))\n        X_missing = X.copy()\n        X_missing[np.random.choice(\n            X_missing.size,\n            replace=False,\n            size=int(X_missing.size * .2))\n        ] = np.nan\n        X_true = X.toarray().copy()\n        statistics = [\n            stats.mode(X_true[:, i])[0][0] for i in range(n_features)\n        ]\n        statistics = np.array(statistics)\n\n        _check_statistics(X_missing, X_true,\n                          \"most_frequent\", statistics, np.nan)\n", "    def test_imputation_with_sparse_matrix_and_zeros(self):\n        # This test checks if the imputer works correctly when\n        # the missing_values is 0 and the input is a sparse matrix\n        X = sparse.csc_matrix([[1, 0, 3],\n                              [4, 0, 6],\n                              [7, 8, 9]])\n        \n        X_true = X.copy()\n        X_true.data[X_true.data == 0] = 5  # Replace zeros with 5\n\n        _check_statistics(X, X_true, strategy=\"mean\",\n                          statistics=[5, 5, 5], missing_values=0)\n", "    def test_imputer_sparse_zero_missing_values(self):\n        # Test imputation on sparse matrices with zero as missing value.\n        X = sparse.csc_matrix([[1, 0, 3], [4, 0, 6], [7, 8, 9]])\n        X_true = X.copy()\n        missing_values = 0\n\n        # Mean\n        statistics = np.array([4., 4., 6.])\n        _check_statistics(X, X_true, \"mean\", statistics, missing_values)\n\n        # Median\n        statistics = np.array([4., 0., 6.])\n        _check_statistics(X, X_true, \"median\", statistics, missing_values)\n", "def test_imputer_axis1_all_missing():\n    # Test imputation when axis=1 and all values in a row are missing\n    X = np.array([[np.nan, np.nan],\n                  [np.nan, np.nan]])\n    imp = Imputer(strategy='mean', axis=1)\n    with assert_raises(ValueError):\n        imp.fit_transform(X)\n"], "sample_1167": ["def test_latex_transfer_function():\n    s = symbols('s')\n    tf1 = TransferFunction(s + 1, s**2 + 2*s + 1)\n    tf2 = TransferFunction(1, (s+1)*(s+2))\n    assert latex(tf1) == r'\\frac{s + 1}{s^{2} + 2 s + 1}'\n    assert latex(tf2) == r'\\frac{1}{\\left(s + 1\\right) \\left(s + 2\\right)}'\n\n    tf3 = tf1*tf2\n    assert latex(tf3) == r'\\frac{s + 1}{\\left(s^{2} + 2 s + 1\\right) \\left(s + 1\\right) \\left(s + 2\\right)}'\n", "def test_latex_vector_print():\n    # Test printing vectors\n    N = CoordSys3D('N')\n    v = N.i + 2*N.j + 3*N.k\n    assert latex(v) == r'\\mathbf{N}_x + 2 \\mathbf{N}_y + 3 \\mathbf{N}_z'\n    assert latex(v, mode='inline') == r'$\\mathbf{N}_x + 2 \\mathbf{N}_y + 3 \\mathbf{N}_z$'\n    assert latex(Cross(v, N.i)) == r\"\\mathbf{N}_y \\times \\mathbf{N}_x + 2 \\mathbf{N}_z \\times \\mathbf{N}_x + 3 \\mathbf{N}_y \\times \\mathbf{N}_z\"\n\n", "    def test_latex_permutationmatrix(self):\n        A = PermutationMatrix(Permutation((1, 2, 3)))\n        self.assertEqual(latex(A), r'\\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 0 \\end{pmatrix}')\n", "def test_latex_MIMOParallel():\n    a = MatrixSymbol('A', 2, 2)\n    b = MatrixSymbol('B', 2, 2)\n    sys1 = TransferFunctionMatrix(a)\n    sys2 = TransferFunctionMatrix(b)\n    system = MIMOParallel(sys1, sys2)\n    assert latex(system) == r'\\begin{pmatrix} \\frac{A_{1,1} }{1} & \\frac{A_{1,2} }{1} \\\\ \\frac{A_{2,1} }{1} & \\frac{A_{2,2} }{1} \\end{pmatrix} \\parallel \\begin{pmatrix} \\frac{B_{1,1} }{1} & \\frac{B_{1,2} }{1} \\\\ \\frac{B_{2,1} }{1} & \\frac{B_{2,2} }{1} \\end{pmatrix}'\n", "def test_latex_set():\n    assert latex(FiniteSet(1, 2, 3)) == '\\\\{1, 2, 3\\\\}'\n    assert latex(ImageSet(x, x**2 > 1)) == '\\\\left\\{x \\\\in \\\\mathbb{R} \\\\mid x^{2} > 1\\\\right\\}'\n    assert latex(Interval(1, 2, True, True)) == '\\\\left[1, 2\\\\right]'\n    assert latex(Interval(1, 2, True, False)) == '\\\\left[1, 2\\\\right)'\n    assert latex(Interval(1, 2, False, True)) == '\\\\left(1, 2\\\\right]'\n    assert latex(Interval(1, 2, False, False)) == '\\\\left(1, 2\\\\right)'\n    assert latex(Union(Interval(1, 2), Interval(3, 4))) == '\\\\left[1, 2\\\\right] \\\\cup \\\\left[3, 4\\\\right]'\n    assert latex(Intersection(Interval(1, 2), Interval(2, 3))) == '\\\\left[2, 2\\\\right]'\n    assert latex(Complement(Interval(1, 3), Interval(2, 3))) == '\\\\left[1, 2\\\\right)\\\\cup \\\\left(3\\\\right]'\n    assert latex(SymmetricDifference(Interval(1, 3), Interval(2, 4))) == '\\\\left[1, 2\\\\right) \\\\cup \\\\left(3, 4\\\\right]'\n    assert latex(ProductSet(Interval(1, 2), Interval(3, 4))) == '\\\\left[1, 2\\\\right] \\\\times \\\\left[3, 4\\\\right]'\n"], "sample_209": ["    def test_ordering_with_respect_to_nulls_last(self):\n        dept = Department.objects.create(name='Sales')\n        worker1 = Worker.objects.create(name='Alice', department=dept)\n        worker2 = Worker.objects.create(name='Bob', department=dept)\n        worker3 = Worker.objects.create(name='Charlie', department=None)\n\n        # Check that nulls are put at the end\n        workers = Worker.objects.all().order_by('_order')\n        self.assertEqual(list(workers), [worker1, worker2, worker3])\n", "    def test_model_unique_together(self):\n        dept1 = Department.objects.create(name='Dept 1')\n\n        # UniqueTogether constraint should prevent duplicate entries\n        worker1 = Worker.objects.create(name='John Doe', department=dept1, email='john@example.com')\n        with self.assertRaises(ValidationError):\n            Worker.objects.create(name='Jane Doe', department=dept1, email='john@example.com')\n\n        # Different email should be allowed\n        worker2 = Worker.objects.create(name='Jane Doe', department=dept1, email='jane@example.com')\n\n        # Different department should be allowed\n        dept2 = Department.objects.create(name='Dept 2')\n        Worker.objects.create(name='John Doe', department=dept2, email='john@example.com')\n", "    def test_get_full_name_returns_correct_string(self):\n        party = Party.objects.create(first_name='John', last_name='Doe')\n        self.assertEqual(Party.objects.get(pk=party.pk).get_full_name(), 'John Doe')\n\n", "    def test_check_constraints(self):\n        class CustomModel(models.Model):\n            name = models.CharField(max_length=100)\n            email = models.EmailField(unique=True, db_index=True)\n            age = models.IntegerField()\n\n        with self.assertRaisesMessage(\n            ValidationError,\n            \"Constraint 'CustomModel_email_unique' failed: 'email' must be unique.\"\n        ):\n            CustomModel.objects.create(name='Alice', email='alice@example.com', age=30)\n            CustomModel.objects.create(name='Bob', email='alice@example.com', age=25)\n", "    def test_order_with_respect_to_ordering(self):\n        party = Party.objects.create(name='Party 1')\n        worker1 = Worker.objects.create(name='Worker 1', party=party)\n        worker2 = Worker.objects.create(name='Worker 2', party=party)\n        worker3 = Worker.objects.create(name='Worker 3', party=party)\n\n        # Check initial order\n        workers = Worker.objects.all()\n        self.assertListEqual(list(workers), [worker1, worker2, worker3])\n\n        # Set a custom order\n        workers = Worker.objects.all().order_by('party')\n        worker_ids = [w.id for w in workers]\n        Worker.objects.set_party_order(party, worker_ids)\n\n        workers = Worker.objects.all()\n        self.assertListEqual(list(workers), [worker1, worker2, worker3])\n\n"], "sample_236": ["    def test_delete_with_restricted_objects_and_circular_dependency(self):\n        # Create a circular dependency between A -> B -> A\n        a1 = A.objects.create()\n        b1 = B.objects.create(a=a1)\n        a1.b = b1\n        a1.save()\n\n        collector = Collector(using=self.using)\n\n        with self.assertRaises(RestrictedError) as cm:\n            collector.collect([a1])\n\n        self.assertIn(\"'A.b'\", str(cm.exception))\n\n", "    def test_can_fast_delete_with_signals(self):\n        # Test whether can_fast_delete correctly identifies models with signal listeners.\n\n        class ModelWithSignals(models.Model):\n            pass\n\n        signals.pre_delete.connect(lambda sender, instance, **kwargs: None, sender=ModelWithSignals)\n\n        collector = Collector(self.using)\n        self.assertFalse(collector.can_fast_delete([ModelWithSignals()]))\n\n", "    def test_protected_objects_with_deferrable_uniqueness(self):\n        a1 = create_a()\n        a2 = create_a()\n        a3 = create_a()\n        b1 = B(a=a1)\n        b2 = B.objects.create(a=a2)\n        b1.save()\n        b2.save()\n\n        with self.assertRaises(ProtectedError) as cm:\n            a1.delete()\n        self.assertIn(\"'B.%s'\" % B._meta.get_field('a').name, str(cm.exception))\n        self.assertEqual(set(cm.exception.protected_objects), {b1})\n        self.assertEqual(B.objects.all().count(), 2)\n", "    def test_protected_error_with_deferrable_uniqueness_constraints(self):\n        b1 = B1.objects.create(name='b1')\n        r = R.objects.create(name='r', b1=b1)\n        with self.assertRaisesMessage(ProtectedError, 'Cannot delete some instances of model \\'R\\' because they are referenced through a protected foreign key: \\'R.b1\\''):\n            b1.delete()\n", "    def test_collect_related_objects_with_fast_delete_and_null_on_delete(self):\n        r = R.objects.create()\n        rchild = RChild.objects.create(r=r)\n        # We will create a cascade to rchild via rchildchild.\n        # rchildchild will have fast delete enabled, and rchild will have\n        # null_on_delete. This test makes sure that rchild's related objects are\n        # collected correctly even if the cascade passes through a fast delete.\n\n        rchildchild = RChildChild.objects.create(rchild=rchild)\n        collector = Collector(using=self.db_alias)\n        collector.collect([rchildchild])\n        self.assertCountEqual(collector.data[RChildChild], [rchildchild])\n        self.assertCountEqual(collector.data[RChild], [rchild])\n        self.assertCountEqual(collector.data[R], [r])\n\n"], "sample_260": ["    def test_multiple_add_fields_to_same_model(self):\n        operations = [\n            operations.CreateModel(\n                name='Person',\n                fields=[\n                    models.CharField(name='first_name', max_length=255),\n                ],\n                options={'managed': False},\n            ),\n            operations.AddField(\n                model_name='Person',\n                name='last_name',\n                field=models.CharField(max_length=255),\n            ),\n            operations.AddField(\n                model_name='Person',\n                name='age',\n                field=models.PositiveIntegerField(),\n            ),\n        ]\n        expected = [\n            operations.CreateModel(\n                name='Person',\n                fields=[\n                    models.CharField(name='first_name', max_length=255),\n                    models.CharField(name='last_name', max_length=255),\n                    models.PositiveIntegerField(name='age'),\n                ],\n                options={'managed': False},\n            ),\n        ]\n        self.assertOptimizesTo(operations, expected)\n", "    def test_model_rename_with_related_fields(self):\n        # Test that renaming a model with related fields is optimized correctly\n        class OldRelatedModel(models.Model):\n            old_model = models.ForeignKey('OldModel', on_delete=models.CASCADE)\n\n        class OldModel(models.Model):\n            pass\n\n        # Migration steps:\n        # 1. Rename OldModel to NewModel\n        # 2. Change ForeignKey in OldRelatedModel to point to NewModel\n\n        operations = [\n            operations.RenameModel('migrations', 'OldModel', 'NewModel'),\n            operations.AlterField(\n                'migrations',\n                'OldRelatedModel',\n                'old_model',\n                field=models.ForeignKey('NewModel', on_delete=models.CASCADE),\n            ),\n        ]\n        expected = [\n            operations.RenameModel('migrations', 'OldModel', 'NewModel'),\n            operations.AlterField(\n                'migrations',\n                'OldRelatedModel',\n                'old_model',\n                field=models.ForeignKey('NewModel', on_delete=models.CASCADE),\n            ),\n        ]\n        self.assertOptimizesTo(operations, expected)\n\n", "    def test_alter_unique_together_empty_to_value(self):\n        # Empty unique_together to a non-empty one\n        before = [\n            migrations.CreateModel(\n                name='UnicodeModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=100)),\n                ],\n                options={'unique_together': set(),\n                         }\n            ),\n        ]\n        after = [\n            migrations.CreateModel(\n                name='UnicodeModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=100)),\n                ],\n                options={\n                    'unique_together': {('name',)},},\n            ),\n        ]\n        op_class = operations.AlterUniqueTogether\n\n        self.assertOptimizesTo(\n            [operations.AlterUniqueTogether(\"UnicodeModel\", ['name'], app_label='tests')],\n            [op_class('UnicodeModel', set([('name',)])),],\n        )\n\n", "    def test_alter_unique_together_with_remove_field(self):\n        # This test verifies that when we remove a field that is part of\n        # unique_together, the optimizer correctly handles the dependency\n        old_model = EmptyManager.objects.model\n        self.assertOptimizesTo(\n            [\n                operations.AlterModelTable(\n                    name=old_model._meta.model_name, table='old_table_name'\n                ),\n                operations.RemoveField(model_name=old_model._meta.model_name, name='field_to_remove'),\n                operations.AlterUniqueTogether(\n                    name=old_model._meta.model_name,\n                    unique_together=set(),\n                ),\n            ],\n            [\n                operations.AlterModelTable(\n                    name=old_model._meta.model_name, table='old_table_name'\n                ),\n                operations.AlterUniqueTogether(\n                    name=old_model._meta.model_name,\n                    unique_together=set(),\n                ),\n                operations.RemoveField(model_name=old_model._meta.model_name, name='field_to_remove')\n            ],\n        )\n\n", "    def test_add_field_and_remove_field_same_type(self):\n        # Ensure that adding and then removing the same field (same type) doesn't produce empty operations.\n        before = [\n            migrations.AddField(\n                model_name='unicodemodel',\n                name='myfield',\n                field=models.CharField(max_length=100),\n            ),\n        ]\n        after = [\n            migrations.RemoveField(\n                model_name='unicodemodel',\n                name='myfield',\n            ),\n        ]\n\n        self.assertOptimizesTo(before + after, [])\n"], "sample_867": ["    def test_parameter_sampler_invalid_input(self):\n        for input, error_type, error_message in [\n                ([{'foo': [0]}, 0], TypeError,\n                 r'Parameter .* is not a dict \\(0\\)'),\n                ({'foo': 0}, TypeError,\n                 \"Parameter.* value is not iterable .*\"\n                 r\"\\(key='foo', value=0\\)\")]:\n            with pytest.raises(error_type, match=error_message):\n                list(ParameterSampler(input, n_iter=1))\n", "def test_parameter_sampler_errors(input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        list(ParameterSampler(input, n_iter=1))\n\n", "def test_parameter_grid_input_validation(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n", "def test_grid_search_invalid_input(klass):\n    with pytest.raises(error_type) as exc_info:\n        klass(input)\n    assert re.search(error_message, str(exc_info.value)) is not None\n", "    def test_empty_param_grid(self, klass):\n        \"\"\"Test that an empty parameter grid raises ValueError.\"\"\"\n        with pytest.raises(ValueError):\n            klass({})\n"], "sample_698": ["def test_log_cli_level_defaults(\n    terminal_reporter: TerminalReporter,", "    def test_log_cli_level_set_by_log_level(\n        self, caplog: LogCaptureFixture,", "    def test_log_file_level(self, capsys, tmp_path, monkeypatch):\n        monkeypatch.setattr(\n            \"pytest.logging._FileHandler.emit\", lambda self, record: None\n        )\n        config = self.config\n        config.getvalue = lambda: {\n            \"log_file\": str(tmp_path / \"testlog.log\"),\n            \"log_file_level\": logging.WARNING,\n        }\n        plugin = self.get_plugin()\n        plugin._log_cli_enabled = lambda: False\n\n        with capsys.disabled():\n            logging.getLogger().setLevel(logging.DEBUG)\n            logging.debug(\"this should not be logged to file\")\n            logging.warning(\"this should be logged to file\")\n            logging.error(\"this should also be logged to file\")\n\n        with open(tmp_path / \"testlog.log\", \"r\") as f:\n            content = f.read()\n        assert \"this should not be logged to file\" not in content\n        assert \"this should be logged to file\" in content\n        assert \"this should also be logged to file\" in content\n", "    def test_colored_level_formatter(\n        self,\n        caplog: Any,", "    def test_colored_level_formatter_with_no_color(\n        mocker: Any, capsys: Any"], "sample_1021": ["def test_Quaternion_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    assert q.pow_cos_sin(4) == Quaternion(900*cos(4*acos(sqrt(30)/30)),\n                                       1800*sqrt(29)*sin(4*acos(sqrt(30)/30))/29,\n                                       2700*sqrt(29)*sin(4*acos(sqrt(30)/30))/29,\n                                       3600*sqrt(29)*sin(4*acos(sqrt(30)/30))/29)\n", "def test_quaternion_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    assert q.pow_cos_sin(4) == Quaternion(900*cos(4*acos(sqrt(30)/30)), 1800*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*I, 2700*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*j, 3600*sqrt(29)*sin(4*acos(sqrt(30)/30))/29*k)\n", "def test_to_rotation_matrix_from_axis_angle():\n    q = Quaternion.from_axis_angle((sqrt(3)/3, sqrt(3)/3, sqrt(3)/3), 2*pi/3)\n    M = q.to_rotation_matrix()\n    assert M == Matrix([[1/2, -sqrt(3)/2, 0],\n                       [sqrt(3)/2, 1/2, 0],\n                       [0, 0, 1]])\n", "def test_rotate_point():\n    q1 = Quaternion(1, 2, 3, 4)\n    p = (1, 1, 1)\n    assert q1.rotate_point(p) == Quaternion.rotate_point(p, q1)\n    q2 = Quaternion(cos(x/2), 0, 0, sin(x/2))\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), q2)) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n    (axis, angle) = q2.to_axis_angle()\n    assert trigsimp(Quaternion.rotate_point((1, 1, 1), (axis, angle))) == (sqrt(2)*cos(x + pi/4), sqrt(2)*sin(x + pi/4), 1)\n", "def test_from_axis_angle():\n    q = Quaternion.from_axis_angle((1, 0, 0), pi / 2)\n    assert q.a == 0\n    assert q.b == 1\n    assert q.c == 0\n    assert q.d == 0\n    assert Quaternion.from_axis_angle((0, 1, 0), pi) == Quaternion(\n        -1, 0, 0, 0)\n    assert Quaternion.from_axis_angle((0, 0, 1), pi / 4).to_axis_angle() == (\n        (0, 0, 1), pi / 4)\n"], "sample_1180": ["    def test_point_intersection():\n        p1 = Point2D(1, 2)\n        p2 = Point2D(2, 3)\n        assert p1.intersection(p2) == []\n        assert p1.intersection(p1) == [p1]\n", "    def test_rotate_3d(self):\n        p = Point3D(1, 0, 0)\n        r = p.rotate(pi/2, Point3D(0, 0, 0))\n        assert r == Point3D(0, 1, 0)\n        p = Point3D(1, 1, 0)\n        r = p.rotate(pi/2, Point3D(0, 0, 0))\n        assert r == Point3D(-1, 1, 0)\n", "def test_point3d_translation():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    assert p1.translate(3, 3, 3) == p2\n", "def test_point3d_scale():\n    p = Point3D(1, 2, 3)\n    assert p.scale(2) == Point3D(2, 2, 3)\n\n", "def test_point3d_scale():\n    p = Point3D(1, 2, 3)\n    assert p.scale(2) == Point3D(2, 2, 3)\n    assert p.scale(2, 3, 4) == Point3D(2, 6, 12)\n    assert p.scale(2, 3, 4, (1, 1, 1)) == Point3D(3, 9, 15)\n\n"], "sample_504": ["def test_colorbar_tick_locator():\n    \"\"\"Test setting TickLocator on colorbars.\"\"\"\n    fig, ax = plt.subplots()\n    data = np.arange(10)\n    im = ax.imshow(data, cmap='viridis')\n    cb = fig.colorbar(im)\n\n    # Test setting a FixedLocator\n    locator = FixedLocator([0.5, 2.5, 4.5, 6.5, 8.5])\n    cb.set_ticks(locator)\n    assert all(cb.get_ticks() == locator())\n\n\n", "def test_colorbar_ticklocation():\n    fig, ax = plt.subplots()\n    X, Y = np.meshgrid(np.arange(10), np.arange(10))\n    Z = np.sin(X) + np.cos(Y)\n    im = ax.pcolormesh(X, Y, Z, cmap='viridis')\n    \n    for loc in ['left', 'right', 'top', 'bottom']:\n        ax.figure.colorbar(im, ax=ax, ticklocation=loc)\n    \n    # Check that the tick locations are correctly set for each location.\n    # Note: This test relies on visual inspection.\n", "    def test_colorbar_extension_shape_uniform(self):\n        fig = _colorbar_extension_shape('uniform')\n        self.compare(fig)\n\n", "    def test_colorbar_extensions_length(self):\n        for spacing in ('uniform', 'proportional'):\n            fig = _colorbar_extension_length(spacing)\n            fig.savefig(\n                f\"test_colorbar_extensions_length_{spacing}.png\")\n", "    def test_colorbar_extension_shape_empty(self):\n        cmap, norms = _get_cmap_norms()\n        fig, ax = plt.subplots()\n        for extend in ['neither', 'min', 'max', 'both']:\n            for spacing in ['uniform', 'proportional']:\n                Colorbar(ax, cmap=cmap, norm=norms[extend],\n                         extend=extend, extendfrac=0.1,\n                         orientation='vertical', spacing=spacing)\n                # Check that the colorbar axes are empty\n                assert len(ax.images) == 0\n\n\n"], "sample_219": ["    def test_coalesce_with_value_function(self):\n        query = Company.objects.annotate(\n            ceo_name=Coalesce(F('ceo__firstname'), Value('No CEO'))\n        ).values('ceo_name')\n        self.assertCountEqual(\n            query.order_by('ceo_name'),\n            [{'ceo_name': 'Frank'}, {'ceo_name': 'Joe'}, {'ceo_name': 'No CEO'}],\n        )\n", "    def test_expression_wrapper(self):\n        expr = ExpressionWrapper(F('num_employees') * 2, output_field=IntegerField())\n        self.assertEqual(list(self.company_query.annotate(double_employees=expr)), [\n            {'name': 'Example Inc.', 'num_employees': 2300, 'num_chairs': 5, 'double_employees': 4600},\n            {'name': 'Foobar Ltd.', 'num_employees': 3, 'num_chairs': 4, 'double_employees': 6},\n            {'name': 'Test GmbH', 'num_employees': 32, 'num_chairs': 1, 'double_employees': 64},\n        ])\n", "    def test_window_function_over_subquery(self):\n        # Verify that a window function can be applied over a subquery.\n        query = (\n            Employee.objects.annotate(\n                rank=Window(\n                    Rank(order_by=F('salary').desc()), partition_by=F('company__name'),\n                )\n            )\n            .filter(company__name='Example Inc.')\n        )\n        self.assertEqual(query.count(), 1)\n\n", "    def test_rawsql_params(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            list(\n                Company.objects.annotate(\n                    num_chairs_plus_one=RawSQL(\n                        'num_chairs + %s', [1]\n                    )\n                ).values('num_chairs_plus_one')\n            )\n        self.assertEqual(\n            captured_queries.get('SELECT')[\n                'params'\n            ],\n            (1,),\n        )\n\n", "    def test_jsonb_extract(self):\n        with mock.patch('django.db.backend.schema.DatabaseSchema.column_type',\n                        return_value='jsonb'):\n            Company.objects.create(name=\"Test Company\", extra_data={\"key\": 1})\n            companies = Company.objects.annotate(\n                extracted_data=Extract('extra_data', 'key')\n            )\n            self.assertEqual(companies.first().extracted_data, 1)\n"], "sample_297": ["    def test_values_select(self):\n        # Test that values() selects the correct fields when there are relations.\n        result = list(Item.objects.values('name', 'creator__name'))\n        self.assertEqual(len(result), 4)\n        self.assertEqual(result[0]['name'], 'one')\n        self.assertEqual(result[0]['creator__name'], 'a1')\n\n", "    def test_ticket_23605(self):\n        \"\"\"\n        Regression test for #23605.\n\n        Previously, if a model had a field with a name that clashed with a\n        related model's field (in this case, 'name' on Ticket23605A and\n        Ticket23605B), using a subquery with a join on related fields\n        caused an error.\n        \"\"\"\n\n        a = Ticket23605A.objects.create(name='a')\n        b1 = Ticket23605B.objects.create(name='b1', ticket=a)\n        b2 = Ticket23605B.objects.create(name='b2', ticket=a)\n        c1 = Ticket23605C.objects.create(name='c1', ticket=b1)\n        c2 = Ticket23605C.objects.create(name='c2', ticket=b2)\n\n        qs = Ticket23605A.objects.annotate(\n            count=Count('ticket__ticket')\n        ).filter(count=2)\n\n        self.assertQuerysetEqual(qs, [a])\n", "    def test_ticket_23605(self):\n        # Test for ticket #23605 - Regression: Lookup with a foreign key field\n        # using a related field causes Django to throw a ValueError\n        c1 = Ticket23605C.objects.create()\n        b1 = Ticket23605B.objects.create(c=c1)\n        a1 = Ticket23605A.objects.create(b=b1)\n\n        self.assertEqual(Ticket23605A.objects.filter(b__c=c1).count(), 1)\n        self.assertEqual(Ticket23605A.objects.filter(b__c__isnull=True).count(), 0)\n\n", "    def test_filter_related_field_with_empty_set(self):\n        # Regression test for #21203\n        self.assertEqual(\n            Ticket21203Child.objects.filter(parent__children__isnull=True).count(),\n            0\n        )\n        self.assertEqual(\n            Ticket21203Parent.objects.filter(children__isnull=True).count(),\n            0\n        )\n", "    def test_annotate_aggregate_count_with_filter(self):\n        with self.assertNumQueries(1):\n            result = Item.objects.annotate(\n                tag_count=Count('tags__name', filter=Q(tags__name='t1'))\n            ).get(pk=self.i1.pk)\n        self.assertEqual(result.tag_count, 1)\n"], "sample_347": ["    def test_make_aware_with_zoneinfo(self):\n        naive = datetime.datetime(2023, 10, 26, 10, 0, 0)\n        aware = timezone.make_aware(naive, timezone=PARIS_ZI)\n        self.assertEqual(aware.tzinfo, PARIS_ZI)\n        self.assertEqual(aware, datetime.datetime(2023, 10, 26, 10, 0, 0, tzinfo=PARIS_ZI))\n\n", "    def test_localtime_with_zoneinfo_aware_datetime(self):\n        dt = datetime.datetime(2023, 10, 26, 12, 0, 0, tzinfo=PARIS_ZI)\n        with timezone.override(timezone.get_current_timezone()):\n            self.assertEqual(timezone.localtime(dt), dt.astimezone(CET))\n", "    def test_localtime_zoneinfo_aware(self):\n        now = timezone.now()\n        self.assertEqual(timezone.localtime(now, PARIS_ZI).tzinfo, PARIS_ZI)\n", "    def test_localtime_with_zoneinfo_timezone(self):\n        with timezone.override(timezone.get_current_timezone()):\n            dt = datetime.datetime(2023, 1, 1, 12, 30, 0, tzinfo=timezone.utc)\n            expected = CET.localize(dt)\n            self.assertEqual(timezone.localtime(dt), expected)\n", "    def test_make_aware_with_zoneinfo(self):\n        dt = datetime.datetime(2023, 1, 1, 12, 0, 0)\n        aware_dt = timezone.make_aware(dt, timezone=PARIS_ZI)\n        self.assertEqual(aware_dt.tzinfo, PARIS_ZI)\n"], "sample_817": ["    def test_sparse_input(self):\n        X = bsr_matrix(data)\n        selector = VarianceThreshold()\n        # Check that the selector works with sparse matrices\n        # and that the output is also a sparse matrix\n        assert_isinstance(selector.fit_transform(X), bsr_matrix)\n\n", "    def test_sparse_input_with_zero_threshold():\n        X = csr_matrix(data)\n        selector = VarianceThreshold(threshold=0)\n        assert_raises(ValueError, selector.fit, X)\n", "def test_variance_threshold_sparse():\n    X = csr_matrix(data)\n    selector = VarianceThreshold()\n    selector.fit(X)\n    assert_array_equal(selector.variances_,\n                       np.array([ 0.66666667,  0.66666667,  0.,  0.66666667,  2.66666667]))\n", "    def test_variance_threshold_sparse_input(self):\n        X = csc_matrix(data)\n        selector = VarianceThreshold(threshold=0.5)\n        selector.fit(X)\n        assert_array_equal(selector.transform(X).toarray(),\n                           np.array([[2, 3, 4],\n                                     [2, 3, 5],\n                                     [1, 4, 0]]))\n        assert_equal(selector.variances_.shape[0], X.shape[1])\n", "def test_variance_threshold_sparse_csr():\n    X = csr_matrix(data)\n    transformer = VarianceThreshold()\n    transformer.fit(X)\n    assert_array_equal(transformer.transform(X).toarray(),\n                       np.array([[1, 2, 3, 4],\n                                 [2, 2, 3, 5],\n                                 [1, 2, 4, 0]]))\n"], "sample_501": ["    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label='Data')\n        ax.legend(loc=loc)\n        # No assertions, just making sure this does not raise an error\n\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label='data')\n        ax.legend(loc=loc)\n        assert ax.legend_.get_loc() == loc\n\n", "    def test_get_legend_handles_labels(self, handles, labels, expected_labels):\n        fig, ax = plt.subplots()\n        for h, l in zip(handles, labels):\n            if l is not None:\n                h.set_label(l)\n            ax.add_artist(h)\n\n        result_handles, result_labels = mlegend._get_legend_handles_labels(\n            [ax]\n        )\n        assert result_labels == expected_labels\n", "def test_legend_bad_handles():\n    fig, ax = plt.subplots()\n    ax.plot([], [])\n    with pytest.raises(TypeError):\n        ax.legend(handles=['not a handle'])\n\n", "    def test_legend_loc(self, loc, expected_loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        legend = ax.legend(loc=loc)\n        assert legend._loc == expected_loc\n\n"], "sample_657": ["    def test_mark_param(self, modulename):\n        mark = getattr(getattr(sys.modules[modulename], \"mark\"), self.attr)\n        with pytest.raises(TypeError):\n            mark(\"name\", x=1)\n\n", "    def test_mark_deprecated_warning(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(True)\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-v\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_skip SKIP*\",\n                \"*skipping because*skipif*\",\n            ]\n        )\n\n\n", "    def test_mark_decorator_with_args(self, modulename):\n        MARKNAME = \"mymark\"\n        mark = getattr(getattr(pytest, modulename), MARKNAME)\n        mark1 = mark(1)\n        mark2 = mark(2)\n        assert mark1.args == (1,)\n        assert mark2.args == (2,)\n        combined = mark1.combined_with(mark2)\n        assert combined.args == (1, 2)\n        assert combined.name == MARKNAME\n", "    def test_create_markdecorator_with_args(self, modulename):\n        mark = getattr(getattr(pytest, modulename), \"mark\")\n        with mock.patch(\n            \"warnings.warn\",\n            mock.MagicMock(return_value=None),\n        ):\n            m = mark.NAME(name1=\"value1\", name2=\"value2\")\n        assert m.name == \"NAME\"\n        assert m.args == ()\n        assert m.kwargs == {\"name1\": \"value1\", \"name2\": \"value2\"}\n", "    def test_mark_access_with_incorrect_name(self):\n        with pytest.raises(AttributeError) as excinfo:\n            getattr(Mark(), \"_not_existing\")\n        assert excinfo.value.args[0] == \"Marker name must NOT start with underscore\"\n\n"], "sample_748": ["def test_gridsearch_invalid_param_grid(input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        GridSearchCV(estimator=MockClassifier(), param_grid=input)\n\n", "    def test_gridsearch_invalid_param_grid(self):\n        est = LinearSVC()\n        param_grid = {'foo': 0}\n        with assert_raises(TypeError) as e:\n            GridSearchCV(est, param_grid=param_grid)\n        assert_equal(str(e.value),\n                     \"Parameter grid value is not iterable \"\n                     \"(key='foo', value=0)\")\n", "    def test_parameter_grid_error_types(self):\n        grid = {'foo': [0, 1]}\n        estimator = MockClassifier()\n        scorer = make_scorer(accuracy_score)\n\n        # Test if wrong data type for fit_params raises TypeError\n        with assert_raises(TypeError,\n                          msg=\"fit_params parameter must be a dictionary\"\n                          \" or None\"):\n            GridSearchCV(estimator, grid, scoring=scorer,\n                         fit_params=1)\n\n", "    def test_gridsearch_refit_metric(self, scoring):\n        # Regression test for issue #8981: refit should use the scoring metric\n        # specified (not just accuracy)\n        clf = DecisionTreeClassifier()\n        grid = {'max_depth': [1, 2]}\n        gs = GridSearchCV(clf, grid, scoring=scoring, cv=2)\n        gs.fit(X, y)\n        assert_equal(gs.best_score_,\n                     gs.cv_results_['mean_test_score'][\n                         gs.best_index_])\n", "def test_randomized_search_cv_error_score():\n    # Test that RandomizedSearchCV handles error_score correctly\n\n    class AlwaysRaisesErrorEstimator(BaseEstimator):\n            raise ValueError(\"Test error\")\n\n    param_grid = {\"alpha\": [0.1, 1.0]}\n    X = np.array([[1], [2], [3]])\n    y = np.array([0, 1, 0])\n\n    # Test with error_score='raise'\n    clf = RandomizedSearchCV(\n        AlwaysRaisesErrorEstimator(), param_grid, error_score='raise',\n        n_iter=2\n    )\n    with pytest.raises(ValueError, match=\"Test error\"):\n        clf.fit(X, y)\n\n    # Test with error_score=np.nan\n    clf = RandomizedSearchCV(\n        AlwaysRaisesErrorEstimator(), param_grid, error_score=np.nan, n_iter=2\n    )\n    clf.fit(X, y)\n    assert np.isnan(clf.best_score_)\n\n    # Test with error_score=0\n    clf = RandomizedSearchCV(\n        AlwaysRaisesErrorEstimator(), param_grid, error_score=0, n_iter=2\n    )\n    clf.fit(X, y)\n\n\n\n"], "sample_473": ["    def test_get_script_prefix_from_settings(self):\n        request = self.request_factory.get(\"/\")\n        self.assertEqual(get_script_prefix(request), \"/some-prefix/\")\n", "    def test_script_name(self):\n        request = self.request_factory.get('/')\n        self.assertEqual(get_script_name(request), \"/test/\")\n        self.assertEqual(request.script_name, \"/test/\")\n\n", "    def test_script_prefix(self):\n        request = self.request_factory.get('/path/')\n        handler = WSGIHandler()\n        handler.load_middleware()\n        response = handler(request)\n        self.assertEqual(request.script_name, '/testscript')\n        self.assertEqual(request.path_info, '/path/')\n        self.assertEqual(request.path, '/testscript/path/')\n", "    def test_get_script_prefix(self):\n        request = self.request_factory.get('/')\n        self.assertEqual(get_script_prefix(request), \"/somepath/\")\n", "    def test_script_prefix_from_settings(self):\n        request = self.request_factory.get('/')\n        self.assertEqual(get_script_name(request), \"/prefix\")\n"], "sample_497": ["    def test_integer_ticks(self, vmin, vmax, steps, expected):\n        locator = mticker.MaxNLocator(steps=steps)\n        ticks = locator.tick_values(vmin, vmax)\n        assert_array_equal(ticks, expected)\n", "compilation error", "    def test_integer(self, vmin, vmax, integer, expected):\n        locator = mticker.MaxNLocator(5, min_n_ticks=integer)\n        if expected is None:\n            expected = locator.tick_values(vmin, vmax)\n        assert_array_equal(locator.tick_values(vmin, vmax), expected)\n", "compilation error", "compilation error"], "sample_422": ["    def test_prefetch_related_with_isnull_filter(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            # Prefetch 'authors' and filter for books with at least one author.\n            books = Book.objects.prefetch_related('authors').filter(authors__isnull=False)\n        self.assertEqual(len(captured_queries), 2)\n        self.assertWhereContains(captured_queries[1]['sql'], 'authors.id')\n\n", "    def test_prefetch_related_with_null_fk(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            authors = AuthorWithAge.objects.prefetch_related(\"bio\").all()\n\n        expected_queries = 2\n        self.assertEqual(len(captured_queries), expected_queries)\n\n        for author in authors:\n            self.assertEqual(len(author.bio.all()), 0)\n", "    def test_prefetch_related_with_field_reversal(self):\n        with CaptureQueriesContext(connection) as queries:\n            author = Author.objects.prefetch_related(\"book_set__authors\").get(name=\"Charlotte\")\n            self.assertEqual(len(queries.captured_queries), 2)\n            self.assertEqual(len(author.book_set.all()), 2)\n\n", "    def test_prefetch_related_with_select_related(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            prefetch_list = Prefetch('authors', select_related='first_book')\n            books_with_authors = Book.objects.prefetch_related(\n                prefetch_list\n            ).all()\n            book = books_with_authors.first()\n            self.assertEqual(\n                book.authors.first().first_book.title,\n                \"Poems\",\n            )\n", "    def test_prefetch_related_with_reverse_fk_no_where_clause_generated(self):\n        with ignore_warnings(RemovedInDjango50Warning):\n            prefetch = Prefetch('authors', queryset=Author.objects.filter(name__startswith='E'))\n            qs = Book.objects.prefetch_related(prefetch)\n            with CaptureQueriesContext(connection) as captured_queries:\n                list(qs)\n        self.assertEqual(captured_queries.count, 2)\n        sql = captured_queries.captured()[1]['sql']\n        self.assertNotIn(\"WHERE\", sql)\n\n"], "sample_753": ["    def test_logisticregressioncv_lbfgs_multiclass_ovr(self):\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=15, random_state=0)\n        le = LabelEncoder()\n        y = le.fit_transform(y)\n        classes = le.classes_\n        n_classes = len(classes)\n\n        # Ensure that the multiclass case works with logisticRegressionCV\n        # and LBFGS solver with ovr.\n        clf = LogisticRegressionCV(solver='lbfgs', multi_class='ovr',\n                                  max_iter=1000, cv=3, random_state=0).fit(X, y)\n        check_predictions(clf, X, y)\n        assert_equal(clf.coef_.shape, (n_classes, X.shape[1]))\n        assert_equal(clf.intercept_.shape, (n_classes,))\n", "    def test_logistic_regression_cv_multiclass_sparse(self):\n        # Test LogisticRegressionCV with sparse input and multiclass classification\n        X, y = make_classification(n_samples=50, n_features=10, n_informative=5,\n                                  n_classes=3, random_state=42)\n        X_sp = sp.csr_matrix(X)\n        clf = LogisticRegressionCV(solver='lbfgs', multi_class='multinomial',\n                                   cv=StratifiedKFold(n_splits=5,\n                                                      shuffle=True,\n                                                      random_state=42))\n\n        # Fit on sparse data\n        clf.fit(X_sp, y)\n        check_predictions(clf, X_sp, y)\n", "    def test_multiclass_ovr_fit_intercept(self):\n        # Test multiclass OvR with fit_intercept=True\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_redundant=0,\n                                   n_classes=3, random_state=0)\n\n        # Set fit_intercept to True\n        clf = LogisticRegressionCV(solver='lbfgs', fit_intercept=True,\n                                    multi_class='ovr', cv=5,\n                                    random_state=0).fit(X, y)\n\n        check_predictions(clf, X, y)\n        assert_true(clf.intercept_.any() != 0)\n", "def test_multi_class_refit_strategy():\n    # Test that the refit strategy works correctly for multi-class problems\n    # Specifically, check that the best C and coef are averaged across folds\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0,\n                               n_classes=3)\n\n    # Create a LogisticRegressionCV with refit=False\n    logit_cv = LogisticRegressionCV(Cs=[0.1, 1, 10], cv=3,\n                                     scoring='accuracy', refit=False,\n                                     multi_class='multinomial')\n\n    logit_cv.fit(X, y)\n\n    # Check that the best C and coef are averaged across folds\n    assert len(logit_cv.C_) == 3\n    assert_allclose(logit_cv.coef_.shape, (3, X.shape[1]))\n\n\n", "    def test_logistic_regressioncv_multiclass_warn(self):\n        # Check that a warning is raised when passing \"multinomial\" to\n        # LogisticRegressionCV.\n        clf = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs')\n        with pytest.warns(FutureWarning):\n            clf.fit(iris.data, iris.target)\n"], "sample_738": ["def test_TfidfVectorizer_vocabulary_parameter():\n    corpus = [\n        'this is the first document',\n        'this document is the second document',\n        'and this is the third one',\n        'is this the first document',\n    ]\n    vocab = {'this': 0, 'is': 1, 'the': 2, 'first': 3, 'document': 4, 'second': 5,\n             'third': 6, 'one': 7}\n\n    vectorizer = TfidfVectorizer(vocabulary=vocab)\n    # Fit and transform, asserting vocabulary is as expected\n    X = vectorizer.fit_transform(corpus)\n    assert_equal(vectorizer.vocabulary_, vocab)\n    assert_equal(X.shape[1], len(vocab))\n\n    # Transform with pre-defined vocabulary, asserting vocabulary stays the same\n    X2 = vectorizer.transform([\n        'this is a new document',\n    ])\n    assert_equal(vectorizer.vocabulary_, vocab)\n    assert_equal(X2.shape[1], len(vocab))\n", "    def test_vectorizer_vocab_limit(self):\n        vectorizer = CountVectorizer(vocabulary=set(['pizza', 'burger', 'beer']))\n        vectorizer.fit(ALL_FOOD_DOCS)\n        assert_equal(len(vectorizer.vocabulary_), 3)\n        assert_true('pizza' in vectorizer.vocabulary_)\n        assert_true('burger' in vectorizer.vocabulary_)\n        assert_true('beer' in vectorizer.vocabulary_)\n        assert_false('coke' in vectorizer.vocabulary_)\n", "def test_tfidf_vectorizer_lowercase():\n    vectorizer = TfidfVectorizer(lowercase=False)\n    corpus = ['This is the first document', 'This document is the second document']\n    vectorizer.fit(corpus)\n    analyze = vectorizer.build_analyzer()\n    assert_equal(analyze('This is a TEST'), ['This', 'is', 'a', 'TEST'])\n\n", "    def test_tfidf_vocabulary_size(self):\n        tfidf = TfidfVectorizer(max_features=5)\n        tfidf.fit(ALL_FOOD_DOCS)\n        assert_equal(len(tfidf.vocabulary_), 5)\n", "    def test_tfidf_vectorizer_vocabulary_consistency(self):\n        # Test that the vocabulary is consistent across multiple calls to fit\n        # and fit_transform.\n        corpus = ['This is the first document.',\n                  'This document is the second document.',\n                  'And this is the third one.',\n                  'Is this the first document?']\n\n        vectorizer = TfidfVectorizer()\n        vectorizer.fit(corpus)\n        vocab1 = vectorizer.vocabulary_\n\n        vectorizer.fit(corpus)\n        vocab2 = vectorizer.vocabulary_\n\n        assert_equal(vocab1, vocab2)\n"], "sample_376": ["    def test_cookie_max_size(self):\n        storage = self.get_storage()\n        messages = [Message(constants.DEBUG, get_random_string(length=100)) for _ in range(20)]\n        set_cookie_data(storage, messages)\n        self.assertEqual(\n            stored_cookie_messages_count(storage, self.client.get('/')),\n            len(messages),\n        )\n\n        # Force messages to exceed the max cookie size\n        messages.append(Message(constants.DEBUG, get_random_string(length=1000)))\n        set_cookie_data(storage, messages)\n        count = stored_cookie_messages_count(storage, self.client.get('/'))\n        self.assertEqual(count, len(messages) - 1)\n\n", "    def test_not_finished_sentinel(self):\n        storage = self.get_storage()\n        messages = [\n            Message(constants.SUCCESS, 'First message'),\n            Message(constants.ERROR, 'Second message'),\n            Message(constants.WARNING, 'Third message'),\n        ]\n        response = self.get_response()\n        \n        # Store messages, exceeding max cookie size\n        storage._store(messages, response, remove_oldest=False)\n            \n        # Check messages stored count\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n        # Assert the last message is not in the cookie\n        self.assertNotEqual(storage._decode(response.cookies.get(storage.cookie_name).value)[-1].message, 'Third message')\n\n", "    def test_cookie_storage_not_finished(self):\n        storage = self.get_storage()\n\n        messages = [\n            Message(constants.ERROR, 'Message 1'),\n            Message(constants.WARNING, 'Message 2'),\n            Message(constants.INFO, 'Message 3'),\n        ]\n\n        set_cookie_data(storage, messages)\n\n        request1, response1 = self.get_request_response()\n\n        # First request should get all messages\n        messages1, all_retrieved1 = storage._get(request1)\n        self.assertEqual(messages1, messages)\n        self.assertTrue(all_retrieved1)\n\n        # Add more messages to exceed cookie size limit\n        for i in range(4, 10):\n            messages.append(Message(constants.SUCCESS, f'Message {i}'))\n\n        set_cookie_data(storage, messages)\n\n        request2, response2 = self.get_request_response()\n\n        # Second request should get some messages and not_finished sentinel\n        messages2, all_retrieved2 = storage._get(request2)\n        self.assertEqual(len(messages2), CookieStorage.max_cookie_size // 50 )\n        self.assertFalse(all_retrieved2)\n", "    def test_max_cookie_size_limit(self):\n        storage = self.get_storage()\n        messages = [Message(level, get_random_string(length=100)) for level in constants.MESSAGE_TAGS\n                    for _ in range(50)]  # Create a lot of messages to exceed the limit\n        set_cookie_data(storage, messages)\n        response = self.request_response()\n        # Assert that only some messages were stored\n        self.assertLess(self.stored_messages_count(storage, response), len(messages))\n        # Assert that the not_finished sentinel was added\n        self.assertTrue(storage._loaded_data[-1] == CookieStorage.not_finished)\n", "    def test_max_cookie_size_remove_oldest(self):\n        storage = self.get_storage()\n        messages = [\n            Message(constants.ERROR, get_random_string(100)),\n            Message(constants.WARNING, get_random_string(100)),\n            Message(constants.DEBUG, get_random_string(100)),\n        ]\n        response = self.request\n\n        # Set cookie size to 100 to ensure messages exceed the limit\n        storage.max_cookie_size = 100\n\n        unstored_messages = storage._store(\n            messages, response, remove_oldest=True\n        )\n\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].level, constants.DEBUG)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n"], "sample_795": ["    def test_sparse_transformer_with_large_sparse_data(self):\n        # Test SparseTransformer with large sparse data\n        X = sp.csr_matrix(np.random.rand(10000, 100))\n        transformer = SparseTransformer()\n        transformer.fit_transform(X)\n", "    def test_sparse_transformer(self):\n        # Test SparseTransformer fit_transform\n        transformer = SparseTransformer()\n        X = np.array([[1, 2], [3, 4]])\n        X_transformed = transformer.fit_transform(X)\n\n        assert_array_equal(X_transformed.toarray(), X)\n        assert isinstance(X_transformed, sp.csr_matrix)\n\n        # Test SparseTransformer transform with incorrect number of features\n        X_new = np.array([[1, 2, 3]])\n        assert_raises(ValueError, transformer.transform, X_new)\n", "    def test_sparse_transformer_with_sparse_input(self):\n        # Test SparseTransformer with sparse input\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        transformer = SparseTransformer()\n        transformed_X = transformer.fit_transform(X)\n        assert isinstance(transformed_X, sp.csr_matrix)\n        assert transformed_X.shape == X.shape\n        \n", "    def test_sparse_transformer(self):\n        # Test SparseTransformer with a non-sparse input\n        transformer = SparseTransformer()\n        X_dense = np.array([[1, 2], [3, 4]])\n        X_sparse = transformer.fit_transform(X_dense)\n        assert sp.issparse(X_sparse)\n        assert X_sparse.shape == (2, 2)\n\n        # Test SparseTransformer with a sparse input\n        X_sparse_input = sp.csr_matrix(X_dense)\n        transformer.fit(X_sparse_input)\n        X_sparse_transformed = transformer.transform(X_sparse_input)\n        assert sp.issparse(X_sparse_transformed)\n        assert X_sparse_transformed.shape == (2, 2)\n\n", "    def test_check_estimators_unfitted_error(self):\n        # Test that estimators raise an error if used before fitting\n        est_err = CorrectNotFittedErrorClassifier()\n        X = np.random.randn(5, 10)\n        y = np.random.randint(0, 2, size=5)\n\n        assert_raises(CorrectNotFittedError, est_err.predict, X)\n"], "sample_28": ["    def test_card_padding(self):\n        card = fits.Card(\"TESTKEY\", \"TESTVALUE\")\n        assert len(card._format_image()) == Card.length\n        # Test padding for shorter strings\n        card = fits.Card(\"KEY\", \"V\")\n        assert len(card._format_image()) == Card.length\n        # Test padding for longer strings\n        card = fits.Card(\"KEY\", \"THIS IS A VERY LONG STRING\" * 5)\n        assert len(card._format_image()) == Card.length\n", "    def test_card_comments(self):\n        card = fits.Card.fromstring(\"COMMENT This is a comment\")\n        assert card.keyword == \"COMMENT\"\n\n        card = fits.Card.fromstring(\"HISTORY This is a HISTORY card\")\n        assert card.keyword == \"HISTORY\"\n\n        # Test that comments can span multiple lines\n        card = fits.Card.fromstring(\"COMMENT This is\\na multi-line\\ncomment\")\n        assert card.comment == \"This is\\na multi-line\\ncomment\"\n\n        # Test that comments are preserved when creating a new header\n        header = fits.Header()\n        header[\"COMMENT\"] = \"This is a comment\"\n        header[\"HISTORY\"] = \"This is a HISTORY card\"\n\n        assert header[\"COMMENT\"] == \"This is a comment\"\n        assert header[\"HISTORY\"] == \"This is a HISTORY card\"\n", "    def test_hierarch_card(self):\n        card1 = fits.Card.fromstring(\"HIERARCH KEYWORD1= 'value 1'\")\n        assert card1.keyword == \"KEYWORD1\"\n        assert card1.value == \"value 1\"\n        assert card1.comment is None\n        assert card1._hierarch is True\n\n        card2 = fits.Card(\"HIERARCH KEYWORD2\", \"value 2\")\n        assert card2.keyword == \"KEYWORD2\"\n        assert card2.value == \"value 2\"\n        assert card2.comment is None\n        assert card2._hierarch is True\n\n        card3 = fits.Card(\"HIERARCH KEYWORD3= 'value 3'\", \"comment\")\n        assert card3.keyword == \"KEYWORD3\"\n        assert card3.value == \"value 3\"\n        assert card3.comment == \"comment\"\n        assert card3._hierarch is True\n", "    def test_card_constructor_rvkc(self):\n        card = Card('HIERARCH.TEST.KEY', 1.23, unit='m/s', comment='Velocity')\n\n        assert card.keyword == 'HIERARCH.TEST.KEY'\n        assert card.value == 1.23\n        assert card.comment == 'Velocity'\n        assert card.unit == 'm/s'\n\n        assert card._rawvalue == '1.23'\n        assert card._field_specifier == 'TEST.KEY'\n", "    def test_card_format_long_string(self):\n        # Test the formatting of long strings in cards.\n        card = fits.Card(\"COMMENT\", \"This is a very long comment that spans multiple lines\")\n        actual = card._format_image()\n        expected = (\n            \"COMMENT                    = 'This is a very long comment that spans\"\n            \" multiple lines' / \\n\"\n            \"CONTINUE                   = '' / This is a very long comment that \"\n            \"spans multiple lines\"\n        )\n        assert actual == expected\n"], "sample_342": ["    def test_permission_denied_for_invalid_field_name(self):\n        with model_admin(Answer, QuestionAdmin):\n            url = self.url + '?app_label={}&model_name={}&field_name=nonexistent'.format(\n                self.opts['app_label'], self.opts['model_name']\n            )\n            request = self.factory.get(url)\n            request.user = self.user\n\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 403)\n            self.assertIsInstance(response.content, bytes)\n\n", "    def test_permission_denied_for_incorrect_to_field(self):\n        with model_admin(\n            Answer,\n            AnswerAdmin,\n        ):\n            answer_admin = site._registry[Answer]\n            request = self.factory.get(self.url, {\n                'app_label': Answer._meta.app_label,\n                'model_name': Answer._meta.model_name,\n                'field_name': 'question',\n                'term': 'test',\n            })\n            request.user = self.user\n            answer_admin.to_field_allowed = lambda *args, **kwargs: False\n            with self.assertRaises(PermissionDenied):\n                AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_autocomplete_permission_denied_search_fields(self):\n        with model_admin(Answer, QuestionAdmin):\n            self.client.force_login(self.user)\n            url = self.url + '?' + '&'.join(f'{k}={v}' for k, v in self.opts.items())\n            response = self.client.get(url)\n            self.assertEqual(response.status_code, 404)\n", "    def test_permission_denied_no_search_fields(self):\n        with model_admin(Manager, ManagerAdmin):\n            url = self.url + '?term=john&app_label={}&model_name={}&field_name={}'.format(\n                Manager._meta.app_label, Manager._meta.model_name, 'employee'\n            )\n            response = self.client.get(url, HTTP_X_REQUESTED_WITH='XMLHttpRequest')\n            self.assertEqual(response.status_code, 404)\n", "    def test_autocomplete_view_no_search_fields(self):\n        # Check that a 404 is raised if the ModelAdmin doesn't have search_fields.\n\n        with model_admin(Answer, AnswerAdmin):\n            AnswerAdmin.search_fields = []\n\n            request = self.factory.get(self.url, data={**self.opts, 'term': 'foo'})\n            request.user = self.user\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request, *[] , **{})\n            self.assertEqual(response.status_code, 404)\n\n"], "sample_19": ["    def test_hdu_header_update(self):\n        # Test that wcs header updates are reflected in the HDU\n\n        with ctx_for_v71_dateref_warnings():\n            with fits.open(\n                get_pkg_data_filename(\"data/fits/cd.fits\")\n            ) as hdul:\n                wcs = wcs.WCS(hdul[0].header)\n                # Change a WCS parameter\n                wcs.wcs.crval = [10.0, 20.0]\n\n                # Ensure changes are reflected in the header\n                assert_allclose(hdul[0].header[\"CRVAL1\"], 10.0)\n                assert_allclose(hdul[0].header[\"CRVAL2\"], 20.0)\n", "    def test_maps_0001(self):\n        with open(get_pkg_data_filename(\"data/maps/maps_0001.hdr\"), \"rb\") as fd:\n            header = fits.Header.fromtextfile(fd)\n        wcs = wcs.WCS(header)\n        assert wcs.wcs.naxis == 2\n        assert wcs.wcs.crpix == [1, 1]\n        assert wcs.wcs.cdelt == [1, 1]\n        assert wcs.wcs.ctype == [\"RA---TAN\", \"DEC--TAN\"]\n        assert wcs.wcs.pc == [[1, 0], [0, 1]]\n        assert wcs.wcs.crval == [0, 0]\n", "    def test_fits2wcs_all(self):\n        \"\"\"Test that fits2wcs works for all supported map types.\"\"\"\n        with ctx_for_v71_dateref_warnings():\n            for filename in self._file_list:\n                with open(filename, \"rb\") as fd:\n                    header = fits.Header.frombytes(fd.read())\n                wcs_obj = wcs.WCS(header)\n                assert wcs_obj is not None\n", "    def test_wcs_find_all(self):\n        with ctx_for_v71_dateref_warnings():\n            for filename in self._file_list:\n                with open(filename, \"rb\") as fd:\n                    header = fits.Header.from_file(fd)\n\n                wcses = wcs.find_all_wcs(header)\n                assert len(wcses) > 0\n", "    def test_map_15_n_pixels(self):\n        with ctx_for_v71_dateref_warnings() as ctx:\n            for filename in self._file_list:\n                if \"map_15_n_pixels.hdr\" in filename:\n                    with open(filename, \"r\") as f:\n                        header = fits.Header.fromstring(f.read())\n                    wcs = wcs.WCS(header)\n\n                    n_pixels = [int(x) for x in header[\"NAXIS\"].split()]\n                    n_pixels_wcs = wcs.pixel_shape\n                    assert n_pixels == n_pixels_wcs, (\n                        f\"Header NAXIS: {n_pixels}, \"\n                        f\"WCS pixel shape: {n_pixels_wcs}\"\n                    )\n"], "sample_118": ["    def test_in_with_different_database_connections(self):\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT * FROM django_migrations')\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT * FROM auth_permission')\n\n", "    def test_isnullwithnoneasrhs(self):\n        self.assertQuerysetEqual(\n            IsNullWithNoneAsRHS.objects.filter(field__isnull=None),\n            IsNullWithNoneAsRHS.objects.all()\n        )\n", "    def test_in_with_empty_iterable(self):\n        with self.assertRaises(EmptyResultSet):\n            list(Article.objects.filter(tags__in=[]))\n", "    def test_isnullwithnoneasrhs(self):\n        self.assertEqual(IsNullWithNoneAsRHS.objects.filter(null_field__isnull=None).count(), 1)\n        self.assertEqual(IsNullWithNoneAsRHS.objects.filter(null_field__isnull=False).count(), 0)\n", "    def test_year_exact_lookup(self):\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__exact=2005),\n            ['<Article: Article 1>', '<Article: Article 2>', '<Article: Article 3>', '<Article: Article 4>', '<Article: Article 5>', '<Article: Article 6>', '<Article: Article 7>'],\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__year__exact=2006),\n            [],\n        )  \n"], "sample_305": ["    def test_window_functions(self):\n        # Test ranking\n        ranked_books = Book.objects.annotate(\n            rank=Window(\n                expression=RowNumber(),\n                partition_by=F('isbn'),\n                order_by=F('rating').desc()\n            )\n        ).filter(rank=1)\n        self.assertQuerysetEqual(ranked_books, [self.b1, self.b4, self.b5, self.b6], attrgetter('isbn'))\n        \n        # Test row number\n        row_numbered_authors = Author.objects.annotate(\n            row_num=Window(expression=RowNumber(), order_by=F('name').asc())\n        )\n        self.assertEqual(row_numbered_authors.first().row_num, 1)\n\n        # Test lead and lag\n        prev_price = Book.objects.annotate(\n            prev_price=Window(expression=Lag(F('price'), 1), order_by=F('pubdate').asc())\n        ).filter(name__startswith='The Definitive Guide')\n        self.assertEqual(prev_price.first().prev_price, None)\n\n        next_price = Book.objects.annotate(\n            next_price=Window(expression=Lead(F('price'), 1), order_by=F('pubdate').asc())\n        ).filter(name__startswith='Practical Django Projects')\n\n        self.assertEqual(next_price.first().next_price, Decimal('29.69'))\n\n", "    def test_when_then_aggregate(self):\n        # Test using WHEN/THEN with aggregate functions. This tests the handling\n        # of complex Case statements when used with aggregates.\n        qs = Book.objects.annotate(\n            price_range=Case(\n                When(price__lt=Decimal('30'), then=Value('cheap')),\n                When(price__gte=Decimal('30'), then=Value('expensive')),\n                default=Value('unknown'),\n                output_field=CharField(),\n            )\n        ).values('price_range').annotate(\n            count=Count('price_range')\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [{'price_range': 'cheap', 'count': 3}, {'price_range': 'expensive', 'count': 3}],\n        )\n", "    def test_aggregate_ordering(self):\n        with self.assertNumQueries(1):\n            result = Book.objects.annotate(num_authors=Count('authors')).order_by('num_authors', '-rating')\n        self.assertEqual(result[0].num_authors, 3)\n        self.assertEqual(result[0].name, 'Artificial Intelligence: A Modern Approach')\n", "    def test_case_when_aggregate(self):\n        q = Author.objects.annotate(\n            age_category=Case(\n                When(age__lt=30, then='Young'),\n                When(age__lte=40, then='Adult'),\n                default='Senior'\n            ),\n        ).values('age_category').annotate(count=Count('id'))\n        self.assertQuerysetEqual(\n            q,\n            [\n                {'age_category': 'Young', 'count': 3},\n                {'age_category': 'Adult', 'count': 4},\n                {'age_category': 'Senior', 'count': 2},\n            ],\n            transform=lambda x: {k: x[k] for k in ['age_category', 'count']},\n        )\n", "    def test_time_field_microsecond(self):\n        Store.objects.create(\n            name='Bookseller',\n            original_opening=datetime.datetime(\n                2023, 10, 26, 10, 15, 30, 999999\n            ),\n            friday_night_closing=datetime.time(20, 30)\n        )\n        store = Store.objects.get(name='Bookseller')\n        self.assertObjectAttrs(\n            store,\n            original_opening=datetime.datetime(2023, 10, 26, 10, 15, 30, 999999),\n        )\n"], "sample_1026": ["    def test_lambdify_tensorflow_issue_18400():\n        # Issue 18400: lambdify with tensorflow fails for piecewise functions\n        if not tensorflow:\n            skip(\"tensorflow not installed\")\n        f = Piecewise((1, x < 0), (2, x >= 0))\n        tf_f = lambdify(x, f, 'tensorflow')\n        assert tf_f(-1) == 1\n        assert tf_f(1) == 2\n", "def test_tensorflow_printing():\n    if not tensorflow:\n        skip(\"TensorFlow is not available.\")\n\n    expr = x**2 + sin(y)\n    f = lambdify((x, y), expr, 'tensorflow')\n    tf_x = tensorflow.placeholder(tensorflow.float32)\n    tf_y = tensorflow.placeholder(tensorflow.float32)\n\n    res = f(tf_x, tf_y)\n    assert isinstance(res, tensorflow.Tensor)\n", "def test_lambdify_piecewise_tensorflow():\n    if not tensorflow:\n        skip(\"Tensorflow is not installed\")\n    \n    x = symbols('x')\n    f = Piecewise((0, x < 0), (1, x >= 0))\n    f_lam = lambdify(x, f, 'tensorflow')\n    assert f_lam(tensorflow.constant(-1)).numpy() == 0\n    assert f_lam(tensorflow.constant(1)).numpy() == 1\n\n", "def test_lambdify_tensorflow_nested_functions():\n    if not tensorflow:\n        skip(\"Tensorflow is not installed.\")\n    f = implemented_function(Function('f'), lambda x: x**2)\n    g = implemented_function(Function('g'), lambda x: f(x) + 1)\n    x = tensorflow.constant(2.0)\n    func = lambdify(x, g(x), \"tensorflow\")\n    assert func(x).numpy() == 5.0\n\n", "def test_issue_17609():\n    f = lambdify(x, x**2, 'tensorflow')\n    assert isinstance(f(tensorflow.constant(2.0)), tensorflow.Tensor)\n"], "sample_925": ["    def test_mock_module_attributes(self):\n        with mock(['test_module']):\n            test_module = import_module('test_module')\n            assert isinstance(test_module, _MockModule)\n            assert hasattr(test_module, 'some_attribute')\n            assert isinstance(test_module.some_attribute, _MockObject)\n", "    def test_mock_submodule_access(self):\n        with mock(['target']):\n            target = import_module('target')\n            assert isinstance(target, _MockModule)\n            assert target.submodule == _MockObject('submodule', 'target')\n            assert target.submodule.nested_attr == _MockObject('nested_attr', 'target.submodule')\n", "    def test_mock_module(self) -> None:\n        with mock(['testing.module']):\n            testing_module = import_module('testing.module')\n            assert isinstance(testing_module, _MockModule)\n            assert testing_module.__name__ == 'testing.module'\n            assert hasattr(testing_module, '__file__')\n            assert testing_module.__file__ == os.devnull\n\n", "    def test_mock_module_attributes(self) -> None:\n        with mock(['mymodule']):\n            mymodule = import_module('mymodule')\n            assert isinstance(mymodule, _MockModule)\n            assert mymodule.__name__ == 'mymodule'\n            assert hasattr(mymodule, 'some_attribute')\n            assert isinstance(mymodule.some_attribute, _MockObject)\n            assert mymodule.some_attribute.__display_name__ == 'mymodule.some_attribute'\n", "    def test_mock_module_attributes(self):\n        with mock(['test_module']):\n            test_module = import_module('test_module')\n            assert isinstance(test_module, _MockModule)\n            assert hasattr(test_module, 'Attribute1')\n            assert isinstance(test_module.Attribute1, _MockObject)\n            assert test_module.Attribute1.__display_name__ == 'test_module.Attribute1'\n"], "sample_480": ["    def test_key_transform_exact_with_null_lookup(self):\n        with self.assertRaisesMessage(IntegrityError, \"null value in column \\\"key\\\" violates not null constraint\"):\n            NullableJSONModel.objects.create(key=None, other_field=\"hello\")\n\n        obj = NullableJSONModel.objects.create(key={\"nested\": 1}, other_field=\"hello\")\n        self.assertEqual(NullableJSONModel.objects.filter(key__nested=1).count(), 1)\n        self.assertEqual(NullableJSONModel.objects.filter(key__nested__isnull=False).count(), 1)\n        self.assertEqual(NullableJSONModel.objects.filter(key__nested=None).count(), 0)\n        self.assertEqual(NullableJSONModel.objects.filter(key__nested__isnull=True).count(), 0)\n", "    def test_transform_function_with_keytransform(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO jsonmodel (json_data) VALUES (%s)\", [\n                json.dumps({\"foo\": {\"bar\": 123}})\n            ])\n            cursor.execute(\"INSERT INTO jsonmodel (json_data) VALUES (%s)\", [\n                json.dumps({\"foo\": {\"bar\": 456}})\n            ])\n\n        q = JSONModel.objects.annotate(\n            foo_bar=ExpressionWrapper(\n                F(\"json_data__foo__bar\"),\n                output_field=IntegerField(),\n            )\n        ).filter(foo_bar__gt=100)\n        self.assertQuerysetEqual(q, [\n            json.loads(obj.json_data) for obj in JSONModel.objects.all()[1:]\n        ])\n\n", "    def test_jsonfield_keytransform_isnull_false(self):\n        self.assertEqual(\n            JSONModel.objects.filter(data__key__isnull=False).count(),\n            JSONModel.objects.filter(data__key=\"value\").count(),\n        )\n", "    def test_key_transform_exact_lookup_with_null_rhs(self):\n        \"\"\"\n        Tests KeyTransformExact lookup with a null rhs value.\n        \"\"\"\n        obj = JSONModel.objects.create(data={'name': 'test'})\n        with self.assertRaises(OperationalError):\n            JSONModel.objects.filter(data__name__exact=None).get()\n", "    def test_key_transform_regex_lookup_case_insensitive(self):\n        json_data = {'name': 'john doe', 'age': 30}\n        j = JSONModel.objects.create(data=json_data)\n        with self.assertQuerySetEqual(\n            JSONModel.objects.filter(data__name__iregex=r\"^john$\"), [j]\n        ):\n            pass\n"], "sample_372": ["compilation error", "    def test_reverse_lazy_with_namespace(self):\n        with self.settings(ROOT_URLCONF='urlpatterns_reverse.urls'):\n            url = reverse_lazy('inc-ns1:inc-ns3:urlobject-view', kwargs={'arg1': '42', 'arg2': '37'})\n            self.assertEqual(url.url, '/test1/inner/42/37/')\n\n", "compilation error", "compilation error", "compilation error"], "sample_915": ["    def test_stringify_signature_partialmethod(self):\n            pass\n        pm = functools.partialmethod(foo, 1) # type: ignore\n        sig = inspect.signature(pm)\n        result = inspect.stringify_signature(sig)\n        assert result == '(a, b, /, c)'\n\n", "    def test_signature_from_str_with_annotation(self):\n        sig = inspect.signature_from_str('(x: int, y: str) -> bool')\n        assert sig.parameters['x'].annotation == int\n        assert sig.parameters['y'].annotation == str\n        assert sig.return_annotation == bool\n\n", "    def test_stringify_signature_with_annotation_and_default(self):\n            pass\n        sig = inspect.signature(func)\n        result = inspect.stringify_signature(sig)\n        assert result == '(a: int = 1, b: str = \\'b\\', *args, c: float = 3.14, **kwargs)'\n\n", "    def test_stringify_signature(self):\n                 **kwargs: bool) -> str:\n            pass\n        sig = inspect.signature(func)\n        assert stringify_signature(sig) == '(a: int, b: str, *args: float, c: datetime.datetime = None, **kwargs: bool) -> str'\n", "    def test_signature_from_str(self):\n        sig = inspect.signature_from_str('(x: int, y: str, *args: float, z: datetime.datetime = None, **kwargs)')\n        assert isinstance(sig, inspect.Signature)\n        assert sig.parameters['x'].annotation == inspect.Parameter.empty\n        assert sig.parameters['y'].annotation == Parameter.empty\n        assert sig.parameters['args'].annotation == Parameter.empty\n        assert sig.parameters['z'].default == Parameter.empty\n        assert sig.return_annotation == Parameter.empty\n"], "sample_496": ["    def test_custom_command(self):\n        self.write_settings('settings.py')\n        os.environ['DJANGO_SETTINGS_MODULE'] = 'settings.py'\n        from django.core.management.base import BaseCommand\n\n        class MyCommand(BaseCommand):\n            help = \"My custom command\"\n\n                self.stdout.write(self.style.SUCCESS('Success!'))\n\n        call_command('mycommand')\n        output = StringIO.getvalue()\n        self.assertIn('Success!', output)\n", "    def test_command_output_with_custom_stdout(self):\n        \"\"\"\n        Test that the output of a command can be redirected.\n        \"\"\"\n        # Write out our test settings\n        self.write_settings('settings.py')\n        os.environ['DJANGO_SETTINGS_MODULE'] = 'settings'\n        # Create temporary file object to hold stdout\n        out = StringIO()\n\n        with mock.patch('sys.stdout', new=out):\n            call_command(\n                'runserver',\n                stdout=out,\n                verbosity=0,\n            )\n\n        output = out.getvalue().strip('\\n')\n        # The output contains the address the development server listens on.\n        self.assertIn('Starting development server', output)\n        self.assertIn('Quit the server with CONTROL-C.', output)\n\n", "    def test_runserver_with_custom_settings(self):\n        self.write_settings('settings.py')\n\n        command_path = os.path.join(self.test_dir, 'manage.py')\n        out = subprocess.check_output(\n            [sys.executable, command_path, 'runserver', '0.0.0.0:8001'],\n            cwd=self.test_dir,\n            stderr=subprocess.STDOUT,\n        ).decode('utf-8')\n\n        self.assertIn(\"Django server is running\", out)\n        self.assertIn(\"Using the development server\", out)\n", "    def test_runserver_with_custom_port(self):\n        \"\"\"\n        Test that the runserver command can be run with a custom port.\n        \"\"\"\n        self.write_settings('settings.py', extra='PORT = 8080')\n        try:\n            with mock.patch.object(socket, 'socket', mock.MagicMock()):\n                out = StringIO()\n                err = StringIO()\n                call_command('runserver', stdout=out, stderr=err, port=8080)\n                self.assertIn(\n                    \"Starting development server at http://127.0.0.1:8080/\",\n                    out.getvalue(),\n                )\n        finally:\n            self.remove_settings('settings.py')\n\n", "    def test_system_check_with_missing_migration(self):\n        \"\"\"\n        Test that the system check raises an error when a required migration\n        is missing.\n        \"\"\"\n        self.write_settings('settings.py', apps=['myapp', 'django.contrib.auth', 'django.contrib.contenttypes'])\n\n        # Create a dummy migration to simulate a missing migration.\n        dummy_migration_file = os.path.join(self.test_dir, 'myapp', 'migrations', '0001_initial.py')\n        with open(dummy_migration_file, 'w') as f:\n            f.write('')\n        \n        try:\n            call_command('check', stdout=StringIO(), stderr=StringIO(), settings={'ENGINE': 'django.db.backends.sqlite3'})\n            self.fail(\"System check should have raised an error due to missing migration.\")\n        except CommandError as e:\n            self.assertIn('Migrationmyapp.0001_initial', str(e))\n        finally:\n            os.remove(dummy_migration_file)\n"], "sample_45": ["    def test_extract_iso_year(self):\n        dt = datetime(2023, 12, 31, tzinfo=timezone.utc)\n        obj = self.create_model(dt, dt + timedelta(hours=1))\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=2023).count(), 1)\n        dt = datetime(2024, 1, 1, tzinfo=timezone.utc)\n        obj = self.create_model(dt, dt + timedelta(hours=1))\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=2024).count(), 1)\n", "    def test_year_lookup(self):\n        self.create_model(\n            start_datetime=datetime(2022, 10, 26, 10, 0, 0),\n            end_datetime=datetime(2022, 10, 27, 10, 0, 0),\n        )\n        self.create_model(\n            start_datetime=datetime(2023, 1, 1, 0, 0, 0),\n            end_datetime=datetime(2023, 1, 1, 1, 0, 0),\n        )\n        qs = DTModel.objects.filter(start_datetime__year=2022)\n        self.assertEqual(qs.count(), 1)\n", "    def test_trunc_date(self):\n        dt = datetime(2023, 4, 12, 15, 30, 25)\n        obj = self.create_model(dt, dt + timedelta(days=2))\n        self.assertEqual(DTModel.objects.filter(start_datetime__trunc_date=dt.date()).exists(), True)\n        self.assertEqual(DTModel.objects.filter(start_datetime__trunc_date__year=2023).exists(), True)\n        self.assertEqual(DTModel.objects.filter(start_datetime__trunc_date__month=4).exists(), True)\n", "    def test_extract_iso_year(self):\n        model = self.create_model(\n            start_datetime=datetime(2023, 12, 31), end_datetime=datetime(2024, 1, 1)\n        )\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=2023).count(), 1)\n        self.assertEqual(DTModel.objects.filter(end_datetime__iso_year=2024).count(), 1)\n", "    def test_extract_week_day(self):\n        now = timezone.now()\n        monday = now - timedelta(days=now.weekday())\n        tuesday = monday + timedelta(days=1)\n        wednesday = monday + timedelta(days=2)\n        weekend = monday + timedelta(days=5)\n\n        self.create_model(monday, None)\n        self.create_model(tuesday, None)\n        self.create_model(wednesday, None)\n        self.create_model(weekend, None)\n\n        self.assertEqual(DTModel.objects.annotate(weekday=ExtractWeekDay('start_datetime')).filter(weekday=1).count(), 1)\n        self.assertEqual(DTModel.objects.annotate(weekday=ExtractWeekDay('start_datetime')).filter(weekday=2).count(), 1)\n        self.assertEqual(DTModel.objects.annotate(weekday=ExtractWeekDay('start_datetime')).filter(weekday=3).count(), 1)\n        self.assertEqual(DTModel.objects.annotate(weekday=ExtractWeekDay('start_datetime')).filter(weekday=6).count(), 1)\n\n"], "sample_663": ["    def test_matchnodes_dotted_name(self, tmpdir):\n        p = tmpdir.mkdir(\"subpkg\").join(\"test_module.py\")\n        p.write(\"def test_func(): pass\")\n        session = Session(config=self.config)\n        session.trace.root.indent = 0\n\n        res = session._matchnodes([session], [\"subpkg.test_module\"])\n        assert len(res) == 1\n        assert res[0].nodeid == p.basename\n\n        res = session._matchnodes([session], [\"subpkg.test_module.test_func\"])\n        assert len(res) == 1\n        assert res[0].nodeid == p.basename + \"::test_func\"\n\n        res = session._matchnodes([session], [\"subpkg.test_module\", \"test_func\"])\n        assert len(res) == 1\n        assert res[0].nodeid == p.basename + \"::test_func\"\n", "    def test_in_venv(self):\n        here = os.path.dirname(__file__)\n        venv = os.path.join(here, \"venv\")\n        if not os.path.exists(venv):\n            os.makedirs(venv)\n            subprocess.check_call([\"python3\", \"-m\", \"venv\", venv])\n        assert _in_venv(py.path.local(venv)) is True\n", "    def test_in_venv(self):\n        with py.path.local(__file__).join(\"venv\").mkdir() as venv_dir:\n            venv_dir.join(\"bin\").ensure(dir=True)\n\n            # Create an empty __init__.py file to mimic a package structure.\n            venv_dir.join(\"__init__.py\").touch()\n\n            assert _in_venv(venv_dir)\n\n", "    def test_in_venv(self):\n        venv_dir = py.path.local(os.path.join(sys.prefix, \"virtualenv\"))\n        assert _in_venv(venv_dir)\n        assert not _in_venv(venv_dir.parent)\n", "    def test_in_venv(self, tmpdir):\n        venv = tmpdir.join('venv')\n        venv.ensure(dir=True)\n        (venv.join('bin').mkdir().join('activate').write(\n            '#!/bin/sh\\n echo \"activated\"'))\n        py.process.cmdexec('chmod', '+x', venv.join('bin/activate'))\n        assert _in_venv(venv)\n        assert not _in_venv(tmpdir)\n"], "sample_387": ["    def test_formfield_for_dbfield_uuidfield(self):\n        ff = self.assertFormfield(Company, 'uuid', forms.UUIDField)\n        self.assertTrue(ff.required)\n", "    def test_formfield_for_dbfield_manytomanyfield(self):\n        ff = self.assertFormfield(Band, 'members', widgets.ManyToManyInput)\n        self.assertIn('class=\"ManyToManyWidget\"', ff.as_widget().attrs['class'])\n", "    def test_modelform_defines_fields_for_formfield_for_dbfield(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            form = MyForm\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = Car\n                fields = \"__all__\"\n\n        ma = MyModelAdmin(Car, admin.site)\n        ff = ma.formfield_for_dbfield(Car._meta.get_field(\"make\"), request=None)\n        self.assertIsInstance(ff.widget, forms.TextInput)\n\n", "    def test_formfield_for_dbfield_uuidfield(self):\n        ff = self.assertFormfield(\n            Company, \"uuid\", forms.UUIDField,\n        )\n        self.assertEqual(ff.initial, None)\n", "    def test_formfield_for_dbfield_choices(self):\n        \n        ff = self.assertFormfield(\n            Advisor,\n            \"student\",\n            widgets.Select,\n            list_display=['student__name'],\n        )\n        self.assertEqual(ff.choices, [(obj.pk, str(obj)) for obj in Student.objects.all()])\n"], "sample_752": ["    def test_contamination_auto(self):\n        # Test that the contamination parameter is properly handled when set to\n        # \"auto\".\n        X = np.random.randn(100, 5)\n        y = np.zeros(100)\n        for contamination in ['auto', 0.1]:\n            model = IsolationForest(contamination=contamination,\n                                    random_state=0).fit(X)\n            assert_almost_equal(model.offset_, -0.5, decimal=2)\n\n", "    def test_prediction_consistency():\n        # test if the same prediction is obtained when providing the same data\n        # in different format (dense, sparse)\n        X_train, X_test = iris.data[:100], iris.data[100:]\n        clf = IsolationForest(random_state=0)\n        clf.fit(X_train)\n\n        y_pred_dense = clf.predict(X_test)\n\n        X_test_sparse = csc_matrix(X_test)\n        y_pred_sparse = clf.predict(X_test_sparse)\n\n        assert_array_equal(y_pred_dense, y_pred_sparse)\n\n", "    def test_contamination_auto(self):\n    \n        n_samples = 100\n        X = rng.uniform(size=(n_samples, 2))\n        y = np.ones(n_samples)\n\n        for contamination in [0.1, 'auto']:\n        \n\n            # Test contamination handling\n            iforest = IsolationForest(contamination=contamination)\n            iforest.fit(X)\n            if contamination == 'auto':\n                assert_equal(iforest.offset_, -0.5)\n            else:\n                assert_almost_equal(iforest.offset_,\n                                     sp.stats.scoreatpercentile(\n                                         iforest.score_samples(X),\n                                         100. * contamination))\n", "    def test_sparse_input(self):\n        # Test isolation forest with sparse input\n        X, y = boston.data, boston.target\n        X_sparse = csc_matrix(X)\n\n        # Test with sparse data\n        clf = IsolationForest(random_state=0).fit(X_sparse)\n        scores = clf.score_samples(X_sparse)\n\n        # Assert that shapes are correct\n        assert_equal(scores.shape, (X_sparse.shape[0],))\n\n        # Test with dense data\n        clf_dense = IsolationForest(random_state=0).fit(X)\n        scores_dense = clf_dense.score_samples(X)\n\n        # Assert scores are almost equal\n        assert_array_almost_equal(scores, scores_dense, decimal=6)\n\n", "    def test_contamination_auto_contamination(self):\n        # Test that contamination is correctly inferred when set to 'auto'\n\n        X = rng.randn(100, 2)\n        # set 10% of data as outliers\n        X[10:] = 10 * X[10:]\n\n        clf = IsolationForest(contamination='auto')\n        clf.fit(X)\n\n        # check if contamination is close to 0.1\n        assert_almost_equal(clf.contamination_, 0.1, decimal=2)\n\n"], "sample_761": ["    def test_missing_indicator_with_errors(self):\n        X = np.array([[1, 2],\n                      [np.nan, 4],\n                      [np.nan, np.nan]])\n        indicator = MissingIndicator(error_on_new=True)\n\n        # Fit on a subset of features\n        indicator.fit(X[:, 0].reshape(-1, 1))\n\n        # Test that predicting on all features raises an error\n        with pytest.raises(ValueError):\n            indicator.transform(X)\n", "def test_missing_indicator_error_on_new_features():\n    X_train = np.array([[1, np.nan, 3],\n                        [4, 0, np.nan],\n                        [8, 1, 0]])\n    X_test = np.array([[5, 1, np.nan],\n                       [np.nan, 2, 3],\n                       [2, 4, 0]])\n\n    transformer = MissingIndicator(features='missing-only', error_on_new=True)\n    transformer.fit(X_train)\n\n    with pytest.raises(ValueError):\n        transformer.transform(X_test)\n", "    def test_missing_indicator_sparse_missing_only(self):\n        # Tests MissingIndicator with missing_values=np.nan, features='missing-only'\n        # and sparse input.\n\n        X = np.array([[1, np.nan, 3],\n                      [4, 0, np.nan],\n                      [8, 1, 0]])\n        X_sparse = sparse.csc_matrix(X)\n\n        indicator = MissingIndicator(missing_values=np.nan,\n                                     features='missing-only',\n                                     sparse='auto')\n        X_trans = indicator.fit_transform(X_sparse)\n\n        # Check that only the features with missing values are returned\n        assert_array_equal(indicator.features_, np.array([1, 2]))\n\n        # Check the shape of the transformed array\n        assert X_trans.shape == (3, 2)\n\n        # Check that the transformed array contains the correct values\n        indicator_true = np.array([[False, True],\n                                   [ True, True],\n                                   [False, False]])\n        assert_array_equal(X_trans.toarray(), indicator_true)\n\n", "    def test_missing_indicator_error_on_new_features():\n        X1 = np.array([[np.nan, 1, 3],\n                       [4, 0, np.nan],\n                       [8, 1, 0]])\n        X2 = np.array([[5, 1, np.nan],\n                       [np.nan, 2, 3],\n                       [2, 4, 0]])\n\n        imp = MissingIndicator(features='missing-only', error_on_new=True)\n        imp.fit(X1)\n        with pytest.raises(ValueError, match=\"have no missing values in fit\"):\n            imp.transform(X2)\n\n", "    def test_iterative_imputer_categorical(self):\n        # Test IterativeImputer with categorical data\n        X = np.array([[1, 'a', 3],\n                      [4, 'b', np.nan],\n                      [8, np.nan, 0],\n                      [np.nan, 'a', 5]])\n        X_true = np.array([[1, 'a', 3],\n                           [4, 'b', 1],\n                           [8, 'b', 0],\n                           [4, 'a', 5]])\n        imputer = IterativeImputer(max_iter=5, random_state=0)\n        X_trans = imputer.fit_transform(X)\n        assert_array_almost_equal(X_trans, X_true)\n\n"], "sample_804": ["    def test_onehot_dtype():\n        X = np.array([[0, 1], [1, 0]])\n        enc = OneHotEncoder(dtype=np.int32)\n        X_t = enc.fit_transform(X)\n        assert X_t.dtype == np.int32\n        assert_array_equal(X_t.toarray(),\n                           np.array([[1, 0, 0, 1],\n                                     [0, 1, 1, 0]]))\n\n        enc = OneHotEncoder(sparse=False, dtype=np.float32)\n        X_t = enc.fit_transform(X)\n        assert X_t.dtype == np.float32\n", "    def test_one_hot_encoder_handle_unknown_ignore():\n            enc = OneHotEncoder(handle_unknown='ignore')\n            X = [['A', 1], ['B', 2], ['C', 1]]\n            enc.fit(X)\n            X_trans = enc.transform([['A', 1], ['B', 3]])\n\n            assert_array_equal(X_trans.toarray(), [[1., 0., 1., 0.], [0., 1., 0., 0.]])\n\n", "    def test_onehotencoder_drop_first():\n        X = np.array([[0, 1], [1, 2], [0, 2]], dtype=object)\n        ohe = OneHotEncoder(drop='first', sparse=False)\n        fit_transform = ohe.fit_transform(X)\n\n        expected = np.array([[0., 1.],\n                             [1., 0.],\n                             [0., 0.]])\n        assert_allclose(fit_transform, expected)\n", "    def test_one_hot_encode_drop_unknown_with_sparse_input(self):\n        X = np.array([[0, 1], [2, 1], [0, 3], [1, 2]]).astype(object)\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True, drop='first')\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        assert X_trans.shape == (4, 4)\n        assert toarray(X_trans).sum() == 4\n", "    def test_onehot_fit_transform_drop_with_unknown(self):\n        X = np.array([['a', 'b'], ['a', 'c'], ['b', 'a']],\n                    dtype=object)\n        enc = OneHotEncoder(handle_unknown='ignore',\n                           sparse=False, drop='first')\n        with pytest.raises(ValueError):\n            enc.fit_transform(X)\n"], "sample_630": ["def test_class_diagram_with_attributes_and_methods(setup):\n    \"\"\"Test if class diagram includes attributes and methods.\"\"\"\n    generated_output = _file_lines(DOT_FILES[1])\n    expected_output = _file_lines(\n        os.path.join(os.path.dirname(__file__), \"expected\", \"classes_No_Name.dot\")\n    )\n    diff = list(unified_diff(expected_output, generated_output))\n    assert not diff, \"\\n\".join(diff)\n", "def test_dot_files(generated_file):\n    expected_file = os.path.join(os.path.dirname(__file__), \"data\", generated_file)\n    with open(generated_file, \"r\") as f:\n        generated_content = f.readlines()\n    with open(expected_file, \"r\") as f:\n        expected_content = f.readlines()\n    diff = list(unified_diff(expected_content, generated_content))\n    assert not diff, \"\\n\".join(diff)\n", "    def test_dot_diagram_output(self, generated_file):\n        \"\"\"compare generated dot file with expected output\"\"\"\n        expected_file = os.path.join(os.path.dirname(__file__), \"data\", generated_file)\n        with open(generated_file, \"r\") as f:\n            generated_content = f.read()\n        with open(expected_file, \"r\") as f:\n            expected_content = f.read()\n        diff = list(unified_diff(expected_content.splitlines(), generated_content.splitlines()))\n        assert not diff, \"\\n\".join(diff)\n", "    def test_dot_file_equal(self, generated_file):\n        \"\"\"compare generated dot file with expected result.\"\"\"\n        expected_file = os.path.join(os.path.dirname(__file__), \"data\", generated_file)\n        with open(generated_file) as f_generated, open(expected_file) as f_expected:\n            diff = list(\n                unified_diff(\n                    f_expected.readlines(), f_generated.readlines(), lineterm=\"\"\n                )\n            )\n        assert not diff, \"\\n\".join(diff)\n\n", "    def test_classes_with_inheritance(self, generated_file, tmpdir):\n        \"\"\"test diagram generation with inheritance.\"\"\"\n        expected_file = os.path.join(os.path.dirname(__file__), \"expected\", generated_file)\n        with patch(\"pylint.pyreverse.writer.DotWriter.close_graph\"):\n            writer = DotWriter(self.CONFIG)\n            writer.write_classes = MagicMock()\n            writer.write_packages = MagicMock()\n            writer.close_graph = MagicMock()\n\n        with open(expected_file) as f:\n            expected_lines = _file_lines(f.name)\n\n        actual_lines = _file_lines(generated_file)\n        diff = list(unified_diff(expected_lines, actual_lines))\n        assert not diff, \"Generated dot file is different from expected:\\n\" + \"\\n\".join(diff)\n"], "sample_198": ["    def test_window_frame_value_range_rows(self):\n        with self.assertRaises(\n            TypeError,\n            msg='ValueRange frame clause requires numeric arguments.'\n        ):\n            Employee.objects.annotate(\n                rank=Window(\n                    Rank().partition_by(F('company')).order_by(F('salary').desc()),\n                    frame=ValueRange(start='abc', end=10),\n                )\n            ).first()\n\n", "    def test_window_function_case(self):\n        query = (\n            Company.objects.annotate(\n                avg_chairs=Avg(\"num_chairs\"),\n            )\n            .filter(num_employees__gt=10)\n            .annotate(\n                rank=Window(\n                    Func(\"rank\", F(\"num_employees\"), order_by=F(\"num_employees\").desc()),\n                    partition_by=F(\"avg_chairs\"),\n                )\n            )\n        )\n        self.assertGreater(query.count(), 0)\n", "    def test_case_when_subquery(self):\n        query = Employee.objects.annotate(\n            ceo_salary_comparison=Case(\n                When(salary__gt=Subquery(Company.objects.filter(id=OuterRef('company_id')).values('ceo__salary')), then=Value('higher')),\n                default=Value('lower'),\n                output_field=CharField()\n            )\n        ).filter(ceo_salary_comparison='higher').order_by('firstname')\n\n        self.assertQuerysetEqual(\n            query,\n            [\n                'Joe',\n                'Frank',\n            ],\n            lambda e: [e.firstname]\n        )\n", "    def test_exists_negated(self):\n        # Test negated EXISTS\n        query = Company.objects.annotate(\n            has_employees=Exists(Employee.objects.filter(company=OuterRef('pk'))),\n        ).filter(~Q(has_employees=True))\n        self.assertQuerysetEqual(query, [self.foobar_ltd])\n\n\n\n", "    def test_expression_wrapper(self):\n        wrapped = ExpressionWrapper(Value(10), output_field=IntegerField())\n        q = self.company_query.annotate(constant=wrapped)\n        self.assertEqual(\n            list(q.values_list(\"constant\")),\n            [(10,) for _ in range(len(self.company_query.all()))],\n        )\n\n"], "sample_165": ["    def test_modelform_field_error_messages(self):\n        class TestForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = ['choice']\n                error_messages = {\n                    'choice': {\n                        'invalid_choice': 'Invalid choice provided'\n                    }\n                }\n\n        form = TestForm(data={'choice': 'invalid'})\n        self.assertFormErrors([{'choice': ['Invalid choice provided']}], form.is_valid)\n", "    def test_modelmultiplechoicefield_empty_choices(self):\n        class MyForm(Form):\n            choices = ModelMultipleChoiceField(queryset=ChoiceModel.objects.none())\n        form = MyForm()\n        self.assertEqual(form.errors, {})\n        self.assertEqual(form.cleaned_data['choices'], [])\n", "    def test_modelmultiplechoicefield_invalid_list(self):\n        field = ModelMultipleChoiceField(ChoiceModel.objects.all())\n        with self.assertRaises(ValidationError) as cm:\n            field.clean(['a', 'b'])\n        self.assertEqual(cm.exception.messages, [\n            '\u201ca\u201d is not a valid value.',\n            '\u201cb\u201d is not a valid value.',\n        ])\n", "    def test_modelform_field_empty_label(self):\n        class MyModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = '__all__'\n                widgets = {\n                    'choice_field': Select(choices=[(\"a\", \"A\"), (\"b\", \"B\"), (\"c\", \"C\")],)\n                }\n\n        form = MyModelForm()\n        self.assertEqual(form.fields['choice_field'].empty_label, None)\n\n        class MyModelForm(ModelForm):\n            class Meta:\n                model = ChoiceModel\n                fields = '__all__'\n                widgets = {\n                    'choice_field': Select(choices=[(\"a\", \"A\"), (\"b\", \"B\"), (\"c\", \"C\")], empty_label=\"Select a choice\")\n                }\n        form = MyModelForm()\n        self.assertEqual(form.fields['choice_field'].empty_label, \"Select a choice\")\n", "    def test_modelmultiplechoicefield_empty_values(self):\n        self.assertEqual(ModelMultipleChoiceField(ChoiceModel.objects.all()).empty_values, ['', None])\n"], "sample_291": ["    def test_decorated_dispatch(self):\n        view = DecoratedDispatchView()\n        request = self.rf.get('/')\n        response = view.dispatch(request)\n        self.assertTrue(getattr(view.dispatch, 'is_decorated', False))\n        self._assert_simple(response)\n", "    def test_dispatch_missing_method(self):\n        view = SimpleView()\n        with self.assertRaisesMessage(AttributeError,\n                                      'SimpleView instance has no \\'request\\' attribute. Did you override setup() and forget to call super()?'):\n            view.dispatch(self.rf.get('/'))\n", "    def test_view_as_view_with_initkwargs(self):\n        view = views.SimpleView.as_view(parameter='value')\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'This is a simple view')\n        self.assertEqual(view.parameter, 'value')\n", "    def test_view_get_context_data(self):\n        view = CustomizableView(parameter={'key': 'value'})\n        context = view.get_context_data()\n        self.assertEqual(context['view'], view)\n        self.assertEqual(context['key'], 'value')\n", "    def test_view_instance(self):\n        view = InstanceView.as_view()\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response, view)\n"], "sample_972": ["    def test_restify_py36_union_none():\n        assert restify(Union[int, None]) == 'Optional[int]'\n\n", "    def test_restify_union_optional_with_none_type(self):\n        assert restify(Union[int, NoneType]) == 'Optional[int]'\n        assert restify(Union[int, None]) == 'Optional[int]'\n        assert restify(Optional[int]) == 'Optional[int]'\n        assert restify(Union[None, int]) == 'Optional[int]'\n        assert restify(Union[NoneType, int]) == 'Optional[int]'\n        assert restify(Union['a', NoneType]) == 'Optional[a]'\n\n\n", "    def test_restify_newtype(self):\n        assert restify(MyInt) == ':py:class:`MyInt`'\n", "    def test_restify_forwardref():\n        assert restify(typing.ForwardRef('MyClass')) == ':py:class:`MyClass'`\n        assert restify(typing.ForwardRef('module.MyClass')) == ':py:class:`module.MyClass'`\n", "    def test_restify_fully_qualified_except_typing():\n        assert restify(Union[str, int], mode='fully-qualified-except-typing') == \\\n            ':py:obj:`~typing.Union`\\\\ [str, int]'\n        assert restify(List[int], mode='fully-qualified-except-typing') == \\\n            ':py:class:`~typing.List`\\\\ [int]'\n\n"], "sample_1117": ["    def test_ask_diagonal_matrixslice_complex_elements(self):\n        A = MatrixSymbol('A', 2, 2)\n        M = MatrixSlice(A, (0, slice(None)))\n        assumptions = Q.complex_elements(A)\n        assert ask(Q.complex_elements(M), assumptions) is True\n\n", "def test_diagonal_matrix_diagonal():\n    assert ask(Q.diagonal(DiagonalMatrix(A1x1))) == True\n", "def test_AskIntegerElementsHandler_DiagMatrix():\n    x = Symbol('x')\n    assert ask(Q.integer_elements(DiagMatrix([x, 2])), {}) == True\n    assert ask(Q.integer_elements(DiagMatrix([x, 2])), {x: 2}) == True\n    assert ask(Q.integer_elements(DiagMatrix([x, 2])), {x: 2.5}) == False\n", "    def test_AskDiagonalHandler_ZeroMatrix(self):\n        z = ZeroMatrix(2, 2)\n        self.assertTrue(ask(Q.diagonal(z)))\n", "def test_AskSymmetricHandler_DiagonalMatrix():\n    A = DiagonalMatrix(Symbol('a'), Symbol('b'))\n    assert ask(Q.symmetric(A)) is True\n"], "sample_233": ["    def test_check_token_with_expired_token(self):\n        user = User.objects.create_user('johndoe')\n        generator = MockedPasswordResetTokenGenerator(now=datetime.now() - timedelta(days=2))\n        token = generator.make_token(user)\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_make_token_with_timestamp_legacy_algorithm(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2023, 10, 26, 10, 0))\n        timestamp = generator._num_seconds(generator._now())\n        token = generator._make_token_with_timestamp(user, timestamp, legacy=True)\n        self.assertEqual(\n            token,\n            '197322-8f8d09941104d2c4a5b343d361297877',\n        )\n\n", "    def test_make_token_with_timestamp_legacy_algorithm(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2023, 1, 1))\n        token = generator._make_token_with_timestamp(user, 1672531200, legacy=True)\n        self.assertEqual(\n            token, generator._make_token_with_timestamp(user, 1672531200, legacy=False)\n        ) \n", "    def test_token_valid_with_legacy_algorithm(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2023, 10, 26, 12, 0, 0))\n        token = generator.make_token(user)\n        self.assertTrue(generator.check_token(user, token))\n\n", "    def test_make_token_with_timestamp_legacy(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        generator = MockedPasswordResetTokenGenerator(datetime(2023, 1, 1))\n        token = generator._make_token_with_timestamp(user, 1672531200, legacy=True) # 2023-01-01 00:00:00\n        self.assertIn('-', token)\n        ts_b36, _ = token.split('-', 1)\n        self.assertEqual(base36_to_int(ts_b36), 1672531200)\n"], "sample_1121": ["def test_as_content_primitive_non_commutative():\n    from sympy.abc import A, B, C\n    \n    # Test case with non-commutative symbols\n    expr = A*B*C + A + B\n    content, primitive = expr.as_content_primitive(clear=True)\n    assert content == S.One\n    assert primitive == expr\n\n    expr = 2*A*B + A\n    content, primitive = expr.as_content_primitive(clear=True)\n    assert content == S.One\n    assert primitive == expr\n\n", "def test_expand_2arg():\n    assert expand_2arg(x*(y + z)) == x*y + x*z\n    assert expand_2arg(2*(x + y + z)) == 2*x + 2*y + 2*z\n    assert expand_2arg(2.5*(x + y)) == 2.5*x + 2.5*y\n    assert expand_2arg(x*(y + 2)) == x*y + 2*x\n    assert expand_2arg(a*sin(x + y)) == a*sin(x + y)\n", "def test_expand_2arg():\n    assert expand_2arg(Mul(x, x+y)) == x*x + x*y\n    assert expand_2arg(2*Add(x, y)) == 2*x + 2*y\n    assert expand_2arg(2*(x+y)) == 2*x + 2*y\n", "def test_Mul_evalf():\n    # Test evalf on various Mul expressions\n    assert Mul(1, 2).evalf() == 2\n    assert Mul(2, 3, evaluate=False).evalf() == 6\n\n    assert (2*sqrt(2)).evalf(3) == 2.828\n    assert (2*x).evalf(subs={x: 3}) == 6\n    assert (2*x).evalf(subs={x: S('1/2')}) == 1\n\n    assert Mul(x, y, evaluate=False).evalf(subs={x: 2, y: 3}) == 6\n    assert ((1 + I)* (2 - I)).evalf() == (3 + 2*I)\n\n\n\n", "def test_Mul_as_content_primitive():\n    assert (x*y).as_content_primitive() == (1, x*y)\n    assert (2*x*y).as_content_primitive() == (2, x*y)\n    assert (2*x*y).as_content_primitive(radical=True) == (2, x*y)\n    assert ((2*x*y)**2).as_content_primitive() == (4, x**2*y**2)\n    assert ((-2*x*y)**2).as_content_primitive() == (4, x**2*y**2)\n    assert ((-2*x*y)**2).as_content_primitive(clear=False) == (4, x**2*y**2)\n    assert (2*sqrt(2)*x).as_content_primitive() == (2*sqrt(2), x)\n    assert (2*sqrt(2)*x).as_content_primitive(radical=True) == (2, sqrt(2)*x)\n    assert (2*sqrt(2)*x).as_content_primitive(clear=False) == (2*sqrt(2), x)\n"], "sample_710": ["    def test_collect_unittest_class_with_setUpClass_tearDownClass(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                @classmethod\n                    pass\n\n                @classmethod\n                    pass\n\n                    pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_collect_test_case_with_setup_teardown(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTests(unittest.TestCase):\n                    pass\n\n                    pass\n\n                    pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == ExitCode.OK\n        # We expect a single test function\n        assert len(result.items) == 1\n        item = result.items[0]\n        assert \"test_foo\" in item.name\n\n        assert any(\n            \"setup\" in name and len(name.split(\".\")) == 2\n            for name in item.nodeid.split(\"::\")\n        )\n", "    def test_skip_reason_is_preserved(self, testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                @unittest.skip(\"some reason\")\n                    pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*1 skipped*\",\n                \"*reason: some reason*\",\n            ]\n        )\n", "    def test_collect_methods_with_skip_reason(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                reason = \"just skipping\"\n\n                @unittest.skip(reason)\n                    pass\n\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(skipped=1)\n        assert \"reason = 'just skipping'\" in result.stdout.str()\n", "    def test_pytest_pycollect_makeitem_returns_None_when_unittest_module_not_imported(\n        self, pytester: Pytester"], "sample_808": ["    def test_decision_function_matches_original_implementation(self):\n        # Test that the decision_function behaves as described in the original\n        # IsolationForest paper.\n        rng = check_random_state(0)\n        X = rng.randn(100, 2)\n        y = rng.randint(0, 2, size=100)\n\n        clf = IsolationForest(behaviour='old', contamination=0.1, random_state=0)\n        clf.fit(X, y)\n\n        # The decision function should be negative for outliers and positive for\n        # inliers\n        decision_function = clf.decision_function(X)\n\n        outlier_indices = np.where(y == 1)[0]\n        inlier_indices = np.where(y == 0)[0]\n\n        assert_greater(np.mean(decision_function[inlier_indices]),\n                       np.mean(decision_function[outlier_indices]))\n", "    def test_iforest_sparse_input(self):\n        # Test isolation forest with sparse input\n        X_csc_sparse = csc_matrix(boston.data)\n        X_csr_sparse = csr_matrix(boston.data)\n\n        for contamination in [0.1, 'auto']:\n            clf = IsolationForest(contamination=contamination, random_state=0)\n            clf.fit(X_csc_sparse)\n            y_pred_csc = clf.predict(X_csc_sparse)\n\n            clf.fit(X_csr_sparse)\n            y_pred_csr = clf.predict(X_csr_sparse)\n\n            assert_array_equal(y_pred_csc, y_pred_csr)\n", "    def test_contamination_auto_behaviour_new(self):\n        X_train = boston.data[:300]\n        y_train = boston.target[:300]\n        X_test = boston.data[300:]\n        y_test = boston.target[300:]\n\n        clf = IsolationForest(contamination='auto', behaviour='new', random_state=0)\n        clf.fit(X_train)\n        y_pred = clf.predict(X_test)\n        assert_array_equal(clf.offset_, -0.5)\n        score_samples = clf.decision_function(X_test)\n        assert_allclose(score_samples, clf.score_samples(X_test) - clf.offset_)\n\n\n\n", "    def test_sparse_input_decision_function(self):\n        # Check decision_function works with sparse input\n        X_train, X_test, y_train, y_test = train_test_split(\n            iris.data, iris.target, test_size=0.2, random_state=0\n        )\n        clf = IsolationForest(random_state=0).fit(X_train)\n        X_test_sparse = csc_matrix(X_test)\n        scores = clf.decision_function(X_test_sparse)\n        assert_array_equal(scores.shape, (X_test.shape[0],))\n\n        # Check that decision_function works with non-sparse input after\n        # training on sparse input\n        clf.fit(X_test_sparse)\n        scores = clf.decision_function(X_test)\n        assert_array_equal(scores.shape, (X_test.shape[0],))\n", "    def test_contamination_auto():\n        # Check if contamination set to 'auto' works correctly\n        X = iris.data[:100]\n        y = iris.target[:100]\n\n        iforest = IsolationForest(contamination=\"auto\", random_state=rng)\n        with assert_warns_message(\n                \"contamination parameter 'auto' will change the predict\"\n                \" method behavior in version 0.22.\",\n                FutureWarning):\n            iforest.fit(X, y)\n\n        assert_allclose(iforest.contamination_, 0.1)\n\n"], "sample_386": ["    def test_safe_string_add(self):\n        s = SafeString(\"Hello, \")\n        t = SafeString(\"world!\")\n        self.assertEqual(s + t, SafeString(\"Hello, world!\"))\n\n        s = SafeString(\"Hello, \")\n        t = \"world!\"\n        self.assertEqual(s + t, \"Hello, world!\")\n\n        s = \"Hello, \"\n        t = SafeString(\"world!\")\n        self.assertEqual(s + t, \"Hello, world!\")\n", "    def test_safe_string_concat(self):\n        self.assertIsInstance(mark_safe(\"Hello\") + mark_safe(\" world!\"), SafeString)\n        self.assertIsInstance(mark_safe(\"Hello\") + \" world!\", str)\n        self.assertEqual(mark_safe(\"Hello\") + mark_safe(\" world!\"), \"Hello world!\")\n        self.assertEqual(mark_safe(\"Hello\") + \" world!\", \"Hello world!\")\n\n", "    def test_safe_string_addition(self):\n        self.assertEqual(mark_safe(\"<b>foo</b>\") + mark_safe(\"<b>bar</b>\"), \"<b>foo</b><b>bar</b>\")\n        self.assertEqual(mark_safe(\"<b>foo</b>\") + \"bar\", \"<b>foo</b>bar\")\n        self.assertEqual(\"foo\" + mark_safe(\"<b>bar</b>\"), \"foo<b>bar</b>\")\n        self.assertIsInstance(mark_safe(\"<b>foo</b>\") + mark_safe(\"<b>bar</b>\"), SafeString)\n        self.assertNotIsInstance(mark_safe(\"<b>foo</b>\") + \"bar\", SafeString)\n        self.assertNotIsInstance(\"foo\" + mark_safe(\"<b>bar</b>\"), SafeString)\n", "    def test_safe_string_concatenation(self):\n        self.assertIsInstance(mark_safe(\"hello\") + mark_safe(\" world\"), SafeString)\n        self.assertIsInstance(mark_safe(\"hello\") + \" world\", str)\n        self.assertEqual(mark_safe(\"hello\") + \" world\", \"hello world\")\n        self.assertEqual(mark_safe(\"<\") + mark_safe(\">\"), \"><\")\n", "    def test_safe_string_addition(self):\n        self.assertEqual(mark_safe(\"hello\") + mark_safe(\" world\"), \"hello world\")\n        self.assertEqual(mark_safe(\"hello\") + \" world\", \"hello world\")\n        self.assertEqual(\"hello\" + mark_safe(\" world\"), \"hello world\")\n        self.assertEqual(mark_safe(\"hello\") + 123, \"hello123\")  # Not safe anymore\n\n"], "sample_82": ["    def test_value_from_datadict_with_none_values(self):\n        data = {'form-0-year': '', 'form-0-month': '', 'form-0-day': ''}\n        widget = SelectDateWidget()\n        self.assertIsNone(widget.value_from_datadict(data, {}, 'form-0'))\n", "    def test_value_from_datadict_empty_values(self):\n        \"\"\"Test that empty values are handled correctly.\"\"\"\n        data = {'form-0-year': '', 'form-0-month': '', 'form-0-day': ''}\n        widget = self.widget\n\n        self.assertIsNone(widget.value_from_datadict(data, {}, 'form-0'))\n", "    def test_empty_values(self):\n        # Make sure empty values are handled correctly.\n\n        data = {}\n\n        empty_form = DateForm(data)\n        self.assertFalse(empty_form.is_valid())\n        self.assertIsNone(empty_form.cleaned_data.get('date'))\n\n        data.update({'date_year': '2010', 'date_month': '', 'date_day': ''})\n        empty_form = DateForm(data)\n        self.assertFalse(empty_form.is_valid())\n        self.assertIsNone(empty_form.cleaned_data.get('date'))\n\n        data.update({'date_year': '', 'date_month': '10', 'date_day': '25'})\n        empty_form = DateForm(data)\n        self.assertFalse(empty_form.is_valid())\n        self.assertIsNone(empty_form.cleaned_data.get('date'))\n", "    def test_value_from_datadict_empty(self):\n        form_data = {}\n        widget = SelectDateWidget()\n        self.assertIsNone(widget.value_from_datadict(form_data, form_data, 'datefield'))\n\n", "    def test_empty_label(self):\n        form = Form(\n            {'date': ''},\n            initial={'date': date(2017, 1, 23)},\n        )\n        self.assertEqual(\n            form.as_p(),\n            '<p><label for=\"id_date_year\">Year:</label> <select name=\"date_year\" id=\"id_date_year\"><option value=\"\">--------</option><option value=\"2007\">2007</option><option value=\"2008\">2008</option><option value=\"2009\">2009</option><option value=\"2010\">2010</option><option value=\"2011\">2011</option><option value=\"2012\">2012</option><option value=\"2013\">2013</option><option value=\"2014\">2014</option><option value=\"2015\">2015</option><option value=\"2016\">2016</option></select> <label for=\"id_date_month\">Month:</label> <select name=\"date_month\" id=\"id_date_month\"><option value=\"\">--------</option><option value=\"1\">January</option><option value=\"2\">February</option><option value=\"3\">March</option><option value=\"4\">April</option><option value=\"5\">May</option><option value=\"6\">June</option><option value=\"7\">July</option><option value=\"8\">August</option><option value=\"9\">September</option><option value=\"10\">October</option><option value=\"11\">November</option><option value=\"12\">December</option></select> <label for=\"id_date_day\">Day:</label> <select name=\"date_day\" id=\"id_date_day\"><option value=\"\">--------</option><option value=\"1\">1</option><option value=\"2\">2</option><option value=\"3\">3</option><option value=\"4\">4</option><option value=\"5\">5</option><option value=\"6\">6</option><option value=\"7\">7</option><option value=\"8\">8"], "sample_375": ["    def test_get_related_models_recursive(self):\n        from .models import Author, Book, Publisher\n\n        state = ProjectState()\n        state.add_model(Author, {})\n        state.add_model(Book, {'author': models.ForeignKey('Author')})\n        state.add_model(Publisher, {})\n\n        related_models = get_related_models_recursive(Book)\n        self.assertEqual(sorted(related_models), [Author, Publisher])\n\n", "    def test_rendering_model_with_custom_base(self):\n        app_state = ProjectState()\n        app_state.add_model(\n            ModelState.from_model(ModelWithCustomBase),\n        )\n        # Note: We're not going to fully test the rendered model here, just\n        # that it renders successfully.\n        app_state.render()\n", "def test_ModelState_construct_managers(self):\n    class MyManager(models.Manager):\n        pass\n\n    class MyModel(models.Model):\n        objects = MyManager()\n\n    model_state = ModelState.from_model(MyModel)\n\n    managers = list(model_state.construct_managers())\n    self.assertEqual(len(managers), 1)\n    self.assertEqual(managers[0][0], 'objects')\n    self.assertIsInstance(managers[0][1], MyManager)\n", "    def test_model_state_managers(self):\n        state = ProjectState.from_apps(Apps())\n        model_state = state.models[((\"test_migrate\", \"UnicodeModel\"),)]\n        manager_names = [m[0] for m in model_state.managers]\n        self.assertEqual(manager_names, [\"objects\"])\n", "    def test_related_models_recursion(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        model_state = ModelState.from_model(C)\n        related_models = get_related_models_recursive(model_state)\n        self.assertEqual(\n            [m.name for m in related_models],\n            ['a', 'b'],\n        )\n"], "sample_100": ["    def test_zipfile_import(self):\n        with tempfile.NamedTemporaryFile(suffix=\".zip\") as f:\n            zipfile_name = f.name\n            with zipfile.ZipFile(zipfile_name, 'w') as zf:\n                zf.writestr('module.py', \"\"\"print('hello')\"\"\")\n            sys.path.append(zipfile_name)\n            self.import_and_cleanup('module')\n            self.assertFileFound(Path(zipfile_name) / 'module.py')\n", "    def test_zipfile_import(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('my_module/__init__.py', '')  \n                zipf.writestr('my_module/file.py', '')\n            sys.path_importer_cache.clear()\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path)\n", "    def test_zipfile_module(self):\n        with tempfile.NamedTemporaryFile(suffix='.zip') as zip_file:\n            test_file = self.temporary_file('test.py')\n            test_file.write_text('pass')\n            with zipfile.ZipFile(zip_file.name, 'w') as zipf:\n                zipf.write(str(test_file), arcname='test.py')\n            self.import_and_cleanup('test_zipfile')\n            self.assertFileFound(test_file.resolve())\n            self.assertEqual(len(list(autoreload.iter_all_python_module_files())), 1)\n\n", "    def test_watchman_glob(self):\n        with mock.patch('django.utils.autoreload.pywatchman') as pywatchman:\n            pywatchman.client.return_value.query.return_value = {\n                'watch': 'watch',\n                'relative_path': 'relative',\n            }\n            reloader = autoreload.WatchmanReloader()\n            reloader._watch_glob(self.temporary_file('foo'), ['*.py'])\n            pywatchman.client.return_value.subscribe.assert_called_once_with(\n                'relative',\n                'glob:relative',\n                ['anyof', ['match', 'relative/*\\.py', 'wholename']],\n            )\n\n", "    def test_zipfile_module_inside_zipfile(self):\n        zip_file = self.temporary_file('test_module.zip')\n        with zipfile.ZipFile(zip_file, 'w') as zf:\n            zf.writestr('myfile.py', 'print(\"Hello from zipfile!\")')\n        self.import_and_cleanup('test_module')\n        with extend_sys_path(str(zip_file.parent)):\n            self.assertFileFound(zip_file)\n"], "sample_1150": ["def test_complex_region_union_intersection():\n    a = Interval(1, 2)\n    b = Interval(3, 4)\n    c = Interval(2, 5)\n    d = Interval(0, pi)\n    e = Interval(pi/2, 3*pi/2)\n\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(c*d, polar=True)\n    c3 = ComplexRegion(a*d, polar=True)\n    c4 = ComplexRegion(c*e, polar=True)\n\n    assert (c1.union(c2)).sets == Union(ProductSet(a, b), ProductSet(c, d))\n    assert (c3.intersect(c4)).sets == ProductSet(a, Intersection(d, e))\n", "    def test_complex_region_polar_intersection():\n        r1 = Interval(0, 1)\n        theta1 = Interval(0, S.Pi/2)\n        r2 = Interval(0.5, 1)\n        theta2 = Interval(S.Pi/4, 3*S.Pi/4)\n        c1 = ComplexRegion(ProductSet(r1, theta1), polar=True)\n        c2 = ComplexRegion(ProductSet(r2, theta2), polar=True)\n        intersection = c1.intersect(c2)\n        assert isinstance(intersection, PolarComplexRegion)\n        assert intersection.sets == ProductSet(Interval(0.5, 1), Interval(S.Pi/4, S.Pi/2))\n", "    def test_polar_complex_region_intersection_empty():\n        a = Interval(0, 1)\n        b = Interval(0, 2*S.Pi)\n        c = Interval(1, 2*S.Pi)\n        c1 = ComplexRegion(a*b, polar=True)\n        c2 = ComplexRegion(a*c, polar=True)\n\n        assert c1.intersect(c2).is_empty\n        assert (c1 & c2).is_empty\n\n", "def test_polar_complex_region():\n    # Test polar complex region representation\n    r = Interval(0, 1)\n    theta = Interval(0, S.Pi/2)\n    region = ComplexRegion(r * theta, polar=True)\n\n    assert region.sets == ProductSet(r, theta)\n    assert region.polar is True\n    assert normalize_theta_set(Interval(0, 7*S.Pi)) == Interval(0, 2*S.Pi)\n    assert normalize_theta_set(Interval(\n        -S.Pi/2, S.Pi/2)) == Union(Interval(0, S.Pi/2), Interval(3*S.Pi/2, 2*S.Pi))\n\n    # Test ComplexRegion creation with normalized theta\n\n    assert ComplexRegion(r*theta, polar=True).sets == ProductSet(\n        r, Interval(0, S.Pi/2)\n    )\n\n    # Test if a point is in a PolarComplexRegion\n    assert (0.5 + 0.5*I) in ComplexRegion(r * theta, polar=True)\n    assert (1 + 2*I) not in ComplexRegion(r * theta, polar=True)\n\n", "def test_complexregion_contains_complex_number():\n    c1 = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    assert 0.5 + 0.5*I in c1\n    assert 1.5 + 1.5*I in c1\n    assert (1 + S.I) in c1\n    assert 1 + 2*I not in c1\n\n"], "sample_202": ["    def test_message_store_exceeding_max_size(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(level='error', message='This is a test message')] * 100\n        with self.assertNumQueries(0), self.assertRaisesMessage(\n                TypeError, 'CookieStorage does not support storing empty message lists' ):\n            storage._store(messages, self.response, remove_oldest=False)\n\n", "    def test_max_cookie_size_with_invalid_hash(self):\n        storage = self.get_storage()\n        messages = [Message('Hello', constants.INFO) for _ in range(5)]\n        set_cookie_data(storage, messages, invalid=True)\n        response = self.get_response()\n\n        # Max cookie size is hit, messages should be truncated and\n        # the not_finished sentinel should be present\n        self.assertEqual(self.stored_messages_count(storage, response), 0)\n", "    def test_cookie_size_limit(self):\n        storage = self.get_storage()\n        messages = [Message(constants.ERROR, 'message 1'),\n                    Message(constants.INFO, 'message 2', extra_tags='important')] * 100\n\n        # Store messages until the cookie size limit is reached\n        response = self.request_factory.get('/')\n        self.assertEqual(storage._store(messages, response), [])\n        self.assertEqual(stored_cookie_messages_count(storage, response), len(messages))\n\n        # Add one more message, triggering the cookie size limit\n        messages.append(Message(constants.SUCCESS, 'message 3'))\n        response = self.request_factory.get('/')\n        unstored_messages = storage._store(messages, response)\n        self.assertEqual(len(unstored_messages), 1)\n        self.assertEqual(unstored_messages[0].level, constants.SUCCESS)\n        self.assertEqual(stored_cookie_messages_count(storage, response), len(messages) - 1)\n\n        # Verify that the not_finished sentinel value is present\n        data = storage._decode(response.cookies[storage.cookie_name].value)\n        self.assertTrue(data[-1] == CookieStorage.not_finished)\n", "    def test_cookie_storage_max_size_remove_oldest(self):\n        storage = self.get_storage()\n\n        # Create messages which will exceed the max size of the cookie.\n        messages = [\n            Message(\n                'Too big message 1',\n                level=constants.ERROR,\n                extra_tags=['tag1', 'tag2', 'tag3'],\n            ),\n            Message('Too big message 2', level=constants.WARNING),\n            Message('Too big message 3', level=constants.SUCCESS, extra_tags=['tag4']),\n            Message('Too big message 4', level=constants.INFO),\n        ]\n        set_cookie_data(storage, messages)\n\n        with self.settings(SESSION_COOKIE_MAX_SIZE=1024):\n            response = self.get_response()\n            storage._store(messages, response, remove_oldest=True)\n\n        self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n", "    def test_max_cookie_size_limit(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(\n            'Some message 1', constants.WARNING),\n            Message('Some message 2', constants.ERROR),\n            Message('Some message 3', constants.DEBUG),\n            Message('Some message 4', constants.INFO),\n        ]\n        set_cookie_data(storage, messages)\n        response = self.client.get('/')\n        self.assertEqual(stored_cookie_messages_count(storage, response), 4)\n\n        for i in range(20, 300):\n            messages.append(Message(f'Some message {i}', constants.SUCCESS))\n        set_cookie_data(storage, messages)\n        response = self.client.get('/')\n        self.assertLess(stored_cookie_messages_count(storage, response), len(messages))\n\n"], "sample_220": ["    def test_set_cookie_expires_naive_datetime(self):\n        response = HttpResponse()\n        expires = datetime(2023, 10, 27, 12, 0, 0)\n        response.set_cookie('my_cookie', expires=expires)\n        self.assertEqual(response.cookies['my_cookie']['expires'], http_date(expires.replace(tzinfo=utc)))\n", "    def test_set_cookie_with_expires_datetime(self):\n        response = HttpResponse()\n        expires = datetime(2024, 1, 1, 12, 0, 0, tzinfo=utc)\n        response.set_cookie('test_cookie', 'value', expires=expires)\n        self.assertEqual(response.cookies['test_cookie'][\"expires\"], http_date(expires.timestamp()))\n", "    def test_set_cookie_with_expires_aware_datetime(self):\n        now = datetime.now(utc)\n        future = now + timedelta(days=1)\n        response = HttpResponse()\n        response.set_cookie('test', 'value', expires=future)\n        self.assertEqual(\n            response.cookies['test']['expires'],\n            http_date(future.timestamp())\n        )\n", "def test_set_cookie_with_timedelta_expires(self):\n        response = HttpResponse()\n        response.set_cookie('my_cookie', 'value', expires=timedelta(days=1))\n        self.assertEqual(response.cookies['my_cookie']['expires'], http_date(time.time() + 86400))\n", "    def test_set_cookie_with_expires(self):\n        response = HttpResponse()\n        response.set_cookie('test', 'value', expires=datetime.now() + timedelta(days=1))\n        self.assertEqual(response.cookies['test']['expires'],\n                         http_date(datetime.now(utc) + timedelta(days=1)))\n"], "sample_981": ["def test_josephus():\n    assert Permutation.josephus(3, 6, 2).array_form == [2, 5, 3, 1, 4, 0]\n    assert Permutation.josephus(2, 5, 1) == Permutation([1, 0, 3, 4, 2])\n    assert Permutation.josephus(5, 5, 5) == Permutation([0, 1, 2, 3, 4])\n\n", "def test_cycle_structure_empty():\n    assert Permutation([]).cycle_structure == {1: 0}\n", "def test_rmul_cycle():\n    p = Permutation([1, 2, 0])\n    c = Cycle(0, 1)\n    assert (p * c) == Permutation([2, 0, 1])\n    assert (c * p) == Permutation([0, 1, 2])\n", "def test_from_inversion_vector():\n    p = Permutation.from_inversion_vector([3, 2, 1, 0, 0])\n    assert p.array_form == [3, 2, 1, 0, 4, 5]\n    raises(ValueError, lambda: Permutation.from_inversion_vector([4, 2, 1, 0]))\n", "def test_cyclic_form():\n    p = Permutation([0, 2, 1, 3])\n    assert p.cyclic_form == [(0, 2, 1), (3,)]\n    p = Permutation([1, 0, 3, 2])\n    assert p.cyclic_form == [(1, 0), (3, 2)]\n    p = Permutation([1,3,2,0])\n    assert p.cyclic_form == [(1, 3, 2, 0)]\n    p = Permutation([0, 1, 2, 3])\n    assert p.cyclic_form == [(0, 1, 2, 3)]\n    p = Permutation([3, 2, 1, 0])\n    assert p.cyclic_form == [(3, 2, 1, 0)]\n"], "sample_240": ["    def test_check_token_legacy_format(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com', password='password')\n        # Create a token using the legacy format (without the '-' separator)\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2023, 1, 1))\n        token = generator._make_token_with_timestamp(user, generator._num_seconds(datetime.now()) , legacy=True)\n        self.assertTrue(generator.check_token(user, token))\n\n", "    def test_check_token_with_legacy_token(self):\n        user = User.objects.create_user(username='testuser', email='user@example.com', password='password')\n        # Create a token using the old (pre-Django 3.1) algorithm.\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 10, 26))\n        token = generator.make_token(user)\n\n        # Simulate a time difference that would have been common before Django 3.1.\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2022, 10, 27))\n\n        self.assertTrue(generator.check_token(user, token))\n", "    def test_legacy_token_check(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com')\n        generator = MockedPasswordResetTokenGenerator(now=datetime(2023, 10, 26, 12, 0, 0))\n        token = generator.make_token(user)\n        # Simulate a legacy token with timestamp in days\n        ts_b36, _ = token.split('-')\n        legacy_token = f\"{ts_b36 * 24 * 60 * 60}-{_}\"\n        self.assertTrue(generator.check_token(user, legacy_token))\n", "    def test_make_token_with_timestamp_legacy(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        token_generator = MockedPasswordResetTokenGenerator(datetime(2023, 1, 1))\n        token = token_generator._make_token_with_timestamp(user, 1672496000, legacy=True)  # Legacy timestamp\n        self.assertEqual(token, '0-b2941f4350f3f097')\n", "    def test_check_token_legacy_format(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com', password='password')\n        generator = MockedPasswordResetTokenGenerator(datetime(2023, 1, 1, 12, 0, 0))\n        token = generator.make_token(user)\n        # Mimic legacy format (timestamp in seconds since epoch)\n        ts_b36, _ = token.split('-')\n        ts = int(ts_b36) * 24 * 60 * 60\n        ts += int((datetime.now() - datetime.combine(datetime.now().date(), time.min)).total_seconds())\n        token = f\"{int_to_base36(ts)}-{_}\"\n        self.assertTrue(generator.check_token(user, token))\n"], "sample_1185": ["def test_decompogen_abs():\n    assert decompogen(Abs(x), x) == [Abs(x)]\n", "def test_decompogen_min():\n    assert decompogen(Min(x, 2*x, x**2), x) == [Min(x, 2*x, x**2)]\n    assert decompogen(Min(x, sin(x), sqrt(x)), x) == [Min(x, sin(x), sqrt(x))]\n", "def test_decompogen_abs():\n    assert decompogen(Abs(x**2), x) == [Abs(x), x**2]\n", "    def test_decompogen_max():\n        assert decompogen(Max(x, sin(x)), x) == [Max(x, sin(x))]\n", "    def test_decompogen_abs():\n        assert decompogen(Abs(x + 1), x) == [Abs(x), x + 1]\n"], "sample_567": ["    def test_annotation_textcoords(self):\n        fig, ax = plt.subplots()\n        anno = Annotation(\"foo\", (0.5, 0.5), xycoords=\"data\",\n                         textcoords=\"offset points\", xytext=(10, 20))\n        ax.add_artist(anno)\n        fig.canvas.draw()\n        ax.clear()\n        anno.set_textcoords(\"axes fraction\")\n        anno.xytext = (0.6, 0.8)\n        ax.add_artist(anno)\n        fig.canvas.draw()\n\n", "    def test_annotation_clip(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n\n        # Annotation inside axes\n        ann = ax.annotate('inside', (0, 0), (0.5, 0.5))\n        assert ann.get_visible()\n\n        # Annotation outside axes, clip=True\n        ann = ax.annotate('outside_clip', (-2, -2), (0.5, 0.5),\n                          annotation_clip=True)\n        assert not ann.get_visible()\n\n        # Annotation outside axes, clip=False\n        ann = ax.annotate('outside_noclip', (-2, -2), (0.5, 0.5),\n                          annotation_clip=False)\n        assert ann.get_visible()\n\n        # Annotation outside axes, clip=None (default)\n        ann = ax.annotate('outside_default', (-2, -2), (0.5, 0.5))\n        assert not ann.get_visible()\n", "    def test_annotation_clip(self):\n        fig, ax = plt.subplots()\n        ann = Annotation(\"Test\", xy=(2, 2), xycoords='data', xytext=(3, 3),\n                        textcoords='offset points',\n                        arrowprops=dict(arrowstyle=\"-\"))\n        ax.add_artist(ann)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n        fig.canvas.draw()\n\n        # Check if annotation is clipped\n        assert ann.get_visible() is False\n        ann.set_annotation_clip(False)\n        ax.figure.canvas.draw()\n        assert ann.get_visible() is True\n", "    def test_annotation_arrowprops(self):\n        fig, ax = plt.subplots()\n        ann = ax.annotate(\"Test\", xy=(0.5, 0.5), xytext=(0.7, 0.8),\n                         arrowprops=dict(arrowstyle=\"->\"))\n        fig.canvas.draw()\n        assert len(ax.patches) == 1\n        assert isinstance(ax.patches[0], mpatches.FancyArrowPatch)\n", "def test_annotation_textcoords_offset_points():\n    fig, ax = plt.subplots()\n    xy = (0.5, 0.5)\n    xytext = (1, 1)\n    annotation = Annotation('Test', xy=xy, xytext=xytext,\n                           textcoords='offset points',\n                           arrowprops=dict(arrowstyle='->'))\n\n    ax.add_artist(annotation)\n    assert_almost_equal(annotation.xyann, (1, 1))\n"], "sample_360": ["    def test_fetch_from_cache_middleware_head_request(self):\n        with self.settings(CACHE_MIDDLEWARE_SECONDS=300):\n            request = HttpRequest()\n            request.method = 'HEAD'\n            request.path = '/'\n            response = HttpResponse('Hello World')\n            response['Cache-Control'] = 'max-age=300'\n\n            middleware = FetchFromCacheMiddleware(lambda r: response)\n            cached_response = middleware.process_request(request)\n\n            self.assertEqual(cached_response, None)\n\n            settings.CACHE_MIDDLEWARE_SECONDS = 300\n            middleware._cache = caches['default']\n\n            middleware._cache.set(get_cache_key(request, '', 'HEAD'), response, 300)\n\n            cached_response = middleware.process_request(request)\n\n            self.assertEqual(cached_response, response)\n\n", "compilation error", "    def test_get_cache_key_with_csrf(self):\n        with translation.override('en'):\n            request = HttpRequest()\n            request.META['CSRF_COOKIE'] = 'abc'\n            request.COOKIES['csrftoken'] = 'abc'\n            request.method = 'GET'\n            request.path = '/'\n            key = get_cache_key(request, 'some-prefix')\n            self.assertIn('csrftoken=abc', key)\n\n\n\n", "    def test_cache_middleware_page_timeout_overrides_cache_timeout(self):\n        settings.CACHE_MIDDLEWARE_SECONDS = 300\n\n        with override_settings(CACHE_MIDDLEWARE_KEY_PREFIX='test_prefix'):\n            response = self._get_cached_response()\n            self.assertEqual(response['Cache-Control'], 'max-age=10')\n\n            # Make sure the cache key is set correctly\n            self.assertIn('test_prefix', response.get('Vary', ''))\n\n        self.assertEqual(caches['default'].get(get_cache_key(self.request, 'test_prefix', 'GET')), response)\n\n", "    def test_cache_middleware_cache_timeout(self):\n        with self.settings(CACHE_MIDDLEWARE_SECONDS=300):\n            response = self.client.get('/')\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('Cache-Control', response)\n            self.assertIn('max-age=300', response['Cache-Control'])\n"], "sample_1011": ["    def test_octave_indexed_print_nocontract():\n        i = Idx('i', 5)\n        A = MatrixSymbol('A', 5, 5)\n        e = A[i, i]\n        code = octave_code(e, contract=False)\n        assert code == 'A(i,i)'\n\n\n", "def test_octave_code_piecewise_nodefault():\n    pw = Piecewise((x + 1, x > 0))  \n    with raises(ValueError):\n        octave_code(pw) # should raise error for missing default term\n\n", "    def test_octave_Piecewise_no_default_arg():\n        pw = Piecewise((x + 1, x > 0), (x - 1, x < 0))\n        with raises(ValueError):\n            octave_code(pw)\n\n", "    def test_expint_different_mu():\n        mu = Symbol('mu', real=True)\n        expr = expint(mu, x)\n        raises(NotImplementedError, lambda: octave_code(expr))\n\n", "def test_airy():\n    assert octave_code(airyai(x)) == \"airy(0, x)\"\n    assert octave_code(airybi(x)) == \"airy(2, x)\"\n    assert octave_code(airyaiprime(x)) == \"airy(1, x)\"\n    assert octave_code(airybiprime(x)) == \"airy(3, x)\"\n\n"], "sample_619": ["    def test_decode_cf_datetime_calendar(self):\n        # Test decoding with non-standard calendars\n        for calendar in _NON_STANDARD_CALENDARS:\n            data = np.arange(10)\n            units = f\"days since 2000-01-01\"\n            variable = Variable(\n                (\"time\",), data, {\"units\": units, \"calendar\": calendar}\n            )\n            decoded = decode_cf(variable)\n            assert isinstance(decoded.data, np.ndarray)\n            assert decoded.data.dtype == np.dtype(\"datetime64[ns]\")\n\n", "def test_cftime_roundtrip(calendar):\n    dates = cftime_range(\n        start=\"2000-01-01\", periods=10, freq=\"D\", calendar=calendar\n    )\n    var = Variable([\"time\"], dates)\n    ds = Dataset({\"var\": var})\n\n    # Encoding should be compatible with both cftime and datetime64\n    encoded = cf_encoder(ds).encode()\n    ds_decoded = decode_cf(encoded)\n    assert_identical(ds_decoded, ds)\n", "compilation error", "    def test_cftime_to_nptime(self):\n        # Test conversion from cftime.datetime to numpy.datetime64\n        import cftime\n\n        dates = cftime_range(\n            start=\"2000-01-01\", periods=5, freq=\"D\", calendar=\"proleptic_gregorian\"\n        )\n        np_dates = cftime_to_nptime(dates)\n\n        expected_dates = np.array(\n            [\"2000-01-01\", \"2000-01-02\", \"2000-01-03\", \"2000-01-04\", \"2000-01-05\"],\n            dtype=\"datetime64[ns]\",\n        )\n        assert_array_equal(np_dates, expected_dates)\n\n", "    def test_decode_cf_datetime_with_out_of_range_dates_with_non_standard_calendar(self):\n        # Test decoding dates outside the range of standard (Gregorian) calendar\n        # with a non-standard calendar\n\n        dates = np.array([1e6] * 10, dtype=np.float64)\n        units = \"days since 0001-01-01\"\n        calendar = \"360_day\"\n        data_array = Variable(\n            (\"t\",), dates, {\"units\": units, \"calendar\": calendar}\n        )\n        dataset = Dataset({\"test\": data_array})\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                category=SerializationWarning,\n                message=\"Unable to decode time axis into full numpy.datetime64 objects, continuing using cftime.datetime objects instead, reason: dates out of range\",\n            )\n            decoded = decode_cf(dataset, decode_times=True)\n\n        assert \"cftime.datetime\" in str(type(decoded[\"test\"][0].values[0]))\n\n\n"], "sample_458": ["    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": Decimal(\"1.23456789\"), \"b\": 1.23456789})\n            self.assertEqual(output, \"1.23456789 1.23456789\")\n\n", "    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": Decimal(\"10.23456789\"), \"b\": 1.234})\n            self.assertEqual(output, \"10.23456789 1.234\")\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):\n            output = self.engine.render_to_string(\n                \"floatformat01\",\n                {\"a\": Decimal(\"12.345\"), \"b\": 12.345},\n            )\n            self.assertEqual(\n                output,\n                \"12.345 12.345\"\n            )\n\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": Decimal(\"12.345\"), \"b\": 12345.6789})\n            self.assertEqual(output, \"12.345 12345.7\")\n", "    def test_floatformat_with_decimal(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            ctx.rounding = 'ROUND_HALF_UP'\n            self.assertEqual(\n                self.engine.render(\"floatformat01\", {\"a\": Decimal('123.4567'), \"b\": Decimal('8.9')}),\n                '123.46 8.90'\n            )\n"], "sample_632": ["    def test_similar_ignore_imports(self):\n        linter = PyLinter(reporter=Reporter())\n        linter.check([HIDE_CODE_WITH_IMPORTS])\n        output = linter.reporter.out\n        assert \"R0801\" not in output\n", "    def test_similar_check_ignore_imports_with_hidden_code(self):\n        linter = PyLinter()\n        reporter = Reporter()\n        linter.set_reporter(reporter)\n        linter.check([HIDE_CODE_WITH_IMPORTS])\n        self.assertEqual(\n            [\n                \"R0801: Similar lines in 1 files\\n==hide_code_with_imports.py:1\\n   import os\\n   import sys\\n   import time\\n\",\n            ],\n            reporter.messages,\n        )\n\n", "    def test_similar_ignore_imports_and_signatures(self):\n        linter = PyLinter()\n        with redirect_stdout(StringIO()) as out:\n            reporter = Reporter(linter)\n            linter.reporter = reporter\n            linter.set_reporter(reporter)\n            linter.add_checker(similar.SimilarChecker(linter))\n            linter.configure({'ignore-imports': True, 'ignore-signatures': True})\n            linter.check([HIDE_CODE_WITH_IMPORTS])\n        output = out.getvalue()\n        assert \"R0801\" not in output, (\n            \"Should not report similarity when ignoring imports and signatures.\"\n        )\n", "    def test_issue_4181(self):\n        # See https://github.com/PyCQA/pylint/issues/4181\n        with redirect_stdout(StringIO()) as output:\n            linter = PyLinter(reporter=Reporter())\n            linter.load_config(None, ['--min-similarity-lines=2'])\n            linter.check([SIMILAR4, SIMILAR5])\n            output_str = output.getvalue()\n            assert \"R0801\" in output_str\n", "    def test_similar_with_ignore_imports(\n        self, reporter: Reporter"], "sample_99": ["    def test_trunc_datetime_year(self):\n        now = timezone.now()\n        dt = now.replace(year=2023, month=10, day=26, hour=10, minute=30, second=15)\n        model = self.create_model(dt, None)\n        self.assertEqual(model.start_datetime.year, 2023)\n\n        with self.assertNumQueries(1):\n            result = DTModel.objects.annotate(truncated_year=TruncYear('start_datetime')).get(pk=model.pk)\n        self.assertEqual(result.truncated_year, 2023)\n\n", "    def test_extract_week_day(self):\n        dt = timezone.make_aware(datetime(2023, 3, 15, 12, 0, 0))\n        self.assertEqual(DTModel.objects.create(start_datetime=dt).start_datetime.weekday(), dt.weekday())\n        self.assertEqual(DTModel.objects.create(start_datetime=dt).week_day, dt.weekday() + 1)\n        self.assertEqual(\n            List(DTModel.objects.values('start_datetime').annotate(day_of_week=ExtractWeekDay('start_datetime'))).get(day_of_week=3)['day_of_week'],\n            3\n        )\n", "    def test_extract_iso_year(self):\n        dt1 = datetime(2023, 12, 31)\n        dt2 = datetime(2024, 1, 1)\n        obj1 = self.create_model(dt1, dt2)\n        obj2 = self.create_model(dt2, None)\n\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=2023).count(), 1)\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=2024).count(), 1)\n        self.assertEqual(DTModel.objects.filter(end_datetime__iso_year=2024).count(), 1)\n\n", "    def test_extract_iso_year(self):\n        dt = datetime(2023, 12, 31, tzinfo=timezone.utc)\n        obj = self.create_model(dt, dt)\n        with self.assertNumQueries(1):\n            self.assertEqual(obj.start_datetime.extract(ExtractIsoYear()), 2024)\n", "    def test_extract_iso_year_from_datetime(self):\n        dt = datetime(2023, 12, 25, tzinfo=timezone.utc)\n        obj = self.create_model(dt, dt)\n        self.assertEqual(DTModel.objects.get(pk=obj.pk).start_datetime.year, dt.year)\n        self.assertEqual(DTModel.objects.get(pk=obj.pk).start_datetime.isocalendar().year, dt.isocalendar().year)\n        self.assertEqual(DTModel.objects.filter(start_datetime__iso_year=dt.isocalendar().year).exists(), True)\n\n"], "sample_1072": ["def test_frac_imaginary():\n    assert frac(I) == I\n    assert frac(2*I) == I\n    assert frac(I/2) == I/2\n    assert frac(-I/2) == -I/2\n    assert frac(I + 1) == I\n\n", "def test_frac_rewrite_as_floor():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x + I*y).rewrite(floor) == x + I*y - floor(x + I*y)\n", "def test_frac_rewrite_floor():\n    assert frac(x).rewrite(floor) == x - floor(x)\n", "def test_frac_rewrite_as_floor():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(-x).rewrite(floor) == -x - floor(-x)\n    assert frac(I*x).rewrite(floor) == I*x - floor(I*x)\n\n", "def test_frac_rewrite_as_floor():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(y).rewrite(floor) == y - floor(y)\n    assert frac(i).rewrite(floor) == i - floor(i)\n    assert frac(k).rewrite(floor) == 0\n"], "sample_1128": ["    def test_point_set_acc_2():\n        N = ReferenceFrame('N')\n        p1 = Point('p1')\n        p2 = Point('p2')\n        p1.set_pos(p2, 10*N.x)\n        p1.set_acc(N, 10*N.x)\n        assert p1.acc(N) == 10*N.x\n", "    def test_locatenew(self):\n        N = ReferenceFrame('N')\n        p1 = Point('p1')\n        p2 = p1.locatenew('p2', 10 * N.x)\n        assert p2.pos_from(p1) == 10*N.x\n        assert p1.pos_from(p2) == -10*N.x\n", "def test_locatenew():\n    N = ReferenceFrame('N')\n    p1 = Point('p1')\n    p2 = p1.locatenew('p2', 10 * N.x)\n    assert p2.pos_from(p1) == 10 * N.x\n    assert p1.pos_from(p2) == -10 * N.x\n", "def test_point_set_vel_and_get_vel():\n    N = ReferenceFrame('N')\n    p = Point('p')\n    p.set_vel(N, 10*N.x)\n    assert p.vel(N) == 10*N.x\n", "    def test_point_set_vel_and_vel_dependency(self):\n        N = ReferenceFrame('N')\n        O = Point('O')\n        P = Point('P')\n        q = dynamicsymbols('q')\n        qd = dynamicsymbols('q', 1)\n        P.set_pos(O, q * N.x)\n\n        O.set_vel(N, 2*N.x)\n        P.set_vel(N, 5*N.y)\n\n        assert P.vel(N) == 5*N.y\n\n        O.set_vel(N, qd * N.x)\n        assert P.vel(N) == qd*N.x + 5*N.y\n"], "sample_1138": ["    def test_trig_split_two_terms():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert trig_split(cos(x), cos(y)) == (1, 1, 1, x, y, True)\n        assert trig_split(cos(x), -sqrt(3)*sin(x), two=True) == (2, 1, -1, x, pi/6, False)\n\n", "def test_trig_split_two_tan():\n    assert trig_split(tan(x), tan(y), two=True) == (\n        sqrt(2), 1, 1, x, y, False)\n\n\n\n", "    def test_sincos_to_sum():\n        x = symbols('x')\n        assert sincos_to_sum(sin(x)*cos(x)) == S.Half*sin(2*x)\n        assert sincos_to_sum(sin(x)*cos(2*x)) == S.Half*(sin(3*x) - sin(x))\n        assert sincos_to_sum(sin(x)**2*cos(x)**2) == S.Quarter*sin(2*x)**2\n", "def test_TR111():\n    x = Symbol('x')\n    assert TR111(cos(x)**4) == cos(2*x)**2/2 + cos(4*x)/2\n    assert TR111(sin(x)**4) == sin(2*x)**2/2 - sin(4*x)/2\n", "    def test_TR111():\n        x = Symbol('x')\n        assert TR111(sin(x)**2 + cos(x)**2) == 1\n        assert TR111(cos(x)**2 + sin(x)**2 + tan(x)**2) == 1 + tan(x)**2\n        assert TR111(sin(x)**2 + cos(x)**2 * tan(x)**2) == sin(x)**2 + (sin(x)/cos(x))**2 * cos(x)**2 == sin(x)**2 + sin(x)**2 == 2 * sin(x)**2\n"], "sample_1140": ["def test_pretty_MatrixSlice():\n    A = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert pretty(A[1:, :2]) == '\u23a14  5\u23a4\\n\u23a37  8\u23a6'\n", "def test_pretty_conjugate_complex_function():\n    assert pretty(conjugate(f(x + 1j))) == 'conjugate(f(x + 1\u22c5I))'\n", "    def test_groebner_basis_with_gens_and_domain(self):\n        R. = QQ['x','y']\n        p1 = x**2 + y**2 - 1\n        p2 = x*y - 1\n        ideal = R.ideal(p1, p2)\n        groebner_basis = ideal.groebner_basis()\n        self.assertEqual(pretty(groebner_basis), pretty(groebner(ideal)))\n", "def test_pretty_piecewise():\n    p = Piecewise((x, x < 1), (x**2, True))\n    assert pretty(p) == '   / x \\n  |---|\\n   \\ x^2 / x >= 1'\n", "def test_pretty_seqper():\n    assert pretty(SeqPer(x, (x, 1, 5))) == 'x_{1}^{5}'\n    assert pretty(SeqPer(x**2, (x, 1, 5))) == 'x^{2}_{1}^{5}'\n    assert pretty(SeqPer(x**(y + 1), (x, 1, 5))) == 'x^{y + 1}_{1}^{5}'\n"], "sample_611": ["    def test_date_range_like(self, calendar, use_cftime, expected_type):\n        source = pd.date_range(\"2000-01-01\", periods=10, freq=\"D\")\n        result = date_range_like(\n            source, calendar=calendar, use_cftime=use_cftime\n        )\n        assert isinstance(result, expected_type)\n", "    def test_offset_apply(self, offset, expected_n):\n        dt = cftime.DatetimeNoLeap(1900, 1, 1)\n        result = offset.apply(dt)\n        assert (result - dt).total_seconds() == expected_n * 86400\n", "    def test_offset_rollforward(self, offset):\n        start = cftime.datetime(1, 1, 1, calendar=self.calendar)\n        expected = offset.apply(start)\n        assert offset.rollforward(start) == expected\n", "    def test_offset_apply(self, offset, expected_n):\n        date = cftime.DatetimeNoLeap(2000, 1, 1)\n        expected_date = date + offset.as_timedelta() * expected_n\n        assert offset.apply(date) == expected_date\n", "    def test_offset_application(self, offset, expected_n):\n        date = cftime.DatetimeNoLeap(2000, 1, 1)\n        expected_date = offset.apply(date)\n        assert (expected_date - date).total_seconds() == expected_n * offset.total_seconds()\n\n"], "sample_135": ["    def test_format_datetime_aware(self):\n        d = datetime(2023, 10, 26, 12, 0, 0, tzinfo=utc)\n        formatted = format(d, 'r')\n        self.assertEqual(formatted, 'Thu, 26 Oct 2023 12:00:00 +0000')\n\n        d = make_aware(datetime(2023, 10, 26, 12, 0, 0), get_default_timezone())\n        formatted = format(d, 'r')\n        self.assertEqual(formatted, 'Thu, 26 Oct 2023 12:00:00 +0200')\n", "    def test_z(self):\n        d = date(2023, 10, 26)\n        self.assertEqual(format(d, 'z'), '299')\n\n", "    def test_format_time_with_timezone(self):\n        d = datetime(2008, 3, 12, 10, 30, 0, tzinfo=get_fixed_timezone(180))\n        formatted = format(d, 'P e')\n        self.assertEqual(formatted, '10:30 AM +0300')\n        formatted = format(d, 'c')\n        self.assertEqual(formatted, '2008-03-12T10:30:00+03:00')\n", "    def test_dateformat_timezone(self):\n        now = datetime.now(utc)\n        aware_dt = make_aware(now, timezone=get_default_timezone())\n        formatted = format(aware_dt, 'jS F Y H:i O')\n        self.assertIn(get_default_timezone(). zone, formatted)\n", "    def test_format_timezone(self):\n        d = datetime(2008, 1, 2, 10, 30, 0, tzinfo=utc)\n        expected = '2008-01-02T10:30:00Z'\n        self.assertEqual(format(d, 'c'), expected)\n\n        d = make_aware(datetime(2008, 1, 2, 10, 30, 0), get_default_timezone())\n        expected = '2008-01-02T10:30:00+01:00'\n        self.assertEqual(format(d, 'c'), expected)\n\n"], "sample_44": ["    def test_lu_instantiation_with_pu(self, lu_class, physical_unit):\n        lu = lu_class(physical_unit=physical_unit)\n        assert lu.physical_unit == physical_unit\n        assert lu.function_unit == lu_class\n\n", "    def test_function_unit_creation(self, lu_unit, pu_unit):\n        func_unit = lu_unit(pu_unit)\n        assert func_unit.function_unit == lu_unit\n        assert func_unit.physical_unit == pu_unit\n", "    def test_unit_arithmetics(self):\n        for unit_class in lu_subclasses:\n            for pu in pu_sample:\n                # Test quantity creation\n                lq = unit_class(10, pu)\n                assert isinstance(lq, lq_subclasses[lu_subclasses.index(unit_class)])\n                assert_quantity_allclose(lq.physical, 10 * pu)\n                assert lq.unit == unit_class(pu)\n\n                # Test unit conversion\n                lq2 = lq.to(unit_class(u.erg/u.s))\n\n                lq2_physical = lq.to(u.erg/u.s)\n\n                assert_quantity_allclose(lq2, lq2_physical)\n                assert lq2.unit == unit_class(u.erg/u.s)\n", "    def test_function_unit_from_physical(self):\n        for lu_unit, lq_subclass in zip(lu_units, lq_subclasses):\n            for pu in pu_sample:\n                # test creation from physical unit\n                luq = self.lq_subclasses[lu_units.index(lu_unit)](1, unit=pu)\n                assert luq.unit.physical_unit == pu\n                assert luq.unit.function_unit == lu_unit\n                assert luq.value == 1\n", "    def test_log_unit_equivalencies(self, lu_unit):\n        \"\"\"Test unit equivalencies.\"\"\"\n        # Check round-trip equivalency\n        assert_allclose(10**lu_unit.to(u.dimensionless_unscaled),\n                        u.dimensionless_unscaled.to(lu_unit))\n        \n        # Check conversion to physical unit\n        if lu_unit.physical_unit is not u.dimensionless_unscaled:\n            assert_allclose(lu_unit.to(lu_unit.physical_unit)*10**lu_unit.to(u.dimensionless_unscaled),\n                            lu_unit.physical_unit.to(lu_unit.physical_unit))\n\n"], "sample_249": ["    def test_test_db_signature(self):\n        conn = get_connection_copy()\n        creation = BaseDatabaseCreation(conn)\n        original_signature = creation.test_db_signature()\n        # Modify a setting that should be part of the signature\n        conn.settings_dict['NAME'] = 'test_modified'\n        modified_signature = creation.test_db_signature()\n        self.assertNotEqual(original_signature, modified_signature)\n", "    def test_destroy_test_db_keepdb(self, mock_destroy_test_db: mock.MagicMock):\n        db_creation = BaseDatabaseCreation(get_connection_copy())\n        db_creation.destroy_test_db(keepdb=True)\n        mock_destroy_test_db.assert_not_called()\n", "    def test_test_db_signature(self):\n        connection = get_connection_copy()\n        creation = BaseDatabaseCreation(connection)\n        original_signature = creation.test_db_signature()\n\n        # Modify settings that should change the signature\n        connection.settings_dict['NAME'] = 'different_name'\n        creation = BaseDatabaseCreation(connection)\n        self.assertNotEqual(original_signature, creation.test_db_signature())\n\n        # Restore original settings\n        connection.settings_dict['NAME'] = 'test_db'\n        creation = BaseDatabaseCreation(connection)\n        self.assertEqual(original_signature, creation.test_db_signature())\n", "    def test_test_db_signature(self):\n        connection = get_connection_copy()\n        db_creation = BaseDatabaseCreation(connection)\n        original_signature = db_creation.test_db_signature()\n        # Modify a setting that should affect signature\n        connection.settings_dict['NAME'] = 'test_modified_name'\n        modified_signature = db_creation.test_db_signature()\n        self.assertNotEqual(original_signature, modified_signature)\n", "    def test_test_db_signature_uniqueness(self):\n        # Ensure test_db_signature includes elements relevant to database identification.\n        conn1 = get_connection_copy()\n        conn2 = get_connection_copy()\n\n        conn1.settings_dict[\"NAME\"] = \"db1\"\n        conn2.settings_dict[\"NAME\"] = \"db2\"\n\n        self.assertNotEqual(BaseDatabaseCreation(conn1).test_db_signature(),\n                           BaseDatabaseCreation(conn2).test_db_signature())\n"], "sample_852": ["    def test_make_checkerboard_shuffle(random_state=42):\n        X, rows, cols = make_checkerboard((50, 60), (2, 3), random_state=random_state)\n        # Check if rows and columns are shuffled correctly\n        assert_array_equal(rows[:, np.argsort(rows.sum(axis=1))],\n                           np.vstack([np.repeat(c, 25) for c in range(2)]).T)\n        assert_array_equal(cols[:, np.argsort(cols.sum(axis=1))],\n                           np.vstack([np.repeat(c, 20) for c in range(3)]).T)\n\n\n\n", "    def test_make_checkerboard_shuffle(self):\n        random_state = 0\n        n_samples = 10\n        n_features = 12\n        n_clusters = 2\n        X, rows, cols = make_checkerboard((n_samples, n_features),\n                                       n_clusters,\n                                       random_state=random_state)\n        assert_array_equal(np.argsort(X[:, 0]), np.argsort(X[0]))\n        X, rows, cols = make_checkerboard((n_samples, n_features),\n                                       n_clusters,\n                                       shuffle=False,\n                                       random_state=random_state)\n        assert_array_equal(np.argsort(X[:, 0]), np.argsort(X[0]))\n", "    def test_make_checkerboard_shape():", "    def test_make_checkerboard_shape():\n        X, rows, cols = make_checkerboard((10, 10), 2)\n        assert X.shape == (10, 10)\n        assert rows.shape == (2, 10)\n        assert cols.shape == (2, 10)\n", "    def test_make_checkerboard_error():\n        with pytest.raises(ValueError):\n            make_checkerboard((10, 10), (2, 0))\n        with pytest.raises(ValueError):\n            make_checkerboard((10, 10), (0, 2))\n\n"], "sample_1083": ["def test_acsch_eval():\n    assert acsch(S.Zero) == S.Infinity\n\n    assert acsch(S.ImaginaryUnit).evalf() == -S.ImaginaryUnit * pi / 2\n\n", "    def test_acsch_evalf():\n        x = Symbol('x')\n        assert abs(acsch(1).evalf() - log(1 + sqrt(2))) < 1e-10\n        assert abs(acsch(2*I).evalf() - I*pi/6) < 1e-10\n        assert acsch(oo).evalf() == 0\n\n\n", "    def test_asech_complex():\n        z = S.ImaginaryUnit*(sqrt(6) - sqrt(2))\n        assert asech(z) == -5*I*pi/12\n        assert asech(-z) == 5*I*pi/12\n\n", "    def test_asech_eval(self):\n        x = Symbol('x')\n        n = Symbol('n')\n        assert asech(1).evalf() == 0\n        assert asech(2).evalf() == 0.481211833615182\n        assert asech(-sqrt(2)).evalf() == 2.356194490192345\n        assert asech((sqrt(6) - sqrt(2))).evalf() == 0.2617993877991494\n        assert asech(S(2)).evalf() == 0.481211833615182\n        assert asech(x).diff(x) == -1/(x*sqrt(1 - x**2))\n        assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n        assert asech(x).series(x, 0, 3) == asech(0) + x/2 - x**2/8 - 3*x**3/16 + O(x**4)\n\n", "    def test_asech_rewrite_as_log():\n        x = Symbol('x')\n        assert asech(x).rewrite(log) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n"], "sample_244": ["    def test_formset_factory_can_delete_extra(self):\n        formset = formset_factory(FavoriteDrinkForm, extra=2, can_delete=True, can_delete_extra=False)\n        formset_instance = formset({\n            'form-0-name': 'Coke',\n            'form-1-name': 'Pepsi',\n            'form-2-DELETE': True,\n        })\n\n        self.assertTrue(formset_instance.is_valid())\n        self.assertEqual(len(formset_instance.cleaned_data), 2)\n        self.assertEqual(formset_instance.cleaned_data[0]['name'], 'Coke')\n        self.assertEqual(formset_instance.cleaned_data[1]['name'], 'Pepsi')\n\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(Choice, absolute_max=3)\n        formset = formset_class(data={'choices-0-choice': 'A', 'choices-1-choice': 'B', 'choices-2-choice': 'C',\n                                     'choices-TOTAL_FORMS': 4, 'choices-INITIAL_FORMS': 0,\n                                     'choices-MAX_NUM_FORMS': 10, 'choices-MIN_NUM_FORMS': 0})\n        self.assertEqual(formset.errors, [\n            {'choice': ['Ensure this field has no more than 3 items.']}\n        ])\n        self.assertFalse(formset.is_valid())\n\n", "    def test_formset_factory_validate_min(self):\n        formset = self.make_choiceformset(total_forms=1, initial_forms=1, min_num=2)\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(len(formset.errors), 0)\n        self.assertEqual(\n            formset.non_form_errors()[0].message,\n            \"Please submit at least 2 forms.\"\n        )\n\n", "    def test_formset_extra_forms(self):\n        formset = self.make_choiceformset(\n            formset_data=[('Choice 1', 1), ('Choice 2', 2)],\n            extra=2,\n        )\n        self.assertEqual(len(formset.forms), 4)\n        self.assertEqual(len(formset.extra_forms), 2)\n        self.assertEqual(len(formset.initial_forms), 2)\n", "    def test_formset_factory_absolute_max(self):\n        # Test the absolute_max parameter in formset_factory\n        class MyForm(Form):\n            field = CharField()\n\n        max_num = 2\n        absolute_max = 5\n        MyFormSet = formset_factory(MyForm, absolute_max=absolute_max, max_num=max_num)\n\n        # Check that the formset's absolute_max is correctly set\n        self.assertEqual(MyFormSet.absolute_max, absolute_max)\n\n        formset = MyFormSet(data={\n            'form-0-field': 'value1',\n            'form-1-field': 'value2',\n            'form-2-field': 'value3',\n            'form-3-field': 'value4',\n            'form-4-field': 'value5',\n            'TOTAL_FORMS': '5',\n            'INITIAL_FORMS': '0',\n        })\n\n        # Check that the formset accepts up to absolute_max forms\n        self.assertEqual(formset.total_form_count(), absolute_max)\n\n        # Attempting to exceed absolute_max should raise a ValidationError\n        formset = MyFormSet(data={\n            'form-0-field': 'value1',\n            'form-1-field': 'value2',\n            'form-2-field': 'value3',\n            'form-3-field': 'value4',\n            'form-4-field': 'value5',\n            'form-5-field': 'value6',  # Beyond absolute_max\n            'TOTAL_FORMS': '6',\n            'INITIAL_FORMS': '0',\n        })\n\n        with self.assertRaises(ValidationError):\n            formset.full_clean()\n\n"], "sample_184": ["    def test_index_together_invalid_field(self):\n        class InvalidModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                index_together = (('field1', 'nonexistent_field'),)\n\n        errors = InvalidModel.check(databases=[])\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].msg, \"'index_together' refers to the nonexistent field 'nonexistent_field'.\")\n", "    def test_check_index_together_with_invalid_field(self):\n        class BadModel(models.Model):\n            name = models.CharField(max_length=100)\n            nonexistent_field = models.CharField(max_length=100)\n\n            class Meta:\n                index_together = [['name', 'nonexistent_field']]\n\n        errors = BadModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn(\"'index_together' refers to the nonexistent field\", str(errors[0]))\n\n\n\n", "    def test_index_together_too_long(self):\n        allowed_len, db_alias = get_max_column_name_length()\n        if allowed_len is None:\n            return\n        \n        class TooLongModel(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = [('name',), ('name', 'id')] \n\n        with self.assertRaises(Error) as context:\n            TooLongModel.check()\n\n        self.assertIn(\n            \"The index name generated for 'TooLongModel' is longer than maximum allowed length\",\n            str(context.exception)\n        )\n\n        class AnotherTooLongModel(models.Model):\n            name = models.CharField(max_length=255)\n\n            class Meta:\n                index_together = [('name',), ('name', Lower('name'))]\n\n        with self.assertRaises(Error) as context:\n            AnotherTooLongModel.check()\n", "    def test_index_together_invalid_field(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n            nonexistent_field = models.CharField(max_length=10)\n\n            class Meta:\n                index_together = [(\"field1\", \"nonexistent_field\")]\n\n        errors = MyModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn(\"'index_together' refers to the nonexistent field\", errors[0].msg)\n", "    def test_check_index_together_invalid_type(self):\n        class MyModel(models.Model):\n            pass\n        MyModel._meta.index_together = ['field1', 1]\n        errors = MyModel.check(databases=['default'])\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn('All \"index_together\" elements must be lists or tuples', errors[0].msg)\n\n"], "sample_1000": ["def test_octave_code_Piecewise_inline():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw, assign_to=x, inline=False) == \"\"\"\\", "def test_octave_piecewise_default():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw, assign_to=x) == 'x = ((x > 0).*(x + 1) + (~(x > 0)).*(x));'\n", "    def test_lambertw():\n        assert octave_code(LambertW(x)) == 'lambertw(x)'\n        assert octave_code(LambertW(x, -1)) == 'lambertw(-1, x)'\n\n", "def test_piecewise_inline_false():\n    tau = symbols('tau')\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert octave_code(pw, assign_to=tau, inline=False) == \\\n        'if (x > 0)\\n' \\\n        '    tau = x + 1;\\n' \\\n        'else\\n' \\\n        '    tau = x;\\n' \\\n        'end'\n", "def test_octave_piecewise_inline():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    # Check if inline=False gives a different result\n    assert mcode(pw, assign_to=x, inline=False) != mcode(pw, assign_to=x)\n"], "sample_884": ["    def test_is_deprecated(self, obj):\n        assert _is_deprecated(obj)\n", "    def test_deprecated_subclass_warning_on_init(self):\n        with pytest.warns(FutureWarning, match=\"Class MockClass1 is deprecated\"):\n            MockClass5(1)\n", "    def test_deprecated_property_inheritance():\n        obj = MockClass2()\n        with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n            n_features = obj.n_features_\n        assert n_features == 10\n", "def test_deprecated_property_inheritance():\n    obj = MockClass2()\n    with pytest.warns(FutureWarning, match=\"n_features_ is deprecated\"):\n        _ = obj.n_features_\n", "    def test_deprecated_class_inheritance():\n        # Test inheritance from a deprecated class.\n        inst = MockClass5(a=1)\n        assert inst.a == 1\n        with pytest.warns(FutureWarning, match='Class MockClass1 is deprecated'):\n            MockClass5(a=1)  \n"], "sample_327": ["    def test_jsonfield_invalid_input(self):\n        form = Form({'jsonfield': 'This is not valid JSON'})\n        with self.assertRaises(ValidationError):\n            form.is_valid()\n", "    def test_jsonfield_invalid_json(self):\n        form = Form({'data': 'invalid json'})\n        with self.assertRaisesMessage(ValidationError, 'Enter a valid JSON.'):\n            form.is_valid()\n\n", "    def test_jsonfield_invalid_json(self):\n        form = Form({'field': 'invalid json'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['field'], [\n            'Enter a valid JSON.'\n        ])\n", "    def test_jsonfield_invalid_input(self):\n        form = Form({'json_field': 'invalid json'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['json_field'], [\n            'Enter a valid JSON.',\n        ])\n", "    def test_jsonfield_with_encoder_decoder(self):\n        class MyForm(Form):\n            my_json = JSONField(encoder=DjangoJSONEncoder, decoder=json.JSONDecoder)\n\n        data = {'key': 'value', 'list': [1, 2, 3]}\n        form = MyForm(data=data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['my_json'], data)\n"], "sample_1032": ["def test_Min_symbolic():\n    x, y, z = symbols('x y z')\n    assert Min(x, y).args == (x, y)\n    assert Min(x, y, z).args == (x, y, z)\n    assert Min(x, y) != Min(y, x)\n    assert Min(x, x) == x\n\n\n\n    f = Function('f')\n    assert Min(x, f(x)).func == Min\n    assert Min(x, f(x)).args == (x, f(x))\n", "    def test_minmax_real_root(self):\n        x = Symbol('x', real=True)\n        assert real_root(x**2 - 1, 2) == 1\n        assert real_root(x**2 - 1, 0) == -1\n        assert real_root(-x**2 + 1, 2) == 1\n        assert real_root(-x**2 + 1, 0) == -1\n\n        assert real_root(x**3 - 1, 3) == 1\n        assert real_root(x**3 - 1, 0) == -1\n        assert real_root(-x**3 + 1, 3) == -1\n        assert real_root(-x**3 + 1, 0) == 1\n", "    def test_Min_Max_Heaviside_rewrite(self):\n        x = Symbol('x', real=True)\n        y = Symbol('y', real=True)\n        assert Min(x, y)._eval_rewrite_as_Piecewise(x, y) ==  Piecewise((y, x >= y), (x, True))\n        assert Max(x, y)._eval_rewrite_as_Piecewise(x, y) == Piecewise((x, x >= y), (y, True))\n        assert Min(x, y, 0)._eval_rewrite_as_Heaviside(x, y, 0), Heaviside(x - y) * Heaviside(x) + Heaviside(y - x) * Heaviside(y)\n        assert Max(x, y, 0)._eval_rewrite_as_Heaviside(x, y, 0) == Heaviside(x - y) * Heaviside(x) + Heaviside(y - x) * Heaviside(y)\n", "def test_Min_rewrite_as_Heaviside():\n    x, y, z = symbols('x y z')\n    assert Min(x, y, z).rewrite(Heaviside) == Heaviside(x - y) * Heaviside(x - z) * x + Heaviside(y - x) * Heaviside(y - z) * y + Heaviside(z - x) * Heaviside(z - y) * z\n", "def test_Max_rewrite_as_Heaviside():\n    x, y = symbols('x y')\n    assert Max(x,y).rewrite(Heaviside) == x*Heaviside(x - y) + y*Heaviside(y - x)\n"], "sample_907": ["def test_cpp_enum_value():\n    check(\n        \"enum\",\n        \"\"\"", "def test_function_with_template_parameters():\n    check(\n        \"function\",\n        \"template <typename T> void foo(T t);\",\n        {1: 'function_template__typename_T_void_foo_T_t'},\n        output=\"void foo(T t)\",\n        key=\"template <typename T> \",\n        asTextOutput=\"void foo(T t)\"\n    )\n", "def test_function_template_ref():\n    _check('func', 'void foo(int x);',\n           {1: 'foo'},\n           'void foo(int x)',\n           key='foo')\n\n", "def test_reference_operator_parentheses():\n    check(\n        'function',\n        'operator() const',\n        {1: 'operator'},\n        'operator() const',\n        asTextOutput='operator() const',\n    )\n", "def test_cpp_enum_nested():\n    check(\n        'enum',\n        \"enum class E {{ k1 = {0}, k2 = 1 }};\",\n        {1: 'E'},\n        output=\"enum class E {{ k1 = {0}, k2 = 1 }};\",\n        key=\"k1\",\n        asTextOutput=\"enum class E\\n{\\n    k1 = {0},\\n    k2 = 1\\n};\"\n    )\n"], "sample_1034": ["    def test_grover_iteration_multiple_targets():\n        nqubits = 3\n        f = lambda qubits: qubits in [IntQubit(2, nqubits=nqubits), IntQubit(5, nqubits=nqubits)]\n        v = OracleGate(nqubits, f)\n        qstate = superposition_basis(nqubits)\n        result = grover_iteration(qstate, v)\n        # Assert that the result is a superposition of states, but with\n        # a phase shift for the target states (2 and 5)\n        assert isinstance(result, (sum, IntQubit))\n        assert any(i == 2 for i in result.args)\n        assert any(i == 5 for i in result.args)\n\n", "def test_grover_iteration_multiple_solutions():\n    nqubits = 3\n    f = lambda qubits: qubits == IntQubit(4, nqubits=nqubits) or qubits == IntQubit(5, nqubits=nqubits)\n    v = OracleGate(nqubits, f)\n    state = superposition_basis(nqubits)\n    iterated = grover_iteration(state, v)\n    assert qapply(iterated) != state\n", "    def test_grover_iteration_two_qubits_different_target():\n        v = OracleGate(2, return_one_on_one)\n        state = superposition_basis(2)\n        expected = qapply(grover_iteration(state, v))\n        assert expected == IntQubit(1) \n", "    def test_grover_iteration_multiple_targets():\n        nqubits = 3\n        v = OracleGate(nqubits, lambda qubits: qubits == IntQubit(2, nqubits.nqubits) or\n                       qubits == IntQubit(4, nqubits.nqubits))\n        state = superposition_basis(nqubits)\n        result = grover_iteration(state, v)\n        assert isinstance(result, IntQubit)\n        assert result == IntQubit(2, nqubits=nqubits) or result == IntQubit(4, nqubits=nqubits)\n\n", "    def test_grover_iteration_twice():\n        nqubits = 2\n        v = OracleGate(nqubits, return_one_on_two)\n        state = superposition_basis(nqubits)\n        state = grover_iteration(state, v)\n        state = grover_iteration(state, v)\n        state = qapply(state)\n        assert state == IntQubit(2, nqubits=nqubits) \n"], "sample_631": ["    def test_unused_variable_in_nested_function(self):\n        with self.assertAddsMessages(\n            Message(\n                \"unused-variable\",\n                node=astroid.Name(name=\"inner_local\", lineno=5, col_offset=4),\n                args=(\"inner_local\",),\n            )\n        ):\n            self.checker.check(self.parse_string(\"def outer():\\n    def inner():\\n        inner_local = 10\\n\\nouter()\"))\n\n\n", "    def test_unused_import_from_nested_function(self):\n        code = \"\"\"", "    def test_unpacking_non_sequence(self):\n        code = \"\"\"\n            a, b = 1\n        \"\"\"\n        tree = astroid.parse(code)\n        with linter.build_checker(tree, 'variables') as checker:\n            checker.check_all(tree)\n        self.assertEqual(\n            checker.messages,\n            [Message(\n                msg_id='unpacking-non-sequence',\n                node=tree.body[0].body[0].targets[0],\n                args=('1,',),\n                line=3,\n                column=10,\n            )],\n        )\n", "    def test_unbalanced_tuple_unpacking_with_starred(self):\n        code = \"\"\"", "    def test_unused_variable_in_lambda(self):\n        self.checker.process(astroid.parse(\"lambda x: x\"))\n        self.assertEqual(\n            self.checker.messages,\n            [Message(\"unused-variable\", node=astroid.parse(\"lambda x: x\").body[0].args[0], args=(\"x\",))],\n        )\n\n"], "sample_295": ["    def test_combined_expression_repr(self):\n        expr = (F('num_employees') + F('num_chairs')) * 2\n        self.assertEqual(repr(expr), \"((F('num_employees') + F('num_chairs')) * 2)\")\n\n", "    def test_f_expressions(self):\n        self.assertCountEqual(\n            Company.objects.filter(ceo__salary__gt=F('num_employees')).values('name'),\n            ['Example Inc.', 'Foobar Ltd.']\n        )\n        self.assertCountEqual(\n            Company.objects.filter(num_employees__gt=F('num_chairs') * 10).values('name'),\n            ['Example Inc.', 'Foobar Ltd.', 'Test GmbH']\n        )\n", "    def test_window_functions(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            list(Experiment.objects.annotate(\n                row_number=RowNumber('id'),\n                rank=Rank('id'),\n                dense_rank=DenseRank('id'),\n                nth_value=NthValue('result', 2, window=Window(order_by=F('result').desc())),\n                lag=Lag('result', default=0),\n                lead=Lead('result', default=0),\n            ))\n            self.assertEqual(len(captured_queries), 1)\n\n", "    def test_f_expressions_with_related_fields(self):\n        # Test F expressions with related fields\n        query = Employee.objects.annotate(\n            company_name=F('company__name'),\n        ).filter(\n            company_name='Example Inc.'\n        )\n        self.assertQuerysetEqual(query, [\n            'Joe Smith',\n            'Frank Meyer'\n        ])\n\n", "    def test_coalesce(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            result = Company.objects.values(\n                'name',\n                'ceo__salary',\n                coalesce=Coalesce('num_chairs', 0),\n            ).get(name__startswith=\"Example\")\n\n        self.assertEqual(result['name'], \"Example Inc.\")\n        self.assertEqual(result['ceo__salary'], 10)\n        self.assertEqual(result['coalesce'], 5)\n        self.assertEqual(len(captured_queries), 1)\n"], "sample_661": ["    def test_junitxml_add_property(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                record_property(\"example_key\", 1)\n        \"\"\"\n        )\n        result, xml = runandparse(testdir)\n\n        assert result.ret == 0\n        testcase = xml.find_first_by_tag(\"testcase\")\n        properties = testcase.find_by_tag(\"properties\")[0].get_unique_child\n        assert properties.tag == \"property\"\n        assert properties[\"name\"] == \"example_key\"\n        assert properties.text == \"1\"\n", "    def test_junitxml_with_properties(self, testdir):\n        testdir.makefile(\n            \".py\",\n            \"\"\"\n            import pytest\n\n                pass\n        \"\"\",\n        )\n        result, dom = runandparse(testdir, \"--junitxml=test.xml\", \"-v\")\n        result.assert_outcomes(passed=1)\n        testcase = dom.find_first_by_tag(\"testcase\")\n        properties = testcase.find_by_tag(\"property\")\n        assert len(properties) == 1\n        assert properties[0][\"name\"] == \"example_key\"\n        assert properties[0].text == \"1\"\n", "    def test_junitxml_collect_error(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert 0\n        \"\"\"\n        )\n        result, dom = runandparse(testdir, \"-v\")\n        assert result.ret == pytest.ExitCode.INTERNAL_ERROR\n        testcase = dom.find_first_by_tag(\"testcase\")\n        assert testcase.assert_attr(classname=\"test_simple\", name=\"test_simple\")\n        error = testcase.find_by_tag(\"error\")[0]\n        assert error.text.startswith(\"E       AssertionError\")\n", "    def test_junitxml_duration_report_report(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            import time\n\n                time.sleep(0.1)\n                assert True\n        \"\"\"\n        )\n        result, dom = runandparse(testdir, \"--junitxml=report.xml\", \"--junit-duration-report=call\")\n        testcase = dom.find_first_by_tag(\"testcase\")\n        assert testcase[\"time\"] == pytest.approx(0.1, 0.01)\n", "    def test_junitxml_suite_name(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n        \"\"\"\n        )\n        result, dom = runandparse(testdir, \"--junitxml=path/to/file.xml\", \"-s\")\n        test_suite = dom.find_first_by_tag(\"testsuite\")\n        assert test_suite[\"name\"] == \"pytest\"\n\n        testdir.conftest.add(\n            \"\"\"\n                ini = {\n                    \"junit_suite_name\": \"my_suite_name\",\n                }\n            \"\"\"\n        )\n        result, dom = runandparse(testdir, \"--junitxml=path/to/file.xml\", \"-s\")\n        test_suite = dom.find_first_by_tag(\"testsuite\")\n        assert test_suite[\"name\"] == \"my_suite_name\"\n"], "sample_830": ["    def test_get_blas_info(self):\n        blas_info = _get_blas_info()\n        self.assertIsInstance(blas_info, dict)\n        for key in ['macros', 'lib_dirs', 'cblas_libs']:\n            self.assertIn(key, blas_info)\n", "    def test_get_blas_info(self):\n        info = _get_blas_info()\n        assert isinstance(info, dict)\n        for k, v in info.items():\n            assert isinstance(k, str)\n            if isinstance(v, str):\n                assert v is not None\n            elif isinstance(v, list):\n                assert all(isinstance(x, str) for x in v)\n            else:\n                raise TypeError(f\"Unexpected type for {k}: {type(v)}\") \n", "    def test_get_blas_info(self):\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        # Check that blas_info contains expected keys\n        for key in ['macros', 'lib_dirs', 'cblas_libs']:\n            assert key in blas_info\n", "    def test_get_blas_info(self):\n        blas_info = _get_blas_info()\n        for k in ['macros', 'lib_dirs', 'cblas_libs']:\n            assert k in blas_info\n", "    def test_get_blas_info():\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        for key in blas_info:\n            assert isinstance(blas_info[key], str)\n\n"], "sample_218": ["    def test_trunc_date(self):\n        before_midnight = datetime(2019, 12, 31, 23, 59, 59, 999999)\n        midnight = datetime(2020, 1, 1, 0, 0, 0)\n        dt = self.create_model(before_midnight, midnight)\n        self.assertEqual(DTModel.objects.filter(start_datetime__date=midnight).exists(), True)\n        self.assertEqual(DTModel.objects.filter(start_datetime__date=before_midnight.date()).exists(), False)\n\n", "    def test_extract_iso_weekday_datetime(self):\n        now = timezone.now()\n        self.create_model(now, now + timedelta(days=1))\n        with self.assertNumQueries(1):\n            result = DTModel.objects.annotate(weekday=ExtractIsoWeekDay('start_datetime')).get()\n        self.assertEqual(result.weekday, now.isoweekday())\n\n", "    def test_extract_iso_year(self):\n        dt = datetime(2023, 12, 31, tzinfo=datetime_timezone.utc)\n        model = self.create_model(dt, dt + timedelta(days=1))\n        with self.assertNumQueries(1):\n            self.assertEqual(DTModel.objects.get(pk=model.pk).start_datetime.iso_year, 2023)\n\n", "    def test_extract_iso_week_day(self):\n        dt = datetime(2023, 10, 26, 12, 0, tzinfo=timezone.utc)\n        self.create_model(dt, dt + timedelta(hours=2))\n        result = DTModel.objects.annotate(day_of_week=ExtractIsoWeekDay('start_datetime')).get()\n        self.assertEqual(result.day_of_week, 4)\n\n", "    def test_extract_year_from_datetime(self):\n        model_data = [\n            (datetime(2023, 12, 25, 12, 0, 0), 2023),\n        ]\n        for start_datetime, expected_year in model_data:\n            model = self.create_model(start_datetime, None)\n            self.assertEqual(model.start_datetime.year, ExtractYear(model.start_datetime), expected_year)\n\n"], "sample_1157": ["def test_factorial2_parsing():\n    raises(TokenError, lambda: parse_expr('x!!'))\n    assert parse_expr('x!!', transformations=(standard_transformations + (function_exponentiation,))) == factorial2(x)\n    assert parse_expr('2!!', transformations=(standard_transformations + (function_exponentiation,))) == factorial2(2)\n\n", "def test_convert_xor():\n    assert parse_expr(\"x^2\", transformations=(convert_xor,)) == x**2\n\n", "    def test_issue_20010():\n        # Test for https://github.com/sympy/sympy/issues/20010\n        expr = '1/(x+1)**2'\n        parsed = parse_expr(expr)\n        assert isinstance(parsed, Pow)\n        assert parsed.base == 1/(x + 1)\n        assert parsed.exp == -2\n\n\n", "def test_issue_4221():\n    expr = '2.0**3/5'\n    expected = Rational(8,5)\n    assert parse_expr(expr) == expected\n", "    def test_function_exponentiation():\n        assert parse_expr('sin^2(x)', transformations=(\n            standard_transformations + (function_exponentiation,))) == sin(x)**2\n        assert parse_expr('sin(x)^2', transformations=(\n            standard_transformations + (function_exponentiation,))) == sin(x)**2\n"], "sample_1197": ["def test_unit_system_extend():\n    u = UnitSystem(\n        base_units=(meter, kilogram, second),\n        name=\"my_system\",\n        descr=\"A custom system\",\n    )\n\n    v = u.extend(\n        base=(ampere,),\n        name=\"extended_system\",\n    )\n\n    assert v.name == \"extended_system\"\n    assert len(v._base_units) == 4\n    assert v._base_units == (meter, kilogram, second, ampere)\n    assert v.descr == \"A custom system\"\n", "def test_get_dimensional_expr():\n    x = Symbol('x')\n    v = Symbol('v')\n    t = Symbol('t')\n    a = Symbol('a')\n    \n    # Test basic cases\n    assert SI.get_dimensional_expr(1) == S.One\n    assert SI.get_dimensional_expr(x) == S.One\n    assert SI.get_dimensional_expr(meter) == length\n    assert SI.get_dimensional_expr(meter * second) == length * time\n    assert SI.get_dimensional_expr(meter**2) == length**2\n    \n    # Test expressions with functions\n    assert SI.get_dimensional_expr(sin(x)) == S.One\n    assert SI.get_dimensional_expr(exp(x)) == S.One\n    assert SI.get_dimensional_expr(log(x)) == S.One\n\n    # Test expressions with derivatives\n    assert SI.get_dimensional_expr(diff(x**2, x)) == S.One\n    assert SI.get_dimensional_expr(diff(meter*x, t)) == length\n\n    # Test expressions with quantities\n    q = Quantity(\"q\", \"C\")\n    assert SI.get_dimensional_expr(q) == charge\n    assert SI.get_dimensional_expr(q * meter / second) == charge * length / time\n", "    def test_UnitSystem_get_dimensional_expr(self):\n        x = symbols('x')\n        v = meter / second\n        a = meter / second**2\n        t = second\n        f = 1 / second\n        e = joule\n\n        # Test basic units\n        assert SI.get_dimensional_expr(meter) == length\n        assert SI.get_dimensional_expr(kilogram) == mass\n        assert SI.get_dimensional_expr(second) == time\n        assert SI.get_dimensional_expr(coulomb) == charge\n\n        # Test derived units\n        assert SI.get_dimensional_expr(v) == length/time\n        assert SI.get_dimensional_expr(a) == length/time**2\n        assert SI.get_dimensional_expr(f) == 1/time\n        assert SI.get_dimensional_expr(e) == mass*length**2/time**2\n        assert SI.get_dimensional_expr(x*v) == length/time\n        assert SI.get_dimensional_expr(x**2*a) == 1\n\n        # Test dimensional equations\n        assert SI.get_dimensional_expr(x*v + t*a) == length/time\n        assert SI.get_dimensional_expr(integrate(v, x)) == length**2/time\n        assert SI.get_dimensional_expr(diff(x**2*a, t)) == length/time**2\n        assert SI.get_dimensional_expr(sin(x*v)) == S.One\n\n", "    def test_physical_constant_unit_system_conversion():\n        with warns_deprecated_sympy():\n            from sympy.physics.units.units import (\n                candela, kelvin\n            )\n            assert convert_to(elementary_charge, volt * coulomb) == elementary_charge\n            assert convert_to(elementary_charge, SI.coulomb) == elementary_charge\n            assert convert_to(speed_of_light, SI.meter/SI.second) == speed_of_light\n            assert convert_to(gravitational_constant, SI.meter**3*SI.kilogram**(-1)*SI.second**(-2)) == gravitational_constant\n            assert convert_to(PhysicalConstant('G',\n                   'gravitational constant',\n                   value=gravitational_constant,\n                   unit=SI.meter**3*SI.kilogram**(-1)*SI.second**(-2)\n                ), SI.meter**3*SI.kilogram**(-1)*SI.second**(-2)) == gravitational_constant\n            assert convert_to(molar_gas_constant, SI.joule/SI.kilogram/SI.kelvin) == molar_gas_constant\n            assert convert_to(vacuum_permittivity, SI.farad*SI.meter) == vacuum_permittivity\n\n", "    def test_is_consistent():\n        assert SI.is_consistent\n"], "sample_946": ["def test_parse_annotation(app):\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('str') == ('str', None)\n    assert _parse_annotation('List[int]') == ('List', ('int',))\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Tuple[str, ...]') == ('Tuple', ('str', Ellipsis))\n    assert _parse_annotation('Callable[[str], int]') == ('Callable', (('str',), 'int'))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n    assert _parse_annotation('Optional[int]') == ('Optional', ('int',))\n    assert _parse_annotation('Any') == ('Any', None)\n", "def test_parse_annotation():\n    assert_node(\n        _parse_annotation('str'),\n        desc_annotation('str', ''),\n    )\n    assert_node(\n        _parse_annotation('List[str]'),\n        desc_annotation('List[str]', ''),\n    )\n    assert_node(\n        _parse_annotation('Dict[str, int]'),\n        desc_annotation('Dict[str, int]', ''),\n    )\n    assert_node(\n        _parse_annotation('Optional[str]'),\n        nodes.container(\n            nodes.paragraph(text='Optional'),\n            desc_annotation('str', ''),\n        ),\n    )\n    assert_node(\n        _parse_annotation('Union[str, int]'),\n        nodes.container(\n            nodes.paragraph(text='Union'),\n            desc_annotation('str', ''),\n            desc_annotation('int', ''),\n        ),\n    )\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('List[str]') == ('List', 'str')\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Union[int, float]') == ('Union', ('int', 'float'))\n    assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', '([int], str)')\n    assert _parse_annotation('Tuple[Tuple[int]]') == ('Tuple', ('Tuple', 'int'))\n\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('List[int]') == ('List', 'int')\n    assert _parse_annotation('Dict[str, int]') == ('Dict', 'str, int')\n    assert _parse_annotation('Tuple[int, float, str]') == ('Tuple', 'int, float, str')\n    assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n    assert _parse_annotation('Union[int, str]') == ('Union', 'int, str')\n", "def test_module_index_entries_with_common_prefix(app, env):\n    env.config.modindex_common_prefix = ['network', 'networkx']\n    env.domaindata['py:modules'] = {\n        'network.core': ('index.html', 'core', 'Core network components', '', False),\n        'networkx.algorithms.shortest_paths': ('index.html', 'shortest_paths',\n                                              'Shortest path algorithms', '', False)\n    }\n\n    index = PythonModuleIndex(app, env)\n    entries = index.generate()[0]\n    assert entries == [\n        ('c', [IndexEntry('core', 'module', 'index.html', 'core', '', '', 'Core network components')]),\n        ('n', [IndexEntry('etwork.core', 'module', 'index.html', 'core', '', '', 'Core network components'),\n               IndexEntry('etworkx.algorithms.shortest_paths', 'module',\n                          'index.html', 'shortest_paths', '', '', 'Shortest path algorithms')])\n    ]\n\n"], "sample_74": ["    def test_runshell_ssl_options(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydb',\n            'user': 'myuser',\n            'password': 'mypassword',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/rootcert.pem',\n            'sslcert': '/path/to/cert.pem',\n            'sslkey': '/path/to/key.pem',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n        self.assertEqual(\n            env,\n            {\n                'PGPASSWORD': 'mypassword',\n                'PGSSLMODE': 'require',\n                'PGSSLROOTCERT': '/path/to/rootcert.pem',\n                'PGSSLCERT': '/path/to/cert.pem',\n                'PGSSLKEY': '/path/to/key.pem',\n            }\n        )\n", "    def test_runshell_db_ssl_options(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': '5432',\n            'database': 'mydb',\n            'user': 'myuser',\n            'password': 'mypassword',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root.crt',\n            'sslcert': '/path/to/client.crt',\n            'sslkey': '/path/to/client.key',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertEqual(args, [\n            'psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'\n        ])\n        self.assertEqual(env, {\n            'PGPASSWORD': 'mypassword',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/root.crt',\n            'PGSSLCERT': '/path/to/client.crt',\n            'PGSSLKEY': '/path/to/client.key',\n        })\n\n", "    def test_runshell_with_ssl_options(self):\n        dbinfo = {\n            'database': 'mydb',\n            'user': 'myuser',\n            'host': 'localhost',\n            'port': 5432,\n            'sslmode': 'verify-full',\n            'sslrootcert': '/path/to/root.crt',\n            'sslcert': '/path/to/client.crt',\n            'sslkey': '/path/to/client.key',\n        }\n        args, pg_env = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n        self.assertEqual(pg_env, {\n            'PGSSLMODE': 'verify-full',\n            'PGSSLROOTCERT': '/path/to/root.crt',\n            'PGSSLCERT': '/path/to/client.crt',\n            'PGSSLKEY': '/path/to/client.key',\n        })\n", "    def test_runshell_db_ssl_params(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'database': 'mydb',\n            'user': 'myuser',\n            'password': 'mypassword',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/root.crt',\n            'sslcert': '/path/to/client.crt',\n            'sslkey': '/path/to/client.key',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h',\n                               'localhost', '-p', '5432', 'mydb'])\n        self.assertEqual(env, {\n            'PGPASSWORD': 'mypassword',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/root.crt',\n            'PGSSLCERT': '/path/to/client.crt',\n            'PGSSLKEY': '/path/to/client.key',\n        })\n", "    def test_runshell_ssl_options(self):\n        dbinfo = {\n            'host': 'test_host',\n            'port': 5432,\n            'database': 'test_db',\n            'user': 'test_user',\n            'password': 'test_password',\n            'sslmode': 'require',\n            'sslrootcert': '/path/to/rootcert.pem',\n            'sslcert': '/path/to/cert.pem',\n            'sslkey': '/path/to/key.pem',\n        }\n        args, env = self._run_it(dbinfo)\n        self.assertEqual(args, [\n            'psql', '-U', 'test_user', '-h', 'test_host', '-p', '5432', 'test_db'\n        ])\n        self.assertEqual(env, {\n            'PGPASSWORD': 'test_password',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/rootcert.pem',\n            'PGSSLCERT': '/path/to/cert.pem',\n            'PGSSLKEY': '/path/to/key.pem',\n        })\n"], "sample_839": ["def test_tfidfvectorizer_stop_words_parameter():\n    corpus = [\n        'This is the first document.',\n        'This document is the second document.',\n        'And this is the third one.',\n        'Is this the first document?',\n    ]\n    stop_words = {'this', 'is', 'the'}\n    vectorizer = TfidfVectorizer(stop_words=stop_words)\n    tfidf = vectorizer.fit_transform(corpus)\n    assert vectorizer.get_feature_names() == ['and', 'document', 'first',\n                                            'one', 'second', 'third']\n\n", "    def test_fit_transform_empty(self):\n        vec = TfidfVectorizer()\n        X = vec.fit_transform([])\n        assert X.shape == (0, 0)\n", "    def test_tfidf_vectorizer_vocabulary_param(self):\n        corpus = ALL_FOOD_DOCS\n        vocab = {'pizza': 0, 'beer': 1, 'salad': 2, 'water': 3}\n\n        # Test with a provided vocabulary\n        vectorizer = TfidfVectorizer(vocabulary=vocab)\n        X = vectorizer.fit_transform(corpus)\n        assert_array_equal(vectorizer.vocabulary_, vocab)\n        assert X.shape[1] == len(vocab)\n\n        # Test that vocabulary is not refit when already provided\n        X2 = vectorizer.transform(corpus)\n        assert_array_equal(X.toarray(), X2.toarray())\n\n        # Test warning if vocabulary and max_features disagree\n        with pytest.warns(UserWarning, match=\"The parameter 'max_features' will be ignored\"):\n            vectorizer = TfidfVectorizer(vocabulary=vocab, max_features=5)\n            vectorizer.fit(corpus)\n\n        # Test warning if vocabulary and max_df disagree\n        with pytest.warns(UserWarning, match=\"The parameter 'max_df' will be ignored\"):\n            vectorizer = TfidfVectorizer(vocabulary=vocab, max_df=0.5)\n            vectorizer.fit(corpus)\n\n", "    def test_tfidf_vectorizer_stop_words_parameter(self):\n        corpus = [\n            'This is the first document.',\n            'This document is the second document.',\n            'And this is the third one.',\n            'Is this the first document?',\n        ]\n        # Ensure that 'the' is always removed regardless of the stop_words\n        # parameter being None or a list\n        stop_words_list = ['the']\n        for stop_words in [None, stop_words_list]:\n            vectorizer = TfidfVectorizer(stop_words=stop_words)\n            X = vectorizer.fit_transform(corpus)\n            assert 'the' not in vectorizer.get_feature_names()\n            assert X.shape[1] == len(vectorizer.get_feature_names())\n\n", "def test_tfidfvectorizer_vocabulary_attribute():\n    # Test that the vocabulary_ attribute is correctly set after fitting\n    corpus = [\n        'This is the first document.',\n        'This document is the second document.',\n        'And this is the third one.',\n        'Is this the first document?',\n    ]\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit(corpus)\n    assert isinstance(vectorizer.vocabulary_, dict)\n    assert len(vectorizer.vocabulary_) == 9\n    assert 'document' in vectorizer.vocabulary_\n\n"], "sample_293": ["compilation error", "compilation error", "compilation error", "    def test_reverse_with_trailing_slash(self):\n        with self.assertRaises(NoReverseMatch):\n            reverse('places2', args=[''])\n        with self.assertRaises(NoReverseMatch):\n            reverse('places2', kwargs={'name': ''})\n", "compilation error"], "sample_1134": ["    def test_latex_permutation_matrix(self):\n        X = PermutationMatrix(Permutation([1, 2, 0]))\n        self.assertEqual(latex(X), r\"\\begin{pmatrix}0 & 1 & 0\\\\ 0 & 0 & 1\\\\ 1 & 0 & 0\\end{pmatrix}\")\n", "def test_latex_singularities():\n    assert latex(SingularityFunction(x, 0, 1)) == '\\\\text{SF}(x, 0, 1)'\n    assert latex(SingularityFunction(x, 1, 1)) == '\\\\delta(x)'\n\n\n", "    def test_latex_SetExpr(self):\n        s1 = SetExpr(Interval(0, 1), Symbol('x'))\n        s2 = SetExpr(Interval(1, 2), Symbol('y'))\n        assert latex(s1) == r\"\\left\\{x \\mid 0 \\le x \\le 1\\right\\}\"\n        assert latex(s2) == r\"\\left\\{y \\mid 1 \\le y \\le 2\\right\\}\"\n        assert latex(Union(s1, s2)) == r\"\\left\\{x \\mid 0 \\le x \\le 1\\right\\} \\cup \\left\\{y \\mid 1 \\le y \\le 2\\right\\}\"\n\n\n", "    def test_latex_KroneckerProduct(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 3, 3)\n        expr = KroneckerProduct(A, B)\n        assert latex(expr) == r'A \\otimes B'\n", "def test_latex_ComplexRegion():\n    r = ComplexRegion(Interval(0, 1))\n    assert latex(r) == r'\\text{ComplexRegion}([0, 1])'\n    r = ComplexRegion(Interval(0, 1), Interval(0, 2*pi))\n    assert latex(r) == r'\\text{ComplexRegion}([0, 1] \\times [0, 2 \\pi])'\n"], "sample_691": ["    def test_faulthandler_timeout_disabled(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import time\n            import pytest\n\n            @pytest.mark.parametrize(\"timeout\", [0.1, 1.0])\n                time.sleep(timeout + 0.1)\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines(\"*2 passed in*\")\n", "    def test_faulthandler_timeout_config(pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n            faulthandler_timeout = 0.1\n            \"\"\"\n        )\n        pytester.makepyfile(\n            \"\"\"\n            import time\n\n                time.sleep(0.2)\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(failed=1)\n        assert \"Timeout\" in result.stdout.str()\n", "    def test_faulthandler_timeout_dump(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import time\n\n                time.sleep(1)\n        \"\"\"\n        )\n        pytester.runpytest(\"--faulthandler-timeout=0.5\")\n        result = pytester.stdout.str()\n        assert \"Thread traceback (most recent call last):\" in result\n", "    def test_faulthandler_timeout_enabled(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import time\n            import faulthandler\n\n                time.sleep(1)\n\n                pass\n            \"\"\"\n        )\n        pytester.addini(\"faulthandler_timeout=0.5\")\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1, failed=1)\n        assert \"dump_traceback_later\" in result.stdout.str()\n\n", "    def test_faulthandler_timeout_dumps_traceback(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import time\n            import pytest\n\n                time.sleep(1)\n                assert 1 == 1\n        \"\"\"\n        )\n        result = pytester.runpytest(\"--faulthandler-timeout=0.5\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_long_running*\",\n                \"*Timeout*\",\n                \"*Current traceback:*\",\n            ]\n        )\n"], "sample_80": ["    def test_split_exclude_with_multi_join(self):\n        query = Query(Item.objects.all())\n        query.add_filter(\n            (\n                'author__books__name',\n                F('author__name'),\n            )\n        )\n        split_query, should_negate = query._split_exclude()\n        self.assertIsNotNone(split_query)\n        self.assertFalse(should_negate)\n", "    def test_add_filters_empty_where(self):\n        q = Query(Item)\n        q.add_filters({'pk': 1})\n        self.assertEqual(len(q.where.children), 1)\n        q.add_filters({})\n        self.assertEqual(len(q.where.children), 1)\n\n", "    def test_join_promotion_negated_or(self):\n        \"\"\"\n        Test that join promotion works correctly with negated OR conditions.\n        \"\"\"\n        query = Query(Item)\n        query.add_filter(Q(author__name__exact='John') | Q(author__name__exact='Jane'))\n        query.add_filter(~(Q(price__gt=10) | Q(price__lt=5)))\n        query.add_ordering(('author__name', 'ASC'))\n        query.build_filter(None)\n        # Check that joins are promoted correctly.\n\n", "    def test_add_to_dict(self):\n        data = {}\n        add_to_dict(data, 'foo', 'bar')\n        self.assertEqual(data['foo'], {'bar'})\n        add_to_dict(data, 'foo', 'baz')\n        self.assertEqual(data['foo'], {'bar', 'baz'})\n        add_to_dict(data, 'qux', 'quux')\n        self.assertEqual(data['qux'], {'quux'})", "    def test_complex_join_promotion(self):\n        author = Author.objects.get(name='Adrian Holovaty')\n        ranking = Ranking.objects.get(author=author)\n        q = Q()\n        q |= Q(ranking__score__gt=100)\n        q |= Q(ranking__score__lt=50)\n        q &= Q(ranking__author__name__icontains='Adrian')\n        qs = Item.objects.filter(ranking=ranking).filter(q)\n        # Verify that the generated query uses inner joins where possible\n        # and the join types are optimized based on the filters being\n        # applied.\n        # ... (Add assertions to verify the query structure)\n\n"], "sample_715": ["    def test_cross_validate_fit_params(self):\n        X = np.random.randn(20, 5)\n        y = np.random.randint(0, 5, size=20)\n\n        # Check if fit_params are properly passed\n        clf = MockClassifier()\n        cv = KFold(n_splits=2)\n        scores = cross_validate(clf, X, y, cv=cv,\n                                fit_params={'dummy_int': 1,\n                                            'dummy_str': 'test',\n                                            'dummy_obj': object()})\n        assert_equal(clf.dummy_int, 1)\n        assert_equal(clf.dummy_str, 'test')\n        assert_equal(clf.dummy_obj, object())\n\n        # Check if callback is properly called\n        called = []\n            called.append(True)\n\n        clf = MockClassifier()\n        cv = KFold(n_splits=2)\n        scores = cross_validate(clf, X, y, cv=cv, fit_params={'callback': callback})\n        assert_true(called)\n", "    def test_cross_validate_with_sparse_matrix(self):\n        # Test cross_validate with sparse matrix input\n        clf = MockClassifier(allow_nd=True)\n        cv = KFold(n_splits=2)\n\n        scores = cross_validate(clf, X_sparse, y, cv=cv, scoring='accuracy')\n\n        assert_array_equal(scores['test_score'], [0.5, 0.5])\n\n        # Test with sample_weight that is not a 1d array\n        sw = np.eye(10)\n        assert_raises(ValueError, cross_validate, clf, X_sparse, y,\n                      cv=cv, scoring='accuracy', sample_weight=sw)\n\n        # Test with sparse_sample_weight that is not a 1d array\n        ssw = np.eye(10)\n        assert_raises(ValueError, cross_validate, clf, X_sparse, y,\n                      cv=cv, scoring='accuracy', sparse_sample_weight=ssw)\n\n", "    def test_cross_validate_with_sparse_matrix(self):\n        # test case for issue #10207\n        clf = MockClassifier(allow_nd=True)\n        scores = cross_validate(clf, X_sparse, y, cv=3, scoring='accuracy',\n                                return_train_score=True)\n        assert_array_equal(scores['test_score'], [1., 1., 1.])\n\n", "    def test_cross_val_predict_sparse_input_with_sample_weight(self):\n        clf = MockClassifier(allow_nd=True)\n        X = coo_matrix([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        sample_weight = np.array([0.5, 1, 1.5])\n        with assert_warns(UserWarning):\n            pred = cross_val_predict(clf,\n                                     X,\n                                     y,\n                                     scoring='accuracy',\n                                     cv=3,\n                                     sample_weight=sample_weight)\n", "    def test_learning_curve_groups_with_shuffle(self):\n        # Check that learning_curve works with groups and shuffle=True\n\n        X = np.random.randn(10, 2)\n        y = np.random.randint(0, 2, size=10)\n        groups = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2])\n\n        estimator = KNeighborsClassifier()\n        train_sizes, train_scores, test_scores = learning_curve(\n            estimator, X, y, groups=groups, train_sizes=np.linspace(0.1, 1.0, 5),\n            cv=GroupKFold(n_splits=3), shuffle=True, random_state=42\n        )\n\n        assert_true(train_scores.shape == (3, 5))\n        assert_true(test_scores.shape == (3, 5))\n\n"], "sample_1158": ["def test_sympify_numpy__ndarray():\n    a = numpy.array([1, 2, 3])\n    assert sympify(a) == ImmutableDenseNDimArray([1, 2, 3])\n\n\n", "def test_sympify_numpy_ndarray():\n    if numpy is None:\n        skip(\"numpy required\")\n\n    a = numpy.array([1, 2, 3])\n    e = sympify(a)\n    assert isinstance(e, ImmutableDenseNDimArray)\n\n    a = numpy.array([[1, 2], [3, 4]])\n    e = sympify(a)\n\n    assert isinstance(e, ImmutableDenseNDimArray)\n\n", "def test_sympify_dict_defaultdict():\n    d = defaultdict(lambda: 0)\n    d[1] = 2\n    assert sympify(d) == {1: 2}\n    assert sympify({1:2}) == {1:2}\n    d = defaultdict(int)\n    d['a'] = 1\n    assert sympify(d) == {'a': 1}\n\n", "def test_sympify_numpy_scalar():\n    import numpy as np\n    a = np.float64(3.14)\n    assert sympify(a) == Float(3.14)\n\n    a = np.int64(42)\n    assert sympify(a) == Integer(42)\n\n    a = np.complex128(1+2j)\n    assert sympify(a) == complex(1, 2)\n\n", "def test_sympify_issue_18066():\n    from sympy.utilities.exceptions import SymPyDeprecationWarning\n    with warns_deprecated_sympy(\n        feature=\"String fallback in sympify\",\n            useinstead= \\\n                'sympify(str(obj)) or ' + \\\n                'sympy.core.sympify.converter or obj._sympy_',\n            issue=18066,\n            deprecated_since_version='1.6'\n    ):\n        a = [1, 2, 3]\n        sympify(a)\n"], "sample_1067": ["def test_expand_2arg():\n    x, y = symbols(\"x y\")\n    assert expand_2arg((x + 1) * (2*y + 3)) == 2*x*y + 3*x + 2*y + 3\n    assert expand_2arg((x + 1) * (y + 2)) == x*y + 2*x + y + 2\n    assert expand_2arg(2*(x + y)) == 2*x + 2*y\n    assert expand_2arg(3*(x + y + 1)) == 3*x + 3*y + 3\n\n", "def test_expand_2arg():\n    x, y = symbols('x y')\n    assert expand_2arg(x*(y + 1)) == x*y + x\n", "def test_expand_2arg():\n    x, y = symbols('x y')\n    assert expand_2arg(x*(y + 1)) == x*y + x\n    assert expand_2arg(2*(x + 1)) == 2*x + 2\n    assert expand_2arg(x*(y + x)) == x*y + x**2\n    assert expand_2arg(2*(x + y + 1)) == 2*x + 2*y + 2\n    assert expand_2arg((x + 1)*(y + 1)) == x*y + x + y + 1\n\n", "def test_expand_2arg():\n    x, y = symbols('x y')\n    assert expand_2arg(2*(x + y)) == 2*x + 2*y\n    assert expand_2arg(x*(y + 1)) == x*y + x\n", "def test_as_ordered_factors():\n    x, y = symbols('x y')\n    f = (2*x*y*sin(x)*cos(x))\n    assert f.as_ordered_factors() == [2, x, y, sin(x), cos(x)]\n"], "sample_31": ["    def test_write_latex_overwrite(self, tmp_path, cosmo):\n        filename = tmp_path / \"test.tex\"\n        write_latex(cosmo, filename)\n        assert filename.exists()\n\n        # Try writing again, should raise error as overwrite is False by default\n        with pytest.raises(IORegistryError):\n            write_latex(cosmo, filename)\n        # Write with overwrite=True\n        write_latex(cosmo, filename, overwrite=True)\n        assert filename.exists()\n\n", "    def test_write_latex_header(self, cosmo, tmp_path):\n        with tmp_path.join(\"test.tex\").open(\"w\") as fd:\n            write_latex(cosmo, fd, format=\"latex\")\n\n        with open(tmp_path / \"test.tex\") as fd:\n            content = fd.read()\n\n        for param_name in cosmo.__parameters__:\n            assert param_name in content\n", "    def test_write_latex_with_cls(self, cosmo, tmp_path):\n        \"\"\"Test writing to latex with different cls.\"\"\"\n\n        # Create temporary filenames\n        ascii_file = tmp_path / \"cosmo.ascii.latex\"\n        latex_file = tmp_path / \"cosmo.latex\"\n\n        # Write to ascii.latex with QTable\n        write_latex(cosmo, ascii_file, cls=QTable)\n        \n        # Write to latex with Table\n        write_latex(cosmo, latex_file, cls=Table)\n\n        # Check if files are created\n        assert ascii_file.exists()\n        assert latex_file.exists()\n", "    def test_write_latex_with_units(self, cosmo):\n        import astropy.units as u\n\n        table = to_table(\n            cosmo, cls=QTable, cosmology_in_meta=False\n        )\n        for param in cosmo.parameters:\n            if cosmo.parameters[param].unit is not None:\n                assert table[param].unit == cosmo.parameters[param].unit\n\n        with pytest.raises(IORegistryError):\n            write_latex(\n                cosmo, \"test.txt\", format=\"unknown_format\"\n            )\n", "    def test_latex_names(self, cosmo, tmp_path):\n        file = tmp_path / \"test.tex\"\n        write_latex(cosmo, file, latex_names=True)\n\n        # Check if LaTeX names were used in the output file\n        with open(file, \"r\") as f:\n            content = f.read()\n        for latex_name in _FORMAT_TABLE.values():\n            assert latex_name in content\n\n"], "sample_666": ["    def test_tee_capture(self, monkeypatch, tmpdir):\n        monkeypatch.setattr(sys, \"stdout\", io.StringIO())\n        monkeypatch.setattr(sys, \"stderr\", io.StringIO())\n        monkeypatch.setattr(os, \"environ\", {\"HOME\": str(tmpdir)})\n\n        capture_manager = capture.CaptureManager(\n            sys.stdout, sys.stderr, sys.stdin\n        )\n        capture_manager.start_capturing()\n\n        print(\"Hello from stdout\")\n        print(\"Hello from stderr\", file=sys.stderr)\n\n        capture_manager.stop_capturing()\n        assert capture_manager.out.getvalue() == \"Hello from stdout\\n\"\n        assert capture_manager.err.getvalue() == \"Hello from stderr\\n\"\n", "    def test_capture_tee(self, tmpdir):\n        out_path = tmpdir.join(\"out.txt\")\n        err_path = tmpdir.join(\"err.txt\")\n        with TeeStdCapture() as cap:\n            print(\"hello stdout\")\n            print(\"hello stderr\", file=sys.stderr)\n\n            # simulate subprocess output\n            with open(out_path, \"w\") as f:\n                f.write(\"subprocess output\\n\")\n            with open(err_path, \"w\") as f:\n                f.write(\"subprocess error\\n\")\n\n        assert cap.out == \"hello stdout\\n\"\n        assert cap.err == \"hello stderr\\n\"\n\n        assert out_path.read() == \"subprocess output\\n\"\n        assert err_path.read() == \"subprocess error\\n\"\n\n", "    def test_capture_output_multicapture(self, monkeypatch):\n        monkeypatch.setattr(sys, 'stdout', StringIO())\n        monkeypatch.setattr(sys, 'stderr', StringIO())\n        capman = CaptureManager()\n        with capman.capture() as multicapture:\n            print(\"hello\")\n            print(\"world\", file=sys.stderr)\n        assert multicapture.out == \"hello\\n\"\n        assert multicapture.err == \"world\\n\"\n", "    def test_tee_sys_capture(self, tmpdir):\n        mc = TeeStdCapture()\n        mc.start()\n        print(\"hello\")\n        print(\"world\", file=sys.stderr)\n        mc.stop()\n        out = mc.get_output()\n        err = mc.get_error()\n        assert out == \"hello\\n\"\n        assert err == \"world\\n\"\n        assert mc.out == out\n        assert mc.err == err\n\n", "    def test_multicapture_basic(self, method) -> None:\n        with StdCapture(out=method) as cap:\n            print(\"hello out\")\n            sys.stderr.write(\"hello err\\n\")\n        assert cap.out == (\"hello out\\n\")\n        assert cap.err == (\"hello err\\n\")\n\n"], "sample_673": ["    def test_doctest_module_collects_doctests_in_setup_py(self, tmpdir):\n        setup_py = tmpdir.join(\"setup.py\")\n        setup_py.write(\n            textwrap.dedent(\n                \"\"\"\n                from setuptools import setup\n\n                setup(\n                    name='test',\n                    version='0.1',\n                    packages=['test'],\n                )\n                \"\"\"\n            )\n        )\n        doctest_file = tmpdir.join(\"test\", \"__init__.txt\")\n        doctest_file.write(\"This is a doctest\")\n        config = pytest.Config()\n        config.doctestmodules = True\n        collector = DoctestModule.from_parent(None, fspath=setup_py, config=config)\n        assert len(list(collector.collect())) == 0\n", "    def test_doctest_module_with_mock(self, tmpdir):\n        # Regression test for issue #5080:\n\n        # Create a test module with a mocked object\n        mod_file = tmpdir.join(\"test_module.py\")\n        mod_file.write(\n            textwrap.dedent(\n                \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return Mock()\n\n                assert my_mock.attr == \"value\"\n                \"\"\"\n            )\n        )\n\n        # Ensure the module can be imported\n        import test_module\n\n        # Collect and run the doctest\n        doctest_item = DoctestModule.from_parent(None, fspath=mod_file, parent=None)\n        with _patch_unwrap_mock_aware():\n            doctest_item._obj = test_module\n            doctest_item.setup()\n\n            # Check that _is_mocked returns True for the mocked object\n            assert _is_mocked(doctest_item._obj.my_mock)\n            # Run the doctest\n            doctest_item.runtest()\n\n", "    def test_doctest_module_mocked_object(self, tmpdir):\n        tmpdir.join(\"conftest.py\").write(\n            textwrap.dedent(\n                \"\"\"\n            metafunc.parametrize(\"x\", [mock.Mock()])\n        \"\"\"\n            )\n        )\n        tmpdir.join(\"test_module.py\").write(\n            textwrap.dedent(\n                \"\"\"\n        # doctest: +SKIP\n\n            return x\n\n        \"\"\"\n            )\n        )\n        config = pytest.Config.fromdict({\"doctestmodules\": True}, rootdir=tmpdir)\n        col = pytest.Collector(config)\n        with _patch_unwrap_mock_aware():\n            items = col.collect()\n        assert len(items) == 1\n        doctest_item = items[0]\n        assert doctest_item.obj is not None\n        assert _is_mocked(doctest_item.obj)", "    def test_doctest_item_repr_failure_multiple_failures(self, tmpdir):\n        # See #936\n        p = tmpdir.join(\"test_module.py\")\n        p.write(\n            textwrap.dedent(\n                \"\"\"\n            assert 1 == 2\n\n            assert 3 == 4\n        \"\"\"\n            )\n        )\n        config = self._make_test_config(p)\n        item = DoctestItem.from_parent(None, name=\"test_module\", runner=None, dtest=None)\n        item._obj = p.pyimport()\n        item._fixture_request = None\n        item.dtest = inspect.getmembers(item._obj)[0][1].__code__.co_consts[1]\n        \n        \n        with pytest.raises(MultipleDoctestFailures) as excinfo:\n            item.runtest()\n        assert len(excinfo.value.failures) == 2\n", "    def test_doctest_module_mocked_object(self, tmpdir):\n        example = \"\"\"\n        >>> from unittest.mock import MagicMock\n        >>> obj = MagicMock()\n        >>> obj.value = 1\n        >>> obj.value\n        1\n        \"\"\"\n        doctest_file = tmpdir.join(\"test.py\")\n        doctest_file.write(example)\n\n        config = pytest.Config.fromdict({\"doctest_modules\": True})\n        item = DoctestModule.from_parent(None, fspath=doctest_file, parent=None)\n        item.config = config\n        item._fixtureinfo = []\n\n        with _patch_unwrap_mock_aware():\n            for subitem in item.collect():\n                assert isinstance(subitem, DoctestItem)\n\n"], "sample_789": ["    def test_ada_boost_regressor_sparse_input():\n        # Test AdaBoostRegressor with sparse input\n\n        # Create a sparse matrix\n        X_sparse = csc_matrix(iris.data)\n        y = iris.target\n\n        # Split the dataset into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_sparse, y, test_size=0.25, random_state=rng)\n\n        # Instantiate a DecisionTreeRegressor as base estimator\n        base_estimator = DecisionTreeRegressor(max_depth=1)\n\n        # Instantiate AdaBoostRegressor with the base estimator\n        reg = AdaBoostRegressor(base_estimator=base_estimator, random_state=rng)\n\n        # Fit the model\n        reg.fit(X_train, y_train)\n\n        # Predict on the test set\n        y_pred = reg.predict(X_test)\n\n        # Check if predictions are not None\n        assert y_pred is not None\n", "    def test_samme_r_sparse_matrix(self):\n        # Check that SAMME.R works correctly with sparse matrices\n        # (csc, csr, coo, dok, lil)\n\n        for matrix_type in [csc_matrix, csr_matrix, coo_matrix, dok_matrix,\n                           lil_matrix]:\n            X = matrix_type(\n                np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n            y = np.array([1, -1, 1])\n            estimator = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(\n            max_depth=1),\n                algorithm='SAMME.R', random_state=rng)\n            estimator.fit(X, y)\n            y_pred = estimator.predict(X)\n            assert_array_equal(y_pred, y)\n\n\n\n", "    def test_sparse_matrix_support(self):\n        # Test with sparse matrices\n        for matrix_type in [csc_matrix, csr_matrix, coo_matrix,\n                            dok_matrix, lil_matrix]:\n            X = matrix_type(X)\n            y = np.array(y_class, dtype=object)\n            clf = AdaBoostClassifier(DecisionTreeClassifier(), random_state=rng)\n            clf.fit(X, y)\n            assert_equal(clf.predict(X), y)\n\n", "def test_predict_proba_sparse_matrix():\n    \"\"\"Test predict_proba for sparse matrix\"\"\"\n    # Create a sparse matrix\n    X_sparse = csr_matrix(X)\n    \n    # Fit model\n    clf = AdaBoostClassifier(n_estimators=10, random_state=rng)\n    clf.fit(X, y_class)\n\n    # Predict probabilities on sparse matrix\n    proba = clf.predict_proba(X_sparse)\n\n    # Check if the shape is correct\n    assert_equal(proba.shape, (len(X), 2))\n\n    # Check if all proba sums to 1\n    assert_array_almost_equal(proba.sum(axis=1), np.ones(len(X)))\n", "    def test_ada_boost_regressor_sparse_matrix(self):\n        X = csc_matrix(iris.data)\n        y = iris.target\n        est = AdaBoostRegressor(random_state=rng).fit(X, y)\n        y_pred = est.predict(X)\n        assert_array_almost_equal(y, y_pred, decimal=2)\n"], "sample_314": ["    def test_creation_form_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.custom_user.CustomUser'):\n            form = UserCreationForm()\n            self.assertEqual(\n                form.fields['username'].widget.attrs['autofocus'], True\n            )\n", "    def test_create_user_with_custom_email_field(self):\n        # Test that UserCreationForm works with a custom email field.\n        data = {'username': 'testuser', 'password1': 'password', 'password2': 'password'}\n        form = UserCreationForm(data=data)\n        form.fields['username'].widget = forms.TextInput()\n        self.assertTrue(form.is_valid())\n        user = form.save()\n        self.assertEqual(user.username, 'testuser')\n", "    def test_creation_form_with_custom_user_model(self):\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta:\n                model = CustomUser\n                fields = (\"username\", \"first_name\", \"last_name\")\n\n        form = CustomUserCreationForm()\n        self.assertEqual(form.fields.keys(), {'username', 'password1', 'password2', 'first_name', 'last_name'})\n", "    def test_empty_password_creation(self):\n        form_data = {'username': 'empty_password', 'password1': '', 'password2': ''}\n        form = UserCreationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password1', form.errors)\n        self.assertEqual(\n            form.errors['password1'][0].code,\n            'password_too_short',\n        )\n", "    def test_password2_field_help_text(self):\n        form = UserCreationForm()\n        self.assertEqual(form.fields['password2'].help_text, _(\n            \"Enter the same password as before, for verification.\"\n        ))\n"], "sample_891": ["    def test_top_k_accuracy_score_multiclass_labels_provided(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                               [0.3, 0.4, 0.2],  # 1 is in top 2\n                               [0.2, 0.4, 0.3],  # 2 is in top 2\n                               [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n        labels = [0, 1, 2]\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_almost_equal(score, 0.75)\n", "    def test_top_k_accuracy_score_multiclass_labels(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                            [0.3, 0.4, 0.2],  # 1 is in top 2\n                            [0.2, 0.4, 0.3],  # 2 is in top 2\n                            [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n        labels = np.array([2, 0, 1])  \n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_array_equal(score, 0.5)\n\n", "    def test_roc_auc_score_multiclass_constant_prediction(self):\n        # Constant prediction should result in a score of 0.5\n        y_true = [0, 1, 2] * 5\n        y_score = [0.5, 0.5, 0.5] * 5\n        assert_almost_equal(roc_auc_score(y_true, y_score, multi_class=\"ovr\"),\n                            0.5)\n        assert_almost_equal(roc_auc_score(y_true, y_score, multi_class=\"ovo\"),\n                            0.5)\n", "    def test_top_k_accuracy_score_multiclass_labels_provided(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],\n                            [0.3, 0.4, 0.2],\n                            [0.2, 0.4, 0.3],\n                            [0.7, 0.2, 0.1]])\n        labels = [0, 1, 2]\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_array_almost_equal(score, 0.75)\n", "    def test_dcg_score_ties(self):\n        y_true = np.array([1, 2, 3, 0, 1, 2])\n        y_score = np.array([0.8, 0.7, 0.6, 0.5, 0.4, 0.3])\n        # Test with ties in predicted scores\n        dcg = dcg_score(y_true, y_score, k=3)\n        assert_almost_equal(dcg, 3.12..., 2)\n\n        # Test with ties in predicted scores and ignore_ties=True\n        dcg_ignore_ties = dcg_score(\n            y_true, y_score, k=3, ignore_ties=True\n        )\n        assert_almost_equal(dcg_ignore_ties, 3.0..., 2)\n"], "sample_485": ["    def test_strip_spaces_between_tags(self):\n        self.check_output(strip_spaces_between_tags, '<p>Hello</p> <p>World!</p>', '<p>Hello</p><p>World!</p>')\n        self.check_output(strip_spaces_between_tags, '<div>  Some text  </div>', '<div>Some text</div>')\n        self.check_output(strip_spaces_between_tags, '<span>Text with<br/> line breaks</span>')\n\n\n\n", "    def test_strip_spaces_between_tags(self):\n        self.check_output(strip_spaces_between_tags, '<p>   hi </p>')\n        self.check_output(strip_spaces_between_tags, '<p>hi  </p>')\n        self.check_output(strip_spaces_between_tags, '<p> hi </p>')\n        self.check_output(strip_spaces_between_tags, '<p>hi</p>')\n        self.check_output(strip_spaces_between_tags, '<p> hi  <br> </p>')\n", "    def test_urlize_with_nofollow(self):\n        self.assertEqual(\n            urlize(\"Visit https://www.djangoproject.com\", nofollow=True),\n            '<a href=\"https://www.djangoproject.com\" rel=\"nofollow\">https://www.djangoproject.com</a>',\n        )\n", "    def test_json_script_safe(self):\n        output = '<script type=\"application/json\">{\"key\": \"value\"}</script>'\n        self.assertEqual(json_script({\"key\": \"value\"}), output)\n        self.assertEqual(\n            json_script({\"key\": \"value\"}, element_id=\"my-element\"),\n            '<script id=\"my-element\" type=\"application/json\">{\"key\": \"value\"}</script>',\n        )\n", "    def test_escapejs(self):\n        self.assertEqual(escapejs('\"'), '\\\\u0022')\n        self.assertEqual(escapejs(\"'\"), '\\\\u0027')\n        self.assertEqual(escapejs(\"<\"), '\\\\u003C')\n        self.assertEqual(escapejs(\">\"), '\\\\u003E')\n        self.assertEqual(escapejs(\"&\\\"\"), '\\\\u0026\\\\u0022')\n        self.assertEqual(escapejs(\"\\u2028\"), \"\\\\u2028\")\n        \n        # Test escaping control characters\n        self.assertEqual(escapejs(\"a\\x00b\"), \"a\\\\u0000b\")\n"], "sample_418": ["    def test_length_is(self):\n        template = self.engine.from_string('{% if some_list|length_is:\"4\" %}Four{% endif %}')\n        self.assertEqual(template.render({'some_list': [1, 2, 3, 4]}), 'Four')\n", "    def test_length_is_deprecated(self):\n        template = self.engine.from_string('{% if some_list|length_is:4 %}Four{% endif %}')\n        with self.assertWarns(RemovedInDjango51Warning):\n            template.render(context={'some_list': [1, 2, 3, 4]})\n", "    def test_length_is01(self):\n        template = self.engine.from_string('{% load templatetag_tests %}{% load length_is %}{{ length_is01 }}')\n        self.assertEqual(template.render({'some_list': [1, 2, 3, 4]}), 'Four')\n", "    def test_length_is(self):\n        template = self.engine.from_string('{% load templatetag %}{{ some_list|length_is:\"4\" }}')\n        self.assertEqual(template.render({\n            'some_list': [1, 2, 3, 4]\n        }), '')\n        template = self.engine.from_string('{% load templatetag %}{{ some_list|length_is:\"3\" }}')\n        self.assertEqual(template.render({\n            'some_list': [1, 2, 3]\n        }), '')\n", "    def test_length_is(self):\n        with self.settings(TEMPLATE_STRING_IF_EMPTY=''):\n            template = self.engine.from_string('{% if some_list|length_is:\"4\" %}Four{% endif %}')\n            self.assertEqual(template.render({'some_list': [1, 2, 3, 4]}), 'Four')\n"], "sample_1119": ["    def test_inverse_ADJ():\n        assert Inverse(C).doit() == _inv_ADJ(C)\n        assert Inverse(D).doit() == _inv_ADJ(D)\n\n", "def test_A_add_B():\n    assert A + B == (A + B)\n\n", "def test_matrix_exp_jblock():\n    from sympy.matrices import Matrix, eye\n    m = Matrix([[0, 1], [-1, 0]])\n    J = diagonalize(m)\n    assert J[0].is_jordan_block\n\n", "def test_rank_decomposition_empty():\n    assert Matrix()._rank_decomposition() == (Matrix(), Matrix())\n\n", "    def test_inverse_ADJ_zero_determinant():\n        X = Matrix([[1, 2], [3, 6]])\n        raises(NonInvertibleMatrixError, lambda: X.inverse_ADJ())\n\n"], "sample_622": ["    def test_encode_decode_bool_scalar(self):\n        original_data = np.array(True, dtype=bool)\n        var = Variable(\"foo\", original_data)\n        ds = Dataset({\"foo\": var})\n        ds_encoded = conventions.encode_cf_variable(\n            ds[\"foo\"], name=\"foo\", encode_dtype=True\n        )\n        assert ds_encoded.dtype == np.dtype(\"bool\")\n        ds_decoded = Variable.from_dict(\n            conventions.decode_cf_variable(ds_encoded, name=\"foo\")\n        )\n        assert_identical(var, ds_decoded)\n        assert ds_decoded.dtype == np.dtype(\"bool\")\n", "    def test_decode_bool(self):\n        data = np.array([True, False, True, False], dtype=bool)\n        var = Variable(\"x\", data)\n        encoded = encode_cf_variable(var)\n        assert encoded.dtype == np.int8\n        decoded = decode_cf_variable(encoded)\n        assert_array_equal(decoded, data)\n", "    def test_bool_dtype_encoding(self, dtype):\n        data = np.array([True, False, True], dtype=dtype)\n        var = Variable(\"x\", data)\n        expected_data = data.astype(bool)\n        encoded = conventions.encode_cf_variable(var)\n        assert_identical(encoded, Variable(\"x\", expected_data, dtype=\"bool\"))\n        decoded = conventions.decode_cf_variable(\n            \"x\", encoded, decode_times=False, scale_factor=1, add_offset=0\n        )\n        assert_array_equal(decoded, expected_data)\n\n", "    def test_encode_decode_cftime_variable(self):\n        times = cftime_range(\n            start=\"2000-01-01\", periods=10, freq=\"D\", calendar=\"standard\"\n        )\n\n        ds = Dataset({\"time\": (\"time\", times), \"data\": (\"time\", np.arange(10))})\n        ds = ds.set_coords(\"time\")\n\n        # Encode the dataset\n        ds_encoded = conventions.encode_cf_variable(ds.time)\n        ds_encoded = Dataset({\"time\": ds_encoded})\n\n        # Decode the dataset\n        ds_decoded = conventions.decode_cf_variable(\n            ds_encoded.time, name=\"time\"\n        )\n\n        assert ds_decoded.dtype == np.dtype('datetime64[ns]')\n        assert_array_equal(ds_decoded, times)\n", "    def test_encode_decode_bool_with_fill_value(self):\n        data = np.array([True, False, True, False], dtype=bool)\n        var = Variable([\"x\"], data)\n        var_encoded = conventions.encode_cf_variable(var, name=\"test\")\n\n        assert var_encoded.dtype == np.dtype(\"bool\")\n\n        ds = Dataset({\"test\": var_encoded})\n        with contextlib.suppress(warnings.catch_warnings()):\n            ds_decoded = decode_cf(ds)\n\n        assert_identical(ds_decoded[\"test\"], var)\n"], "sample_962": ["    def test_mock_undecorate(self):\n        with mock(['test_module']):\n            mocked_object = _MockObject('mocked_function')\n            mocked_object.__sphinx_decorator_args__ = ('real_function', )\n            assert undecorate(mocked_object) == 'real_function'\n\n            mocked_object.__sphinx_decorator_args__ = ()\n            assert undecorate(mocked_object) == mocked_object\n", "    def test_ismock(self):\n        with mock(['test_module']):\n            assert mock.ismock(test_module)\n            assert not mock.ismock(MyClass1)\n            assert not mock.ismock(10)\n", "    def test_mock_object():\n        \"\"\"Test _MockObject functionality.\"\"\"\n        with mock(['mocked_module']):\n            from mocked_module import MockedClass\n            assert ismock(MockedClass)\n            assert MockedClass.__name__ == 'MockedClass'\n            assert MockedClass.__module__ == 'mocked_module'\n            assert MockedClass.__display_name__ == 'mocked_module.MockedClass'\n            assert isinstance(MockedClass(), _MockObject)\n\n            instance = MockedClass()\n            assert instance.something is None\n\n\n\n", "    def test_mock_module(self):\n        with mock(['target']):\n            from target.module import SomeClass\n            assert ismock(SomeClass)\n            assert SomeClass.__name__ == 'SomeClass'\n            assert SomeClass.__module__ == 'target.module'\n", "    def test_mock(self, app) -> None:\n        with mock(['target.module.name']):\n            from target.module.name import MyFunction\n            assert ismock(MyFunction)\n            assert undecorate(MyFunction) is MyFunction\n"], "sample_409": ["    def test_blocktranslate_with_variable(self):\n        template = Template(\n            \"{% load i18n %}{% blocktranslate with foo='bar' %}{{ foo }}{% endblocktranslate %}\"\n        )\n        expected = trans_real(\"bar\")\n        with translation.override(\"en\"):\n            self.assertEqual(template.render(Context({})), expected)\n\n", "    def test_blocktranslate_variable_in_context(self):\n        template = \"\"\"", "    def test_blocktranslate_empty(self):\n        template = Template(\"{% load i18n %}{% blocktranslate %}  {% endblocktranslate %}\")\n        rendered = template.render(Context({}))\n        self.assertEqual(rendered, \"\")\n", "    def test_blocktranslate_simple(self):\n        rendered = self.template.render(Context({'anton': 'Anton'}))\n        self.assertEqual(rendered, 'Anton')\n", "    def test_blocktranslate_var_as_var(self):\n        template = Template(\n            \"{% load i18n %}{% blocktranslate with foo='Bar' asvar var %}This is {{ foo }}.{% endblocktranslate %}{{ var }}\"\n        )\n        output = template.render(Context({}))\n        self.assertEqual(output, \"This is Bar.This is Bar.\")\n"], "sample_537": ["    def test_stride_windows_1d(self):\n        x = np.arange(20)\n        NFFT = 8\n        noverlap = 4\n        result = mlab._stride_windows(x, NFFT, noverlap)\n        target = self.calc_window_target(x, NFFT, noverlap)\n        assert_array_equal(result, target)\n\n", "    def test_stride_windows_2d(self):\n        x = np.arange(20).reshape(5, 4)\n        NFFT = 3\n        noverlap = 1\n        target = self.calc_window_target(x.ravel(), NFFT, noverlap)\n        result = mlab._stride_windows(x, NFFT, noverlap)\n        assert_array_almost_equal_nulp(result.reshape(*target.shape), target)\n", "    def test_stride_windows_shape(self):\n        x = np.arange(10)\n        NFFT = 5\n        noverlap = 2\n        res = mlab._stride_windows(x, NFFT, noverlap)\n        assert res.shape == (5, 4)\n", "    def test_stride_windows_multidimensional(self):\n        x = np.arange(200).reshape(10, 20)\n        nfft = 8\n        noverlap = 4\n        out = mlab.stride_windows(x, nfft, noverlap)\n        target = self.calc_window_target(x.ravel(), nfft, noverlap)\n        assert_array_almost_equal_nulp(out.ravel(), target.ravel())\n\n", "    def test_stride_windows_empty(self):\n        x = np.array([])\n        NFFT = 10\n        noverlap = 5\n        expected = np.array([])\n        result = mlab._stride_windows(x, NFFT, noverlap)\n        assert_array_equal(result, expected)\n"], "sample_660": ["    def test_escaped_characters_in_test_name(self, testdir):\n        testdir.create_file(\"test_foo.py\", \"\"\"\n            def test_a_test with &<>\\\"' characters():\n                pass\n        \"\"\")\n        result, dom = runandparse(testdir)\n        testcase = dom.find_first_by_tag(\"testcase\")\n        assert testcase[\"classname\"] == \"test_foo\"\n        assert testcase[\"name\"] == \"a_test with &lt;&gt;&quot;' characters\"\n", "    def test_junitxml_output_with_properties(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n                pytest.record_property(\"prop1\", \"value1\")\n                assert True\n        \"\"\"\n        )\n        result, dom = runandparse(testdir)\n\n        testcase = dom.find_first_by_tag(\"testcase\")\n        properties = testcase.find_by_tag(\"properties\")\n        assert len(properties) == 1\n        property_node = properties[0].find_first_by_tag(\"property\")\n        assert property_node.assert_attr(name=\"prop1\", value=\"value1\")\n", "    def test_junitxml_properties(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"name, value\", [\n                (\"prop1\", \"value1\"),\n                (\"prop2\", 42),\n            ])\n                record_property(name, value)\n        \"\"\"\n        )\n        result, dom = runandparse(testdir)\n        assert result.ret == 0\n        tc = dom.find_first_by_tag(\"testcase\")\n        properties = tc.find_by_tag(\"property\")\n        assert len(properties) == 2\n        assert properties[0].text == \"value1\"\n        assert properties[1].text == \"42\"\n        assert properties[0][\"name\"] == \"prop1\"\n        assert properties[1][\"name\"] == \"prop2\"\n", "    def test_junitxml_double_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                assert 0\n                pytest.fail(\"double fail\")\n        \"\"\"\n        )\n        result, dom = runandparse(testdir)\n        assert result.ret == 1\n\n        test = dom.find_first_by_tag(\"testcase\")\n        assert test[\"classname\"] == \"test\"\n        assert test[\"name\"] == \"test_func\"\n        assert test.find_by_tag(\"failure\")\n\n        # There should be a separate error inside the testcase for the teardown failure\n        assert test.find_by_tag(\"error\")\n\n\n\n", "    def test_record_property(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x, expected\", [\n                (\"one\", 1),\n                (\"two\", 2),\n            ])\n                record_property(\"my_key\", x)\n                assert int(x) == expected\n        \"\"\"\n        )\n        result, dom = runandparse(testdir)\n        tc = dom.find_first_by_tag(\"testcase\")\n        assert tc[\n            \"classname\"\n        ] == \"test_record_property\"  # class name taken from the test function\n        assert tc[\"name\"] == \"test_foo[one]\"\n        properties = tc.find_by_tag(\"properties\")[0]\n        assert properties.children[0].text == \"one\"\n"], "sample_1132": ["    def test_multiset_partitions_empty():\n        assert list(multiset_partitions([], 3)) == []\n\n", "    def test_permute_signs():\n\n        assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2),\n                                                (0, 1, -2), (0, -1, -2)]\n        assert list(permute_signs((1, 0, -1))) == [(1, 0, -1), (-1, 0, 1),\n                                                (1, 0, 1), (-1, 0, -1)]\n        assert list(permute_signs((1,))) == [(1,)]\n        assert all(\n            all(isinstance(i, Basic) for i in s) for s in\n            permute_signs([1, x + 1, -2]))\n\n", "def test_minlex():\n    assert minlex([1, 2, 0]) == (0, 1, 2)\n    assert minlex([1, 0, 2]) == (0, 2, 1)\n    assert minlex([1, 0, 2], directed=False) == (0, 1, 2)\n    assert minlex('11010011000', directed=True) == '00011010011'\n    assert minlex('11010011000', directed=False) == '00011001011'\n", "    def test_generate_oriented_forest():\n        for n in range(1, 6):\n            for forest in generate_oriented_forest(n):\n                assert len(forest) == n\n                for i, p in enumerate(forest):\n                    if p != i:\n                        assert forest[p] == i\n                        assert (p < i)\n\n\n\n", "def test_is_palindromic():\n    assert is_palindromic(\"\") is True\n    assert is_palindromic(\"a\") is True\n    assert is_palindromic(\"aa\") is True\n    assert is_palindromic(\"aba\") is True\n    assert is_palindromic(\"abba\") is True\n    assert is_palindromic(\"racecar\") is True\n    assert is_palindromic(\"A man, a plan, a canal: Panama\") is True\n    assert is_palindromic(\"Was it a car or a cat I saw?\") is True\n    assert is_palindromic(\"No 'x' in Nixon\") is True\n    assert is_palindromic([1, 2, 3, 2, 1]) is True\n    assert is_palindromic([1, 2, 3, 4, 5]) is False\n"], "sample_24": ["    def test_shape(self):\n        self.check(np.shape)\n        self.check(np.shape, axis=0)\n", "    def test(self, func=np.shape):\n        self.check(func)\n", "    def test_unique(self):\n        self.check(np.unique)\n\n", "    def test_shape(self):\n        self.check(np.shape)\n", "    def test_shape(self):\n        self.check(np.shape,\n"], "sample_271": ["    def test_zipfile_module_in_sys_path(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_file, 'w') as zf:\n            zf.writestr('test_module.py', '')\n        sys.path.append(str(zip_file))\n        self.addCleanup(sys.path.pop)\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_file)\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / \"test.zip\"\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('test_module.py', 'print(\"Hello from zip file!\")')\n            sys.path.append(tmpdir)\n            self.addCleanup(sys.path.pop)\n\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(zip_path)\n\n            # Ensure that changing the zip file triggers a reload\n            with open(zip_path, 'ab') as f:\n                f.write(b'\\nprint(\"Hello again!\")')\n\n            self.assertFileFound(zip_path)\n\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n               zf.writestr('module.py', 'print(\"Hello from module\")')\n            sys.path.append(tempdir)\n            self.import_and_cleanup('module')\n            self.assertFileFound(zip_filename)\n            sys.path.remove(tempdir)\n\n", "    def test_zipfile_modules(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = os.path.join(tempdir, 'my_module.zip')\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/my_package/__init__.py', '')\n                zf.writestr('my_module/my_package/module.py', 'a = 1')\n\n            sys.path.append(tempdir)\n            self.import_and_cleanup('my_module.my_package.module')\n            self.assertFileFound(Path(zip_filename) / 'my_module/my_package/module.py')\n\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        # Create a zip file with a module\n        zip_filename = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_filename, 'w') as zipf:\n            zipf.writestr('test_module.py', 'print(\"Hello from zip file!\")')\n        self.addCleanup(lambda: zip_filename.unlink())\n\n        # Install the zip file as an importable module\n        sys.path.append(str(zip_filename.parent))\n        self.import_and_cleanup('test_module')\n\n        self.assertFileFound(zip_filename)\n"], "sample_910": ["    def test_is_suppressed_warning(self, app):\n        app.config.suppress_warnings = ['sphinx.ext.autodoc.type']\n        assert logging.is_suppressed_warning('type', 'object', app.config.suppress_warnings)\n        assert logging.is_suppressed_warning('sphinx.ext.autodoc.type', 'object', app.config.suppress_warnings)\n        assert not logging.is_suppressed_warning('sphinx.ext.autodoc.type', 'module', app.config.suppress_warnings)\n        assert not logging.is_suppressed_warning('sphinx.ext.autodoc.something', 'object', app.config.suppress_warnings)\n        assert not logging.is_suppressed_warning('reST', 'unknown', app.config.suppress_warnings)\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('app', None, []) is False\n    assert is_suppressed_warning('app', 'sub', []) is False\n    assert is_suppressed_warning('app', 'sub', ['app']) is True\n    assert is_suppressed_warning('app', 'sub', ['app.sub']) is True\n    assert is_suppressed_warning('app', 'sub', ['app.*']) is True\n    assert is_suppressed_warning('app', 'sub', ['another.warning']) is False\n    assert is_suppressed_warning('app', None, ['app.sub']) is True\n    assert is_suppressed_warning('app', 'sub', ['app1.sub']) is False\n", "def test_is_suppressed_warning(app):\n    app.config.suppress_warnings = ['sphinx.config', 'sphinx.builders.dirhtml.DirHTMLBuilder.']\n    assert is_suppressed_warning('sphinx.config', None, app.config.suppress_warnings)\n    assert is_suppressed_warning('sphinx.builders.dirhtml.DirHTMLBuilder', None, app.config.suppress_warnings)\n    assert is_suppressed_warning('sphinx.builders.dirhtml.DirHTMLBuilder', 'build_all', app.config.suppress_warnings)\n    assert not is_suppressed_warning('sphinx.something', None, app.config.suppress_warnings)\n    assert not is_suppressed_warning('sphinx.builders.dirhtml.DirHTMLBuilder', 'something', app.config.suppress_warnings)\n\n", "    def test_is_suppressed_warning(self):\n        suppress_warnings = ['sphinx.ext.autodoc.LoggerWarning',\n                            'sphinx.ext.autodoc.LoggerWarning.missing',\n                            'sphinx.ext.autodoc.*.deprecated',]\n        assert is_suppressed_warning('sphinx.ext.autodoc.LoggerWarning',\n                                    None, suppress_warnings)\n        assert is_suppressed_warning('sphinx.ext.autodoc.LoggerWarning',\n                                    'missing', suppress_warnings)\n        assert is_suppressed_warning('sphinx.ext.autodoc.LoggerWarning',\n                                    'deprecated', suppress_warnings)\n        assert not is_suppressed_warning('sphinx.ext.autodoc.SomeWarning',\n                                        None, suppress_warnings)\n", "def test_prefixed_warnings(app):\n    # check prefixing warning message\n    with prefixed_warnings('prefix:'):\n        with app.app.warningiserror(False):\n            app.warn('Warning message!')\n\n    out, err = app.flush_warnings()\n    assert 'prefix: Warning message!' in err\n\n\n\n"], "sample_809": ["    def test_mi_sparse_continuous_target():\n        rng = check_random_state(0)\n        X = csr_matrix(rng.rand(10, 5))\n        y = rng.rand(10)\n        mi = mutual_info_regression(X, y)\n        assert_equal(mi.shape[0], 5)\n", "    def test_mutual_info_classif_sparse_discrete_features(self):\n        rng = check_random_state(0)\n        n_samples = 100\n        n_features = 5\n        X = csr_matrix(rng.randint(0, 2, size=(n_samples, n_features)))\n        y = rng.randint(0, 2, size=(n_samples,))\n\n        mi = mutual_info_classif(X, y, discrete_features=True)\n        assert_equal(mi.shape[0], n_features)\n        assert_greater(np.sum(mi), 0)\n\n", "def test_mutual_info_sparse_regression():\n    random_state = check_random_state(0)\n    n_samples = 100\n    n_features = 5\n    X = csr_matrix(random_state.randn(n_samples, n_features))\n    y = random_state.randn(n_samples)\n    mi = mutual_info_regression(X, y, discrete_features=True)\n    assert_equal(mi.shape[0], n_features)\n    assert_greater(np.sum(mi), 0) \n", "    def test_mutual_info_sparse_data(self):\n        rng = check_random_state(0)\n        n_samples = 20\n        n_features = 10\n        X = csr_matrix(rng.rand(n_samples, n_features))\n        y = rng.rand(n_samples)\n\n        # Test mutual_info_regression with sparse data\n        mi_regression = mutual_info_regression(X, y)\n        assert_equal(mi_regression.dtype, np.float64)\n        assert_equal(mi_regression.shape[0], n_features)\n\n        # Test mutual_info_classif with sparse data\n        y_discrete = rng.randint(0, 2, size=n_samples)\n        mi_classif = mutual_info_classif(X, y_discrete)\n        assert_equal(mi_classif.dtype, np.float64)\n        assert_equal(mi_classif.shape[0], n_features)\n", "    def test_mutual_info_classif_sparse_discrete_features(self):\n        random_state = check_random_state(0)\n        X = csr_matrix(random_state.randint(0, 2, size=(10, 5)))\n        y = random_state.randint(0, 2, size=10)\n\n        mi = mutual_info_classif(X, y, discrete_features=True)\n\n        # Check that the MI values are not all zeros\n        assert_greater(np.sum(mi), 0)\n"], "sample_1015": ["    def test_ccode_piecewise_no_default(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter('error', SymPyDeprecationWarning)\n            expr = Piecewise((x, x > 0), (y, x < 0))\n            raises(ValueError, lambda: ccode(expr))\n\n", "    def test_elementwise_add_assign(self):\n        x = symbols('x')\n        i = symbols('i', integer=True)\n        A = IndexedBase('A', shape=(3,))\n        B = IndexedBase('B', shape=(3,))\n        code = ccode(\n            aug_assign(\n                A[i],\n                AddAugmentedAssignment,\n                B[i]\n            ),\n            assign_to=None,\n            standard='C99'\n        )\n        expected = \"\"\"\n        A[i] += B[i];\n        \"\"\".strip()\n        self.assertEqual(code, expected)\n\n", "    def test_ccode_function_prototype(self):\n        f = Function('f')\n        proto = FunctionPrototype(f(x, y), return_type=real)\n        code = ccode(proto, standard='C99')\n        assert code == 'real f(real x, real y);'\n", "    def test_ccode_math_macros():", "    def test_ccode_looping_indexedbase(self):\n        from sympy import Eq, IndexedBase, Idx\n        len_y = 5\n        y = IndexedBase('y', shape=(len_y,))\n        t = IndexedBase('t', shape=(len_y,))\n        Dy = IndexedBase('Dy', shape=(len_y-1,))\n        i = Idx('i', len_y-1)\n        e=Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n        code = ccode(e, assign_to=e.lhs, contract=True, standard='C89')\n        expected = \"\"\""], "sample_175": ["    def test_cascade_delete_with_select_related(self):\n        \"\"\"\n        Test that cascading deletes work correctly even when using select_related.\n        \"\"\"\n        b1 = B1.objects.create()\n        b2 = B2.objects.create(b1=b1)\n        b3 = B3.objects.create(b2=b2)\n\n        # Select related B1 through a chain of B2 and B3\n        with connection.CursorWrapper() as cursor:\n            cursor.execute(\"SELECT * FROM django_tests_b3 WHERE id = %s\", [b3.id])\n            row = cursor.fetchone()\n            self.assertEqual(row[1], b3.id)\n        b1_id = row[2]\n\n        # Delete B3, which should cascade through B2 and delete B1\n        b3.delete()\n\n        with connection.CursorWrapper() as cursor:\n            cursor.execute(\"SELECT * FROM django_tests_b1 WHERE id = %s\", [b1_id])\n            row = cursor.fetchone()\n            self.assertIsNone(row)\n", "    def test_fast_delete_with_many_to_many(self):\n        # Create a bunch of objects for testing fast deletes.\n\n        num_objects = GET_ITERATOR_CHUNK_SIZE * 2\n        objects = [M2MFrom(name=\"obj%s\" % i) for i in range(num_objects)]\n        M2MFrom.objects.bulk_create(objects)\n\n        # Connect each object to a single M2MTo object.\n        m2m_to = M2MTo.objects.create(name=\"to_obj\")\n        for obj in objects:\n            obj.m2m_to.add(m2m_to)\n\n        collector = Collector(using=self.DEFAULT)\n\n        # Verify that the m2m relationship doesn't\n        # prevent fast deletion.\n        self.assertTrue(collector.can_fast_delete(objects))\n\n        with connection.cursor() as cursor:\n            collector.collect(objects)\n            # Ensure all objects are deleted.\n\n            collector.delete()\n            self.assertEqual(M2MFrom.objects.count(), 0)\n            cursor.execute(\"SELECT COUNT(*) FROM {app_label}_{model_name}\".format(\n                app_label=M2MFrom._meta.app_label,\n                model_name=M2MFrom._meta.model_name,\n            ))\n            self.assertEqual(cursor.fetchone()[0], 0)\n", "    def test_cascade_through_proxy_model(self):\n        p1 = Parent.objects.create()\n        c1 = Child.objects.create(parent=p1)\n        rcc1 = RChildChild.objects.create(parent=c1)\n\n        with self.assertNumQueries(3):\n            Collector(using='default').collect([rcc1])\n\n        self.assertFalse(Parent.objects.exists())\n        self.assertFalse(Child.objects.exists())\n        self.assertFalse(RChildChild.objects.exists())\n", "    def test_cascade_with_null_fk_and_deferrable_checks(self):\n        # Test cascading deletion with a nullable foreign key on a database that\n        # supports deferred constraint checks.\n\n        r = R.objects.create(name='r')\n        s1 = S.objects.create(name='s1', r=r)\n        s2 = S.objects.create(name='s2', r=r)\n        t1 = T.objects.create(name='t1', s=s1)\n        t2 = T.objects.create(name='t2', s=s1)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET CONSTRAINT ALL DEFERRED\")\n        r.delete()\n        self.assertEqual(S.objects.count(), 2)\n        self.assertEqual(T.objects.count(), 2)\n        with connection.cursor() as cursor:\n            cursor.execute(\"SET CONSTRAINT ALL IMMEDIATE\")\n        self.assertEqual(S.objects.count(), 0)\n        self.assertEqual(T.objects.count(), 0)\n\n", "    def test_cascade_parent_model_keeps_parents(self):\n        # Create a parent model\n        parent = Parent.objects.create(name='Parent')\n        # Create a child model with a CASCADE deletion on_delete\n        child = Child.objects.create(parent=parent, name='Child')\n        # Create a model that references Child with DO_NOTHING deletion\n        p = P.objects.create(child=child)\n\n        # Delete the child model\n        with self.assertRaises(ProtectedError):\n            Collector(self.using).collect([child], source=Child, keep_parents=True)\n\n        # Make sure the parent model is still there\n        self.assertEqual(Parent.objects.count(), 1)\n"], "sample_402": ["    def test_should_redirect_with_slash_valid_url(self):\n        request = self.rf.get('/test/')\n        middleware = CommonMiddleware()\n        self.assertFalse(middleware.should_redirect_with_slash(request))\n", "    def test_response_redirect_with_slash_post(self):\n        request = self.rf.post('/test/')\n        middleware = CommonMiddleware()\n        response = middleware.process_response(request, HttpResponse())\n        self.assertIsNone(response)\n", "    def test_should_redirect_with_slash_valid_url(self):\n        request = self.rf.get('/test/')\n        middleware = CommonMiddleware()\n        self.assertFalse(middleware.should_redirect_with_slash(request))\n", "    def test_should_redirect_with_slash(self):\n        request = self.rf.get('/foo')\n        request.urlconf = 'middleware.urls'\n\n        with self.settings(APPEND_SLASH=True):\n            self.assertFalse(\n                CommonMiddleware().should_redirect_with_slash(request)\n            )\n\n        with self.settings(APPEND_SLASH=True, DEBUG=False):\n            common_middleware = CommonMiddleware()\n            self.assertTrue(common_middleware.should_redirect_with_slash(request))\n\n        with self.settings(APPEND_SLASH=True, DEBUG=True):\n            common_middleware = CommonMiddleware()\n            with self.assertRaises(RuntimeError):\n                common_middleware.get_full_path_with_slash(request)\n\n", "    def test_should_redirect_with_slash_when_append_slash_is_true_and_appending_a_slash_makes_a_valid_path(self):\n        request = self.rf.get('/test')\n        with self.settings(APPEND_SLASH=True, URLCONF='middleware.urls'):\n            middleware = CommonMiddleware()\n            self.assertTrue(middleware.should_redirect_with_slash(request))\n"], "sample_108": ["    def test_reverse_with_converter(self):\n        for url, (url_name, app_name, kwargs) in converter_test_data:\n            with self.subTest(url=url):\n                self.assertEqual(reverse(url_name, kwargs=kwargs, app_name=app_name), url)\n", "    def test_reverse_with_converter(self):\n        url_name = 'converter_test'\n        for url, expected_result in converter_test_data:\n            with self.subTest(url=url):\n                resolved_url = self.client.urlresolvers.reverse(url_name, kwargs={'base64': url.split('/')[2]})\n                self.assertEqual(resolved_url, url)\n\n", "    def test_resolve_with_default_kwargs(self):\n        resolved = resolve('/base64/aGVsbG8=/')\n        self.assertEqual(resolved.func, empty_view)\n        self.assertEqual(resolved.kwargs, {'value': b'hello'})\n", "    def test_include_with_kwargs(self):\n        response = self.client.get('/include/', follow=True)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content.decode(), 'Included')\n", "    def test_reverse_with_dynamic_converter(self):\n        resolver = get_resolver(None)\n        converter = DynamicConverter()\n        resolver._reverse_dict = {\n            'base64': [\n                (r'^base64/(?P<value>[a-zA-Z0-9+/]+)/$', r'^base64/(?P<value>[a-zA-Z0-9+/]+)/$', {}, {'value': converter}),\n            ],\n        }\n        self.assertEqual(\n            resolver._reverse_with_prefix('base64', '', value='hello'),\n            '/base64/aGVsbG8=/',\n        )\n"], "sample_1135": ["def test_mul_simplification():\n    # Test simplification of Mul objects\n    assert Mul(2, x, 3) == 6*x\n    assert Mul(2, x, 3).simplify() == 6*x\n    assert Mul(2, x, 3, evaluate=False) == 2*x*3\n    assert Mul(2, x, 3, evaluate=False).simplify() == 6*x\n    assert (x*y).simplify() == x*y\n    assert (x*y).expand() == x*y\n\n", "def test_expand_2arg():\n    assert expand_2arg(x*(y + 1)) == x*y + x\n    assert expand_2arg(2*(x + y)) == 2*x + 2*y\n    assert expand_2arg(Rational(1, 2)*(x + y)) == Rational(1, 2)*x + Rational(1, 2)*y\n", "    def test_expand_2arg():\n        assert expand_2arg(2*(x + y)) == 2*x + 2*y\n        assert expand_2arg((x + y)*z) == x*z + y*z\n        assert expand_2arg(2*(x + y)*z) == 2*x*z + 2*y*z\n        assert expand_2arg(2*x*(x + y)) == 2*x**2 + 2*x*y\n", "def test_expand_2arg_complex():\n    e = (x + 2*I)*(y + 3*I)\n    f = expand_2arg(e)\n    assert f.args == [x*y + 3*x*I + 2*I*y - 6]\n\n\n\n", "    def test_mul_expand_complex():\n        # test that complex multiplication is handled correctly\n        assert Mul(2+3*I, 4-5*I).expand() == (2 + 3*I)*(4 - 5*I)\n        assert complex(Mul(2+3*I, 4-5*I).expand()) == \\\n            complex(2 + 3*I)*(4 - 5*I)\n"], "sample_1186": ["def test_array_diff_indexed():\n    a = ImmutableDenseNDimArray([x, y, 1, 2], (2, 2))\n    assert a.diff(x) == ImmutableDenseNDimArray([1, 0, 0, 0], (2, 2))\n", "    def test_array_diff_symbolic(self):\n        M = ImmutableDenseNDimArray([[x, y], [1, x*y]])\n        res = M.diff(x)\n        assert res == ImmutableDenseNDimArray([[1, 0], [0, y]])\n", "def test_array_iter():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4], (2, 2))\n    i = 0\n    for ele in a:\n        assert ele == a[i]\n        i += 1\n", "    def test_array_diff(self):\n        A = ImmutableDenseNDimArray([x, y, 1], (3, 1))\n        dAdx = A.diff(x)\n        assert (dAdx[0] == 1).simplify()\n        assert (dAdx[1] == 0).simplify()\n        assert (dAdx[2] == 0).simplify()\n", "    def test_array_diff_against_scalar(self):\n        a = ImmutableDenseNDimArray([x, 2*x, 3*x], (3,))\n        b = a.diff(x)\n        assert b.tolist() == [1, 2, 3]\n        b = a.diff(x, 2)\n        assert b.tolist() == [0, 0, 0]\n\n"], "sample_1189": ["    def test_lambdify_tensorflow():\n        if not tensorflow:\n            skip(\"Tensorflow not installed: skipping test\")\n        from tensorflow import config\n        config.experimental.set_visible_devices([], 'GPU')\n\n        x = tf.Variable(2.0, dtype=tf.float32)\n        f = sympy.exp(x)\n        lam_f = lambdify(x, f, 'tensorflow')\n        assert lam_f(x).numpy() == math.exp(2.0)\n", "def test_lambdify_tensorflow_matrix():\n    if not tensorflow:\n        skip(\"Tensorflow not installed!\")\n    A = MatrixSymbol('A', 2, 2)\n    expr = A**2\n    f = lambdify(A, expr, 'tensorflow')\n    a = tensorflow.constant([[1, 2], [3, 4]], dtype=tensorflow.float32)\n    assert (f(a).numpy() == numpy.dot(a, a)).all()\n", "def test_lambdify_piecewise():\n    x = sympy.Symbol('x')\n    f = Piecewise((0, x < -1), (x**2, -1 <= x <= 1), (1, x > 1))\n    g = lambdify(x, f, 'numpy')\n    assert g(-2) == 0\n    assert g(0) == 0\n    assert g(2) == 1\n\n", "def test_lambdify_matrix_symbol():\n    A = MatrixSymbol('A', 2, 2)\n    expr = A*A\n    f = lambdify((A,), expr)\n    m = numpy.array([[1, 2], [3, 4]])\n    assert (f(m) == m @ m).all()\n", "def test_lambdify_tensorflow_sparse():\n    if not tensorflow:\n        skip(\"Tensorflow is not installed.\")\n    from sympy.tensor.array import SparseMatrix\n    A = SparseMatrix([1, 2, 3], [4, 5, 6])\n    f = lambdify(x, A*x, 'tensorflow')\n    assert f(tensorflow.constant(2.0)).numpy().tolist() == [2, 4, 6]\n"], "sample_309": ["    def test_urlencode_multivaluedict(self):\n        qs = MultiValueDict({'a': ['1', '2'], 'b': '3'})\n        self.assertEqual(urlencode(qs), 'a=1&a=2&b=3')\n        qs = MultiValueDict({'a': [None, '2'], 'b': '3'})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(qs)\n", "    def test_urlencode_multivaluedict(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(urlencode(query), 'a=1&a=2&b=3')\n\n", "    def test_urlencode_multivaluedict(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': '3'})\n        self.assertEqual(urlencode(query), 'a=1&a=2&b=3')\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(urlencode(data), 'a=1&a=2&b=3')\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(urlencode(data), 'a=1&a=2&b=3')\n"], "sample_1045": ["def test_im_inf_neg_inf():\n    assert (I*S.Infinity) == S.Infinity * I\n    assert (I*S.NegativeInfinity) == S.NegativeInfinity * I\n\n", "    def test_algebraic_number_evalf_rational():\n        a = AlgebraicNumber(sqrt(2))\n        assert a.evalf() == a.evalf(2)\n", "    def test_AlgebraicNumber_evalf():\n        a = AlgebraicNumber((1, 2), [1, 0, 1])\n        assert a.evalf() == a\n        assert a.evalf(2) == a\n        assert a.evalf(5) == a\n", "def test_Mod():\n    assert Mod(9, 4) == 1\n    assert Mod(9, 5) == 4\n    assert Mod(-9, 5) == 1\n    assert Mod(9, -5) == 4\n    assert Mod(S.Infinity, 5) == S.NaN\n    assert Mod(S.NegativeInfinity, 5) == S.NaN\n\n", "def test_NumberSymbol_hashing():\n    assert hash(S.Pi) == hash(S.Pi)\n    assert hash(S.Pi) != hash(S.E)\n    assert hash(S.Infinity) == hash(S.Infinity)\n"], "sample_629": ["    def test_expand_modules_ignore_patterns(\n        self, tmpdir: Path", "    def test_expand_modules_with_ignore_list_re(\n        self, tmp_path: Path", "    def test_expand_modules_relative_path(self):\n        modules = [\".\"]\n        ignore_list = []\n        ignore_list_re = []\n        ignore_list_paths_re = []\n        expected = [\n            {\n                \"path\": str(Path(\"./__init__.py\").resolve()),\n                \"name\": \"__init__\",\n                \"isarg\": True,\n                \"basepath\": str(Path(\"./__init__.py\").resolve()),\n                \"basename\": \"__init__\",\n            }\n        ]\n        result, errors = expand_modules(\n            modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        assert result == expected\n        assert not errors\n", "    def test_expand_modules_ignore_list_paths_re(self):\n        files_or_modules = [\"moduleA\", \"moduleB/\"]\n        ignore_list = []\n        ignore_list_re = [re.compile(r\".*moduleB.*\")]\n        ignore_list_paths_re = [re.compile(r\".*moduleA.*\")]\n        result, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n        assert len(result) == 1\n        assert result[0][\"name\"] == \"moduleA\"\n", "    def test_expand_modules_ignore_list_re(\n        self, tmpdir: Path"], "sample_677": ["    def test_nested_not(self) -> None:\n        assert evaluate(\"not (not ident1 and ident2)\", lambda s: s == \"ident1\") is True\n", "    def test_not_with_parentheses():\n            return x == \"test\"\n\n        assert evaluate(\"(not test)\", matcher) is False\n        assert evaluate(\"not(test)\", matcher) is False\n        assert evaluate(\"(not (test))\", matcher) is False\n", "    def test_not_expression(self) -> None:\n            return x == \"hello\"\n\n        assert evaluate(\"not hello\", matcher) is False\n        assert evaluate(\"not world\", matcher) is True\n", "    def test_not_expr_parentheses():\n        assert evaluate(\"(not a)\", lambda x: x == \"a\") is False\n        assert evaluate(\"(not (not a))\", lambda x: x == \"a\") is True\n", "def test_match_nested_parentheses():\n        return ident == \"nested\"\n\n    assert evaluate(\"((nested))\", matcher) is True\n"], "sample_323": ["    def test_migrate_unapplied_migrations_with_dependencies(self):\n        # Create a migration with a dependency on a migration from another app.\n        self.create_migration(\n            \"migrations2\",\n            \"0003_dependent_migration\",\n            operations=[migrations.CreateModel(name=\"DependentModel\", fields=[\n                (\"id\", models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n            ])],\n            dependencies=[(\"migrations\", \"0002_second_migration\")],\n        )\n\n\n        # Mock the database connection to return an error when creating the\n        # DependentModel table. This simulates a scenario where the migration\n        # failed to apply due to a database error.\n\n        with mock.patch.object(connection, 'cursor', return_value=mock.MagicMock(spec=connection.cursor, exec_sql=mock.MagicMock(side_effect=DatabaseError) )), \\\n             mock.patch('django.db.migrations.executor.MigrationExecutor.unapply_migration') as mock_unapply_migration:\n            executor = MigrationExecutor(self.connection)\n            with self.assertRaises(DatabaseError):\n                executor.migrate(targets=[('migrations2', '0003_dependent_migration')])\n            # Verify that the unapply_migration method was called\n            mock_unapply_migration.assert_called_once_with(mock.ANY, mock.ANY, fake=False)\n", "    def test_migrate_forwards_with_replacements(self):\n        \"\"\"\n        Test migrating forwards when replacements are involved.\n        \"\"\"\n\n        # Set up a project state with the initial migrations applied.\n        with connection.cursor() as cursor:\n            self.connection.create_test_db()\n            with self.override_settings(MIGRATION_MODULES={\"migrations\": \"migrations.test_migrations\"}):\n                (self.loader.project_state((\n                    'migrations', '0001_initial'),\n                    at_end=True\n                ))\n        self.assertEqual(self.loader.graph.leaf_nodes(), [\n            ('migrations', '0003_auto'),\n        ])\n\n        # Create a migration executor.\n        executor = MigrationExecutor(connection)\n\n        # Migrate forwards with the replacement migration.\n        state = executor.migrate(targets=[('migrations', '0003_auto')])\n\n        # Verify that the replacement migration is recorded as applied.\n        self.assertIn(('migrations', '0002_replacetarget'), self.recorder.applied_migrations())\n\n        # Verify that the state after the migration is what we expect.\n        self.assertEqual(state.apps.get_model('migrations', 'Blog').objects.count(), 1)\n", "    def test_migrate_partial_backwards(self):\n        with connection.cursor() as cursor:\n            cursor.execute('CREATE TABLE \"migrations2_person\" (\"id\" INTEGER PRIMARY KEY, \"name\" VARCHAR(255))')\n        # Set up a database state mimicking the end-state after applying\n        # migrations up to the '0003_add_name' migration in the 'migrations2' app\n        self.database.reset()\n        self.apply_migration('migrations2', '0003_add_name')\n        recorder = MigrationRecorder(connection)\n        recorder.ensure_schema()\n        recorder.record_applied('migrations2', '0003_add_name')\n\n        # Now try unapplying only the '0003_add_name' migration.\n        executor = MigrationExecutor(connection)\n        target = ('migrations2', '0002_person')\n        plan = executor.migration_plan([target])\n        state = executor.migrate(target, plan=plan, fake=True)\n\n        # Check that the table still exists after the partial backwards migration.\n        self.assertTableExists('migrations2_person')\n\n", "    def test_migrate_backwards_to_initial(self):\n        \"\"\"\n        Tests that migrating backwards to the initial migration works correctly.\n        \"\"\"\n        # Create a test database\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE migrations2_testmodel (id SERIAL PRIMARY KEY, name VARCHAR(255))\")\n        self.connection.close()\n\n        # Apply migrations\n        executor = MigrationExecutor(self.connection)\n        executor.migrate([\"migrations2\"], plan=executor.migration_plan([\"migrations2\", (None, None)]))\n\n        # Now migrate backwards to initial\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.check_replacements') as mock_check_replacements:\n            executor.migrate([\"migrations2\"], plan=executor.migration_plan([\"migrations2\", (\"migrations2\", \"0001_initial\")]))\n\n        # Check that the table is gone\n        with self.connection.cursor() as cursor:\n            with self.assertRaises(DatabaseError):\n                cursor.execute(\"SELECT * FROM migrations2_testmodel\")\n\n        # Make sure check_replacements was called\n        mock_check_replacements.assert_called_once()\n\n", "    def test_unapply_replacement_migration(self):\n        \"\"\"\n        Test that unapplying a replacement migration correctly reverts all\n        replaced migrations.\n        \"\"\"\n\n        # Create a new migration that replaces two existing migrations.\n        with self.createConnection():\n            self.migrate('migrations2', '0001_initial')\n            with mock.patch('django.db.migrations.executor.MigrationExecutor.recorder') as mock_recorder:\n                mock_recorder.record_unapplied.side_effect = lambda app_label, name: None\n                self.migrate('migrations2', '0002_migration_with_replacement')\n            self.assertMigrationHistory([], ['migrations2', '0001_initial', 'migrations2', '0002_migration_with_replacement'])\n\n        # Unapply the replacement migration, which should also unapply the replaced migrations.\n        with self.createConnection():\n            self.migrate('migrations2', '0002_migration_with_replacement', fake=True)\n            self.migrate('migrations2', '0002_migration_with_replacement', reverse=True, fake=True)\n\n        # Check that the replaced migrations are no longer recorded as applied.\n        self.assertMigrationHistory([])\n\n        # Check that the replacement migration is also recorded as unapplied.\n        self.assertMigrationHistory([], ['migrations2', '0001_initial'])\n"], "sample_597": ["    def test_merge_explicit_coords(self):\n        data = create_test_data()\n        ds1 = xr.Dataset({\"foo\": ([\"x\"], [1, 2])}, coords={\"x\": [0, 1]})\n        ds2 = xr.Dataset({\"bar\": ([\"x\"], [3, 4])}, coords={\"x\": [0, 1]})\n        expected = xr.Dataset({\"foo\": ([\"x\"], [1, 2]), \"bar\": ([\"x\"], [3, 4])}, coords={\"x\": [0, 1]})\n        actual = merge([ds1, ds2], explicit_coords=[\"x\"])\n\n        assert_identical(actual, expected)\n\n", "    def test_merge_override(self) -> None:\n        ds1 = create_test_data()\n        ds2 = create_test_data(seed=42)\n        ds2[\"var1\"] = ds2[\"var1\"] * 2\n\n        expected = merge([ds1, ds2], combine_attrs=\"override\")\n        actual = merge([ds1, ds2], combine_attrs=\"override\")\n\n        assert_identical(actual, expected)\n", "    def test_merge_explicit_coords(self):\n        a = xr.Dataset({'foo': (('x', 'y'), np.arange(6).reshape(2, 3))})\n        b = xr.Dataset({'bar': (('x', 'y'), np.arange(6).reshape(2, 3))})\n        c = xr.Dataset({'baz': ([1, 2], np.arange(2))}, coords={'x': [1, 2]})\n\n        result = xr.merge(\n            [a, b, c],\n            explicit_coords=['x'],\n        )\n        expected = xr.Dataset(\n            {\n                'foo': (('x', 'y'), np.arange(6).reshape(2, 3)),\n                'bar': (('x', 'y'), np.arange(6).reshape(2, 3)),\n                'baz': ([1, 2], np.arange(2)),\n            },\n            coords={'x': [1, 2]},\n        )\n        assert_identical(result, expected)\n", "    def test_merge_priority_arg(self):\n        ds1 = xr.Dataset({'a': ('x', [1, 2])})\n        ds2 = xr.Dataset({'a': ('x', [3, 4]), 'b': ('x', [5, 6])})\n        expected = xr.Dataset({'a': ('x', [3, 4]), 'b': ('x', [5, 6])})\n        actual = merge([ds1, ds2], priority_arg=1)\n        assert_identical(actual, expected)\n", "    def test_merge_explicit_coords(self) -> None:\n        ds1 = xr.Dataset({'a': (['x'], [1, 2]), 'b': (['x'], [3, 4])},\n                         coords={'x': [0, 1]}, attrs={'foo': 'bar'})\n        ds2 = xr.Dataset({'c': (['x'], [5, 6])},\n                         coords={'x': [0, 1], 'y': [7, 8]})\n\n        # Explicitly include 'x' as a coordinate in the merging process\n        merged = xr.merge([ds1, ds2], explicit_coords=['x'])\n        expected = xr.Dataset({'a': (['x'], [1, 2]),\n                               'b': (['x'], [3, 4]),\n                               'c': (['x'], [5, 6])},\n                              coords={'x': [0, 1], 'y': [7, 8]},\n                              attrs={'foo': 'bar'})\n        assert_identical(merged, expected)\n\n        # Attempting to merge 'y' as a coordinate should fail\n        with pytest.raises(MergeError):\n            xr.merge([ds1, ds2], explicit_coords=['y'])\n"], "sample_553": ["def test_null_movie(anim):\n    writer = NullMovieWriter()\n    writer.setup(anim.fig, 'temp.mp4', 100)\n    anim.save('temp.mp4', writer=writer)\n    assert writer._count == anim.frames\n\n\n\n", "def test_save_count_with_generator(anim):\n    anim.save_count = 3\n    assert len(anim._save_seq) == 0\n    anim._iter_gen = lambda: iter(range(5))\n    anim._draw_frame(0)\n    anim._draw_frame(1)\n    anim._draw_frame(2)\n    assert len(anim._save_seq) == 3\n    anim._draw_frame(3)\n    assert len(anim._save_seq) == 3\n", "    def test_grab_frame_no_args(anim):\n        anim.save('test.mp4', writer='null')\n        assert anim.writer.args == ()\n        assert anim.writer._count == 5\n", "    def test_grab_frame_kwargs_validation(anim):\n        writer = NullMovieWriter()\n        anim.save('test.mp4', writer=writer)\n        assert writer._count == anim.frames\n        assert 'bbox_inches' not in writer.savefig_kwargs\n\n", "def test_grab_frame_and_finish_kwargs(anim):\n    fig, _ = anim.fig.get_axes()\n    writer = animation.writers['ffmpeg']()\n    writer.setup(fig=fig, outfile='output.mp4', dpi=100)\n    anim._init_draw()\n    writer.grab_frame()\n    writer.finish()\n    assert writer._count == 1\n"], "sample_415": ["    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(name='Foo', price=10)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name='FOO', price=10)\n\n            # Test case-insensitive uniqueness with expressions\n            UniqueConstraintProduct.objects.create(name='bar', price=10)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name='BAR', price=10)\n\n\n", "    def test_unique_constraint_with_expressions_and_condition(self):\n        with atomic():\n            # Test with expressions and condition\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product A\", price=100, quantity=10\n                )\n            try:\n                with self.assertRaises(ValidationError) as context:\n                    UniqueConstraintProduct.objects.create(\n                        name=\"Product B\", price=100, quantity=10\n                    )\n            except IntegrityError:\n                pass\n            self.assertIn(\n                \"UniqueConstraintProduct with this name and this price already exists.\",\n                str(context.exception),\n            )\n\n", "    def test_unique_constraint_with_expressions_and_condition(self):\n        with connection.creation.create_test_db(\n            autocommit=False,\n        ):\n            with atomic():\n                model = UniqueConstraintProduct\n\n                constraint = model._meta.get_constraints()[0]\n                self.assertEqual(\n                    len(constraint._get_index_expressions(model, connection.schema_editor)),\n                    2,\n                )\n                with self.assertRaises(ValidationError):\n                    model.objects.create(\n                        name=\"Test product\",\n                        price=100,\n                        description=\"Description\",\n                        product_category=\"Electronics\",\n                    )\n", "    def test_unique_constraint_include(self):\n        with atomic():\n            UniqueConstraintInclude.objects.create(name=\"one\", value=1)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintInclude.objects.create(name=\"one\", value=1)\n\n            UniqueConstraintInclude.objects.create(name=\"two\", value=2)\n", "    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            product_1 = UniqueConstraintProduct.objects.create(\n                name=\"Product 1\", value=1\n            )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 2\", value=1\n                )\n"], "sample_160": ["    def test_format_large_decimal_with_scientific_notation(self):\n        self.assertEqual(\n            nformat(Decimal('1.2345678901234567890123456789e+50'), decimal_sep=',', decimal_pos=2, grouping=3,\n                    thousand_sep='.'),\n            '123.46e+50'\n        )\n", "    def test_format_grouping_sequence(self):\n        self.assertEqual(\n            nformat(1234567890, decimal_sep='.', decimal_pos=2, grouping=(3, 2, 0)),\n            '1.234.567.890.00'\n        )\n", "    def test_format_with_grouping_sequence(self):\n        self.assertEqual(nformat(1234567890.123, decimal_sep='.', decimal_pos=2, grouping=(3, 2, 0)), '1.234.567.890,12')\n", "    def test_format_decimal_with_very_large_exponent(self):\n        number = Decimal('1e+100')\n        formatted = nformat(\n            number, decimal_sep='.', decimal_pos=2, thousand_sep=',',\n            grouping=3,\n        )\n        self.assertEqual(formatted, '10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000.00')\n\n", "    def test_format_large_decimal_with_grouping(self):\n        self.assertEqual(\n            nformat(Decimal('12345678901234567890.123456789'), decimal_sep=',', decimal_pos=2, grouping=3, thousand_sep='.'),\n            '12.345.678.901.234.567.890,12',\n        )\n"], "sample_55": ["    def test_inline_edit(self):\n        response = self.client.get(reverse('admin:books_chapter_add'), follow=True)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Add Chapter')\n        form_data = {\n            'title': 'Chapter 4',\n            'content': '[ insert contents here ]',\n            'book': self.b1.pk,\n            'chapxtra1_set-TOTAL_FORMS': '1',\n            'chapxtra1_set-INITIAL_FORMS': '0',\n            'chapxtra1_set-MAX_NUM_FORMS': '0',\n            'chapxtra1_set-0-id': '',\n            'chapxtra1_set-0-chap': self.chap1.pk,\n            'chapxtra1_set-0-xtra': 'ChapterXtra1 4',\n\n        }\n        response = self.client.post(reverse('admin:books_chapter_add'), form_data, follow=True)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, reverse('admin:books_chapter_changelist'))\n", "    def test_change_list_with_filter_exclude_field(self):\n        # Check that a filter exclude field works correctly.\n        response = self.client.get(reverse('admin:books_book_changelist'))\n        self.assertContains(response, 'Promo 1')\n        self.assertContains(response, 'Promo 2')\n        response = self.client.get(reverse('admin:books_book_changelist') + '?promo__name__exact=Promo 1')\n        self.assertContains(response, 'Promo 1')\n        self.assertNotContains(response, 'Promo 2')\n", "    def test_add_view_with_inline(self):\n        response = self.client.get(reverse('admin:books_chapter_add'))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Add Chapter')\n        self.assertContains(response, 'Book')\n        self.assertContains(response, 'Title')\n        self.assertContains(response, 'Content')\n\n\n\n", "    def test_edit_inline_ordering(self):\n        response = self.client.get(reverse('admin:books_chapter_changelist'))\n        \n        self.assertContains(response, 'Chapter 1', count=2)\n\n        # Create a new Chapter\n        data = {\n            'title': 'Chapter 3',\n            'content': 'New chapter content',\n            'book': self.b1.pk,\n            'ORDERING': 1,\n        }\n        response = self.client.post(reverse('admin:books_chapter_add'), data)\n        self.assertRedirects(response, reverse('admin:books_chapter_changelist'))\n\n        response = self.client.get(reverse('admin:books_chapter_changelist'))\n        self.assertContains(response, 'Chapter 3', count=1)\n        self.assertEqual(response.context['cl'].queryset.order_by('ordering')[0].title, 'Chapter 3')\n        self.assertEqual(response.context['cl'].queryset.order_by('ordering')[1].title, 'Chapter 1')\n\n", "    def test_change_list_preserved_filters(self):\n        # Create a Book object\n        Book.objects.create(name='Book 3')\n        response = self.client.get('/admin/admin_views/book/')\n        self.assertContains(response, 'Book 1')\n        self.assertContains(response, 'Book 2')\n        self.assertContains(response, 'Book 3')\n\n        # Add a filter for 'name__icontains=Book'\n        response = self.client.get('/admin/admin_views/book/?name__icontains=Book')\n        self.assertContains(response, 'Book 1')\n        self.assertContains(response, 'Book 2')\n        self.assertContains(response, 'Book 3')\n        # Paginate to the next page\n        response = self.client.get(add_preserved_filters(response.build_absolute_uri(), {'page': 2}))\n        self.assertContains(response, 'Book 1')\n        self.assertContains(response, 'Book 2')\n        self.assertContains(response, 'Book 3')\n"], "sample_230": ["    def test_jsonfield_invalid_json(self):\n        form = JSONForm({'data': 'this is not json'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['data'], [ValidationError('Enter a valid JSON.')])\n", "    def test_invalid_json(self):\n        form = Form({'json_field': '{ \"key\": \"value\" \"name\": \"John\"}'})\n        with self.assertRaises(ValidationError):\n            form.is_valid()\n", "    def test_jsonfield_with_invalid_json(self):\n        class TestForm(Form):\n            data = JSONField()\n\n        form = TestForm({'data': 'not valid json'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['data'], [ValidationError('Enter a valid JSON.')])\n", "    def test_jsonfield_with_custom_encoder(self):\n        class MyEncoder(DjangoJSONEncoder):\n                if isinstance(obj, uuid.UUID):\n                    return str(obj)\n                return super().default(obj)\n        class MyForm(Form):\n            data = JSONField(encoder=MyEncoder)\n\n        form = MyForm({'data': json.dumps({'my_uuid': uuid.uuid4()})})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['data']['my_uuid'], str(uuid.uuid4()))\n", "    def test_jsonfield_encoding(self):\n        class MyForm(Form):\n            data = JSONField(encoder=DjangoJSONEncoder)\n\n        data = {'key': 'value', 'list': [1, 2, 3]}\n        form = MyForm({'data': json.dumps(data)})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['data'], data)\n"], "sample_539": ["def test_spanselector_onselect_arguments(ax):\n        assert verts == (0.5, 1.5)\n\n    span = widgets.SpanSelector(ax, onselect, 'horizontal', useblit=False)\n    click_and_drag(ax, (0.5, 0.5), (0.5, 1.5), button=1)\n", "def test_spanselector_rect(ax):\n        assert_allclose(vmin, [2, 3])\n        assert_allclose(vmax, [8, 9])\n    \n    span = widgets.SpanSelector(ax, onselect, direction='vertical',\n                                minspan=2, useblit=False, button=1,\n                                rectprops=dict(facecolor='lightblue'))\n    click_and_drag(ax, (2, 3), (8, 9), button=1)\n    assert span.rect is not None\n    assert span.rect.get_xy() == (2, 3)\n    assert span.rect.get_width() == 6\n", "    def test_spanselector_onselect(self, ax, kwargs):\n            self.vmin = vmin\n            self.vmax = vmax\n\n        span = widgets.SpanSelector(ax, onselect, **kwargs)\n        click_and_drag(ax, (0.2, 0.3), (0.8, 0.9))\n        assert_allclose(self.vmin, 0.2)\n        assert_allclose(self.vmax, 0.8)\n        span.disconnect_events()\n", "def test_spanselector_onselect(ax):\n        nonlocal called\n        called = True\n        assert xmin == 0.2\n        assert xmax == 0.8\n\n    called = False\n    span = widgets.SpanSelector(ax, onselect, 'horizontal',\n                                minspan=0.1, rectprops=dict(facecolor=None))\n\n    click_and_drag(span, (0.2, 0.5), (0.8, 0.5))\n    assert called\n", "def test_span_selector_onselect(ax):\n        assert xmin == 0\n        assert xmax == 1\n    span = widgets.SpanSelector(ax, onselect, 'horizontal',\n                                minspan=0.1, rectprops=dict(facecolor='red'))\n    click_and_drag(ax, (0, 0.5), (1, 0.5), span)\n"], "sample_394": ["    def test_ordering_of_inline_fields(self):\n        response = self.client.get(reverse('admin:admin_views.section_change', args=(self.s1.pk,)))\n        self.assertContentBefore(response, b'Article set', b'Article 1:', 'Article set should come before Article 1')\n", "    def test_changelist_search_form(self):\n        response = self.client.get(reverse('admin:articles_article_changelist'))\n        self.assertContains(response, '<input type=\"text\" name=\"q\"')\n        self.assertContains(response, '<input type=\"submit\" value=\"Search\">')\n", "    def test_change_list_filters_preserve_selected_filters_on_change(self):\n        response = self.client.get(reverse(\"admin:books_book_changelist\"))\n        self.assertEqual(response.status_code, 200)\n\n        # Select a filter and submit the form.\n        form_data = {\n            'title__icontains': 'Book',\n        }\n        response = self.client.post(\n            reverse(\"admin:books_book_changelist\"), data=form_data, follow=True\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('title__icontains=Book', response.request['GET']['preserved_filters'])\n\n        # Change the filter selection and submit again.\n        form_data = {\n            'title__icontains': 'NewBook',\n        }\n        response = self.client.post(\n            reverse(\"admin:books_book_changelist\"), data=form_data, follow=True\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('title__icontains=NewBook', response.request['GET']['preserved_filters'])\n", "    def test_can_delete_object(self):\n        response = self.client.get(reverse(\"admin:books_book_delete\", args=(self.b1.pk,)))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Book 1\")\n        self.assertContains(response, \"Are you sure you want to delete this book?\")\n        self.assertTrue(\"name\" in response.context[\"adminform\"].form.fields)\n        self.assertTrue(\"promo_set\" in response.context[\"adminform\"].form.fields)\n", "    def test_admin_site_requires_permission(self):\n        # Ensure that accessing the admin site without permission redirects to login\n        response = self.client.get(reverse('admin:index'))\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, '/admin/login/')\n\n        # Ensure logging out redirects to the login page\n        self.client.logout()\n        response = self.client.get(reverse('admin:index'))\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, '/admin/login/')\n\n"], "sample_1094": ["def test_basic_slicing():\n    x, y = symbols('x y')\n    e = x + y\n    assert e[0] == x\n    assert e[1] == y\n    raises(IndexError, lambda: e[2])\n    raises(IndexError, lambda: e[-1])\n", "def test_Basic_eq():\n    assert b1 != b2\n    assert b2 != b3\n    assert b2 == b21\n    assert b1 == Basic()\n\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Basic(x, y) != Basic(y, x)\n\n    assert Basic(*tuple(range(5))) == Basic(*tuple(range(5)))\n    assert Basic(*tuple(range(5))) != Basic(*tuple(range(6)))\n\n    assert Basic(1) != 1\n\n", "def test_basic_evalf():\n    x, y = symbols('x y')\n    assert b1.evalf() == b1\n    assert b2.evalf() == b2\n    assert (x+y).evalf() == x+y\n    assert sin(x).evalf() == sin(x)\n    assert (x + S.One).evalf() == x + 1\n\n\n", "def test_as_Basic():\n    assert as_Basic(1) == S.One\n    assert as_Basic([1, 2, 3]) == Tuple(1, 2, 3)\n    assert as_Basic(str(b1)) == Symbol(str(b1))\n    assert as_Basic(b1) == b1\n", "def test_has():\n    assert b1.has(b1)\n    assert b2.has(b1)\n    assert not b1.has(b2)\n    assert b3.has(b2)\n    assert b3.has(b1)\n    assert b21.has(b2)\n    assert b21.has(b1)\n    assert not b1.has(b3)\n"], "sample_287": ["    def test_check_middleware_settings(self):\n        admin_site = AdminSite()\n        admin_site.register(Album, ValidFields)\n        self.assertListEqual(admin_site.check(all_models=True), [])\n        admin_site.middleware = [\n            AuthenticationMiddlewareSubclass,\n            MessageMiddlewareSubclass,\n            SessionMiddlewareSubclass,\n        ]\n        self.assertListEqual(admin_site.check(all_models=True), [])\n\n", "    def test_auth_middleware_subclass(self):\n        class CustomAdminSite(AdminSite):\n            pass\n        site = CustomAdminSite()\n\n        admin_check = AdminChecks.check_middleware(site)\n        self.assertEqual(len(admin_check), 0)\n", "    def test_check_relation(self):\n        class MyInline(GenericStackedInline):\n            model = Song\n            fk_name = 'album'\n            parent_model = Album\n\n        errors = MyInline().check()\n        self.assertEqual(len(errors), 0)\n\n        class MyInline2(GenericStackedInline):\n            model = Song\n            fk_name = 'nonexistent_field'\n            parent_model = Album\n\n        errors = MyInline2().check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E202')\n", "    def test_check_middleware(self):\n        site = AdminSite()\n        site.register(Album, ValidFields)\n\n        # Middleware order should not affect check results\n        middleware_classes = [\n            AuthenticationMiddlewareSubclass,\n            MessageMiddlewareSubclass,\n            SessionMiddlewareSubclass,\n        ]\n        with self.settings(MIDDLEWARE=middleware_classes + ['django.middleware.common.CommonMiddleware']):\n            errors = site.check()\n            self.assertEqual(len(errors), 0)\n", "    def test_check_inline_formset(self):\n        class MyInlineFormset(admin.TabularInline):\n            model = Song\n            formset = forms.BaseModelFormSet\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [MyInlineFormset]\n\n        errors = MyAdmin().check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E206')\n\n"], "sample_970": ["    def test_stringify_signature(self):\n            pass\n\n        sig = inspect.signature(function)\n        assert inspect.stringify_signature(sig) == '(a: int, b: int, *args, c=4, **kwargs) -> str'\n        assert inspect.stringify_signature(sig, show_annotation=False) == '(a, b, *args, c=4, **kwargs)'\n        assert inspect.stringify_signature(sig, show_return_annotation=False) == '(a: int, b: int, *args, c=4, **kwargs)'\n\n            pass\n\n        sig2 = inspect.signature(function2)\n        assert inspect.stringify_signature(sig2, unqualified_typehints=True) == '(a: list, ) -> dict'\n\n\n", "    def test_stringify_signature_unqualified_typehints(self):\n        sig = inspect.signature(lambda x: x, globalns={'StringIO': types.StringType})\n        result = inspect.stringify_signature(sig, unqualified_typehints=True)\n        assert result == '(x: StringIO)'\n", "def test_stringify_annotation_typealias(app):\n    typealias = TypeAliasNamespace({'Annotated': 'typing.Annotated'})\n\n    annotation = typealias['Annotated']('[int, \"description\"]')\n    result = inspect.stringify_annotation(annotation, unqualified_typehints=False)\n    assert result == 'typing.Annotated[int, \"description\"]'\n\n    annotation = typealias['Annotated'](\n        inspect.TypeAliasForwardRef('typing.List[int]')\n    )\n    result = inspect.stringify_annotation(annotation, unqualified_typehints=False)\n    assert result == 'typing.Annotated[typing.List[int]]'\n\n", "    def test_stringify_signature_keyword_only_args(self):\n        sig = inspect.signature(\n            lambda a, b, *, c, d=None: None,\n        )\n        assert stringify_signature(sig) == '(a, b, *, c, d=None)'\n", "    def test_signature_from_ast_with_posonlyargs(self):\n        code = 'def func(a, /, b, c=10, *args, **kwargs): pass'\n        module = ast.parse(code)\n        function = cast(ast.FunctionDef, module.body[0])  # type: ignore\n        sig = inspect.signature_from_ast(function, code)\n        assert str(sig) == '(a, /, b, c=10, *args, **kwargs)'\n\n"], "sample_543": ["def test_polygonselector_draw_bounding_box(ax):\n    # Create a PolygonSelector with draw_bounding_box=True\n    polygon_selector = widgets.PolygonSelector(\n        ax, lambda verts: None, draw_bounding_box=True)\n\n    # Click to add a few vertices\n    click_and_drag(ax, (0.2, 0.2), (0.5, 0.5))\n    click_and_drag(ax, (0.5, 0.5), (0.8, 0.2))\n\n    # Assert that the bounding box is present\n    assert polygon_selector._box is not None\n\n    # Release the mouse button to complete the polygon\n    do_event(ax, 'button_release_event', x=0.8, y=0.2)\n\n    # Check that the polygon is complete\n    assert polygon_selector._selection_completed is True\n\n    # Check that the bounding box is still present\n    assert polygon_selector._box is not None\n\n    # Remove the selector\n    polygon_selector.disconnect_events()\n", "    def test_lasso_callback(ax):\n\n        x = np.linspace(0, 10, 50)\n        y = np.sin(x)\n\n        ax.plot(x, y)\n        points = [(1, 0.8), (2, 0.9), (3, 0.5), (4, 0.6)]\n        captured_verts = []\n\n            nonlocal captured_verts\n            captured_verts = verts\n\n        lasso = widgets.Lasso(ax, points[0], callback)\n        for p in points[1:]:\n            do_event(lasso.ax.figure.canvas, MockEvent('motion_notify_event', x=p[0], y=p[1]))\n        do_event(lasso.ax.figure.canvas, MockEvent('button_release_event', x=p[0], y=p[1]))\n\n        assert_allclose(captured_verts, points)\n\n", "def test_lasso_empty(fig_test, fig_ref):\n    ax = fig_test.add_subplot()\n    lasso = widgets.Lasso(ax, (0, 0), lambda verts: None)\n    lasso.onrelease(mock_event(x=5, y=5))\n    fig_test.canvas.draw()\n    fig_ref.canvas.draw()\n\n", "def test_lasso(ax):\n    # Create a Lasso widget with a dummy callback\n    callback = mock.Mock()\n    lasso = widgets.Lasso(ax, (0.5, 0.5), callback)\n\n    # Simulate a mouse click-and-drag event\n    click_and_drag(lasso, (0.2, 0.2), (0.8, 0.8))\n\n    # Assert that the callback was called with the correct vertices\n    # The call order should be: button press, motion, motion,... button release\n    assert callback.call_count == 1\n    verts = callback.call_args[0][0]\n    assert_allclose(verts, [(0.5, 0.5), (0.8, 0.8)])\n", "def test_lasso_callback(ax):\n    callback = mock.Mock()\n    lasso = widgets.Lasso(ax, (0, 0), callback)\n    click_and_drag(lasso, (0, 0), (1, 1))\n    lasso.onrelease(mock_event(ax, xdata=1, ydata=1))\n    assert callback.call_count == 1\n    verts = callback.call_args[0][0]\n    assert len(verts) == 3\n    assert_allclose(verts[0], (0, 0))\n"], "sample_161": ["    def test_many_to_many_field_db_table_uniqueness(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n                return self.name\n\n        with mock.patch('django.db.connections.databases', {'default': {'ENGINE': 'django.db.backends.sqlite3'}}), \\\n                override_settings(DATABASE_ROUTERS=[]), \\\n                self.assertRaises(checks.Error):\n            MyModel.objects.create()\n            MyModel.objects.create(name='Second')\n            class AnotherModel(models.Model):\n                mymodel = models.ManyToManyField(MyModel, through='JoinedModel')\n            class JoinedModel(models.Model):\n                mymodel = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n                anothermodel = models.ForeignKey(AnotherModel, on_delete=models.CASCADE)\n\n            AnotherModel._meta.installed = True\n\n            AnotherModel._meta.check_constraints()\n", "    def test_many_to_many_through_default_db_table_name(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n            books = models.ManyToManyField(Book, through='BookAuthor')\n\n        class BookAuthor(models.Model):\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            publication_date = models.DateField()\n            class Meta:\n                db_table = 'book_author'\n        with override_settings(DATABASE_ROUTERS=[\n            lambda model, instance: 'other' if isinstance(model, Book) else 'default'\n        ]):\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT * FROM book_author\")\n                tables = cursor.fetchall()\n          self.assertEqual(len(tables), 0) \n\n", "    def test_many_to_many_field_check_through_fields_invalid(self):\n        class InvalidModel(models.Model):\n            name = models.CharField(max_length=100)\n            friends = models.ManyToManyField('self', through='FriendRelation', through_fields='invalid_fields')\n\n        class FriendRelation(models.Model):\n            from_user = models.ForeignKey(InvalidModel, on_delete=models.CASCADE, related_name='from_friends')\n            to_user = models.ForeignKey(InvalidModel, on_delete=models.CASCADE, related_name='to_friends')\n\n        with self.assertRaisesMessage(ValueError, \"Field specifies 'through_fields' but does not provide the names of the two link fields that should be used for the relation\"):\n            InvalidModel()\n\n", "    def test_manytomanyfield_check_table_uniqueness_with_auto_created_model(self):\n        with override_settings(DATABASE_ROUTERS=[\n            lambda model, instance, **kwargs: 'router_a' if model == ManyToManyFieldAutoCreatedModel else None\n        ]):\n            class ManyToManyFieldAutoCreatedModel(models.Model):\n                pass\n\n            class AnotherModel(models.Model):\n                items = models.ManyToManyField(ManyToManyFieldAutoCreatedModel)\n\n            errors = AnotherModel._meta.get_field('items').check(from_model=AnotherModel)\n            self.assertListEqual(errors, [])\n\n", "    def test_many_to_many_through_model_with_custom_field_name(self):\n\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class MyIntermediate(models.Model):\n            mymodel = models.ForeignKey(MyModel, on_delete=models.CASCADE, related_name='mymodel_set')\n            othermodel = models.ForeignKey(models.Model, on_delete=models.CASCADE, related_name='othermodel_set')\n\n        class OtherModel(models.Model):\n            name = models.CharField(max_length=100)\n            models = models.ManyToManyField(MyModel, through=MyIntermediate, related_name='othermodels')\n\n        mmf = OtherModel._meta.get_field('models')\n        self.assertEqual(mmf.remote_field.through, MyIntermediate)\n        self.assertEqual(mmf.m2m_field_name(), 'mymodel')\n        self.assertEqual(mmf.m2m_reverse_field_name(), 'othermodel')\n"], "sample_183": ["    def test_case_when_expression_with_subquery(self):\n        result = list(CaseTestModel.objects.annotate(\n            case_result=Case(\n                When(Q(integer=2), then=Subquery(FKCaseTestModel.objects.filter(fk_id=OuterRef('id')).values('integer').annotate(max_int=Max('integer')))),\n                default_value=0,\n                output_field=IntegerField()\n            )\n        ).values('integer', 'case_result'))\n        \n        self.assertEqual(result[0]['integer'], 1)\n        self.assertEqual(result[0]['case_result'], 0)\n\n        self.assertEqual(result[1]['integer'], 2)\n        self.assertEqual(result[1]['case_result'], 3)\n", "    def test_case_with_when_and_then(self):\n        results = list(CaseTestModel.objects.annotate(\n            case_result=Case(\n                When(integer=1, then=Value('one')),\n                When(integer=2, then=Value('two')),\n                When(integer=3, then=Value('three')),\n                default=Value('other'),\n                output_field=CharField(),\n            )\n        ).values_list('case_result', flat=True))\n        self.assertEqual(results, ['one', 'two', 'three', 'two', 'three', 'three', 'other', 'other'])\n\n", "    def test_case_when_then_when_then(self):\n        results = CaseTestModel.objects.annotate(\n            case_value=Case(\n                When(integer=1, then=Value('one')),\n                When(integer=2, then=Value('two')),\n                default=Value('other'),\n                output_field=CharField(),\n            )\n        ).values_list('integer', 'case_value')\n        self.assertListEqual(\n            list(results),\n            [(1, 'one'), (2, 'two'), (3, 'other'), (4, 'other')]\n        )\n", "    def test_when_then_multiple_cases(self):\n        qs = CaseTestModel.objects.annotate(\n            case_when=Case(\n                When(integer=1, then=Value('one')),\n                When(integer=2, then=Value('two')),\n                When(integer=3, then=Value('three')),\n                default=Value('other')\n            )\n        )\n        self.assertEqual(\n            list(qs.values_list('integer', 'case_when')),\n            [(1, 'one'), (2, 'two'), (2, 'two'), (3, 'three'), (3, 'three'), (3, 'three'), (4, 'other')]\n        )\n\n", "    def test_case_when_q_object_lookups(self):\n        qs = CaseTestModel.objects.annotate(\n            case_when=Case(\n                When(Q(integer=1), then=Value('one')),\n                When(Q(integer=2), then=Value('two')),\n                default=Value('other'),\n                output_field=CharField(),\n            ),\n        )\n        self.assertQuerysetEqual(\n            qs.filter(case_when='one'),\n            [{'id': 1, 'integer': 1, 'integer2': 1, 'string': '1', 'case_when': 'one'}],\n        )\n        self.assertQuerysetEqual(\n            qs.filter(case_when='two'),\n            [\n                {'id': 2, 'integer': 2, 'integer2': 3, 'string': '2', 'case_when': 'two'},\n                {'id': 6, 'integer': 2, 'integer2': 2, 'string': '2', 'case_when': 'two'},\n            ],\n        )\n        self.assertQuerysetEqual(\n            qs.filter(case_when='other'),\n            [\n                {'id': 3, 'integer': 3, 'integer2': 4, 'string': '3', 'case_when': 'other'},\n                {'id': 7, 'integer': 3, 'integer2': 3, 'string': '3', 'case_when': 'other'},\n                {'id': 8, 'integer': 4, 'integer2': 5, 'string': '4', 'case_when': 'other'},\n            ],\n        )\n\n"], "sample_560": ["    def test_legend_loc_str(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend(loc=loc)\n        # Can't assert exact locations due to font size variations\n        # but we can check that they are in the right quadrant\n        bbox = ax.legend().get_window_extent()\n        if 'lower' in loc:\n            assert bbox.y0 < 0.5\n        elif 'upper' in loc:\n            assert bbox.y0 > 0.5\n\n", "    def test_legend_bestloc(self, loc, expected_bbox):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend(loc=loc)\n        fig.canvas.draw()\n        bbox = ax.get_legend().get_window_extent()\n        assert_allclose(bbox.transformed(fig.dpi_scale_trans.inverted()).bounds,\n                        eval(expected_bbox))\n", "def test_legend_with_proxy_artist():\n    fig, ax = plt.subplots()\n    # Create a patch without adding to the axes\n    patch = mpatches.Rectangle((0, 0), 1, 1, fc=\"r\")\n    # Create a legend handle using the ProxyArtist\n    proxy_artist = mlegend.Legend.proxy_artist(patch)\n\n    ax.add_patch(patch)\n\n    # Make a legend\n    ax.legend(handles=[proxy_artist], labels=[\"My Patch\"])\n    # Check if the proxy artist is in the legend handles\n\n    assert proxy_artist in ax.get_legend().get_legend_handles()[0]\n\n", "    def test_legend_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n\n        # Create a legend with a specific bbox_to_anchor\n        ax.plot([1, 2, 3], [4, 5, 6], label='Line 1')\n        ax.legend(bbox_to_anchor=(0.1, 0.5))\n\n        #  Also test the deprecated 'loc' argument\n        ax.plot([1, 2, 3], [7, 8, 9], label='Line 2', loc='upper left')\n\n", "    def test_legend_bbox_to_anchor(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend(bbox_to_anchor=(1, 1))\n        plt.tight_layout()\n\n\n"], "sample_173": ["    def test_explain_query_prefix_raises_not_supported_error(self):\n        with self.assertRaises(NotSupportedError) as cm:\n            self.ops.explain_query_prefix(format='text')\n        self.assertEqual(str(cm.exception), 'This backend does not support explaining query execution.')\n", "    def test_combine_duration_expression(self):\n        expression = self.ops.combine_duration_expression(' + ', ['duration_field', 'interval \\'1 day\\''])\n        self.assertEqual(expression, 'duration_field + interval \\'1 day\\'')\n", "    def test_window_frame_range_start_end(self):\n        with self.assertRaises(NotSupportedError) as cm:\n            self.ops.window_frame_range_start_end(start=-1, end=1)\n        self.assertIn(\n            'only supports UNBOUNDED together with PRECEDING and FOLLOWING',\n            str(cm.exception)\n        )\n\n", "    def test_integer_field_range(self):\n        self.assertEqual(self.ops.integer_field_range('SmallIntegerField'), (-32768, 32767))\n        self.assertEqual(self.ops.integer_field_range('IntegerField'), (-2147483648, 2147483647))\n\n\n", "    def test_date_extract_sql_microseconds_not_supported(self):\n        with self.assertRaises(NotImplementedError):\n            self.ops.date_extract_sql('microsecond', 'some_field')\n\n"], "sample_993": ["def test_power_of():\n    assert (x*y).power_of(x*y) == True\n    assert (x**2*y**2*x**-1).power_of(x*y) == False\n    assert (x**-3*y**-1*x**5).power_of(x**-3*y*x**3) == True\n    assert (x**-3*y*x**3).power_of(x**-3*y**-1*x**5) == True\n\n    assert ((x*y)**2).power_of(x*y) == True\n    assert (x**-3*y**-2*x**3).power_of(x**-3*y*x**3) == True\n    assert (x**3*y*x**-3).power_of(x*y) == True\n\n", "def test_cyclic_conjugates():\n    assert len(x*y*x*y*x.cyclic_conjugates()) == 5\n    assert (x*y*x**2*y).cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "    def test_free_group_cyclic_conjugates():\n        w = x*y*x*y*x\n        assert w.cyclic_conjugates() == {x*y*x**2*y, x**2*y*x*y, y*x*y*x**2, y*x**2*y*x, x*y*x*y*x}\n        s = x*y*x**2*y*x\n        assert s.cyclic_conjugates() == {x**2*y*x**2*y, y*x**2*y*x**2, x*y*x**2*y*x}\n", "    def test_cyclic_conjugates():\n        assert len(x.cyclic_conjugates()) == 1\n        assert len((x*y).cyclic_conjugates()) == 2\n        assert len((x*y*x*y*x).cyclic_conjugates()) == 5\n        assert len((x*y*x**2*y*x).cyclic_conjugates()) == 3\n        assert len((x*y**2*x**3*y**2*x).cyclic_conjugates()) == 6\n\n\n", "def test_exponent_sum():\n    assert x.exponent_sum(x) == 1\n    assert x.exponent_sum(x**-1) == -1\n    assert (x**2*y**3).exponent_sum(x) == 2\n    assert (x**2*y**3).exponent_sum(y) == 3\n    assert (x**2*y**3).exponent_sum(z) == 0\n"], "sample_814": ["def test_sparse_matrix_classification(self):\n    # Check that the classifier works with sparse matrices\n    X = csr_matrix(iris.data)\n    y = iris.target\n    est = GradientBoostingClassifier(random_state=0)\n    est.fit(X, y)\n    score = est.score(X, y)\n    assert score > 0.9\n\n", "    def test_classification_sparse_input(self):\n        # Test classification on sparse input\n        clf = GradientBoostingClassifier(random_state=0)\n        X_sparse = csr_matrix(iris.data)\n        y = iris.target\n\n        clf.fit(X_sparse, y)\n        score = clf.score(X_sparse, y)\n        assert score > 0.9\n\n        # check if predict_proba outputs valid probabilities\n        y_proba = clf.predict_proba(X_sparse)\n        assert_almost_equal(y_proba.sum(axis=1), np.ones(len(y)), decimal=6)\n", "    def test_early_stopping_regression(self):\n        # Regression test with early stopping\n        X, y = make_regression(n_samples=100, random_state=0)\n        reg = GradientBoostingRegressor(n_estimators=100,\n                                     validation_fraction=0.2, n_iter_no_change=5,\n                                       tol=0.01, random_state=0)\n        reg.fit(X, y)\n        assert reg.n_estimators_ < 100\n\n", "    def test_predict_proba(self):\n        clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\n        clf.fit(iris.data, iris.target)\n\n        proba = clf.predict_proba(iris.data)\n        assert proba.shape == (iris.data.shape[0], 3)\n        assert_almost_equal(proba.sum(axis=1), np.ones(iris.data.shape[0]))\n", "    def test_classification_dataset(self, presort):\n        # Check classification on a real dataset.\n        clf = GradientBoostingClassifier(n_estimators=100,\n                                         random_state=1, presort=presort,\n                                         learning_rate=0.1)\n        clf.fit(iris.data, iris.target)\n        score = clf.score(iris.data, iris.target)\n        assert score > 0.9\n\n"], "sample_72": ["    def test_serialize_regex_with_explicit_flags(self):\n        regex = re.compile(r'^[a-z]+$', re.ASCII)\n        serialized, imports = serializer_factory(regex).serialize()\n        self.assertEqual(serialized, \"re.compile('^[a-z]+$', re.ASCII)\")\n        self.assertEqual(imports, {'import re'})\n", "    def test_serialize_lazyobject(self):\n        settings_ref = SettingsReference('DATABASE_ROUTERS')\n        lazy_settings_ref = SimpleLazyObject(lambda: settings_ref)\n        serialized = serializer_factory(lazy_settings_ref).serialize()[0]\n        self.assertEqual(serialized, 'settings.DATABASE_ROUTERS')\n\n\n\n", "    def test_serialize_lazy_translated_string(self):\n        lazy_string = SimpleLazyObject(lambda: _('Example'))\n        self.assertEqual(\n            Serializer.serialize(lazy_string),\n            (\"gettext_lazy('Example')\", {'from django.utils.translation import gettext_lazy'}),\n        )\n", "    def test_serialize_custom_operation(self):\n        operation = custom_migration_operations.operations.CustomOperation(\n            name='custom_op',\n            argument='value',\n        )\n        writer = OperationWriter(operation)\n        serialized = writer.serialize()\n        expected = (\n            'custom_migration_operations.operations.CustomOperation(name=%s,'\n            ' argument=%s)' % (repr('custom_op'), repr('value'))\n        )\n        self.assertEqual(serialized[0], expected)\n", "    def test_serialize_lazy_object(self):\n        lazy_object = SimpleLazyObject(lambda: 'hello')\n        serializer = Serializer(lazy_object)\n        string, imports = serializer.serialize()\n\n        self.assertEqual(string, \"'hello'\")\n        self.assertEqual(imports, set())\n"], "sample_1133": ["    def test_transverse_magnification():\n        assert transverse_magnification(30, 15) == -2\n        assert transverse_magnification(10, 20) == -0.5\n        assert transverse_magnification(20, 10) == 2\n", "def test_deviation_with_normal():\n    n1 = 1.33\n    n2 = 1\n    angle_of_incidence = 0.5\n    normal = Matrix([0, 0, 1])\n\n    # Test deviation with a normal vector\n    deviation_result = deviation(\n        angle_of_incidence, n1, n2, normal=normal\n    )\n    assert ae(deviation_result, -asin(sin(angle_of_incidence)/n2), 5)\n", "    def test_deviation():\n        n1, n2 = symbols('n1, n2')\n        assert ae(deviation(0.2, n1, n2),\n                    acos(sqrt(1 - (n1/n2)**2*sin(0.2)**2)) - 0.2, 5)\n        r1 = Ray3D(Point3D(-1, -1, 1), Point3D(0, 0, 0))\n        n = Matrix([0, 0, 1])\n        P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n        assert isinstance(deviation(r1, 1, 1, n), float)\n        assert isinstance(deviation(r1, 1, 1, plane=P), float)\n        raises(ValueError, lambda: deviation(r1, 1.2, 1.5, plane=P, normal=n))\n", "    def test_deviation_ray3d():\n        n1, n2 = symbols('n1 n2')\n        r = Ray3D(Point3D(0, 0, 1), Point3D(0, 0, 0))\n        n = Matrix([0, 0, 1])\n        P = Plane(Point3D(0, 0, 0), normal_vector=[0, 0, 1])\n        assert ae(deviation(r, n1, n2, plane=P), -acos(-sqrt(-2*n1**2/(3*n2**2) + 1)) + acos(-sqrt(3)/3), 7)\n", "def test_lens_formula():\n    f = 10\n    u = 20\n    v = lens_formula(focal_length=f, u=u)\n    assert ae(v, 20, 5)\n"], "sample_1063": ["def test_lambdify_with_tensorflow_function():\n    from sympy.utilities.lambdify import _TensorflowEvaluatorPrinter\n\n    # Create a simple TensorFlow function\n    import tensorflow as tf\n\n    @tf.function\n        return x**2\n\n    # Create a SymPy expression\n    expr = x**2\n\n    # Lambdify the expression using TensorFlow backend\n    f = lambdify(x, expr, 'tensorflow')\n\n    # Assert that the function uses the TensorFlow evaluator\n    assert isinstance(f._evaluator, _TensorflowEvaluatorPrinter)\n\n    # Test the function\n    assert f(tf.constant(3.0)) == 9.0\n", "    def test_lambdify_tensorflow_constant():\n        if not tensorflow:\n            skip(\"Tensorflow is not installed.\")\n\n        x = tensorflow.Variable(2.0)\n        f = lambdify(x, x**2, 'tensorflow')\n        assert f(x).numpy() == 4\n", "    def test_lambdify_matrix():\n        A = MatrixSymbol('A', 2, 2)\n        x = Matrix([1, 2])\n        expr = A @ x\n        f = lambdify((A, x), expr)\n        numpy_A = numpy.array([[1, 2], [3, 4]])\n        numpy_x = numpy.array([1, 2])\n        assert (f(numpy_A, numpy_x) == numpy.dot(numpy_A, numpy_x)).all()\n", "    def test_lambdify_tensorflow_nested_function():\n        # Test lambdify with TensorFlow and nested functions\n        if not tensorflow:\n            skip(\"TensorFlow not installed\")\n\n        x = sympy.Symbol('x')\n        f = sympy.sin(x)\n        g = sympy.cos(f)\n        f_lam = lambdify(x, g, 'tensorflow')\n        result = f_lam(tensorflow.constant(0))\n        assert result.numpy() == math.cos(math.sin(0))\n\n", "def test_lambdify_indexed_symbol():\n    i = symbols('i', integer=True)\n    f = lambdify((z, i), z**i)\n\n    assert [f(2, 0), f(2, 1), f(2, 2)] == [1, 2, 4]\n    assert [f(3, 0), f(3, 1), f(3, 2)] == [1, 3, 9]\n\n\n\n"], "sample_266": ["    def test_migration_loader_check_key(self):\n        # Test cases for check_key method of MigrationLoader\n        # Case 1: __first__ dependency on a migrated app\n        with override_settings(MIGRATION_MODULES={'migrations_tests': 'tests.migrations_tests.migrations'}), connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE tests_migrations_tests_model (id INT PRIMARY KEY)\")\n            loader = MigrationLoader(connection, ignore_no_migrations=True)\n            loader.build_graph()\n            key = ('migrations_tests', '__first__')\n            self.assertEqual(loader.check_key(key, 'migrations_tests'), ('migrations_tests', '0001_initial'))\n\n        # Case 2: __first__ dependency on an unmigrated app\n        with override_settings(MIGRATION_MODULES={'migrations_tests': 'tests.migrations_tests.migrations'}), connection.cursor() as cursor:\n            loader = MigrationLoader(connection, ignore_no_migrations=True)\n            loader.build_graph()\n            key = ('migrations_tests_other', '__first__')\n            self.assertIsNone(loader.check_key(key, 'migrations_tests'))\n\n        # Case 3: __latest__ dependency on a migrated app\n        with override_settings(MIGRATION_MODULES={'migrations_tests': 'tests.migrations_tests.migrations'}), connection.cursor() as cursor:\n            loader = MigrationLoader(connection, ignore_no_migrations=True)\n            loader.build_graph()\n            key = ('migrations_tests', '__latest__')\n            self.assertEqual(loader.check_key(key, 'migrations_tests'), ('migrations_tests', '0002_alter_model'))\n\n        # Case 4: Dependency on a nonexistent app\n        with override_settings(MIGRATION_MODULES={'migrations_tests': 'tests.migrations_tests.migrations'}), connection.cursor() as cursor:\n            loader = MigrationLoader(connection, ignore_no_migrations=True)\n            loader.build_graph()\n            key = ('nonexistent_app', 'some_migration')\n            ", "    def test_migrations_module_customization(self):\n        \"\"\"\n        Tests loading migrations from a custom module\n        specified in settings.MIGRATION_MODULES\n        \"\"\"\n        with connection.cursor() as cursor:\n            cursor.execute(\"DROP TABLE IF EXISTS testapp_custommodel\")\n        self.migrate_all_apps(['testapp'], 'testapp.migrations_custom')\n        self.assertMigrationHistory(['testapp', '0001_initial'])  \n\n", "    def test_migrations_module_explicit(self):\n\n        # Create some migrations\n        self.migrate_test_data()\n\n        # Check that we can load from custom path\n        loader = MigrationLoader()\n        loader.build_graph()\n        self.assertEqual(len(loader.graph.nodes), 3)\n        self.assertIn(('testapp', '0001_initial'), loader.graph.nodes)\n\n        # Test with an invalid path\n        with self.assertRaises(ImportError):\n            loader = MigrationLoader(\n                connection=connections['other'],\n                ignore_no_migrations=True,\n            )\n            loader.build_graph()\n\n", "    def test_replacements_applied(self):\n        \"\"\"\n        Tests loading migrations with replacements where all replaced\n        migrations are applied.\n        \"\"\"\n        # Setup\n        with connections['default'].cursor() as cursor:\n            cursor.execute(\"DELETE FROM django_migrations\")\n        with connections['other'].cursor() as cursor:\n            cursor.execute(\"DELETE FROM django_migrations\")\n\n        # Apply the initial migration\n        self.migrate(\n            'testapp',\n            '0001_initial',\n            connections['default'],\n        )\n        self.migrate(\n            'testapp',\n            '0001_initial',\n            connections['other'],\n        )\n\n        # Apply the replacing migration\n        self.migrate(\n            'testapp',\n            '0002_replaced_migration',\n            connections['default'],\n        )\n        self.migrate(\n            'testapp',\n            '0002_replaced_migration',\n            connections['other'],\n        )\n        # Check applied status\n        loader = MigrationLoader(connections['default'], ignore_no_migrations=True)\n        applied_migrations = MigrationRecorder(connections['default']).applied_migrations()\n\n        self.assertEqual(\n            applied_migrations,\n            {('testapp', '0002_replaced_migration')},\n        )\n        loader = MigrationLoader(connections['other'], ignore_no_migrations=True)\n        applied_migrations = MigrationRecorder(connections['other']).applied_migrations()\n\n        self.assertEqual(\n            applied_migrations,\n            {('testapp', '0002_replaced_migration')},\n        )\n\n", "    def test_migrations_module(self):\n        loader = MigrationLoader(None)\n        app_name = 'test_migrations'\n        expected_module = 'another_app.migrations'\n        expected_explicit = True\n        actual_module, actual_explicit = loader.migrations_module(app_name)\n        self.assertEqual(actual_module, expected_module)\n        self.assertEqual(actual_explicit, expected_explicit)\n\n"], "sample_1003": ["def test_Options__init__defaults():\n    opts1 = Options((x, y), defaults={'expand': True})\n    assert opts1.expand is True\n\n    opts2 = Options((x, y), {'expand': False}, defaults={'expand': True})\n    assert opts2.expand is False\n\n    opts3 = Options((x, y), {'expand': False, 'order': lex}, defaults={'expand': True, 'order': lex})\n    assert opts3.expand is False\n    assert opts3.order == lex\n\n", "def test_domain_with_gens():\n    raises(GeneratorsError, lambda: Options((x, y), {'domain': ZZ.poly_ring(x)}))\n    o = Options((x, y), {'domain': ZZ.poly_ring(y)})\n    assert o['domain'] == ZZ.poly_ring(y)\n", "    def test_option_preprocess_domain_ZZ(self):\n        opt = Options((x, y), {'domain': 'ZZ'})\n        assert opt['domain'] == ZZ\n", "def test_build_options():\n    opt = Options((x, y), {'domain': ZZ})\n    assert opt['gens'] == (x, y)\n    assert opt['domain'] == ZZ\n\n    opt = build_options((x, y), {'domain': ZZ, 'auto': True})\n    assert opt['gens'] == (x, y)\n    assert opt['domain'] == ZZ\n    assert opt['auto'] is True\n\n", "def test_Options__init__():\n    with raises(OptionError):\n        Options((x, y, z), {'gens': x})\n\n    opts = Options((x, y, z), {'gens': (x, y)})\n    assert opts.gens == (x, y)\n\n    opts = Options((x, y), {'gens': (x, y), 'order': 'grlex'})\n    assert opts.order == lex\n\n    opts = Options((x, y, z), {'domain': ZZ})\n    assert opts.domain == ZZ\n\n    opts = Options(\n        (x, y, z), {'domain': 'ZZ', 'order': 'grlex', 'expand': False})\n\n    assert opts.domain == ZZ\n    assert opts.order == lex\n    assert opts.expand is False\n\n    opts = Options((x, y, z), {'domain': 'EX'})\n    assert opts.domain == EX\n\n    with raises(GeneratorsError):\n        Options((x, y, z), {'domain': ZZ, 'gens': (x, y, z, x)})\n\n"], "sample_180": ["    def test_index_together_with_expression(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField()\n\n            class Meta:\n                index_together = [\n                    (Lower('name'), 'value'),\n                ]\n\n        with self.assertRaises(checks.Error):\n            MyModel.check(databases=['default'])\n\n", "    def test_check_long_column_names_with_truncation(self):\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': ':memory:', 'TRUNCATE_COLUMN_NAMES': True}}):\n            # Create a model with a field name longer than the max length\n            class LongNameModel(models.Model):\n                long_field_name = models.CharField(max_length=100)\n            # Verify that no errors are raised\n            errors = LongNameModel._meta.check(databases=['default'])\n            self.assertEqual(len(errors), 0)\n\n", "    def test_check_index_together_invalid_field_types(self):\n        errors = _check_index_together(\n            ModelB,\n            databases=['default']\n        )\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn('related field', errors[0].msg)\n\n", "    def test_check_long_column_names_with_truncation(self):\n        max_len, db_alias = get_max_column_name_length()\n        if max_len is None:\n            self.skipTest(\"No database supports non-truncating column names\")\n\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}), self.assertRaises(Error):\n            class OverlongName(models.Model):\n                name = models.CharField(max_length=max_len + 1)\n\n", "    def test_unique_together_error(self):\n        class TestModel(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n            class Meta:\n                unique_together = [('field1', 'field2'), ('field1', 'field2')]\n\n        errors = TestModel.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'models.E011')\n"], "sample_999": ["def test_latex_SingularityFunction():\n    s = SingularityFunction(x, 0)\n    assert latex(s) == '\\\\operatorname{Sing}(x,0)'\n", "def test_latex_transpose():\n    M = Matrix([[1, 2], [3, 4]])\n    assert latex(M.transpose()) == r\"\\begin{pmatrix}1 & 3 \\\\ 2 & 4 \\end{pmatrix}\"\n\n", "    def test_latex_MatrixSymbol(self):\n        A = MatrixSymbol('A', 2, 3)\n        assert latex(A) == r'A_{2\\times3}'\n        assert latex(A[1, 2]) == r'A_{1,2}'\n", "def test_latex_tensorproduct():\n    # Test latex printing of tensor product\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    B = ImmutableDenseNDimArray([[5, 6], [7, 8]])\n    C = tensorproduct(A, B)\n    assert latex(C) == r\"\\begin{pmatrix}5 & 6 \\\\ 7 & 8\\end{pmatrix} \\otimes \\begin{pmatrix}1 & 2 \\\\ 3 & 4\\end{pmatrix}\"\n", "def test_latex_MatrixSymbol():\n    A = MatrixSymbol('A', 3, 3)\n    assert latex(A) == r'\\mathbf{A}'\n\n    B = MatrixSymbol('B', 2, 3)\n    assert latex(B) == r'\\mathbf{B}'\n"], "sample_1002": ["def test_fibonacci():\n    assert fibonacci(0) == 0\n    assert fibonacci(1) == 1\n    assert fibonacci(2) == 1\n    assert fibonacci(3) == 2\n    assert fibonacci(4) == 3\n    assert fibonacci(5) == 5\n    assert fibonacci(10) == 55\n    assert fibonacci(20) == 6765\n    assert fibonacci(50) == 12586269025\n\n", "    def test_fibonacci():\n        assert fibonacci(0) == 0\n        assert fibonacci(1) == 1\n        assert fibonacci(2) == 1\n        assert fibonacci(3) == 2\n        assert fibonacci(4) == 3\n        assert fibonacci(5) == 5\n        assert fibonacci(6) == 8\n        assert fibonacci(10) == 55\n\n", "def test_GoldenRatio_evalf():\n    assert same_and_same_prec(GoldenRatio.evalf(10), Float('1.6180339887'))\n    assert same_and_same_prec(GoldenRatio.evalf(20),\n                              Float('1.61803398874989484820'))\n", "def test_int_nthroot():\n    assert integer_nthroot(8, 3) == 2\n    assert integer_nthroot(-8, 3) == -2\n    assert integer_nthroot(27, 3) == 3\n    assert integer_nthroot(-27, 3) == -3\n    raises(ValueError, lambda: integer_nthroot(8, 2))\n    raises(ValueError, lambda: integer_nthroot(-8, 2))\n    assert integer_nthroot(256, 4) == 4\n    assert integer_nthroot(-256, 4) == -4\n\n\n\n", "def test_mod_inverse():\n    assert mod_inverse(3, 7) == 5\n    assert mod_inverse(5, 7) == 3\n    assert mod_inverse(4, 7) == 2\n    raises(ValueError, lambda: mod_inverse(6, 7))\n    raises(ValueError, lambda: mod_inverse(7, 7))\n    raises(ValueError, lambda: mod_inverse(0, 7))\n"], "sample_655": ["    def test_capture_binary(self):\n        # Test capturing binary output.\n        with capture.MultiCapture(out=True, err=True, Capture=capture.FDCaptureBinary) as cap:\n            # Write a byte string to stdout.\n            sys.stdout.buffer.write(b\"Hello, world!\\n\")\n\n        captured = cap.readouterr()\n        assert captured.out == b\"Hello, world!\\n\"\n", "    def test_capture_manager_uses_fd(self):\n        test_path = py.path.local(__file__).dirpath(\"data/test_capture.py\")\n        result = pytest.main(\n            [\"-s\", test_path], plugins=[capture.CaptureManagerPlugIn(test_path)]\n        )\n        assert result.ret == EXIT_NOTESTSCOLLECTED\n\n\n", "    def test_capture_fd_binary(self, tmpdir):\n        with StdCaptureFD(out=True, err=True):\n            captured = subprocess.run(\n                [\"python\", \"-c\", \"print('hello', file=2); print('world')\"],\n                cwd=str(tmpdir),\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n            )\n\n        assert captured.stdout == \"world\\n\"\n        assert captured.stderr == \"hello\\n\"\n", "    def test_capture_manager_with_patch(self, monkeypatch):\n        cm = CaptureManager()\n\n            print(\"hello\")\n\n        monkeypatch.setattr(sys, 'stdout', io.StringIO())\n        with cm.capture_output():\n            myfunc()\n        assert cm.readouterr().out == \"hello\\n\"\n\n", "def test_disable_global_capture_during_test(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"capmethod\", [\"no\", \"sys\", \"fd\"])\n            with pytest.config.get_plugin(\"capturemanager\").global_and_fixture_disabled():\n                print(\"captured output\")\n            assert capmethod in pytest.config.getoption(\"capture\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n"], "sample_615": ["    def test_apply_ufunc_binary_with_dask_array(self, func):\n        if not has_dask:\n            pytest.skip(\"Requires dask\")\n        x = xr.DataArray(np.arange(10), dims=(\"x\",))\n        y = xr.DataArray(\n            np.arange(10), dims=(\"x\",), chunks=(5,)\n        )\n        expected = func(x, y)\n        with raise_if_dask_computes():\n            actual = apply_ufunc(func, x, y)\n        assert_identical(actual.compute(), expected)\n\n", "    def test_where_scalar_broadcast():\n        da = xr.DataArray(np.arange(4).reshape(2, 2), dims=['x', 'y'])\n        cond = True\n        expected = xr.DataArray(np.full((2, 2), 0), dims=['x', 'y'])\n\n        actual = xr.where(cond, da, 0)\n        assert_identical(actual, expected)\n", "    def test_unified_dim_sizes(left, right, expected_dim):\n        result = unified_dim_sizes(left, right)\n        if expected_dim is None:\n            assert result is None\n        else:\n            assert result == {expected_dim: 3}\n\n", "    def test_where_keeps_attrs():\n        with xr.set_options(keep_attrs=True):\n            x = xr.DataArray([1, 2, 3], dims=[\"A\"], attrs={\"foo\": \"bar\"})\n            y = xr.DataArray([4, 5, 6], dims=[\"A\"], attrs={\"foo\": \"baz\"})\n            cond = xr.DataArray([True, False, True], dims=[\"A\"])\n            expected = xr.DataArray([1, 5, 3], dims=[\"A\"], attrs={\"foo\": \"bar\"})\n            actual = xr.where(cond, x, y)\n            assert_identical(actual, expected)\n\n", "    def test_dot_broadcast_1d():\n        x = xr.DataArray(np.arange(4), dims=[\"x\"])\n        y = xr.DataArray(np.arange(4), dims=[\"y\"])\n        z = xr.DataArray(np.arange(16).reshape(4, 4), dims=[\"x\", \"y\"])\n        expected = z.sum([\"x\", \"y\"])\n        result = xr.dot(x, y, dims=[\"x\", \"y\"])\n        assert_identical(result, expected)\n\n\n"], "sample_362": ["    def test_remove_field_with_default(self):\n        before = self.make_project_state([\n            self.author_with_biography_non_blank,\n        ])\n        after = self.make_project_state([\n            self.author_with_biography_blank,\n        ])\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"biography\")\n", "    def test_circular_fk_constraint(self):\n        \"\"\"\n        Tests that a circular foreign key relationship correctly identifies\n        the dependency between models.\n\n        This checks that the migrations generated correctly reflect the\n        dependency on the 'Knight' model when creating the 'Rabbit' model.\n        \"\"\"\n        changes = self.get_changes(\n            [self.knight],\n            [self.knight, self.rabbit],\n        )\n        self.assertNumberMigrations(changes, \"eggs\", 2)\n        self.assertMigrationDependencies(changes, \"eggs\", 1, [('eggs', '0001_initial')])\n        self.assertOperationTypes(changes, \"eggs\", 1, ['CreateModel'])\n        self.assertOperationAttributes(changes, \"eggs\", 1, 0, name='rabbit')\n", "    def test_remove_field_with_default(self):\n        before_state = self.make_project_state([\n            self.author_with_biography_non_blank,\n        ])\n        after_state = self.make_project_state([\n            self.author_with_biography_blank,\n        ])\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"biography\")\n", "    def test_field_rename_with_same_default(self):\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_default_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name')\n\n", "    def test_empty_migrations(self):\n        changes = self.get_changes(\n            [self.author_name],\n            [self.author_name],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 0)\n\n"], "sample_114": ["    def test_m2m_field_through_rename(self):\n        before = [\n            self.book_with_multiple_authors,\n            self.attribution,\n        ]\n        after = [\n            self.book_with_multiple_authors_through_attribution,\n            self.attribution,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"authors\",\n                                       field_name=\"authors\",\n                                       relation_name=\"book_authors\",\n                                       )\n\n", "    def test_remove_field_with_default(self):\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_default_no_default,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField', 'RunPython'])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='name',\n        )\n\n", "    def test_rename_field_with_default(self):\n        before_state = self.make_project_state([\n            self.author_with_name,\n        ])\n        after_state = self.make_project_state([\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"new_name\", models.CharField(max_length=200, default='Anonymous')),\n            ]),\n        ])\n        changes = self.get_changes(before_state.models.values(), after_state.models.values())\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(\n            changes,\n            \"testapp\",\n            0,\n            0,\n            name_old=\"name\",\n            name_new=\"new_name\",\n        )\n\n", "    def test_alter_field_unique_together_change(self):\n        before_states = [\n            self.author_name,\n            self.book_foo_together,\n        ]\n        after_states = [\n            self.author_name,\n            self.book_foo_together_2,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='book', field=self.book_foo_together_2.fields['title'])\n", "    def test_no_change_to_field_def(self):\n        self.assertNumberMigrations(self.get_changes(\n            [self.author_name],\n            [self.author_name]),\n            \"testapp\",\n            0,\n        )\n"], "sample_268": ["    def test_iter_modules_and_files_with_zip_import(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / \"test.zip\"\n            with zipfile.ZipFile(zip_filename, 'w') aszf:\n                zf.writestr('test/module.py', 'a = 1')\n\n            sys.path.append(tmpdir)\n            self.addCleanup(sys.path.remove, tmpdir)\n\n            # Import from the zip file, forcing py_compile to cache the\n            # module in memory\n            self.import_and_cleanup('test.module')\n\n            # Verify we can find the module in the zip file\n            self.assertFileFound(zip_filename)\n", "    def test_watchman_unavailable(self):\n        with mock.patch('django.utils.autoreload.pywatchman', mock.MagicMock(spec=pywatchman)):\n            with mock.patch('django.utils.autoreload.WatchmanUnavailable') as watchman_unavailable:\n                watchman_unavailable.side_effect = WatchmanUnavailable('Watchman service is unavailable.')\n                reloader = autoreload.get_reloader()\n                self.assertIsInstance(reloader, autoreload.StatReloader)\n", "    def test_autoreload_watches_zipfile(self):\n        zip_path = self.temporary_file('test_package.zip')\n        with zipfile.ZipFile(zip_path, 'w') as zipf:\n            zipf.writestr('test_module.py', 'print(\"Hello\")')\n        sys.path.insert(0, str(zip_path.parent))\n        self.addCleanup(sys.path.pop, len(sys.path) - 1)\n\n        self.import_and_cleanup('test_package.test_module')\n        self.assertFileFound(zip_path)\n", "    def test_watchman_unavailable(self):\n        with mock.patch('autoreload.pywatchman.client', side_effect=pywatchman.WatchmanError('Connection error')):\n            reloader = autoreload.WatchmanReloader()\n            with self.assertRaises(autoreload.WatchmanUnavailable):\n                reloader.check_availability()\n", "    def test_iter_modules_and_files_zip_file_import(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zipfilename = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zipfilename, 'w') as zf:\n                zf.writestr('module_in_zip.py', 'print(\"Hello from zip file!\")')\n            sys.path.append(tempdir)\n            self.import_and_cleanup('module_in_zip')\n            self.assertFileFound(zipfilename)\n"], "sample_330": ["    def test_truncate_name(self):\n        self.assertEqual(truncate_name('tablename'), 'tablename')\n        self.assertEqual(truncate_name('\"tablename\"'), '\"tablename\"')\n        self.assertEqual(truncate_name('\"namespace\".\"tablename\"', length=20), '\"namespace\".\"tablea6389')\n        self.assertEqual(truncate_name('veryveryveryverylongtablename', length=15), 'veryveryve8207')\n", "    def test_truncate_name(self):\n        self.assertEqual(truncate_name('tablename'), 'tablename')\n        self.assertEqual(truncate_name('tablename', length=5), 'tablen')\n        self.assertEqual(truncate_name('\"schema\".\"tablename\"', length=10), '\"schema\".\"tablena')\n        self.assertEqual(truncate_name('\"schema\".\"tablename\"', length=10, hash_len=2), '\"schema\".\"tablen83')\n", "    def test_typecast_date(self):\n        self.assertEqual(typecast_date('2005-07-29'), datetime.date(2005, 7, 29))\n        self.assertEqual(typecast_date('1982-10-26'), datetime.date(1982, 10, 26))\n        self.assertIsNone(typecast_date(None))\n", "    def test_truncate_name(self):\n        self.assertEqual(truncate_name(\"a\"), \"a\")\n        self.assertEqual(truncate_name(\"a\" * 100), \"a\" * 96 + '...' )\n        self.assertEqual(truncate_name('\"a\".\"b\"'), '\"a\".\"b\"')\n        self.assertEqual(truncate_name('\"a\".\"b\"', length=15), '\"a\".\"b\"')\n        self.assertEqual(truncate_name('\"a\".\"b\"', length=10), '\"a\".\"b... ...')\n        self.assertEqual(truncate_name('\"a\".\"b\"', length=5), '\"a\".\"...')\n", "    def test_split_identifier(self):\n        self.assertEqual(split_identifier('\"namespace\".\"name\"'), ('namespace', 'name'))\n        self.assertEqual(split_identifier('\"name\"'), ('', 'name'))\n        self.assertEqual(split_identifier('name'), ('', 'name'))\n"], "sample_791": ["    def test_onehot_inverse_transform_object_dtype(self):\n        enc = OneHotEncoder(handle_unknown='ignore', dtype=object)\n        X = [[1, 'a'], [2, 'b'], [1, 'c']]\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        X_inv = enc.inverse_transform(X_trans)\n        assert_equal(X_inv, X)\n", "    def test_onehotencoder_sparse_legacy_unknown_error(self):\n        X = np.array([[0, 1], [1, 1], [2, 0], [0, 0]])\n        enc = OneHotEncoder(handle_unknown='error', sparse=True,\n                            categories='auto',\n                            legacy=True).fit(X)\n        with pytest.raises(ValueError):\n            enc.transform([[0, 3]])\n\n\n", "    def test_onehot_sparse_drop_unknown(self):\n        X = np.array([[0, 1], [1, 0], [2, 1]]).astype(int)\n        enc = OneHotEncoder(drop='first', sparse=True, handle_unknown='ignore')\n\n        with pytest.warns(UserWarning, match='Found unknown categories'):\n            enc.fit(X)\n\n        X_out = enc.transform(np.array([[2, 2], [0, 3]])).toarray()\n        assert_array_equal(X_out, [[0, 1, 0], [1, 0, 0]])\n", "    def test_onehot_encoder_handle_unknown_error(self):\n        X = np.array([[0, 1], [1, 2], [2, 0]])\n        enc = OneHotEncoder(handle_unknown='error')\n        enc.fit(X)\n        with pytest.raises(ValueError):\n            enc.transform([[0, 3]])\n", "    def test_onehot_encoder_drop_unknown(self):\n        X = np.array([[1, \"a\"], [2, \"a\"], [1, \"b\"]])\n        enc = OneHotEncoder(handle_unknown='ignore', drop='first')\n        enc.fit(X)\n        Xt = enc.transform(X)\n        expected_shape = (3, 3)\n        assert Xt.shape == expected_shape\n"], "sample_506": ["    def test_spines_proxy(self):\n        fig, ax = plt.subplots()\n        spines = ax.spines\n        spines['top'].set_visible(False)\n        spines[['left', 'right']].set_color('red')\n        return fig\n", "    def test_set_position(self, fig_test, fig_ref, spine_type):\n        ax = fig_test.add_subplot()\n        ax.spines[spine_type].set_position(('outward', 10))\n        fig_ref.add_subplot().spines[spine_type].set_position(('outward', 10))\n", "    def test_spine_proxy_set_visible(self, key):\n        fig, ax = plt.subplots()\n        spines = ax.spines\n\n        # Test setting visibility via SpinesProxy\n        spines[[key]].set_visible(False)\n\n        assert not getattr(ax.spines[key], 'get_visible')()\n\n        # Test setting visibility via SpinesProxy on a single spine\n        spines[key].set_visible(True)\n\n        assert getattr(ax.spines[key], 'get_visible')()\n", "    def test_getitem_getattr(self, key):\n        spines = Spines(left=plt.Spine, bottom=plt.Spine)\n        if key in ['left', 'bottom']:\n            assert spines[key] is spines.left\n            assert getattr(spines, key) is spines.left\n        else:\n            with pytest.raises(KeyError):\n                spines[key]\n            with pytest.raises(AttributeError):\n                getattr(spines, key)\n\n", "    def test_spine_set_bounds(self, spine_type):\n        fig, ax = plt.subplots()\n        spine = ax.spines[spine_type]\n        spine.set_bounds(0.2, 0.8)\n        assert spine._bounds == (0.2, 0.8)\n"], "sample_477": ["    def test_random(self):\n        self.rendered = self.engine.render_to_string(\"random01\", context={\"a\": [\"one\", \"two\"], \"b\": [1, 2, 3]})\n        self.assertIn(\n            self.rendered, [\"one 1\", \"one 2\", \"one 3\", \"two 1\", \"two 2\", \"two 3\"]\n        )\n\n", "    def test_random(self):\n        self.assertTemplateSnippet(\n            self.engine.from_string(\"{{ a|random }} {{ b|random }}\"),\n            {\n                \"a\": [\"a1\", \"a2\", \"a3\"],\n                \"b\": [\"b1\", \"b2\",\n                    \"b3\"],\n            },\n            \"random output\",\n        )\n", "    def test_random(self):\n        self.assertContains(self.render({'a': [1, 2, 3], 'b': ['a', 'b']}), '2 a')\n", "    def test_random(self):\n        self.assertEqual(\n            self.engine.render({\"a\": [1, 2, 3], \"b\": [\"x\", \"y\", \"z\"]}),\n            '1 x',\n        )\n        self.assertEqual(\n            self.engine.render({\"a\": [1, 2, 3], \"b\": [\"x\", \"y\", \"z\"]}),\n            '3 z',\n        )\n", "    def test_random(self):\n        self.template.context_data['a'] = ['apple', 'banana', 'cherry']\n        self.template.context_data['b'] = [1, 2, 3]\n        output = self.render_template('random01')\n        self.assertIn(output, ['apple 1', 'banana 1', 'cherry 1', 'apple 2', 'banana 2', 'cherry 2', 'apple 3', 'banana 3', 'cherry 3'])\n\n"], "sample_450": ["    def test_get_admin_log_tag(self):\n        template = \"\"\"\n        {% load admin_utils_tags %}\n        {% get_admin_log 1 as log_entries %}\n        {% for entry in log_entries %}\n            {{ entry.object_repr }}\n        {% endfor %}\n        \"\"\"\n        response = self.client.get(reverse(\"admin:site:site_settings\"))\n        self.assertContains(response, repr(self.a1))\n        response = self.client.get(reverse('admin:index'))\n        self.assertContains(response, repr(self.a1))\n\n", "    def test_get_admin_log_template_tag(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, '{% get_admin_log 10 as admin_log %}', count=1)\n\n        self.assertIn('admin_log', response.context)\n        self.assertIsInstance(response.context['admin_log'], list)\n        self.assertEqual(len(response.context['admin_log']), 1)\n", "    def test_get_admin_log_template_tag(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, '{% get_admin_log 1 as admin_log %}')\n        self.assertContains(response, '<ul class=\"loglist\">')\n", "    def test_get_admin_log_for_user(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, \"Changed something\")\n\n        LogEntry.objects.log_action(\n            self.user.pk,\n            ContentType.objects.get_for_model(ArticleProxy).pk,\n            Article.objects.create(site=self.site, title=\"New Title\").pk,\n            repr(Article.objects.get(pk=Article.objects.latest(\"pk\").pk)),\n            ADDITION,\n            change_message=\"Added a new article\",\n        )\n\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, \"Added a new article\")\n\n        # Test with a specific user\n\n        response = self.client.get(reverse(\"admin:index\"))\n\n        self.assertNotContains(response, \"Added a new article\")\n", "    def test_get_admin_log_tag(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, '{% get_admin_log 1 as recent_logs %}', count=1)\n        self.assertContains(response, '{% get_admin_log 5 as recent_logs for_user user %}', count=1) \n"], "sample_162": ["    def test_comment_location_without_line_number(self):\n        \"\"\"\n        Test that comments are correctly generated when line number is not provided.\n        \"\"\"\n        copytree(os.path.join(str(Path(__file__).parent), 'test_app'), self.work_dir)\n        output, po_contents = self._run_makemessages(\n\n            )\n        self.assertLocationCommentPresent(self.PO_FILE, None, 'test_app', 'templates', 'test_app.html')\n        self.assertLocationCommentNotPresent(self.PO_FILE, 5, 'test_app', 'templates', 'test_app.html')\n", "    def test_make_messages_with_no_locale_path(self):\n        \"\"\"\n        #25297 - makemessages should not create locale dirs if no locale path is configured\n        \"\"\"\n        with override_settings(LOCALE_PATHS=[]):\n            output, po_contents = self._run_makemessages(locale=[LOCALE])\n\n            self.assertIn(f\"processing locale {LOCALE}\", output)\n            self.assertNotIn('Creating locale directory', output)\n            self.assertTrue(os.path.exists(self.PO_FILE))\n", "    def test_ignores_dotfiles(self):\n        # Create a file that should be ignored.\n        with open(\".hidden_file.py\", 'w') as f:\n            f.write(\"This should be ignored.\")\n        \n        out, po_contents = self._run_makemessages()\n        self.assertNotMsgId(\"This should be ignored.\", po_contents)\n", "    def test_makemessages_ignores_commented_out_strings(self):\n        # Regression test for #24171\n        os.makedirs(os.path.join(self.work_subdir, 'templates'))\n        with open(os.path.join(self.work_subdir, 'templates', 'test.html'), 'w') as f:\n            f.write(\"\"\"", "    def test_ignore_pattern(self):\n        \"\"\"\n        Test whether ignore_patterns correctly excludes files.\n        \"\"\"\n        with open('commands/test1.html', 'w') as f:\n            f.write('Test string 1.')\n        with open('commands/test2.html', 'w') as f:\n            f.write('Test string 2.')\n        output, po_contents = self._run_makemessages(ignore_patterns=['test2.html'])\n        self.assertMsgId('Test string 1.', po_contents)\n        self.assertNotMsgId('Test string 2.', po_contents)\n"], "sample_709": ["    def test_runpytest_subprocess_timeout(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import time\n                time.sleep(10)\n        \"\"\"\n        )\n        with pytest.raises(pytester.TimeoutExpired) as excinfo:\n            pytester.runpytest_subprocess(p, timeout=1)\n        assert \"1 second\" in str(excinfo.value)\n", "    def test_parse_args_plugin_with_hyphen(self) -> None:\n        \"\"\"Test parsing of arguments when a plugin name contains a hyphen.\"\"\"\n        plugin_name = \"my-plugin\"\n        args = f\"-p {plugin_name}\".split()\n        config = self.parseconfig(args)\n        assert plugin_name in config.pluginmanager.list_names()\n\n", "    def test_syspathinsert_works_with_other_paths(self, monkeypatch) -> None:\n        monkeypatch.chdir(self.testdir)\n        # Use a path that doesn't exist to ensure sys.path is really modified\n        other_path = Path(\"completely_nonexistent_path\")\n        self.runpytest(syspathinsert=True, plugins=[other_path.as_posix()])\n        assert other_path.as_posix() in sys.path\n\n", "    def test_runpytest_subprocess_timeout(self, pytester: Pytester) -> None:\n        \"\"\"Test that runpytest_subprocess() times out correctly.\"\"\"\n        p = pytester.runpytest(\"-p no:random\", \"--timeout=0.1\", \"-k\", \"some_test\")\n        assert p.ret == ExitCode.TIMEOUT\n", "    def test_runpytest_subprocess_timeout(self, pytester: Pytester) -> None:\n        \"\"\"Test that runpytest_subprocess handles timeouts.\"\"\"\n        p = pytester.makepyfile(\n            \"\"\"\n            import time\n                time.sleep(20)\n        \"\"\"\n        )\n        ret = pytester.runpytest_subprocess(p, timeout=1)\n        assert ret.retcode == ExitCode.TIMEOUT\n        assert \"timeout expired\" in ret.errlines\n\n\n"], "sample_1024": ["def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(16) == 4\n    assert isqrt(25) == 5\n    raises(ValueError, lambda: isqrt(-1))\n\n", "    def test_sympify_PythonRational():\n        assert sympify(PythonRational(1, 2)) == Rational(1, 2)\n\n", "def test_catalan():\n    assert Catalan > 0\n    assert Catalan < 1\n\n    assert Catalan.is_irrational is None\n\n    assert (Catalan - S.Catalan) is S.Zero\n", "def test_sympify_issue_10943():\n    assert sympify(\"2**(-1/2)\") == 1/(2**Rational(1,2))\n", "def test_GoldenRatio_approximation():\n    assert S.GoldenRatio.approximation_interval(Rational) == (Rational(223, 71), Rational(22, 7))\n\n"], "sample_794": ["    def test_ridge_gcv_accuracy(self, solver, X_filter, fit_intercept):\n        X = X_filter(X_diabetes)\n        y = y_diabetes\n        ridge_gcv = _RidgeGCV(alphas=np.array([0.1, 1, 10]),\n                              fit_intercept=fit_intercept,\n                              gcv_mode='svd' if solver == 'svd'\n                              else 'eigen', solver=solver)\n        ridge_gcv.fit(X, y)\n\n        y_pred = ridge_gcv.predict(X)\n        mse = mean_squared_error(y, y_pred)\n        ridge = Ridge(alpha=ridge_gcv.alpha_, solver=solver,\n                      fit_intercept=fit_intercept)\n        ridge.fit(X, y)\n\n        y_pred_ridge = ridge.predict(X)\n        mse_ridge = mean_squared_error(y, y_pred_ridge)\n\n        assert_almost_equal(mse, mse_ridge, decimal=4)\n", "    def test_ridgecv_sparse_fit_intercept(self):\n        # Test RidgeCV with sparse input and fit_intercept=True\n        X = sp.csr_matrix(np.eye(10))\n        y = np.arange(10)\n        clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n        assert_array_almost_equal(clf.coef_, np.arange(10) / 2, decimal=5)\n        assert_array_almost_equal(clf.intercept_, 0., decimal=5)\n", "def test_ridge_sparse_input_solver(solver):\n    # Check that Ridge works with sparse input and different solvers\n\n    X = SPARSE_FILTER(make_regression(n_samples=100, n_features=10,\n                                       random_state=0)[0])\n    y = make_regression(n_samples=100, n_features=10,\n                        random_state=0)[1]\n\n    ridge = Ridge(alpha=1.0, solver=solver, random_state=0)\n    ridge.fit(X, y)\n\n    score = ridge.score(X, y)\n    assert_greater(score, 0.9)\n\n", "    def test_ridge_sparse_cg_solver(self):\n        # Check Ridge with sparse_cg solver and a sparse input\n        X = sp.csr_matrix(X_diabetes)\n        y = y_diabetes\n        alphas = [1e-3, 1e-2, 1e-1, 1]\n        ridge = RidgeCV(alphas=alphas, solver='sparse_cg', cv=None).fit(X, y)\n        dense_ridge = RidgeCV(alphas=alphas, cv=None).fit(X.toarray(), y)\n        assert_allclose(ridge.coef_, dense_ridge.coef_, rtol=1e-3)\n\n\n", "    def test_ridge_solver_sparse_cg(self):\n        # Regression test for sparse_cg solver\n        X = sp.csc_matrix(np.eye(10))\n        y = np.arange(10)\n        clf = Ridge(solver=\"sparse_cg\")\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.coef_, y)\n\n"], "sample_1027": ["def test_groebner_basis_is_zero_dimensional_zero():\n    R = ZZ[x, y]\n    f = 1\n    G = groebner([f], x, y)\n\n    assert G.is_zero_dimensional\n\n", "def test_groebner_coeffs():\n    F = [a*x + b*y - c, a*x**2 + b*y**2 - c**2]\n    G = groebner(F, x, y)\n    assert _strict_eq(G[0].coeffs(), [a, b, -c])\n\n", "def test_groebner_basis_empty_list():\n    F = []\n    G = groebner(F, x, y)\n    assert G.exprs == []\n", "def test_poly_gcd_list():\n    F = field(QQ)\n    p = Poly(x**2 + 1, x)\n    q = Poly(2*x + 2, x)\n    r = Poly(x - 1, x)\n\n    assert _strict_eq(gcd_list([p, q, r]), [Poly(1, x)])\n", "def test_Poly_addsub_Mul():\n    p = Poly(x**2 + 2*x, x)\n    q = Poly(2*x + 1, x)\n\n    assert p + q == Poly(x**2 + 4*x + 1, x)\n    assert p - q == Poly(x**2 - 1, x)\n\n    assert p + 2*q == Poly(x**2 + 6*x + 2, x)\n    assert p - 2*q == Poly(x**2 - 2*x - 2, x)\n\n    assert 2*p + q == Poly(2*x**2 + 5*x, x)\n    assert 2*p - q == Poly(2*x**2 - x, x)\n\n\n\n"], "sample_519": ["    def test_tight_layout_empty(self):\n        fig, ax = plt.subplots()\n        ax.remove()\n        fig.tight_layout()\n", "    def test_figure_savefig_pil_kwargs(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n        with io.BytesIO() as buf:\n            fig.savefig(buf, format='png', pil_kwargs={'optimize': True})\n            buf.seek(0)\n            img = Image.open(buf)\n            assert img.mode == 'RGB'\n", "def test_figure_dpi_awareness():\n    fig = Figure(dpi=100)\n    assert fig.dpi == 100\n    fig.set_dpi(50)\n    assert fig.dpi == 50\n    # Check that changing dpi also updates bbox_inches\n    fig.set_size_inches(5, 5)\n    assert fig.bbox_inches.width == 250\n    fig.set_dpi(200)\n    assert fig.bbox_inches.width == 1000\n", "    def test_savefig_transparent(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3])\n        with io.BytesIO() as buf:\n            fig.savefig(buf, transparent=True, format=\"png\")\n            buf.seek(0)\n            img = Image.open(buf)\n            assert img.mode == \"RGBA\"\n", "    def test_figure_dpi_ratio(self):\n        # Check that setting dpi does not affect the figure size when\n        # dpi_ratio is set\n\n        fig, ax = plt.subplots(figsize=(5, 5), dpi=100)\n        dpi_ratio = mpl.rcParams['figure.dpi'] / fig.dpi\n        assert fig.get_size_inches() == np.array([5, 5])\n\n        fig.set_dpi_ratio(dpi_ratio)\n        assert fig.get_size_inches() == np.array([5, 5])\n"], "sample_1089": ["    def test_factor_nc_issue_17266():\n        A = Symbol('A', commutative=False)\n        B = Symbol('B', commutative=False)\n        C = Symbol('C', commutative=False)\n        expr = A*B + A*C + B*C\n        factored = factor_nc(expr)\n        assert factored == A*(B + C) + B*C\n\n\n\n", "def test_factor_nc_issue17875():\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    e = (A*B*x + A*B*y).expand()\n    f = factor_nc(e)\n    assert f == A*B*(x + y)\n\n", "    def test_factor_nc_issue_18252_4():\n        A = Symbol('A', commutative=False)\n        B = Symbol('B', commutative=False)\n        expr = (A*x*B + A*y*B).expand()\n        assert factor_nc(expr) == B*(A*x + A*y)\n", "def test_factor_nc_noncommutative_power():\n    x, y = symbols('x y', commutative=False)\n    expr = (x**2 + 2*x*y + y**2).expand()\n    assert factor_nc(expr) == (x + y)**2\n", "def test_factor_nc_issue_20662():\n    A, B = symbols('A B', commutative=False)\n    expr = (A + B)**2\n    assert factor_nc(expr) == (A + B)**2\n"], "sample_725": ["    def test_check_X_y_dtype_warning(self):\n        # Check that DataConversionWarning is raised when dtype of X is object\n        # and y is numeric.\n\n        X = np.array([['a', 'b'], ['c', 'd']], dtype=object)\n        y = np.array([1, 2])\n\n        with warnings.catch_warnings(record=True) as w:\n            check_X_y(X, y)\n            assert_equal(len(w), 1)\n            assert_true(issubclass(w[0].category, DataConversionWarning))\n            assert_equal(w[0].message.args[0],\n                         \"Data with input dtype object was converted to float64\"\n                         )\n", "    def test_check_array_dtype_object_warning(self):\n        with warnings.catch_warnings():\n            warnings.simplefilter('always')\n            X = np.array([['a', 1], ['b', 2]], dtype=object)\n\n            # No warning when dtype is None\n            assert_no_warnings(check_array, X, dtype=None)\n\n            # Warning when 'numeric' is requested\n            assert_warns_message(DataConversionWarning,\n                                 \"Data with input dtype object was converted to \"\n                                 \"float64 by check_array.\",\n                                 check_array, X, dtype='numeric')\n", "    def test_check_consistent_length_sparse_input():\n        X = sparse_random_matrix(10, 5, density=0.5)\n        y = np.ones(10)\n        check_consistent_length(X, y)\n        y = np.ones(9)\n        assert_raises(ValueError, check_consistent_length, X, y)\n", "    def test_check_symmetric_sparse(self):\n        X = sparse_random_matrix(10, 10, density=0.5, random_state=0)\n        # test case: already symmetric\n        assert_allclose_dense_sparse(X.tocsr(),\n                                     check_symmetric(X.tocsr()))\n        # test case: non symmetric, sparse\n        X = X + X.T\n        X[0, 1] = 10\n        X_sym = check_symmetric(X.tocsr())\n        assert_allclose_dense_sparse(0.5 * (X + X.T), X_sym)\n", "    def test_check_array_object_dtype_no_warn(self):\n        # Test that DataConversionWarning is not raised when converting\n        # object dtype array to numeric dtype\n\n        with warnings.catch_warnings(record=True) as w:\n            check_array(np.array([[1, 2], [3, 4]], dtype=object),\n                        dtype='numeric')\n        assert_equal(len(w), 0)\n"], "sample_542": ["    def test_annotation_get_position(self):\n        fig, ax = plt.subplots()\n        annotation = ax.annotate('foo', (0.5, 0.5), xycoords='data')\n        assert_almost_equal(annotation.get_position(), (0.5, 0.5))\n", "    def test_annotation_clip(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n\n        # Annotation within axes limits\n        ann = ax.annotate(\"Test\", xy=(0.5, 0.5), xycoords='data')\n        assert ann.get_visible()\n\n        # Annotation outside axes limits\n        ann = ax.annotate(\"Test\", xy=(2, 2), xycoords='data',\n                          annotation_clip=True)\n        assert not ann.get_visible()\n\n        # Annotation outside axes limits, but clip is False\n        ann = ax.annotate(\"Test\", xy=(2, 2), xycoords='data',\n                          annotation_clip=False)\n        assert ann.get_visible()\n", "    def test_annotation_transform(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        text = ax.annotate(\"Test\", xy=(5, 5), xycoords='data',\n                           xytext=(2, 2), textcoords='offset points')\n        assert text._get_position_xy(fig.canvas.get_renderer()) == (\n            2, 2)\n\n        # Test offset relative to a Bbox\n        rect = mpatches.Rectangle((1, 1), 2, 2, facecolor='red',\n                                 transform=ax.transData)\n        ax.add_patch(rect)\n        text = ax.annotate(\"Test\", xy=(5, 5), xycoords='data',\n                           xytext=(0.5, 0.5), textcoords='offset points',\n                           arrowprops=dict(arrowstyle=\"-\"))\n        text.set_rotation(45)\n        text._bbox_patch = rect\n        assert_almost_equal(text._get_position_xy(fig.canvas.get_renderer()),\n                            rect.get_bbox().mean(axis=0))\n", "    def test_annotation_arrowprops_mutation_aspect(self):\n        fig, ax = plt.subplots()\n        text = ax.annotate(\"test\", (0.5, 0.5), xycoords='axes fraction',\n                           arrowprops=dict(arrowstyle='->',\n                                           mutation_aspect=2))\n        self.compare_figure_to_reference(fig, 'annotation_arrowprops_mutation_aspect')\n", "    def test_annotation_xycoords_transform(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n\n        # Create a transform that shifts the x-coordinate by 5\n        transform = mtransforms.Affine2D().translate(5, 0)\n\n        # Create an annotation using the transform\n        annotation = ax.annotate(\n            \"Test\", xy=(0, 0), xycoords=transform, xytext=(2, 2),\n            textcoords='data', arrowprops=dict(arrowstyle=\"->\")\n        )\n\n        # Assert that the annotation is drawn at the expected location\n        # after applying the transform\n        assert_almost_equal(annotation.xyann, (5, 0))\n"], "sample_48": ["    def test_aggregate_with_case_when_clause(self):\n        with CaptureQueriesContext(connection) as queries:\n            result = Book.objects.annotate(\n                price_category=Case(\n                    When(price__gt=30, then=Value('Expensive')),\n                    When(price__gt=20, then=Value('Moderate')),\n                    default=Value('Cheap'),\n                    output_field=CharField(),\n                )\n            ).aggregate(\n                avg_price_expensive=Avg(F('price'), filter=Q(price_category='Expensive')),\n            )\n        self.assertEqual(len(queries), 2)\n        self.assertAlmostEqual(result['avg_price_expensive'], Decimal('82.80'))\n\n", "    def test_count_with_filter(self):\n        with Approximate(Decimal('22.00'), places=2):\n            self.assertEqual(\n                Book.objects.filter(authors__in=[self.a1, self.a4]).aggregate(\n                    cnt=Count('id', filter=Q(price__gte=Decimal('25.00')))\n                )['cnt'],\n                2\n            )\n", "    def test_stddev_variance_empty(self):\n        with Approximate(\n            Decimal('0'), rel=Decimal('0.001'), abs=Decimal('0.001')\n        ) as approx:\n            self.assertEqual(StdDev('age')._get_sql(connection, self.a1), (approx, []))\n            self.assertEqual(Variance('age')._get_sql(connection, self.a1), (approx, []))\n\n", "    def test_aggregate_filter_with_case(self):\n        with CaptureQueriesContext(connection) as queries:\n            self.assertEqual(\n                Book.objects.filter(rating__gt=4).annotate(\n                    avg_price_above_4=Avg('price', filter=Q(rating__gt=4))\n                ).first(),\n                Book.objects.filter(rating__gt=4).annotate(\n                    avg_price_above_4=Case(\n                        When(rating__gt=4, then=Avg('price')),\n                        default=Value(0, output_field=DecimalField())\n                    )\n                ).first()\n            )\n\n        self.assertEqual(\n            len(queries.captured_queries),\n            3 if connection.features.supports_aggregate_filter_clause else 4,\n        )\n", "    def test_avg_distinct(self):\n        with Approximate(Decimal('30.00'), Decimal('0.01')):\n            self.assertEqual(\n                Book.objects.annotate(avg_price=Avg('price', distinct=True)).values_list('avg_price', flat=True)[0],\n                Decimal('30.00'),\n            )\n"], "sample_9": ["    def test_html_write_raw_html_cols(self):\n        \"\"\"\n        Test writing a table with a column containing raw HTML content.\n        \"\"\"\n\n        if not HAS_BLEACH:\n            pytest.skip(\"bleach is required for this test\")\n\n        data = Table([[1, 2, '<a href=\"http://www.example.com\">Link</a>'],\n                      [3, 4, '<b>Bold text</b>']],\n                     names=['col1', 'col2', 'html_col'])\n\n        htmldict = {'raw_html_cols': 'html_col'}\n\n        # Write the table\n        out = StringIO()\n        ascii.write(data, out, format='html', htmldict=htmldict)\n\n        # Parse the HTML output\n        soup = BeautifulSoup(out.getvalue(), 'html.parser')\n        html_col = soup.find_all('td', text='Link')\n\n        assert '<a href=\"http://www.example.com\">Link</a>' in str(html_col[0])\n\n        # Test that multiline HTML is preserved\n        data = Table([['<p>Line 1</p><p>Line 2</p>'],\n                      ['<div>Another line</div>']],\n                     names=['html_col'])\n        out = StringIO()\n\n        ascii.write(data, out, format='html', htmldict=htmldict)\n        soup = BeautifulSoup(out.getvalue(), 'html.parser')\n        html_col = soup.find_all('td')\n        assert '<p>Line 1</p><p>Line 2</p>' in str(html_col[0])\n        assert '<div>Another line</div>' in str(html_col[1])\n", "    def test_html_multicol(self):\n        # Test multidimensional columns in HTML output\n        table = Table([[1, 2, 3, 4],\n                       [5, 6, 7, 8]], names=['a', 'b', 'c', 'd'])\n        table['a'].shape = (2, 2)\n        table['b'].shape = (2, 2)\n        htmldict = {'multicol': True}\n        reader = html.HTML(htmldict=htmldict)\n        out = StringIO()\n        reader.write(table, out)\n\n        expected_output = (\n            '<!DOCTYPE html>\\n'\n            '<html>\\n'\n            '<head>\\n'\n            '<meta charset=\"utf-8\" >\\n'\n            '<meta http-equiv=\"Content-type\" content=\"text/html;charset=UTF-8\" >\\n'\n            '</head>\\n'\n            '<body>\\n'\n            '<table id=\"\">\\n'\n            '<thead>\\n'\n            '<tr>\\n'\n            '<th colspan=\"2\">a</th>\\n'\n            '<th colspan=\"2\">b</th>\\n'\n            '</tr>\\n'\n            '</thead>\\n'\n            '<tr>\\n'\n            '<td>1</td>\\n'\n            '<td>2</td>\\n'\n            '<td>5</td>\\n'\n            '<td>6</td>\\n'\n            '</tr>\\n'\n            '<tr>\\n'\n            '<td>3</td>\\n'\n            '<td>4</td>\\n'\n            '<td>7</td>\\n'\n            '<td>8</td>\\n'\n            '</tr>\\n'\n            '</table>\\n'\n            '</body>\\n'\n            '</html>')\n        assert out.getvalue() == expected_output\n\n", "    def test_html_table_with_colspan(self, tmpdir):\n        # Test that the HTML reader can handle tables with colspan attributes\n        table_str = \"\"\"\n        <table>\n        <thead>\n        <tr>\n            <th>Col A</th>\n            <th colspan=\"2\">Col B-C</th>\n        </tr>\n        </thead>\n        <tbody>\n        <tr>\n            <td>1</td>\n            <td>2</td>\n            <td>3</td>\n        </tr>\n        </tbody>\n        </table>\n        \"\"\"\n        with open(tmpdir.join('test.html'), 'w') as f:\n            f.write(table_str)\n\n        table = ascii.read(tmpdir.join('test.html'), format='html')\n\n        assert len(table.colnames) == 3\n        assert table['Col A'][0] == 1\n        assert table['Col B-C'][0] == (2, 3)\n", "    def test_html_multicol_output():\n        \"\"\"Test writing a table with multi-dimensional columns to HTML.\"\"\"\n        a = np.arange(6).reshape((2, 3))\n        b = np.arange(6, 12).reshape((2, 3))\n        table = Table([a, b], names=('col1', 'col2'))\n\n        # Write the table to HTML\n        output = StringIO()\n        ascii.write(table, output, format='html', html={'multicol': True})\n        html_string = output.getvalue()\n\n        # Ensure that the HTML output contains the correct number of columns\n        soup = BeautifulSoup(html_string, 'html.parser')\n        table = soup.find('table')\n        rows = table.find_all('tr')\n        assert len(rows) == 2\n        assert len(rows[0].find_all('th')) == 6\n\n        # Check the content of the table cells\n        for i in range(2):\n            for j in range(3):\n                assert rows[i].find_all('td')[j].text == str(a[i, j])\n                assert rows[i].find_all('td')[j+3].text == str(b[i, j])\n", "    def test_html_table_with_multicol(self):\n        \"\"\"Test reading HTML table with multicolumn cells.\"\"\"\n        html_data = \"\"\"\n        <table>\n        <thead>\n        <tr><th>Column 1</th><th colspan=\"2\">Column 2</th></tr>\n        </thead>\n        <tbody>\n        <tr><td>row1_col1</td><td>row1_col2_entry1</td><td>row1_col2_entry2</td></tr>\n        <tr><td>row2_col1</td><td>row2_col2_entry1</td><td>row2_col2_entry2</td></tr>\n        </tbody>\n        </table>\n        \"\"\"\n        table = Table.read(html_data, format='html')\n\n        # Check the shape of the multicolumn\n        assert table['Column 2'].shape == (2, 2)\n\n        # Check the values\n        assert table['Column 2'][0, 0] == 'row1_col2_entry1'\n        assert table['Column 2'][0, 1] == 'row1_col2_entry2'\n        assert table['Column 2'][1, 0] == 'row2_col2_entry1'\n        assert table['Column 2'][1, 1] == 'row2_col2_entry2'\n\n"], "sample_734": ["    def test_fowlkes_mallows_score_sparse(self):\n        for n_samples in [10, 100]:\n            labels_true = np.random.randint(0, 5, n_samples)\n            labels_pred = np.random.randint(0, 5, n_samples)\n            score_dense = fowlkes_mallows_score(labels_true, labels_pred)\n            score_sparse = fowlkes_mallows_score(\n                labels_true, labels_pred, sparse=True\n            )\n            assert_almost_equal(score_dense, score_sparse)\n", "    def test_fowlkes_mallows_score_empty_clusters():\n        # Test case with empty clusters in both labelings\n        labels_true = np.array([0, 0, 1, 2, 2])\n        labels_pred = np.array([0, 0, 1, 3, 3])\n        score = fowlkes_mallows_score(labels_true, labels_pred)\n        assert_almost_equal(score, 0.7071, decimal=4)\n\n", "    def test_fowlkes_mallows_score_empty(self):\n        assert_equal(fowlkes_mallows_score([], []), 1.0)\n        assert_equal(fowlkes_mallows_score([0], [0]), 1.0)\n\n        # Test with different number of samples\n        assert_equal(fowlkes_mallows_score([0, 0], [0, 0]), 1.0)\n        assert_equal(fowlkes_mallows_score([0, 0, 0], [0, 0, 0]), 1.0)\n", "    def test_fowlkes_mallows_score_sparse(self):\n        # Test Fowlkes-Mallows score with sparse contingency matrix.\n\n        labels_true = np.array([0, 0, 1, 1])\n        labels_pred = np.array([0, 1, 1, 0])\n        fmi = fowlkes_mallows_score(labels_true, labels_pred, sparse=True)\n        expected_fmi = 0.0\n        assert_almost_equal(fmi, expected_fmi)\n", "    def test_empty_cluster_scores(self):\n        # Test the behavior of the metrics when some clusters are empty.\n        labels_true = np.array([0, 0, 1, 2, 2])\n        labels_pred = np.array([0, 0, 1, 1, 1])\n        for func in score_funcs:\n            with assert_raise_message(ValueError, \"Number of clusters\"):\n                func(labels_true, labels_pred)\n"], "sample_489": ["    def test_bulk_create_with_fields_with_db_columns(self):\n        data = [\n            FieldsWithDbColumns(\n                regular_field=\"regular\",\n                db_column_field=\"db_column\",\n                another_regular_field=\"another_regular\",\n            )\n            for _ in range(3)\n        ]\n        FieldsWithDbColumns.objects.bulk_create(data)\n        self.assertEqual(FieldsWithDbColumns.objects.count(), 3)\n\n", "    def test_bulk_create_with_related_objects_and_prefetch(self):\n        Restaurant.objects.bulk_create(\n            [\n                Restaurant(name=\"Pizza Place 1\", country=self.data[0]),\n                Restaurant(name=\"Pizza Place 2\", country=self.data[1]),\n            ]\n        )\n        with self.assertNumQueries(1):\n            restaurants = Restaurant.objects.prefetch_related(\"country\").all()\n        self.assertEqual(len(restaurants), 2)\n        for restaurant in restaurants:\n            self.assertIsNotNone(restaurant.country)\n", "    def test_bulk_create_with_callable_default(self):\n        class CustomModel(models.Model):\n            value = models.IntegerField(default=lambda: 42)\n\n        with self.assertNumQueries(1):  # One INSERT statement\n            CustomModel.objects.bulk_create([CustomModel() for _ in range(10)])\n\n        self.assertEqual(CustomModel.objects.all().count(), 10)\n", "    def test_bulk_create_with_auto_increment_pk(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"TRUNCATE TABLE dj_test_country CASCADE\")\n        original_id = Country.objects.latest('id').id if Country.objects.exists() else 0\n\n        Country.objects.bulk_create(self.data)\n\n        self.assertEqual(Country.objects.count(), 4)\n        self.assertEqual(Country.objects.filter(name=\"United States of America\").count(), 1)\n\n        # Check that the primary keys are consecutive\n        countries = Country.objects.all()\n        ids = [c.id for c in countries]\n        self.assertEqual(ids, list(range(original_id + 1, original_id + 5)))\n\n", "    def test_bulk_create_with_proxy_models(self):\n        proxy_countries = [\n            ProxyCountry(name=\"United States of America\", iso_two_letter=\"US\"),\n            ProxyMultiCountry(name=\"The Netherlands\", iso_two_letter=\"NL\"),\n            ProxyProxyCountry(name=\"Germany\", iso_two_letter=\"DE\"),\n            ProxyMultiProxyCountry(name=\"Czech Republic\", iso_two_letter=\"CZ\"),\n        ]\n        with transaction.atomic():\n            ProxyCountry.objects.bulk_create(proxy_countries)\n        self.assertEqual(Country.objects.count(), 4)\n\n"], "sample_771": ["    def test_PowerTransformer_inverse_transform_negative_values():\n        # Test inverse transform for negative values with Yeo-Johnson transform\n\n        pt = PowerTransformer(method='yeo-johnson')\n        X = np.array([-1, -2, -3, 0, 1, 2, 3])\n        X_trans = pt.fit_transform(X.reshape(-1, 1))\n        X_inv_trans = pt.inverse_transform(X_trans)\n        assert_allclose(X_inv_trans, X)\n", "    def test_quantile_transform_sparse_constant_column(self):\n        X = sparse.csr_matrix([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n        qt = QuantileTransformer()\n\n        # Check that a warning is raised when a column is constant\n        with pytest.warns(DataConversionWarning):\n            qt.fit_transform(X)\n", "    def test_power_transform_yeo_johnson_sparse(self):\n        X = sparse.csr_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        pt = PowerTransformer(method='yeo-johnson', copy=True)\n        X_transformed = pt.fit_transform(X)\n        assert_allclose_dense_sparse(X_transformed, pt.transform(X))\n\n\n", "    def test_power_transform_inverse_transform_yeo_johnson_sparse(self):\n        # test inverse transform for sparse data with yeo-johnson transform\n        X = sparse.csr_matrix([[1, 2, 3], [-1, 0, 1], [0, 2, -1]])\n        pt = PowerTransformer(method='yeo-johnson', copy=True)\n        X_trans = pt.fit_transform(X)\n        X_inv = pt.inverse_transform(X_trans)\n\n        assert_array_almost_equal(toarray(X_inv), toarray(X))\n", "    def test_power_transform_fit_transform_edge_cases(self):\n        # Test transform on edge cases: empty array, single feature.\n        X_empty = np.array([]).reshape(0, 5)\n        X_single_feature = np.array([[1], [2], [3]])\n        pt = PowerTransformer()\n\n        # Empty array\n        assert_array_equal(pt.fit_transform(X_empty), np.array([]).reshape(0, 5))\n\n        # Single feature\n        X_t = pt.fit_transform(X_single_feature)\n        assert_equal(X_t.shape, (3, 1))\n"], "sample_982": ["    def test_primeomega(self):\n        assert primeomega(1) == 0\n        assert primeomega(2) == 1\n        assert primeomega(4) == 2\n        assert primeomega(6) == 2\n        assert primeomega(12) == 3\n        assert primeomega(20) == 3\n        assert primeomega(30) == 3\n", "    def test_primeomega(self):\n        assert primeomega(1) == 0\n        assert primeomega(2) == 1\n        assert primeomega(4) == 2\n        assert primeomega(6) == 2\n        assert primeomega(12) == 3\n        assert primeomega(20) == 3\n", "def test_primeomega():\n    assert primeomega(1) == 0\n    assert primeomega(2) == 1\n    assert primeomega(4) == 2\n    assert primeomega(6) == 2\n    assert primeomega(12) == 3\n    assert primeomega(30) == 3\n", "def test_core():\n    assert core(24, 2) == 6\n    assert core(9424, 3) == 1178\n    assert core(379238) == 379238\n    assert core(15**11, 10) == 15\n", "    def test_core_simple():\n        assert core(24, 2) == 6\n        assert core(9424, 3) == 1178\n        assert core(379238) == 379238\n        assert core(15**11, 10) == 15\n"], "sample_68": ["    def test_callable_setting_wrapper_repr(self):\n        class CallableSetting:\n                return 'CallableSetting instance'\n        wrapped = CallableSettingWrapper(CallableSetting())\n        self.assertEqual(repr(wrapped), 'CallableSetting instance')\n", "    def test_callable_setting_wrapper_repr(self):\n        class MySetting:\n                return 'MySetting()'\n        wrapped_setting = CallableSettingWrapper(MySetting())\n        self.assertEqual(repr(wrapped_setting), 'MySetting()')\n", "    def test_callable_setting_wrapper(self):\n        wrapper = CallableSettingWrapper(lambda: 'foo')\n        self.assertEqual(repr(wrapper), '<lambda>')\n        \n\n", "    def test_callable_setting_wrapper_repr(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'Wrapped Callable')\n        self.assertEqual(repr(wrapped_callable), '<CallableSettingWrapper object at 0x...>')\n", "    def test_callable_setting_wrapper_repr(self):\n        class Callable:\n                return 'Callable()'\n        \n        wrapped_callable = CallableSettingWrapper(Callable())\n        self.assertEqual(repr(wrapped_callable), 'Callable()')\n"], "sample_357": ["    def test_rename_field_with_default(self):\n        before = self.make_project_state([\n            self.author_with_biography_non_blank\n        ])\n        after = self.make_project_state([\n            self.author_with_biography_renamed\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='bio', old_name='biography')\n\n", "    def test_indexes(self):\n        changes = self.get_changes([\n            self.author_name,\n            self.book,\n        ], [\n            self.author_name,\n            self.book_indexes,\n        ])\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"CreateIndex\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book_title_author_idx\", fields=[\"author\", \"title\"])\n", "    def test_rename_field_with_default(self):\n        # Check renaming a field with a default value\n        before = self.make_project_state([\n            self.author_name,\n        ])\n        after = self.make_project_state([\n            self.author_name_renamed_field,\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='my_name',\n            old_name='name',\n        )\n\n", "    def test_migration_dependency_ordering_circular_fk(self):\n        changes = self.get_changes(\n            [self.knight],\n            [self.rabbit],\n        )\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertMigrationDependencies(changes, 'eggs', 0, [])\n        self.assertOperationTypes(changes, 'eggs', 0, ['CreateModel'])\n\n", "    def test_rename_field_with_default(self):\n        before = [\n            self.author_name_default_value,\n        ]\n        after = [\n            self.author_name_default_value_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name_old=\"name\", name=\"other_name\")\n"], "sample_95": ["    def test_vary_on_headers(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        \n        # Test that vary_on_headers adds the specified headers to the Vary header\n        decorated_view = vary_on_headers('Accept-Language')(lambda x: response)\n        response = decorated_view(request)\n        self.assertEqual(response['Vary'], 'Accept-Language')\n\n        # Test that vary_on_headers appends to an existing Vary header\n        response['Vary'] = 'Cookie'\n        decorated_view = vary_on_headers('Accept-Encoding')(lambda x: response)\n        response = decorated_view(request)\n        self.assertEqual(response['Vary'], 'Cookie, Accept-Encoding')\n", "    def test_cache_page_with_vary(self):\n        response = HttpResponse('Hello, world!')\n        \n        wrapped_view = cache_page(60)(lambda request: response)\n        response1 = wrapped_view(HttpRequest())\n        \n        wrapped_view = vary_on_headers('Accept-Language')(wrapped_view)\n        response2 = wrapped_view(HttpRequest())\n        self.assertEqual(response1, response2)\n", "    def test_cache_control(self):\n        response = fully_decorated(HttpRequest())\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('Cache-Control', response)\n        self.assertEqual(response['Cache-Control'], 'private, max-age=900, no-cache')\n", "    def test_vary_on_headers_empty_headerlist(self):\n        @vary_on_headers()\n            return HttpResponse(\"OK\")\n        request = HttpRequest()\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertFalse('Vary' in response)\n\n", "    def test_vary_on_cookie(self):\n        response = HttpResponse('Test')\n        vary_on_cookie(response)\n        self.assertEqual(response['Vary'], 'Cookie')\n\n"], "sample_687": ["    def test_log_records_in_teardown(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n\n                logger.info(\"Setup\")\n                yield\n                logger.info(\"Teardown\")\n\n                logger.debug(\"Another debug message\")\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n\n        for item in result.session.items:\n            if \"test_function\" in item.name:\n                records = item._store[caplog_records_key][\"teardown\"]\n                assert len(records) == 1\n                assert records[0].levelno == logging.INFO\n                assert records[0].getMessage() == \"Teardown\"\n", "    def test_log_records_with_different_levels(\n        self, testdir: Testdir", "    def test_log_capture_with_handler_level(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.setLevel(logging.DEBUG)  # Explicitly set a handler level\n\n                logger.info(\"test message\")\n\n                logger.warning(\"another message\")\n\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--log-level=WARNING\")\n\n        assert result.ret == 0\n        assert len(result.stdout.lines) == 2\n        assert \"test_other_func\" in result.stdout.str()\n        assert result.stdout.str().count(\"another message\") == 1\n        result = testdir.runpytest()\n        assert \"test_func\" in result.stdout.str()\n        assert result.stdout.str().count(\"test message\") == 1\n\n", "    def test_at_level(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.error(\"First message\")\n        \"\"\"\n        )\n        with testdir.log_cap() as log:\n            testdir.runpytest(\"--log-level=WARNING\", \"-r\", \"f\")\n\n        assert len(log.records) == 1\n        assert log.records[0].levelno == logging.ERROR\n", "    def test_log_level_setting(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n            logger = logging.getLogger(__name__)\n\n                logger.info(\"test message\")\n\n            @pytest.mark.parametrize(\"level,expected_level\", [\n                (\"INFO\", logging.INFO),\n                (\"DEBUG\", logging.DEBUG),\n                (\"WARNING\", logging.WARNING),\n            ])\n                testdir.runpytest(\n                    \"--log-level\", level,\n                )\n                assert testdir.stdout.str().find(\"test message\") != -1\n                assert testdir.items[0].log_records[0].levelno == expected_level\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n"], "sample_850": ["    def test_nystroem_kernel_params():\n        # Test that Nystroem passes kernel params correctly\n        n_components = 10\n        # Test a non-standard kernel\n        kernel_params = {\"gamma\": .5, \"coef0\": 1.5, \"degree\": 3}\n        nystroem = Nystroem(kernel=\"polynomial\", kernel_params=kernel_params,\n                           n_components=n_components, random_state=0)\n        nystroem.fit(X)\n        # Check if kernel params are passed correctly\n        assert_array_almost_equal(\n            nystroem._get_kernel_params(), kernel_params\n        )\n\n", "    def test_nystroem_sparse_input(self):\n        nystroem = Nystroem(n_components=10, random_state=0)\n        X_sparse = csr_matrix(X)\n        nystroem.fit(X_sparse)\n        X_transformed = nystroem.transform(X_sparse)\n        assert isinstance(X_transformed, np.ndarray)\n\n", "def test_nystroem_sparse_input():\n    \"\"\"Check that Nystroem works with sparse input.\"\"\"\n    n_samples = 10\n    n_features = 5\n    X = csr_matrix(np.random. rand(n_samples, n_features))\n    nystroem = Nystroem(random_state=0)\n    nystroem.fit(X)\n    nystroem.transform(X)\n", "    def test_nystroem_sparse_input(self):\n        # Test Nystroem with sparse input\n        sparse_X = csr_matrix(X)\n        nystroem = Nystroem(kernel='rbf', n_components=100, random_state=0)\n        transformed_X = nystroem.fit_transform(sparse_X)\n        assert_raises(ValueError, nystroem.transform, X)\n", "    def test_nystroem_sparse_input_precomputed_kernel():\n        # Test Nystroem with sparse input and precomputed kernel\n        X_sparse = csr_matrix(X)\n\n        nystroem = Nystroem(kernel='precomputed', n_components=100)\n        embedding = nystroem.fit_transform(X_sparse)\n\n        assert embedding.shape == (X.shape[0], 100)\n"], "sample_301": ["    def test_iter_modules_and_files_with_zip(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('my_module/__init__.py', '')\n            sys.path.append(tempdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path)\n            sys.path.pop()\n", "    def test_zipfile_modules(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'test.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                script_content = 'print(\"Hello from zip file\")'\n                zipf.writestr('my_module.py', script_content)\n\n            sys.path.insert(0, str(tempdir))\n            self.addCleanup(lambda: sys.path.pop(0))\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n\n", "    def test_iter_modules_and_files_zip_imported(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_file, 'w') as zf:\n            zf.writestr('module.py', 'print(\"Hello from zip file\")')\n        sys.path.append(str(zip_file.parent))\n        self.addCleanup(sys.path.remove, str(zip_file.parent))\n        self.import_and_cleanup('module')\n        self.assertFileFound(zip_file)\n        self.clear_autoreload_caches()\n\n", "    def test_iter_modules_and_files_zip_file(self):\n        with self.temporary_file('test_module.zip') as zip_file:\n            with zipfile.ZipFile(zip_file, 'w') as z:\n                z.writestr('test_module/__init__.py', '')\n                z.writestr('test_module/test_file.py', '')\n            autoreload.sys_path_directories()\n            self.assertFileFound(zip_file.parent / 'test_module.zip')\n            self.assertFileFound(zip_file.parent / 'test_module.zip')\n            with open(zip_file, 'rb') as f:\n                py_compile.compile(f, doraise=True)\n\n            self.assertFileFound(zip_file.parent / 'test_module.zip')\n\n\n\n", "    def test_watch_dir_with_zipfile(self):\n        with self.settings(ROOT_URLCONF='test_module.urls'):\n            test_dir = self.temporary_file('test_dir')\n            zip_file = test_dir / 'test_package.zip'\n\n            with zipfile.ZipFile(zip_file, 'w') as zf:\n                zf.writestr('test_module/__init__.py', '')\n                zf.writestr('test_module/views.py', 'def my_view(): pass')\n\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(test_dir / 'test_package.zip')\n\n            autoreload.StatReloader().watch_dir(test_dir, 'test_package.zip')\n            self.assertFileFound(test_dir / 'test_package.zip')\n"], "sample_1020": ["    def test_Sum():\n        assert mcode(Sum(x**2, (x, 1, 10))) == \"Hold[Sum[{x^2}, {x, 1, 10}]]\"\n", "    def test_derivative():\n        assert mcode(Derivative(sin(x), x)) == \"Hold[D[Sin[x],x]]\"\n        assert mcode(Derivative(f(x), x)) == \"Hold[D[f[x],x]]\"\n        assert mcode(Derivative(sin(x), x, 2)) == \"Hold[D[Sin[x],{x,2}]]\"\n", "    def test_print_MinMaxBase():\n        assert mcode(Max(x, y)) == 'Max[x, y]'\n        assert mcode(Min(x, y)) == 'Min[x, y]'\n", "    def test_print_Sum(self):\n        self.assertEqual(mcode(Sum(x, (x, 1, 5))), 'Hold[Sum[x, {x, 1, 5}]]')\n", "    def test_print_MaxMin(self):\n        assert mcode(Max(x, y)) == 'Max[x, y]'\n        assert mcode(Min(x, y)) == 'Min[x, y]'\n        assert mcode(Max(x, y, z)) == 'Max[x, y, z]'\n"], "sample_356": ["    def test_field_rename_across_apps(self):\n        \"\"\"\n        Tests renaming a field across apps.\n        \"\"\"\n        before = [\n            self.author_name,\n            self.book_with_author,\n        ]\n        after = [\n            self.author_renamed_name,\n            self.book_with_author_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, ['RenameField'])\n        self.assertOperationTypes(changes, \"otherapp\", 0, ['AlterField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name_old='name', name_new='new_name')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 0, field=self.fields.author)\n", "    def test_remove_field_with_default(self):\n        before = self.make_project_state([\n            self.author_field_default,\n        ])\n        after = self.make_project_state([\n            self.author_no_field,\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RemoveField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='field_default')\n", "    def test_renamed_model_with_m2m_through(self):\n        \"\"\"\n        Tests renaming a model which has a ManyToManyField through another model.\n        \"\"\"\n        before = [\n            self.author_with_m2m_through,\n        ]\n        after = [\n            self.author_with_renamed_m2m_through,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\n            'RenameModel',\n        ])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                        old_name='Author',\n                                        new_name='Authi')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1,\n                                        name='RenameModel',\n                                        old_name='Author',\n                                        new_name='Authi')\n", "    def test_field_rename_with_default(self):\n        before = self.make_project_state([self.author_name])\n\n        after = self.make_project_state([self.author_name_renamed])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name_old='name', name=\n        'full_name')\n\n", "    def test_rename_field_with_unique_together(self):\n        before_state = ProjectState()\n        before_state.add_model(self.book_foo_together)\n        after_state = ProjectState()\n        after_state.add_model(self.book_foo_together_3)\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, 'otherapp', 1)\n        self.assertOperationTypes(changes, 'otherapp', 0, ['RenameField', 'AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'otherapp', 0, 1, name='newfield')\n        self.assertOperationAttributes(changes, 'otherapp', 0, 2, name='unique_together',\n\n"], "sample_1182": ["    def test_print_MatrixSolve(self):\n        n = symbols('n', integer=True)\n        A = MatrixSymbol('A', n, n)\n        b = MatrixSymbol('b', n, 1)\n        x = MatrixSymbol('x', n, 1)\n        sol = MatrixSolve(A, b)\n        code = SymPyPrinter().doprint(sol)\n        assert code == 'sympy.solveset(A*x - b, x)'\n", "def test_print_MatrixSolve():\n    A = MatrixSymbol('A', 2, 2)\n    b = MatrixSymbol('b', 2, 1)\n    eq = MatrixSolve(A, b)\n    assert pycode(eq) == 'sympy.solve(A, b)'\n", "def test_print_FunctionDefinition_with_return():\n    expr = FunctionDefinition(\n        name='f',\n        parameters=[x],\n        body=[\n            Return(x**2)\n        ]\n    )\n    expected = \"\"\"def f(x):\n    return x**2\"\"\"\n\n    assert pycode(expr) == expected\n", "def test_print_Piecewise():\n    expr = Piecewise((1, Eq(x,1)), (2, Eq(x, 2)), (3, True)) \n    assert pycode(expr) == '((1 if x == 1 else (2 if x == 2 else 3)))'\n", "    def test_print_FunctionDefinition(self):\n        a, b = symbols('a b')\n        f = FunctionDefinition(Eq(x*y, a*b))\n        s = SymPyPrinter().doprint(f)\n        assert s == \"def _sympify(a, b):\\n    return x*y - a*b\"\n\n"], "sample_258": ["    def test_signal_weakref(self):\n        # Clean up any leftover garbage from previous tests\n        garbage_collect()\n            pass\n\n        ref = weakref.ref(receiver_func)\n        a_signal.connect(receiver_func)\n        self.assertTrue(a_signal.has_listeners())\n        del receiver_func\n        garbage_collect()\n        self.assertFalse(a_signal.has_listeners())\n        self.assertIsNone(ref())\n", "    def test_receiver_garbage_collection(self):\n\n            pass\n\n        weak_ref = weakref.ref(myreceiver)\n        a_signal.connect(myreceiver)\n        del myreceiver\n        garbage_collect()\n        self.assertIsNone(weak_ref())\n        self.assertTestIsClean(a_signal)\n\n", "    def test_signal_disconnect_with_dispatch_uid(self):\n            pass\n\n            pass\n\n        a_signal.connect(signal_receiver_1, dispatch_uid='uid1')\n        a_signal.connect(signal_receiver_2, dispatch_uid='uid2')\n\n        self.assertEqual(len(a_signal.receivers), 2)\n        \n        a_signal.disconnect(dispatch_uid='uid1')\n        self.assertEqual(len(a_signal.receivers), 1)\n\n        a_signal.disconnect(dispatch_uid='uid2')\n        self.assertEqual(len(a_signal.receivers), 0)\n", "    def test_disconnect_weakref(self):\n        class Receiver:\n                self.called = False\n\n                self.called = True\n\n        receiver = Receiver()\n        a_signal.connect(receiver)\n        self.assertTrue(a_signal.has_listeners())\n        del receiver\n        garbage_collect()\n        self.assertFalse(a_signal.has_listeners())\n", "    def test_disconnect_weakref_garbage_collected(self):\n            pass\n\n        weak_ref_receiver = weakref.ref(receiver_func)\n        a_signal.connect(weak_ref_receiver, weak=True)\n        self.assertTrue(a_signal.has_listeners())\n\n        del receiver_func\n        garbage_collect()\n        self.assertFalse(a_signal.has_listeners())\n"], "sample_197": ["    def test_timesince_aware_datetime(self):\n        with timezone.override(timezone.get_current_timezone()):\n            aware_t = timezone.make_aware(self.t)\n            self.assertEqual(timesince(aware_t, self.t), '0 minutes')\n            self.assertEqual(timesince(self.t, aware_t), '0 minutes')\n", "    def test_timesince_midnight(self):\n        with translation.override('en-us'):\n            # Test with different aware timezones\n            for tzinfo in (timezone.utc, timezone.get_default_timezone()):\n                d = timezone.datetime(2007, 8, 14, 0, 0, 0, tzinfo=tzinfo)\n                self.assertEqual(timesince(d), '0 minutes')\n                self.assertEqual(timesince(d, depth=1), '0 minutes')\n                self.assertEqual(timesince(d, depth=2), '0 minutes')\n\n", "    def test_timesince_aware_datetimes(self):\n        with translation.override('en'):\n            t = timezone.make_aware(self.t, timezone.utc)\n            now = timezone.now()\n            self.assertEqual(timesince(t, now), timesince(t.replace(tzinfo=None), now.replace(tzinfo=None)))\n", "    def test_timesince_aware_datetime(self):\n        with translation.override('en-us'):\n            now = timezone.make_aware(self.t)\n            past = now - self.oneday\n            self.assertEqual(timesince(past, now), '1 day')\n", "    def test_timesince_aware_datetime(self):\n        with translation.override('en'):\n            self.assertEqual(\n                timesince(\n                    self.t.replace(tzinfo=timezone.utc),\n                    now=self.t.replace(tzinfo=timezone.utc) + self.oneminute\n                ),\n                '1 minute',\n            )\n\n\n\n"], "sample_364": ["    def test_include_with_namespace(self):\n        self.assertEqual(\n            resolve('/base64/aGVsbG8=/namespaced/d29ybGQ=/').view_name,\n            'subpattern-base64'\n        )\n        self.assertEqual(\n            resolve('/base64/aGVsbG8=/namespaced/d29ybGQ=/').namespace,\n            'namespaced-base64'\n        )\n", "    def test_include_with_namespace_and_appname_in_module(self):\n        with self.assertRaises(ImproperlyConfigured):\n            include(\n                ('urlpatterns.include_test_urls', 'my_app'),\n                namespace='my_namespace',\n            )\n", "    def test_include_with_namespace_and_app_name(self):\n        response = self.client.get('/namespaced/hello/')\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.content, b'hello')\n        self.assertEqual(resolve('/namespaced/hello/').view_name, 'namespaced-app:hello')\n", "    def test_include_dynamic_module_with_namespace(self):\n        from .path_urls import urlpatterns as original_urlpatterns\n\n        with self.assertRaises(ImproperlyConfigured):\n            include((original_urlpatterns, 'my_app'), namespace='my_namespace')\n", "    def test_include_invalid_tuple(self):\n        with self.assertRaisesMessage(ImproperlyConfigured,\n                                     'Passing a 1-tuple to include() is not supported. Pass a '\n                                     '2-tuple containing the list of patterns and app_name, and '\n                                     'provide the namespace argument to include() instead.'):\n            resolve(reverse('include_test', kwargs={'url_name': 'single_tuple'}))\n"], "sample_366": ["    def test_parse_datetime_with_postgres_timezone(self):\n        \"\"\"\n        Tests parsing datetime strings with PostgreSQL timezone format.\n        \"\"\"\n        dt_str = '2023-10-27 14:30:00+02'\n        expected_dt = datetime(2023, 10, 27, 14, 30, tzinfo=get_fixed_timezone(120))\n        self.assertEqual(parse_datetime(dt_str), expected_dt)\n", "    def test_parse_datetime_with_timezone(self):\n        # Test datetime parsing with timezone information.\n        dt_str = '2023-10-27T12:34:56+02:00'\n        parsed_dt = parse_datetime(dt_str)\n        self.assertEqual(parsed_dt, datetime(2023, 10, 27, 12, 34, 56, tzinfo=get_fixed_timezone(120)))\n\n", "    def test_parse_datetime_tzinfo(self):\n        # Test parsing with various timezone offsets\n        self.assertEqual(\n            parse_datetime('2023-10-26T12:00:00Z'),\n            datetime(2023, 10, 26, 12, 0, 0, tzinfo=utc),\n        )\n        self.assertEqual(\n            parse_datetime('2023-10-26T12:00:00+01:00'),\n            datetime(2023, 10, 26, 12, 0, 0, tzinfo=get_fixed_timezone(60)),\n        )\n        self.assertEqual(\n            parse_datetime('2023-10-26T12:00:00-05:30'),\n            datetime(2023, 10, 26, 12, 0, 0, tzinfo=get_fixed_timezone(-330)),\n        )\n", "    def test_parse_datetime_with_timezone(self):\n        aware_dt = parse_datetime(\"2023-10-27T12:34:56+03:00\")\n        self.assertEqual(aware_dt, datetime(2023, 10, 27, 12, 34, 56, tzinfo=get_fixed_timezone(180)))\n        aware_dt = parse_datetime(\"2023-10-27T12:34:56Z\")\n        self.assertEqual(aware_dt, datetime(2023, 10, 27, 12, 34, 56, tzinfo=utc))\n", "    def test_parse_duration_iso_8601(self):\n        self.assertEqual(parse_duration('P3DT4H15M'), timedelta(days=3, hours=4, minutes=15))\n        self.assertEqual(parse_duration('-P1DT2H'), timedelta(days=-1, hours=-2))\n        self.assertEqual(parse_duration('PT10S'), timedelta(seconds=10))\n"], "sample_1035": ["    def test_grover_search_2qubits():\n        qbit = IntQubit(0, nqubits=2)\n        oracle = OracleGate(return_one_on_two, 2)\n        n = 2\n        qbit = apply_grover(qbit, oracle, n)\n        assert qbit == IntQubit(2, nqubits=2)\n", "def test_measure_partial_oneshot():\n    q = qapply(IntQubit(0, nqubits=3))\n    assert measure_partial_oneshot(q, (2,), format='sympy') == IntQubit(0, nqubits=3)\n\n    q = qapply(H(0)*H(1)*IntQubit(0, nqubits=2))\n    assert measure_partial_oneshot(q, (1,), format='sympy') in [IntQubit(0, nqubits=2), IntQubit(2, nqubits=2)]\n\n", "def test_measure_all_oneshot_simple():\n    q = Qubit('01')\n    result = measure_all_oneshot(q)\n    assert isinstance(result, IntQubit)\n    assert result in [IntQubit(0), IntQubit(3)]\n", "compilation error", "    def test_measure_partial_oneshot_with_oracle():\n        q = IntQubit(0, nqubits=3)\n        oracle = OracleGate(return_one_on_two, 3)\n        q = qapply(oracle*q)\n        result = measure_partial_oneshot(q, (0, 1))\n        assert result == IntQubit(2, nqubits=3) or result == IntQubit(3, nqubits=3)\n"], "sample_671": ["    def test_evalxfail_istrue_with_condition(self):\n        item = runtestprotocol.Item.from_parent(None, \"foo\")\n        item._store[evalxfail_key] = MarkEvaluator(item, \"xfail\", {\"condition\": \"sys.version_info > (3, 8)\"})\n        assert item._store[evalxfail_key].istrue()\n", "    def test_evalxfail_key_is_set(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(reason=\"intentional xfail\")\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.report.sections[1].tests[0].name == \"test_func\"\n        assert \"reason: intentional xfail\" in str(result)\n\n", "    def test_pytest_runtest_setup_skipif_with_false_condition(self):\n        item = runtestprotocol.Item.from_parent(None, self.fspath.join(\"test_module.py\"), \"test_function\")\n        item._markernames = [\"skipif\"]\n        item._args = ['False']\n        pytest_runtest_setup(item)\n        assert not item._store[skipped_by_mark_key]\n", "    def test_evalxfail_key_stored_and_retrieved(self):\n        item = runtestprotocol.Function(\n            lambda: None, config=pytest.Config(), name=\"test_func\", nodeid=\"test_func\"\n        )\n        evalxfail = MarkEvaluator(item, \"xfail\")\n        item._store[pytest.skipping.evalxfail_key] = evalxfail\n        retrieved_evalxfail = item._store[pytest.skipping.evalxfail_key]\n        assert retrieved_evalxfail == evalxfail\n", "    def test_evalxfail_raises(self):\n        item = runtestprotocol.Item.from_call(\n            lambda: None,\n            \"test_func\",\n            None,\n            config=pytest.Config(),\n            nodeid=\"test_module.py::test_func\",\n        )\n        item._store[evalxfail_key] = MarkEvaluator(item, 'xfail', {'raises': [ValueError]})\n        check_xfail_no_run(item)\n        # xfail should not be considered as run\n\n        pytest_runtest_setup(item)\n        with pytest.raises(ValueError):\n            item.runtest()\n"], "sample_952": ["    def test_stringify_signature_with_forwardref(self):\n        class MyType:\n            pass\n\n        # https://github.com/sphinx-doc/sphinx/issues/7935\n        sig = inspect.signature(\n            lambda x: x, type_aliases={'MyType': 'typing.List[int]'})\n        result = inspect.stringify_signature(sig)\n        assert result == '(x)'\n\n        sig = inspect.signature(\n            lambda x: x,\n            type_aliases={'MyType': 'MyType'})\n        result = inspect.stringify_signature(sig)\n        assert result == '(x)'\n", "def test_get_type_hints_forwarded_ref():\n    class A:\n        pass\n\n    class B(A):\n        pass\n\n    type_aliases = {'mod1.B': 'mod2.C'}\n    namespace = TypeAliasNamespace(type_aliases)\n\n    assert inspect.get_type_hints(B, None, namespace) == {}\n", "    def test_signature_from_ast_posonlyargs(self):\n        code = \"\"\"\n        \"\"\"\n        module = ast.parse(code)\n        function = cast(ast.FunctionDef, module.body[0])\n        sig = inspect.signature_from_ast(function, code)\n        assert sig.parameters[0].kind == Parameter.POSITIONAL_ONLY\n        assert sig.parameters[1].kind == Parameter.POSITIONAL_ONLY\n        assert sig.parameters[2].kind == Parameter.KEYWORD_ONLY\n        assert sig.parameters[3].kind == Parameter.KEYWORD_ONLY\n\n", "    def test_stringify_signature_with_empty_return_annotation(self):\n        sig = inspect.Signature(\n            [Parameter('a', inspect.Parameter.POSITIONAL_OR_KEYWORD)],\n            return_annotation=inspect.Parameter.empty)\n        self.assertEqual(inspect.stringify_signature(sig), '(a)')\n\n", "    def test_signature_from_ast_with_posonlyargs():\n        code = 'def func(a, b, /, c, d=42, *args, **kwargs): pass'\n        module = ast.parse(code)\n        function = module.body[0]  # type: ignore\n        sig = inspect.signature_from_ast(function, code)\n        assert str(sig) == '(a, b, /, c, d=42, *args, **kwargs)'\n\n"], "sample_953": ["def test_quickstart_overwrite(tmpdir):\n    d = {\n        'path': str(tmpdir),\n        'sep': False,\n        'dot': '_',\n        'project': 'Test Project',\n        'author': 'Test Author',\n        'version': '1.0',\n        'release': '1.0',\n        'suffix': '.rst',\n        'master': 'index',\n        'extensions': [],\n        'makefile': True,\n        'batchfile': True,\n    }\n\n    qs.generate(d, overwrite=False)\n    assert path.exists(path.join(d['path'], 'conf.py'))\n    assert path.exists(path.join(d['path'], 'index.rst'))\n\n    qs.generate(d, overwrite=True)\n    # Files should be overwritten\n    assert path.exists(path.join(d['path'], 'conf.py'))\n    assert path.exists(path.join(d['path'], 'index.rst'))\n", "def test_quickstart_overwrite(tmpdir):\n    # check if overwrite flag works\n    answers = {\n        'Path for documentation': str(tmpdir),\n        'Separate source and build dirs': 'y',\n        'Project name': 'test',\n        'Author name(s)': 'Me',\n        'Project version': '1.0',\n        'Project release': '1.0',\n        'Language (en)': 'en',\n        'Source file suffix': '.rst',\n        'Name of your master document (without suffix)': 'index',\n        'sphinx.ext.autodoc': 'y',\n        'sphinx.ext.viewcode': 'y',\n        'Create Makefile?': 'y',\n        'Create Windows command file?': 'y',\n    }\n\n    qs.term_input = mock_input(answers)\n    qs.generate(vars(qs.get_parser().parse_args(['-q'])))\n\n    # create a second set of answers, but overwrite the existing files\n    answers2 = {\n        'Path for documentation': str(tmpdir),\n        'Separate source and build dirs': 'y',\n        'Project name': 'test2',\n        'Author name(s)': 'Me2',\n        'Project version': '2.0',\n        'Project release': '2.0',\n        'Language (en)': 'en',\n        'Source file suffix': '.rst',\n        'Name of your master document (without suffix)': 'index2',\n        'sphinx.ext.autodoc': 'y',\n        'sphinx.ext.viewcode': 'y',\n        'Create Makefile?': 'y',\n        'Create Windows command file?': 'y',\n    }\n\n    qs.term_input = mock_input(answers2)\n    qs.generate(vars(qs.get_parser().parse_args(['-q', '-o'])))\n\n    assert path.exists(path.join(tmpdir, 'source', 'conf.py'))\n    assert path.exists(path.join(tmpdir, 'source', 'index.rst'))\n\n    with open(path.join(tmpdir, 'source', 'conf.py'), 'r') as f:\n        content = f.read()\n\n    assert 'project = \"test2", "def test_quickstart_default_values(capsys):\n    answers = {'project': 'Foo', 'author': 'Me', 'suffix': '.rst',\n               'version': '1.0', 'release': '1.0'}\n    qs.term_input = mock_input(answers)\n    qs.main(['-q', 'test_project'])\n    out, err = capsys.readouterr()\n    assert 'quiet mode' in err\n\n\n", "def test_quickstart_overwrite_existing_files(tmpdir):\n    qs.term_input = mock_input({'path': str(tmpdir), 'sep': 'y'})\n    qs.generate(qs.DEFAULTS.copy(), overwrite=True)\n    \n    # Create a new conf.py to simulate existing one\n    with open(path.join(tmpdir, 'source', 'conf.py'), 'w'):\n        pass\n    \n    qs.term_input = mock_input({'path': str(tmpdir), 'sep': 'y'})\n    qs.generate(qs.DEFAULTS.copy(), overwrite=False)\n\n    # Assert that conf.py was not overwritten\n    assert path.exists(path.join(tmpdir, 'source', 'conf.py'))\n\n", "    def test_quickstart_interactive_no_overwrite(tmpdir):\n        answers = {\n            'Project name': 'My Project',\n            'Author name(s)': 'Me',\n            'Project version': '1.0',\n            'Project release': '1.0',\n            'Document language': '',\n            'Source file suffix': '.rst',\n            'Name of your master document (without suffix)': 'index',\n            'Do you want to use epub extension? (y/n)': 'n',\n        }\n        # Create mock input function with answers\n        qs.term_input = mock_input(answers)\n\n        # Run quickstart\n        quickstart_dir = tmpdir.mkdir(\"quickstart\")\n        qs.generate({'path': str(quickstart_dir)}, overwrite=False)\n\n        # Assertions for specific files and content\n        assert qs.valid_dir({'path': str(quickstart_dir)})\n        assert path.exists(path.join(str(quickstart_dir), 'source', 'conf.py'))\n        assert not path.exists(path.join(str(quickstart_dir), 'Makefile'))\n\n        # Remove mock input function\n        qs.term_input = real_input\n\n"], "sample_1077": ["def test_complexregion_contains_polar():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    cr = ComplexRegion(a*b, polar=True)\n    assert 0.5*(cos(1) + I*sin(1)) in cr\n    assert 1.5*(cos(1) + I*sin(1)) not in cr\n\n", "def test_complexregion_polar_intersection():\n    r = Interval(0, 1)\n    theta1 = Interval(0, S.Pi)\n    theta2 = Interval(S.Pi/2, 3*S.Pi/2)\n    c1 = ComplexRegion(r*theta1, polar=True)\n    c2 = ComplexRegion(r*theta2, polar=True)\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(ProductSet(Interval(0, 1), Interval(S.Pi/2, S.Pi)), True)\n\n", "    def test_complexregion_intersection():\n        a = Interval(2, 3)\n        b = Interval(4, 5)\n        c = Interval(1, 7)\n        C1 = ComplexRegion(a*b)\n        C2 = ComplexRegion(c*b)\n        intersection = C1.intersect(C2)\n        assert intersection == ComplexRegion(a.intersect(c)*b)\n", "    def test_complexregion_intersection(self):\n        a = Interval(0, 1)\n        b = Interval(0, 2*S.Pi)\n        c = Interval(S.Pi/2, 3*S.Pi/2)\n        unit_disk = ComplexRegion(a*b, polar=True)\n        upper_half_unit_disk = ComplexRegion(a*c, polar=True)\n        intersection = unit_disk.intersect(upper_half_unit_disk)\n        assert intersection == ComplexRegion(ProductSet(a, c), True)\n\n", "    def test_ComplexRegion_polar_intersect():\n        r = Interval(0, 1)\n        theta = Interval(0, 2*S.Pi)\n        unit_disk = ComplexRegion(r*theta, polar=True)\n        upper_half_unit_disk = ComplexRegion(r*Interval(0, S.Pi), polar=True)\n        intersection = unit_disk.intersect(upper_half_unit_disk)\n        assert intersection == ComplexRegion(ProductSet(r, Interval(0, S.Pi)), True)\n\n"], "sample_216": ["    def test_deconstructible_field_with_args(self):\n        changes = self.get_changes(\n            [\n                self.author_name_deconstructible_list_1,\n                self.book_with_author_renamed,\n            ],\n            [\n                self.author_name_deconstructible_list_2,\n                self.book_with_author_renamed,\n            ],\n        )\n        self.assertNumberMigrations(\"testapp\", 1)\n        self.assertOperationTypes(\"testapp\", 0, [\n            'AlterField',\n        ])\n        self.assertOperationAttributes(\"testapp\", 0, 0, field='name',\n                                     )\n\n", "    def test_autodetector_rename_field_with_default(self):\n        before_states = [\n            self.author_name_default,\n        ]\n        after_states = [\n            self.author_name_default_renamed,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name',\n                                       old_name='name', field_type=models.CharField)\n\n", "    def test_add_unique_together_after_fk_creation(self):\n        before = self.make_project_state([\n            self.book,\n        ])\n        after = self.make_project_state([\n            self.book_foo_together_3,\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AddField\", \"AlterUniqueTogether\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name='newfield')\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 2, name='Book', unique_together=[('title', 'newfield')])\n\n", "    def test_custom_pk_rename(self):\n        before = ProjectState()\n        before.add_model(self.author_unmanaged_default_pk.clone())\n\n        after = ProjectState()\n        after.add_model(self.author_unmanaged_custom_pk.clone())\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, new_field_name=\"pk_field\")\n\n", "    def test_circular_dependencies(self):\n        changes = self.get_changes(\n            [\n                self.knight,\n                self.rabbit,\n            ],\n            [\n                self.knight,\n                self.rabbit,\n            ],\n        )\n\n        self.assertNumberMigrations(changes, 'eggs', 0)\n\n"], "sample_120": ["    def test_serialize_lazy_object(self):\n        lazy_object = SimpleLazyObject(lambda: 'test')\n        serialized, imports = serializer_factory(lazy_object).serialize()\n        self.assertEqual(serialized, repr('test'))\n        self.assertEqual(imports, set())\n", "    def test_serialize_lazy_object(self):\n        lazy_string = SimpleLazyObject(lambda: 'lazy string')\n        serializer = serializer_factory(lazy_string)\n        self.assertEqual(serializer.serialize(), (\"'lazy string'\", set()))\n", "    def test_serialize_email_validator(self):\n        validator = EmailValidator()\n        serialized = serializer_factory(validator).serialize()\n        self.assertEqual(serialized, (\n            'EmailValidator()',\n            {'from django.core.validators import EmailValidator'}\n        ))\n", "    def test_serialize_lazy_object(self):\n        lazy_value = SimpleLazyObject(lambda: 'hello')\n        serialized, imports = serializer_factory(lazy_value).serialize()\n        self.assertEqual(serialized, repr('hello'))\n        self.assertEqual(imports, set())\n", "    def test_filefield_upload_to_function(self):\n        writer = OperationWriter([\n            migrations.CreateModel(\n                name='TestModel1',\n                fields=[\n                    models.FileField(upload_to=TestModel1.upload_to),\n                ],\n            ),\n        ], indentation=4)\n        expected = \"\"\"\n        migrations.CreateModel(\n            name='TestModel1',\n            fields=[\n                ('thing', models.FileField(upload_to='<function TestModel1.upload_to at 0x%x>')),\n            ],\n        ),\n        \"\"\" % id(TestModel1.upload_to)\n        self.assertEqual(writer.serialize(), expected)\n"], "sample_869": ["    def test_precision_recall_fscore_support_empty_labels():\n        y_true = [1, 2, 3, 1, 2]\n        y_pred = [1, 1, 1, 2, 2]\n        with pytest.raises(ValueError):\n            precision_recall_fscore_support(y_true, y_pred, labels=[])\n", "def test_brier_score_loss_with_labels():\n    # Test brier_score_loss with labels for multiclass and multilabel problems\n    y_true = [0, 1, 2, 1, 0]\n    y_prob = [[0.1, 0.8, 0.1], [0.2, 0.3, 0.5], [0.7, 0.1, 0.2],\n              [0.3, 0.6, 0.1], [0.9, 0.05, 0.05]]\n    labels = [0, 1, 2]\n    score = brier_score_loss(y_true, y_prob, pos_label=None, labels=labels)\n    assert_almost_equal(score, 0.28)\n\n    y_true = [[0, 1], [1, 0], [1, 1], [0, 0], [0, 1]]\n    y_prob = [[0.1, 0.9], [0.8, 0.2], [0.6, 0.4], [0.9, 0.1], [0.2, 0.8]]\n    labels = [0, 1]\n    score = brier_score_loss(y_true, y_prob, pos_label=None, labels=labels)\n    assert_almost_equal(score, 0.3)\n", "def test_brier_score_loss_multiclass():\n    # Check that Brier score loss raises an error for multiclass problems\n    y_true = np.array([0, 1, 2, 0])\n    y_prob = np.array([[0.1, 0.2, 0.7],\n                       [0.3, 0.6, 0.1],\n                       [0.5, 0.3, 0.2],\n                       [0.8, 0.1, 0.1]])\n\n    with pytest.raises(ValueError, match=\"Only binary classification is supported\"):\n        brier_score_loss(y_true, y_prob)\n", "    def test_brier_score_loss_edges(self):\n        y_true = np.array([0, 0, 1, 1])\n        y_prob = np.array([0, 0.5, 0.5, 1])\n        score = brier_score_loss(y_true, y_prob)\n        assert_almost_equal(score, 0.125)\n\n", "def test_brier_score_loss_binary_weights():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    sample_weight = np.array([0.5, 1, 1.5, 0.5])\n    score = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n    assert_almost_equal(score, 0.044)\n"], "sample_647": ["    def test_warn_explicit_for(self, pytester: Pytester, warning_class: type) -> None:\n            pass\n\n        pytester.makepyfile(\n            \"\"\"\n            import warnings\n\n                pass\n            \"\"\"\n        )\n        with warnings.catch_warnings(record=True) as rec:\n            #  Trigger the warning\n            warning_types.warn_explicit_for(myfunc, warning_class(\"message\"))\n        assert len(rec) == 1\n        assert isinstance(rec[0].message, warning_class)\n        assert str(rec[0].message) == \"message\"\n", "    def test_warning_class_has_correct_module(self, warning_class: type[warning_types.PytestWarning]) -> None:\n        assert warning_class.__module__ == \"pytest\"\n", "    def test_warn_explicit_for(self, warning_class: type, pytester: Pytester) -> None:\n            pass\n\n        myfunc.__module__ = \"mymodule\"\n        myfunc.__code__ = type(myfunc.__code__)(\n            co_firstlineno=123, args=myfunc.__code__.co_args, varargs=myfunc.__code__.co_varargs,\n            kwargs=myfunc.__code__.co_kwargs, names=myfunc.__code__.co_names,\n            nlocals=myfunc.__code__.co_nlocals, flags=myfunc.__code__.co_flags,\n            codestring=myfunc.__code__.co_code, constants=myfunc.__code__.co_consts,\n            filename=\"myfile.py\"\n        )\n        \n        with pytest.raises(type(warning_class(f\"my message\"))):\n            warning_types.warn_explicit_for(myfunc, warning_class(f\"my message\"))\n\n", "    def test_warn_explicit_for(self, warning_class: type[warning_types.PytestWarning]) -> None:\n            pass\n\n        with pytest.raises(warning_class):\n            warning_types.warn_explicit_for(func, warning_class())\n", "    def test_warning_class_has_correct_module(self, warning_class: Type[warning_types.PytestWarning]) -> None:\n        assert warning_class.__module__ == \"pytest\"\n"], "sample_1126": ["    def test_Dagger_conjugate_transpose():\n        m = Matrix([[1, I], [2, I]])\n        assert Dagger(m) == m.conjugate().transpose()\n", "    def test_dagger_complex():\n        x = symbols('x')\n        assert Dagger(x*I) == x*conjugate(I)  \n", "    def test_dagger_mul():\n        A = Operator('A')\n        B = Operator('B')\n        assert Dagger(A * B) == Dagger(B) * Dagger(A)\n", "    def test_dagger_mul():\n        A = Operator('A')\n        B = Operator('B')\n        assert Dagger(A*B) == Dagger(B)*Dagger(A)\n", "    def test_dagger_mul():\n        A = Operator('A')\n        B = Operator('B')\n        assert Dagger(A*B) == Dagger(B)*Dagger(A)\n"], "sample_242": ["    def test_custom_lookup_process_lhs(self):\n        lookup = CustomLookup(Value(1), Value(2))\n        with mock.patch('django.db.models.lookups.Lookup.get_prep_lookup') as mock_method:\n            lookup.process_lhs(None, None)\n            mock_method.assert_called_once_with()\n", "    def test_year_lookup_bounds(self):\n        lookup = YearLookup(Value(DateTimeField()), 2023)\n        with mock.patch('django.db.backends.base.DatabaseOperations.year_lookup_bounds_for_datetime_field') as mock_method:\n            mock_method.return_value = (datetime(2023, 1, 1, 0, 0, 0), datetime(2024, 1, 1, 0, 0, 0))\n            bounds = lookup.year_lookup_bounds(mock.MagicMock(), 2023)\n            mock_method.assert_called_once_with(2023)\n            self.assertEqual(bounds, (datetime(2023, 1, 1, 0, 0, 0), datetime(2024, 1, 1, 0, 0, 0)))\n", "    def test_year_lookup_bounds(self):\n        class MockConnection:\n            features = mock.MagicMock()\n            features.has_native_year_field = False\n\n                return mock.MagicMock(\n                    year_lookup_bounds_for_datetime_field=mock.MagicMock(return_value=(1, 2)),\n                    year_lookup_bounds_for_date_field=mock.MagicMock(return_value=(3, 4)),\n                )\n\n        lookup = YearExact(DateTimeField(), 2023)\n        start, finish = lookup.year_lookup_bounds(MockConnection(), 2023)\n        self.assertEqual(start, 1)\n        self.assertEqual(finish, 2)\n\n        lookup = YearExact(\n            Value(datetime(2023, 1, 1)),\n            2023,\n        )\n        start, finish = lookup.year_lookup_bounds(MockConnection(), 2023)\n        self.assertEqual(start, 3)\n        self.assertEqual(finish, 4)\n", "    def test_get_bound_params_year_lookup(self):\n        lookup = YearLookup(Value('field'), 2023)\n        with mock.patch.object(lookup.lhs.lhs.output_field, 'get_internal_type') as mock_get_internal_type:\n            mock_get_internal_type.return_value = 'DateTimeField'\n            with mock.patch.object(lookup, 'year_lookup_bounds') as mock_year_lookup_bounds:\n                mock_year_lookup_bounds.return_value = (\n                    datetime(2023, 1, 1, 0, 0, 0),\n                    datetime(2023, 12, 31, 23, 59, 59),\n                )\n                params = lookup.get_bound_params(datetime(2023, 1, 1, 0, 0, 0), datetime(2023, 12, 31, 23, 59, 59))\n                self.assertEqual(\n                    params,\n                    (datetime(2023, 1, 1, 0, 0, 0), datetime(2023, 12, 31, 23, 59, 59)),\n                )\n", "    def test_year_lookup_with_direct_value(self, mock_connection):\n        mock_connection.ops.year_lookup_bounds_for_datetime_field.return_value = (\n            Value('2023-01-01'),\n            Value('2024-01-01'),\n        )\n        lookup = YearExact(DateTimeField(), Value(2023))\n        sql, params = lookup.as_sql(mock_connection, mock_connection)\n        self.assertEqual(sql, 'EXTRACT(YEAR FROM <lhs>) BETWEEN %s AND %s')\n        self.assertEqual(params, ['2023-01-01', '2024-01-01'])\n\n"], "sample_106": ["    def test_cache_control_no_cache(self):\n        response = HttpResponse('Hello, world!')\n        patch_cache_control(response, no_cache=True)\n        self.assertEqual(response['Cache-Control'], 'no-cache, must-revalidate')\n\n", "    def test_cache_control_max_age_collision(self):\n        response = HttpResponse()\n        patch_cache_control(response, max_age=3600)\n        patch_cache_control(response, max_age=1800)\n\n        self.assertEqual(response['Cache-Control'], 'max-age=1800')\n", "    def test_get_cache_key_with_vary_header(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us'\n        response = HttpResponse('Hello World')\n        response['Vary'] = 'Accept-Language'\n        cache_key = learn_cache_key(request, response)\n        self.assertIsNotNone(cache_key)\n        request = HttpRequest()\n        request.method = 'GET'\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us'\n        self.assertEqual(cache_key, get_cache_key(request, cache=caches['default']))\n", "    def test_get_cache_key_with_vary(self):\n        request = RequestFactory().get('/test/')\n        request.META = {'HTTP_ACCEPT_LANGUAGE': 'en-us', 'HTTP_USER_AGENT': 'Mozilla'}\n        response = HttpResponse()\n        patch_vary_headers(response, ['accept-language',\n                                    'user-agent'])\n        cache_key = learn_cache_key(request, response)\n        self.assertIn('accept-language', cache_key)\n        self.assertIn('user-agent', cache_key)\n", "    def test_get_cache_key_with_cache_key_suffix(self):\n        request = HttpRequest()\n        request.META = {'HTTP_ACCEPT_LANGUAGE': 'en-us',\n                        'HTTP_HOST': 'example.com'}\n        with translation.override('de'):\n            cache_key = get_cache_key(request, key_prefix='test')\n            self.assertIn('.de', cache_key)\n"], "sample_61": ["    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        valid_usernames = ['john_doe', 'Jane.Doe123', 'j_o-hn.doe']\n        invalid_usernames = ['Jo\u00e3o Doe', 'john doe', 'john@example.com']\n        for username in valid_usernames:\n            validator(username)\n        for username in invalid_usernames:\n            with self.assertRaises(ValidationError):\n                validator(username)\n", "    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        valid_usernames = ['john', 'jane.doe', 'john_123', 'j.o.h.n', 'j+ohn']\n        invalid_usernames = ['john!', 'jane doe', 'j@hn', 'j \u00f6 hn']\n\n        for username in valid_usernames:\n            validator(username)\n\n        for username in invalid_usernames:\n            with self.assertRaises(ValidationError):\n                validator(username)\n", "    def test_unicode_username_validator(self):\n        validator = validators.UnicodeUsernameValidator()\n        valid_usernames = ['john.doe', 'jan_kowalski', 'user123', 'john.doe123']\n        for username in valid_usernames:\n            validator(username)\n\n        invalid_usernames = ['john doe', 'jan kowalski', 'user@example.com', 'j0hn.d0e', 'john.doe!']\n        for username in invalid_usernames:\n            with self.assertRaises(ValidationError):\n                validator(username)\n", "    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        self.assertRaises(ValidationError, validator.validate, 'user@example.com')\n        self.assertRaises(ValidationError, validator.validate, '\u00fcser')\n        validator.validate('user_123')\n        validator.validate('user.123')\n        validator.validate('user+123')\n", "    def test_ascii_username_validator(self):\n        validator = validators.ASCIIUsernameValidator()\n        valid_usernames = ['john_doe', 'jane.doe', 'jdoe123', 'john+doe']\n        invalid_usernames = ['j\u00f5hn_doe', 'jane doe', 'jdoe!', 'john@example.com']\n        for username in valid_usernames:\n            self.assertIsNone(validator(username))\n        for username in invalid_usernames:\n            with self.assertRaises(ValidationError):\n                validator(username)\n"], "sample_508": ["def test_setp_alias():\n    fig, ax = plt.subplots()\n    line, = ax.plot([1, 2, 3], [1, 2, 3])\n    setp(line, 'lw', 5)\n    assert line.get_linewidth() == 5\n\n    setp(line, 'color', 'r')\n    assert line.get_color() == 'r'\n\n    setp(line, 'markerfacecolor', 'g')\n    assert line.get_markerfacecolor() == 'g'\n", "    def test_getp_setp():\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [4, 5, 6])\n\n        # Test getp()\n        assert getp(line, 'color') == line.get_color()\n\n        # Test setp() with single keyword argument\n        setp(line, color='red')\n        assert getp(line, 'color') == 'red'\n\n        # Test setp() with multiple keyword arguments\n        setp(line, linewidth=2, linestyle='--')\n        assert getp(line, 'linewidth') == 2\n        assert getp(line, 'linestyle') == '--'\n\n        # Test setp() with positional arguments\n        setp(line, 'marker', 'o', 'markersize', 10)\n        assert getp(line, 'marker') == 'o'\n        assert getp(line, 'markersize') == 10\n\n        # Test setp() with a list of artists\n        lines = [line, ax.plot([4, 5, 6], [7, 8, 9])[0]]\n        setp(lines, color='green')\n        for l in lines:\n            assert getp(l, 'color') == 'green'\n\n        # Test setp() with file redirection\n        with io.StringIO() as buf:\n            setp(line, 'linestyle', file=buf)\n            output = buf.getvalue()\n            assert 'linestyle:' in output\n\n    \n", "    def test_setp_docstring_rst(self):\n        # Check setp docstring formatting\n        artist = plt.plot([1, 2, 3])[0]\n\n        # Mock the docstring output to avoid side effects\n        with io.StringIO() as file:\n            setp(artist, file=file)\n            output = file.getvalue()\n        assert output.startswith('.. table::\\n')\n\n", "    def test_artist_setp():\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3])\n        # Test setting properties with setp.\n        setp(line, linewidth=2, color='r')\n        assert line.get_linewidth() == 2\n        assert line.get_color() == 'r'\n\n        # Test setting properties with keyword arguments.\n        setp(line, linestyle='--')\n        assert line.get_linestyle() == '--'\n\n        # Test using MATLAB style string/value pairs.\n        setp(line, 'markerfacecolor', 'b', 'markersize', 10)\n        assert line.get_markerfacecolor() == 'b'\n        assert line.get_markersize() == 10\n\n        # Test getting properties.\n        res = io.StringIO()\n        setp(line, 'linewidth', file=res)\n\n        assert res.getvalue() == 'linewidth: 2\\n'\n\n", "    def test_setp_with_alias(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([0, 1, 2], [0, 1, 0])\n        setp(line, 'lw', 3)\n        assert line.get_linewidth() == 3\n        setp(line, 'linewidth', 5)\n        assert line.get_linewidth() == 5\n"], "sample_690": ["    def test_evaluate_skip_marks_reason(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"Just skipping\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*Reason: Just skipping*\"])\n", "    def test_evaluates_skip_marks_with_boolean_condition(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(True, reason=\"always skip\")\n                assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n        assert \"always skip\" in result.stdout.str()\n", "    def test_evaluate_skip_marks_unconditional_skip(self, pytester: Pytester) -> None:\n        item = pytester.makepytest(\"def test_func(): skip()\")\n        result = evaluate_skip_marks(item)\n        assert result is not None\n        assert result.reason == \"unconditional skip\"\n", "    def test_evaluate_skip_marks_boolean(self, pytester: Pytester) -> None:\n        item = pytester.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.skipif(True, reason=\"True\")\n                assert True\n        \"\"\"\n        ).collect(\"test_true\")\n        skip_result = evaluate_skip_marks(item)\n        assert isinstance(skip_result, Skip)\n        assert skip_result.reason == \"True\"\n", "    def test_evaluate_skip_marks_multiple_conditions(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(\"sys.platform == 'win32'\", reason=\"windows not supported\")\n            @pytest.mark.skipif(\"1 == 1\", reason=\"always skip\")\n                pass\n            \"\"\"\n        )\n        item = testdir.getitems()[0]\n        skip_info = evaluate_skip_marks(item)\n\n        assert skip_info is not None\n        assert skip_info.reason == \"windows not supported\"\n"], "sample_341": ["    def test_formset_factory_with_initial(self):\n        formset_data = [('Choice 1', 10), ('Choice 2', 5)]\n        formset = self.make_choiceformset(\n            formset_data, initial_forms=2, total_forms=2,\n        )\n        self.assertEqual(formset.initial_forms, 2)\n        self.assertEqual(len(formset.forms), 2)\n        self.assertEqual(formset.forms[0].cleaned_data['choice'], 'Choice 1')\n        self.assertEqual(formset.forms[1].cleaned_data['choice'], 'Choice 2')\n\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(FavoriteDrinkForm, max_num=3, absolute_max=5)\n        formset = formset_class({\"form-0-name\": \"Pepsi\"})\n        self.assertEqual(formset.total_form_count(), 1)\n        formset.full_clean()\n        # We should be able to add more forms, as absolute_max is higher than max_num\n        formset = formset_class({\"form-0-name\": \"Pepsi\",\n                               \"form-1-name\": \"Coke\"})\n        self.assertEqual(formset.total_form_count(), 2)\n        formset.full_clean()\n\n        # But exceeding absolute_max should raise a ValidationError\n        with self.assertRaises(ValidationError) as cm:\n            formset = formset_class({\"form-0-name\": \"Pepsi\",\n                                   \"form-1-name\": \"Coke\",\n                                   \"form-2-name\": \"Sprite\",\n                                   \"form-3-name\": \"Fanta\",\n                                   \"form-4-name\": \"Dr Pepper\",\n                                   \"form-5-name\": \"7UP\"})\n        self.assertEqual(cm.exception.message, 'Please submit at most 5 forms.')\n", "    def test_formset_factory_max_num_absolute_max(self):\n        # Test that absolute_max limits form instantiation\n        formset_class = formset_factory(\n            CharField, max_num=10, absolute_max=5, validate_max=True\n        )\n        formset = formset_class({'form-TOTAL_FORMS': '15'})\n        self.assertEqual(formset.errors, [\n            {\n                'TOTAL_FORMS': [\n                    'Please submit at most 5 forms.'\n                ],\n            }\n        ])\n        self.assertEqual(len(formset.forms), 5) \n", "    def test_formset_max_num(self):\n        formset = self.make_choiceformset(\n            formset_data=[('A', 1), ('B', 2), ('C', 3)],\n            max_num_forms=2,\n        )\n\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'A', 'votes': 1},\n            {'choice': 'B', 'votes': 2},\n        ])\n        self.assertTrue(formset.is_valid())\n\n        formset = self.make_choiceformset(\n            formset_data=[('A', 1), ('B', 2), ('C', 3)],\n            max_num_forms=1,\n        )\n\n        self.assertEqual(formset.cleaned_data, [\n            {'choice': 'A', 'votes': 1},\n        ])\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_factory_custom_kwarg(self):\n        class CustomKwargFormSet(BaseFormSet):\n                super().__init__(*args, **kwargs)\n                self.custom_value = kwargs.pop('custom_kwarg', None)\n\n        CustomKwargFormSet = formset_factory(CustomKwargForm, formset=CustomKwargFormSet, extra=1, custom_kwarg='test_value')\n        formset = CustomKwargFormSet()\n        self.assertEqual(formset.custom_value, 'test_value')\n"], "sample_932": ["    def test_function_template_parameter_pack(self):\n        check(\"function\",\n              \"void f(int a, typename... Types)\",\n              {1: \"f\"},\n              \"void f(int a, typename... Types)\",\n              key=\"f\",\n              asTextOutput=\"f(int a, typename... Types)\")\n", "    def test_alias_with_nested_name():\n        check('alias', 'alias {int foo::bar::baz}', idDict={1: 'foo_bar_baz'},\n              output='{int foo::bar::baz}')\n", "def test_cpp_enum_scoped_enum():\n    check(\n        \"enum\",\n        \"\"\"\n        enum class MyEnum : int {{\n            VALUE1,\n            VALUE2\n        }};\n        \"\"\",\n        {1: 'myenum'}\n        ,\n        output='''\n        enum class MyEnum : int {{\n            VALUE1,\n            VALUE2\n        }}\n        ''',\n    )\n", "    def test_alias():\n        check(\n            'alias',\n            'using MyAlias = std::vector<int>;',\n            {},\n            'using MyAlias = std::vector<int>;',\n        )\n", "def test_cpp_nested_template():\n    check('class',\n          \"template <typename T> class MyClass { };\",\n          {1: \"MyClass\", 2: \"MyClass_t\"},\n          \"template <typename T> class MyClass { }\",\n          key='MyClass', asTextOutput='template <typename T> class MyClass { }')\n"], "sample_12": ["    def test_angle_wrap_at_inplace(self):\n        a = Angle([-20.0, 150.0, 350.0] * u.deg)\n\n        # Wrap to 360 degrees\n        a.wrap_at(360 * u.deg, inplace=True)\n        assert_array_equal(a.degree, [340., 150., 350.])\n\n        # Wrap to 180 degrees\n        a.wrap_at('180d', inplace=True)\n        assert_array_equal(a.degree, [-20., 150., -10.])\n\n", "    def test_angle_from_scalar_unitless():\n        a = Angle(100)\n        assert a.unit == u.deg\n\n", "    def test_longitude_wrap_angle():\n        a = Longitude(180 * u.deg)\n        assert a.wrap_angle == 360 * u.deg\n        a.wrap_angle = 180 * u.deg\n        assert a.wrap_angle == Angle(180 * u.deg)\n        assert_allclose(a.degree, 180)\n\n        a = Longitude([180, 360, 540] * u.deg)\n        assert a.wrap_angle == 360 * u.deg\n        assert_allclose(a.degree, [180, 0, 180])\n\n        # Test setting wrap_angle with in-place wrapping\n        b = Longitude([180, 360, 540] * u.deg)\n        b.wrap_angle = 180 * u.deg\n        assert b.wrap_angle == 180 * u.deg\n        assert_allclose(b.degree, [180, 0, 180])\n", "    def test_angle_wrap_at_inplace(self):\n        a = Angle(['-120d', '30d', '420d'], unit=u.deg)\n        a.wrap_at('180d', inplace=True)\n        assert_allclose(a.degree, [60, 30, 60])\n\n        a = Angle(['-120d', '30d', '420d'], unit=u.deg)\n        a.wrap_at('0d', inplace=True)\n        assert_allclose(a.degree, [240, 30, 0])\n\n        a = Angle(['-120d', '30d', '420d'], unit=u.deg)\n        a.wrap_at(Angle('0d'), inplace=True)\n        assert_allclose(a.degree, [240, 30, 0])\n", "def test_angle_from_quantity():\n    \"\"\"Test Angle initialization from Quantity instance with unit\"\"\"\n    q = 1.23 * u.rad\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.value == 1.23\n    assert a.unit == u.rad\n\n    q = 45.6 * u.deg\n    a = Angle(q)\n    assert isinstance(a, Angle)\n    assert a.value == 45.6\n    assert a.unit == u.deg\n"], "sample_1044": ["def test_Pow_as_content_primitive():\n    x = Symbol('x')\n    assert (2*x**2).as_content_primitive() == (4, x**2/2)\n    assert (2*(x + 1)**2).as_content_primitive() == (4, (x + 1)**2/2)\n    assert (2**(1 + x)).as_content_primitive() == (2, 2**x)\n    assert (2**(2 + x)).as_content_primitive() == (4, 2**x)\n    assert (2**(3 + x)).as_content_primitive() == (8, 2**x)\n", "def test_eval_as_leading_term():\n    x = Symbol('x')\n    assert Pow(x + 1, 2).as_leading_term(x) == (x + 1)**2\n    assert Pow(x + 1, 2).as_leading_term(x) == x**2 + 2*x + 1\n    assert Pow(x, 2).as_leading_term(x) == x**2\n    assert Pow(1 + x, 2).as_leading_term(x) == x**2 + 2*x + 1\n    assert Pow(1 + 2*x, 2).as_leading_term(x) == 4*x**2 + 4*x + 1\n\n", "def test_Pow_as_base_exp_rational():\n    x = Symbol('x')\n    b = 2*x\n    e = Rational(3, 2)\n    assert Pow(b, e).as_base_exp() == (b, e)\n\n", "def test_Pow_as_base_exp():\n    x = Symbol('x')\n    assert Pow(2, x).as_base_exp() == (2, x)\n    assert Pow(x, 2).as_base_exp() == (x, 2)\n    assert Pow(2, x + 1).as_base_exp() == (2, x + 1)\n    assert Pow(x, 2 + y).as_base_exp() == (x, 2 + y)\n", "    def test_Pow_as_content_primitive():\n        x = Symbol('x')\n        assert (2*x).as_content_primitive() == (2, x)\n        assert ((2*x)**2).as_content_primitive() == (4, x**2)\n        assert ((2*x + 2)**2).as_content_primitive() == (4, (x + 1)**2)\n        assert (4**((1 + x)/2)).as_content_primitive() == (2**(1 + x)/2, 2**(x/2))\n        assert (3**((1 + x)/2)).as_content_primitive() == (1, 3**((x + 1)/2))\n        assert (3**((5 + x)/2)).as_content_primitive() == (9, 3**((x + 1)/2))\n        eq = 3**(2 + 2*x)\n        assert powsimp(eq) == eq\n        assert eq.as_content_primitive() == (9, 3**(2*x))\n        assert powsimp(Mul(*eq.as_content_primitive())) == 3**(2*x + 2)\n\n        eq = (2 + 2*x)**x\n        assert eq.as_content_primitive() == (1, (2*(x + 1))**x)\n\n"], "sample_845": ["    def test_tfidf_vocabulary_size(self):\n        tfidf = TfidfVectorizer(max_features=5)\n        tfidf.fit(ALL_FOOD_DOCS)\n        assert len(tfidf.vocabulary_) == 5\n\n", "    def test_tfidf_get_feature_names_out(self):\n        vectorizer = TfidfVectorizer()\n        vectorizer.fit(ALL_FOOD_DOCS)\n        expected = vectorizer.get_feature_names()\n        observed = vectorizer.get_feature_names_out()\n        assert_array_equal(expected, observed)\n        # Test with a vocabulary\n        vectorizer.vocabulary = {\n            'the': 0, 'pizza': 1, 'burger': 2, 'beer': 3, 'coke': 4,\n            'salad': 5, 'celeri': 6, 'tomato': 7, 'water': 8, 'copyright': 9\n        }\n        expected = vectorizer.get_feature_names()\n        observed = vectorizer.get_feature_names_out()\n        assert_array_equal(expected, observed)\n\n", "    def test_tfidfvectorizer_lowercase_stop_words_vocabulary(self):\n        # Test that lowercase and stop words are correctly handled when\n        # vocabulary is given.\n        corpus = [\n            'This is the first document.',\n            'This document is the second document.',\n            'And this is the third one.',\n            'Is this the first document?',\n        ]\n        # Stop words are removed\n        stop_words = {\"the\", \"is\", \"this\"}\n        vocab = {'first': 0, 'document': 1, 'second': 2, 'third': 3,\n                 'one': 4}\n        vectorizer = TfidfVectorizer(vocabulary=vocab, stop_words=stop_words,\n                                     lowercase=True)\n        X = vectorizer.fit_transform(corpus)\n        assert_array_equal(X.toarray(),\n                           [[0, 1, 0, 0, 0],\n                            [0, 2, 0, 0, 0],\n                            [0, 0, 1, 1, 1],\n                            [0, 1, 0, 0, 0]])\n\n", "    def test_tfidf_vectorizer_stop_words_with_vocabulary(self):\n        # Test that stop_words is correctly handled when vocabulary is provided\n        corpus = [\n            'This is the first document.',\n            'This document is the second document.',\n            'And this is the third one.',\n            'Is this the first document?',\n        ]\n        vocabulary = {'document': 0, 'this': 1, 'is': 2, 'the': 3, 'first': 4,\n                      'second': 5, 'third': 6, 'one': 7}\n        vectorizer = TfidfVectorizer(vocabulary=vocabulary, stop_words='english')\n        X = vectorizer.fit_transform(corpus)\n        assert_array_equal(X.getnnz(), 22)\n        assert vectorizer.stop_words_ is None\n\n", "    def test_tfidfvectorizer_empty_vocabulary(self):\n        # Test TFIDF vectorizer with empty vocabulary\n        vectorizer = TfidfVectorizer()\n        X = vectorizer.fit_transform([])\n        assert X.shape == (0, 0)\n        assert 'vocabulary_' in vectorizer.get_params(deep=True)\n"], "sample_540": ["    def test_FuncAnimation_null_writer(anim):\n        writer = NullMovieWriter()\n        with pytest.raises(RuntimeError):\n            anim.save(\"nonexistent.mp4\", writer=writer)\n        assert writer.fig is anim.fig\n        assert writer._count == 0\n        \n", "def test_dpi(anim):\n    orig_dpi = anim.fig.dpi\n    anim.save('temp.mp4', dpi=100)\n    assert anim.fig.dpi == orig_dpi\n", "    def test_null_writer(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([], [])\n\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n\n            line.set_data([], [])\n            return line,\n\n            x = np.linspace(0, 10, 100)\n            y = np.sin(x + i)\n            line.set_data(x, y)\n            return line,\n\n        writer = NullMovieWriter()\n        ani = animation.FuncAnimation(fig, animate, init_func=init,\n                                      frames=10, blit=False,\n                                      writer=writer)\n\n        ani.save('test.mp4')\n        assert writer.outfile == 'test.mp4'\n        assert writer._count == 10\n\n", "    def test_blit(self, anim):\n        # This test case creates an animation with blitting enabled. It checks\n        # that each frame is rendered correctly, without redrawing the\n        # entire figure.\n\n        # Create a mock figure for testing purposes\n        fig, ax = plt.subplots()\n        line, = ax.plot([], [])\n        ax.set_xlim(0, 10)\n        ax.set_ylim(-1, 1)\n        # Setup function to return a reference to the line\n            return line,\n        # Animation function that modifies the y data on each frame\n            x = np.linspace(0, 10, 100)\n            y = np.sin(x + i)\n            line.set_data(x, y)\n            return line,\n        # Create an animation object with blitting enabled\n        anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                       frames=5, blit=True)\n\n", "def test_func_animation_blit(blit):\n    \"\"\"Test FuncAnimation with blitting.\"\"\"\n    fig, ax = plt.subplots()\n    line, = ax.plot([], [])\n\n    ax.set_xlim(0, 10)\n    ax.set_ylim(-1, 1)\n\n        line.set_data([], [])\n        return line,\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x + i)\n        line.set_data(x, y)\n        return line,\n\n    anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                  frames=5, blit=blit, interval=20)\n    anim.save('temp_animation.mp4')\n    with pytest.raises(ValueError):\n        anim.save('temp_animation.mp4', writer='no_such_writer')\n    os.remove('temp_animation.mp4')\n\n"], "sample_977": ["def test_mcode_Derivative():\n    assert mcode(Derivative(sin(x), x)) == \"Hold[D[Sin[x],x]]\"\n", "    def test_mcode_sum(self):\n        assert mcode(Sum(x, (x, 1, 5))) == 'Hold[Sum[x, {x, 1, 5}]]'\n", "def test_mcode_sum():\n    assert mcode(Sum(x**2, (x, 1, 3))) == 'Hold[Sum[x^2, {x, 1, 3}]]'\n", "    def test_print_Sum():\n        assert mcode(Sum(x**2, (x, 1, 5))) == 'Hold[Sum[x^2, {x, 1, 5}]]'\n\n", "    def test_print_Derivative(self):\n        assert mcode(Derivative(sin(x), x)) == 'Hold[D[Sin[x],x]]'\n"], "sample_712": ["    def test_fit_transform_sparse_dtype(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        X = np.array([[0, 1], [1, 0]]).astype('int32')\n        X_transformed = enc.fit_transform(sparse.csr_matrix(X))\n        assert X_transformed.dtype == np.float64\n\n", "    def test_onehot_sparse_handle_unknown_ignore_multioutput(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        X = np.array([[0, 1], [1, 2], [0, 3]])\n        enc.fit(X)\n        X_new = np.array([[0, 4], [1, 2]])\n        X_trans = enc.transform(X_new)\n        assert_equal(X_trans.shape, (2, 4))\n        assert_equal(X_trans.toarray(),\n                     [[1, 0, 0, 0],\n                      [0, 1, 1, 0]])\n\n", "    def test_onehot_handle_unknown_ignore_sparse(self):\n        X = [['Male', 1], ['Female', 3], ['Female', 2], ['Male', 1]]\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        enc.fit(X)\n        X_new = [['Male', 4], ['Unknown', 1]]\n        with ignore_warnings():\n            X_trans = enc.transform(X_new)\n        assert X_trans.shape == (2, 5)\n        assert_array_equal(toarray(X_trans),\n                            [[1, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n\n", "    def test_inverse_transform_unknown_categories_ignore(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc.fit(X)\n        X_trans = enc.transform([['Female', 4], ['Male', 1]])\n        X_inv = enc.inverse_transform(X_trans)\n        assert_equal(X_inv, [['Female', None], ['Male', 1]])\n", "    def test_sparse_indicator_transform(self):\n        X = np.array([[0, 1], [1, 0], [2, 1]])\n        enc = OneHotEncoder(sparse=True, handle_unknown='ignore')\n        X_t = enc.fit_transform(X)\n        assert isinstance(X_t, sparse.csr_matrix)\n        assert X_t.shape == (3, 3)\n"], "sample_950": ["def test_parse_annotation_complex():\n    # Test parsing complex annotations with nested types and generics\n    annotation_str = \"List[Tuple[str, int], Dict[str, Union[float, bool]]]\"\n    expected_annotation = (\n        \"List[Tuple[str, int], Dict[str, Union[float, bool]]]\"\n    )\n    result = _parse_annotation(annotation_str)\n    assert result == expected_annotation\n", "def test_parse_annotation_with_complex_types():\n    annotations = _parse_annotation(\"List[Tuple[str, int]]\")\n    assert len(annotations) == 1\n    assert annotations[0].node_type == 'desc_annotation'\n    assert annotations[0].astext() == \"List[Tuple[str, int]]\"\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation(\"int\"), (\"int\", None))\n        self.assertEqual(_parse_annotation(\"str, int\"), (\"str\", \"int\"))\n        self.assertEqual(_parse_annotation(\"List[str]\"), (\"List\", \"str\"))\n        self.assertEqual(_parse_annotation(\"Optional[int]\"), (\"Optional\", \"int\"))\n        self.assertEqual(_parse_annotation(\"Dict[str, int]\"), (\"Dict\", \"str\", \"int\"))\n        self.assertEqual(_parse_annotation(\"Union[str, int]\"), (\"Union\", \"str\", \"int\"))\n        self.assertEqual(_parse_annotation(\"Tuple[str, ...]\"), (\"Tuple\", \"str\"))\n        self.assertEqual(_parse_annotation(\"Callable[[int, str], None]\"), (\"Callable\", \"[int, str]\", \"None\"))\n        self.assertEqual(_parse_annotation(\"Any\"), (\"Any\", None))\n\n", "    def test_parse_annotation_with_ellipsis():\n        result, type_ = _parse_annotation(\"...\")\n        assert result == \"...\"\n        assert type_ is None\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == 'int'\n        assert _parse_annotation('List[str]') == 'List[str]'\n        assert _parse_annotation('Dict[str, int]') == 'Dict[str, int]'\n        assert _parse_annotation('Tuple[int, ...]') == 'Tuple[int, ...]'\n        assert _parse_annotation('Optional[Union[int, str]]') == 'Optional[Union[int, str]]'\n        assert _parse_annotation(\n            'Callable[[str], List[Tuple[int, float]]]'\n        ) == 'Callable[[str], List[Tuple[int, float]]]'\n        assert _parse_annotation('Any') == 'Any'\n"], "sample_445": ["    def test_timesince_future(self):\n        # Test case where d is in the future compared to now.\n        future_time = self.t + self.oneday\n        with translation.override('en-us'):\n            self.assertEqual(timesince(future_time, self.t), '0 minutes')\n", "    def test_timesince_timezone_aware(self):\n        self.assertEqual(timesince(self.t), \"15 years, 2 months\")\n        self.assertEqual(timeuntil(self.t), \"15 years, 2 months\")\n", "    def test_timesince_timezone_aware(self):\n        with translation.override('en-us'):\n            # Test with timezone-aware datetimes.\n            now = timezone.now()\n            past = now - self.oneday\n            self.assertEqual(timesince(past), \"1 day\")\n            self.assertEqual(timesince(now), \"0 minutes\")\n", "    def test_timesince_future_date(self):\n        \"\"\"\n        Tests timesince when d is in the future.\n        \"\"\"\n        future_date = self.t + self.oneday\n        result = timesince(future_date, self.t)\n        self.assertEqual(result, \"0 minutes\")\n", "    def test_timesince_aware_datetime(self):\n        # Test with aware datetimes\n        with translation.override(\"en\"):\n            now = timezone.make_aware(self.t)\n            d = now - self.oneday\n            self.assertEqual(timesince(d, now), \"1 day\")\n            d = now - self.onehour\n            self.assertEqual(timesince(d, now), \"1 hour\")\n\n"], "sample_340": ["    def test_migration_loader_detect_conflicts(self):\n        \"\"\"Tests that MigrationLoader.detect_conflicts() correctly identifies conflicts.\"\"\"\n        # Create a test app with conflicting migrations\n        with self.temporary_app(name='conflicting_app') as app_path:\n            migrations_dir = os.path.join(app_path, 'migrations')\n            os.makedirs(migrations_dir)\n            with open(os.path.join(migrations_dir, '__init__.py'), 'w'):\n                pass\n            with open(os.path.join(migrations_dir, '0001_initial.py'), 'w') as f:\n                f.write(\"from django.db import migrations\\n\")\n                f.write(\"class Migration(migrations.Migration):\\n\")\n                f.write(\"    dependencies = [\\n\")\n                f.write(\"    ]\\n\")\n                f.write(\"    operations = [\\n\")\n                f.write(\"    ]\\n\")\n\n            # Compile the migrations\n            compileall.compiledir(migrations_dir)\n\n        # Get the loader for the test app\n        loader = MigrationLoader(connection)\n\n        # Load the migrations\n        loader.build_graph()\n\n        # Assert that there are conflicts\n        conflicts = loader.detect_conflicts()\n        self.assertEqual(conflicts, {'conflicting_app': ['0001_initial']})\n\n", "    def test_add_internal_dependencies(self):\n        with override_settings(INSTALLED_APPS=['migrations_tests']):\n            # Create a migration graph.\n            loader = MigrationLoader(connection=connections['default'])\n            loader.build_graph()\n\n            # Get a migration and its dependencies\n            migration = loader.get_migration('migrations_tests', '0001_initial')\n            dependencies = migration.dependencies\n\n            # Add internal dependencies\n            loader.add_internal_dependencies((migration.app_label, migration.name), migration)\n\n            # Assert that dependencies were added correctly\n            for dependency in dependencies:\n                self.assertIn((dependency[0], dependency[1]), loader.graph.node_map[(migration.app_label, migration.name)].parents)\n", "    def test_check_consistent_history(self):\n        \"\"\"\n        Check that checking consistent history raises InconsistentMigrationHistory\n        if there are any applied migrations with unapplied dependencies.\n        \"\"\"\n        with override_settings(MIGRATION_MODULES={'test_app': 'test_app.migrations_with_dependency'}):\n            # Create a migration graph with a dependency.\n            loader = MigrationLoader(connection=connections['default'], load=True, ignore_no_migrations=True)\n            loader.build_graph()\n            # Apply the migration that depends on another one.\n            recorder = MigrationRecorder(connections['default'])\n            recorder.record_applied('test_app', '0002_migration_with_dependency')\n            # Now try to check the history, should raise InconsistentMigrationHistory\n\n            with self.assertRaises(InconsistentMigrationHistory):\n                loader.check_consistent_history(connections['default'])           \n", "    def test_migrations_module(self):\n", "    def test_check_consistent_history(self):\n        # This test ensures that Django raises an InconsistentMigrationHistory\n        # exception when an applied migration has unapplied dependencies.\n        with connections['default'].cursor() as cursor:\n            cursor.execute(\"CREATE TABLE migrations_test_model (id INT PRIMARY KEY)\")\n            compileall.compiledir(os.path.join(this_dir, 'migrations_test'))\n        with connections['default'] as connection:\n            loader = MigrationLoader(connection, ignore_no_migrations=True)\n            loader.build_graph()\n            try:\n                recorder = MigrationRecorder(connection)\n                recorder.record_applied('migrations_test', '0001_initial')\n                # This migration depends on 'dummy_app.0001_initial', which hasn't\n                # been applied yet.\n                recorder.record_applied('migrations_test', '0002_add_field')\n                self.fail(\"Should raise InconsistentMigrationHistory exception\")\n            except InconsistentMigrationHistory as e:\n                self.assertIn('Migration migrations_test.0002_add_field', str(e))\n"], "sample_1069": ["    def test_glsl_Piecewise_with_default(self):\n        x = Symbol('x')\n        expr = Piecewise((x + 1, x > 0), (x, True))\n        code = glsl_code(expr, x)\n        expected = \"\"\"", "def test_glsl_Piecewise_default_condition():\n    expr = Piecewise((x + 1, x > 0), (x, True))\n    raises(ValueError, lambda: glsl_code(expr))\n\n", "    def test_matrix_solve_printed_as_function_call(self):\n        A = MatrixSymbol('A', 3, 3)\n        b = MatrixSymbol('b', 3, 1)\n        sol = MatrixSolve(A, b)\n        assert octave_code(sol) == 'linsolve(A, b)'\n\n", "def test_glsl_matrix_inverse():\n    A = Matrix([[1, 2],\n              [3, 4]])\n    code = glsl_code(A**-1)\n    expected = \"Inverse(mat2(1, 2, 3, 4))\" \n    assert code == expected \n", "    def test_glsl_piecewise_with_functions(self):\n        expr = Piecewise((sin(x), x < 0), (cos(x), x >= 0))\n        code = glsl_code(expr, assign_to=\"y\")\n        expected = (\n            \"if (x < 0) {\\n\"\n            \"   y = sin(x);\\n\"\n            \"} else {\\n\"\n            \"   y = cos(x);\\n\"\n            \"}\\n\"\n        )\n        assert code == expected\n"], "sample_412": ["    def test_escapejs(self):\n        self.check_output(escapejs, '<script>\"hello world\"</script>', r'\\\\u003Cscript\\\\u003E\\\\\"hello world\\\\\"\\\\u003C\\\\/script\\\\u003E')\n        self.check_output(escapejs, \"'hello world'\")\n        self.check_output(escapejs, \"hello\\\\ world\")\n", "    def test_urlize_trim_url_limit(self):\n        self.check_output(\n            lambda: urlize(\n                \"This is a link to https://www.djangoproject.com/ with a very long URL.\",\n                trim_url_limit=30,\n            ),\n            'This is a link to <a href=\"https://www.djangoproject.com/\">https://www.djangoproject.com/...</a> with a very long URL.',\n        )\n\n", "    def test_smart_urlquote(self):\n        self.assertEqual(smart_urlquote(\"http://www.example.com\"), \"http://www.example.com\")\n        self.assertEqual(smart_urlquote(\"https://www.example.com\"), \"https://www.example.com\")\n        self.assertEqual(smart_urlquote(\"www.example.com\"), \"http://www.example.com\")\n        self.assertEqual(smart_urlquote(\"example.com\"), \"http://example.com\")\n        self.assertEqual(\n            smart_urlquote(\"http://user:password@www.example.com\"),\n            \"http://user:password@www.example.com\",\n        )\n        self.assertEqual(\n            smart_urlquote(\"http://example.com/path?query=value#fragment\"),\n            \"http://example.com/path?query=value#fragment\",\n        )\n        self.assertEqual(\n            smart_urlquote(\"http://example.com/?q=\u4e2d\u6587\"),\n            \"http://example.com/?q=%E4%B8%AD%E6%96%87\",\n        )\n", "    def test_urlize_nofollow(self):\n        self.check_output(\n            lambda: urlize(\"http://example.com\", nofollow=True),\n            '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>',\n        )\n", "    def test_urlize_mailtos(self):\n        self.check_output(urlize, \"Send email to me@example.com\", '<a href=\"mailto:me@example.com\">Send email to me@example.com</a>')\n        self.check_output(urlize, 'Contact me at <a href=\"mailto:me@example.com\">me@example.com</a>', 'Contact me at <a href=\"mailto:me@example.com\">me@example.com</a>')\n"], "sample_324": ["    def test_csrf_cookie_is_set_on_successful_post_with_referer_checking(self):\n        request = self._get_POST_request_with_token()\n        request.META['HTTP_REFERER'] = 'https://testserver/path'\n\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(request, post_form_view, (), {})\n        self.assertIsNone(response)\n\n        response = middleware.process_response(request, HttpResponse())\n        self.assertEqual(response.COOKIES[settings.CSRF_COOKIE_NAME], self._csrf_id_cookie)\n", "    def test_csrf_cookie_expiry(self):\n        middleware = CsrfViewMiddleware()\n        request = TestingHttpRequest()\n        request.META[\"CSRF_COOKIE\"] = self._csrf_id\n        response = HttpResponse()\n        middleware.process_response(request, response)\n        self.assertEqual(\n            response.get(\"Set-Cookie\")[0].split(\";\")[\n                -1\n            ].strip().lower(),\n            \"max-age=1209600\",\n        )\n", "    def test_csrf_token_in_cookie_and_header(self):\n        \"\"\"\n        Test that the csrf token is included in both the cookie and the\n        'HTTP_X_CSRFTOKEN' header if the setting is enabled.\n        \"\"\"\n        with override_settings(CSRF_HEADER_NAME='HTTP_X_CSRFTOKEN'):\n            response = self.client.get(post_form_view)\n            self.assertEqual(response.status_code, 200)\n            self.assertIn('csrfmiddlewaretoken', response.cookies)\n            self.assertIn('HTTP_X_CSRFTOKEN', response.headers)\n            self.assertEqual(response.cookies['csrfmiddlewaretoken'].value, response.headers['HTTP_X_CSRFTOKEN'])\n", "    def test_csrf_token_rejected_if_origin_doesnt_match_trusted_origins(\n        self,", "    def test_csrf_token_in_cookie_when_csrf_cookie_is_not_set(self):\n        request = self._get_GET_no_csrf_cookie_request()\n        response = self.middleware.process_view(request, post_form_view, {}, {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue('Set-Cookie' in response['Set-Cookie'])\n\n        # Test that the CSRF cookie is set correctly\n        cookie = response['Set-Cookie'].split(';')[0]\n\n        # Extract the CSRF token from the cookie\n        match = re.search('csrftoken=(.+)', cookie)\n        self.assertIsNotNone(match)\n        csrf_token = match.group(1)\n\n        # Assert that the CSRF token is the same as the one generated\n        self.assertTrue(equivalent_tokens(self._csrf_id_cookie, csrf_token))\n"], "sample_71": ["    def test_format_grouping_non_uniform(self):\n        self.assertEqual(\n            nformat(\n                1234567890,\n                decimal_sep='.',\n                thousand_sep=',',\n                grouping=(3, 2, 0),\n            ),\n            '1,234,567,890',\n        )\n", "    def test_format_large_decimal(self):\n        # Test formatting a large decimal with a specified number of decimal places.\n        num = Decimal(\"12345678901234567890.1234567890\")\n        formatted_num = nformat(num, decimal_sep=',', decimal_pos=4)\n        self.assertEqual(formatted_num, '12,345,678,901,234,567,890.1235')\n", "    def test_format_grouping(self):\n        self.assertEqual(nformat(1234567.89, decimal_sep=',', thousand_sep='.'), '1.234.567,89')\n", "    def test_format_large_decimal(self):\n        self.assertEqual(\n            nformat(Decimal('123456789012345678901234567890.1234567890'), '.', 4, 3, ','),\n            '12,345,678,901,234,567,890.1235',\n        )\n", "    def test_format_large_number_scientific_notation(self):\n        self.assertEqual(\n            nformat(Decimal('1e+200'), decimal_sep='.', decimal_pos=5, grouping=3, thousand_sep=',', force_grouping=True),\n            '1.00000e+200',\n        )\n"], "sample_281": ["    def test_permission_denied_for_missing_search_fields(self):\n        with model_admin(Question, QuestionAdmin, admin_site=site):\n            # Remove search_fields from QuestionAdmin.\n            QuestionAdmin.search_fields = []\n            request = self.factory.get(\n                self.url,\n                data={\n                    'app_label': self.opts['app_label'],\n                    'model_name': self.opts['model_name'],\n                    'field_name': self.opts['field_name'],\n                    'term': 'foo',\n                },\n            )\n            request.user = self.user\n            with self.assertRaises(Http404):\n                AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "    def test_autocomplete_view_search_use_distinct(self):\n\n        with model_admin(\n            Answer,\n            AnswerAdmin,\n        ):\n            answer1 = Answer.objects.create(question=Question.objects.create(id=1))\n            answer2 = Answer.objects.create(question=Question.objects.create(id=2), text=answer1.text)\n\n            request = self.factory.get(self.url, {\n                'app_label': self.opts['app_label'],\n                'model_name': self.opts['model_name'],\n                'field_name': self.opts['field_name'],\n                'term': answer1.text,\n            })\n            request.user = self.user\n\n            response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n            self.assertEqual(response.status_code, 200)\n            data = json.loads(response.content)\n            self.assertEqual(len(data['results']), 1)\n", "    def test_autocomplete_permission_denied(self):\n        # Simulate a user without permission to view Answers.\n        self.user.user_permissions.exclude(codename='view_answer').delete()\n        request = self.factory.get(self.url, {'term': 'foo', **self.opts})\n        request.user = self.user\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 403)\n", "    def test_no_search_fields(self):\n        with model_admin(\n            model=Author,\n            model_admin=AuthorAdmin,\n        ):\n            with self.assertRaisesMessage(Http404, 'Author must have search_fields for the autocomplete_view.'):\n                self.client.get(self.url, {\n                    'app_label': 'tests',\n                    'model_name': 'author',\n                    'field_name': 'name',\n                    'term': 'test',\n                }, HTTP_X_CSRFTOKEN=self.csrf_token)\n", "    def test_no_search_fields(self):\n        with model_admin(Answer, AnswerAdmin, admin_site=site):\n            AnswerAdmin.get_search_fields = lambda request: []\n            response = self.client.get(\n                self.url,\n                {'app_label': Answer._meta.app_label,\n                 'model_name': Answer._meta.model_name,\n                 'field_name': 'question',\n                 'term': 'foo'})\n            self.assertEqual(response.status_code, 404)\n"], "sample_603": ["def test_summarize_coord_multiindex(multiindex):\n    expected = fh.summarize_coord(\"x\", multiindex.x)\n    assert isinstance(expected, dict)\n    assert len(expected) == 2\n    assert \"x\" in expected\n    assert \"level_1\" in expected\n", "def test_summarize_coord_multiindex(multiindex):\n    expected_preview = \"(level_1, level_2)\"\n    expected_name = \"x\"\n    result = fh._summarize_coord_multiindex(expected_name, multiindex.coords[\"x\"])\n    assert expected_preview in result\n    assert expected_name in result\n", "def test_summarize_coord_multiindex(multiindex):\n    html_repr = fh.summarize_coord(\"x\", multiindex[\"x\"])\n    assert \"<li class='xr-var-item'>\" in html_repr\n    assert \"MultiIndex\" in html_repr\n    assert \"level_1\" in html_repr\n    assert \"level_2\" in html_repr\n", "def test_summarize_coords_multiindex(multiindex):\n    html_repr = fh.summarize_coords(multiindex.coords)\n    assert \"level_1\" in html_repr\n    assert \"level_2\" in html_repr\n\n", "def test_dataset_repr_with_multiindex(dataset, multiindex):\n    ds = xr.merge([dataset, multiindex])\n    expected = fh.dataset_repr(ds)\n    assert \"level_1\" in expected\n    assert \"level_2\" in expected\n\n"], "sample_166": ["    def test_pbkdf2_with_default_digest(self):\n        password = b'mypassword'\n        salt = b'mysalt'\n        iterations = 10000\n        expected_hash = hashlib.pbkdf2_hmac(\n            'sha256', password, salt, iterations\n        )\n        self.assertEqual(pbkdf2(password, salt, iterations), expected_hash)\n", "    def test_get_random_string_deprecated_length(self):\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            get_random_string()\n\n", "    def test_salted_hmac_invalid_algorithm(self):\n        with self.assertRaisesMessage(InvalidAlgorithm, \"is not an algorithm accepted by the hashlib module.\"):\n            salted_hmac(b'salt', b'value', algorithm='invalid')\n", "    def test_salted_hmac_invalid_algorithm(self):\n        with self.assertRaises(InvalidAlgorithm):\n            salted_hmac('test', 'test', algorithm='invalid-algorithm')\n", "    def test_get_random_string(self):\n        self.assertEqual(len(get_random_string(12)), 12)\n        self.assertEqual(len(get_random_string()), 12)\n\n"], "sample_1004": ["def test_conditionset_dummy_eq():\n    a = ConditionSet(x, x < 1, S.Integers)\n    b = ConditionSet(y, y < 1, S.Integers)\n    assert a.dummy_eq(b)\n\n    a = ConditionSet(x, x < y, S.Integers)\n    b = ConditionSet(z, z < y, S.Integers)\n    assert a.dummy_eq(b)\n\n    a = ConditionSet(x, x < y, ConditionSet(y, x + y < 2, S.Integers))\n    b = ConditionSet(z, z < y, ConditionSet(y, x + y < 2, S.Integers))\n    assert a.dummy_eq(b)\n\n    a = ConditionSet(x, x < y, ConditionSet(y, x + y < 2, S.Integers))\n    b = ConditionSet(x, x < z, ConditionSet(y, x + y < 2, S.Integers))\n    assert not a.dummy_eq(b)\n\n", "def test_conditionset_empty_base():\n    assert ConditionSet(x, x < 1, EmptySet()) == EmptySet()\n", "    def test_conditionset_subs_dummy():\n        # Check that substitution of the dummy symbol only affects\n        # the condition if the new symbol has the same assumptions\n        # as the old one.\n\n        c = ConditionSet(x, x < 1, S.Integers)\n        c1 = c.subs(x, y)\n        assert c1.sym == y\n        assert c1.condition == (y < 1)\n        assert c1.base_set == S.Integers\n\n        c = ConditionSet(x, x < 1, S.Integers)\n        c1 = c.subs(x, z)\n        assert c1.sym == z\n        assert c1.condition == (z < 1)\n        assert c1.base_set == S.Integers\n\n        raises(ValueError, lambda: c.subs(x, f(1)))\n\n", "def test_ConditionSet_subs_dummy_in_base():\n    c = ConditionSet(x, x < 1, {x, z})\n    assert c.subs(x, y) == ConditionSet(x, x < 1, {y, z})\n    assert c.subs(x, y).subs(x, y) == ConditionSet(y, y < 1, {y, z})\n    assert ConditionSet(y, y < 1, {y, z}).subs(y, 1).subs(y, 1) == ConditionSet(y, y < 1, {z})\n", "def test_conditionset_emptyset_base():\n    assert ConditionSet(x, x > 1, EmptySet()) == EmptySet()\n"], "sample_643": ["def test_on_set_current_module_unrecognized_argument(\n    linter: PyLinter, reporter: TextReporter, disable: list[str]", "def test_text_reporter_handles_disabled_messages_by_category(reporter, disable):\n    \"\"\"Test if TextReporter handles disabled messages by category.\"\"\"\n    linter = PyLinter()\n    linter.reporter = reporter\n    linter.disable = disable\n    module = \"test_module\"\n    msg_id = \"test-message\"\n\n    # create a mock message\n    message = Message(msg_id, \"My message\", \"C\", HIGH, \"testfile.py\", 1, 1,\n                      None, None, None)\n    message_location = MessageLocationTuple(module, 1, 1)\n    message_location = FileItem(message_location, \"testfile.py\")\n\n    \n    # Test that the message is not added to output if disabled\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", checkers.message.Message.DisabledMessage)\n        linter._handle_message(message, message_location)\n\n    # Assert the message didn't print\n    output = reporter.out.getvalue()\n    assert msg_id not in output\n\n", "def test_text_reporter_handles_message_with_empty_end_line_and_column(\n    reporter: TextReporter,\n    disable: list[str],", "def test_text_reporter_with_message_template(\n    linter: PyLinter, reporter: TextReporter, disable: list[str]", "def test_text_reporter_with_custom_template(reporter, disable):\n    with redirect_stdout(StringIO()) as output:\n        linter = PyLinter()\n        linter.options = {\n            \"disable\": disable,\n            \"msg-template\": \"{path}:{line}:{column}: {msg_id}: {msg} ({symbol})\",\n        }\n        linter.set_reporter(reporter)\n        linter.process_modules([\"test_file.py\"])\n    output_str = output.getvalue()\n    assert \"hello.py:1:0: C0111: Missing docstring (missing-docstring)\" in output_str\n"], "sample_758": ["    def test_check_X_y_multi_output_sparse(self):\n        X, y = make_blobs(n_samples=10, n_features=5, centers=2, random_state=0)\n        y = np.array([y, y]).T\n        y_sparse = sp.csr_matrix(y)\n\n        check_X_y(X, y_sparse, multi_output=True)\n", "    def test_check_memory_invalid_cachedir():\n        # Test if invalid cachedir raises a ValueError\n        with pytest.raises(ValueError):\n            check_memory('foo', cachedir='/invalid/path')\n", "    def test_check_consistent_length_sparse_matrix():\n        X = sp.csr_matrix([[0, 1], [1, 0]])\n        y = np.array([0, 1])\n        check_consistent_length(X, y)\n\n        y = np.array([0, 1, 2])\n        with pytest.raises(ValueError):\n            check_consistent_length(X, y)\n", "    def test_check_X_y_multi_output_sparse_invalid_data():\n        X = sparse_random_matrix(10, 5, density=0.5)\n        y = sparse_random_matrix(10, 3, density=0.5)\n\n        with pytest.raises(ValueError):\n            check_X_y(X, y, multi_output=True)\n", "    def test_check_consistent_length_pandas_dataframe():\n        # Test with pandas DataFrame\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = pd.DataFrame({'col1': [1, 2, 3]})\n        with pytest.raises(ValueError):\n            check_consistent_length(X, y)\n        y = pd.DataFrame({'col1': [1, 2]})\n        with pytest.raises(ValueError):\n            check_consistent_length(X, y['col1'])\n\n"], "sample_1098": ["    def test_appellf1_derivative():\n        a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n        f = appellf1(a, b1, b2, c, x, y)\n        tn(f.diff(x), appellf1(a, b1 + 1, b2, c + 1, x, y)*a*b1/c, x)\n        tn(f.diff(y), appellf1(a, b1, b2 + 1, c + 1, x, y)*a*b2/c, y)\n", "def test_appellf1_derivative():\n    a, b1, b2, c = symbols('a b1 b2 c')\n    assert td(appellf1(a, b1, b2, c, x, S.Zero), x, 0) == appellf1(a, b1, b2, c, x, S.Zero)\n", "def test_appellf1_diff():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    assert td(f, x, x0=randcplx()) == f.fdiff(5)\n    assert td(f, y, x0=randcplx()) == f.fdiff(6)\n\n", "def test_appellf1_series_expansion():\n    a, b1, b2, c, x, y = symbols('a b1 b2 c x y')\n    f = appellf1(a, b1, b2, c, x, y)\n    tn(f.series(x, 0, 3), appellf1(a, b1, b2, c, 0, y) +\n       (a*b1/c)*x*appellf1(a + 1, b1 + 1, b2, c + 1, 0, y) +\n       (a*(a + 1)*b1*(b1 + 1)/(2*c*(c + 1)))*x**2*appellf1(a + 2, b1 + 2, b2, c + 2, 0, y) + O(x**3))\n    tn(f.series(y, 0, 3), appellf1(a, b1, b2, c, x, 0) +\n       (a*b2/c)*y*appellf1(a + 1, b1, b2 + 1, c + 1, x, 0) +\n       (a*(a + 1)*b2*(b2 + 1)/(2*c*(c + 1)))*y**2*appellf1(a + 2, b1, b2 + 2, c + 2, x, 0) + O(y**3))\n\n", "    def test_appellf1_derivative():\n        a = randcplx()\n        b1 = randcplx()\n        b2 = randcplx()\n        c = randcplx()\n        x = randcplx()\n        y = randcplx()\n\n        f = appellf1(a, b1, b2, c, x, y)\n        # Test derivative with respect to x\n        d = Derivative(f, x).doit()\n        td(f, d, x)\n\n        # Test derivative with respect to y\n        d = Derivative(f, y).doit()\n        td(f, d, y)\n"], "sample_746": ["    def test_brier_score_loss_multilabel(self):\n        # test brier score for multilabel classification\n        y_true = np.array([[1, 0], [0, 1], [1, 1]])\n        y_prob = np.array([[0.8, 0.2], [0.2, 0.8], [0.6, 0.5]])\n\n        score1 = brier_score_loss(y_true, y_prob)\n        \n        # test with pos_label for multilabel\n        score2 = brier_score_loss(y_true, y_prob, pos_label=0)\n        \n        score3 = brier_score_loss(y_true, y_prob, pos_label=1)\n        \n        assert_almost_equal(score1, (0.04 + 0.04 + 0.04) / 3)\n        assert_almost_equal(score2, (0.04 + 0.04 + 0.08) / 3)\n        assert_almost_equal(score3, (0.04 + 0.04 + 0.08) / 3)\n\n", "    def test_brier_score_loss_with_labels(self):\n        y_true = np.array([0, 1, 1, 0])\n         y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        \n        # Test with labels\n        for pos_label in [1, '1']:\n            score = brier_score_loss(y_true, y_prob, pos_label=pos_label)\n            assert_almost_equal(score, 0.0375)\n\n\n\n", "    def test_empty_sample_weight_hinge_loss():\n        y_true = np.array([1, -1, 1, -1])\n        pred_decision = np.array([1.1, -0.5, 0.2, -1.2])\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=UndefinedMetricWarning)\n            assert_equal(hinge_loss(y_true, pred_decision, sample_weight=[]),\n                         hinge_loss(y_true, pred_decision))\n", "    def test_brier_loss_binary_with_weights_and_pos_label(self):\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        sample_weight = np.array([0.5, 1, 1.5, 0.5])\n        # Test with pos_label=0\n        expected_score = 0.1375\n        score = brier_score_loss(\n            y_true, y_prob, sample_weight=sample_weight, pos_label=0\n        )\n        assert_almost_equal(score, expected_score)\n\n", "    def test_brier_score_loss_binary_with_weights(self):\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        sample_weight = np.array([0.5, 1, 1.5, 0.5])\n        score = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n        assert_almost_equal(score, 0.04625)\n"], "sample_1146": ["def test_latex_SingularityFunction():\n    assert latex(SingularityFunction(x, 2, 1)) == r\"\\operatorname{S}_{2,1}\\left(x\\right)\"\n\n", "    def test_latex_printing_of_operator_symbols(self):\n        x, y, z, t = symbols('x y z t')\n        assert latex(Operator(x, y, z)) == r'\\operatorname{Operator}\\left(x, y, z\\right)'\n        assert latex(Operator(x, y, z).subs(x, t)) == r'\\operatorname{Operator}\\left(t, y, z\\right)'\n", "    def test_latex_lowergamma(self):\n        assert latex(lowergamma(x, y)) == '\\\\operatorname{lowergamma}\\\\left(x, y \\\\right)'\n", "def test_latex_SetExpr():\n    x = symbols('x')\n    assert latex(Union(Interval(0, 1), Interval(2, 3))) == r'\\left\\{ x \\in \\mathbb{R} \\mid x \\in \\left(0, 1\\right) \\text{ or } x \\in \\left(2, 3\\right) \\right\\}'\n    assert latex(Intersection(Interval(0, 1), Interval(0.5, 2))) == r'\\left\\{ x \\in \\mathbb{R} \\mid x \\in \\left(0.5, 1\\right) \\right\\}'\n    assert latex(Complement(Interval(0, 1), Interval(0.5, 1))) == r'\\left\\{ x \\in \\mathbb{R} \\mid x \\in \\left(0, 0.5\\right) \\right\\}'\n\n", "def test_latex_MatrixSymbol():\n    A = MatrixSymbol('A', 2, 2)\n    assert latex(A) == 'A'\n\n    B = MatrixSymbol('B', 2, 2, real=True)\n    assert latex(B) == 'B'\n\n    C = MatrixSymbol('C', 2, 2, imaginary=True)\n    assert latex(C) == 'C'\n"], "sample_460": ["    def test_edit_inline_with_empty_dates(self):\n        response = self.client.get(reverse(\"admin:admin_views_section_change\", args=(self.s1.pk,)))\n        self.assertContains(response, '<input type=\"text\" name=\"article_set-0-date_0\"')\n\n        # Submit the form with empty date fields\n        response = self.client.post(\n            reverse(\"admin:admin_views_section_change\", args=(self.s1.pk,)),\n            self.inline_post_data,\n            follow=True,\n        )\n        self.assertContains(response, 'Please correct the errors below.')\n", "    def test_admin_site_index(self):\n        response = self.client.get(reverse(\"admin:index\"))\n        self.assertContains(response, \"Welcome to the Django admin!\")\n        self.assertEqual(response.status_code, 200)\n", "    def test_add_related_object(self):\n        response = self.client.get(reverse(\"admin:polls_question_add\"))\n        self.assertEqual(response.status_code, 200)\n\n        response = self.client.post(\n            reverse(\"admin:polls_question_add\"),\n            {\n                \"question_text\": \"What's your favorite color?\",\n                \"pub_date_0\": \"2023-03-23\",\n                \"pub_date_1\": \"12:00:00\",\n            },\n        )\n        self.assertRedirects(response, reverse(\"admin:polls_question_changelist\"))\n\n\n", "    def test_get_view_on_site(self):\n        response = self.client.get(reverse(\"admin:view_on_site\", args=[1, 2]))\n        self.assertEqual(response.status_code, 302)\n        self.assertRedirects(response, reverse(\"admin:index\"))\n", "    def test_changelist_filtering_with_foreign_key(self):\n        response = self.client.get(reverse(\"admin:admin_views.books_changelist\"))\n        self.assertContains(response, 'Book 1')\n        self.assertContains(response, 'Book 2')\n        response = self.client.get(\n            reverse(\"admin:admin_views.books_changelist\") + \"?promo__name=Promo%201\"\n        )\n        self.assertContains(response, 'Book 1')\n        self.assertNotContains(response, 'Book 2')\n"], "sample_21": ["def test_read_qdp_no_header():\n    lines = \"\"\"", "def test_write_read_masked():\n    \"\"\"Test that masked values are written and read correctly.\"\"\"\n    data = {\"col1\": [1, 2, np.ma.masked, 4], \"col2\": [5, np.ma.masked, 7, 8]}\n    t = Table(data)\n    lines = _write_table_qdp(t)\n    new_t = _read_table_qdp(\"\\n\".join(lines))\n    assert np.allclose(t[\"col1\"], new_t[\"col1\"])\n    assert np.allclose(t[\"col2\"], new_t[\"col2\"])\n\n", "def test_read_write_err():\n    # Create a table with errors\n\n    data = {\n        \"a\": [1, 2, 3],\n        \"a_perr\": [0.1, 0.2, 0.3],\n        \"a_nerr\": [-0.1, -0.2, -0.3],\n        \"b\": [4, 5, 6],\n        \"b_err\": [0.5, 0.6, 0.7],\n    }\n    t = Table(data)\n\n    # Write and read back\n    filename = \"test.qdp\"\n    _write_table_qdp(t, filename)\n    t_read = ascii.read(filename, format=\"qdp\")\n\n    # Compare the original and read-back tables\n    assert np.allclose(t[\"a\"], t_read[\"a\"])\n    assert np.allclose(t[\"a_perr\"], t_read[\"a_perr\"])\n    assert np.allclose(t[\"a_nerr\"], t_read[\"a_nerr\"])\n    assert np.allclose(t[\"b\"], t_read[\"b\"])\n    assert np.allclose(t[\"b_err\"], t_read[\"b_err\"])\n\n\n", "def test_read_qdp_empty_table():\n    lines = [\n        \"! Initial comment line 1\",\n        \"! Initial comment line 2\",\n        \"READ TERR 1\",\n        \"!\",\n        \"NO NO NO NO NO NO\",\n        \"! Table 1 comment\",\n    ]\n    table = _read_table_qdp(lines, table_id=0)\n    assert len(table) == 0\n    assert table.meta[\"initial_comments\"] == [\"! Initial comment line 1\", \"! Initial comment line 2\"]\n    assert table.meta[\"comments\"] == [\"! Table 1 comment\"]\n\n\n\n", "    def test_write_read_qdp_with_error_columns():\n        \"\"\"Test writing and reading back a table with error columns.\"\"\"\n        data = {'a': [1, 2, 3], 'a_err': [0.1, 0.2, 0.3],\n                'b': [4, 5, 6], 'b_perr': [0.4, 0.5, 0.6], 'b_nerr': [0.7, 0.8, 0.9]}\n        table = Table(data)\n\n        # Write the table to a temporary file\n        with open(\"test.qdp\", \"w\") as f:\n            _write_table_qdp(table, filename=\"test.qdp\", err_specs={'serr': [1], 'terr': [2]})\n\n        # Read the table back from the file\n        table_read = ascii.read(\"test.qdp\", format='ascii.qdp')\n\n        # Compare the original and read tables\n        assert np.allclose(table_read['a'], table['a'])\n        assert np.allclose(table_read['a_err'], table['a_err'])\n        assert np.allclose(table_read['b'], table['b'])\n        assert np.allclose(table_read['b_perr'], table['b_perr'])\n        assert np.allclose(table_read['b_nerr'], table['b_nerr'])\n\n"], "sample_548": ["def test_colorbar_extendfrac_limits():\n    cmap, norms = _get_cmap_norms()\n    fig, axs = plt.subplots(2, 2)\n\n    for ax, extendfrac in zip(axs.flat, [0.1, 'auto', 0.5, 1]):\n        cbar = Colorbar(ax, cmap=cmap, norm=norms['both'],\n                        extend='both', extendfrac=extendfrac)\n        ax.set_title(f'extendfrac={extendfrac}')\n", "    def test_colorbar_ticklocation(self):\n        fig, ax = plt.subplots()\n        norm = Normalize(vmin=0, vmax=100)\n        sc = ax.scatter([0, 1, 2], [0, 1, 2], c=np.arange(3), cmap='viridis', norm=norm)\n        cb = fig.colorbar(sc, ax=ax, location='top')\n        assert cb.ax.get_position().y1 >= ax.get_position().y1\n        cb.set_label(\"Label\")\n        cb.set_ticklocation('bottom')\n        assert cb.ax.get_position().y0 <= ax.get_position().y0\n\n\n", "    def test_colorbar_extension_length_ticks(self):\n        fig, ax = plt.subplots()\n        norm = Normalize(vmin=0, vmax=100)\n        cmap = cm.viridis\n        cb = Colorbar(ax, cmap=cmap, norm=norm,\n                      extend='both', extendfrac=0.1)\n        # Make sure ticks are correctly placed\n        assert len(cb.ax.get_xticks()) == 3\n\n        # Check tick locations\n        ticks = cb.ax.get_xticks()\n        assert np.allclose(ticks, [0, 50, 100])\n        plt.close('all')\n", "def test_colorbar_ticks():\n    fig, ax = plt.subplots()\n    im = ax.imshow([[1, 2], [3, 4]], cmap='viridis')\n    cb = fig.colorbar(im)\n\n    # Test default major/minor tick locators\n    assert isinstance(cb.ax.yaxis.get_major_locator(),\n                      mticker.MaxNLocator)\n    assert isinstance(cb.ax.yaxis.get_minor_locator(),\n                      mticker.AutoMinorLocator)\n\n    # Test setting custom locators\n    cb.ax.yaxis.set_major_locator(mticker.MultipleLocator(1))\n    cb.ax.yaxis.set_minor_locator(mticker.MultipleLocator(0.5))\n    assert isinstance(cb.ax.yaxis.get_major_locator(),\n                      mticker.MultipleLocator)\n    assert isinstance(cb.ax.yaxis.get_minor_locator(),\n                      mticker.MultipleLocator)\n", "    def test_colorbar_ticks_formatter(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow([[1, 2], [3, 4]], cmap='viridis')\n        cb = plt.colorbar(im)\n\n        # Test default format\n        assert isinstance(cb.ax.yaxis.get_major_formatter(), mcolors.ScalarFormatter)\n\n        # Test custom Formatter\n        cb.set_ticks([1, 2, 3, 4])\n        cb.ax.yaxis.set_major_formatter(LogFormatter())\n        ax.figure.canvas.draw()\n\n        # Check tick labels are formatted with LogFormatter\n        for label in cb.ax.yaxis.get_ticklabels():\n            assert label.get_text().startswith('1.0') or label.get_text().startswith(\n                '10.0')\n\n        cb.remove()\n"], "sample_577": ["    def test_multi_facet_stat(self):\n\n        p = Plot()\n        p.add(MockMark(), \"scatter\", color=\"species\")\n        p.add(Agg(), \"mean\", x=\"sepal_width\", y=\"sepal_length\")\n        p.stat_params[\"mean\"][\"estimator\"] = np.mean\n\n        data = pd.DataFrame({\n            \"sepal_width\": [3, 3.5, 4, 3, 3.2, 3.8],\n            \"sepal_length\": [5, 4.5, 5.5, 4.8, 4.9, 5.2],\n            \"species\": [\"setosa\", \"setosa\", \"setosa\", \"versicolor\", \"versicolor\", \"versicolor\"],\n        })\n\n        result = p.plot(data=data, facets=[\"species\"])\n\n        assert result.n_splits == 2\n        assert len(result.passed_data) == 2\n\n        expected_data = [\n            data.loc[data[\"species\"] == \"setosa\"],\n            data.loc[data[\"species\"] == \"versicolor\"],\n        ]\n\n        for i, df in enumerate(result.passed_data):\n            assert_frame_equal(df, expected_data[i])\n\n", "    def test_faceted_plot_with_shared_axes(self):\n\n        data = pd.DataFrame({\n            \"x\": np.random.randn(100),\n            \"y\": np.random.randn(100),\n            \"g\": [\"A\"] * 50 + [\"B\"] * 50,\n            \"h\": [\"1\"] * 25 + [\"2\"] * 25 + [\"1\"] * 25 + [\"2\"] * 25,\n        })\n\n        p = Plot()\n        p.add(\n            MockMark(),\n            data,\n            x=\"x\",  y=\"y\",\n            col=\"g\", row=\"h\",\n            sharex=True, sharey=True,\n        )\n\n        p.plot()\n        assert p._subplots[0][\n            \"ax\"\n        ].get_shared_x_axes().get_siblings(p._figure) == [\n            p._subplots[i][\"ax\"] for i in range(4)\n        ]\n\n        assert p._subplots[0][\n            \"ax\"\n        ].get_shared_y_axes().get_siblings(p._figure) == [\n            p._subplots[i][\"ax\"] for i in range(4)\n        ]\n\n", "    def test_pair_spec_structure(self):\n\n        p = Plot(data=pd.DataFrame(\n            {\"a\": [1, 2, 3], \"b\": [4, 5, 6], \"c\": [7, 8, 9]}\n        ))\n        p._pair_spec = {\"x\": [\"a\"], \"y\": [\"b\"], \"col\": [\"c\"]}\n\n        # Test that the pairs are correctly identified\n        assert p._pair_spec[\"structure\"] == {\"a\": \"x\", \"b\": \"y\", \"c\": \"col\"}", "    def test_facetgrid_limits(self):\n\n        p = Plot(data=pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6], \"col\": [\"a\", \"a\", \"b\"]}),\n                 x=\"x\", y=\"y\", col=\"col\", kind=\"scatter\")\n\n        p._scales[\"x\"] = Continuous(limits=(0, 4))\n        p._limits = {\"x\": (1, 3)}\n        p._setup_subplots()\n\n        p._plot()\n\n        for ax in p._subplots:\n            assert_array_equal(ax[\"ax\"].get_xlim(), (1, 3))\n", "    def test_empty_frame(self):\n        df = pd.DataFrame({'x': [], 'y': []})\n        p = Plot()\n        p.add(MockMark(), data=df, x='x', y='y')\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n            # TODO what warnings should we expect?\n            p.plot()\n\n        assert p._subplots[0][\"ax\"].get_xlim() == (0, 1)\n        assert p._subplots[0][\"ax\"].get_ylim() == (0, 1)\n\n"], "sample_195": ["    def test_no_transaction_support(self):\n        with self.assertRaises(NotSupportedError):\n            with transaction.atomic():\n                pass\n\n\n\n", "    def test_combine_duration_expression(self):\n        with transaction.atomic():\n            d1 = DurationField().from_db_value(timezone.timedelta(seconds=30))\n            d2 = DurationField().from_db_value(timezone.timedelta(seconds=10))\n            with self.assertRaisesMessage(DatabaseError, 'Invalid connector for timedelta: *.'):\n                self.ops.combine_duration_expression('*', [d1, d2])\n            with self.assertRaisesMessage(ValueError, 'Too many params for timedelta operations.'):\n                self.ops.combine_duration_expression('+', [d1, d2, d2])\n            self.assertEqual(\n                self.ops.combine_duration_expression('+', [d1, d2]),\n                'django_format_dtdelta(\\'+\\', \\'%s\\', \\'%s\\')' % (d1.get_db_prep_value(None), d2.get_db_prep_value(None))\n            )\n            self.assertEqual(\n                self.ops.combine_duration_expression('-', [d1, d2]),\n                'django_format_dtdelta(\\'-\\', \\'%s\\', \\'%s\\')' % (d1.get_db_prep_value(None), d2.get_db_prep_value(None))\n            )\n", "    def test_bulk_insert_sql(self):\n        fields = ['name', 'email']\n        placeholder_rows = [\n            ['John Doe', 'john@example.com'],\n            ['Jane Doe', 'jane@example.com'],\n        ]\n        expected_sql = \"SELECT 'John Doe', 'john@example.com' UNION ALL SELECT 'Jane Doe', 'jane@example.com'\"\n        self.assertEqual(self.ops.bulk_insert_sql(fields, placeholder_rows), expected_sql)\n", "    def test_subtract_temporals(self):\n        with transaction.atomic():\n            Author.objects.create(name='Author A',\n                                   joined_date=timezone.now())\n            author = Author.objects.get(name='Author A')\n            with self.assertRaisesMessage(\n                NotSupportedError,\n                self.may_require_msg % 'subtract_temporals',\n            ):\n                self.ops.subtract_temporals('DateTimeField',\n                                            (self.ops.quote_name('joined_date'),\n                                             (author.id,)),\n                                            (self.ops.quote_name('joined_date'),\n                                             (author.id,)))\n", "    def test_format_for_duration_arithmetic(self):\n        with transaction.atomic():\n            b = Book.objects.create(name='Test Book', author=Author.objects.create(name='Test Author'), publication_date=timezone.now())\n            d = duration_field = DurationField()\n            self.assertEqual(self.ops.format_for_duration_arithmetic(d), d) \n"], "sample_659": ["    def test_raises_unicode(self, testdir):\n        testdir.create_test_file(\"\"\"\n            import pytest\n\n                raise UnicodeError(\"some unicode error\")\n\n            pytest.raises(UnicodeError, test_func)\n        \"\"\")\n        result = testdir.runpytest()\n        assert result.ret == 0\n\n", "    def test_raises_with_message_match(self):\n        with pytest.raises(ValueError, message=\"somemessage\"):\n            raise ValueError(\"somemessage\")\n", "    def test_raises_deprecated_warning_for_raises_without_match(self, recwarn):\n        with pytest.raises(DeprecationWarning, match=\"Using 'raises without 'match' is deprecated\"):\n            pytest.raises(ValueError)\n", "def test_explicit_raises_with_contextmanager():\n    with pytest.raises(ValueError) as excinfo:\n        with dummy_context_manager():\n            raise ValueError\n    assert \"ValueError\" in str(excinfo.value)\n", "    def test_raises_type_and_value(self):\n        with pytest.raises(ValueError, match=\"hello\"):\n            raise ValueError(\"hello\")\n"], "sample_945": ["def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('str?') == ('str', True)\n    assert _parse_annotation('List[str]') == ('List', ('str',))\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Callable[[int, str], None]') == ('Callable', (('int', 'str'), 'None'))\n\n", "def test_parse_annotation():\n    assert _parse_annotation('None') == ('None', None)\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('list[str]') == ('list', 'str')\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n    assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n    assert _parse_annotation('Tuple[str, int, bool]') == ('Tuple', ('str', 'int', 'bool'))\n", "def test_python_domain_parse_signature_default_values():\n    \"\"\"Test parsing of default values in function signatures.\"\"\"\n    env = Mock()\n    env.ref_context = {}\n    domain = PythonDomain(env, None)\n    \n\n    # Test default values parsed correctly\n    sig = 'test_func(arg1, arg2=1, arg3=\"hello\")'\n    signode = addnodes.desc_signature(sig, '')\n    domain.parse_signature(signode, sig)\n    assert 'arg2=1' in signode.astext()\n    assert 'arg3=\"hello\"' in signode.astext()\n    \n    # Test complex default value\n    sig = 'test_func(arg1, arg2=[1,2,3], arg3={\"foo\": \"bar\"})'\n    signode = addnodes.desc_signature(sig, '')\n    domain.parse_signature(signode, sig)\n    assert 'arg2=[1, 2, 3]' in signode.astext()\n    assert 'arg3={\"foo\": \"bar\"}' in signode.astext()\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", None)\n    assert _parse_annotation(\"str, int\") == (\"str\", (\"int\", None))\n    assert _parse_annotation(\"str: int\") == (\"str\", (\"int\", None))\n    assert _parse_annotation(\"List[int]\") == (\"List\", (\"int\", None))\n    assert _parse_annotation(\"Optional[str]\") == (\"Optional\", (\"str\", None))\n    assert _parse_annotation(\"Union[str, int]\") == (\"Union\", (\"str\", (\"int\", None)))\n    assert _parse_annotation(\"Dict[str, int]\") == (\"Dict\", (\"str\", (\"int\", None)))\n    assert _parse_annotation(\"Callable[[int, str], bool]\") == (\"Callable\", ([(\"int\", None), (\"str\", None)], (\"bool\", None)))\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('typing.List[str]'), 'typing.List[str]')\n        self.assertEqual(_parse_annotation('list of str'), 'list of str')\n        self.assertEqual(_parse_annotation('something.Module.Class'), 'something.Module.Class')\n        self.assertEqual(_parse_annotation('typing.Optional[int]'), 'typing.Optional[int]')\n        self.assertEqual(_parse_annotation('typing.Union[int, str]'), 'typing.Union[int, str]')\n        self.assertEqual(_parse_annotation('Dict[str, int]'), 'Dict[str, int]')\n"], "sample_163": ["    def test_password_reset_confirm_get_user_error(self):\n        with mock.patch(\"django.contrib.auth.views.urlsafe_base64_encode\") as mock_b64encode:\n            mock_b64encode.return_value = \"invalid-uid\"\n            response = self.client.get(\n                reverse(\"password_reset_confirm\", kwargs={\"uidb64\": \"invalid-uid\", \"token\": \"valid-token\"})\n            )\n            self.assertEqual(response.status_code, 200)\n            self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n            self.assertFalse(response.context_data[\"validlink\"])\n", "    def test_redirect_to_login_with_next(self):\n        response = redirect_to_login(next=\"/some/page/\")\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response[\"Location\"], \"/accounts/login/?next=/some/page/\")\n", "    def test_login_view_redirecting_authenticated_user(self):\n        self.login()\n        with self.assertRaises(ValueError):\n            self.client.get(reverse('login'))\n", "    def test_password_reset_confirm_redirect_with_session_token(self):\n        user = User.objects.create_user(username=\"testclient\", password=\"password\")\n        token = urlsafe_base64_encode(str(user.pk).encode()).decode()\n        url = reverse(\"password_reset_confirm\", kwargs={\"uidb64\": token, \"token\": \"some-token\"})\n        self.client.session.set_expiry(0)  # Expire session immediately\n        self.client.session[INTERNAL_RESET_SESSION_TOKEN] = \"some-token\"\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response[\"Location\"], urljoin(url, \"password_reset/\"))\n", "    def test_logout_then_login_redirect(self):\n        self.login()\n        response = logout_then_login(self.client.request, login_url='/accounts/login/')\n        self.assertRedirects(response, '/accounts/login/?next=/')\n"], "sample_456": ["    def test_formset_absolute_max(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"3\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"foo\",\n            \"choices-0-votes\": \"1\",\n            \"choices-1-choice\": \"bar\",\n            \"choices-1-votes\": \"2\",\n            \"choices-2-choice\": \"baz\",\n            \"choices-2-votes\": \"3\",\n        }\n\n        formset = self.make_choiceformset(\n            formset_data=[(\"foo\", 1), (\"bar\", 2), (\"baz\", 3)],\n            total_forms=3,\n            absolute_max=2,\n        )\n\n        self.assertRaises(ValidationError, formset.full_clean)\n        self.assertEqual(formset.errors.as_data(), [\n            {'choice': [], 'votes': []},\n            {'choice': [], 'votes': []},\n            {'choice': ['This field is required.'], 'votes': ['This field is required.']},\n        ])\n\n", "    def test_formset_factory_with_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            max_num=5,\n            absolute_max=3,\n        )\n        formset = formset_class(data={\n            \"choices-TOTAL_FORMS\": \"4\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-MAX_NUM_FORMS\": \"5\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Choice 1\",\n            \"choices-0-votes\": \"1\",\n            \"choices-1-choice\": \"Choice 2\",\n            \"choices-1-votes\": \"2\",\n            \"choices-2-choice\": \"Choice 3\",\n            \"choices-2-votes\": \"3\",\n            \"choices-3-choice\": \"Choice 4\",\n            \"choices-3-votes\": \"4\",\n        })\n\n        self.assertEqual(formset.errors, [])\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 3)\n\n\n", "    def test_formset_can_delete(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"3\",\n            \"choices-INITIAL_FORMS\": \"0\",\n            \"choices-0-choice\": \"Pizza\",\n            \"choices-0-votes\": \"5\",\n            \"choices-1-choice\": \"Burgers\",\n            \"choices-1-votes\": \"3\",\n            \"choices-1-DELETE\": \"on\",\n            \"choices-2-choice\": \"Tacos\",\n            \"choices-2-votes\": \"2\",\n        }\n\n        formset = self.make_choiceformset(\n            formset_data=[(\"Pizza\", 5), (\"Burgers\", 3), (\"Tacos\", 2)],\n            can_delete=True,\n            data=data,\n        )\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.cleaned_data), 2)\n        self.assertEqual(formset.cleaned_data[0][\"choice\"], \"Pizza\")\n        self.assertEqual(formset.cleaned_data[1][\"choice\"], \"Tacos\")\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            max_num=3,\n            absolute_max=2,\n        )\n        formset = formset_class(data={\n            'choices-TOTAL_FORMS': '3',\n            'choices-0-choice': 'option1',\n            'choices-0-votes': '1',\n            'choices-1-choice': 'option2',\n            'choices-1-votes': '2',\n            'choices-2-choice': 'option3',\n            'choices-2-votes': '3',\n\n        })\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [\n            ErrorList([\n                ValidationError('Please submit at most 2 forms.', code='too_many_forms')\n            ])\n        ])\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            max_num=10,\n            absolute_max=5,\n        )\n\n        formset = formset_class(data={\n            \"choices-TOTAL_FORMS\": \"6\",\n            \"choices-0-choice\": \"foo\",\n            \"choices-0-votes\": \"1\",\n        })\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(\n            formset.errors.as_data(),\n            {\n                \"non_form_errors\": [\n                    {\"code\": \"too_many_forms\", \"message\": \"Please submit at most 5 forms.\"}\n                ]\n            },\n        )\n"], "sample_109": ["    def test_autocomplete_select_required_empty_value(self):\n        form = RequiredBandForm()\n        expected_options = (\n            f'<select name=\"band\" id=\"id_band\" class=\"my-class vTextField admin-autocomplete\" '\n            f'data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" '\n            f'data-ajax--url=\"/admin/music/band/autocomplete/\" data-theme=\"admin-autocomplete\" '\n            f'data-allow-clear=\"false\" data-placeholder=\"\">'\n            f'{self.empty_option}'\n            f'</select>'\n        )\n\n        self.assertEqual(str(form['band']), expected_options)\n", "    def test_autocomplete_select_not_required(self):\n        form = NotRequiredBandForm()\n        self.assertEqual(form.as_p(), '<p><label for=\"id_band\">Band:</label><select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/music/band/autocomplete/\" data-theme=\"admin-autocomplete\" data-allow-clear=\"true\" data-placeholder=\"\" class=\"my-class admin-autocomplete\" id=\"id_band\" name=\"band\"><option value=\"\"></option></select></p>')\n\n", "    def test_autocomplete_select_required_field(self):\n        form = RequiredBandForm()\n        self.assertFalse(self.empty_option in form.as_p())\n", "    def test_required_field_no_empty_option(self):\n        form = RequiredBandForm()\n        self.assertEqual(form.as_p(), '<p><label for=\"id_band\">Band:</label><select id=\"id_band\" class=\"my-class select2-hidden-accessible\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/myapp/album/band_autocomplete/\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"\" name=\"band\"><option value=\"\"></option></select></p>')\n\n\n\n", "    def test_required_select_should_not_have_empty_option(self):\n        form = RequiredBandForm()\n        self.assertEqual(\n            str(form.as_p()),\n            '<p><label for=\"id_band\">Band:</label> <select id=\"id_band\" name=\"band\" class=\"admin-autocomplete my-class AutocompleteSelect\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/band/autocomplete/\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"\">'\n        )\n\n"], "sample_800": ["    def test_check_estimators_unfitted(self):\n        # Test that estimators raise an exception when accessed before fitting\n\n        for _, estimator_orig in enumerate(self.estimator_list):\n            with self.subTest(estimator=estimator_orig.__class__.__name__):\n                try:\n                    estimator = clone(estimator_orig)\n                    estimator.predict(np.array([[0, 0]]))\n                except NotFittedError:\n                    pass\n\n                except Exception as e:\n                    raise AssertionError(\n                        f\"Unexpected exception raised: {type(e)}. \"\n                        f\"Estimators should raise NotFittedError \"\n                        f\"before fitting.\"\n                    )\n                else:\n                    raise AssertionError(\n                        f\"Estimator {estimator.__class__.__name__} \"\n                        f\"did not raise an error when accessed before fitting.\"\n\n                    )\n", "    def test_check_estimators_non_transformer(self):\n        # Test whether check_estimators ignores transformers\n        skip_for_classes = ['TransformerMixin', 'TransformedTargetRegressor',\n                            'MultiOutputMixin']\n        check_estimator(RandomForestClassifier(),\n                        skip_for_classes=skip_for_classes)\n        check_estimator(LogisticRegression(),\n                        skip_for_classes=skip_for_classes)\n        check_estimator(SVC(),\n                        skip_for_classes=skip_for_classes)\n        check_estimator(GaussianMixture(),\n                        skip_for_classes=skip_for_classes)\n        check_estimator(MiniBatchKMeans(),\n                        skip_for_classes=skip_for_classes)\n\n\n\n", "    def test_check_estimator_sparse_input(self):\n        # Test for estimators that don't support sparse input\n        msg = \"Estimator does not accept sparse matrices.\"\n        set_random_state(RandomForestClassifier())\n        with self.assertRaisesRegex(ValueError, msg):\n            check_estimator(RandomForestClassifier(),\n                            sparse_input=True)\n\n        set_random_state(NoSparseClassifier())\n        with self.assertRaisesRegex(ValueError, msg):\n            check_estimator(NoSparseClassifier(),\n                            sparse_input=True)\n", "    def test_requires_positive_y_regressor(self):\n        # Check error when y is not positive\n        regressor = RequiresPositiveYRegressor()\n        X = [[1]]\n        y = [-1]\n        assert_raises(ValueError, regressor.fit, X, y)\n        # Check that it works with positive y\n        y = [1]\n        regressor.fit(X, y)\n\n", "    def test_check_estimators_unfitted(self):\n        estimator = ChangesDict()\n        check_estimators_unfitted(estimator, \"key\")\n        assert estimator.key == 0\n\n        estimator = SetsWrongAttribute(acceptable_key='wrong')\n        check_estimators_unfitted(estimator, \"acceptable_key\")\n        assert not hasattr(estimator, 'wrong_attribute')\n\n        estimator = ChangesWrongAttribute()\n        check_estimators_unfitted(estimator, \"wrong_attribute\")\n        assert estimator.wrong_attribute == 0\n\n        estimator = ChangesUnderscoreAttribute()\n        check_estimators_unfitted(estimator, \"_good_attribute\")\n        assert not hasattr(estimator, '_good_attribute')\n"], "sample_637": ["    def test_fixme_with_pragma(self):\n        stmts = _tokenize_str(\n            \"\"\"\n            # pylint: disable=some-message\n            # FIXME: needs refactoring\n                pass\n            \"\"\"\n        )\n        with set_config(notes=['FIXME']):\n            self. Checker = misc.EncodingChecker()\n            self.Checker.process_tokens(stmts)\n            self.assertMessageCount(1)\n\n", "    def test_multiple_notes(self):\n        stmt = \"\"\"\n        # FIXME: this needs to be fixed\n            pass\n        # TODO: implement this\n        \"\"\"\n\n        tokens = _tokenize_str(stmt)\n        checker = self.checker_class(self.linter)\n        checker.process_tokens(tokens)\n        self.assertAddsMessages(\n            checker,\n            expected_messages=[\n                MessageTest(msg_id=\"fixme\", line=2, col_offset=12, args=(\"FIXME: this needs to be fixed\")),\n                MessageTest(msg_id=\"fixme\", line=5, col_offset=12, args=(\"TODO: implement this\")),\n            ],\n        )\n\n", "    def test_fixme_in_comment(self):\n        stmts = _tokenize_str(\n            \"\"\"\n            # This is a FIXME\n            #FIXME something to fix\n            # TODO: do this later\n            \"\"\"\n        )\n        with set_config(notes=[\"FIXME\", \"TODO\"]):\n            self.checker.process_tokens(stmts)\n        self.assertAddsMessages(\n            MessageTest(\n                message_id=\"fixme\", line=2, args=\"# This is a FIXME\", col_offset=1\n            ),\n            MessageTest(\n                message_id=\"fixme\", line=3, args=\"#FIXME something to fix\", col_offset=1\n            ),\n        )\n", "    def test_fixme_with_nonascii_char(self):\n        source = \"\"\"", "    def test_fixme_with_rgx(self):\n        code = \"\"\"\n        # FIXME: this is a bug\n        # TODO: implement this feature\n        # XXX: this is a hack\n        # This is a regular comment\n        \"\"\"\n        tokens = _tokenize_str(code)\n        self.checker.set_config(notes_rgx='FIXME|TODO|XXX')\n        self.checker.process_tokens(tokens)\n        self.assertMessage(\n            'fixme',\n            line=2,\n            col_offset=1,\n            msg='FIXME: this is a bug'\n        )\n\n"], "sample_193": ["    def test_many_to_many_field_operations(self):\n        apps = Apps()\n        apps.populate(get_related_models_recursive(UnicodeModel))\n\n        pre_state = ProjectState(apps=apps, models={\"test_migrations\": {\"UnicodeModel\": UnicodeModel}})\n        operation = AddField(\n            model_name=\"UnicodeModel\",\n            name=\"foods\",\n            field=models.ManyToManyField(UnicodeModel, related_name=\"related_unicode\"),\n        )\n\n        # Add the field.\n        post_state = operation.apply_to_project(pre_state)\n        self.assertEqual(\n            post_state.models[\"test_migrations\"][\"UnicodeModel\"].fields[\"foods\"].related_name,\n            \"related_unicode\",\n        )\n\n        # Delete the field.\n        operation = RemoveField(model_name=\"UnicodeModel\", name=\"foods\")\n        post_state = operation.apply_to_project(post_state)\n        self.assertFalse(\"foods\" in post_state.models[\"test_migrations\"][\"UnicodeModel\"].fields)\n", "    def test_m2m_with_custom_base(self):\n        \"\"\"\n        #23651 - Ensure models with custom bases are handled correctly\n        \"\"\"\n        class OtherModel(models.Model):\n            pass\n\n        class CustomBaseModel(models.Model):\n            custom_field = models.CharField(max_length=100)\n\n        class Food(CustomBaseModel):\n            name = models.CharField(max_length=100)\n            friends = models.ManyToManyField(OtherModel)\n\n        state_before = ProjectState.from_apps(Apps())\n        models = get_related_models_recursive(\n            state_before.models.values(),\n            Food,\n        )\n        initial_state = ModelState.from_models(models)\n\n        # Add an OtherModel instance\n        other_model = OtherModel.objects.create()\n\n        # Now change the Food instance\n        food = Food.objects.create(name=\"Pizza\", custom_field=\"test\")\n        food.friends.add(other_model)\n\n        state_after = ProjectState.from_apps(Apps())\n        models = get_related_models_recursive(\n            state_after.models.values(),\n            Food,\n        )\n        final_state = ModelState.from_models(models)\n\n        # Check that the state is updated correctly.\n        self.assertEqual(\n            initial_state.models[Food].fields.get('friends').related_model,\n            final_state.models[Food].fields.get('friends').related_model,\n        )\n", "    def test_many_to_many_field_with_through_model(self):\n        class Article(models.Model):\n            name = models.CharField(max_length=200)\n\n        class Author(models.Model):\n            name = models.CharField(max_length=200)\n            articles = models.ManyToManyField(Article, through=\"ArticleAuthor\")\n\n        class ArticleAuthor(models.Model):\n            article = models.ForeignKey(Article, on_delete=models.CASCADE)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            order = models.PositiveIntegerField()\n\n        # Create the initial state\n        project_state = ProjectState()\n        project_state.add_model(Article)\n        project_state.add_model(Author)\n        project_state.add_model(ArticleAuthor)\n\n        # Apply a migration that changes the order field to null\n        operation = AlterField(\n            model_name=\"articleauthor\",\n            name=\"order\",\n            field=models.PositiveIntegerField(null=True),\n        )\n        new_state = project_state.clone()\n        new_state.apply_operation(operation)\n\n        self.assertEqual(\n            new_state.models[\"articleauthor\"].fields[\"order\"].null, True\n        )\n", "    def test_many_to_many_implicit_through(self):\n        \"\"\"\n        #25429 - Ensuring ManyToManyField with an implicit through model\n        can be rendered correctly.\n        \"\"\"\n        with isolate_apps():\n            class Tag(models.Model):\n                name = models.CharField(max_length=50)\n\n            class Post(models.Model):\n                title = models.CharField(max_length=200)\n                tags = models.ManyToManyField(Tag)\n\n        initial_state = ProjectState.from_apps(Apps())\n        post_model_state = initial_state.models[\"myapp\"][\"Post\"]\n        self.assertIsNotNone(post_model_state.fields.get('tags'))\n        self.assertEqual(post_model_state.fields['tags'].remote_field.through.objects.name, \n                        'Post_tags')\n\n", "    def test_many_to_many_through_with_custom_manager(self):\n        apps = Apps()\n        apps.load_model('state_tests', 'Food', create=True)\n\n        # Setup initial state\n        food_model = FoodManager._meta.model\n        state = ProjectState.from_apps(apps)\n\n        # Check initial state\n        food_state = state.models[food_model]\n        self.assertEqual(food_state.fields['tags'].remote_field.through._meta.manager_name, 'objects')\n\n        # Apply operation to change manager\n        state = ApplyOperation(state, AddField(\n            model_name=\"Food\",\n            name=\"extra_field\",\n            field=models.CharField(max_length=10),\n            manager_name='custom_manager',\n        ))\n\n        food_state = state.models[food_model]\n        # Check updated state\n        self.assertEqual(food_state.fields['tags'].remote_field.through._meta.manager_name, 'custom_manager')\n"], "sample_1124": ["def test_FracElement_diff():\n    x, y = symbols('x y')\n    F, x = field('x', ZZ)\n    f = (x**2 + 1)/(x + 1)\n    assert f.diff(x) == (2*x*(x + 1) - (x**2 + 1))/(x + 1)**2\n", "    def test_evaluate_poly_domain():\n        R, x, y = ring(\"x,y\", ZZ)\n        F, x, y = field(\"x,y\", ZZ)\n        f = F(x + y)\n        assert f.evaluate([x, 1]) == F(2)\n        assert f.evaluate([x, 2], y=3) == F(5)\n        assert f.evaluate([y, 2], x=3) == F(5)\n\n", "def test_FracElement_subs():\n    F, x, y = field(\"x,y\", ZZ)\n    f = (x + 1) / (y + 2)\n\n    assert f.subs(x, 3) == (4/(y + 2))\n    assert f.subs(y, 3) == (x + 1)/5\n\n", "    def test_sfield_fraction_field_conversion(self):\n        x, y = symbols('x y')\n        K = sfield([x/(1+y)], x, y)\n        assert isinstance(K, FracField)\n        f = K.new((x, 1+y))\n        assert isinstance(f, FracElement)\n\n", "def test_sfield_from_expr():\n    x, y = symbols('x y')\n    F = sfield((x*log(x) + 4*x**2)*exp(1/x + log(x)/3)/x**2, x, y)\n    assert F[0].domain == QQ\n\n"], "sample_878": ["    def test_remainder_sparse_output_with_sparse_input(self):\n        sparse_data = sparse.csr_matrix([[1, 2], [3, 4]])\n        ct = ColumnTransformer(\n            transformers=[(\"trans\", Trans(), [0])],\n            remainder=\"passthrough\",\n            sparse_threshold=0,\n        )\n        # Force a sparse output\n        result = ct.fit_transform(sparse_data)\n        assert isinstance(result, sparse.csr_matrix)\n", "def test_column_transformer_passthrough_remainder_sparse():\n    \"\"\"Test that remainder='passthrough' works with sparse data.\"\"\"\n    X = sparse.csc_matrix([[1, 2], [3, 4]])\n    transformer = ColumnTransformer(\n        transformers=[('passthrough', 'passthrough', [0]), ('scaler', StandardScaler(), [1])],\n        remainder='passthrough'\n    )\n    Xt = transformer.fit_transform(X)\n    assert Xt.shape == (2, 3)\n    assert sparse.issparse(Xt)\n    assert_allclose_dense_sparse(Xt[:, 0], X[:, 0])  # Passthrough column\n    assert Xt[:, [1, 2]].nnz == 4  # Scaled column and remainder\n\n", "    def test_make_column_selector(self):\n        X = pd.DataFrame({'a': [1, 2, 3], 'b': [1.0, 2.0, 3.0], 'c': ['a', 'b', 'c']})\n\n        selector = make_column_selector(dtype_include=np.number)\n        assert_array_equal(selector(X), ['a', 'b'])\n\n        selector = make_column_selector(dtype_include=object)\n        assert_array_equal(selector(X), ['c'])\n\n        selector = make_column_selector(dtype_exclude=np.number)\n        assert_array_equal(selector(X), ['c'])\n\n        selector = make_column_selector(pattern='a')\n        assert_array_equal(selector(X), ['a'])\n\n", "    def test_sparse_output_with_all_sparse(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        ct = ColumnTransformer(\n            transformers=[\n                (\"trans1\", Trans(), [0]),\n                (\"trans2\", Trans(), [1]),\n            ],\n            sparse_threshold=0,\n        )\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, sparse.csr_matrix)", "    def test_sparse_output(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        X = np.vstack([X, np.zeros((1, 2))])\n        ct = ColumnTransformer(\n            transformers=[\n                (\"trans1\", Trans(), [0]),\n                (\"trans2\", Trans(), [1]),\n            ],\n            sparse_threshold=0.5,\n        )\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, sparse.csr_matrix)\n        assert Xt.shape == (3, 2)\n\n        ct = ColumnTransformer(\n            transformers=[\n                (\"trans1\", Trans(), [0]),\n                (\"trans2\", Trans(), [1]),\n            ],\n            sparse_threshold=0.2,\n        )\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, np.ndarray)\n        assert Xt.shape == (3, 2)\n"], "sample_531": ["    def test_pickle(self):\n        # test saving and restoring a figure\n        fig = Figure(figsize=(4, 4), dpi=100)\n        ax = fig.add_subplot(111)\n        ax.plot([1, 2, 3, 4], [5, 6, 7, 8])\n        ax.set_title('Test Figure')\n\n        # pickle the figure\n        pickled_fig = pickle.dumps(fig)\n\n        # unpickle the figure\n        loaded_fig = pickle.loads(pickled_fig)\n\n        # ensure the restored figure is identical to the original\n        assert loaded_fig.figsize == fig.figsize\n        assert loaded_fig.dpi == fig.dpi\n        assert type(loaded_fig.axes[0]) == type(fig.axes[0])\n", "    def test_set_constrained_layout(self):\n        fig, ax = plt.subplots()\n        fig.set_constrained_layout(True)\n        assert isinstance(fig.get_layout_engine(), ConstrainedLayoutEngine)\n        fig.set_constrained_layout(False)\n        assert fig.get_layout_engine() is None\n", "    def test_fig_set_size_inches_with_dpi(self):\n        fig = plt.figure()\n        fig.set_size_inches(10, 8)\n        assert fig.get_size_inches() == (10, 8)\n        fig.dpi = 200\n        fig.set_size_inches(5, 4)\n        assert fig.get_size_inches() == (5, 4)\n        \n", "def test_figure_savefig_bbox():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [1, 2, 3])\n\n    # Test saving with bbox_inches=\"tight\"\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format=\"png\", bbox_inches=\"tight\")\n        buf.seek(0)\n        img = Image.open(buf)\n        assert img.size[0] < 500  # Check image width is significantly smaller\n\n    # Test saving with bbox_inches=None (default)\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format=\"png\")\n        buf.seek(0)\n        img = Image.open(buf)\n        assert img.size[0] > 500  # Check image width is larger\n\n", "def test_figure_savefig_transparent():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    # Save the figure with transparency\n    with io.BytesIO() as buf:\n        fig.savefig(buf, transparent=True)\n        buf.seek(0)\n\n        # Check if the image has an alpha channel\n        img = Image.open(buf)\n        assert img.mode == 'RGBA'\n"], "sample_383": ["    def test_multicol_related_isnull_filter(self):\n        # Testing related IS NULL filter with a multi-column lookup\n        related_isnull_qs = Item.objects.filter(\n            tags__isnull=True, creator__isnull=True\n        )\n        self.assertEqual(related_isnull_qs.count(), 0)\n", "    def test_related_in_multi_field_lookup(self):\n        # Test multi-column lookups with RelatedIn\n        qs = Item.objects.filter(\n            tags__in=[self.t1, self.t3], creator__extra__info=\"e1\"\n        )\n        self.assertQuerysetEqual(\n            qs, [self.i2], attrgetter(\"name\")\n        )\n\n", "    def test_related_exact_on_multi_column_source(self):\n        # Simulate a related field lookup across multiple columns.\n        with self.assertNumQueries(1):\n            qs = (\n                Author.objects.annotate(\n                    item_name_and_created=F(\"item__name\") + F(\"item__created\")\n                )\n                .filter(item_name_and_created__exact=\"four2007-12-20 21:00:00\")\n                .values_list(\"name\", \"item__name\")\n            )\n            self.assertEqual(list(qs), [(\"a4\", \"four\")])\n\n", "    def test_multi_col_lookup_subquery(self):\n        # Test multicolumn lookups with subqueries for related fields.\n        # This test checks that the correct subquery is built and executed.\n\n        qs = Item.objects.filter(\n            author__in=Author.objects.filter(\n                extra__note__in=[self.n1, self.n2]\n            )\n        )\n\n        expected_sql = (\n            \"SELECT DISTINCT \"\n            '\"test_item\".\"id\", \"test_item\".\"name\", \"test_item\".\"created\", '\n            '\"test_item\".\"modified\", \"test_item\".\"creator_id\", \"test_item\".\"note_id\" '\n            \"FROM \"\n            '\"test_item\" INNER JOIN \"test_author\" ON ('\n            '\"test_item\".\"creator_id\" = \"test_author\".\"id\") WHERE '\n            '\"test_author\".\"id\" IN (SELECT DISTINCT \"test_author\".\"id\" FROM \"test_author\" '\n            'INNER JOIN \"test_extrainfo\" ON (\"test_author\".\"extra_id\" = \"test_extrainfo\".\"id\") '\n            'INNER JOIN \"test_note\" ON (\"test_extrainfo\".\"note_id\" = \"test_note\".\"id\") '\n            'WHERE \"test_note\".\"id\" IN (%s, %s))'\n            % (self.n1.pk, self.n2.pk)\n        )\n        with CaptureQueriesContext(connection) as queries:\n            list(qs)\n\n        self.assertEqual(len(queries), 2)\n        self.assertEqual(\n            str(queries[0]['sql']),\n            expected_sql\n        )\n", "    def test_related_isnull_with_multi_column_source(self):\n        with self.assertRaises(ValueError):\n            list(\n                Item.objects.filter(\n                    tags__isnull=True,\n                    extra=ExtraInfo.objects.filter(\n                        Q(value=42) & Q(filterable=False)\n                    ),\n                )\n            )\n\n"], "sample_164": ["    def test_require_debug_false_filter(self):\n        logger = logging.getLogger('test')\n        filter_ = RequireDebugFalse()\n        self.assertFalse(filter_.filter(logging.LogRecord(level=logging.ERROR, name='test')))\n        with override_settings(DEBUG=True):\n            self.assertTrue(filter_.filter(logging.LogRecord(level=logging.ERROR, name='test')))\n", "    def test_require_debug_false_filter(self):\n        handler = logging.StreamHandler(StringIO())\n        handler.addFilter(RequireDebugFalse())\n\n        record = logging.makeRecord('test', logging.ERROR, 'module', 1, 'message', None, None)\n\n        with override_settings(DEBUG=True):\n            self.assertFalse(handler.filter(record))\n\n        with override_settings(DEBUG=False):\n            self.assertTrue(handler.filter(record))\n", "    def test_require_debug_true_filter(self):\n        logging.basicConfig()\n\n        filter_ = RequireDebugTrue()\n\n        # Test when DEBUG is True\n        with override_settings(DEBUG=True):\n            self.assertTrue(filter_.filter(logging.makeRecord('test', logging.INFO)))\n\n        # Test when DEBUG is False\n        with override_settings(DEBUG=False):\n            self.assertFalse(filter_.filter(logging.makeRecord('test', logging.INFO)))\n", "    def test_require_debug_true_filter(self):\n        filter_ = RequireDebugTrue()\n        self.assertTrue(filter_.filter(logging.makeRecord('test', logging.DEBUG, '', 0, '', '', '')))\n        self.assertFalse(filter_.filter(logging.makeRecord('test', logging.DEBUG, '', 0, '', '', '',\n           extra={'settings': settings})))\n", "    def test_callback_filter(self):\n        class AlwaysLog(CallbackFilter):\n                return True\n\n        logger = logging.getLogger(__name__)\n        logger.addFilter(AlwaysLog(lambda r: True))\n        \n        with self.assertLogs(logger=logger.name, level='INFO') as cm:\n            logger.info(\"This should be logged\")\n        self.assertEqual(len(cm.records), 1)\n\n        logger.filters = []  # Reset logger filters\n\n"], "sample_310": ["    def test_view_detail_view_with_model_method(self):\n        response = self.client.get(reverse('admin:admindocs-view-detail', args=['admin_doc.tests.views.get_company']))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('get_company', response.context['name'])\n        self.assertIn('description', response.context)\n        self.assertIn('List', response.context['fields'][0]['data_type'])\n", "    def test_model_detail_no_docstring(self):\n        # Test that the model detail view works even if the model has no docstring.\n        response = self.client.get(reverse('admin:admindocs-model-detail', args=(\n            Person._meta.app_label, Person._meta.model_name,\n        )))\n        self.assertEqual(response.status_code, 200)\n", "    def test_view_detail(self):\n        view = views.ModelDetailView\n        response = self.client.get(reverse('admin:admindocs-model-detail', args=[Person._meta.app_label, Person._meta.model_name]))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Person')\n", "    def test_model_detail_view_related_objects(self):\n        response = self.client.get(reverse('admin:admindocs_model_detail', args=['person', 'myapp']))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('related `myapp.company` objects', response.content.decode())\n        self.assertIn(\"all related `myapp.company` objects\", response.content.decode())\n        self.assertIn(\"number of related `myapp.company` objects\", response.content.decode())\n\n\n\n", "    def test_model_detail_view_no_docstring(self):\n        # Test ModelDetailView when the model has no docstring.\n        response = self.client.get(reverse('admin_doc:model_detail', kwargs={'model_name': 'person', 'app_label': 'testapp'}))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Person')\n        self.assertNotContains(response, 'Summary:', html=True)\n"], "sample_91": ["    def test_page_not_found_custom_template(self):\n        for url in self.nonexistent_urls:\n            response = page_not_found(self.request_factory.get(url), Http404())\n            self.assertEqual(response.status_code, 404)\n            self.assertEqual(response['Content-Type'], 'text/html')\n            self.assertIn('Not Found', response.content.decode('utf-8'))\n\n", "    def test_page_not_found_custom_template(self):\n        for url in self.nonexistent_urls:\n            response = self.client.get(url)\n            self.assertEqual(response.status_code, 404)\n            self.assertTemplateUsed(response, '404.html')\n            self.assertIn(\n                quote(url), response.content.decode('utf-8'),\n            )\n", "    def test_page_not_found_custom_template(self):\n        response = page_not_found(self.request_factory.get('/nonexistent_url/'), Http404('Custom message'))\n        self.assertEqual(response.status_code, 404)\n        self.assertEqual(response.template_name, '404.html')\n        self.assertIn('\"request_path\": \"%2Fnonexistent_url%2F\"', response.content.decode())\n        self.assertIn('\"exception\": \"Custom message\"', response.content.decode())\n", "    def test_page_not_found_custom_template(self):\n        \n        response = page_not_found(self.request_factory.get('/nonexistent_url/'), Http404(\"Custom exception message\"))\n        self.assertEqual(response.status_code, 404)\n        self.assertTemplateUsed('custom_404.html')\n        self.assertEqual(response.context['exception'], 'Custom exception message')\n\n\n", "    def test_page_not_found_with_custom_template(self):\n        response = page_not_found(self.request_factory.get('/nonexistent_url/'), Http404('Page not found'))\n        self.assertEqual(response.status_code, 404)\n        self.assertContains(response, 'Page not found')\n        self.assertEqual(response.template_name[0], 'my_404.html')\n"], "sample_228": ["    def test_empty_formset_is_valid(self):\n        formset = self.make_choiceformset(total_forms=0)\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_factory_absolute_max(self):\n        formset_factory(\n            FavoriteDrinkForm,\n            formset=BaseFavoriteDrinksFormSet, extra=3, absolute_max=5,\n        )\n        \n\n        with self.assertRaises(ValueError):\n            formset_factory(\n                FavoriteDrinkForm,\n                formset=BaseFavoriteDrinksFormSet, extra=3, max_num=6,\n                absolute_max=5,\n            )\n\n", "    def test_formset_factory_custom_kwargs(self):\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1, custom_kwarg='foo')\n        formset = CustomKwargFormSet()\n        self.assertEqual(formset.forms[0].custom_kwarg, 'foo')\n", "    def test_formset_factory_absolute_max(self):\n        formset_data = [('Pizza', 1), ('Burgers', 2), ('Tacos', 3)]\n        formset = self.make_choiceformset(formset_data, max_num_forms=2, absolute_max=3)\n        self.assertEqual(formset.max_num, 2)\n        self.assertEqual(formset.absolute_max, 3)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.cleaned_data), 2)\n\n        # Assert that we can still access the data for all forms\n        self.assertEqual(formset.cleaned_data[0]['choice'], 'Pizza')\n        self.assertEqual(formset.cleaned_data[1]['choice'], 'Burgers')\n\n        formset = self.make_choiceformset(formset_data, max_num_forms=2, absolute_max=2)\n        with self.assertRaises(ValidationError) as context:\n            formset.full_clean()\n        self.assertEqual(str(context.exception), \"Please submit at most 2 forms.\")\n", "    def test_empty_formset(self):\n        formset = self.make_choiceformset(total_forms=0)\n        self.assertEqual(len(formset), 0)\n        self.assertFalse(formset.is_bound)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.cleaned_data, [])\n"], "sample_285": ["    def test_filesystem_finder_check_finds_nonexistent_dir(self):\n        check_warnings = check_finders(settings)\n        self.assertEqual(len(check_warnings), 1)\n        self.assertIsInstance(check_warnings[0], Warning)\n        self.assertEqual(check_warnings[0].id, 'staticfiles.W004')\n", "    def test_check_finders_with_invalid_finder(self):\n        settings.STATICFILES_FINDERS = ['invalid.path.Finder']\n        errors = check_finders(None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, 'staticfiles.E004')\n\n", "    def test_check_finders_invalid_finder(self):\n        \"\"\"\n        Test that check_finders() raises an error if a finder isn't a subclass\n        of BaseFinder.\n        \"\"\"\n        with mock.patch(\n            'django.contrib.staticfiles.finders.get_finder',\n            return_value=object\n        ):\n            settings.STATICFILES_FINDERS = ['some_invalid_path']\n            errors = check_finders(None)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Error)\n            self.assertIn(\n                'Finder \"some_invalid_path\" is not a subclass of \"django.contrib.staticfiles.finders.BaseFinder\"',\n                str(errors[0]),\n            )\n", "    def test_check_finders_invalid_staticfiles_dirs(self):\n        with mock.patch('django.conf.settings.STATICFILES_DIRS', [123]):\n            errors = check_finders(None)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Error)\n\n\n\n", "    def test_check_finders_with_invalid_dir(self):\n        \"\"\"\n        Tests that check_finders() catches invalid STATICFILES_DIRS entries.\n        \"\"\"\n        with mock.patch('os.path.isdir', return_value=False):\n            errors = check_finders(None)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Warning)\n            self.assertEqual(errors[0].id, 'staticfiles.W004')\n"], "sample_517": ["def test_annotation_with_offset_transform():\n    fig, ax = plt.subplots()\n    # Create an annotation with an offset coordinate that depends on the axes\n    # bounds.\n    ann = ax.annotate(\"test\",\n                      xy=(0.5, 0.5),\n                      xycoords='data',\n                      xytext=(0, 10),\n                      textcoords='offset pixels',\n                      arrowprops=dict(arrowstyle=\"->\"))\n    assert ann.get_position() == (0.5, 0.5)\n    assert isinstance(ann._get_xy(fig.canvas.get_renderer(), 0.5, 0.5,\n                                    'data'), tuple)\n    fig.canvas.draw()\n\n", "def test_annotation_arrowprops():\n    fig, ax = plt.subplots()\n    text = ax.annotate(\"Test\", xy=(0.5, 0.5), xytext=(0.7, 0.8),\n                       arrowprops=dict(arrowstyle=\"->\",\n                                       connectionstyle=\"arc3,rad=.2\"))\n    assert isinstance(text.arrow_patch, mpatches.FancyArrowPatch)\n    fig.canvas.draw()\n", "def test_annotation_offset():\n    fig, ax = plt.subplots()\n    t = ax.annotate(\"test\", xy=(0.5, 0.5), xycoords='axes fraction',\n                    xytext=(10, 10), textcoords='offset points')\n    assert isinstance(t._get_xy_transform(None, t.xycoords),\n                      mtransforms.BlendedGenericTransform)\n\n", "        def test_annotation_xycoords_data_offset(self):\n            fig, ax = plt.subplots()\n            ax.plot([0, 1], [0, 1])\n            ann = ax.annotate(\"Test\", xy=(0.5, 0.5), xycoords=\"data\",\n                              xytext=(10, 10), textcoords=\"offset points\")\n            assert ann.xycoords == \"data\"\n            assert ann.get_position() == (0.5, 0.5)\n", "    def test_annotation_draggable(self, capsys):\n        fig, ax = plt.subplots()\n        txt = ax.annotate(\"test\", (0.5, 0.5), xycoords=\"data\")\n        txt.draggable()\n\n        # click and drag\n        with mpl.testing.switch_backend('Qt5Agg'):\n            event = MouseEvent('button_press_event', fig.canvas, x=100, y=100)\n            txt._draggable.on_press(event)\n\n            event = MouseEvent('motion_notify_event', fig.canvas, x=150, y=150)\n            txt._draggable.on_motion(event)\n\n            event = MouseEvent('button_release_event', fig.canvas, x=150, y=150)\n            txt._draggable.on_release(event)\n\n        assert txt.get_position() != (0.5, 0.5)\n\n"], "sample_1070": ["def test_LambertW_eval_k():\n    assert LambertW(1.2, 1).is_complex\n", "def test_exp_polar_log():\n    z = exp_polar(x*pi/3, y)\n    assert ln(z) == x*I*pi/3 + log(abs(z))\n    assert log(z) == x*I*pi/3 + log(abs(z))\n    assert log(z, 2) == x*I*pi/3/log(2) + log(abs(z))/log(2)\n\n", "def test_LambertW_eval_k():   \n    assert LambertW(1, k=S.Zero).n() == 0.567143290409784\n    assert LambertW(1, k=S.NegativeOne).n() == -1.56714329040978\n    assert LambertW(1, k=S.One).n() ==  2.63556401636487\n\n", "def test_log_as_real_imag():\n    assert log(x).as_real_imag() == (log(Abs(x)), arg(x))\n    assert log(I).as_real_imag() == (0, pi/2)\n    assert log(1 + I).as_real_imag() == (log(sqrt(2)), pi/4)\n    assert log(I*x).as_real_imag() == (log(Abs(x)), arg(I*x))\n", "def test_LambertW_1():\n    assert LambertW(-1, -1).is_real\n\n"], "sample_329": ["    def test_serialize_functools_partial(self):\n        partial = functools.partial(str.upper, 'hello')\n        serializer = serializer_factory(partial)\n        string, imports = serializer.serialize()\n        self.assertEqual(string, 'functools.partial(str.upper, \"hello\")')\n        self.assertEqual(imports, {'import functools'})\n", "    def test_serialize_lazy_object(self):\n        lazy_value = SimpleLazyObject(lambda: 'lazy')\n        serialized, imports = serializer_factory(lazy_value).serialize()\n        self.assertEqual(serialized, \"'lazy'\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_decimal(self):\n        decimal_object = Money(100)\n        serializer = migrations.serializer.serializer_factory(decimal_object)\n        serialized, imports = serializer.serialize()\n        self.assertEqual(serialized, 'Money(\\'100\\')')\n        self.assertEqual(imports, {'import decimal'})\n\n", "    def test_serialize_settings_reference(self):\n        instance = SettingsReference('DATABASE_NAME')\n        writer = OperationWriter(migrations.Migration(\n            'test',\n        ), indentation=0)\n        result, imports = writer.serialize_object(instance)\n        self.assertEqual(result, 'settings.DATABASE_NAME')\n        self.assertEqual(imports, {'from django.conf import settings'})\n", "    def test_serialize_lazy_object(self):\n        lazy_object = SimpleLazyObject(lambda: 'foo')\n        serialized, imports = serializer_factory(lazy_object).serialize()\n        self.assertEqual(serialized, \"'foo'\")\n        self.assertEqual(imports, set())\n"], "sample_430": ["    def test_rabbit_unique_together_with_circular_fk_index(self):\n        self.assertModelHasUniqueTogetherConstraint(\n            model=models.Rabbit, fields=['parent', 'knight']\n        )\n        self.assertModelHasIndex(model=models.Rabbit, fields=['parent', 'knight'], name='rabbit_circular_fk_index')\n", "    def test_rabbit_circular_fk_model_state_compare(self):\n        self.assertModelStatesEqual(\n            self.rabbit,\n            ModelState(\n                \"eggs\",\n                \"Rabbit\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"knight\", models.ForeignKey(\"eggs.Knight\", models.CASCADE)),\n                    (\"parent\", models.ForeignKey(\"eggs.Rabbit\", models.CASCADE)),\n                ],\n                {\n                    \"unique_together\": {(\"parent\", \"knight\")},\n                    \"indexes\": [\n                        models.Index(\n                            fields=[\"parent\", \"knight\"],\n                            name=\"rabbit_circular_fk_index\",\n                        )\n                    ],\n                },\n            ),\n        )\n\n", "    def test_rabbit_circular_fk_create(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        with pytest.raises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_circular_fk_rabbit_index(self):\n        with self.assertRaises(django.db.utils.OperationalError):\n            self.testapp.create_table(self.rabbit)\n\n", "    def test_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n        rabbit = Rabbit.objects.create(knight=knight)\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit)\n"], "sample_406": ["    def test_manager_deconstruct(self):\n        class CustomManager(BaseManager):\n            pass\n\n        manager = CustomManager()\n        deconstructed = manager.deconstruct()\n        self.assertEqual(deconstructed, (False, 'tests.ModelInstanceCreationTests.CustomManager', None, (), {}))\n", "    def test_manager_creation_counter(self):\n        ArticleCount = Article.objects.model._meta.creation_counter\n        self.assertEqual(Article.objects.creation_counter, ArticleCount)\n        self.assertEqual(Article.objects_published.creation_counter, ArticleCount + 1)\n", "    def test_manager_deconstruct_from_queryset(self):\n        class MyManager(BaseManager.from_queryset(models.QuerySet)):\n            pass\n\n        manager = MyManager()\n        deconstructed = manager.deconstruct()\n        self.assertEqual(deconstructed, (False, 'tests.models_tests.MyManager', None, (), {}))\n", "    def test_manager_deconstruct(self):\n        class MyManager(BaseManager):\n            pass\n        manager = MyManager()\n        deconstructed = manager.deconstruct()\n        self.assertEqual(\n            deconstructed,\n            (False, \"tests.model_instance_creation_tests.MyManager\", None, (), {}),\n        )\n", "    def test_manager_deconstruct_dynamically_created(self):\n        class MyManager(BaseManager):\n            pass\n\n        manager = MyManager.from_queryset(QuerySet)\n        with self.assertRaises(ValueError):\n            manager.deconstruct()\n"], "sample_40": ["    def test_brightness_temperature_beam_area():\n        beam_sigma = 50*u.arcsec\n        beam_area = 2*np.pi*(beam_sigma)**2\n        freq = 5*u.GHz\n        equiv = u.brightness_temperature(freq, beam_area=beam_area)\n        assert_quantity_allclose((1*u.Jy/beam_area).to(u.K, equivalencies=equiv), 3.526295144567176 * u.K)\n", "    def test_with_H0():\n        with warnings.catch_warnings() as w:\n            warnings.simplefilter(\"ignore\")\n            h = u.with_H0(H0=70 * u.km / (u.s * u.Mpc))\n            assert h == [\n                (Unit(0.7 * u.littleh), None)\n            ]\n", "    def test_brightness_temperature_beam():\n        freq = 5 * u.GHz\n        beam_area = np.pi * (10 * u.arcsec)**2\n        equiv = u.brightness_temperature(freq, beam_area)\n        assert_quantity_allclose((1 * u.Jy / beam_area).to(u.K, equivalencies=equiv),\n                                 3.526295144567176 * u.K)\n", "    def test_brightness_temperature():\n        freq = 1 * u.GHz\n        beam_area = (1 * u.arcsec)**2\n\n        equiv = u.brightness_temperature(freq, beam_area)\n        assert_quantity_allclose(\n            (1 * u.Jy/beam_area).to(u.K, equivalencies=equiv),\n            2.725 * u.K,\n            rtol=1e-3,\n        )\n\n        with pytest.raises(ValueError) as exc:\n            u.brightness_temperature((1 * u.m).to(u.sr))\n        assert \"The inputs to `brightness_temperature`\" in str(exc.value)\n\n        equiv = u.brightness_temperature(freq)\n        assert_quantity_allclose(\n            (1 * u.Jy/u.sr).to(u.K, equivalencies=equiv),\n            1.236 * u.K,\n            rtol=1e-3,\n        )\n\n", "    def test_brightness_temperature_beam(monkeypatch):\n        # This test checks that brightness_temperature works for inputs in Jy/beam\n\n        monkeypatch.setattr('warnings.warn', lambda *args, **kwargs: None)\n        freq = 1.4 * u.GHz\n        beam_area = (2 * np.pi * (10 * u.arcsec)**2)\n        equiv = u.brightness_temperature(freq, beam_area)\n\n        assert_quantity_allclose((1 * u.Jy / beam_area).to(u.K,\n                                                         equivalencies=equiv),\n                                10.233 * u.K, rtol=1e-3)\n"], "sample_644": ["    def test_imports_from_ignored_module(self):\n        # Check that imports from modules specified in the ignored_modules\n        # configuration option are not reported as errors.\n        tree = astroid.parse(\n            \"\"\"\n            from ignored_module import some_function\n            \"\"\"\n        )\n        with CaptureFixture() as cap:\n            Run([tree], config={\"ignored_modules\": [\"ignored_module\"]})\n            self.assertEqual(cap.messages, [])\n\n", "    def test_wrong_import_order(self, linecache):\n        \"\"\"Test that wrong-import-order message is given when importing\n        standard library modules after third party modules.\n        \"\"\"\n        self.checker.config.wrong_import_order = True\n        fname = REGR_DATA + \"wrong_import_order.py\"\n        with open(fname, \"rb\") as f:\n            with CaptureFixture() as captured:\n                astroid.parse(f.read(), fname)\n\n        self.assertEqual(captured.getvalue(), (\n            \"wrong-import-order: standard import 'math' should be before third party \"\n            \"import 'requests'\\n\"\n            \"wrong-import-order: standard import 'os' should be before third party \"\n            \"import 'requests'\\n\"\n        ))\n\n", "    def test_wrong_import_order_when_nested_import_is_ignored(\n        self, checker: imports.ImportsChecker, *, capture_output: CaptureFixture,", "    def test_import_from_submodule_with_relative_path(self):\n        # See https://github.com/PyCQA/pylint/issues/4231\n        code = \"\"\"\n        from .bar import foo\n        \"\"\"\n        tree = astroid.parse(code)\n        self.checker.process_module(tree.root())\n        self.assertNoMessages()\n", "    def test_import_from_future(self, checker: imports.ImportsChecker) -> None:\n        tree = astroid.parse(\n            \"\"\""], "sample_1036": ["def test_expand_2arg():\n    from sympy import Symbol, Mul, expand\n    x = Symbol('x')\n    y = Symbol('y')\n    assert expand(Mul(x + y, 2)) == 2*x + 2*y\n", "    def test_matmul_expand_issue_11972():\n        X = MatrixSymbol('X', 2, 2)\n        Y = MatrixSymbol('Y', 2, 2)\n        Z = MatrixSymbol('Z', 2, 2)\n        expr = (X*Y)*Z\n        expanded = expr.expand()\n        assert expanded == X*(Y*Z)\n\n", "    def test_factor_in_front_matmul():\n        M = MatrixSymbol('M', 2, 2)\n        N = MatrixSymbol('N', 2, 2)\n        O = MatrixSymbol('O', 2, 2)\n        P = MatrixSymbol('P', 2, 2)\n        expr = MatMul(2 * M, N, O, P)\n        result = factor_in_front(expr)\n        assert result == MatMul(2, M, N, O, P)\n", "def test_matmul_identity_inv():\n    I = Identity(n)\n    assert MatMul(A, I) == A\n    assert MatMul(I, A) == A\n    assert MatMul(A, Inverse(A)) == I\n\n    Ainv = Inverse(A)\n    assert MatMul(A, Ainv) == I\n", "def test_matmul_with_inverse():\n    A_inv = Inverse(A)\n    assert MatMul(A, A_inv) == Identity(n)\n    assert MatMul(A_inv, A) == Identity(n)\n\n"], "sample_584": ["    def test_infer_concat_order_from_positions_with_empty_datasets(self):\n        datasets = [\n            Dataset({'foo': (('x',), [1, 2])}),\n            Dataset({'foo': (('x',), [3, 4])}),\n            Dataset({'foo': (('x',), [])}),\n        ]\n\n        expected_tile_ids = OrderedDict([\n            ((0,), datasets[0]), \n            ((1,), datasets[1]),\n            ((2,), datasets[2]), \n        ])\n\n\n        \n        combined_ids = _infer_concat_order_from_positions(datasets)\n        assert_combined_tile_ids_equal(combined_ids, expected_tile_ids) \n", "    def test_infer_concat_order_from_coords(self):\n\n        # Simple case - two datasets with identical coordinates\n        ds1 = Dataset({'a': (('x',), [1, 2])}, coords={'x': [0, 1]})\n        ds2 = Dataset({'a': (('x',), [3, 4])}, coords={'x': [0, 1]})\n        expected_order = [{(0,), ds1}, {(1,), ds2}]\n        actual_order = _infer_concat_order_from_coords([ds1, ds2])\n        assert_combined_tile_ids_equal(actual_order, expected_order)\n\n        # More complex case - two datasets with different coordinates\n        ds1 = Dataset({'a': (('x',), [1, 2])}, coords={'x': [0, 1]})\n        ds2 = Dataset({'a': (('x',), [3, 4])}, coords={'x': [1, 2]})\n        expected_order = [{(0,), ds1}, {(1,), ds2}]\n\n        actual_order = _infer_concat_order_from_coords([ds1, ds2])\n        assert_combined_tile_ids_equal(actual_order, expected_order)\n\n", "    def test_infer_concat_order_from_coords(self):\n        # Test case where datasets have different coordinates\n        ds1 = Dataset({'a': (('x',), [1, 2, 3]), 'b': (('x',), [4, 5, 6])},\n                      coords={'x': [0, 1, 2]})\n        ds2 = Dataset({'a': (('x',), [4, 5, 6]), 'b': (('x',), [7, 8, 9])},\n                      coords={'x': [3, 4, 5]})\n        datasets = [ds1, ds2]\n        expected_ids = OrderedDict([(0, ds1), (1, ds2)])\n        expected_concat_dims = ['x']\n        actual_ids, actual_concat_dims = _infer_concat_order_from_coords(datasets)\n        assert_combined_tile_ids_equal(expected_ids, actual_ids)\n        assert actual_concat_dims == expected_concat_dims\n\n", "    def test_combine_nested_infer_concat_order_from_coords(self):\n        # Test case with time dimension and some data variables\n        ds1 = Dataset({'temperature': (('x', 'time'), np.random.rand(3, 2)),\n                       'precipitation': (('x', 'time'),\n                                        np.random.rand(3, 2))},\n                      coords={'time': np.array(['2023-01-01', '2023-01-02']),\n                              'x': np.arange(3)})\n        ds2 = Dataset({'temperature': (('x', 'time'), np.random.rand(3, 2)),\n                       'precipitation': (('x', 'time'),\n                                        np.random.rand(3, 2))},\n                      coords={'time': np.array(['2023-01-03', '2023-01-04']),\n                              'x': np.arange(3)})\n        \n        combined = combine_nested([ds1, ds2], concat_dims=['time'])\n\n        # Check the shape of the concatenated array\n        assert combined.shape == (3, 4)\n\n        # Check the order of the time dimension\n\n", "    def test_combine_nested_infer_concat_order_from_coords(self):\n        data_vars = ['temperature', 'precipitation']\n        coords = ['x', 'y', 'time']\n        ds1 = create_test_data(data_vars, coords, dims={'x': 2, 'y': 2},\n                                time=(datetime(2019, 1, 1),))\n        ds2 = create_test_data(data_vars, coords, dims={'x': 2, 'y': 2},\n                                time=(datetime(2019, 1, 2),))\n        ds3 = create_test_data(data_vars, coords, dims={'x': 2, 'y': 2},\n                                time=(datetime(2019, 1, 3),))\n        ds_grid = [[ds1, ds2], [ds3]]\n\n        # Combine along 'time' dimension\n        combined = combine_nested(ds_grid, concat_dim=['time'])\n"], "sample_159": ["    def test_user_model_username_field_in_required_fields(self):\n        msg = (\n            \"The field named as the 'USERNAME_FIELD' \"\n            \"for a custom user model must not be included in 'REQUIRED_FIELDS'.\"\n        )\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserUsernameInRequiredFields'):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(str(errors[0]), msg)\n", "    def test_username_field_not_unique_with_custom_backend(self):\n        with override_settings(\n            AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername',\n            AUTHENTICATION_BACKENDS=[\n                'django.contrib.auth.backends.ModelBackend',\n                'auth_tests.backends.DummyBackend',\n            ],\n        ):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.W004')\n", "    def test_username_field_not_unique(self):\n        errors = check_user_model(app_configs=self.apps.get_app_configs())\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.E003')\n", "    def test_username_not_unique(self):\n        errors = check_user_model(app_configs=self.apps)\n        self.assertEqual(len(errors), 2)\n        self.assertEqual(errors[0].id, 'auth.E003')\n        self.assertEqual(errors[1].id, 'auth.W004') \n", "    def test_user_model_username_field_in_required_fields(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserUsernameRequired'):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'auth.E002')\n"], "sample_716": ["    def test_ridgecv_gcv_sparse(self):\n        X = sp.csr_matrix(np.eye(10))\n        y = np.zeros(10)\n        ridgecv = RidgeCV(alphas=[1.0, 2.0], gcv_mode='svd', store_cv_values=True)\n        ridgecv.fit(X, y)\n        assert_array_equal(ridgecv.coef_, np.zeros(10))\n", "    def test_ridge_cv_sparse_cv(self):\n        X, y = make_regression(n_samples=50, n_features=10, random_state=0)\n        X = sp.csr_matrix(X)\n        ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0], cv=KFold(5))\n        ridge_cv.fit(X, y)\n        assert_array_equal(ridge_cv.predict(X),\n                           ridge_regression(X, y, ridge_cv.alpha_)[0])\n", "    def test_ridge_cv_sparse_fit_intercept_false(self):\n        X = sp.csr_matrix([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n\n        model = RidgeCV(alphas=[1], fit_intercept=False, cv=None)\n        model.fit(X, y)\n        assert_array_almost_equal(model.coef_,\n                                  np.dot(linalg.inv(X.T.dot(X)), X.T.dot(y)))\n", "    def test_ridge_classifier_cv_sparse_input(self):\n        # Check that RidgeClassifierCV works with sparse input\n        clf = RidgeClassifierCV(alphas=[0.1, 1., 10.], cv=5)\n        clf.fit(X_iris, y_iris)\n        assert_greater(clf.score(X_iris, y_iris), 0.8)\n\n\n", "    def test_ridge_classifier_cv_sparse_scoring(self):\n        # Test RidgeClassifierCV with sparse input and custom scoring\n        X = sp.csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n        y = np.array([0, 1, 0])\n        clf = RidgeClassifierCV(alphas=[0.1, 1], scoring='accuracy')\n        clf.fit(X, y)\n        assert_true(clf.best_score_ >= 0 and clf.best_score_ <= 1)\n"], "sample_1017": ["    def test_bool_map_complex():\n        expr1 = And(\n            Or(And(x, y), And(w, z)),\n            Or(And(a, b), And(c, d))\n        )\n        expr2 = And(\n            Or(And(e, d), And(y, z)),\n            Or(And(w, z), And(a, b))\n        )\n        mapping = bool_map(expr1, expr2)\n        assert mapping is not False\n        assert len(mapping) == 2\n        assert simplify(expr1.subs(mapping[1])) == expr2\n\n", "    def test_bool_map_inequality():\n        a, b, c, d = symbols('a:d')\n        f1 = a & b > c & d\n        f2 = (a & b) >> (c & d)\n        assert bool_map(f1, f2) is False\n", "    def test_bool_map_with_multiple_matches():\n        from sympy.abc import a, b, c, d, e\n        f1 = SOPform([a, b, c], [[1, 1, 1]])\n        f2 = SOPform([d, e, w], [[1, 1, 1]])\n        result = bool_map(f1, f2)\n        assert result == (a & b & c, {a: d, b: e, c: w})\n", "def test_SOPform():\n    from sympy.logic.boolalg import SOPform\n    from sympy import symbols\n    w, x, y, z = symbols('w x y z')\n    minterms = [[0, 0, 0, 1], [0, 0, 1, 1],\n                [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1]]\n    dontcares = [[0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 1]]\n    assert SOPform([w, x, y, z], minterms, dontcares) == (y & z) | (z & ~w)\n\n", "    def test_simplify_logic_deep():\n        expr = Or(And(x, y), And(~x, z))\n        simplified = simplify_logic(expr, deep=True)\n        assert simplified == And(~x, z) | And(x, y)\n\n        expr = And(x & ~y, Not(x & z))\n        simplified = simplify_logic(expr, deep=True)\n        assert isinstance(simplified, And)\n\n        expr = (x & ~y) | (~x & z) | (~x & ~z & y)\n        simplified = simplify_logic(expr, deep=True)\n        assert simplified == (x & ~y) | (~x & (z | (y & ~z)))\n"], "sample_971": ["def test_prefixed_warnings(app, status, warning):\n    \"\"\"Test the prefixed_warnings context manager.\"\"\"\n    app.config.suppress_warnings = []\n\n    with prefixed_warnings(\"prefix:\"):\n        app.warn('Warning message!')\n\n    output = status.getvalue().strip()\n    assert output == colorize('yellow', 'prefix: Warning message!')\n", "    def test_is_suppressed_warning(self):\n        assert not is_suppressed_warning('not-found', None, [])\n        assert not is_suppressed_warning('sphinx.warning', None, [])\n        assert is_suppressed_warning('sphinx.warning', None, ['sphinx.warning'])\n        assert is_suppressed_warning('sphinx.warning', 'toctree', ['sphinx.warning.toctree'])\n        assert is_suppressed_warning('sphinx.warning', None, ['sphinx.warning.*'])\n        assert is_suppressed_warning('sphinx.warning', 'toctree', ['sphinx.warning.*'])\n\n", "    def test_is_suppressed_warning(self):\n        suppress_warnings = [\"sphinx.ext.autodoc.DocstringSignatureWarning\",\n                             \"sphinx.ext.autodoc.DocstringParseError.argument_type\"]\n        assert is_suppressed_warning(\"sphinx.ext.autodoc.DocstringSignatureWarning\", None, suppress_warnings)\n        assert is_suppressed_warning(\"sphinx.ext.autodoc.DocstringParseError\", \"argument_type\", suppress_warnings)\n        assert not is_suppressed_warning(\"sphinx.ext.autodoc.DocstringParseError\", \"other_type\", suppress_warnings)\n", "def test_is_suppressed_warning(app):\n    conf = app.config\n    conf.suppress_warnings = [\"sphinx.ext.autosummary.autodoc_process_signature\"]\n    assert not is_suppressed_warning(\"sphinx.errors\", \"SphinxWarning\", conf.suppress_warnings)\n    assert is_suppressed_warning(\"sphinx.ext.autosummary\", \"autodoc_process_signature\", conf.suppress_warnings)\n    assert is_suppressed_warning(\"sphinx.ext.autosummary\", \"*\", conf.suppress_warnings)\n\n    conf.suppress_warnings = [\"sphinx.ext.autosummary.*\"]\n    assert is_suppressed_warning(\"sphinx.ext.autosummary\", \"autodoc_process_signature\", conf.suppress_warnings)\n", "def test_is_suppressed_warning():\n    assert is_suppressed_warning('app.some_warning', None, ['app.some_warning']) is True\n    assert is_suppressed_warning('app.some_warning', 'subtype', ['app.some_warning']) is True\n    assert is_suppressed_warning('app.some_warning', 'subtype', ['app.some_warning.*']) is True\n    assert is_suppressed_warning('app.some_warning', None, []) is False\n    assert is_suppressed_warning('app.other_warning', None, ['app.some_warning']) is False\n\n"], "sample_796": ["    def test_huber_regressor_sparse_input(self):\n        X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n        X_sparse = sparse.csr_matrix(X)\n        huber = HuberRegressor()\n        huber.fit(X_sparse, y)\n        linear = LinearRegression()\n        linear.fit(X, y)\n        assert_array_almost_equal(huber.coef_, linear.coef_, decimal=2)\n        assert_almost_equal(huber.intercept_, linear.intercept_, decimal=2)\n\n", "    def test_huber_outliers_sparse(self):\n        X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n        X_sparse = sparse.csr_matrix(X)\n\n        huber_dense = HuberRegressor().fit(X, y)\n        huber_sparse = HuberRegressor().fit(X_sparse, y)\n\n        assert_almost_equal(huber_dense.coef_, huber_sparse.coef_, decimal=5)\n        assert_almost_equal(huber_dense.intercept_, huber_sparse.intercept_,\n                           decimal=5)\n\n", "    def test_huber_regression_with_sparse_matrix(self):\n        X, y = make_regression_with_outliers(n_samples=50, n_features=20)\n        X_sparse = sparse.csr_matrix(X)\n        huber = HuberRegressor().fit(X_sparse, y)\n        residuals = np.abs(y - huber.predict(X_sparse))\n        assert_greater(residuals.max(), 1)\n\n", "    def test_huber_vs_linear_regression_with_outliers(self):\n        X, y = make_regression_with_outliers()\n        huber_regressor = HuberRegressor(epsilon=1.35)\n        linear_regressor = LinearRegression()\n        huber_regressor.fit(X, y)\n        linear_regressor.fit(X, y)\n        assert_greater(huber_regressor.score(X, y), linear_regressor.score(X, y))\n", "    def test_huber_loss_gradient():\n        # Test the _huber_loss_and_gradient function\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        w = np.array([0.1, 0.2, 0.3])\n\n        epsilon = 1.35\n        alpha = 0.0001\n\n        loss, grad = _huber_loss_and_gradient(\n            w, X, y, epsilon, alpha, sample_weight=None)\n\n        # Assertions for the loss and gradient\n        assert_almost_equal(loss, 0.01)\n        assert_array_almost_equal(grad, [0.0, 0.0, 0.0])\n        \n"], "sample_573": ["    def test_fit_predict(self, df):\n\n        data = df[[\"x\", \"y\"]].copy()\n        data.loc[::2, \"y\"] = np.nan\n        expected = PolyFit()._fit_predict(data)\n\n        test = PolyFit()._fit_predict(data)\n\n        assert_frame_equal(test, expected)\n\n", "    def test_polyfit_basic(self, df):\n        result = PolyFit()(df, GroupBy(\"group\"), \"x\", {})\n        assert isinstance(result, pd.DataFrame)\n        assert \"x\" in result.columns\n        assert \"y\" in result.columns\n", "    def test_polyfit_with_few_unique_x(self, df):\n        df = df.copy()\n        df[\"x\"] = df[\"x\"].apply(lambda x: round(x))\n        gb = GroupBy(df, [\"group\", \"color\"])\n        stat = PolyFit(order=2)\n        result = stat(df, gb, orient=\"v\", scales=None)\n        expected = gb.apply(df.dropna(subset=[\"x\", \"y\"]), lambda x: pd.DataFrame())\n        assert_frame_equal(result, expected)\n", "    def test_polyfit(self, df):\n\n        df['y'] = df['x']**2 + df['x'] + np.random.randn(len(df))\n        gb = GroupBy(df)\n\n        result = PolyFit(order=2)(gb, \"group\", \"x\", {})\n        expected = gb.apply(df, lambda x: pd.DataFrame({'x': np.linspace(x['x'].min(), x['x'].max(), 100),\n                                                        'y': np.polyval(np.polyfit(x['x'], x['y'], 2),\n                                                                        np.linspace(x['x'].min(), x['x'].max(), 100))}))\n        assert_frame_equal(result, expected)\n\n", "    def test_polyfit(self, df):\n        groupby = GroupBy(df, \"group\")\n        stat = PolyFit(order=2)\n        result = stat(df, groupby, \"xy\", {})\n        assert isinstance(result, pd.DataFrame)\n        assert \"x\" in result.columns\n        assert \"y\" in result.columns\n"], "sample_741": ["    def test_randomized_multi_metric(self):\n        # Check that randomized search works with multiple metrics\n        estimator = LinearSVC()\n        param_distributions = {'C': [0.1, 1, 10]}\n        scoring = {'accuracy': 'accuracy', 'f1': 'f1_macro'}\n\n        grid_search = RandomizedSearchCV(estimator, param_distributions,\n                                         n_iter=2, scoring=scoring, refit='f1')\n        grid_search.fit(X, y)\n        assert_true(hasattr(grid_search, 'best_score_'))\n        assert_true(hasattr(grid_search, 'best_params_'))\n        assert_true(hasattr(grid_search, 'best_index_'))\n        assert_equal(grid_search.best_score_, grid_search.cv_results_['mean_test_f1'][grid_search.best_index_])\n\n", "    def test_randomized_search_cv_refit_multi_metric(self):\n        # Test refit with multiple metrics\n        clf = LinearSVC()\n        param_grid = {'C': [0.1, 1, 10]}\n        scoring = {'accuracy': 'accuracy',\n                   'roc_auc': 'roc_auc'}\n        rs = RandomizedSearchCV(clf, param_grid, scoring=scoring, refit='roc_auc',\n                                cv=5, verbose=1, n_iter=2)\n        rs.fit(X, y)\n        assert_equal(rs.refit, 'roc_auc')\n        assert_true(hasattr(rs, 'best_score_'))\n        assert_true(hasattr(rs, 'best_params_'))\n", "    def test_gridsearchcv_score_string_deprecation_warning(self):\n        # Test that a warning is issued when passing a string to scoring\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            GridSearchCV(SVC(), param_grid={'kernel': ['rbf']}, scoring='accuracy')\n        assert_equal(len(w), 1)\n        assert_true(\"scoring\" in str(w[0].message))\n        assert_true(\"string\" in str(w[0].message))\n\n", "    def test_randomized_search_cv_refit_multiclass(self):\n        # Test refitting with multiclass classification\n        clf = DecisionTreeClassifier()\n        param_distributions = {'max_depth': [2, 3, 4],\n                               'min_samples_split': [2, 3]}\n        X = np.random.randn(100, 5)\n        y = np.random.randint(0, 5, size=100)\n\n            return accuracy_score(y, estimator.predict(X))\n        scoring = {'accuracy': 'accuracy', 'my_scorer': custom_scorer}\n\n        search = RandomizedSearchCV(clf, param_distributions, scoring=scoring,\n                                     refit='accuracy', cv=3, n_iter=3)\n        search.fit(X, y)\n        assert_true(hasattr(search, 'best_estimator_'))\n        assert_true(hasattr(search, 'best_score_'))\n        assert_true(hasattr(search, 'best_params_'))\n", "    def test_randomized_search_cv_refit_with_multi_metric(self):\n        # Check that refit works with multi-metric evaluation\n\n        clf = SVC()\n        param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1]}\n        scoring = {'accuracy': 'accuracy', 'f1': 'f1_macro'}\n        gs = RandomizedSearchCV(clf, param_grid, scoring=scoring, cv=2,\n                               refit='accuracy', n_iter=3)\n        gs.fit(X, y)\n        # refit should have used the best parameters for accuracy\n        assert_equal(gs.best_params_['C'], gs.best_estimator_.C)\n        assert_equal(gs.best_params_['gamma'], gs.best_estimator_.gamma)\n\n        # Check that the best score is indeed the best accuracy\n        best_accuracy = cross_val_score(\n            gs.best_estimator_, X, y, scoring='accuracy', cv=2).mean()\n        assert_almost_equal(gs.best_score_, best_accuracy)\n"], "sample_877": ["    def test_out_of_bounds_error():\n        X = np.array([1, 2, 3, 4, 5])\n        y = np.array([1, 2, 3, 4, 5])\n\n        model = IsotonicRegression(out_of_bounds=\"raise\")\n        model.fit(X, y)\n\n        with pytest.raises(ValueError, match=\"X value out of bounds\"):\n            model.predict([0])\n        with pytest.raises(ValueError, match=\"X value out of bounds\"):\n            model.predict([6])\n\n        model = IsotonicRegression(out_of_bounds=\"clip\")\n        model.fit(X, y)\n        assert_array_equal(model.predict([0]), [1])\n        assert_array_equal(model.predict([6]), [5])\n", "    def test_out_of_bounds_raise(self):\n        for increasing in [True, False]:\n            X = np.array([1, 2, 3])\n            y = np.array([1, 2, 3])\n            iso = IsotonicRegression(increasing=increasing, out_of_bounds=\"raise\")\n            iso.fit(X, y)\n            with pytest.raises(ValueError):\n                iso.predict([0])\n            with pytest.raises(ValueError):\n                iso.predict([4])\n", "    def test_isotonic_regression_with_equal_values_in_y():\n        y = np.array([1, 1, 1, 2, 2, 3])\n        X = np.array([1, 2, 3, 4, 5, 6])\n        increasing = True\n        y_pred = isotonic_regression(y, increasing=increasing)\n\n        assert_array_equal(y_pred, [1, 1, 1, 2, 2, 3])\n", "    def test_isotonic_regression_out_of_bounds_clip():\n        # test out_of_bounds = 'clip'\n\n        rng = np.random.RandomState(42)\n        X = rng.uniform(size=10)\n        y = 2 * X + 1\n        iso = IsotonicRegression(out_of_bounds='clip')\n        iso.fit(X, y)\n        # predictions at the edges should be clipped to the nearest observed values\n        assert iso.predict([0, 1.1])[0] == iso.predict(X[0])\n        assert iso.predict([0, 1.1])[1] == iso.predict(X[-1])\n\n\n", "    def test_isotonic_regression_increasing_decreasing():\n        X = np.array([1, 2, 3, 4, 5])\n        y_increasing = np.array([2, 4, 6, 8, 10])\n        y_decreasing = np.array([10, 8, 6, 4, 2])\n\n        # Test increasing\n        isotonic_reg = IsotonicRegression(increasing=True)\n        isotonic_reg.fit(X, y_increasing)\n        assert_array_equal(isotonic_reg.predict(X), y_increasing)\n\n        # Test decreasing\n        isotonic_reg = IsotonicRegression(increasing=False)\n        isotonic_reg.fit(X, y_decreasing)\n        assert_array_equal(isotonic_reg.predict(X), y_decreasing)\n"], "sample_5": ["def test_model_bounding_box(model_class, parameters):\n    model = model_class(**parameters)\n    if 'bounding_box' in parameters:\n        bb = parameters['bounding_box']\n        if bb:\n            assert isinstance(model.bounding_box, ModelBoundingBox)\n            bb.sort()\n            model_bb = model.bounding_box.get_bounding_box(model.parameters)\n            model_bb.sort()\n            assert_quantity_allclose(model_bb, bb)\n        else:\n            assert model.bounding_box is None\n", "    def test_model_bounding_box(self):\n        for model_dict in MODELS:\n            model_class = model_dict['class']\n            if model_dict['bounding_box'] is not False:\n                with pytest.raises(InputParameterError):\n                    model = model_class(**model_dict['parameters'])\n                    bbox = model.bounding_box\n            else:\n                model = model_class(**model_dict['parameters'])\n                assert bbox is None\n", "    def test_bounding_box(self, model, model_class):\n        if isinstance(model, dict):\n            model = model_class(**model['parameters'])\n        if model.bounding_box is not False:\n            bbox = model.bounding_box\n            if isinstance(bbox, list):\n                assert isinstance(bbox[0], (u.Quantity, float, int))\n                assert isinstance(bbox[1], (u.Quantity, float, int))\n            if isinstance(bbox, u.Quantity):\n                assert isinstance(bbox, u.Quantity)\n            x_range = np.linspace(bbox[0], bbox[1], 100)\n            y_range = model(x_range)\n            assert np.all(np.isfinite(y_range))\n", "    def test_model_bounding_box(self):\n        for model_info in MODELS:\n            if model_info['bounding_box'] is not False:\n                with self.subTest(model=model_info['class']):\n                    model = model_info['class'](**model_info['parameters'])\n                    bbox = model.bounding_box\n\n                    if isinstance(bbox, ModelBoundingBox):\n                        assert bbox.is_valid()\n                    else:\n                        assert bbox == model_info['bounding_box']\n", "    def test_model_bounding_box(self):\n        for model_dict in MODELS:\n            model_class = model_dict['class']\n            if model_dict.get('bounding_box') is False:\n                continue\n            model = model_class(**model_dict['parameters'])\n            if 'x_0' in model_dict['parameters']:\n                bounding_box = ModelBoundingBox(model_dict['bounding_box'])\n                assert bounding_box == model.bounding_box\n\n"], "sample_1188": ["def test_pretty_del():\n    assert pretty(Del) == '\u2207'\n\n", "def test_pretty_express():\n    assert pretty(v[8]) == pretty_v_8\n    assert upretty(v[8]) == upretty_v_8\n    assert pretty(v[11]) == pretty_v_11\n    assert upretty(v[11]) == upretty_v_11\n    assert pretty(s) == pretty_s\n    assert upretty(s) == upretty_s\n    assert pretty(d[7]) == pretty_d_7\n    assert upretty(d[7]) == upretty_d_7\n\n", "def test_pretty_vector_dot_product():\n    assert pretty(d[7]) == pretty_d_7\n    assert upretty(d[7]) == upretty_d_7\n", "def test_pretty_del():\n    assert pretty(Del) == '\u2207'\n    assert upretty(Del) == '\u2207'\n    assert pretty(Del(C)) == '\u2207_C'\n\n", "def test_pretty_derivatives():\n    assert pretty(Derivative(f(x), x)) == \"d/dx f(x)\"\n    assert pretty(Derivative(f(x), x, 2)) == \"d\u00b2/dx\u00b2 f(x)\"\n    assert pretty(Derivative(f(x), (x, 2))) == \"d\u00b2/dx\u00b2 f(x)\"\n\n"], "sample_813": ["    def test_ard_regression_fit_intercept(self):\n        rng = check_random_state(0)\n        n_samples = 100\n        n_features = 10\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        clf = ARDRegression(fit_intercept=False)\n        clf.fit(X, y)\n\n        # make sure intercept is zero\n        assert_almost_equal(clf.intercept_, 0.)\n", "    def test_ard_regression_stability(self):\n        # Check that ARDRegression gives stable results even when\n        # the data has high variance.\n        rng = check_random_state(0)\n        X = rng.randn(100, 5)\n        y = rng.randn(100)\n        # Introduce a large variance feature\n        X[:, 0] *= 100\n\n        model = ARDRegression()\n        model.fit(X, y)\n        # Check for stability:\n        # - alpha should not be too large\n        assert_array_less(model.alpha_, 1e4)\n        # - lambda values for the highly variant feature should be small\n        assert_array_less(model.lambda_[0], model.lambda_[1:].mean())\n", "    def test_ard_regression_alpha(self):\n        rng = check_random_state(42)\n        X = rng.randn(10, 5)\n        y = rng.randn(10)\n        coef = rng.randn(5)\n\n        n_samples, n_features = X.shape\n        alpha = 1.\n        alpha_1 = 1e-6\n        alpha_2 = 1e-6\n        lambda_1 = 1e-6\n        lambda_2 = 1e-6\n        threshold_lambda = 1e4\n\n        ard = ARDRegression(alpha_1=alpha_1, alpha_2=alpha_2,\n                           lambda_1=lambda_1, lambda_2=lambda_2,\n                           threshold_lambda=threshold_lambda)\n        ard.fit(X, y)\n\n        # Test if the alpha is updated correctly\n        assert_almost_equal(ard.alpha_, alpha, 2)\n\n", "    def test_ard_regression_alpha(self):\n        rng = check_random_state(0)\n        X = rng.randn(100, 5)\n        y = rng.randn(100)\n        clf = ARDRegression(alpha_1=1e-1, alpha_2=1e-1,\n                           lambda_1=1e-1, lambda_2=1e-1,\n                           n_iter=100, tol=1e-3).fit(X, y)\n        assert np.isclose(clf.alpha_, 1.0 / (np.var(y) + 1e-6),\n                          atol=1e-2)\n", "    def test_ard_regression_alpha(self):\n        # Test that alpha_ is estimated correctly\n        rng = check_random_state(0)\n        n_samples = 100\n        n_features = 10\n        X = rng.randn(n_samples, n_features)\n        y = X[:, 1] + rng.randn(n_samples)\n        clf = ARDRegression(tol=1e-10, n_iter=1000)\n        clf.fit(X, y)\n        assert_almost_equal(clf.alpha_,\n                            1. / (np.var(y) + 1e-6),\n                            decimal=3)\n"], "sample_1106": ["def test_MatAdd_merge_explicit_zero():\n    from sympy.matrices.expressions.matadd import MatAdd\n    Z = ZeroMatrix(2,2)\n    A = MatrixSymbol('A', 2, 2)\n    x = MatAdd(A, Z, A)\n    assert x == 2*A\n", "    def test_merge_explicit():\n        A = MatrixSymbol('A', 2, 2)\n        B = eye(2)\n        C = Matrix([[1, 2], [3, 4]])\n        X = MatAdd(A, B, C)\n\n        expected = MatAdd(A, Matrix([[2, 2], [3, 5]]))\n        assert canonicalize(X) == expected\n", "def test_MatAdd_transpose_adjoint():\n    X = MatAdd(A, B, A)\n    assert transpose(X) == MatAdd(transpose(A), transpose(B), transpose(A))\n    assert adjoint(X) == MatAdd(adjoint(A), adjoint(B), adjoint(A))\n", "    def test_merge_explicit(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = Matrix([[1, 2], [3, 4]])\n        C = eye(2)\n        X = MatAdd(A, B, C)\n        X = merge_explicit(X)\n        assert isinstance(X, MatAdd)\n        assert len(X.args) == 2\n        assert X.args[0] == A\n        assert X.args[1] == B + C\n", "    def test_merge_explicit():\n        A = MatrixSymbol('A', 2, 2)\n        B = eye(2)\n        C = Matrix([[1, 2], [3, 4]])\n        X = MatAdd(A, B, C)\n        assert (X.doit() == MatAdd(A + C + B))\n\n"], "sample_1054": ["def test_complexRegion_intersection():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = Interval(S.Pi/2, 3*S.Pi/2)\n    cr1 = ComplexRegion(a*b, polar=True)\n\n    # Intersection with itself\n    assert cr1.intersect(cr1) == cr1\n\n    cr2 = ComplexRegion(a*c, polar=True)\n\n    # Intersection with a subset\n    assert cr1.intersect(cr2) == ComplexRegion(a*Intersection(b, c), polar=True)\n\n    # Intersection with a disjoint set\n    assert cr1.intersect(ComplexRegion(Interval(2, 3)*b, polar=True)) == \\\n        ComplexRegion(EmptySet(), polar=True)\n", "    def test_ComplexRegion_construction_union():\n        a = Interval(2, 3)\n        b = Interval(4, 6)\n        c = Interval(1, 8)\n        c1 = ComplexRegion(a*b)\n        c2 = ComplexRegion(b*c)\n        c3 = ComplexRegion(Union(a*b, b*c))\n        assert c3 == Union(c1, c2)\n\n        theta = Interval(0, 2*S.Pi)\n\n        c4 = ComplexRegion(a*theta, polar=True)\n        c5 = ComplexRegion(c*theta, polar=True)\n        c6 = ComplexRegion(Union(a*theta, c*theta), polar=True)\n        assert c6 == Union(c4, c5)\n\n", "    def test_ComplexRegion_intersection_issue_17680(self):\n        a = Interval(0, 1)\n        b = Interval(0, 2*S.Pi)\n        c = Interval(S.Pi/2, 3*S.Pi/2)\n        d = Interval(S.Pi, 2*S.Pi)\n        c1 = ComplexRegion(a*b, polar=True)  \n        c2 = ComplexRegion(a*c, polar=True)  \n        c3 = ComplexRegion(a*d, polar=True)\n        \n        assert c1.intersect(c2) == ComplexRegion(a*c, polar=True)\n        assert c2.intersect(c1) == ComplexRegion(a*c, polar=True)\n        assert c1.intersect(c3) == ComplexRegion(a*d, polar=True)\n        assert c3.intersect(c1) == ComplexRegion(a*d, polar=True)\n", "    def test_complexregion_empty(self):\n        # Empty set in rectangular form\n        c1 = ComplexRegion(Interval(2, 1)*Interval(4, 6))\n        assert c1.is_empty is True\n        # Empty set in Polar Form\n        c2 = ComplexRegion(Interval(2,1)*Interval.Ropen(0, 2*S.Pi), polar=True)\n        assert c2.is_empty is True\n\n\n\n", "def test_complexregion_polar_intersection():\n    r = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(r*theta, polar=True)\n\n    r1 = Interval(0, 0.5)\n    theta1 = Interval(0, S.Pi)\n    c2 = ComplexRegion(r1*theta1, polar=True)\n\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(Interval(0, 0.5)*Interval(0, pi), True)\n"], "sample_526": ["    def test_concisedateformatter_offset(self):\n        # Test ConciseDateFormatter with offsetformats\n        dates = [\n            datetime.datetime(2023, 1, 1, 12, 0, 0, tzinfo=dateutil.tz.UTC),\n            datetime.datetime(2023, 1, 1, 13, 0, 0, tzinfo=dateutil.tz.UTC),\n            datetime.datetime(2023, 1, 2, 12, 0, 0, tzinfo=dateutil.tz.UTC),\n        ]\n        fig, ax = plt.subplots()\n        formatter = mdates.ConciseDateFormatter(\n            mdates.AutoDateLocator(),\n            tz=dateutil.tz.UTC,\n            offset_formats=[\"%H:%M\"],\n        )\n        ax.plot(dates, [1, 2, 3])\n        ax.xaxis_date()\n        ax.xaxis.set_major_formatter(formatter)\n        fig.canvas.draw()\n        labels = [t.get_text() for t in ax.xaxis.get_majorticklabels()]\n        assert labels == [\"12:00\", \"13:00\", \"12:00\"]\n", "    def test_concise_date_formatters(self):\n        fig, ax = plt.subplots()\n        # Use a date range that spans across multiple years\n        dates = [datetime.datetime(2022, 1, 1),\n                 datetime.datetime(2023, 10, 26)]\n        ax.plot(dates, [1, 2])\n\n        fmt = ConciseDateFormatter(mdates.AutoDateLocator(),\n                                   formats=['%Y-%m-%d', '%b %Y'],\n                                   zero_formats=['Year %Y'],\n                                   offset_formats=['%Y-%m'])\n\n        ax.xaxis.set_major_formatter(fmt)\n        self.assertNotIn(\"2022-10-26\", ax.xaxis.get_ticklabels()[0].get_text())\n        self.assertIn(\"2023-10\", ax.xaxis.get_ticklabels()[0].get_text())\n\n\n\n", "    def test_concise_date_formatter_with_offset_formats(self):\n        fig, ax = plt.subplots()\n        dates = [\n            datetime.datetime(2023, 1, 1),\n            datetime.datetime(2023, 1, 15, 12),\n            datetime.datetime(2023, 2, 1, 12),\n            datetime.datetime(2023, 2, 15, 12),\n        ]\n        ax.plot(dates, np.arange(len(dates)))\n\n        formatter = ConciseDateFormatter(\n            mdates.AutoDateLocator(),\n            tz=dateutil.tz.UTC,\n            offset_formats=[\"%H:%M\"],\n            show_offset=True,\n        )\n        ax.xaxis.set_major_formatter(formatter)\n\n        fig.autofmt_xdate()\n        self.assert_xlabel(ax, '2023-01-01\\n12:00')\n            \n", "    def test_date_formatter_formats(self, format, expected):\n        formatter = mdates.DateFormatter(format)\n        result = formatter.format_data(\n            np.datetime64(\"2023-10-26T12:00:00\")\n        )\n        assert result == expected\n\n", "    def test_concise_dateformat_offset(self):\n        fig, ax = plt.subplots()\n        # Test offset formatting\n\n        date = datetime.datetime(2023, 1, 1) + datetime.timedelta(hours=12)\n        ax.plot([date])\n\n        with rc_context({'date.converter': 'concise' }):\n            ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(\n                mdates.AutoDateLocator(), tz=dateutil.tz.UTC,\n                offset_formats=['%H:%M %Z%z'], show_offset=True))\n\n        plt.draw()\n        fig.canvas.draw()\n        # We can't directly check the text as the timezone\n        # is dependent on the user's locale.\n        labels = [label.get_text() for label in ax.get_xticklabels()]\n        assert any('12:00 UTC+0000' in label for label in labels)\n\n"], "sample_259": ["    def test_prefetch_related_objects_with_prefetch_related_lookup(self):\n        authors = Author.objects.all().prefetch_related(Prefetch('books__reads'))\n\n        reader_names = set()\n        for author in authors:\n            for book in author.books.all():\n                for reader in book.reads.all():\n                    reader_names.add(reader.name)\n\n        self.assertEqual(reader_names, {'Amy', 'Belinda'})\n", "    def test_prefetch_related_objects_deep(self):\n        prefetch_related_objects(\n            [self.reader1, self.reader2],\n            Prefetch('books_read', Prefetch('authors', to_attr='reader_authors')),\n        )\n        self.assertEqual(self.reader1.reader_authors, [self.author1, self.author2, self.author3, self.author4])\n        self.assertEqual(self.reader2.reader_authors, [self.author1, self.author4])\n", "    def test_prefetch_related_objects_with_prefetch_related_lookups(self):\n        prefetch_related_lookups = [\n            Prefetch('authors', Prefetch('books_written', to_attr='books')),\n        ]\n\n        books = Book.objects.prefetch_related(*prefetch_related_lookups).all()\n        self.assertEqual(books[0].authors, [self.author1, self.author2, self.author3])\n        self.assertEqual(books[0].authors[0].books, [self.book1])\n", "    def test_prefetch_related_objects_to_attr(self):\n        authors = Author.objects.filter(first_book__in=[self.book1, self.book4]).prefetch_related(\n            Prefetch('first_book', to_attr='my_book')\n        )\n        for author in authors:\n            self.assertEqual(author.my_book, author.first_book)\n\n", "    def test_prefetch_related_empty_queryset(self):\n        authors = Author.objects.filter(id__gt=5)  # Empty queryset\n        prefetch_related_objects(authors, Prefetch('books'))\n        self.assertEqual(list(authors), [])\n"], "sample_231": ["    def test_cleanse_setting(self):\n        filter = SafeExceptionReporterFilter()\n\n        self.assertEqual(filter.cleanse_setting('SECRET_KEY', 'mysecret'),\n                         filter.cleansed_substitute)\n        self.assertEqual(filter.cleanse_setting('DEBUG', True), True)\n\n        settings_dict = {'API_TOKEN': 'secret_token', 'DEBUG': False}\n        cleansed_settings = filter.cleanse_setting('DEBUG', settings_dict)\n        self.assertEqual(cleansed_settings, {'API_TOKEN': filter.cleansed_substitute, 'DEBUG': False})\n", "    def test_callable_setting_wrapper_repr(self):\n        mocked_callable = mock.Mock()\n        wrapper = CallableSettingWrapper(mocked_callable)\n        self.assertEqual(repr(wrapper), repr(mocked_callable))\n", "    def test_callable_setting_wrapper_repr(self):\n        wrapped_callable = CallableSettingWrapper(lambda x: x)\n        self.assertEqual(repr(wrapped_callable), repr(lambda x: x))\n", "    def test_callable_setting_wrapper_repr(self):\n            pass\n        wrapper = CallableSettingWrapper(my_func)\n        self.assertEqual(repr(wrapper), repr(my_func))\n", "    def test_callable_setting_wrapper(self):\n        wrapper = CallableSettingWrapper(lambda: 10)\n        self.assertEqual(repr(wrapper), 'lambda()')\n"], "sample_571": ["    def test_residplot(self):\n\n        # Basic residplot\n        fig, ax = plt.subplots()\n        lm.residplot(x=\"x\", y=\"y\", data=self.df, ax=ax)\n        assert len(ax.lines) == 2\n        assert len(ax.collections) == 1\n\n        # residplot with lowess smoothing\n        fig, ax = plt.subplots()\n        lm.residplot(x=\"x\", y=\"y\", data=self.df, ax=ax, lowess=True)\n        assert len(ax.lines) == 3\n        assert len(ax.collections) == 1\n\n        # residplot with robust regression\n        fig, ax = plt.subplots()\n        lm.residplot(x=\"x\", y=\"y\", data=self.df, ax=ax, robust=True)\n        assert len(ax.lines) == 2\n        assert len(ax.collections) == 1\n", "    def test_residplot(self):\n        with mpl.rc_context({'figure.figsize': (5, 5)}):\n            g = lm.residplot(self.df.x, self.df.y_na,\n                             data=self.df,\n                             dropna=True)\n            assert len(g.lines) == 2\n            assert isinstance(g.collections[0], mpl.collections.PathCollection)\n\n            g = lm.residplot(self.df.x, self.df.y_na,\n                             data=self.df,\n                             dropna=False)\n            assert len(g.lines) == 2\n            assert isinstance(g.collections[0], mpl.collections.PathCollection)\n            assert len(g.collections[0].get_paths()) == 60\n\n            g = lm.residplot(self.df.x, self.df.y,\n                             data=self.df, lowess=True)\n            assert len(g.lines) == 3\n\n            g = lm.residplot(self.df.x, self.df.y,\n                             data=self.df,\n                             x_partial=['d'],\n                             y_partial=['s'])\n            assert len(g.lines) == 2\n", "    def test_regplot_x_estimator(self):\n\n        with mpl.rc_context({'axes.facecolor': 'white'}):\n            x = self.df.x.cat.codes\n            y = self.df.y\n            g = lm.regplot(x=x, y=y, data=self.df,\n                           x_estimator=np.mean)\n            npt.assert_allclose(g.lines[0].get_xdata(),\n                               np.unique(x))\n            npt.assert_allclose(g.lines[0].get_ydata(),\n                               [y[x == i].mean() for i in np.unique(x)])\n", "    def test_residplot_no_data(self):\n        with pytest.raises(ValueError):\n            lm.residplot()\n        with pytest.raises(ValueError):\n            lm.residplot(x=\"x\")\n\n", "    def test_residplot(self):\n\n        # Test basic residplot functionality\n        with plt.Figure() as fig, mpl.rc_context({'figure.figsize': (6, 4)}):\n            ax = lm.residplot(x=\"x\", y=\"y\", data=self.df)\n            assert ax is not None\n            assert len(ax.lines) == 2\n\n        # Test with lowess smoothing\n        with plt.Figure() as fig, mpl.rc_context({'figure.figsize': (6, 4)}):\n            ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, lowess=True)\n            assert len(ax.lines) == 3\n\n        # Test with partial residuals\n        with pytest.raises(ValueError):\n            lm.residplot(x=\"x\", y=\"y\", data=self.df, x_partial=\"z\")\n        with plt.Figure() as fig, mpl.rc_context({'figure.figsize': (6, 4)}):\n            ax = lm.residplot(x=\"x\", y=\"y\", data=self.df, y_partial=\"d\")\n\n"], "sample_1055": ["    def test_encipher_bg_decipher_bg():\n        p = 17\n        q = 23\n        key = bg_private_key(p, q)\n        key_public = bg_public_key(p, q)\n        msg = 50\n\n        enc_msg, x_L = encipher_bg(msg,key_public)\n        dec_msg = decipher_bg((enc_msg, x_L), key)\n\n        assert dec_msg == msg\n", "def test_encipher_gm_randomness():\n    p = nextprime(100)\n    q = nextprime(p + 1)\n    key = gm_private_key(p, q)\n    msg = 54321\n    \n    # Check if two encryptions with the same message and key are different\n    enc1 = encipher_gm(msg, gm_public_key(key[0], key[1]))\n    enc2 = encipher_gm(msg, gm_public_key(key[0], key[1]))\n    assert enc1 != enc2\n", "def test_gm_private_key():\n    p = 17\n    q = 23\n    try:\n        gm_private_key(p, p)\n        assert False, 'Failed to catch duplicate primes'\n    except ValueError:\n        pass\n\n    try:\n        gm_private_key(p, 2)\n        assert False, 'Failed to catch non-prime'\n    except ValueError:\n        pass\n\n    p, q = gm_private_key(p, q)\n    assert p == 17 and q == 23\n\n\n\n", "    def test_encipher_gm():\n        p = 7\n        q = 11\n        a = 5\n        public_key = gm_public_key(p, q, a)\n\n        message = 15  \n        cipher_text = encipher_gm(message, public_key)\n        assert decipher_gm(cipher_text, (p, q)) == message\n", "    def test_gm_private_key():\n        p = nextprime(10)\n        q = nextprime(15)\n        with raises(ValueError):\n            gm_private_key(p, p)\n        with raises(ValueError):\n            gm_private_key(p, 2)\n\n        _p, _q = gm_private_key(p, q)\n        assert _p == p\n        assert _q == q\n\n"], "sample_557": ["def test_add_axes():\n    fig, ax = plt.subplots()\n    assert ax == fig.add_axes([0.1, 0.1, 0.8, 0.8])\n\n    with pytest.raises(ValueError):\n        fig.add_axes([1.2, 0.1, 0.8, 0.8])  # x position out of bounds\n\n    ax2 = fig.add_axes([0.2, 0.2, 0.5, 0.5])\n    assert len(fig.axes) == 2\n    assert ax2 is not ax\n", "    def test_figure_tight_layout(self, layout_engine):\n        fig = Figure()\n        fig.set_layout_engine(layout_engine)\n        ax1 = fig.add_subplot(211)\n        ax2 = fig.add_subplot(212)\n        ax2.set_xlabel('X label')\n        fig.tight_layout()\n        assert fig._layout_engine is not None\n\n", "    def test_figure_tight_layout_rect(self):\n        fig, ax = plt.subplots(1, 1)\n        fig.suptitle('Test title')\n        ax.set_xlabel('Test xlabel')\n        ax.set_ylabel('Test ylabel')\n        fig.tight_layout(rect=(0.1, 0.1, 0.9, 0.9))\n        # Verify that the suptitle, xlabel, and ylabel fit within the\n        # specified rect. This is a visual check for now.\n        \n", "def test_figure_add_axes():\n    fig = Figure()\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert isinstance(ax1, Axes)\n    ax2 = fig.add_axes(ax1)  # add axes sharing the same area\n    assert ax1 == ax2\n    assert ax1.get_position().bounds == ax2.get_position().bounds\n", "    def test_tight_layout(self):\n\n        fig, axs = plt.subplots(2, 2)\n        fig.tight_layout()\n\n        # check if the layout is tight\n        assert fig.bbox_inches.x0 == 0\n        assert fig.bbox_inches.y0 == 0\n        assert fig.bbox_inches.x1 == 1\n        assert fig.bbox_inches.y1 == 1\n\n"], "sample_980": ["def test_from_inversion_vector():\n    assert Permutation.from_inversion_vector([3, 2, 1, 0, 0]).array_form == [3, 2, 1, 0, 4] \n", "    def test_from_inversion_vector():\n        p = Permutation.from_inversion_vector([3, 2, 1, 0, 0])\n        assert p.array_form == [3, 2, 1, 0, 4, 5]\n        raises(ValueError, Permutation.from_inversion_vector, [4, 2, 1, 0])\n\n", "    def test_cyclic_form_from_array_form_empty():\n        assert Permutation([]).cyclic_form == []\n", "def test_from_inversion_vector():\n    p = Permutation.from_inversion_vector([2, 1, 0])\n    assert p == Permutation([2, 0, 1])\n    assert p.inversion_vector() == [2, 1, 0] \n", "def test_from_inversion_vector():\n    assert Permutation.from_inversion_vector([2, 1, 0]) == Permutation([2, 0, 1])\n    assert Permutation.from_inversion_vector([3, 2, 1, 0, 0]) == Permutation([3, 2, 1, 0, 4])\n    with raises(ValueError):\n        Permutation.from_inversion_vector([3, 2, 1, 0, 1])  \n"], "sample_448": ["    def test_unique_constraint_with_condition_and_expressions(self):\n        with atomic():\n            product_1 = UniqueConstraintConditionProduct.objects.create(\n                name=\"Product 1\", price=10.00\n            )\n\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name=\"Product 2\", price=20.00, condition=True\n                )\n            with self.assertRaises(ValidationError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name=\"Product 1\", price=20.00, condition=True\n                )\n\n            product_2 = UniqueConstraintConditionProduct.objects.create(\n                name=\"Product 2\", price=20.00, condition=False\n            )\n            self.assertIsNotNone(product_2)\n", "    def test_unique_constraint_with_expressions(self):\n        with atomic(using=\"default\"):\n            UniqueConstraintProduct.objects.create(name=\"Book A\", description=\"A good book\")\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name=\"Book A\", description=\"Another description\")\n\n\n", "    def test_unique_constraint_with_expressions(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        F(\"name\") + F(\"value\"),\n                        name=\"my_unique_constraint\",\n                    )\n                ]\n\n        with atomic():\n            MyModel.objects.create(name=\"test\", value=1)\n            with self.assertRaises(IntegrityError):\n                MyModel.objects.create(name=\"test\", value=1)\n", "    def test_unique_constraint_with_expressions_deferrable(self):\n        with atomic():\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 1\", code=\"ABC\", sku=\"SKU1\"\n                )\n\n            # Deferrable constraints are not enforced immediately\n            UniqueConstraintProduct.objects.create(\n                name=\"PRODUCT 1\", code=\"ABC\", sku=\"SKU2\",\n            )\n\n            # Now enforce the constraint\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    f\"ALTER TABLE {UniqueConstraintProduct._meta.db_table} \"\n                    f\"ALTER CONSTRAINT {UniqueConstraintProduct._meta.unique_together[0][1]} \"\n                    f\"VALIDATE;\"\n                )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 2\", code=\"ABC\", sku=\"SKU1\"\n                )\n\n\n", "    def test_unique_constraint_with_opclasses(self):\n        with atomic(using=self.db_alias), self.assertRaises(IntegrityError):\n            UniqueConstraintProduct.objects.create(\n                name=\"test product\",\n                slug=\"test-product\",\n                price=10.00,\n            )\n        with atomic(using=self.db_alias):\n            UniqueConstraintProduct.objects.create(\n                name=\"Another test product\",\n                slug=\"another-test-product\",\n                price=10.00,\n            )\n"], "sample_1025": ["    def test_print_Piecewise(self):\n        piecewise = Piecewise((x**2, x < 0), (x, x >= 0))\n        self.assertEqual(pycode(piecewise, standard='python3'),\n                         'Piecewise((x**2, x < 0), (x, x >= 0))')\n\n\n", "def test_numpy_array_diagonal():\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    expr = A[0, 0]\n    code = NumPyPrinter().doprint(expr)\n    assert code == \"A[0, 0]\"\n", "    def test_print_SparseMatrix(self):\n        m = SparseMatrix([[1, 2], [3, 4]])\n        assert pycode(m, language='python') == 'sympy.SparseMatrix([[1, 2], [3, 4]])'\n\n", "    def test_sympy_printing():\n        expr = acos(x)\n        expected = 'acos(x)'\n        assert pycode(expr) == expected\n\n        expr = Piecewise((x, x < 0), (1, True))\n        expected = 'Piecewise((x, x < 0), (1, True))'\n        assert pycode(expr) == expected\n", "    def test_print_SparseMatrix(self):\n        a = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        code = SciPyPrinter().doprint(a)\n        expected = (\n            \"scipy.sparse.coo_matrix([1, 2, 3], ([0, 1, 2], [0, 1, 2]), shape=(3, 3))\"\n        )\n        assert code == expected\n"], "sample_87": ["    def test_watchman_watch_project_with_nested_directories(self):\n        with mock.patch('django.utils.autoreload.pywatchman', autospec=True) as watchman:\n            watchman.client.side_effect = mock.MagicMock(\n                return_value=mock.MagicMock(\n                    query=mock.MagicMock(\n                        return_value={\n                            'watch': 'watch_hash',\n                            'relative_path': 'nested-directory',\n                        }\n                    ),\n                    getSubscription=mock.MagicMock(\n                        return_value=[{'subscription': 'files:nested-directory', 'files': ['changed_file']}]\n                    ),\n                )\n            )\n\n            reloader = autoreload.WatchmanReloader()\n            reloader._watch_root = mock.MagicMock()\n            nested_dir = self.temporary_file('nested-directory')\n\n            reloader._update_watches()\n            reloader._check_subscription('files:nested-directory')\n\n            self.assertEqual(reloader.client.query.call_args,\n                             (('watch-project', str(nested_dir)), {'timeout': 5})) \n            self.assertEqual(\n                reloader.client.getSubscription.call_args,\n                (('files:nested-directory',), {})\n            )\n            reloader._watch_root.assert_called_once_with(nested_dir)\n\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'my_package.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zipf:\n                zipf.writestr('my_package/__init__.py', '')\n                zipf.writestr('my_package/module.py', '')\n            sys.path.append(tmpdir)\n            self.import_and_cleanup('my_package')\n            self.assertFileFound(zip_filename.parent / zip_filename.name)\n\n", "    def test_module_in_zipfile(self):\n        with self.temporary_file('mypackage.zip') as zipfile_path:\n            with zipfile.ZipFile(zipfile_path, 'w') as zf:\n                zf.writestr('mypackage/__init__.py', '')\n                zf.writestr('mypackage/module.py', 'x = 10')\n            sys.path.append(zipfile_path.parent)\n            self.addCleanup(sys.path.pop)\n            self.import_and_cleanup('mypackage.module')\n            self.assertFileFound(Path(zipfile_path) / 'mypackage/module.py')\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'my_package.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_package/__init__.py', '')\n                zf.writestr('my_package/module.py', '')\n            sys.path.append(str(tempdir))\n            self.addCleanup(sys.path.remove, str(tempdir))\n\n            self.import_and_cleanup('my_package.module')\n            self.assertFileFound(zip_filename)\n", "    def test_zipfile_module(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / \"my_module.zip\"\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr(\"my_module/__init__.py\", \"\")\n            sys.path_importer_cache.clear()\n            self.import_and_cleanup(\"my_module\")\n            self.assertFileFound(zip_path)\n\n"], "sample_777": ["    def test_gradient_boosting_regressor_sparse_input(self):\n        # Check ability to work with sparse matrices\n        rng = np.random.RandomState(0)\n        X, y = make_regression(n_samples=100, n_features=10, random_state=rng)\n        X_sparse = csr_matrix(X)\n        est = GradientBoostingRegressor(random_state=rng)\n        est.fit(X_sparse, y)\n        y_pred = est.predict(X_sparse)\n        assert_almost_equal(mean_squared_error(y, y_pred), 0, decimal=2)\n", "def test_sparse_input_feature_importance(self):\n        # Test feature importances on sparse input.\n        # This test checks if feature importances are computed correctly for sparse\n        # input data.\n\n        X = csr_matrix([[1, 0], [0, 1], [1, 1]])\n        y = np.array([1, 0, 1])\n        gb = GradientBoostingClassifier(random_state=0).fit(X, y)\n        importances = gb.feature_importances_\n\n        assert_array_equal(importances.shape, (X.shape[1],))\n        assert np.all(importances >= 0)\n        assert np.isclose(np.sum(importances), 1.0)\n\n", "def check_regression_toy(presort, loss):\n    # Check regression on a toy dataset.\n    reg = GradientBoostingRegressor(loss=loss, n_estimators=2,\n                                   random_state=1, presort=presort)\n\n    assert_raises(ValueError, reg.predict, T)\n\n    reg.fit(X, y)\n    #Assert predictions are close enough within tolerance\n    assert_array_almost_equal(reg.predict(T),\n                             [ -0.95, 0.95, 1.0 ], decimal=2)\n    assert_equal(2, len(reg.estimators_))\n\n    deviance_decrease = (reg.train_score_[:-1] - reg.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n\n    leaves = reg.apply(X)\n    assert_equal(leaves.shape, (6, 2, 1))\n\n", "def test_gradient_boosting_classifier_sparse_input():\n    # Checks if GradientBoostingClassifier can handle sparse input.\n\n    X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n    X_sparse = csr_matrix(X)\n    gbdt = GradientBoostingClassifier(random_state=0)\n\n    gbdt.fit(X_sparse, y)\n    y_pred = gbdt.predict(X_sparse)\n    assert_array_equal(gbdt.n_classes_, 2)\n\n", "def test_classification_toy_sparse(presort, loss):\n    # Check classification on a toy dataset.\n    X = csr_matrix(X)\n    clf = GradientBoostingClassifier(loss=loss, n_estimators=10,\n                                     random_state=1, presort=presort)\n    clf.fit(X, y)\n    assert_array_equal(clf.predict(csr_matrix(T)), true_result)\n    assert_equal(10, len(clf.estimators_))\n\n    deviance_decrease = (clf.train_score_[:-1] - clf.train_score_[1:])\n    assert np.any(deviance_decrease >= 0.0)\n"], "sample_102": ["    def test_union_with_different_models(self):\n        reserved_names = ReservedName.objects.all()\n        numbers = Number.objects.filter(num__gt=5)\n        with self.assertRaises(ValueError):\n            numbers.union(reserved_names)\n", "    def test_select_related(self):\n        # Select related objects across multiple levels\n        numbers = Number.objects.select_related('number_set').filter(number_set__num=5)\n        self.assertEqual(numbers.count(), 1)\n\n        # Check that the 'number_set' objects are prefetched\n        self.assertTrue(all(number.number_set is not None for number in numbers))\n\n", "    def test_union_with_different_fields(self):\n        qs1 = Number.objects.values('num')\n        qs2 = Number.objects.values('other_num')\n        result = qs1.union(qs2)\n        self.assertNumbersEqual(result, [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0])\n", "    def test_union_with_different_selects(self):\n        # Ensure union works when SELECT clauses differ.\n        self.assertNumbersEqual(\n            Number.objects.filter(num__lt=5).union(Number.objects.filter(num__gt=5)),\n            Number.objects.exclude(num__in=[5]),\n        )\n\n", "    def test_complex_select_related_prefetch_related_union(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                SELECT id, num, other_num FROM number_number WHERE num > 5\n                UNION ALL\n                SELECT id, num, other_num FROM number_number WHERE num < 2\n            \"\"\")\n            result = cursor.fetchall()\n        expected_numbers = [Number.objects.get(id=row[0]) for row in result]\n        \n        # Test complex select_related and prefetch_related with union\n        queryset = Number.objects.filter(\n            Q(num__gt=5) | Q(num__lt=2)\n        ).select_related('related_number')\n        queryset = queryset.prefetch_related(\n            Prefetch('related_number__another_number',\n                     queryset=Number.objects.filter(num__lt=5))\n        )\n        \n        self.assertSequencesEqual(\n            [(n.id, n.num, n.other_num, n.related_number_id, n.related_number.num if n.related_number else None)\n             for n in queryset],\n            [(n.id, n.num, n.other_num, n.related_number_id, n.related_number.num if n.related_number else None)\n             for n in expected_numbers],\n        )\n"], "sample_1087": ["def test_interpolating_poly():\n    x = symbols('x')\n    X = symbols('X:3')\n    Y = symbols('Y:3')\n\n    f = interpolating_poly(3, x, X, Y)\n\n    assert f.subs({x: X[0]}).subs(zip(X, Y)) == Y[0]\n    assert f.subs({x: X[1]}).subs(zip(X, Y)) == Y[1]\n    assert f.subs({x: X[2]}).subs(zip(X, Y)) == Y[2]\n\n    raises(ValueError, lambda: interpolating_poly(3, x, [1, 2, 3], [4, 5]))\n    raises(ValueError, lambda: interpolating_poly(3, x, symbols('a:3'),\n                                                symbols('b:4')))\n\n", "    def test_interpolating_poly():\n        x, y = symbols('x y')\n        points = [(1, 2), (2, 3), (3, 4)]\n        p = interpolating_poly(3, x, *zip(*points))\n        assert Poly(p, x).subs(x, 1) == 2\n        assert Poly(p, x).subs(x, 2) == 3\n        assert Poly(p, x).subs(x, 3) == 4\n        raises(ValueError, lambda: interpolating_poly(3, x, y, y))\n\n", "    def test_interpolating_poly():\n        x = symbols('x')\n        Y = [1, 2, 3, 4]\n        X = [0, 1, 2, 3]\n        p = interpolating_poly(4, x, X=X, Y=Y)\n        assert Poly(p, x).coeffs() == [1, 0, 0, 1, -2]\n", "    def test_interpolating_poly():\n        x = symbols('x')\n        Y = [1, 2, 3]\n        expected = Poly(x**2 - 4*x + 3, x)\n        assert interpolating_poly(2, x, X=range(3), Y=Y) == expected\n        assert interpolating_poly(2, x, Y=Y) == expected\n", "    def test_swinnerton_dyer_poly_polys_arg():\n        p = swinnerton_dyer_poly(3, polys=True)\n        assert isinstance(p, Poly)\n        assert p.args == (x,)\n        assert p.ring == ZZ\n\n"], "sample_763": ["    def test_check_array_non_numeric_dtype_object_warn(self):\n        X = np.array(['a', 'b', 1, 2], dtype=object)\n        with warnings.catch_warnings(record=True) as w:\n            check_array(X, dtype=np.float64, warn_on_dtype=True)\n        assert_equal(len(w), 1)\n        assert (\"Data with input dtype object was converted to float64\"\n                in str(w[0].message))\n", "    def test_check_consistent_length_pandas_dataframe(self):\n        # Test with pandas DataFrame\n\n        X = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        y = np.array([1, 2, 3])\n        check_consistent_length(X, y)\n", "    def test_check_X_y_multioutput_sparse_numeric_y():\n        X = sparse_random_matrix(10, 5, density=0.5).tocsr()\n        y = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10],\n                      [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n\n        check_X_y(X, y, multi_output=True, accept_sparse=True)\n", "    def test_check_is_fitted_with_sparse_matrix_and_all():\n        # Test check_is_fitted with sparse matrix and all_or_any=all\n\n        X, y = make_blobs(n_samples=10, random_state=42)\n        sparse_X = sp.csr_matrix(X)\n\n        estimator = ARDRegression()\n        estimator.fit(sparse_X, y)\n        check_is_fitted(estimator, [\"coef_\", \"estimator_\", \"X\"], all_or_any=all)\n\n        # Test case where not all attributes are fitted\n        estimator = ARDRegression()\n        with pytest.raises(NotFittedError):\n            check_is_fitted(estimator, [\"coef_\", \"fitted_\"], all_or_any=all)\n", "    def test_check_consistent_length_dataframe_errors(self):\n        df = MockDataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n        X = df[['A']]\n\n        # Test that mismatch in sample count raises a ValueError\n        y = np.array([1, 2])\n        with pytest.raises(ValueError,\n                           match='Found input variables with inconsistent'):\n            check_consistent_length(X, y)\n\n        # Test that mismatch with a Series also raises a ValueError\n        y = pd.Series([1, 2, 3, 4])\n        with pytest.raises(ValueError,\n                           match='Found input variables with inconsistent'):\n            check_consistent_length(X, y)\n\n"], "sample_30": ["    def test_table_to_xml_with_custom_format(tmp_path):\n        data = {\"col1\": [1, 2, 3], \"col2\": [4.0, 5.0, 6.0]}\n        table = Table(rows=data)\n        votable = tree.VOTableFile()\n        resource = tree.Resource()\n        resource.tables.append(table)\n        votable.resources.append(resource)\n\n        with tmp_path.joinpath(\"test.vot\").open(\"w\") as fd:\n            votable.to_xml(fd, tabledata_format=\"binary\")\n\n        with open(tmp_path / \"test.vot\", \"rb\") as f:\n            content = f.read()\n        assert content.startswith(b\"<?xml\")\n\n", "    def test_write_to_compressed_gzip():\n        try:\n            import gzip  # Verify gzip is importable\n        except ImportError:\n            return\n\n        filename = get_pkg_data_filename(\n            'data/tiny.vot'\n        )\n        with open(filename, 'r') as f:\n            contents = f.read()\n\n        # Write to a compressed gzip file\n        with io.BytesIO() as buffer:\n            tree.VOTableFile.from_file(filename).to_xml(\n                buffer, compressed=True\n            )\n            buffer.seek(0)\n\n            with gzip.open(buffer) as f:\n                compressed_contents = f.read().decode('utf-8')\n\n        # Validate the contents\n        assert compressed_contents == contents\n\n\n\n", "def test_votable_file_get_table_by_index():\n    with get_pkg_data_filename('data/test1.xml', package='astropy.io.votable') as filename:\n        votable = tree.parse(filename)\n        assert votable.get_table_by_index(0).ID == 'table1'\n        with pytest.raises(IndexError):\n            votable.get_table_by_index(1)\n", "    def test_roundtrip_with_empty_array():\n        \"\"\"Test that a table with an empty array column can be read and written.\"\"\"\n        table = tree.Table()\n        table.fields.append(tree.Field(name=\"col1\", datatype=\"int\"))\n        table.fields.append(tree.Field(name=\"col2\", datatype=\"array\", arraysize=\"*\", datatype=\"double\"))\n        table.rows.append([1, []]) \n        \n        votable = tree.VOTableFile()\n        votable.resources.append(tree.Resource(table=table))\n\n        # Write the VOTable to a string\n        with io.StringIO() as buffer:\n            votable.to_xml(buffer)\n            xml_string = buffer.getvalue()\n\n        # Parse the VOTable from the string\n        votable = tree.parse(xml_string)\n\n        # Check that the empty array column is preserved\n        assert len(votable.resources[0].tables[0].array.shape) == 1\n        assert votable.resources[0].tables[0].array.size == 0 \n\n", "    def test_table_get_field_by_id_or_name_multiple(self, tmpdir):\n        \"\"\"Test get_field_by_id_or_name when there are multiple matches.\"\"\"\n        data = \"\"\"\n        <VOTABLE version=\"1.4\">\n        <RESOURCE>\n        <TABLE ID=\"t1\">\n            <FIELD ID=\"f1\" name=\"f1\" utype=\"float\"/>\n            <FIELD ID=\"f2\" name=\"f2\" utype=\"float\"/>\n            <DATA>\n            <TABLEDATA>\n                1 2\n                3 4\n            </TABLEDATA>\n            </DATA>\n        </TABLE>\n        </RESOURCE>\n        </VOTABLE>\n        \"\"\"\n        filename = tmpdir.join(\"test.xml\")\n        filename.write(data)\n\n        votable = tree.parse(str(filename))\n        with pytest.raises(ValueError) as excinfo:\n            votable.get_field_by_id_or_name(\"f2\")\n\n        assert \"Multiple fields match\" in str(excinfo.value)\n"], "sample_46": ["    def test_uuid_exact_lookup_with_hyphenated_value(self):\n        model = UUIDModel.objects.create(id=uuid.UUID('a1b2c3d4-e5f6-7890-1234-567890abcdef'), data='test')\n        with self.assertNumQueries(1):\n            match = UUIDModel.objects.get(id=uuid.UUID('a1b2c3d4-e5f6-7890-1234-567890abcdef'))\n        self.assertEqual(match.id, uuid.UUID('a1b2c3d4-e5f6-7890-1234-567890abcdef'))\n        self.assertEqual(match.data, 'test')\n\n", "    def test_uuid_lookup_case_insensitive(self):\n        obj1 = UUIDModel.objects.create(uuid_field=uuid.UUID('a1b2c3d4-e5f6-7890-1234-567890abcdef'))\n        obj2 = UUIDModel.objects.create(uuid_field=uuid.UUID('f1e2d3c4-b5a6-9870-4321-fedcba098765'))\n\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid_field__iexact=obj1.uuid_field.hex).count(), 1\n        )\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid_field__icontains='aBc').count(), 1\n        )\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid_field__istartswith='a1b').count(), 1\n        )\n        self.assertEqual(\n            UUIDModel.objects.filter(uuid_field__iendswith='abcdef').count(), 1\n        )\n\n", "    def test_uuid_lookup_regex(self):\n        # Test various regex lookups against UUID values\n\n        uuid_str = str(uuid.uuid4())\n        model = UUIDModel.objects.create(uuid_field=uuid.UUID(uuid_str))\n\n        # Exact match\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__regex=uuid_str).count(), 1)\n\n        # Partial match\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__iregex=f'{uuid_str[:8]}.*').count(), 1)\n\n        # No match\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__regex='bogus').count(), 0)\n\n", "    def test_uuid_lookup_with_contains(self):\n        uuid1 = uuid.uuid4()\n        uuid2 = uuid.uuid4()\n\n        UUIDModel.objects.create(uuid_field=uuid1)\n        UUIDModel.objects.create(uuid_field=uuid2)\n\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__contains=str(uuid1)).count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__contains=str(uuid2)).count(), 1)\n", "    def test_uuid_lookup_case_insensitive(self):\n        obj1 = UUIDModel.objects.create(uuid_field=uuid.UUID('a1b2c3d4-e5f6-7890-1234-567890abcdef'))\n        obj2 = UUIDModel.objects.create(uuid_field=uuid.UUID('a1b2c3d4e5f678901234567890abcdef'))\n\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__iexact=obj1.uuid_field).count(), 1)\n        self.assertEqual(UUIDModel.objects.filter(uuid_field__icontains=obj1.uuid_field.hex).count(), 2)\n"], "sample_196": ["    def test_year_lookup_bounds_for_datetime_field(self):\n        with override_settings(USE_TZ=True), transaction.atomic():\n            tz = timezone.get_current_timezone()\n            first = datetime.datetime(2000, 1, 1).replace(tzinfo=tz)\n            second = datetime.datetime(2000, 12, 31, 23, 59, 59, 999999).replace(tzinfo=tz)\n            bounds = self.ops.year_lookup_bounds_for_datetime_field(2000)\n            self.assertEqual(bounds, [self.ops.adapt_datetimefield_value(first),\n                                    self.ops.adapt_datetimefield_value(second)])\n", "    def test_convert_durationfield_value(self):\n        with self.assertRaises(NotImplementedError):\n            self.ops.convert_durationfield_value(123, None, None)\n", "    def test_returning_clause_unsupported(self):\n        with self.assertRaisesMessage(NotSupportedError,\n                                     'This backend does not support returning clauses'):\n            self.ops.returning_clause([])\n\n", "    def test_savepoint_create_sql(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'savepoint_create_sql'):\n            self.ops.savepoint_create_sql('my_savepoint')\n", "    def test_date_extract_sql(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'date_extract_sql'):\n            self.ops.date_extract_sql('year', 'some_field')\n"], "sample_227": ["    def test_list_filter_with_queryset_based_lookups(self):\n        admin_class = DecadeFilterBookAdminWithQuerysetBasedLookups\n        request = self.request_factory.get('/admin/books/book/')\n        request.user = self.alfred\n        admin = admin_class(Book, site)\n\n        # No books from 1980s\n        self.assertEqual(admin.get_list_filter(request)[0].lookups(request, admin), [\n            ('the 90s', \"the 1990's\"),\n            ('the 00s', \"the 2000's\"),\n        ])\n\n", "    def test_list_filter_parameter_ends_with__isnull(self):\n        request = self.request_factory.get('/')\n        admin = DecadeFilterBookAdminParameterEndsWith__Isnull()\n        list_filter = admin.get_list_filter(request, None)[0]\n        self.assertEqual(list_filter.title, 'publication decade')\n        self.assertEqual(list_filter.parameter_name, 'decade__isnull')\n", "    def test_decade_filter_with_none_returning_lookups(self):\n        request = self.request_factory.get('/')\n        admin_class = DecadeFilterBookAdminWithNoneReturningLookups\n        admin = admin_class(Book, None)\n        qs = admin.get_queryset(request)\n        with self.assertRaises(TypeError):\n            list(admin.list_filter[0].lookups(request, admin))\n\n", "    def test_list_filter_empty_field_filter(self):\n        request = self.request_factory.get('/')\n        admin_site = site.AdminSite()\n        admin_site.register(Book, BookAdminWithEmptyFieldListFilter)\n        admin_site.register(Department, DepartmentAdminWithEmptyFieldListFilter)\n        books_admin = admin_site._registry[Book]\n        departments_admin = admin_site._registry[Department]\n\n        empty_filter_books = books_admin.get_list_filter(request)\n        empty_filter_departments = departments_admin.get_list_filter(request)\n\n        self.assertEqual(len(empty_filter_books), 3)\n        self.assertEqual(len(empty_filter_departments), 2)\n", "    def test_bookmark_admin_generic_relations(self):\n        request = self.request_factory.get('/')\n        admin = BookmarkAdminGenericRelation(Bookmark, site)\n        qs = admin.get_queryset(request)\n        filter_spec = {'tags__tag': ['django']}\n        filtered_qs = admin.apply_list_filter(request, qs, filter_spec)\n        self.assertEqual(filtered_qs.count(), 0)  # No bookmarks created yet\n\n"], "sample_32": ["    def test_wz_validation(self):\n        \"\"\"Tests that invalid values for wz are caught.\"\"\"\n        with pytest.raises(TypeError, match=r\"wz must be a float\"):\n            self._cls(**self._cls_args, **{\"wz\": \"string\", **self.cls_kwargs})\n\n        with pytest.raises(TypeError, match=r\"wz must be a float\"):\n            self._cls(**self._cls_args, **{\"wz\": [1.0], **self.cls_kwargs})\n", "    def test_wz_is_parameter(self):\n        \"\"\"Test w0wzCDM.wz is an astropy.cosmology.parameter.Parameter.\"\"\"\n\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n        assert isinstance(cosmo.wz, Parameter)\n", "    def test_wz_setter(self, cosmo_cls, cosmo_args, cosmo_kwargs):\n        cosmo_kwargs[\"wz\"] = 0.5\n        cosmo = cosmo_cls(*cosmo_args, **cosmo_kwargs)\n        assert cosmo.wz == 0.5\n", "    def test_wz_input_validation(self):\n\n        with pytest.raises(TypeError):\n            self._cls(**self._cls_args, wz=\"invalid\", **self.cls_kwargs)\n", "    def test_wz_setter_getter(self):\n        \"\"\"Test setting and getting the wz parameter.\"\"\"\n        cosmo = self._cls(**self._cls_args, **self._cls_kwargs)\n\n        # Test getting the parameter\n        assert cosmo.wz == self._cls_kwargs[\"wz\"]\n\n        # Test setting the parameter\n        new_wz = 0.3\n        cosmo.wz = new_wz\n        assert cosmo.wz == new_wz\n"], "sample_780": ["    def test_partial_fit_online_learning(self):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        learning_method='online',\n                                        batch_size=10,\n                                        verbose=False)\n        # Check fitting on small batches\n        lda.partial_fit(X[:10, :])\n        lda.partial_fit(X[10:, :])\n        doc_topic_distr = lda.transform(X)\n        assert_equal(doc_topic_distr.shape, (X.shape[0], n_components))\n\n", "    def test_partial_fit(self):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                        learning_method='online',\n                                        batch_size=1000)\n\n        lda.partial_fit(X)\n        assert_greater_equal(lda.n_iter_, 1)\n        assert_array_almost_equal(lda.components_.sum(axis=1),\n                                  np.full(n_components, 1.0), decimal=2)\n", "    def test_partial_fit_batch_size(self):\n        n_components, X = _build_sparse_mtx()\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                       learning_method='online',\n                                       batch_size=5, random_state=0)\n        lda.partial_fit(X)\n        assert_equal(lda.n_batch_iter_, 1)\n        lda.partial_fit(X)\n        assert_equal(lda.n_batch_iter_, 2)\n", "    def test_partial_fit_warm_start(self):\n        n_components, X = _build_sparse_mtx()\n\n        lda = LatentDirichletAllocation(n_components=n_components,\n                                       random_state=42)\n        lda.fit(X)\n        score1 = lda.score(X)\n\n        lda.partial_fit(X)\n        score2 = lda.score(X)\n\n        assert_greater_equal(score2, score1)\n", "    def test_partial_fit_partial_data(self):\n        lda = LatentDirichletAllocation(n_components=2, random_state=0)\n        # Use a subset of data to fit the model\n        lda.partial_fit(X[:10, :])\n        # Check that the model is fitted\n        assert lda.components_.shape == (2, 10)\n        # Fit the model with the remaining data\n        lda.partial_fit(X[10:, :])\n        # Evaluate the perplexity on the full dataset\n        score = lda.perplexity(X)\n        assert_greater_equal(score, 0)\n\n"], "sample_651": ["    def test_deprecated_call_no_args(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                warnings.warn('use v3 of this api', DeprecationWarning)\n                return 100\n\n                assert my_fixture == 100\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n        result.stdout.fnmatch_lines(\n            [\n                \"*DeprecationWarning: use v3 of this api*\",\n            ]\n        )\n", "    def test_warns_with_match_regex(self, recwarn: WarningsRecorder) -> None:\n        with warnings.catch_warnings(record=True) as w:\n            warnings.warn(\"my warning\", UserWarning)\n        recwarn.clear()\n        with pytest.warns(UserWarning, match=r\".*warning.*\"):\n            warnings.warn(\"another warning\", UserWarning)\n\n        assert len(recwarn) == 1\n        assert recwarn[0].category == UserWarning\n        assert re.search(r\"another warning\", recwarn[0].message) is not None\n", "def test_warnings_recorder_pop_matches_exact_type(pytester: Pytester) -> None:\n    with pytester.module(\"p\", \"\"\"\n        import warnings\n            warnings.warn(\"Hello world\", UserWarning)\n            warnings.warn(\"This one is DeprecationWarning\", DeprecationWarning)\n    \"\"\") as x:\n        p = x.getpath(\"p\")\n        pytester.runpytest(\"-c\", p)\n\n\n\n", "    def test_warn_multiple_matches(self, pytester: Pytester) -> None:\n        # Ensure warnings of different types are captured and handled individually\n        pytester.runpytest(\n            \"--capture=sys\",\n            \"-v\",\n            \"\"\"\n            import warnings\n\n                warnings.warn(\"Foo warning\", UserWarning)\n                warnings.warn(\"Bar warning\", SyntaxWarning)\n\n                with pytest.warns(UserWarning, match=\"Foo warning\"):\n                    foo()\n                with pytest.warns(SyntaxWarning, match=\"Bar warning\"):\n                    foo()\n\n        \"\"\",\n        )\n        # ensure we don't have spurious warnings from pytest itself\n        assert pytester.stdout.str().splitlines()[-2] == \"=== 2 passed in 0.01s ===\"\n", "    def test_warns_reemitting_unmatched(self, pytester: Pytester) -> None:\n        pytester.makefile(\".py\", \"\"\"\n            import warnings\n\n                warnings.warn(\"this is not here\", UserWarning)\n\n                with pytest.warns(UserWarning, match=r'must be \\d+$'):\n                    myfunc()\n        \"\"\")\n        result = pytester.runpytest()\n        assert result.ret == 1\n        result.stdout.fnmatch_lines(\n            [\n                \"*Failed*\",\n                \"*DID NOT WARN*\",\n                \"*No warnings of type ...UserWarning... were emitted*\",\n            ]\n        )\n\n"], "sample_594": ["    def test_summarize_variable_non_index(self):\n        var = xr.Variable([\"x\"], np.arange(10))\n        expected = dedent(\n            \"\"\"\n          * var: (x) float64 0 1 2 3 4 5 6 7 8 9\n        \"\"\"\n        )\n        actual = formatting.summarize_variable(\"var\", var, 25)\n        assert actual == expected\n", "    def test_inline_sparse_repr(self):\n        data = np.array([[1, 0, 0], [0, 2, 3], [0, 0, 4]]).astype(float)\n        array = sparse.COO(data)\n        expected = \"<sparse.COO: nnz=4, fill_value=0.0>\"\n        assert formatting.inline_sparse_repr(array) == expected\n\n", "    def test_short_numpy_repr(self, array, expected):\n        with formatting.set_numpy_options(precision=2, threshold=100):\n            assert formatting.short_numpy_repr(array) == expected\n\n", "    def test_first_n_items(self, shape, n_desired, expected):\n        array = np.arange(np.prod(shape)).reshape(shape)\n        result = formatting.first_n_items(array, n_desired)\n        assert np.array_equal(result, expected)\n", "    def test_format_timedelta_timedelta64(self):\n        data = np.array(\n            [\n                pd.Timedelta(\"1 days 2 hours 30 minutes\"),\n                pd.Timedelta(\"1 day\"),\n                pd.Timedelta(\"2 hours\"),\n            ],\n            dtype=\"timedelta64[ns]\",\n        )\n        expected = [\n            \"1 days 02:30:00\",\n            \"1 days 00:00:00\",\n            \"0 days 02:00:00\",\n        ]\n        actual = formatting.format_items(data)\n        assert all(e == a for e, a in zip(expected, actual))\n"], "sample_302": ["    def test_settings_to_cmd_args_env(self):\n        with mock.patch('os.environ') as mock_env:\n            settings = {\n                'HOST': 'localhost',\n                'PORT': 5432,\n                'NAME': 'test_db',\n                'USER': 'test_user',\n                'PASSWORD': 'test_password',\n                'OPTIONS': {'sslmode': 'require'},\n            }\n            expected_args = [\n                'psql',\n                '-h',\n                'localhost',\n                '-p',\n                '5432',\n                'test_db',\n                '-U',\n                'test_user',\n            ]\n            expected_env = {\n                'PGPASSWORD': 'test_password',\n                'PGSSLMODE': 'require',\n            }\n            args, env = DatabaseClient.settings_to_cmd_args_env(settings, [])\n            self.assertEqual(args, expected_args)\n            self.assertEqual(env, expected_env)\n", "    def test_settings_to_cmd_args_env(self):\n        settings_dict = {\n            'NAME': 'mydb',\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'USER': 'myuser',\n            'PASSWORD': 'mypassword',\n            'OPTIONS': {'sslmode': 'require',\n                        'sslrootcert': '/path/to/rootcert.pem',\n                       }\n        }\n        params = ['param1', 'param2']\n        expected_args = [\n            'psql',\n            '-U', 'myuser',\n            '-h', 'localhost',\n            '-p', '5432',\n            'mydb',\n            'param1',\n            'param2'\n        ]\n        expected_env = {\n            'PGPASSWORD': 'mypassword',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/rootcert.pem'\n        }\n\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, params)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env) \n", "    def test_settings_to_cmd_args_env(self):\n        with mock.patch('django.db.backends.postgresql.client.DatabaseClient.executable_name', 'mock_psql'):\n            settings_dict = {\n                'HOST': 'localhost',\n                'PORT': 5432,\n                'NAME': 'test_db',\n                'USER': 'test_user',\n                'PASSWORD': 'test_password',\n                'OPTIONS': {\n                    'passfile': 'path/to/passfile',\n                    'service': 'test_service',\n                    'sslmode': 'require',\n                    'sslrootcert': 'path/to/rootcert',\n                    'sslcert': 'path/to/cert',\n                    'sslkey': 'path/to/key',\n                },\n            }\n            parameters = ['-c', 'SELECT 1;']\n\n            expected_args = [\n                'mock_psql',\n                '-U', 'test_user',\n                '-h', 'localhost',\n                '-p', '5432',\n                'test_db',\n                '-c', 'SELECT 1;',\n            ]\n            expected_env = {\n                'PGPASSWORD': 'test_password',\n                'PGSERVICE': 'test_service',\n                'PGSSLMODE': 'require',\n                'PGSSLROOTCERT': 'path/to/rootcert',\n                'PGSSLCERT': 'path/to/cert',\n                'PGSSLKEY': 'path/to/key',\n                'PGPASSFILE': 'path/to/passfile',\n            }\n\n            args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n\n            self.assertEqual(args, expected_args)\n            self.assertEqual(env, expected_env)\n\n", "    def test_settings_to_cmd_args_env(self):\n        settings = {\n            'NAME': 'test_db',\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'USER': 'test_user',\n            'PASSWORD': 'test_password',\n            'OPTIONS': {'sslmode': 'require', 'sslrootcert': '/path/to/rootcert.pem'},\n        }\n        expected_args = [\n            'psql',\n            '-U',\n            'test_user',\n            '-h',\n            'localhost',\n            '-p',\n            '5432',\n            'test_db',\n        ]\n        expected_env = {\n            'PGPASSWORD': 'test_password',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/rootcert.pem',\n        }\n        self.assertEqual(DatabaseClient.settings_to_cmd_args_env(settings, []), (expected_args, expected_env))\n", "    def test_settings_to_cmd_args_env(self):\n        settings_dict = {\n            'HOST': 'localhost',\n            'PORT': 5432,\n            'NAME': 'test_db',\n            'USER': 'test_user',\n            'PASSWORD': 'test_password',\n            'OPTIONS': {'sslmode': 'require', 'sslrootcert': '/path/to/rootcert.pem'}\n        }\n        parameters = ['-c', 'SELECT 1']\n        expected_args = [\n            'psql',\n            '-U', 'test_user',\n            '-h', 'localhost',\n            '-p', '5432',\n            'test_db',\n            '-c', 'SELECT 1'\n        ]\n        expected_env = {\n            'PGPASSWORD': 'test_password',\n            'PGSSLMODE': 'require',\n            'PGSSLROOTCERT': '/path/to/rootcert.pem'\n        }\n\n        args, env = DatabaseClient.settings_to_cmd_args_env(settings_dict, parameters)\n        self.assertEqual(args, expected_args)\n        self.assertEqual(env, expected_env)\n\n"], "sample_831": ["    def test_export_text_max_depth(self):\n        clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n        report = export_text(clf, max_depth=2)\n\n        # Check if the report is truncated\n        assert \"truncated branch\" in report\n        # Check that the truncated branch appears only once\n        assert report.count(\"truncated branch\") == 1\n\n        report = export_text(clf, max_depth=3)\n        assert \"truncated branch\" not in report\n        \n", "    def test_export_text_with_weights(self):\n        # Test export_text with sample weights\n        clf = DecisionTreeClassifier(random_state=0)\n        clf.fit(X, y, sample_weight=w)\n        tree_str = export_text(clf, show_weights=True)\n        # Ensure that weights are included in the output\n        assert_in(\"weights:\", tree_str)\n\n", "    def test_export_text_max_depth(self):\n        tree = DecisionTreeClassifier(max_depth=2).fit(X, y)\n        expected = dedent(\"\"\"\n        |--- feature_0 <= 0.00\n        |   |--- class: -1\n        |--- feature_0 >  0.00\n        |   |--- feature_1 <= 1.50\n        |   |   |--- class: 1\n        |   |--- feature_1 >  1.50\n        |   |   |--- class: 1\n        \"\"\")\n        assert_equal(export_text(tree, max_depth=2), expected)\n\n", "    def test_export_text_class_weights(self):\n        # checking if weights are displayed correctly\n        clf = DecisionTreeClassifier(random_state=0).fit(X, y, sample_weight=w)\n        report = export_text(clf, show_weights=True)\n        assert_in(\"weights: [1.00, 1.50]\", report)\n", "    def test_export_text_max_depth(self):\n        clf = DecisionTreeClassifier(max_depth=3, random_state=42).fit(X, y)\n\n        # Test that the text representation is correctly truncated\n        # when max_depth is smaller than the actual depth.\n        report = export_text(clf, max_depth=1)\n        assert_in(\"|--- feature_0 <= 0.00\", report)\n        assert_in(\"|--- feature_0 >  0.00\\n\", report)\n        assert_not_in(\"|--- feature_1 <= 1.00\", report)\n\n"], "sample_3": ["def test_ecsv_masked_column():\n    \"\"\"Test ECSV handling of MaskedColumns.\"\"\"\n\n    data = np.ma.array([1, 2, 3, np.ma.masked], dtype='int64')\n    col = MaskedColumn(data, name='a', unit='m/s', dtype='int64',\n                        description='Column a')\n    table = Table([col])\n\n    # Write to a string, then read it back\n    with StringIO() as out:\n        ascii.write(table, out, format='ecsv')\n        out.seek(0)\n        table_read = ascii.read(out, format='ecsv')\n    assert table_read[0].mask[3]\n\n    # Test writing Table with a masked column\n    table_write = ascii.write(table, format='ecsv')\n    table_read = ascii.read(table_write, format='ecsv')\n    assert table_read[0].mask[3]\n", "def test_ecsv_masked_column_with_null():\n    \"\"\"Test reading and writing of MaskedColumns with null values.\"\"\"\n    data = np.array([1, 2, None], dtype=float)\n    mask = np.array([False, False, True])\n    col = MaskedColumn(data, mask=mask, unit='m/s')\n\n    # Create a simple table with the masked column\n    table = Table([col], names=['a'])\n    table.meta['comments'] = ['comment1', 'comment2']\n\n    # Write the table to a StringIO object and read it back\n    with StringIO() as f:\n      ascii.write(table, f, format='ecsv')\n      f.seek(0)\n      table_read = ascii.read(f, format='ecsv')\n\n    # Compare the original and read tables\n    assert np.all(table_read['a'].data == np.array([1, 2, 0]))\n    assert np.all(table_read['a'].mask == mask)\n    assert table_read.meta['comments'] == table.meta['comments']\n\n", "def test_ecsv_masked_column_write_read(tmpdir):\n    \"\"\"Test writing and reading a table with a MaskedColumn.\"\"\"\n    # Create a table with a MaskedColumn\n    data = {'a': [1, 2, 3], 'b': [4, 5, np.ma.masked]}\n    t = Table(data)\n\n    # Write the table to an ECSV file\n    fname = os.path.join(tmpdir, 'test.ecsv')\n    ascii.write(t, fname, format='ecsv')\n\n    # Read the table from the ECSV file\n    t_read = ascii.read(fname, format='ecsv')\n\n    # Check that the MaskedColumn was read correctly\n    assert isinstance(t_read['b'], MaskedColumn)\n    assert np.all(t_read['b'].data == np.array([4, 5, np.ma.masked]))\n    assert np.array_equal(t_read['b'].mask, np.array([False, False, True]))\n", "compilation error", "def test_ecsv_masked_column_write():\n    \"\"\"Test writing and reading of a masked column.\"\"\"\n    mask = [True, False, True]\n    data = np.array([1, 2, 3], dtype=int)\n    col = MaskedColumn(data, mask=mask)\n\n    t = Table([col], names=['a'])\n    with open(os.path.join(TEST_DIR, 'test.ecsv'), 'w') as f:\n        ascii.write(t, f, format='ecsv')\n\n    t2 = ascii.read(os.path.join(TEST_DIR, 'test.ecsv'), format='ecsv')\n\n    assert (t2['a'].data == data).all()\n    assert (t2['a'].mask == mask).all()\n"], "sample_626": ["    def test_pandas_multiindex_adapter_copy(self, level):\n        arrays = [\n            pd.Index([\"a\", \"b\", \"c\"]),\n            pd.Index([1, 2, 3]),\n        ]\n        multiindex = pd.MultiIndex.from_arrays(arrays, names=[\"a\", \"b\"])\n        adapter = PandasMultiIndexingAdapter(multiindex, level=level)\n        copied = adapter.copy()\n        assert copied.level == adapter.level\n        assert copied.array is not adapter.array\n", "    def test_pandas_multiindex_adapter_level(self, level):\n        mi = pd.MultiIndex.from_tuples([('a', 1), ('a', 2), ('b', 1), ('b', 2)], names=['one', 'two'])\n        da = xr.DataArray(np.arange(4), coords={'x': mi}, dims=['x'])\n\n        adapter = da.x.to_index_accessor()\n        assert adapter.level == 0\n        adapter = da.x.get_level_values(level).to_index_accessor()\n        assert adapter.level == level\n\n        adapter = PandasMultiIndexingAdapter(mi, level=level)\n        result = adapter[:]\n        expected = pandas_to_xarray_index(mi.get_level_values(level))\n        assert_identical(result, expected)\n", "    def test_pandas_multiindex_adapter_copy(self, dtype):\n        # This tests whether copying a MultiIndexAdapter correctly copies\n        # the underlying data structures\n        dates = pd.to_datetime(\n            [\n                datetime(2023, 3, 1),\n                datetime(2023, 3, 2),\n                datetime(2023, 3, 3),\n            ]\n        ).astype(dtype)\n\n        mi = pd.MultiIndex.from_tuples(\n            [(\"A\", d) for d in dates], names=[\"A\", \"time\"]\n        )\n        da = xr.DataArray(\n            np.arange(3),\n            coords={\"time\": mi},\n            dims=(\"time\",),\n        )\n\n        adapter = da.coords[\"time\"].values.to_pandas_multiindex_adapter()\n\n        copied = copy.copy(adapter)\n        assert copied is not adapter\n        assert copied.array is not adapter.array\n\n        assert_identical(copied._get_array_subset(), adapter._get_array_subset())\n", "    def test_pandas_multi_index_adapter_level(self, level):\n        index = pd.MultiIndex.from_tuples(\n            [(\"a\", 1), (\"a\", 2), (\"b\", 1), (\"b\", 2)], names=[\"level1\", \"level2\"]\n        )\n        array = xr.DataArray(np.arange(4), dims=(\"x\",), coords={\"x\": index})\n\n        adapter = PandasMultiIndexingAdapter(array.x.to_pandas_multiindex(), level=level)\n        assert adapter.level == level\n\n        if isinstance(level, str):\n            expected_adapter = PandasIndexingAdapter(index.get_level_values(level))\n        else:\n            expected_adapter = PandasIndexingAdapter(index.levels[level])\n        assert_identical(adapter[...], expected_adapter[...])\n", "    def test_pandas_multiindex_adapter_copy(self, dtype):\n        index = pd.MultiIndex.from_tuples(\n            [\n                (1, \"a\"),\n                (2, \"b\"),\n                (3, \"c\"),\n            ],\n            names=[\"level1\", \"level2\"],\n        )\n        var = Variable([\"x\"], [1, 2, 3], {\"index\": index})\n        adapter = PandasMultiIndexingAdapter(var.index, dtype=dtype)\n        copied = adapter.copy(deep=True)\n        assert copied is not adapter\n        assert copied.array is not adapter.array\n        assert copied.level == adapter.level\n        assert_array_equal(copied.array.get_level_values(\"level1\").values, adapter.array.get_level_values(\"level1\").values)\n"], "sample_561": ["    def test_triangle_fillstyle(self, marker, expected_path):\n        style = markers.MarkerStyle(marker)\n        assert style._half_fill()\n        assert isinstance(style._path, Path)\n        assert np.allclose(style._path.vertices, expected_path.vertices)\n", "    def test_caret_markers(self):\n        fig, ax = plt.subplots()\n\n        for marker in ['caretdown', 'caretup', 'caretleft', 'caretright',\n                       'caretdownbase', 'caretupbase', 'caretleftbase',\n                       'caretrightbase']:\n            ax.plot([0], [0], marker=marker, markersize=10, color='red')\n\n        plt.show()\n        fig.canvas.draw()\n        # This is a very basic check that the correct marker is drawn\n        # We should ideally check for the correct shape and position\n        # of the marker\n        assert len(ax.lines) == 8\n", "    def test_marker_path(self, marker):\n        style = markers.MarkerStyle(marker)\n        assert isinstance(style.get_path(), Path)\n", "def test_marker_fillstyle(marker):\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1], marker=marker)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    path = marker._path\n    vertices = path.vertices\n    for x, y in vertices:\n        point = renderer.to_screen((x, y))\n        assert point.x >= 0 and point.y >= 0\n\n", "    def test_marker_set_tri_up(self):\n        # Test setting the 'tri_up' marker\n        marker = markers.MarkerStyle(marker='tri_up')\n        assert marker._marker == 'tri_up'\n\n        # Test the shape and transform of the marker\n        expected_path = Path([[0.0, 0.0], [0.0, -1.0],\n                              [0.0, 0.0], [0.8, 0.5],\n                              [0.0, 0.0], [-0.8, 0.5]],\n                             [Path.MOVETO, Path.LINETO,\n                              Path.MOVETO, Path.LINETO,\n                              Path.MOVETO, Path.LINETO])\n\n        expected_transform = Affine2D().scale(0.5).rotate_deg(180)\n        np.testing.assert_allclose(marker._path.vertices, expected_path.vertices)\n        assert marker._transform.equals(expected_transform)\n\n"], "sample_348": ["    def test_inline_formset_fk_name(self):\n        class MyInline(TabularInline):\n            model = Song\n            fk_name = 'band_name'\n\n        class MyAdmin(ModelAdmin):\n            inlines = [MyInline]\n\n        self.assertIsInvalidRegexp(\n            MyAdmin, Band,\n            r\"The InlineFormSet 'MyInline' for the model 'Band' requires a 'fk_name' attribute because the model has more than one ForeignKey field to 'Band'.\",\n        )\n", "    def test_inlineformset_factory_foreign_key_to_parent_model_pk_unique(self):\n        class UniqueParentModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class UniqueChildModel(models.Model):\n            parent = models.ForeignKey(UniqueParentModel, on_delete=models.CASCADE, unique=True)\n            name = models.CharField(max_length=100)\n\n        class UniqueParentInline(admin.TabularInline):\n            model = UniqueChildModel\n            fk_name = 'parent'\n\n        class UniqueParentAdmin(admin.ModelAdmin):\n            inlines = (UniqueParentInline,)\n\n        self.assertIsInvalid(\n            UniqueParentAdmin,\n            UniqueParentModel,\n            r'unique_together constraint required for inline',\n        )\n", "    def test_inlineformset_factory_with_fk_name(self):\n        class TestInline(admin.TabularInline):\n            model = Song\n            fk_name = 'band'\n        class TestAdmin(admin.ModelAdmin):\n            inlines = [TestInline]\n        self.assertIsValid(TestAdmin, Band)\n", "    def test_raw_id_field_inline_model_check(self):\n        class MyInlineAdmin(TabularInline):\n            model = ValidationTestInlineModel\n            raw_id_fields = ('validation_test_model', )\n\n        class MyModelAdmin(ModelAdmin):\n            inlines = (MyInlineAdmin,)\n\n        self.assertIsInvalid(\n            MyModelAdmin, ValidationTestModel,\n            r\"RawIdField 'validation_test_model' in inline 'MyInlineAdmin' \"\n            r\"references a ForeignKey 'validation_test_model' that does not \"\n            r\"have a 'to_field' attribute defined.\",\n            hint=\"Define a 'to_field' attribute on the ForeignKey \"\n            \"'validation_test_model' in 'ValidationTestInlineModel' model.\",\n        )\n\n", "    def test_inline_fieldsets_with_modelform_defines_fields(self):\n        class BookModelAdmin(ModelAdmin):\n            model = ValidationTestModel\n\n                return [\n                    (\n                        TabularInline,\n                        ValidationTestInlineModel,\n                        fields=['field1', 'field2'],\n                    )\n                ]\n        self.assertIsInvalid(\n            BookModelAdmin,\n            ValidationTestModel,\n            r\"Inline 'ValidationTestInlineModel' in model 'ValidationTestModel' has fieldsets defined but ModelForm defines fields: \\['field1', 'field2'\\]\\. You should either specify fieldsets or define fields on ModelForm but not both.\",\n        )\n"], "sample_674": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_902": ["    def test_pipeline_memory_caching(self):\n        # Test that Pipelines cache fitted transformers\n        cachedir = mkdtemp()\n        memory = Memory(cachedir=cachedir, verbose=0)\n        pipeline = Pipeline([\n            ('scaler', StandardScaler()),\n            ('classifier', SVC())\n        ], memory=memory)\n\n        X, y = load_iris(return_X_y=True)\n        pipeline.fit(X, y)\n\n        # Check that the first transformer is cached\n        for filename in os.listdir(cachedir):\n            if filename.startswith('scaler'):\n                break\n        else:\n            raise AssertionError(\"Scaler was not cached\")\n\n        # Second fit should load from cache\n        pipeline.fit(X, y)\n        for filename in os.listdir(cachedir):\n            if filename.startswith('scaler'):\n                break\n        else:\n            raise AssertionError(\"Scaler was not loaded from cache\")\n\n        shutil.rmtree(cachedir)\n", "    def test_pipeline_memory(self):\n        # Test that the pipeline's steps are cached when memory is given\n        # as a string path\n\n        memory = Memory(location=mkdtemp())\n        steps = [('scaler', StandardScaler()),\n                 ('classifier', SVC())]\n        pipeline = Pipeline(steps, memory=memory)\n\n        X = np.array([[1], [2], [3]])\n        y = np.array([0, 1, 0])\n\n        pipeline.fit(X, y)\n        # Cache should be populated\n        self.assertTrue(len(memory.cache) > 0)\n\n        # Let's check if the cached results are being used\n        pipeline.fit(X, y)\n        # Assuming fit is deterministic, timestamp should be the same\n\n        shutil.rmtree(memory.cachedir)\n", "    def test_pipeline_memory_cache(self):\n        # Check that the pipeline caches the fitted transformers\n        memory = Memory(location=mkdtemp())\n        clf = Pipeline([\n            ('scaler', StandardScaler()),\n            ('classifier', DummyEstimatorParams())\n        ], memory=memory)\n\n        # First fit\n        X = np.array([[1, 2], [3, 4]])\n        clf.fit(X, [0, 1])\n        # There should be a cache file\n        self.assertTrue(len(memory.cachedir.glob('*')) > 0)\n\n        # Second fit should be faster (loaded from cache)\n        start = time.time()\n        clf.fit(X, [0, 1])\n        end = time.time()\n        self.assertLess(end - start, 0.1, \"The second fit should be faster\"\n                       \" as it loads from cache\")\n\n        # Clean up the cache directory\n        shutil.rmtree(memory.cachedir)\n", "    def test_pipeline_memory(self):\n        # Test memory cache for pipeline\n\n        # we need a transformer that is picklable\n        transf = DummyTransf()\n        clf = SVC()\n        pipe = Pipeline([('transf', transf), ('clf', clf)])\n\n        # create a cache directory\n        cachedir = mkdtemp()\n        memory = Memory(cachedir=cachedir, verbose=0)\n\n        pipe = Pipeline([('transf', transf), ('clf', clf)], memory=memory)\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n\n        # fit the pipeline\n        pipe.fit(X, y)\n        pipe.predict(X)\n        # Second fit should be faster as it's using the cache\n\n        # remove the cache directory\n        shutil.rmtree(cachedir)\n\n", "def test_pipeline_memory():\n    # Test that pipeline caching works correctly\n\n    memory = Memory(location=mkdtemp(), verbose=0)\n\n    # create a pipeline that will be cached\n    clf = Pipeline([\n        ('scaler', StandardScaler()),\n        ('pca', PCA(n_components=2)),\n        ('classify', SVC())], memory=memory)\n\n    # fit the pipeline\n    X = iris.data\n    y = iris.target\n    clf.fit(X, y)\n\n    # check that the cache exists\n    assert 'scaler.StandardScaler' in memory.cache_.keys()\n    assert 'pca.PCA' in memory.cache_.keys()\n\n    # fit again, this should be cached\n    clf.fit(X, y)\n\n    # check that the cache was used\n    assert 'scaler.StandardScaler' in memory.cache_.keys()\n    assert 'pca.PCA' in memory.cache_.keys()\n    assert clf.named_steps['pca'].timestamp_ == \\\n        memory.cache_['pca.PCA'].timestamp_\n    # Remove the temporary directory\n    shutil.rmtree(memory.location)\n\n"], "sample_826": ["    def test_fit_transform_sparse_with_unknown(self):\n        X = [['Male', 1], ['Female', 3], ['Female', 2], ['Male', 4]]\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        with assert_warns(UserWarning):\n            enc.fit(X)\n        X_out = enc.transform([['Male', 5]])\n        assert_equal(X_out.shape, (1, 5))\n\n", "    def test_onehot_encoder_drop_first():\n        X = [['a', 1], ['b', 2], ['a', 3]]\n        encoder = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n        encoder.fit(X)\n        X_trans = encoder.transform(X)\n        assert_array_equal(\n            X_trans,\n            [[0., 1., 0.],\n             [1., 0., 1.],\n             [0., 1., 1.]]\n        )\n", "    def test_onehotencoder_inverse_transform_unknown(self):\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        # introduce unknown category\n        X_new = [['Female', 4]]\n        X_trans_new = enc.transform(X_new)\n        X_inv = enc.inverse_transform(X_trans_new)\n        assert_equal(X_inv, [['Female', None]]) \n\n", "    def test_onehot_transform_error_unknown(self):\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc = OneHotEncoder(handle_unknown='error')\n        enc.fit(X)\n\n        X_test = [['Female', 4]]\n        with pytest.raises(ValueError) as exc:\n            enc.transform(X_test)\n        assert \"Found unknown categories in column 0 during transform.\" in str(exc.value)\n\n\n\n", "    def test_onehotencoder_sparse_transform_unknown_ignore(self):\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        X = [['Male'], ['Female'], ['Female'], ['Unknown']]\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        assert_equal(toarray(X_trans).shape, (4, 2))\n        assert_array_equal(toarray(X_trans).sum(axis=0), [3, 3])\n\n        X_new = [['Male'], ['Unknown']]\n        X_trans_new = enc.transform(X_new)\n        assert_equal(toarray(X_trans_new).shape, (2, 2))\n        assert_array_equal(toarray(X_trans_new).sum(axis=0), [1, 1])\n\n"], "sample_827": ["    def test_inplace_swap_row_csc(global_random_seed):\n        rs = RandomState(global_random_seed)\n        n_samples = 10\n        n_features = 5\n        X = sp.csc_matrix(rs.rand(n_samples, n_features))\n\n        # Swap two rows\n        m, n = rs.randint(0, n_samples, size=2)\n        inplace_swap_row(X, m, n)\n        expected = X.toarray()\n\n        # Check if the swap happened correctly\n        X2 = sp.csc_matrix(rs.rand(n_samples, n_features))\n        X2[m, :] = X[n, :].copy()\n        X2[n, :] = X[m, :].copy()\n        assert_allclose(X.toarray(), X2.toarray())\n\n        # Swap rows in a CSC matrix with itself\n        inplace_swap_row(X, m, m)\n        assert_allclose(X.toarray(), expected)\n\n        # Swap rows when m is a list\n        with pytest.raises(TypeError):\n            inplace_swap_row(X, [m], n)\n        with pytest.raises(TypeError):\n            inplace_swap_row(X, m, [n])\n", "compilation error", "def test_inplace_swap_column_csc():\n    rng = RandomState(0)\n    X = sp.csc_matrix(rng.rand(5, 4))\n    X.data = np.arange(1, X.nnz + 1)\n\n    X_copy = X.copy()\n    inplace_swap_column(X, 0, 1)\n    inplace_swap_column(X_copy, 1, 0)\n\n    assert_array_equal(X.toarray(), X_copy.toarray())\n", "    def test_csc_median_axis_0(random_state=0):\n        rng = RandomState(random_state)\n        X = sp.csc_matrix(rng.randn(10, 5), dtype=np.float64)\n        expected = np.median(X.toarray(), axis=0)\n        assert_allclose(csc_median_axis_0(X), expected)\n", "    def test_inplace_swap_column_csc(self):\n        X = sp.csc_matrix(np.array([[1, 2, 3], [4, 5, 6]]))\n        inplace_swap_column(X, 0, 2)\n        assert_array_equal(X.toarray(), np.array([[3, 2, 1],\n                                                [6, 5, 4]]))\n\n"], "sample_128": ["    def test_index_condition(self):\n        with self.assertRaises(ValueError):\n            Index(fields=['name'], condition=123)\n\n        index = Index(fields=['name'], condition=Q(published_date__isnull=False))\n        self.assertEqual(index.condition, Q(published_date__isnull=False))\n\n        index = Index(fields=['name'])\n        self.assertIsNone(index.condition)\n\n", "    def test_index_remove_sql(self):\n        \"\"\"\n        Test the _delete_index_sql() method.\n        \"\"\"\n        model = IndexedArticle2\n        index = Index(fields=['title'])\n        sql = index.remove_sql(model, connection.features)\n        self.assertIn('DROP INDEX', sql)\n        self.assertIn(model._meta.db_table, sql)\n        self.assertIn('title', sql)\n", "    def test_index_with_condition(self):\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT COUNT(*) FROM django_migrations')\n            num_migrations = cursor.fetchone()[0]\n\n        indexed_articles = IndexedArticle2.objects.create(\n            title='Test Article',\n            pub_date=timezone.now(),\n        )\n        indexed_articles.tags.add('tag1', 'tag2')\n        IndexedArticle2.objects.create(\n            title='Another test',\n            pub_date=timezone.now() + datetime.timedelta(days=1),\n        )\n\n        # Ensure index is created\n        with connection.cursor() as cursor:\n            cursor.execute(f\"SELECT count(*) FROM sqlite_master WHERE name = 'idx_indexedarticle2_title'\")\n            self.assertEqual(cursor.fetchone()[0], 1)\n\n        # Test that the index is usable with a condition\n        results = IndexedArticle2.objects.filter(title__startswith='Test').filter(\n            Q(tags__name='tag1')\n        ).order_by('pub_date')\n        self.assertEqual(results.count(), 1)\n        self.assertEqual(results[0].title, 'Test Article')\n\n        # Ensure no extra migrations are created after the test\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT COUNT(*) FROM django_migrations')\n            self.assertEqual(cursor.fetchone()[0], num_migrations + 1)\n        \n", "    def test_index_set_name_with_model(self):\n        article = Article.objects.create(\n            headline='Test article', pub_date=timezone.now()\n        )\n        index = Index(fields=['headline'], name='test_index')\n        index.set_name_with_model(article)\n        self.assertEqual(\n            index.name, 'article_headli_757564_idx',\n        )\n", "    def test_index_with_condition(self):\n        condition = Q(published__lt=timezone.now())\n        index = Index(fields=['published'], condition=condition)\n        index.set_name_with_model(Article)\n        self.assertEqual(index.name, 'article_published_%s' % names_digest('article', 'published', index.suffix))\n"], "sample_1030": ["def test_farthest_points():\n    p1 = Point(0, 0)\n    p2 = Point(1, 1)\n    p3 = Point(2, 0)\n    p4 = Point(3, 1)\n    assert farthest_points(p1, p2, p3, p4) == {(p1, p3)}\n\n", "    def test_farthest_points_empty():\n        raises(ValueError, lambda: farthest_points())\n", "def test_farthest_points():\n    p1 = Point(0, 0)\n    p2 = Point(1, 0)\n    p3 = Point(0, 1)\n    assert farthest_points(p1, p2, p3) == {(p1, p3)}\n\n", "def test_farthest_points_empty():\n    p = []\n    assert farthest_points(*p) == set()\n", "    def test_closest_points_identical():\n        p1 = Point(0, 0)\n        p2 = Point(0, 0)\n        assert closest_points(p1, p2) == {(p1, p2)}\n"], "sample_724": ["compilation error", "compilation error", "compilation error", "compilation error", "    def test_imputer_dtype_preservation():\n        # Test that the dtype of the input array is preserved after imputation.\n        X = np.array([[1, 2], [np.nan, 4], [5, np.nan]])\n        imputer = Imputer(strategy=\"mean\")\n        X_imputed = imputer.fit_transform(X)\n        assert_equal(X_imputed.dtype, X.dtype)\n\n"], "sample_979": ["def test_Transpose_MatrixSymbol():\n    assert Transpose(A).shape == (m, n)\n    assert isinstance(Transpose(A), Transpose)\n    assert Transpose(A)[0, 0] == MatrixElement(Transpose(A), 0, 0)\n", "def test_matrix_determinant():\n    assert (A + B).det() == None\n    assert (A*B).det() == None\n    assert C.det().is_symbol\n    assert Identity(3).det() == 1\n    assert ZeroMatrix(2, 3).det() == 0\n", "def test_MatrixElement_diff():\n    i, j = symbols('i j')\n    M = MatrixSymbol('M', n, n)\n    me = MatrixElement(M, i, j)\n    assert me.diff(M[i, j]) == 1\n    assert me.diff(M[j, i]) == 0\n    assert me.diff(M[i + 1, j]) == 0\n    assert me.diff(M[i, j + 1]) == 0\n\n    assert me.diff(x) == 0\n\n    # Test with a MatrixSymbol and a scalar\n    scalar = Symbol('a')\n    me = MatrixElement(scalar, i, j)\n    assert me.diff(scalar) == KroneckerDelta(i, 0)*KroneckerDelta(j, 0)\n", "    def test_transpose(self):\n        self.assertEqual(transpose(A).shape, (m, n))\n        self.assertEqual(transpose(A*B).shape, (l, n))\n        self.assertEqual(transpose(2*A).shape, (n, m))\n        self.assertEqual(transpose(A.T), A)\n", "def test_MatrixSymbol_subs():\n    a = MatrixSymbol('A', n, m)\n    b = a.subs(n, n + 1)\n    assert b.shape == (n + 1, m)\n    assert b.name == 'A'\n"], "sample_541": ["    def test_polygon_selector_modify_after_completion(self, ax):\n        fig, ax = plt.subplots()\n\n            assert_allclose(verts, [\n                (0.1, 0.2), (0.3, 0.4), (0.5, 0.6), (0.7, 0.8), (0.1, 0.2)\n            ])\n\n        selector = widgets.PolygonSelector(ax, onselect, useblit=False)\n\n        click_and_drag(ax, (0.1, 0.2), (0.3, 0.4))\n        click_and_drag(ax, (0.3, 0.4), (0.5, 0.6))\n        click_and_drag(ax, (0.5, 0.6), (0.7, 0.8))\n        click_and_drag(ax, (0.7, 0.8), (0.1, 0.2))\n\n        # Complete the polygon\n        do_event(ax, 'button_release_event', x=0.1, y=0.2)\n\n        # Modify one of the vertices\n        click_and_drag(ax, (0.3, 0.4), (0.35, 0.45))\n        do_event(ax, 'button_release_event', x=0.35, y=0.45)\n", "def test_lasso_selector(ax):\n    callback = mock.Mock()\n\n    lasso = widgets.Lasso(ax, (0.5, 0.5), callback)\n    event = mock_event(ax, \"button_press_event\", x=0.2, y=0.2, button=1)\n    lasso.onpress(event)\n\n    click_and_drag(lasso, ax, [(0.2, 0.2), (0.8, 0.2)])\n    lasso.onrelease(mock_event(ax, \"button_release_event\", x=0.8, y=0.2))\n\n    assert callback.call_count == 1\n    verts = callback.call_args[0][0]\n    assert len(verts) > 2\n    assert_allclose(verts[0], (0.2, 0.2))\n    assert_allclose(verts[-1], (0.8, 0.2))\n\n", "    def test_polygonselector_incomplete(ax):\n        fig, ax = plt.subplots()\n        selector = widgets.PolygonSelector(ax, lambda verts: None)\n\n        click_and_drag(ax, (0.1, 0.1), (0.2, 0.2))\n        assert len(selector._xys) == 2\n        assert not selector._selection_completed\n\n", "def test_polygonselector_draw_box(ax, useblit):\n    fig, ax = plt.subplots()\n\n    # Create a mock event for initialization\n    event = mock.Mock()\n    event.xdata = 0.5\n    event.ydata = 0.5\n\n    selector = widgets.PolygonSelector(ax, lambda verts: None, useblit=useblit)\n    selector._xys = [(0.1, 0.1), (0.9, 0.1), (0.9, 0.9), (0.1, 0.9), (0.1, 0.1)]\n    selector._selection_completed = True\n\n    selector._draw_box = True\n\n    # Trigger the box drawing\n    selector._draw_polygon()\n\n    # Assert that the box exists\n    assert selector._box is not None\n\n    # Remove the selector and the box\n    selector.disconnect_events()\n    selector._remove_box()\n\n", "def test_polygon_selector_draw_box():\n    fig, ax = plt.subplots()\n    selector = widgets.PolygonSelector(ax, noop, useblit=False,\n                                       draw_bounding_box=True)\n\n    selector._xys = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n    selector._selection_completed = True\n    selector._draw_polygon()\n\n    assert isinstance(selector._box, widgets.RectangleSelector)\n\n    # Check that the box is visible and has the correct extent\n    assert selector._box.get_visible()\n    assert selector._box.extents == [0, 1, 0, 1]\n\n    # Remove the box and check that it is no longer visible\n    selector._remove_box()\n    assert selector._box is None\n"], "sample_786": ["    def test_transform_with_constant_feature(self):\n        X = [[1, 2], [1, 2], [1, 2]]\n        est = KBinsDiscretizer(n_bins=5, encode='onehot', strategy='uniform')\n        with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                Xt = est.fit_transform(X)\n        assert_array_equal(Xt.toarray(), [[1, 0, 0, 0, 0],\n                                       [1, 0, 0, 0, 0],\n                                       [1, 0, 0, 0, 0]])\n", "def test_transform_encode(encode, expected_shape, expected_type):\n    est = KBinsDiscretizer(n_bins=3, encode=encode, strategy='uniform')\n    Xt = est.fit_transform(X)\n    assert Xt.shape == expected_shape\n    assert isinstance(Xt, type(None if expected_type == 'None' else eval(expected_type)))\n", "def test_transform_encode_onehot_dense(strategy):\n    est = KBinsDiscretizer(n_bins=3, encode='onehot-dense', strategy=strategy)\n    Xt = est.fit_transform(X)\n    expected = np.array([[1., 0., 0., 1.],\n                        [0., 1., 0., 0.],\n                        [0., 0., 1., 0.],\n                        [0., 0., 0., 0.]])\n    assert_array_equal(Xt, expected)\n\n", "def test_constant_feature():\n\n    X = [[1, 1, 1, 1],\n         [1, 1, 1, 1],\n         [1, 1, 1, 1],\n         [1, 1, 1, 1]]\n\n    est = KBinsDiscretizer(n_bins=3, encode='onehot', strategy='uniform')\n    with warnings.catch_warnings(record=True) as w:\n        est.fit(X)\n        assert len(w) == 1\n        assert \"Feature 0 is constant\" in str(w[0].message)\n        assert \"Feature 1 is constant\" in str(w[1].message)\n        assert \"Feature 2 is constant\" in str(w[2].message)\n        assert \"Feature 3 is constant\" in str(w[3].message)\n    Xt = est.transform(X)\n\n    assert_array_equal(Xt, [[0, 0, 1],\n                           [0, 0, 1],\n                           [0, 0, 1],\n                           [0, 0, 1]]) \n", "    def test_inverse_transform_constant_feature(self):\n        # Test inverse_transform when a feature is constant\n        X = [[1, 1, 1],\n             [1, 1, 1],\n             [1, 1, 1]]\n        est = KBinsDiscretizer(n_bins=2, encode='ordinal', strategy='uniform')\n        est.fit(X)\n        Xt = est.transform(X)\n        Xinv = est.inverse_transform(Xt)\n        assert_array_equal(Xinv, [[1., 1., 1.],\n                                  [1., 1., 1.],\n                                  [1., 1., 1.]])\n"], "sample_1183": ["def test_field_from_expr():\n    F, x = field('x', QQ)\n    assert F.from_expr(x**2 + 1) == F(x**2 + 1)\n    assert F.from_expr(1/x) == F(1/x)\n    assert F.from_expr(sin(x)) == F(sin(x))\n    assert F.from_expr((x+1)/(x-1)) == F((x+1)/(x-1))\n    assert F.from_expr(exp(x)).to_expr() == exp(x)\n", "def test_field_sfield():\n    F, x = field(\"x\", ZZ)\n    f = F(x**2 + 2*x + 1)\n\n    assert f.field == F\n    assert f.to_poly() == x**2 + 2*x + 1\n\n    assert field(\"x,y\", QQ) == field(\"y,x\", QQ)\n\n    F, x, y = field(\"x,y\", QQ)\n    assert F(x*y + x + y) == F((1 + y)*x + y)\n\n    F, x = field(\"x\", ZZ)\n    p = Poly(x**2 + 1, x)\n    assert F(p) == F(x**2 + 1)\n\n    G, y = field(\"y\", ZZ)\n    assert F.extend(G) == F.field\n\n\n", "def test_field_from_expr():\n    F, x, y = field(\"x,y\", QQ)\n    f = (x**2 + y)/(x - 1)\n    assert F.from_expr(f) == f\n    assert F.from_expr(f.as_numer_denom()) == f\n    assert F(f) == f\n\n    F, x = field(\"x\", ZZ)\n    raises(ValueError, lambda: F.from_expr(sqrt(2)))\n    raises(ValueError, lambda: F.from_expr(pi))\n    raises(ValueError, lambda: F(sqrt(2)))\n    raises(ValueError, lambda: F(pi))\n\n", "    def test_field_vfield_sfield(self):\n        from sympy import Symbol\n        x = Symbol('x')\n        K0, _ = field(\"x\", ZZ)\n        K1, _ = vfield(\"x\", ZZ)\n        unify(K0, K1)\n        K2, _ = sfield(x**2/2+1, x)\n        assert unify(K0, K2) == K2\n", "def test_field_compose():\n    R, x = ring(\"x\", QQ)\n    F = field(\"y\", QQ)\n    f = F(x**2 + 1)\n    assert f.compose(x, x + 1) == F((x + 1)**2 + 1)\n"], "sample_563": ["    def test_draggable_annotation(self):\n        # Test that DraggableAnnotation moves the annotation correctly.\n\n        fig, ax = plt.subplots()\n        x, y = 0.5, 0.5\n        text = ax.text(x, y, \"Hello\")\n        ann = AnnotationBbox(\n            text, (x, y),  bboxprops=dict(boxstyle=\"round\", fc=\"w\"),\n            frameon=True)\n        ax.add_artist(ann)\n        dragger = DraggableAnnotation(ann)\n\n        # Simulate dragging the annotation\n\n        mousedown_event = MouseEvent(\n            'button_press_event', fig.canvas, xtrans=x, ytrans=y,\n            button=MouseButton.LEFT.value)\n        dragger.on_pick(mousedown_event)\n        \n        # Move the mouse \n        mousemove_event = MouseEvent(\n            'motion_notify_event', fig.canvas, xtrans=x + 0.1, ytrans=y + 0.1,\n            button=MouseButton.LEFT.value)\n        dragger.on_motion(mousemove_event)\n        \n        mouseup_event = MouseEvent(\n            'button_release_event', fig.canvas, xtrans=x + 0.1, ytrans=y + 0.1,\n            button=MouseButton.LEFT.value)\n        dragger.on_release(mouseup_event)\n\n        # Check that the annotation has moved to the new position\n        assert_allclose(ann.xyann, (x + 0.1, y + 0.1))\n\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        txt = ax.text(0.5, 0.5, \"hello\")\n        ann = AnnotationBbox(\n            OffsetImage(np.array([[1, 2], [3, 4]]), cmap='viridis'),\n            (0.6, 0.6),\n            xybox=(0.7, 0.7),\n            frameon=True,\n            boxcoords=\"axes fraction\",\n            arrowprops=dict(arrowstyle=\"->\"))\n        ann.set_draggable(True)\n        ax.add_artist(ann)\n        fig.canvas.draw()\n        event = MouseEvent('motion_notify_event', fig.canvas, x=500, y=400)\n        ann.on_motion(event)\n        assert_allclose(ann.xybox, (0.7, 0.7))\n\n\n", "def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    ann = ax.annotate(\"Annotation\", xy=(0.5, 0.5), xycoords=\"data\")\n    draggable = DraggableAnnotation(ann)\n\n    event = MouseEvent(\n        'motion_notify_event', fig.canvas, x=200, y=200, button=None\n    )\n    draggable.on_motion(event)\n    assert ann.xyann == ann.get_transform().transform((0.5, 0.5))\n\n    event = MouseEvent(\n        'pick_event', fig.canvas, x=200, y=200, button={'button'}\n    )\n    draggable.on_pick(event)\n    assert draggable.got_artist\n\n\n", "def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    text = ax.text(0.5, 0.5, 'Draggable', ha='center', va='center')\n    ann = AnnotationBbox(\n        text, (0.7, 0.7), xycoords='axes fraction',\n        boxcoords='axes fraction',\n        box_alignment=(0.5, 0.5),\n        arrowprops=dict(arrowstyle='->'))\n    \n    draggable = DraggableAnnotation(ann)\n    fig.canvas.draw()\n    \n    # Emulate mouse click and drag\n    event = MouseEvent(\n        'button_press_event', ax, x=250, y=250, button=MouseButton.LEFT)\n    draggable.on_pick(event)\n\n    event = MouseEvent(\n        'motion_notify_event', ax, x=300, y=300)\n    draggable.on_motion(event)\n\n    event = MouseEvent(\n        'button_release_event', ax, x=300, y=300, button=MouseButton.LEFT)\n    draggable.on_release(event)\n\n    assert_allclose(ann.xyann, (0.8, 0.8))\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        txt = ax.text(0.5, 0.5, \"Hello\")\n        ann = AnnotationBbox(\n            txt, (0.8, 0.8), xycoords='data', boxcoords='data',\n            arrowprops=dict(arrowstyle=\"->\"),\n            frameon=True,\n            pad=0.5,\n        )\n        ax.add_artist(ann)\n\n        dragged = [False]\n            dragged[0] = True\n        ann.draggable()\n\n        fig.canvas.mpl_connect('button_release_event', on_release)\n\n        # Trigger the drag events manually\n        mouse_event = MouseEvent(\n            'button_release_event', fig.canvas, 100, 100, button=MouseButton.LEFT\n        )\n        fig.canvas.draw()\n"], "sample_1048": ["def test_parabola_intersection_with_parabola():\n    p1 = Parabola(Point(0, 0), Line(Point(1, 0), Point(2, 0)))\n    p2 = Parabola(Point(0, 2), Line(Point(1, 2), Point(2, 2)))\n    assert p1.intersection(p2) == []\n", "def test_parabola_p_parameter():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    assert p1.p_parameter == -4\n    p2 = Parabola(Point(0, 0), Line(Point(0, 8), Point(0, 14)))\n    assert p2.p_parameter == 4\n", "def test_parabola_intersecion_with_line():\n    p1 = Parabola(Point(0,0), Line(Point(1, -2), Point(-1,-2)))\n    l1 = Line(Point(-7, 3), Point(12, 3))\n    intersection_points = p1.intersection(l1)\n    assert intersection_points == [Point2D(-4, 3), Point2D(4, 3)]\n\n", "def test_parabola_intersection_with_line():\n    p1 = Parabola(Point(0, 0), Line(Point(5, 8), Point(7, 8)))\n    l1 = Line(Point(-7, 3), Point(12, 3))\n    assert p1.intersection(l1) == [Point2D(-4, 3), Point2D(4, 3)]\n", "def test_parabola_intersection_ellipse():\n    p1 = Parabola(Point(0, 0), Line(Point(1, 0), Point(2, 0)))\n    e1 = Ellipse(Point2D(0, 0), 3, 2)\n    intersection_points = p1.intersection(e1)\n    assert len(intersection_points) == 4\n    for point in intersection_points:\n        assert isinstance(point, Point2D)\n    assert intersection_points[0].x < intersection_points[1].x < intersection_points[2].x < intersection_points[3].x\n\n"], "sample_1202": ["def test_isqrt():\n    assert isqrt(0) == 0\n    assert isqrt(1) == 1\n    assert isqrt(2) == 1\n    assert isqrt(3) == 1\n    assert isqrt(4) == 2\n    assert isqrt(8) == 2\n    assert isqrt(9) == 3\n\n\n", "def test_Rational_as_continued_fraction():\n    assert Rational(3, 7).as_continued_fraction() == [0, 2, 1, 1, 1, 6]\n    assert Rational(22, 7).as_continued_fraction() == [3, 1, 1, 1, 1, 6]\n    assert Rational(1, 3).as_continued_fraction() == [0, 3]\n    assert Rational(-1, 3).as_continued_fraction() == [-1, 3]\n    assert Rational(5, 5).as_continued_fraction() == [1]\n    assert Rational(0, 1).as_continued_fraction() == [0]\n\n\n\n", "    def test_Integer_is_prime():\n        assert Integer(2).is_prime\n        assert Integer(3).is_prime\n        assert not Integer(4).is_prime\n        assert Integer(17).is_prime\n        assert not Integer(1).is_prime\n        assert not Integer(0).is_prime\n\n        assert S.Zero.is_prime is False \n        assert S.One.is_prime is False\n\n", "def test_sympify_numbers_complex():\n    x = complex(1.2, 3.4)\n    assert sympify(x) == 1.2 + 3.4*S.ImaginaryUnit\n    assert type(sympify(x)) == Add\n", "def test_Float_Floor():\n    assert float(floor(Float(3.7))) == 3\n    assert float(floor(Float(-3.7))) == -4\n    assert float(floor(Float('1.234e+100'))) == float(1.234e+100)\n\n\n"], "sample_111": ["    def test_changelist_pagination_with_invalid_page_number(self):\n        request = self._mocked_authenticated_request('/admin/polls/question/', self.superuser)\n        from django.contrib.admin.views.main import ChangeList\n        cl = ChangeList(request, Question, list_display=('question_text',),\n                        list_per_page=2)\n        cl.page_num = -1\n        with self.assertRaisesMessage(IncorrectLookupParameters, 'Invalid page number.'):\n            cl.get_results(request)\n", "    def test_get_queryset_ordering_default_ordering(self):\n        band = Band.objects.create(name='The Beatles')\n        musician1 = Musician.objects.create(name='John Lennon', band=band)\n        musician2 = Musician.objects.create(name='Paul McCartney', band=band)\n        request = self._mocked_authenticated_request('/admin/music/band/', self.superuser)\n        cl = BandAdmin(Band, self.superuser.id)\n        cl.get_queryset(request)\n        self.assertEqual(cl.queryset.first().name, 'The Beatles')\n\n", "    def test_get_ordering_field_columns(self):\n        request = self.factory.get(reverse('admin:polls_question_changelist'))\n        request.user = self.superuser\n        cl = ChangeList(request, Question, ['name', 'pub_date'], ['name'],\n                        ['pub_date'], 100, 100, None,\n                        QuestionAdmin(Question, site.ModelAdmin), None)\n        self.assertEqual(cl.get_ordering_field_columns(),\n                         {0: 'asc', 1: 'asc',})\n        request.GET = {'ORDER_VAR': '0.desc'}\n        cl = ChangeList(request, Question, ['name', 'pub_date'], ['name'],\n                        ['pub_date'], 100, 100, None,\n                        QuestionAdmin(Question, site.ModelAdmin), None)\n        self.assertEqual(cl.get_ordering_field_columns(),\n                         {0: 'desc',})\n", "    def test_get_ordering_field_columns(self):\n        request = self.factory.get(reverse('admin:musician_musician_changelist'))\n        request.user = self.superuser\n        musician_admin = MusicianAdmin(Musician, site=custom_site)\n        cl = musician_admin.changelist_view(request)\n\n        ordering_fields = cl.get_ordering_field_columns()\n        self.assertEqual(ordering_fields[0], 'asc')\n        self.assertEqual(ordering_fields[2], 'asc')\n        musician_admin.list_display = ['name', 'instrument', 'age', 'band']\n        cl = musician_admin.changelist_view(request)\n        ordering_fields = cl.get_ordering_field_columns()\n        self.assertEqual(ordering_fields[3], 'asc')\n\n", "    def test_get_ordering_field_columns(self):\n        request = self._mocked_authenticated_request(reverse('admin:musician_musician_changelist'), self.superuser)\n        musician_admin = MusicianAdmin(Musician, site=custom_site)\n        changelist = musician_admin.changelist_view(request)\n        ordering_fields = changelist.get_ordering_field_columns()\n        self.assertEqual(ordering_fields, {0: 'asc'})\n\n"], "sample_1041": ["def test_transpose_mul():\n    # Test transposing a multiplication of matrices\n    expr = Transpose(A*B)\n    assert expr == B.T*A.T\n\n    # Test transposing a multiplication of matrices with transposes\n    expr = Transpose(A.T*B.T)\n    assert expr == B*A\n\n", "def test_matrix_pow_diff():\n    M = MatrixSymbol('M', 2, 2)\n    x = symbols('x')\n\n    assert diff(M**2, x) == ZeroMatrix(2, 2)\n    assert diff(M**x, x) == M**x*log(M)\n\n", "    def test_transpose_add():\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        expr = A + B\n        assert isinstance(expr.transpose(), MatAdd)\n        assert expr.transpose().shape == (2, 2)\n        assert expr.transpose().args == (A.transpose(), B.transpose())\n\n", "    def test_matadd_scalar():\n        assert MatAdd(A, 1) == A + 1\n        assert MatAdd(1, A) == 1 + A\n        assert MatAdd(1, 2) == 3\n        assert MatAdd(A, B, 2) == A + B + 2\n", "    def test_transpose_add():\n        assert (A + B).T == A.T + B.T\n"], "sample_77": ["    def test_strip_tags_recursive(self):\n        \"\"\"Test strip_tags with nested tags.\"\"\"\n        value = \"\"\"\n            <p>This is a paragraph <b>with <i>nested</i> tags</b>.</p>\n            <div><span>Another</span> <span>element</span></div>\n        \"\"\"\n        expected = (\n            \"This is a paragraph with nested tags. \"\n            \"Another element\"\n        )\n        self.assertEqual(strip_tags(value), expected)\n", "    def test_urlize_nofollow(self):\n        self.check_output(urlize, 'https://www.djangoproject.com',\n                          '<a href=\"https://www.djangoproject.com\">https://www.djangoproject.com</a>')\n        self.check_output(urlize, 'https://www.djangoproject.com',\n                          '<a href=\"https://www.djangoproject.com\" rel=\"nofollow\">https://www.djangoproject.com</a>',\n                          nofollow=True)\n", "    def test_json_script(self):\n        data = {'key': 'value', 'list': [1, 2, 3]}\n        script = json_script(data, 'my_script')\n        self.assertIn('<script id=\"my_script\" type=\"application/json\">', script)\n        self.assertIn(json.dumps(data).replace('<', '\\\\u003C').replace('>', '\\\\u003E'), script)\n        self.assertIn('</script>', script)\n", "    def test_urlize_nofollow(self):\n        self.check_output(\n            urlize,\n            'Check out http://www.djangoproject.com/',\n            '<a href=\"http://www.djangoproject.com/\">Check out http://www.djangoproject.com/</a>',\n        )\n        self.check_output(\n            urlize,\n            'Check out http://www.djangoproject.com/',\n            '<a href=\"http://www.djangoproject.com/\" rel=\"nofollow\">Check out http://www.djangoproject.com/</a>',\n            nofollow=True,\n        )\n", "    def test_urlize_simple_email(self):\n        self.assertEqual(\n            urlize(\"Contact me at my.address@example.com.\"),\n            \"<p>Contact me at <a href=\\\"mailto:my.address@example.com\\\">my.address@example.com</a>.</p>\",\n        )\n"], "sample_507": ["    def test_update_convertible(self):\n        ud = cat.UnitData([\"1\", \"2\"])\n        assert list(ud._mapping.keys()) == [\"1\", \"2\"]\n\n        ud.update([\"3\", \"4\"])\n        assert list(ud._mapping.keys()) == [\"1\", \"2\", \"3\", \"4\"]\n\n        # check if warning is raised for convertible strings\n        with pytest.warns(MatplotlibDeprecationWarning):\n            ud.update([\"5.5\", \"6.6\"])\n        assert list(ud._mapping.keys()) == [\"1\", \"2\", \"3\", \"4\", \"5.5\", \"6.6\"]\n", "    def test_update(self):\n        ud = cat.UnitData()\n        ud.update(['A', 'B', 'C'])\n        assert ud._mapping == {'A': 0, 'B': 1, 'C': 2}\n        ud.update(['B', 'D'])\n        assert ud._mapping == {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n", "    def test_UnitData(self, data, locs):\n        data, locs = data\n        ud = cat.UnitData(data)\n        assert list(ud._mapping.values()) == locs\n", "    def test_numbers_warning(self):\n        with pytest.warns(MatplotlibDeprecationWarning):\n            cat.UnitData([1, 2, 3])\n", "    def test_update_with_existing_data(self):\n        data = ['A', 'B', 'C']\n        ud = cat.UnitData(data)\n        assert list(ud._mapping.values()) == [0, 1, 2]\n        ud.update(['B', 'D'])\n        assert list(ud._mapping.values()) == [0, 1, 2, 3]\n"], "sample_441": ["    def test_empty_password_creation(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': '', 'password2': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            _(\n                \"This field is required.\"\n            ),\n        ])\n        self.assertEqual(form.errors['password2'], [\n            _(\n                \"This field is required.\"\n            ),\n        ])\n", "    def test_password_validation(self):\n        form = UserCreationForm({\"username\": \"newuser\", \"password1\": \"short\", \"password2\": \"short\"})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors[\"password2\"],\n            [\n                _(\"The two password fields didn\u2019t match.\"),\n            ],\n        )\n\n        form = UserCreationForm({\"username\": \"newuser\", \"password1\": \"password\", \"password2\": \"password\"})\n        self.assertTrue(form.is_valid())\n", "    def test_form_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL=\"auth_tests.models.CustomUser\"):\n            form = UserCreationForm()\n            self.assertEqual(\n                form.fields[\"username\"].widget.attrs.get(\"autocapitalize\"), \"none\"\n            )\n", "    def test_uppercase_username(self):\n        data = {\"username\": \"TESTCLIENT\", \"password1\": \"password\", \"password2\": \"password\"}\n        form = UserCreationForm(data=data)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data[\"username\"], \"testclient\")\n", "    def test_password_validation_fails(self):\n        form = UserCreationForm({'username': 'newuser', 'password1': 'password', 'password2': 'different'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [_( 'The two password fields didn\u2019t match.')])\n"], "sample_482": ["    def test_escapeseq(self):\n        self.assertEqual(self.render('escapeseq_basic', {'a': ['<a>', '<b>'], 'b': mark_safe('<a><b>')}), \n                         '&lt;a&gt;, &lt;b&gt; -- &lt;a&gt;&lt;b&gt;')\n", "    def test_escapeseq_basic(self):\n        self.assertEqual(\n            self.render({\"a\": [\"<script>\", \"foo\"], \"b\": [\"&\", \"<br>\"]}),\n            \"&lt;script&gt;, foo -- &amp;, &lt;br&gt;\",\n        )\n", "    def test_escapeseq_with_text_content(self):\n        self.assertEqual(\n            self.render('escapeseq_basic', {'a': ['Hello', '<world>'], 'b': ['<p>', '</p>']}),\n            'Hello, &lt;world&gt; -- &lt;p&gt;, &lt;/p&gt;',\n        )\n", "    def test_escapeseq_basic(self):\n        result = self.render(\"escapeseq_basic\", {\"a\": ['<a>', '\"b\"', '&c'], \"b\": mark_safe('<a>, \"b\", &c')})\n        self.assertEqual(result, \"&lt;a&gt;, &quot;b&quot;, &amp;c -- &lt;a&gt;, &quot;b&quot;, &amp;c\")\n", "    def test_escapeseq_basic(self):\n        template = self.engine.get_template('escapeseq_basic')\n        self.assertEqual(\n            template.render(context={'a': ['<p>', '&'], 'b': [mark_safe('<p>'), '&']}),\n            '&lt;p&gt;, & -- &lt;p&gt;, &'\n        )\n"], "sample_90": ["    def test_custom_error_message(self):\n        form = CustomErrorMessageForm({'name1': 'invalid'})\n        self.assertTrue(form.is_valid() is False)\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n", "    def test_save_new_with_related_instance(self):\n        author = Author.objects.create(name='Author Name')\n        form = PostForm(data={'title': 'Post Title', 'author': author.pk, 'content': 'Content'})\n        self.assertTrue(form.is_valid())\n        post = form.save(commit=False)\n        self.assertEqual(post.author, author)\n        post.save()\n        self.assertEqual(Post.objects.count(), 1)\n        self.assertEqual(Post.objects.first().author, author)\n", "    def test_to_python_handles_empty_string(self):\n        form_data = {'name': ''}\n        form = CustomErrorMessageForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertEqual(instance.name1, '')\n", "    def test_custom_error_messages(self):\n        form = CustomErrorMessageForm(data={'name1': 'bad'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['name1'],\n            ['Form custom error message.']\n        )\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm({\n            'name1': 'invalid',\n        })\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n"], "sample_25": ["    def test_card_format_long_string(self):\n        \"\"\"Test the formatting of long string values.\"\"\"\n\n        card = fits.Card(\"COMMENT\", \"This is a very long comment that \"\n                         \"will be split into multiple lines.\")\n        assert card._format_image() == (\n            \"COMMENT= 'This is a very long comment that ' / \"\n            \"CONTINUE  '&' / This will be split into multiple lines.'\"\n        )\n\n        card = fits.Card(\"HISTORY\", \"This is a very long history entry that\"\n                         \" will be split into multiple lines.\")\n        assert card._format_image() == (\n            \"HISTORY= 'This is a very long history entry that' / \"\n            \"CONTINUE  '&' / will be split into multiple lines.'\"\n        )\n\n        card = fits.Card(\"KEY\", \"This is a very long string\" * 20)\n\n        # Should truncate comment to fit in 80 characters\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", VerifyWarning)\n            assert card._format_image() == (\n                \"KEY     = 'This is a very long stringThis is a very \"\n                \"long stringThis is a very long stringThis is a very long \"\n                \"stringThis is a very long stringThis is a very long \"\n                \"stringThis is a very long stringThis is a very long \"\n                \"stringThis is a very long stringThis is a very long \"\n                \"stringThis is a very long stringThis is a very long \"\n                \"stringThis is a very long stringThis is a very long ' / \"\n            )\n\n\n\n", "    def test_fix_invalid_keyword(self):\n        card = Card.fromstring('  invalid keyword =  \"value\"')\n        assert card.keyword == 'invalid keyword'\n        card._fix_keyword()\n        assert card.keyword == 'INVALID KEYWORD'\n        assert card.image == 'INVALID KEYWORD       = \"value\" '\n\n\n", "    def test_card_keyword_fix_case(self):\n        card = fits.Card(\"temp\", 1)\n        card._keyword = \"tEmp\"\n        card._fix_keyword()\n        assert card.keyword == \"TEMP\"\n\n\n", "    def test_card_fix_valuestring(self):\n        card = fits.Card.fromstring('HIERARCH TEST =   100.012345678')\n        assert card.value == 100.012345678\n        assert card._valuestring == '100.012345678'\n        card._fix_value()\n        assert card.value == 100.012345678\n        assert card._valuestring == '100.012345678'\n\n", "    def test_invalid_keyword_fix(self):\n        card = fits.Card.fromstring('  BADKEY = 123')\n        with pytest.warns(AstropyUserWarning, match=_NOT_OVERWRITING_MSG_MATCH):\n            card.verify('fix')\n        assert card.keyword == 'BADKEY'\n        assert card._image == 'BADKEY= 123'\n\n"], "sample_201": ["    def test_cookie_storage_max_size(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(constants.ERROR, 'Message 1'),\n                    Message(constants.ERROR, 'Message 2'),\n                    Message(constants.ERROR, 'Message 3'),\n                    Message(constants.ERROR, 'Message 4')]\n        set_cookie_data(storage, messages)\n        with self.settings(SESSION_COOKIE_MAX_SIZE=100):\n            response = self.get_response()\n            self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n            self.assertEqual(storage._loaded_data[-1], CookieStorage.not_finished)\n\n\n", "    def test_max_cookie_size_invalid_message_data(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(\n            'This is a test message', level=constants.INFO, extra_tags=['test_tag']\n        ) for _ in range(5)] * 10\n        set_cookie_data(storage, messages)\n        response = self.client.get('/')\n        self.assertEqual(stored_cookie_messages_count(storage, response),\n                         CookieStorage.max_cookie_size // 10)\n", "    def test_cookie_storage_max_size_limit(self):\n        # Create a storage instance.\n        storage = self.get_storage()\n\n        # Create a list of messages that exceeds the max cookie size.\n        messages = [Message(constants.INFO, f'Message {i}') for i in range(100)]\n\n        # Store the messages.\n        response = storage.get_response(messages)\n\n        # Check that the number of stored messages is less than the original list.\n        self.assertLess(stored_cookie_messages_count(storage, response), len(messages))\n\n        # Retrieve the stored messages.\n        retrieved_messages, all_retrieved = storage.get()\n\n        # Check that not all messages were retrieved.\n        self.assertFalse(all_retrieved)\n\n        # Check that the last message is the sentinel value.\n        self.assertEqual(retrieved_messages[-1], CookieStorage.not_finished)\n\n        # Check that the number of retrieved messages is less than the original list.\n        self.assertLess(len(retrieved_messages), len(messages))\n\n", "    def test_max_cookie_size_removal(self):\n        storage = self.storage_class(self.request)\n        messages = [Message(level=constants.ERROR, message='Hello', extra_tags=['error']) for _ in range(10)]\n        set_cookie_data(storage, messages)\n        response = self.handle_request(lambda r: None)\n        self.assertEqual(stored_cookie_messages_count(storage, response), storage.max_cookie_size // 100)\n        storage = self.storage_class(self.request)\n        response = self.handle_request(lambda r: None)\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_cookie_storage_max_size(self):\n        storage = self.get_storage()\n        messages = [Message(level=constants.SUCCESS, message='Message 1')]\n        for i in range(2, 101):\n            messages.append(Message(level=constants.SUCCESS, message=f'Message {i}'))\n        response = self.get_response()\n        storage._store(messages, response)\n\n        # Make sure all messages are stored initially\n        self.assertEqual(self.stored_messages_count(storage, response), len(messages))\n\n        # Modify max cookie size to be smaller than required for all messages\n        storage.max_cookie_size = 100\n\n        # Store messages again, some should be truncated due to size limit\n        response = self.get_response()\n        unstored_messages = storage._store(messages, response)\n\n        # Assert that some messages were not stored\n        self.assertGreater(len(messages), len(unstored_messages))\n\n        # Assert that the truncated cookie contains the 'not_finished' sentinel\n        data = storage._decode(response.cookies.get(storage.cookie_name).value)\n        self.assertIn(storage.not_finished, data)\n"], "sample_119": ["    def test_build_filter_negation(self):\n        q = Query(model=Author)\n        q.add_filter(('name__startswith', 'John'), AND)\n        q.add_filter(('age__gt', 30), AND)\n        negated_filter = q.build_filter(('name__endswith', 'Doe'), True, False, {})\n        self.assertEqual(negated_filter.children[0].negated, True)\n        self.assertEqual(negated_filter.children[0].lhs.col, \"name\")\n        self.assertEqual(negated_filter.children[0].rhs, \"Doe\")\n", "    def test_q_objects_and_related_lookups(self):\n        item1 = Item.objects.create(name='Item 1')\n        item2 = Item.objects.create(name='Item 2')\n\n        obj_c1 = ObjectC.objects.create(item=item1, name='Object C 1')\n        obj_c2 = ObjectC.objects.create(item=item2, name='Object C 2')\n\n        obj_c3 = ObjectC.objects.create(item=item1, name='Object C 3')\n\n        # Test Q object with related lookup (related field is null)\n        qs = ObjectC.objects.filter(Q(item__name='Item 1') & Q(item__objectb__isnull=True))\n        self.assertSequenceEqual(qs, [obj_c1, obj_c3])\n\n        # Test Q object with related lookup\n        qs = ObjectC.objects.filter(Q(item__name='Item 1') | Q(item__objectb=None))\n        self.assertSequenceEqual(qs, [obj_c1, obj_c3])\n\n", "    def test_select_related_with_extra(self):\n        author = Author.objects.create(name='John Doe')\n        author.item_set.create(name='Item 1')\n\n        qs = Item.objects.filter(author=author).select_related('author').extra(\n            select={'author_name': \"UPPER(author__name)\"}\n        )\n        self.assertQuerysetEqual(\n            qs, [\n                {'id': 1, 'name': 'Item 1', 'author': author, 'author_name': 'JOHN DOE'}\n            ],\n            transform=lambda obj: {\n                'id': obj.id,\n                'name': obj.name,\n                'author': obj.author,\n                'author_name': obj['author_name'],\n            },\n        )\n", "    def test_promote_joins(self):\n        item1 = Item.objects.create(name='Item 1')\n        item2 = Item.objects.create(name='Item 2')\n        author1 = Author.objects.create(name='Author 1', items=[item1])\n        author2 = Author.objects.create(name='Author 2', items=[item2])\n\n        query = Item.objects.filter(author__name__startswith='Author 1')\n        query._is_filtered = True\n        query._promote_joins({\n            'author': 1  # Promote join for 'author' table.\n        })\n\n        # Check if join type is INNER.\n        self.assertEqual(query.alias_map['author'].join_type, 'INNER')\n\n", "    def test_join_promotion(self):\n        author1 = Author.objects.create(name='Author 1')\n        author2 = Author.objects.create(name='Author 2')\n        item1 = Item.objects.create(name='Item 1', author=author1)\n        item2 = Item.objects.create(name='Item 2', author=author2)\n\n        query = Item.objects.filter(Q(name='Item 1') | Q(author__name='Author 2'))\n\n        self.assertQuerysetEqual(\n            query,\n            [item1, item2],\n            transform=lambda obj: obj.name\n        )\n\n"], "sample_624": ["    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n", "    def test_diff_array_repr_with_indexes(self):\n        data = np.arange(100).reshape(10, 10)\n        idx = pd.Index(np.arange(10), name=\"foo\")\n        a = xr.DataArray(data, coords={\"x\": idx, \"y\": idx}, dims=[\"x\", \"y\"])\n        b = xr.DataArray(data[1:, 1:], coords={\"x\": idx[1:], \"y\": idx[1:]}, dims=[\"x\", \"y\"])\n\n        expected = dedent(\n            \"\"\"\n            Left and right DataArray objects are not close\n            Differing dimensions:\n                (['y', 'x']) != (['y', 'x'])\n            Differing values:\n            L\n                [[ 1  2  3  4  5  6  7  8  9 10]\n                 [11 12 13 14 15 16 17 18 19 20]\n                 [21 22 23 24 25 26 27 28 29 30]\n                 [31 32 33 34 35 36 37 38 39 40]\n                 [41 42 43 44 45 46 47 48 49 50]\n                 [51 52 53 54 55 56 57 58 59 60]\n                 [61 62 63 64 65 66 67 68 69 70]\n                 [71 72 73 74 75 76 77 78 79 80]\n                 [81 82 83 84 85 86 87 88 89 90]\n                 [91 92 93 94 95 96 97 98 99 100]]\n            R\n                [[13 14 ", "    def test_inline_sparse_repr(self):\n        data = np.array([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n        sparse_array = sparse.COO(data)\n        array = xr.DataArray(sparse_array)\n        expected = \"<sparse.COO: nnz=5, fill_value=0>\"\n        assert formatting.inline_sparse_repr(array) == expected\n\n", "    def test_diff_variable_with_attrs(self):\n        a = xr.Variable([\"x\"], [1, 2, 3], attrs={\"foo\": \"bar\"})\n        b = xr.Variable([\"x\"], [1, 2, 3], attrs={\"foo\": \"baz\"})\n\n        expected = dedent(\n            \"\"\"\n            Left and right Variable objects are not identical\n            Differing Attributes:\n                foo: 'bar' != 'baz'\n            \"\"\"\n        )\n        assert formatting.diff_array_repr(a, b, \"identical\") == expected\n", "    def test_short_numpy_repr(self, data, expected):\n        assert formatting.short_numpy_repr(data) == expected\n\n"], "sample_70": ["    def test_delete_with_deferrable_uniqueness(self):\n        # Test deletion with deferrable uniqueness constraints. Databases\n        # with this feature can defer uniqueness constraint checks until the\n        # end of the transaction. This allows for temporary violations.\n\n        # Create a parent object.\n        parent = Parent.objects.create(name='Parent')\n\n        # Create child objects with the same name.\n        Child.objects.create(name='Child', parent=parent)\n        Child.objects.create(name='Child', parent=parent)\n\n        # Deleting the parent should succeed even though it\n        # violates the unique constraint on 'name'.\n        with transaction.atomic():\n            parent.delete()\n\n        # Assert that the parent is deleted.\n        self.assertEqual(Parent.objects.count(), 0)\n\n        # Assert that the child objects are also deleted.\n        self.assertEqual(Child.objects.count(), 0)\n", "    def test_deferrable_uniqueness(self):\n        parent = Parent.objects.create()\n\n        # Create some child objects with a deferrable uniqueness constraint.\n        child1 = Child.objects.create(parent=parent, data='1')\n        child2 = Child.objects.create(parent=parent, data='2')\n\n        # Change the data of child2 to the same as child1.\n        child2.data = '1'\n\n        with self.assertRaises(IntegrityError):\n            child2.save()\n\n        # Disable the uniqueness constraint and change the data again.\n        with connection.cursor() as cursor:\n            cursor.execute(\n                'ALTER TABLE \"child\" ADD CONSTRAINT child_parent_id_data_key UNIQUE DEFERRABLE INITIALLY IMMEDIATE'\n            )\n        child2.data = '1'\n        child2.save()\n\n        # Re-enable the uniqueness constraint.\n        with connection.cursor() as cursor:\n            cursor.execute(\n                'ALTER TABLE \"child\" ALTER CONSTRAINT child_parent_id_data_key DEFERRABLE INITFALSE'\n            )\n\n        # Trying to save child2 again should raise an IntegrityError.\n        with self.assertRaises(IntegrityError):\n            child2.save()\n\n", "    def test_protected_error_with_multiple_objects(self):\n        # Create two instances of M2MTo related to the same M2MFrom instance.\n        m2m_from = M2MFrom.objects.create()\n        m2m_to1 = M2MTo.objects.create(m2m_from=m2m_from)\n        m2m_to2 = M2MTo.objects.create(m2m_from=m2m_from)\n\n        # Attempt to delete the M2MFrom instance.\n        with self.assertRaises(ProtectedError) as e:\n            m2m_from.delete()\n\n        # Check that the ProtectedError raises correctly and includes both related M2MTo objects.\n        self.assertEqual(len(e.exception.protected_objects), 2)\n        self.assertIn(m2m_to1, e.exception.protected_objects)\n        self.assertIn(m2m_to2, e.exception.protected_objects)\n\n", "    def test_delete_deferred_uniqueness(self):\n        a1 = create_a(a_id=1)\n        a2 = create_a(a_id=2)\n        r1 = R(a=a1, r_id=1)\n        r2 = R(a=a2, r_id=2)\n        r1.save()\n        r2.save()\n\n        s1 = S(r=r1, s_id=1)\n        s2 = S(r=r2, s_id=2)\n        s1.save()\n        s2.save()\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"ALTER TABLE django_tests_s ADD CONSTRAINT unique_s UNIQUE (r_id, s_id)\"\n            )\n\n        with transaction.atomic():\n            r1.delete()\n            self.assertRaises(IntegrityError, s1.delete)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"ALTER TABLE django_tests_s DROP CONSTRAINT unique_s\"\n            )\n\n\n", "    def test_cascade_to_parent_with_keep_parents(self):\n        parent = Parent.objects.create()\n        child = Child.objects.create(parent=parent)\n        collector = Collector(self.using)\n        collector.collect([child], keep_parents=True)\n        collector.delete()\n        self.assertEqual(Parent.objects.count(), 1)\n        self.assertEqual(Child.objects.count(), 0)\n\n"], "sample_940": ["    def test_is_builtin_class_method(self):\n        assert inspect.is_builtin_class_method(int, '__init__')\n        assert not inspect.is_builtin_class_method(str, 'upper')\n", "    def test_is_builtin_class_method(self):\n        assert inspect.is_builtin_class_method(int, '__init__')\n", "    def test_signature_from_ast(self):\n        code = 'def func(a, b: int, *c, d=1, **e): pass'\n        module = ast.parse(code)\n        function = cast(ast.FunctionDef, module.body[0])  # type: ignore\n\n        sig = inspect.signature_from_ast(function, code)\n        self.assertEqual(sig.parameters['a'].kind, Parameter.POSITIONAL_OR_KEYWORD)\n        self.assertEqual(sig.parameters['b'].kind, Parameter.POSITIONAL_OR_KEYWORD)\n        self.assertEqual(sig.parameters['c'].kind, Parameter.VAR_POSITIONAL)\n        self.assertEqual(sig.parameters['d'].kind, Parameter.KEYWORD_ONLY)\n        self.assertEqual(sig.parameters['e'].kind, Parameter.VAR_KEYWORD)\n\n        self.assertEqual(sig.parameters['b'].annotation, 'int')\n", "def test_is_builtin_class_method():\n    assert inspect.is_builtin_class_method(int, '__init__')\n", "    def test_stringify_signature_with_forwardref(self):\n        # Test stringify_signature with ForwardRef annotations.\n\n                 **kwargs: 'ForwardRef(\"D\")'):\n            pass\n\n        sig = inspect.signature(func)\n        result = inspect.stringify_signature(sig)\n\n        expected = '(a: A, b: B, *args: C, **kwargs: D)'\n        assert result == expected\n"], "sample_887": ["    def test_calibrated_classifier_fit_predict(self, method, ensemble):\n        X, y = self.data\n\n        # Test with a simple classifier\n        if ensemble:\n            clf = RandomForestClassifier(random_state=42)\n        else:\n            clf = LogisticRegression(random_state=42)\n        calibrated_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n        calibrated_clf.fit(X, y)\n\n        y_pred = calibrated_clf.predict(X)\n        assert_array_equal(y_pred.shape, y.shape)\n\n        y_prob = calibrated_clf.predict_proba(X)\n        assert_array_equal(y_prob.shape, (N_SAMPLES, 2))\n\n        # Test predict_proba with a multiclass classifier\n        if ensemble:\n            clf = RandomForestClassifier(random_state=42, n_estimators=10)\n        else:\n            clf = LogisticRegression(random_state=42, multi_class='multinomial')\n        X, y = make_classification(\n            n_samples=N_SAMPLES,\n            n_features=6,\n            n_classes=3,\n            random_state=42,\n        )\n        calibrated_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n        calibrated_clf.fit(X, y)\n        y_prob = calibrated_clf.predict_proba(X)\n        assert_array_equal(y_prob.shape, (N_SAMPLES, 3))\n", "    def test_calibrated_classifier_predict_proba_with_sparse_input(self, data):\n        X, y = data\n        X_sparse = sparse.csr_matrix(X)\n        clf = LogisticRegression(random_state=0)\n        clf.fit(X, y)\n        calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv='prefit')\n        calibrated_clf.fit(X, y)\n        proba = calibrated_clf.predict_proba(X_sparse)\n        assert_allclose(proba.sum(axis=1), np.ones(N_SAMPLES))\n", "def test_calibration_curve_sparse_matrix(data):\n    X, y = data\n    X_sparse = sparse.csr_matrix(X)\n    clf = LogisticRegression()\n    clf.fit(X, y)\n    prob_pos = clf.predict_proba(X_sparse)[:, 1]\n    prob_true, prob_pred = calibration_curve(y, prob_pos, n_bins=10)\n    assert len(prob_true) == 10\n    assert len(prob_pred) == 10\n\n", "    def test_calibrated_classifier_predict_proba(self, method, ensemble):\n        X, y = self.data\n        n_classes = len(np.unique(y))\n\n        if ensemble and method == \"isotonic\":\n            pytest.skip(\"Isotonic calibration doesn't work with ensembles\")\n\n        clf = LogisticRegression(random_state=0)\n        if ensemble:\n            clf = VotingClassifier(estimators=[\n                (\"lr\", clf),\n                (\"dt\", DecisionTreeClassifier(random_state=0))])\n        clf.fit(X, y)\n\n        calibrated_clf = CalibratedClassifierCV(\n            clf, method=method, cv=\"prefit\"\n        )\n        calibrated_clf.fit(X, y)\n\n        # Check shape of output\n        probs = calibrated_clf.predict_proba(X)\n        assert probs.shape == (N_SAMPLES, n_classes)\n\n        # Check probabilities sum to 1\n        assert_allclose(probs.sum(axis=1), np.ones(N_SAMPLES))\n", "    def test_calibration_curve_with_sparse_data(self):\n        # Test calibration curve with sparse data\n        X, y = make_classification(n_samples=N_SAMPLES, n_features=6, random_state=42)\n        X_sparse = sparse.csr_matrix(X)\n        clf = LogisticRegression(random_state=42)\n        y_prob = clf.fit(X, y).predict_proba(X_sparse)[:, 1]\n        prob_true, prob_pred = calibration_curve(y, y_prob, n_bins=5)\n\n        assert isinstance(prob_true, np.ndarray)\n        assert isinstance(prob_pred, np.ndarray)\n        assert prob_true.shape[0] <= 5\n        assert prob_pred.shape[0] <= 5\n"], "sample_335": ["    def test_decimal_field_max_digits_and_decimal_places(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(field.clean('12.34'), decimal.Decimal('12.34'))\n        with self.assertRaises(ValidationError):\n            field.clean('123.45')  # More than 2 decimal places\n        with self.assertRaises(ValidationError):\n            field.clean('12345.67')  # More than 5 digits\n\n", "    def test_decimalfield_max_digits(self):\n        f = DecimalField(max_digits=3)\n        self.assertEqual(f.clean('123.456'), decimal.Decimal('123.456'))\n        with self.assertRaises(ValidationError):\n            f.clean('1234.56')\n        with self.assertRaises(ValidationError):\n            f.clean('12345')\n", "    def test_decimal_field_localized_thousands_separator(self):\n        with translation.override('de-de'):\n            field = DecimalField(max_digits=10, decimal_places=2)\n            self.assertEqual(field.clean('1.000,00'), decimal.Decimal('1000.00'))\n            self.assertEqual(field.widget.__class__, NumberInput)\n\n", "    def test_decimal_field_max_digits_and_decimal_places(self):\n        field = DecimalField(max_digits=5, decimal_places=2)\n        self.assertEqual(field.clean('123.45'), decimal.Decimal('123.45'))\n        self.assertRaises(ValidationError, field.clean, '12345.67')\n        self.assertRaises(ValidationError, field.clean, '123456')\n        self.assertRaises(ValidationError, field.clean, '12.345')\n", "    def test_decimal_field_localize(self):\n        with translation.override('fr'):\n            field = DecimalField(max_digits=4, decimal_places=2)\n            self.assertWidgetSubclass(field.widget, NumberInput)\n            value = '1234,56'\n            self.assertEqual(field.clean(value), decimal.Decimal('1234.56'))\n            self.assertEqual(field.to_python(value), decimal.Decimal('1234.56'))\n            with self.assertRaises(ValidationError):\n                field.clean('1234,567')\n"], "sample_368": ["    def test_migrate_empty_database_with_replacements(self):\n        \"\"\"\n        Tests that migrations with replacements are correctly handled when\n        migrated to an empty database.\n        \"\"\"\n        # Create an empty database.\n        self.create_test_database(syncdb=True)\n\n        # Load the migration graph.\n        loader = migrations.loader.MigrationLoader(self.connection)\n        graph = loader.graph\n\n        # Get the migration with replacements.\n        migrations_app = graph.nodes['migrations']\n        replacement_migration = next(migration for migration in migrations_app.values()\n                                    if migration.replaces)\n\n        # Get the target nodes.\n        target_nodes = [\n            (\n                replacement_migration.app_label,\n                replacement_migration.name\n            )\n        ]\n\n        # Create the migration executor.\n        executor = MigrationExecutor(connection=self.connection)\n\n        # Migrate to the target nodes.\n        executor.migrate(target_nodes)\n\n        # Check that the replaced migrations are marked as applied in the\n        # migration recorder.\n        recorder = MigrationRecorder(self.connection)\n        applied_migrations = recorder.applied_migrations()\n        self.assertIn((replacement_migration.app_label, replacement_migration.name), applied_migrations)\n\n        for app_label, name in replacement_migration.replaces:\n            self.assertIn((app_label, name), applied_migrations)\n", "    def test_migrate_unapplied_initial(self):\n        \"\"\"\n        Test that we correctly detect and apply initial migrations that are\n        implicitly applied (tables exist, but not recorded in django_migrations).\n        \"\"\"\n        with self.connection.cursor() as cursor:\n            with self.temporary_table(cursor, 'migrations2_article'):\n                # Ensure the Django migration history is empty\n                self.assertEqual(MigrationRecorder(self.connection).applied_migrations(), {})\n\n                # Force a migration to be not applied\n                recorder = MigrationRecorder(self.connection)\n                recorder.unapplied_migrations = {\n                    ('migrations2', '0001_initial'),\n                }\n\n                # Run migrations\n                executor = MigrationExecutor(self.connection)\n                executor.migrate(\n                    targets=[('migrations2', '0002_article')],\n                    fake_initial=True\n                )\n                \n                # Check the migration was applied\n                self.assertIn(('migrations2', '0001_initial'), recorder.applied_migrations())\n\n\n\n", "    def test_apply_migration_with_fake_initial(self):\n        \"\"\"\n        Test that apply_migration handles fake_initial correctly.\n        \"\"\"\n        with self.temporary_database():\n            # Create a migration with initial flag set to True\n            migration = migrations.Migration(\n                app='migrations2',\n                name='0001_initial',\n                operations=[\n                    migrations.CreateModel(\n                        name='InitialModel',\n                        fields=[\n                            ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ],\n                    )\n                ],\n            )\n            # Create a migration executor and apply the migration with fake_initial=True\n            executor = MigrationExecutor(self.connection)\n            state = executor.apply_migration(ProjectState(), migration, fake=True, fake_initial=True)\n            # Check if the state has the apps after applying the migration\n            self.assertIn('migrations2', state.apps)\n            # The migration recorder should not record the migration, as it's a fake initial\n            self.assertFalse(MigrationRecorder.objects.filter(app='migrations2', name='0001_initial').exists())\n", "    def test_migrate_unapplied_initial_migration_with_database_tables_present(self):\n        # This test should ensure that if a database table exists for the model\n        # from an initially-unapplied migration, the migration executor will\n        # correctly treat that migration as already applied, rather than\n        # attempting to create duplicate tables.\n        with self.connection.cursor() as cursor:\n           with connection.schema_editor() as editor:\n                # Create a table manually\n                editor.execute(\"CREATE TABLE migrations2_bogusmodel (id INT PRIMARY KEY)\")\n        # Re-run migrations to ensure we don't see errors\n        self.migrate_apps([\"migrations2\"])\n        # Reset the database to a clean state\n        self.migrate_apps([\"migrations2\"], target='zero')\n", "    def test_migrate_forwards_with_replacements(self):\n        with self.database() as connection:\n            with connection.cursor() as cursor:\n                cursor.execute(\"DROP TABLE IF EXISTS migrations_test_model\")\n            self.migrate([\n                (\"migrations2\", \"0001_initial\"),\n            ])\n            # Replace migration 0002 with 0003\n            self.migrate([\n                (\"migrations2\", \"0003_replacement\"),\n            ], fake=True)\n            with self.assertNumQueries(0):\n                self.migrate([\n                    (\"migrations2\", \"0003_replacement\"),\n                ])\n"], "sample_361": ["    def test_urlize_nofollow(self):\n        self.check_output(\n            lambda: urlize('http://example.com', nofollow=True),\n            '<a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a>'\n        )\n        self.check_output(\n            lambda: urlize('www.djangoproject.com', nofollow=True),\n            '<a href=\"http://www.djangoproject.com\" rel=\"nofollow\">www.djangoproject.com</a>'\n        )\n", "    def test_urlize(self):\n        self.check_output(\n            urlize,\n            'This is a test with a http://www.example.com link.',\n            'This is a test with a <a href=\"http://www.example.com\">http://www.example.com</a> link.',\n        )\n        self.check_output(\n            urlize, 'https://example.com', '<a href=\"https://example.com\">https://example.com</a>'\n        )\n        self.check_output(\n            urlize, 'This is a test with www.example.com link.',\n            'This is a test with a <a href=\"http://www.example.com\">www.example.com</a> link.',\n        )\n        self.check_output(urlize, 'test@example.com', '<a href=\"mailto:test@example.com\">test@example.com</a>')\n\n        self.check_output(\n            urlize,\n            'This is a test with an example(www.example.com) link.',\n            'This is a test with an example(<a href=\"http://www.example.com\">www.example.com</a>) link.',\n        )\n\n        self.check_output(\n            urlize,\n            'This is a test with a link. http://www.example.com/path.html',\n            'This is a test with a link. <a href=\"http://www.example.com/path.html\">http://www.example.com/path.html</a>',\n        )\n\n", "    def test_urlize_mailto(self):\n        self.assertEqual(\n            urlize('Send mail to me: johndoe@example.com'),\n            '<a href=\"mailto:johndoe@example.com\">johndoe@example.com</a>'\n        )\n        self.assertEqual(\n            urlize('Send mail to me: johndoe@example.com?subject=Hello'),\n            '<a href=\"mailto:johndoe@example.com?subject=Hello\">johndoe@example.com?subject=Hello</a>'\n        )\n", "    def test_smart_urlquote(self):\n        self.assertEqual(smart_urlquote(\"https://example.com\"), \"https://example.com\")\n        self.assertEqual(smart_urlquote(\"http://example.com\"), \"http://example.com\")\n        self.assertEqual(smart_urlquote(\"www.example.com\"), \"http://www.example.com\")\n        self.assertEqual(smart_urlquote(\"example.com\"), \"http://example.com\")\n        self.assertEqual(smart_urlquote(\"example.com/?query=value\"), \"http://example.com/?query=value\")\n        self.assertEqual(smart_urlquote(\"example.com#fragment\"), \"http://example.com#fragment\")\n        self.assertEqual(smart_urlquote(\"ftp://ftp.example.com/\"), \"ftp://ftp.example.com/\")\n", "    def test_strip_tags_multiple_times(self):\n        self.check_output(strip_tags, '<p>This is some <strong>text</strong></p>')\n        self.check_output(strip_tags, '<p>This is some <strong>text</strong></p>', 'This is some text')\n        self.check_output(strip_tags, '<p>This is some <strong>text</strong></p>', 'This is some text') \n\n"], "sample_113": ["    def test_parse_docstring(self):\n        docstring = \"\"\"\n        This is a docstring.\n\n        It has some metadata:\n\n        :meta key: value\n        :meta another: other\n\n        And some more text.\n        \"\"\"\n        title, body, metadata = utils.parse_docstring(docstring)\n        self.assertEqual(title, \"This is a docstring.\")\n        self.assertEqual(body, \"And some more text.\")\n        self.assertEqual(metadata, {'key': 'value', 'another': 'other'})\n", "    def test_replace_named_groups(self):\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\w+)/b/(\\w+)$'), '^<a>/b/(\\w+)$')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), '^<a>/b/<c>/$')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\w+)/b/(\\w+)'), '^<a>/b/(\\w+)')\n        self.assertEqual(utils.replace_named_groups('^(?P<a>\\w+)/b/(?P<c>\\w+)'), '^<a>/b/<c>')\n\n", "    def test_replace_unnamed_groups(self):\n        test_cases = [\n            (r'^(\\w+)/b/(\\w+)$', r'^<var>/b/<var>$'),\n            (r'^(\\w+)/b/((x|y)\\w+)$', r'^<var>/b/<var>$'),\n            (r'^(\\w+)/b/(?:(\\w+))$', r'^<var>/b/<var>$'),\n            (r'^(?P<a>\\w+)/b/(\\w+)$', r'^(?P<a>\\w+)/b/<var>$'),\n            (r'^(?P<a>\\w+)/b/((x|y)\\w+)$', r'^(?P<a>\\w+)/b/<var>$'),\n\n        ]\n        for pattern, expected_pattern in test_cases:\n            self.assertEqual(utils.replace_unnamed_groups(pattern), expected_pattern)\n", "    def test_replace_named_and_unnamed_groups(self):\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^<a>/b/(\\w+)$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)/$'), r'^<a>/b/<c>/$')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^<a>/b/(\\w+)')\n        self.assertEqual(utils.replace_named_groups(r'^(?P<a>\\w+)/b/(?P<c>\\w+)'), r'^<a>/b/<c>')\n        self.assertEqual(utils.replace_unnamed_groups(r'^(?P<a>\\w+)/b/(\\w+)$'), r'^(?P<a>\\w+)/b/<var>$')\n        self.assertEqual(utils.replace_unnamed_groups(r'^(?P<a>\\w+)/b/((x|y)\\w+)$'), r'^(?P<a>\\w+)/b/<var>$')\n        self.assertEqual(utils.replace_unnamed_groups(r'^(?P<a>\\w+)/b/(\\w+)'), r'^(?P<a>\\w+)/b/<var>')\n        self.assertEqual(utils.replace_unnamed_groups(r'^(?P<a>\\w+)/b/((x|y)\\w+)'), r'^(?P<a>\\w+)/b/<var>')\n", "    def test_replace_unnamed_groups(self):\n        self.assertEqual(utils.replace_unnamed_groups('^(\\w+)/b/(\\w+)$'), '^<var>/b/<var>$')\n        self.assertEqual(utils.replace_unnamed_groups('^(\\w+)/b/(x|y)\\w+$'), '^<var>/b/<var>$')\n        self.assertEqual(utils.replace_unnamed_groups('^(\\w+)/b/(\\w+)'), '^<var>/b/<var>')\n        self.assertEqual(utils.replace_unnamed_groups('^(x|y)\\w+/b/(a|b)\\w+$'), '^<var>/b/<var>$')\n        self.assertEqual(utils.replace_unnamed_groups('^((x|y)\\w+)/b/(\\w+)'), '^<var>/b/<var>')\n"], "sample_1193": ["def test_convex_hull_empty():\n    assert convex_hull() is None\n    assert convex_hull(Point(0,0)) == Point(0,0)\n", "    def test_farthest_points_line():\n        p1, p2 = Point(0, 0), Point(2, 0)\n        line = Line(p1, p2)\n        p3 = Point(0, 1)\n        assert farthest_points(p1, p2, p3) == {(p1, p2)}\n", "def test_idiff_implicit():\n    x, y = map(Symbol, 'xy')\n    eq = x**2 + y**2 - 4\n\n    # Test case for implicit differentiation\n    dydx = idiff(eq, y, x)\n    assert dydx == -x/y\n\n    # Test case for higher-order implicit differentiation\n    d2ydx2 = idiff(eq, y, x, 2)\n    assert d2ydx2.simplify() == (-x**2 - y**2)/y**3\n\n    # Test case with a non-x-dependent parameter\n    a = Symbol('a')\n    eq = x + a + y\n    dydx = idiff(eq, y, x)\n    assert dydx == -1\n\n    # Test case with a x-dependent parameter\n    eq = x + a + y\n    dydx = idiff(eq, [y, a], x)\n    assert dydx == -Derivative(a, x) - 1\n\n", "def test_are_coplanar_mixed():\n    p1 = Point(1, 2, 3)\n    p2 = Point(4, 5, 6)\n    p3 = Point(7, 8, 9)\n    l = Line3D(p1, p2)\n    assert are_coplanar(p1, p2, p3, l)\n", "def test_closest_points_repeated():\n    p = Point2D(0, 0)\n    q = Point2D(1, 1)\n    r = Point2D(0, 0)\n    assert closest_points(p, q, r) == {(p,q)} \n"], "sample_676": ["def test_plugin_nameversions(option):\n    plugininfo = [\n        (\"testplugin\", DistInfo(project_name=\"testplugin\", version=\"1.0.0\")),\n        (\"anotherplugin\", DistInfo(project_name=\"anotherplugin\", version=\"2.0.0\")),\n        (\"pytest-testplugin\", DistInfo(project_name=\"pytest-testplugin\", version=\"3.0.0\")),\n    ]\n    expected = [\"testplugin-1.0.0\", \"anotherplugin-2.0.0\", \"testplugin-3.0.0\"]\n    result = _plugin_nameversions(plugininfo)\n    assert result == expected\n", "    def test_plugin_nameversions(self, option):\n        plugininfo = [\n            (\"pytest-xdist\", DistInfo(project_name=\"pytest-xdist\", version=\"2.5.0\")),\n            (\"pytest-cov\", DistInfo(project_name=\"pytest-cov\", version=\"3.0.0\")),\n        ]\n        expected = [\"xdist-2.5.0\", \"cov-3.0.0\"]\n        result = _plugin_nameversions(plugininfo)\n        assert result == expected\n", "    def test_plugin_nameversions(self, option):\n        plugininfo = [\n            (\n                \"pytest-xdist\",\n                DistInfo(project_name=\"pytest-xdist\", version=\"1.0\"),\n            ),\n            (\n                \"pytest-cov\",\n                DistInfo(project_name=\"pytest-cov\", version=\"2.0\"),\n            ),\n        ]\n        expected = [\"pytest-cov-2.0\", \"xdist-1.0\"]\n        assert _plugin_nameversions(plugininfo) == expected\n", "    def test_plugin_nameversions(self, option):\n        plugininfo = pluggy.PluginManager(\"pytest\").list_plugin_distinfo()\n        expected = _plugin_nameversions(plugininfo)\n        result = TerminalReporter._plugin_nameversions(plugininfo)\n        assert result == expected\n", "    def test_plugin_nameversions(self, option):\n        plugininfo = [\n            (\n                \"pytest-myplugin\",\n                DistInfo(\n                    project_name= \"pytest-myplugin\",\n                    version=\"1.0.0\",\n                ),\n            ),\n            (\"pytest-anotherplugin\", DistInfo(project_name=\"pytest-anotherplugin\", version=\"2.0.1\")),\n        ]\n        expected = [\"myplugin-1.0.0\", \"anotherplugin-2.0.1\"]\n        result = _plugin_nameversions(plugininfo)\n        assert result == expected\n"], "sample_835": ["    def test_adaboost_regressor_sparse_input(self):\n        # Test AdaBoostRegressor with sparse input\n        X, y = boston.data[:50], boston.target[:50]\n        X_sparse = csc_matrix(X)\n\n        # Test with the default base estimator (DecisionTreeRegressor)\n        regressor = AdaBoostRegressor(random_state=0)\n        regressor.fit(X_sparse, y)\n        y_pred = regressor.predict(X_sparse)\n\n", "def test_ada_boost_regressor_sparse_input():\n    # Test AdaBoostRegressor with sparse input\n\n    X_sparse = csc_matrix(boston.data)\n    y = boston.target\n\n    regressor = AdaBoostRegressor(random_state=rng)\n    regressor.fit(X_sparse, y)\n    y_pred = regressor.predict(X_sparse)\n\n    assert_array_almost_equal(y_pred, regressor.predict(X_sparse.toarray()),\n                             decimal=2)\n", "    def test_sparse_matrix_input(self):\n        # Test with sparse matrices\n\n        # Classification\n        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1))\n        clf.fit(csc_matrix(X), y_class)\n        assert_array_equal(clf.predict(csc_matrix(T)), y_t_class)\n        clf.fit(csr_matrix(X), y_class)\n        assert_array_equal(clf.predict(csr_matrix(T)), y_t_class)\n        clf.fit(coo_matrix(X), y_class)\n        assert_array_equal(clf.predict(coo_matrix(T)), y_t_class)\n        clf.fit(dok_matrix(X), y_class)\n        assert_array_equal(clf.predict(dok_matrix(T)), y_t_class)\n        clf.fit(lil_matrix(X), y_class)\n        assert_array_equal(clf.predict(lil_matrix(T)), y_t_class)\n\n        # Regression\n        reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1))\n        reg.fit(csc_matrix(X), y_regr)\n        assert_array_almost_equal(reg.predict(csc_matrix(T)),\n                                  y_t_regr, 0)\n        reg.fit(csr_matrix(X), y_regr)\n        assert_array_almost_equal(reg.predict(csr_matrix(T)),\n                                  y_t_regr, 0)\n        reg.fit(coo_matrix(X), y_regr)\n        assert_array_almost_equal(reg.predict(coo_matrix(T)),\n                                  y_t_regr, 0)\n        reg.fit(dok_matrix(X), y_regr)\n        assert_array_almost_equal(reg.predict(dok_matrix(T)),\n                                  y_t_regr, 0)\n        reg.fit(lil_matrix(X), y_regr)\n        assert_array_almost_equal(reg.predict(lil_matrix(T)),\n                                  y_t_regr", "    def test_ada_boost_regressor_sparse_input(self):\n        # Test AdaBoostRegressor with sparse input\n        sparse_X = csc_matrix(boston.data)\n        regressor = AdaBoostRegressor(random_state=rng)\n        regressor.fit(sparse_X, boston.target)\n        predictions = regressor.predict(sparse_X)\n        assert_array_almost_equal(predictions.shape, boston.target.shape)\n", "def test_predict_proba_sparse_format():\n    # Test predict_proba with different sparse matrix formats.\n    clf = AdaBoostClassifier(n_estimators=10, random_state=rng)\n    clf.fit(csc_matrix(X), y_class)\n    proba_csc = clf.predict_proba(csc_matrix(T))\n    clf.fit(csr_matrix(X), y_class)\n    proba_csr = clf.predict_proba(csr_matrix(T))\n    clf.fit(coo_matrix(X), y_class)\n    proba_coo = clf.predict_proba(coo_matrix(T))\n    clf.fit(dok_matrix(X), y_class)\n    proba_dok = clf.predict_proba(dok_matrix(T))\n    clf.fit(lil_matrix(X), y_class)\n    proba_lil = clf.predict_proba(lil_matrix(T))\n    assert_array_almost_equal(proba_csc, proba_csr)\n    assert_array_almost_equal(proba_csc, proba_coo)\n    assert_array_almost_equal(proba_csc, proba_dok)\n    assert_array_almost_equal(proba_csc, proba_lil)\n"], "sample_1010": ["def test_latex_lowergamma():\n    assert latex(lowergamma(z, a)) == '\\\\operatorname{lowergamma}\\\\left(z, a \\\\right)'\n", "def test_latex_transpose():\n    A = Matrix([[1, 2], [3, 4]])\n    assert latex(A.T) == r'\\begin{pmatrix}1 & 3 \\\\ 2 & 4 \\end{pmatrix}'\n", "def test_latex_unevaluated_expr():\n    x = Symbol('x')\n    f = Function('f')\n    ue = UnevaluatedExpr(f(x) + 1)\n    assert latex(ue) == '\\\\operatorname{uexpr}{\\left (f(x) + 1 \\\\right )}'\n", "def test_latex_SingularityFunction():\n    assert latex(SingularityFunction(x, 0)) == '\\\\left(x \\\\right)^{0}'\n    assert latex(SingularityFunction(x, 1)) == '\\\\frac{1}{x}'\n    assert latex(SingularityFunction(x, -1)) == 'x'\n\n", "def test_latex_ComplexRegion():\n    R = ComplexRegion(Interval(0, 1), Interval(0, 1))\n    assert latex(R) == \"\\\\text{ComplexRegion}\\\\left(\\\\left[0, 1\\\\right], \\\\left[0, 1\\\\right]\\\\right)\"\n"], "sample_532": ["def test_contour_set_cmap_update():\n    fig, ax = plt.subplots()\n    X, Y = np.meshgrid(np.arange(5), np.arange(5))\n    Z = np.sin(X) + np.cos(Y)\n\n    cs = ax.contourf(X, Y, Z)\n    fig.canvas.draw()\n\n    # Change colormap after the ContourSet has been created\n    cs.set_cmap('viridis')\n\n    fig.canvas.draw()\n\n", "    def test_contour_manual_levels_log_scale(self):\n        # Test manual levels with logarithmic scaling\n        x = np.linspace(1, 10, 100)\n        y = np.linspace(1, 10, 100)\n        X, Y = np.meshgrid(x, y)\n        Z = np.log(X * Y)\n\n        fig, ax = plt.subplots()\n        levels = [1, 2, 3]\n\n        ax.contour(X, Y, Z, levels=levels, norm=LogNorm())\n", "def test_contour_negative_linestyles():\n    x = np.arange(-5, 6)\n    y = x.reshape(-1, 1)\n    z = x * y\n    fig, ax = plt.subplots()\n\n    cs = ax.contour(z,\n                    levels=[-10, -5, 0, 5, 10],\n                    linestyles=['solid', 'dashed', 'dashdot', 'dotted',\n                                'solid'])\n    assert all(l.get_linestyle() == s for l, s in zip(cs.collections,\n                                                      ['solid', 'dashed',\n                                                       'dashdot', 'dotted',\n                                                       'solid']))\n\n    # Test monochrome negative linestyle handling\n    with rc_context({\n            'lines.linewidth': 2.0,\n            'contour.linestyles': None,\n            'contour.negative_linestyles': 'dashed'\n    }):\n        fig, ax = plt.subplots()\n        cs = ax.contour(z,\n                        levels=[-10, -5, 0, 5, 10],\n                        cmap='gray')\n        assert all(l.get_linestyle() == s for l, s in zip(cs.collections[:-1],\n                                                        ['dashed'] * 4))\n        assert cs.collections[-1].get_linestyle() == 'solid'\n\n\n\n", "    def test_contour_corner_mask(self):\n        x = np.arange(10)\n        y = np.arange(10)\n        z = np.ones((10, 10))\n        z[2:4, 2:4] = np.nan\n        fig, ax = plt.subplots()\n        ax.contour(x, y, z, corner_mask=True)\n", "    def test_locator(self):\n        x = np.arange(-5, 6, 0.1)\n        y = np.arange(-5, 6, 0.1)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig, ax = plt.subplots()\n        cs = ax.contour(X, Y, Z, locator=ticker.MultipleLocator(0.5))\n        ax.clabel(cs, inline=True, fontsize=8)\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n"], "sample_855": ["    def test_dummy_classifier_sparse_uniform_strategy():\n        X = np.array([[0], [0], [0]])\n        y = sp.csc_matrix([[0, 1], [1, 0], [0, 1]])\n        clf = DummyClassifier(strategy='uniform', random_state=0)\n\n        with warnings.catch_warnings(record=True) as w:\n            clf.fit(X, y)\n\n        assert len(w) == 1\n        assert \"A local copy of the target data has been converted to a numpy array. Predicting on sparse target data with the uniform strategy would not save memory and would be slower.\" in str(w[0].message)\n\n", "    def test_dummy_regressor_quantile_sparse_input():\n        # Test quantile strategy with sparse input\n        X = sp.csc_matrix([[0], [0], [0], [0]])\n        y = np.array([[1, 0],\n                      [2, 0],\n                      [1, 0],\n                      [1, 3]])\n        clf = DummyRegressor(strategy='quantile', quantile=0.5)\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n\n        # Calculate the median of each column of y\n        median_y = np.median(y, axis=0)\n        assert_array_almost_equal(y_pred, np.tile(median_y, (y.shape[0], 1)))\n", "    def test_dummy_regressor_quantile_sparse():\n        X = np.zeros((10, 1))\n        y = sp.csc_matrix(np.arange(10).reshape((-1, 1)))\n        reg = DummyRegressor(strategy='quantile', quantile=0.75)\n        reg.fit(X, y)\n        assert_array_equal(reg.predict(X),\n                           np.tile(np.array([_weighted_percentile(y[:, 0].toarray(),\n                           weights=None, percentile=75.0)]), (10, 1)))\n", "    def test_dummy_regressor_constant_sparse_input():\n        X, y = sp.csr_matrix([[1], [2], [3]]), np.array([1, 2, 3])\n        clf = DummyRegressor(strategy=\"constant\", constant=42)\n        clf.fit(X, y)\n        assert_array_equal(clf.predict(X), np.array([42, 42, 42]))\n\n", "def test_dummy_regressor_quantile_sparse():\n    # Test quantile strategy with sparse input in DummyRegressor\n\n    rng = np.random.RandomState(42)\n    X = sp.csr_matrix(rng.rand(10, 5))\n    y = rng.rand(10, 2)\n\n    for quantile in [0.25, 0.5, 0.75]:\n        regressor = DummyRegressor(strategy='quantile', quantile=quantile)\n        regressor.fit(X, y)\n        y_pred = regressor.predict(X)\n        statistic = _weighted_percentile(y, sample_weight=None,\n                                         percentile=quantile * 100)\n        _check_equality_regressor(statistic, y, y_pred, y, y_pred)\n"], "sample_1097": ["def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert ImmutableMatrix(B.blocks[0, 1]) == ImmutableMatrix([[1, 2, 3]])\n", "def test_block_inverse_with_symbols():\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', m, m)\n    Z = MatrixSymbol('Z', n, m)\n    B = BlockMatrix([[X, Z], [ZeroMatrix(m,n), Y]])\n\n    # Test inverse with symbolic entries\n    invB = block_collapse(Inverse(B))\n    assert isinstance(invB, BlockMatrix)\n    assert invB.blocks[0,0] == X.I\n    assert invB.blocks[0,1] == -X.I*Z*Y.I\n    assert invB.blocks[1,0] == ZeroMatrix(m,n)\n    assert invB.blocks[1,1] == Y.I\n", "def test_block_inverse_3x3():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    M = BlockMatrix([[X, Z], [ZeroMatrix(2, 2), Y]])\n    invM = Inverse(M)\n    assert isinstance(invM, BlockMatrix)\n    assert invM.blocks[0, 0] == X.I\n    assert invM.blocks[1, 1] == Y.I\n\n\n\n", "    def test_blockinverse_2x2_symbolic(self):\n        X = MatrixSymbol('X', n, n)\n        Y = MatrixSymbol('Y', n, n)\n        Z = MatrixSymbol('Z', n, n)\n        B = BlockMatrix([[X, Z], [ZeroMatrix(n, n), Y]])\n        BI = Inverse(B)\n        self.assertEqual(block_collapse(BI),\n                         BlockMatrix([[ (X.I), -X.I*Z*Y.I],\n                                      [ZeroMatrix(n, n), Y.I]]))\n", "def test_blockcut():\n    M = ImmutableMatrix(4, 4, range(16))\n    B = blockcut(M, (1, 3), (1, 3))\n    assert isinstance(B, BlockMatrix)\n    assert ImmutableMatrix(B.blocks[0, 1]) == Matrix([[1, 2, 3]])\n    assert ImmutableMatrix(B.blocks[1, 0]) == Matrix([[4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n"], "sample_470": ["    def test_lazystr(self):\n        called = [False]\n            called[0] = True\n            return 'lazy text'\n        lazy_text = lazystr(get_text)\n        self.assertFalse(called[0])\n        self.assertEqual(str(lazy_text), 'lazy text')\n        self.assertTrue(called[0])\n", "    def test_lazy_str(self):\n        lazy_str = lazystr(\"hello\")\n        self.assertIsInstance(lazy_str, str)\n        self.assertEqual(lazy_str, \"hello\")\n", "    def test_lazy_text(self):\n        lazy_func = lazy(str, str)\n        self.assertEqual(lazy_func('Hello'), 'Hello')\n", "    def test_lazystr(self):\n        lazy_string = lazystr(\"hello\")\n        self.assertTrue(isinstance(lazy_string, lazy))\n        self.assertEqual(str(lazy_string), \"hello\")\n", "    def test_lazystr(self):\n            return \"hello world\"\n\n        lazy_text = lazystr(get_text)\n        self.assertIsInstance(lazy_text, str)\n        self.assertEqual(lazy_text, \"hello world\")\n        with mock.patch('django.utils.functional.str') as mock_str:\n            lazy_text()\n            mock_str.assert_called_once_with(get_text())\n"], "sample_41": ["    def test_unit_decompose_to_dimensionless():\n        # Test that decompose() correctly handles dimensionless units\n        u1 = u.m * u.m / u.m\n        assert u1.is_unity()\n        assert list(u1.decompose().bases) == []\n        assert u1.decompose().scale == 1\n\n        u2 = u.m / u.m\n        assert u2.is_unity()\n        assert list(u2.decompose().bases) == []\n        assert u2.decompose().scale == 1\n\n        u3 = 1 * u.m / u.m\n        assert u3.is_unity()\n        assert list(u3.decompose().bases) == []\n        assert u3.decompose().scale == 1\n\n", "def test_unit_from_string_with_negative_exponent():\n    \"\"\"\n    Test that negative exponents are correctly parsed from strings\n    and applied\n    \"\"\"\n    assert u.Unit('m^-1') == u.Unit('1/m')\n\n", "    def test_unit_decompose():\n        x = u.m**2 / u.s\n        assert isinstance(x, u.CompositeUnit)\n        y = u.m**2 / u.s**2\n        assert isinstance(x, u.CompositeUnit)\n        z = x.decompose()\n        assert len(z.bases) == 2\n        assert isinstance(z.bases[0], u.meter)\n        assert isinstance(z.bases[1], u.second)\n\n", "    def test_unit_from_string():\n        assert u.Unit('km/s') == u.km/u.s\n", "    def test_unit_from_string_unicode_fraction():\n        with pytest.raises(ValueError) as exc:\n            u.Unit('1/2 m')\n        assert exc.value.args[0] == \"Could not parse unit: '1/2 m'\"\n\n"], "sample_247": ["    def test_annotate_with_subquery(self):\n        # Tests annotating with a subquery using a different model as the source\n        queryset = Book.objects.annotate(\n            avg_age_of_authors=Subquery(\n                Author.objects.filter(books=OuterRef('pk')).values('age').annotate(avg_age=Avg('age'))\n                .values('avg_age')\n            )\n        ).order_by('avg_age_of_authors')\n        self.assertQuerysetEqual(queryset, [\n            'b6', 'b5', 'b2', 'b1', 'b3', 'b4',\n        ], lambda b: b.isbn)\n", "    def test_annotate_with_coalesce(self):\n        # Test annotating with Coalesce\n            return Coalesce(author.friends.name, author.name)\n\n        queryset = Author.objects.annotate(\n            full_name=coalesce_name('friends')\n        )\n\n        self.assertEqual(\n            list(queryset.values_list('full_name', flat=True)),\n            [\n                'Jacob Kaplan-Moss', 'Adrian Holovaty', 'Brad Dayley',\n                'James Bennett', 'Jeffrey Forcier', 'Paul Bissex',\n                'Wesley J. Chun', 'Paul Bissex', 'Wesley J. Chun',\n                'Peter Norvig', 'Stuart Russell',\n            ]\n        )\n", "    def test_annotated_values(self):\n        books = Book.objects.annotate(\n            published_years_ago=Func(F('pubdate'), function='DATE_PART', template=\"'year'\"),\n        ).filter(published_years_ago__gte=5)\n\n        self.assertQuerysetEqual(books, [\n            '<Book: Practical Django Projects>',\n            '<Book: Python Web Development with Django>',\n            '<Book: Artificial Intelligence: A Modern Approach>',\n            '<Book: Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp>',\n\n        ], transform=lambda b: str(b))\n", "    def test_annotate_conditional_expression(self):\n        # Test that annotating with a conditional expression works as expected.\n        # Specifically, this tests whether a ConditionalExpression can be used\n        # as the argument to an aggregation function like Count.\n\n        # This query counts the number of books for each author, but only\n        # considers books with a rating greater than 4.\n        query = Book.objects.values('contact__name').annotate(\n            high_rated_book_count=Count(\n                Case(\n                    When(rating__gt=4, then=1),\n                    default=0,\n                    output_field=IntegerField(),\n                )\n            ),\n        )\n        self.assertQuerysetEqual(\n            query, [\n                {'contact__name': 'Adrian Holovaty', 'high_rated_book_count': 1},\n                {'contact__name': 'Brad Dayley', 'high_rated_book_count': 0},\n                {'contact__name': 'James Bennett', 'high_rated_book_count': 1},\n                {'contact__name': 'Jeffrey Forcier', 'high_rated_book_count': 1},\n                {'contact__name': 'Paul Bissex', 'high_rated_book_count': 0},\n                {'contact__name': 'Wesley J. Chun', 'high_rated_book_count': 0},\n                {'contact__name': 'Peter Norvig', 'high_rated_book_count': 1},\n                {'contact__name': 'Stuart Russell', 'high_rated_book_count': 0},\n                {'contact__name': 'Jacob Kaplan-Moss', 'high_rated_book_count': 0},\n            ],\n            transform=lambda x: {k: x[k] for k in ['contact__name', 'high_rated_book_count']},\n        )\n\n", "    def test_annotate_with_subquery_count(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM books\")\n            num_books = cursor.fetchone()[0]\n\n        result = Book.objects.annotate(\n            num_books=Subquery(Book.objects.all().values('id').count())\n        ).order_by('id')\n        self.assertEqual(result.count(), num_books)\n\n"], "sample_1016": ["    def test_octave_code_complex_matrix_multiply():\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        expr = A*B\n        expected = 'A*B'\n        assert octave_code(expr) == expected\n\n\n\n", "def test_octave_code_Piecewise_inline_False():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    code = octave_code(pw, assign_to='tau', inline=False)\n    assert code == 'if (x > 0)\\n  tau = x + 1;\\nelse\\n  tau = x;\\nend'\n", "    def test_octave_code_hankel_functions(self):\n        assert octave_code(hankel1(x, x)) == \"besselh(x, 1, x)\"\n        assert octave_code(hankel2(x, x)) == \"besselh(x, 2, x)\"\n", "def test_octave_code_Piecewise_inline_no_default():\n    pw = Piecewise((x, x > 0), (x + 1, True))\n    raises(ValueError, lambda: octave_code(pw, assign_to=y, inline=False))\n", "def test_octave_piecewise_no_default():\n    raises(ValueError, lambda: octave_code(Piecewise((x, x>0))))\n"], "sample_555": ["    def test_ConnectionPatch_coords(self):\n        fig, ax = plt.subplots()\n        con = ConnectionPatch((0.1, 0.1), (0.9, 0.9),\n                              coordsA=\"axes fraction\",\n                              coordsB=\"axes fraction\",\n                              axesA=ax, axesB=ax,\n                              arrowstyle=\"-\")\n        ax.add_patch(con)\n\n        fig, ax = plt.subplots()\n        con = ConnectionPatch((0.1, 0.1), (0.9, 0.9),\n                              coordsA=\"data\",\n                              coordsB=\"data\",\n                              axesA=ax, axesB=ax,\n                              arrowstyle=\"-\")\n        ax.add_patch(con)\n", "    def test_fancyarrowpatch_set_arrowstyle(self, style):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch((0, 0), (1, 1), arrowstyle=style)\n        ax.add_patch(arrow)\n\n        # Check if the arrowstyle is updated correctly\n        assert arrow.get_arrowstyle().name == style\n\n        arrow.set_arrowstyle(style + \"2 \")\n        assert arrow.get_arrowstyle().name == style + \"2 \"\n\n        # Test setting with a custom style\n        custom_style = BoxStyle(\"Round\", pad=0.5)\n        arrow.set_arrowstyle(custom_style)\n        assert arrow.get_arrowstyle() == custom_style\n\n", "    def test_fancyarrowpatch_coords(self):\n        fig, ax = plt.subplots()\n        # Test ConnectionPatch with different coordinate systems\n        con = FancyArrowPatch((0.2, 0.2), (0.8, 0.8),\n                              coordsA='axes fraction',\n                              coordsB='axes fraction',\n                              arrowstyle='->',\n                              connectionstyle='arc3')\n        ax.add_patch(con)\n        fig.canvas.draw()\n        # Now check if the connection starts and ends at the right place\n\n        # We need to convert the axes fraction coordinates to pixels first.\n        trans = ax.transData\n        start = trans.transform((0.2, 0.2))\n        end = trans.transform((0.8, 0.8))\n        assert_array_equal(con.get_path().vertices[0], start)\n        assert_array_equal(con.get_path().vertices[-1], end)\n\n", "    def test_fancyarrowpatch_mutation_scale(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch((0, 0), (1, 1),\n                                arrowstyle='simple', mutation_scale=20)\n        ax.add_patch(arrow)\n        fig.canvas.draw()\n        # This test asserts that the arrow head's width is scaled correctly.\n        # The exact value may change due to figure dpi, but it should be\n        # approximately 20 * default_head_width.\n        default_head_width = 10\n        head_width = arrow.get_path().vertices[-4, 0] - arrow.get_path().vertices[-5, 0]\n        assert_almost_equal(head_width, 20 * default_head_width, 1)\n\n", "    def test_fancyarrow(self):\n        fig, ax = plt.subplots()\n        ar = FancyArrowPatch(\n            (0.2, 0.2), (0.8, 0.8),\n            arrowstyle='-|>',\n            connectionstyle='arc3,rad=0.2',\n            mutation_scale=20,\n            linewidth=2,\n            color='red')\n        ax.add_patch(ar)\n\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n"], "sample_1066": ["    def test_print_vector_cross(self):\n        C = CoordSys3D('C')\n        v1 = C.i + 2*C.j\n        v2 = 3*C.j + C.k\n        cross = Cross(v1, v2)\n        self.assertEqual(mathml(cross),\n                         '<apply><cross/>'\n                         '<vector><ci>C_1</ci></vector>'\n                         '<vector><ci>C_2</ci><ci>C_2</ci><ci>C_3</ci></vector>'\n                         '</apply>')\n\n\n\n", "    def test_mathml_print_MatrixSymbol(self):\n        A = MatrixSymbol('A', 3, 3)\n        self.assertEqual(mp.doprint(A), '<ci>A</ci>')\n        self.assertEqual(mpp.doprint(A), '<mi>A</mi>')\n\n", "    def test_mathml_print_singularities(self):\n        f = SingularityFunction(x, 2)\n        assert mp.doprint(f) == \"<apply> <sin> <ci>x</ci> </sin> </apply>\"\n        assert mpp.doprint(f) == \"<mrow> <mi>sin</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> </mrow>\"\n", "def test_AccumBounds():\n    b = AccumBounds(n, (n, 1, 3))\n    assert mathml(b) == '<apply><accumulate/> <csymbol>n</csymbol><bind><csymbol>n</csymbol><interval><cn>1</cn><cn type=\"integer\">3</cn></interval></bind></apply>'\n    s = SymPy\n\n", "    def test_print_SingularityFunction(self):\n        s = SingularityFunction(x, 0, 1)\n        self.assertEqual(\n            mp.doprint(s),\n            '<apply><csymbol cd=\"sympy\">SingularityFunction</csymbol>'\n            '<ci>x</ci><cn>0</cn><cn>1</cn></apply>'\n        )\n\n"], "sample_805": ["    def test_mean_tweedie_deviance_p_negative(self):\n        y_true = [2, 0, 1, 4]\n        y_pred = [0.5, 0.5, 2., 2.]\n\n        # p < 0, requires y_pred > 0\n        with pytest.raises(ValueError):\n            mean_tweedie_deviance(y_true, [-1, 0.5, 2., 2.], p=-0.5)\n", "def test_mean_tweedie_deviance_p_negative():\n    y_true = np.array([1, 2, 3, 4])\n    y_pred = np.array([1.5, 1.8, 2.8, 4.2])\n    p = -0.5\n\n    # Should raise ValueError for negative p\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=p)\n", "def test_mean_tweedie_deviance_p0():\n    y_true = [2, 0, 1, 4]\n    y_pred = [0.5, 0.5, 2., 2.]\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=0),\n                    mean_squared_error(y_true, y_pred))\n\n", "def test_mean_tweedie_deviance_p_values():\n    # Test various p values for mean_tweedie_deviance\n    y_true = np.array([2, 0, 1, 4])\n    y_pred = np.array([0.5, 0.5, 2., 2.])\n\n    # Test p < 0\n    with pytest.raises(ValueError):\n        mean_tweedie_deviance(y_true, y_pred, p=-1)\n\n    # Test p = 0\n    expected_mse = mean_squared_error(y_true, y_pred)\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=0),\n                    expected_mse)\n\n    # Test p = 1\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=1),\n                    mean_poisson_deviance(y_true, y_pred))\n\n    # Test p = 2\n    assert_allclose(mean_tweedie_deviance(y_true, y_pred, p=2),\n                    mean_gamma_deviance(y_true, y_pred))\n\n    # Test other p values\n    for p in [1.5, 2.5]:\n        mean_tweedie_deviance(y_true, y_pred, p=p)\n", "    def test_mean_tweedie_deviance_p_values():\n        # Test Tweedie deviance with different p values\n        y_true = np.array([2, 0, 1, 4])\n        y_pred = np.array([0.5, 0.5, 2., 2.])\n        for p in [-1, 0, 1, 2, 3]:\n            loss_value = mean_tweedie_deviance(y_true, y_pred, p=p)\n            assert isinstance(loss_value, float) and loss_value >= 0\n\n"], "sample_742": ["    def test_logistic_regression_cv_solver_liblinear_multiclass_ovr(self):\n        # Test LogisticRegressionCV with 'liblinear' solver and 'ovr'\n        # multi_class strategy.\n\n        X, y = make_classification(n_samples=20, n_features=4,\n                                 n_informative=3, n_redundant=0,\n                                 n_classes=3, random_state=0)\n\n        # Use a small number of folds to make the test faster.\n        clf = LogisticRegressionCV(solver='liblinear', multi_class='ovr',\n                                   cv=StratifiedKFold(n_splits=2),\n                                   random_state=0)\n        clf.fit(X, y)\n\n        # Check that the model can predict\n        check_predictions(clf, X, y)\n", "def test_logistic_regression_cv_multiclass_ovr():\n    # Test LogisticRegressionCV with multi_class='ovr'\n    X, y = make_classification(n_samples=50, n_features=20,\n                               n_informative=10, n_classes=3, random_state=0)\n    clf = LogisticRegressionCV(cv=5, multi_class='ovr', solver='lbfgs',\n                               random_state=0).fit(X, y)\n    check_predictions(clf, X, y)\n    assert_equal(clf.coef_.shape, (3, 20))\n    assert_equal(clf.intercept_.shape, (3,))\n", "    def test_logistic_regression_cv_multiclass(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                  n_informative=5, n_classes=3,\n                                  random_state=0)\n\n        # Test with liblinear solver\n        clf = LogisticRegressionCV(solver='liblinear',\n                                   multi_class='multinomial',\n                                   cv=StratifiedKFold(5),\n                                   random_state=0)\n        clf.fit(X, y)\n        assert_equal(clf.coef_.shape, (3, 10))\n        assert_equal(clf.intercept_.shape, (3,))\n\n        # Test with saga solver\n\n        clf = LogisticRegressionCV(solver='saga',\n                                   multi_class='multinomial',\n                                   cv=StratifiedKFold(5),\n                                   random_state=0)\n        clf.fit(X, y)\n        assert_equal(clf.coef_.shape, (3, 10))\n        assert_equal(clf.intercept_.shape, (3,))\n", "def test_logistic_regression_cv_multiclass_ovr_with_sample_weights():\n    # Test that LogisticRegressionCV works correctly with sample_weights\n    # and multiclass classification using 'ovr'\n\n    X, y = make_classification(n_samples=100, n_features=20,\n                               n_informative=10, n_classes=3,\n                               random_state=0)\n    sample_weight = np.random.rand(len(y))\n\n    clf = LogisticRegressionCV(multi_class='ovr', solver='lbfgs',\n                               scoring='accuracy', max_iter=1000,\n                               random_state=0)\n\n    # Fit the model with sample weights\n    clf.fit(X, y, sample_weight=sample_weight)\n\n    # Get the predicted probabilities\n    probs = clf.predict_proba(X)\n\n    # Check that the probabilities sum to 1 for each sample\n    assert_allclose(probs.sum(axis=1), np.ones(len(y)))\n\n    # Check that the predicted labels match the shape of the true labels\n    assert_equal(clf.predict(X).shape, y.shape)\n\n    # Check that the best C value is not None\n    assert_not_equal(clf.C_, None)\n", "    def test_logistic_regression_cv_multi_class(self):\n        # Test LogisticRegressionCV with multinomial option for multiclass\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_classes=3,\n                                   random_state=0)\n\n        clf = LogisticRegressionCV(cv=StratifiedKFold(5),\n                                   multi_class='multinomial',\n                                   solver='lbfgs', random_state=0,\n                                   max_iter=1000)\n        clf.fit(X, y)\n\n        # check if the predictions are consistent with the training labels\n        assert_array_equal(clf.predict(X), y)\n        assert_equal(clf.coef_.shape, (3, 20))\n        assert_equal(clf.intercept_.shape, (3,))\n\n\n"], "sample_50": ["    def test_runshell_with_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'user': 'myuser',\n            'password': 'mypassword',\n            'database': 'mydb',\n        }\n\n        args, pgpassword = self._run_it(dbinfo)\n\n        self.assertEqual(args, ['psql', '-U', 'myuser', '-h', 'localhost', '-p', '5432', 'mydb'])\n        self.assertEqual(pgpassword, 'mypassword')\n", "    def test_runshell_db_with_all_params(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'user': 'testuser',\n            'password': 'testpassword',\n            'database': 'testdb',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        expected_args = ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432',\n                         'testdb']\n        self.assertEqual(args, expected_args)\n        self.assertEqual(pgpassword, 'testpassword')\n", "    def test_runshell_with_password(self):\n        dbinfo = {\n            'host': 'localhost',\n            'port': 5432,\n            'user': 'testuser',\n            'password': 'testpassword',\n            'database': 'testdb',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'testuser', '-h', 'localhost', '-p', '5432', 'testdb'])\n        self.assertEqual(pgpassword, 'testpassword')\n", "    def test_runshell_db_with_password(self):\n        dbinfo = {\n            'user': 'test_user',\n            'host': 'test_host',\n            'port': 5432,\n            'database': 'test_db',\n            'password': 'test_password',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'test_user', '-h', 'test_host', '-p', '5432', 'test_db'])\n        self.assertEqual(pgpassword, 'test_password')\n", "    def test_runshell_db_with_all_parameters(self):\n        dbinfo = {\n            'host': 'test_host',\n            'port': 5432,\n            'database': 'test_db',\n            'user': 'test_user',\n            'password': 'test_password',\n        }\n        args, pgpassword = self._run_it(dbinfo)\n        self.assertEqual(args, ['psql', '-U', 'test_user', '-h', 'test_host', '-p', '5432', 'test_db'])\n        self.assertEqual(pgpassword, 'test_password')\n"], "sample_130": ["    def test_join_promotion_negated_or(self):\n        q = Query(model=Author)\n        q.add_filter(Q(item__name__icontains='Foo') | Q(rankings__rank=1))\n        q.set_select([F('id')])\n        promoter = JoinPromoter(OR, 2, True)\n        promoter.add_votes(['item', 'rankings'])\n        promoted_tables = promoter.update_join_types(q)\n        self.assertEqual(promoted_tables, {'item', 'rankings'})\n", "    def test_join_promotion_negated_or(self):\n        query = Query(model=Author)\n        query.add_filter(\n            Q(books__title__startswith='Django') | Q(books__title__startswith='Python')\n        )\n        query.promote_joins({\n            'books': 1\n        })\n        self.assertEqual(query.alias_refcount['books'], 2)\n        self.assertEqual(query.alias_map['books'].join_type, 'OUTER')\n", "    def test_join_promotion_with_related_null_lookups(self):\n        # Test join promotion when dealing with related null lookups\n\n        author = Author.objects.create(name='Test Author')\n        Item.objects.create(name='Test Item', author=author)\n\n        # Query with a related null lookup and an additional filter\n        qs = Item.objects.filter(author__isnull=False, name='Test Item')\n        self.assertEqual(qs.count(), 1)\n        query = qs.query\n\n", "    def test_join_promotion_with_nested_ors(self):\n        author = Author.objects.create(name='John Doe')\n        Item.objects.create(author=author, name='Book A')\n        Item.objects.create(author=author, name='Book B')\n\n        # Test nested OR conditions with inner joins\n        qs = Item.objects.filter(\n            Q(author__name='John Doe') | Q(name__startswith='Book')\n        ).filter(\n            Q(author__name='John Doe') | Q(name__startswith='Book')\n        )\n\n        # The inner OR condition should promote all joins to INNER JOIN\n\n        query = qs.query\n\n        self.assertEqual(len(query.alias_map), 2)\n        self.assertIn('author', query.alias_map)\n        self.assertEqual(query.alias_map['author'].join_type, 'INNER')\n\n", "    def test_join_promotion(self):\n        a1 = Author.objects.create(name='Adrian')\n        a2 = Author.objects.create(name='Barbara')\n        item1 = Item.objects.create(author=a1, name='Item 1')\n        item2 = Item.objects.create(author=a2, name='Item 2')\n        item3 = Item.objects.create(author=a1, name='Item 3')\n\n        # Test case 1: Single condition, should promote to INNER\n        results = Item.objects.filter(author__name='Adrian')\n        self.assertCountEqual(results, [item1, item3])\n\n        # Test case 2: OR clause, should promote to LOUTER\n        results = Item.objects.filter(Q(author__name='Adrian') | Q(author__name='Barbara'))\n        self.assertCountEqual(results, [item1, item2, item3])\n\n        # Test case 3: AND clause with different joins, should demote to INNER\n        results = Item.objects.filter(Q(author__name='Adrian') & Q(name='Item 1'))\n        self.assertCountEqual(results, [item1])\n\n"], "sample_1142": ["def test_matrixelement_diff():\n    i, j = symbols('i j', integer=True)\n    X = MatrixSymbol('X', 3, 3)\n    me = MatrixElement(X, i, j)\n    assert diff(me, X[1, 1]) == KroneckerDelta(i, 1)*KroneckerDelta(j, 1)\n", "def test_matrix_derivative_add():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    expr = A + B\n    assert expr.diff(A) == Identity(2) \n    assert expr.diff(B) == Identity(2)\n\n", "    def test_matrixelement_derivative():\n        i, j = 1, 2\n        M = MatrixSymbol('M', 3, 3)\n        assert M[i, j].diff(M[i, j]) == 1\n        assert M[i, j].diff(M[j, i]) == 0\n", "def test_matrix_derivative_symbol_trace():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = Trace(X*Y)\n    assert expr.diff(X) == Y.T\n\n", "def test_matrix_element_doit_with_matrix_symbol():\n    M = MatrixSymbol('M', 3, 3)\n    i, j = symbols('i j', integer=True)\n    expr = MatrixElement(M, i, j)\n    assert expr.doit() == M[i, j]\n\n"], "sample_127": ["    def test_bulk_create_with_existing_pk(self):\n        Country.objects.bulk_create(self.data)\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(self.data)\n", "    def test_bulk_create_with_related_fields(self):\n        Restaurant.objects.bulk_create([\n            Restaurant(name=\"Pizzeria Uno\", country=self.data[0]),\n            Restaurant(name=\"Pizzeria Due\", country=self.data[1]),\n        ])\n        self.assertEqual(Restaurant.objects.count(), 2)\n        self.assertEqual(Country.objects.count(), 4)\n\n", "    def test_bulk_create_with_related_objects(self):\n        pizzerias = [\n            Pizzeria(name='Pizzeria A', country=self.data[0]),\n            Pizzeria(name='Pizzeria B', country=self.data[1]),\n            Pizzeria(name='Pizzeria C', country=self.data[2]),\n        ]\n        with atomic():\n            Pizzeria.objects.bulk_create(pizzerias)\n            self.assertEqual(Pizzeria.objects.count(), 3)\n            self.assertEqual(Country.objects.count(), 3)\n", "    def test_bulk_create_with_custom_pk(self):\n        \"\"\"\n        Test bulk_create when using a custom primary key field.\n        \"\"\"\n        Restaurant.objects.bulk_create([\n            Restaurant(name=\"Pizza Hut\", owner=self.data[0]),\n            Restaurant(name=\"Dominos\", owner=self.data[1]),\n        ])\n", "    def test_bulk_create_with_related_objects(self):\n        pizzeria1 = Pizzeria(name='Pizza Palace', country=self.data[0])\n        pizzeria2 = Pizzeria(name='Pizza Hut', country=self.data[1])\n        pizzeria1.restaurants.create(name='Pizza Palace Downtown', state=self.data[0].states.create(name='California'))\n        pizzeria2.restaurants.create(name='Pizza Hut Amsterdam', state=self.data[1].states.create(name='North Holland'))\n        with transaction.atomic():\n            Country.objects.bulk_create(self.data)\n            Pizzeria.objects.bulk_create([pizzeria1, pizzeria2])\n\n"], "sample_1086": ["def test_printmethod():\n    class MyClass(Expr):\n            self.arg = arg\n\n            return f\"MyClass({self.arg})\"\n\n    a = MyClass(x)\n    assert sstr(a) == \"MyClass(x)\"\n", "def test_subs():\n    e = 2*x + 3*y\n\n    # Direct positional substitution\n    assert sstr(Subs(e, x, 1)) == '2 + 3*y'\n    assert sstr(Subs(e, y, 2)) == '2*x + 6'\n    assert sstr(Subs(e, x, 1, y, 2)) == '2 + 6'\n\n    # Substitution using a dictionary\n    assert sstr(Subs(e, {x:1, y:2})) == '2 + 6'\n\n    # Substitution with a function\n    f = Lambda(x, x**2)\n    assert sstr(Subs(e, x, f(y))) == '2*y**2 + 3*y'\n\n    # Test with nested subs\n    assert sstr(Subs(Subs(e, x, y), y, 2)) == '4 + 6'\n\n    # Test with non-commutative symbols\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    e = A*B\n    assert sstr(Subs(e, A, B)) == 'B*B'\n", "def test_printmethod():\n    class Foo(Expr):\n            self.arg = arg\n            return \"Foo(%s)\" % printer.doprint(self.arg)\n\n    f = Foo(x)\n    assert sstr(f) == \"Foo(x)\"\n\n", "def test_printmethod():\n    class TestClass(Expr):\n            return \"TestClass()\"\n\n    assert sstr(TestClass()) == \"TestClass()\"\n", "def test_printmethod():\n    class Foo(Expr):\n            self.arg = arg\n\n            return 'Foo(%s)' % self.arg\n\n    f = Foo(x)\n    assert sstr(f) == 'Foo(x)'\n\n"], "sample_692": ["    def test_getbasetemp_from_env(\n        self, monkeypatch: MonkeyPatch, tmpdir: Path,", "    def test_getbasetemp_from_env(self, tmpdir: Path) -> None:\n        os.environ[\"PYTEST_DEBUG_TEMPROOT\"] = str(tmpdir)\n        factory = TempPathFactory.from_config(self.config, _ispytest=True)\n        assert factory.getbasetemp().as_posix() == tmpdir.as_posix()\n", "    def test_getbasetemp_user_with_illegal_characters(self, tmpdir_factory: TempdirFactory) -> None:\n        with MonkeyPatch.context() as mp:\n            mp.setattr(os, \"getuser\", lambda: \"user/name with illegal characters\")\n\n            basetemp = tmpdir_factory._tmppath_factory.getbasetemp()\n            assert basetemp.name.startswith(\"pytest-of-unknown\")\n", "    def test_tmpdir_factory_getbasetemp_user_from_getuser(self, tmpdir, monkeypatch):\n        monkeypatch.setattr(getpass, \"getuser\", lambda *args: \"someuser\")\n        assert tmpdir.getbasetemp().name.startswith(\"pytest-of-someuser\")\n\n", "    def test_tmpdirfactory_getbasetemp(self) -> None:\n        tmpdir = self.tmpdir_factory.mktemp(\"test_getbasetemp\", numbered=True)\n        assert tmpdir.exists()\n        basetemp = self.tmpdir_factory.getbasetemp()\n        assert tmpdir.parent == basetemp\n"], "sample_492": ["    def test_operation_with_setting_reference(self):\n        operation = migrations.RunPython(\n            lambda apps, schema_editor: None,\n            lambda apps, schema_editor: None,\n            reverse_code=None,\n            dependencies=[\n                migrations.swappable_dependency(settings.AUTH_USER_MODEL)\n            ],\n        )\n\n        writer = OperationWriter(operation)\n        expected_output = (\n            \"migrations.RunPython(lambda apps, schema_editor: None, lambda apps, schema_editor: None, reverse_code=None, dependencies=[migrations.swappable_dependency(settings.AUTH_USER_MODEL)],)\"\n        )\n        self.assertEqual(writer.serialize()[0], expected_output)\n\n", "    def test_serialize_complex_objects(self):\n        operation = migrations.CreateModel(\n            name=\"ExampleModel\",\n            fields=[\n                models.ForeignKey(\n                    TestModel1,\n                    on_delete=models.CASCADE,\n                    related_name=\"related_models\",\n                ),\n                models.IntegerField(default=lambda: int(time.time())),\n                models.BooleanField(default=lambda: True if os.path.exists('test_file') else False),\n            ],\n            options={\n                \"verbose_name\": \"Example Model\",\n                \"verbose_name_plural\": \"Example Models\",\n            },\n        )\n        writer = OperationWriter(operation)\n        serialized_operation, _ = writer.serialize()\n        self.assertEqual(\n            serialized_operation,\n            \"\"\"migrations.CreateModel(", "    def test_serialize_operation_with_settings_reference(self):\n        class MyOperation(migrations.Operation):\n                super().__init__(*args, **kwargs)\n                self.setting_name = setting_name\n\n                return (\n                    self.__class__.__name__,\n                    [self.setting_name],\n                    {},\n                )\n\n        operation = MyOperation(settings.DEBUG)\n        writer = OperationWriter(operation)\n        serialized, _ = writer.serialize()\n\n        expected = (\n            \"MyOperation(settings.DEBUG,)\"\n        )\n        self.assertEqual(serialized.strip(), expected)\n\n", "    def test_serialize_enum_member(self):\n        operation = migrations.RunPython(\n            lambda apps: None,\n            lambda apps: None,\n            e=TextEnum.A\n        )\n        writer = OperationWriter(operation)\n        serialized, imports = writer.serialize()\n        self.assertIn(\"e=TextEnum.A,\", serialized)\n        self.assertIn(\"from enum import Enum\", imports)\n", "    def test_operation_writer_handles_decimal_objects(self):\n        operation = migrations.RunPython(\n            code=lambda apps, schema_editor: None,\n            reverse_code=lambda apps, schema_editor: None,\n            elide_operations=False,\n            atomic=False,\n        )\n        operation.state = {\"value\": Money(10.0)}\n\n        writer = OperationWriter(operation)\n        serialized, imports = writer.serialize()\n\n        # Check for the expected serialized output\n        self.assertIn(\"migrations.RunPython(\", serialized)\n        self.assertIn(\"value=Money('10.0'),\", serialized)\n        self.assertIn(\"import decimal\", imports)\n"], "sample_1152": ["    def test_powdenest_polar():\n        p = symbols('p', polar=True)\n\n        assert powdenest(p**2) == p**2\n        assert powdenest((p**2)**3) == p**6\n        assert powdenest(sqrt(p**2)) == p\n        assert powdenest(sqrt(p**4)) == p**2\n\n", "    def test_powdenest_polar():\n        Rp = Symbol('Rp', real=True, positive=True)\n        Tp = Symbol('Tp', real=True)\n        assert powdenest((Rp*exp(I*Tp))**(2*x)) == Rp**(2*x)*exp(2*I*x*Tp)\n        assert powdenest((Rp*exp(I*Tp))**(x + y)) == Rp**(x + y)*exp(I*(x + y)*Tp)\n\n", "    def test_powsimp_polar():\n        assert powdenest(\n            (sqrt(2)**I*sqrt(2)**-I)**2\n        ) == 2\n        assert powsimp((y**(2*I))**(2*I)) == y**-4\n\n", "    def test_powdenest_polar():\n        # Test denesting with polar numbers\n        p = Symbol('p', polar=True)\n        assert powdenest((p**2)**3) == p**6\n        assert powdenest(sqrt((p**2)**3)) == p**3\n        assert powdenest((p*p**2)**3) == p**18\n\n", "def test_powdenest_polar():\n    x = Symbol('x', positive=True)\n    y = Symbol('y', polar=True)\n    assert powdenest(x**(2*y)) == x**(2*y)\n\n"], "sample_870": ["def test_gaussian_process_regressor_different_kernels(kernel):\n    # Test GaussianProcessRegressor with different kernels\n    gpr = GaussianProcessRegressor(kernel=kernel)\n    gpr.fit(X, y)\n\n    test_points = np.array([-1.0, 2.0, 9.0]).reshape(-1, 1)\n    y_pred, sigma = gpr.predict(test_points, return_std=True)\n\n    assert len(y_pred) == len(test_points)\n    assert len(sigma) == len(test_points)\n\n", "def test_gp_regressor_predict_with_different_kernels():\n    # Test that predict works for different kernel types\n\n    for kernel in non_fixed_kernels:\n        gpr = GaussianProcessRegressor(kernel=kernel\n                                    )\n        gpr.fit(X, y)\n        y_pred, sigma = gpr.predict(X2, return_std=True)\n        assert len(y_pred) == len(X2)\n        assert len(sigma) == len(X2)\n\n", "    def test_gpr_with_kernel_gradient(self, kernel):\n        # Test that the gradient of the log marginal likelihood is correctly computed\n        # for a simple example.\n        gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n        gpr.fit(X, y)\n\n            return gpr.log_marginal_likelihood(theta, eval_gradient=True)[0], gpr.log_marginal_likelihood(theta, eval_gradient=True)[1]\n\n        gradient_approx = approx_fprime(gpr.kernel_.theta, obj_func, 1e-6)\n\n        theta = gpr.kernel_.theta\n        log_likelihood, log_likelihood_gradient = gpr.log_marginal_likelihood(\n            theta, eval_gradient=True\n        )\n        assert_allclose(log_likelihood_gradient, gradient_approx, rtol=1e-4)\n", "def test_predict_non_fitted():\n        gpr = GaussianProcessRegressor(kernel=kernels[0])\n        y_pred = gpr.predict(X)\n        assert_array_almost_equal(y_pred, np.zeros(X.shape[0]))\n", "    def test_gpr_sample_y(self, kernel):\n        # Check that sample_y produces samples with mean and variance\n        # consistent with the predictive distribution\n        gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n        gpr.fit(X, y)\n        y_samples = gpr.sample_y(X, n_samples=10, random_state=0)\n        y_mean, y_std = gpr.predict(X, return_std=True)\n\n        assert_allclose(np.mean(y_samples, axis=0), y_mean, atol=0.1)\n        assert_allclose(np.std(y_samples, axis=0), y_std, atol=0.1)\n"], "sample_486": ["    def test_inlineformset_factory_uuid_parent_auto_pk_child(self):\n        ParentFormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=['name']\n        )\n        formset = ParentFormSet()\n        self.assertEqual(formset.forms.model, UUIDPKChild)\n        self.assertEqual(formset.forms[0].fields['name'].widget.attrs, {})\n", "    def test_inlineformset_factory_with_uuid_pk_parent(self):\n        ParentFormSet = inlineformset_factory(\n            UUIDPKParent,\n            UUIDPKChild,\n            fields=[\"name\"],\n            fk_name=\"parent\",\n        )\n        formset = ParentFormSet()\n        self.assertEqual(formset.forms[0].fields[\"parent\"].widget.__class__, HiddenInput)\n", "    def test_inlineformset_factory_with_uuid_pk_parent_and_auto_pk_child(self):\n        \"\"\"Test inlineformset_factory with a UUID primary key parent and an auto\n        incrementing primary key child.\"\"\"\n\n        UUIDPKParentFormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fk_name=\"parent\", extra=1\n        )\n        data = {\n            \"form-TOTAL_FORMS\": \"2\",\n            \"form-INITIAL_FORMS\": \"0\",\n            \"form-0-name\": \"Child 1\",\n            \"form-1-name\": \"Child 2\",\n        }\n\n        formset = UUIDPKParentFormSet(data=data)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n", "    def test_inlineformset_factory_with_alternate_key(self):\n        ChildFormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=[\"name\"],\n            fk_name=\"parent_id\", extra=1\n        )\n        formset = ChildFormSet()\n        self.assertEqual(formset.forms[0].instance.parent_id, None)\n", "    def test_inlineformset_factory_with_uuid_pk_parent_and_auto_pk_child(self):\n        UUIDPKParentFormSet = inlineformset_factory(\n            UUIDPKParent, UUIDPKChild, fields=(\"name\",)\n        )\n        parent = UUIDPKParent.objects.create(name=\"parent\")\n        formset = UUIDPKParentFormSet(instance=parent)\n        self.assertEqual(len(formset.forms), 0)\n        formset.save()\n        self.assertEqual(UUIDPKChild.objects.filter(parent=parent).count(), 0)\n\n        formset = UUIDPKParentFormSet(instance=parent, data=[{\"name\": \"child1\"}])\n        self.assertTrue(formset.is_valid())\n        formset.save()\n\n        self.assertEqual(UUIDPKChild.objects.filter(parent=parent).count(), 1)\n"], "sample_1163": ["def test_polar_lift_conjugate():\n    x, y = symbols('x y', real=True)\n    z = (x + I*y)\n    assert polar_lift(conjugate(z)) == conjugate(polar_lift(z))\n", "def test_principal_branch_simple():\n    p = Symbol('p', polar=True)\n    assert principal_branch(p, 2*pi) == p\n    assert principal_branch(p, pi) == p\n    assert principal_branch(p*exp_polar(2*pi*I), 2*pi) == p\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3\n    assert principal_branch(exp_polar(2*pi*I)*3*p, 2*pi) == 3*principal_branch(p, 2*pi)\n", "    def test_principal_branch_infinity(self):\n        x = Symbol('x')\n        self.assertEqual(principal_branch(exp_polar(2*pi*I)*3, oo), 3*exp_polar(0))\n        self.assertEqual(principal_branch(exp_polar(2*pi*I)*3*x, oo), 3*principal_branch(x, oo))\n", "    def test_polarify_subs():\n        x, y = symbols('x y')\n        expr = x**y\n        eq, reps = polarify(expr, subs=True)\n        assert eq == _x**_y\n        assert reps == {_x: x, _y: y}\n", "def test_principal_branch_unbranched():\n    z = Symbol('z')\n    assert principal_branch(exp_polar(2*pi*I)*3, 2*pi) == 3*exp_polar(0)\n    assert principal_branch(exp_polar(2*pi*I)*3*z, 2*pi) == 3*principal_branch(z, 2*pi)\n    assert principal_branch(3*exp_polar(2*pi*I)*z, 2*pi) == 3*principal_branch(z, 2*pi)\n    assert principal_branch(z, oo) == z\n\n"], "sample_1177": ["    def test_principal_branch_evalf(self):\n        z = exp_polar(2*pi*I)*3\n        p = principal_branch(z, 2*pi)\n        assert N_equals(p._evalf(), 3)\n\n        z = exp_polar(5*pi*I)*3\n        p = principal_branch(z, 2*pi)\n        assert N_equals(p._evalf(), -3)\n\n        z = exp_polar(100*pi*I)*3\n        p = principal_branch(z, 2*pi)\n        assert N_equals(p._evalf(), 3)\n\n        z = Symbol('z')\n        p = principal_branch(z, 2*pi)\n        assert p == principal_branch(z, 2*pi)\n", "def test_principal_branch_symbolic():\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    assert principal_branch(exp_polar(I*pi*x), 2*pi) == exp_polar(I*pi*x)\n    assert principal_branch(exp_polar(I*pi*x), p) == exp_polar(I*pi*x).subs(\n        p, 2*pi)\n ", "    def test_polar_lift_Mul():\n        x, y = symbols('x y', real=True)\n        assert polar_lift(x*y) == polar_lift(x)*polar_lift(y)\n", "def test_principal_branch_infinity():\n    x = Symbol('x')\n    assert principal_branch(exp(2*pi*I)*3, oo) == 3*exp_polar(0)\n", "def test_periodic_argument_with_period_infinite():\n    assert periodic_argument(exp_polar(5*I*pi), oo) == 0\n    assert periodic_argument(exp_polar(5*I*pi), oo) == periodic_argument(\n        exp_polar(5*I*pi), 2*pi\n    )\n\n"], "sample_141": ["    def test_datetime_serialization(self):\n        \"\"\"\n        Test serialization and deserialization of datetime fields.\n        \"\"\"\n        pub_date = datetime.datetime(2006, 6, 16, 11, 0, 0)\n        a1 = Article.objects.create(\n            headline=\"Putin's Latest\",\n            pub_date=pub_date,\n            categories=[Category.objects.get(pk=1)],\n            author=Author.objects.get(pk=1),\n        )\n        serialized = serializers.serialize(\"json\", [a1])\n        deserialized = serializers.deserialize(\"json\", serialized)\n        obj = list(deserialized)[0].object\n\n        self.assertEqual(obj.headline, \"Putin's Latest\")\n        self.assertEqual(obj.pub_date, pub_date)\n\n", "    def test_deserialization_deferred_fk(self):\n        with self.assertRaisesMessage(DeserializationError, \"Article: (pk=None) field_value was '1'\"):\n            data = \"\"\"", "    def test_deserialize_manytomany_with_natural_keys(self):\n        \"\"\"\n        Test deserialization of ManyToManyField with natural keys.\n        \"\"\"\n        # Create objects with natural keys\n        categories = [\n            Category.objects.create(name='Cat 1'),\n            Category.objects.create(name='Cat 2'),\n        ]\n\n        article = Article.objects.create(\n            headline='Article with natural key categories',\n            author=self.author,\n            pub_date=datetime.datetime(2023, 10, 26),\n        )\n        article.categories.add(*categories)\n        # Serialize the article\n        serialized_data = serializers.serialize('json', [article],\n                                                use_natural_foreign_keys=True)\n        # Deserialize the article\n        deserialized_data = serializers.deserialize('json', serialized_data)\n        obj = list(deserialized_data)[0].object\n        # Check if categories are deserialized correctly\n        self.assertEqual(obj.categories.all(), categories)\n\n", "    def test_serialize_empty_queryset(self):\n        \"\"\"\n        Test serialization of an empty QuerySet.\n        \"\"\"\n        with self.assertRaisesMessage(SerializationError, \"QuerySet is empty\"):\n            serializers.serialize(self.serializer_name, Score.objects.none())\n", "    def test_deserialize_with_deferred_fields(self):\n        with override(gettext_lazy):\n            # Create two articles, one with a Score object\n            article1 = Article.objects.create(headline=\"Article 1\", pub_date=datetime.datetime(2023, 10, 26), author=self.author)\n            score1 = Score.objects.create(article=article1, value=5)\n\n            article2 = Article.objects.create(headline=\"Article 2\", pub_date=datetime.datetime(2023, 10, 27), author=self.author)\n\n            # Serialize the articles\n            serialized_data = serializers.serialize('json', [article1, article2], use_natural_foreign_keys=True, fields=['headline', 'author'])\n\n            # Deserialize the data, but simulate a missing Score object\n            deserialized_data = json.loads(serialized_data)\n            deserialized_article1 = De serializedObject(Article(**deserialized_data[0]['fields']), deferred_fields={'score': deserialized_data[0]['fields']['score']})\n\n            # Save the deserialized article, which will trigger deferred field processing\n            deserialized_article1.save()\n\n            # Check if the score object was correctly created\n            self.assertEqual(Score.objects.filter(article=article1).count(), 1)\n"], "sample_4": ["    def test_html_latex_names(self, cosmo, tmp_path):\n        # Create a temporary file\n        filepath = tmp_path / \"test.html\"\n\n        # Write the cosmology to the file as HTML\n        write_html_table(cosmo, filepath, latex_names=True)\n\n        # Read the cosmology from the HTML file\n        cosmo_read = read_html_table(filepath, latex_names=True)\n\n        # Check that the cosmology parameters match\n        assert cosmo == cosmo_read\n", "    def test_write_read_latex_names(self, cosmo, tmp_path):\n        \"\"\"Test writing and reading a Cosmology with LaTeX column names.\n        \"\"\"\n\n        html_file = tmp_path / \"cosmology.html\"\n        write_html_table(cosmo, html_file, latex_names=True)\n\n        cosmo_read = read_html_table(html_file, latex_names=True)\n\n        assert cosmo_read == cosmo\n", "    def test_write_read_html_latex_names(self, cosmo, tmp_path):\n        \"\"\"Test writing and reading HTML with latex names.\"\"\"\n        filename = tmp_path / 'test.html'\n\n        write_html_table(cosmo, filename, latex_names=True)\n        new_cosmo = read_html_table(filename, latex_names=True)\n        self.assertEqual(cosmo, new_cosmo)\n", "    def test_read_html_latex_names(self, cosmo_class, tmp_path):\n        \"\"\"Test for reading from HTML with latex names.\"\"\"\n        # Write a cosmology to a temporary file in HTML format\n        filename = tmp_path / \"cosmology.html\"\n        write_html_table(cosmo_class(), filename, latex_names=True)\n\n        # Read the cosmology back from the file, using latex names\n        cosmo_read = read_html_table(filename, latex_names=True)\n\n        # Assert that the cosmology parameters match\n        assert cosmo_read == cosmo_class()\n", "    def test_write_read_latex_names(self, cosmo, tmpdir):\n        \"\"\"Test writing `Cosmology` to HTML and reading back with latex names.\"\"\"\n        html_file = tmpdir.join(\"cosmo.html\")\n        write_html_table(cosmo, html_file, latex_names=True)\n\n        # Read the cosmology back in\n        cosmo_read = read_html_table(html_file, latex_names=True)\n\n        assert cosmo.to_format() == cosmo_read.to_format()\n\n"], "sample_277": ["    def test_q_resolve_expression_empty_q(self):\n        q = Q()\n        with self.assertRaises(ValueError):\n            q.resolve_expression(None)\n\n", "    def test_q_empty_or(self):\n        q = Q() | Q()\n        self.assertFalse(q)\n", "    def test_q_combine_with_empty_q(self):\n        q1 = Q(foo=1)\n        q2 = Q()\n        self.assertEqual(q1 | q2, q1)\n        self.assertEqual(q2 | q1, q1)\n        self.assertEqual(q1 & q2, q1)\n        self.assertEqual(q2 & q1, q1)\n", "    def test_q_invert(self):\n        q = Q(pk=1)\n        inverted_q = ~q\n        self.assertEqual(inverted_q.connector, Q.AND)\n        self.assertEqual(len(inverted_q.children), 1)\n        self.assertEqual(inverted_q.children[0][0], 'NOT')\n        self.assertEqual(inverted_q.children[0][1], q)\n", "    def test_q_resolution_empty(self):\n        q = Q()\n        self.assertEqual(q.resolve_expression(None), '')\n"], "sample_440": ["    def test_bulk_create_with_null_values(self):\n        NullableFields.objects.bulk_create(\n            [\n                NullableFields(\n                    char_field=\"a\",\n                    text_field=\"a\",\n                    integer_field=1,\n                    float_field=1.0,\n                    boolean_field=True,\n                ),\n                NullableFields(\n                    char_field=None,\n                    text_field=None,\n                    integer_field=None,\n                    float_field=None,\n                    boolean_field=None,\n                ),\n            ]\n        )\n        self.assertEqual(NullableFields.objects.count(), 2)\n", "    def test_bulk_create_with_proxy_models(self):\n        ProxyCountry.objects.bulk_create([p.as_proxy() for p in self.data])\n        self.assertEqual(ProxyCountry.objects.count(), 4)\n        self.assertEqual(Country.objects.count(), 4)\n", "    def test_bulk_create_with_related_objects(self):\n        pizzeria = Pizzeria.objects.create(name=\"Pizza Place\")\n        p_data = [\n            Restaurant(name=\"Restaurant A\", pizzeria=pizzeria),\n            Restaurant(name=\"Restaurant B\", pizzeria=pizzeria),\n        ]\n        Restaurant.objects.bulk_create(p_data)\n        self.assertEqual(Restaurant.objects.count(), 2)\n        self.assertEqual(\n            Restaurant.objects.filter(pizzeria=pizzeria).count(), 2\n        )\n", "    def test_bulk_create_with_inheritance(self):\n        ProxyCountry.objects.bulk_create(self.data)\n        for country in self.data:\n            proxy = ProxyCountry.objects.get(name=country.name)\n            self.assertEqual(proxy.name, country.name)\n            self.assertEqual(proxy.iso_two_letter, country.iso_two_letter)\n\n", "    def test_bulk_create_with_nulls(self):\n        data = [\n            NullableFields(\n                name=\"A\", value=1, nullable_field=1, another_nullable_field=\"a\"\n            ),\n            NullableFields(\n                name=\"B\", value=2, nullable_field=None, another_nullable_field=None\n            ),\n            NullableFields(\n                name=\"C\", value=3, nullable_field=None, another_nullable_field=\"c\"\n            ),\n        ]\n        created = NullableFields.objects.bulk_create(data)\n        self.assertEqual(len(created), 3)\n        retrieved = NullableFields.objects.all()\n        self.assertEqual(len(retrieved), 3)\n\n"], "sample_739": ["    def test_inverse_transform_sparse_float(self):\n        lb = LabelBinarizer()\n        y = np.array([[0, 1], [2, 1], [0, 0]])\n        lb.fit(y)\n        y_bin = lb.transform(y)\n\n        # Check sparse input\n        y_inv = lb.inverse_transform(csc_matrix(y_bin))\n        assert_array_equal(y_inv, y)\n", "    def test_multilabel_binarizer_sparse_output(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        y = [[1, 2],\n             [3,],\n             [1, 3]]\n        yt = mlb.fit_transform(y)\n        assert_true(issparse(yt))\n        assert_equal(yt.shape, (3, 4))\n        assert_array_equal(yt.toarray(), mlb.transform(y).toarray())\n        # Check that the output is in CSR format\n        assert_true(isinstance(yt, csr_matrix))\n", "    def test_label_binarize_multi_output():\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y_multioutput = toarray([\n            [1, 0],\n            [0, 1],\n            [1, 1]\n        ])\n        with ignore_warnings():\n            Y = label_binarize(X, classes=[1, 2, 3, 4, 5, 6],\n                               sparse_output=True)\n        assert_equal(toarray(Y), y_multioutput)\n\n", "    def test_label_binarize_single_class(self):\n        y = np.array([0] * 10)\n        classes = [0]\n        expected = np.zeros((10, 1), dtype=int)\n        assert_array_equal(label_binarize(y, classes), expected)\n\n", "    def test_label_binarize_multioutput(self):\n        y = np.array([[1, 2], [3, 4]])\n        classes = np.array([1, 2, 3, 4])\n        y_bin = label_binarize(y, classes=classes)\n        assert_equal(y_bin.shape, (2, 4))\n        assert_array_equal(y_bin, [[1, 1, 0, 0], [0, 0, 1, 1]])\n"], "sample_754": ["def test_sparsity_control(n_components=5, n_samples=100, image_size=(10, 10),\n                          random_state=0, alpha=0.1, tol=1e-2):\n    Y, _, _ = generate_toy_data(n_components, n_samples, image_size,\n                                random_state)\n    spca = SparsePCA(n_components=n_components, alpha=alpha,\n                     random_state=random_state)\n    spca.fit(Y)\n\n    # Check that the components are indeed sparse\n    for i in range(n_components):\n        spca_comp = spca.components_[i]\n        sparsity = np.sum(np.abs(spca_comp) > 1e-10) / len(spca_comp)\n        assert sparsity < 0.5, \"Component {} sparsity: {} > {}\".format(\n            i, sparsity, 0.5)\n", "    def test_sparse_pca_warm_start(self):\n        # Test warm start functionality of SparsePCA\n        n_samples = 100\n        n_components = 5\n        image_size = (10, 10)\n        X, _, _ = generate_toy_data(n_components, n_samples, image_size,\n                                    random_state=0)\n        alpha = 0.1\n        spca = SparsePCA(n_components=n_components, alpha=alpha,\n                         random_state=0, normalize_components=norm_comp)\n        spca.fit(X)\n        U_init = spca.components_.copy()\n        V_init = spca.components_.T.copy()\n        spca_warm = SparsePCA(n_components=n_components, alpha=alpha,\n                             U_init=U_init, V_init=V_init, random_state=0,\n                             normalize_components=norm_comp)\n        # Forcing warm start to be used\n        spca_warm.fit(X)\n\n        assert_allclose(spca.components_, spca_warm.components_)\n", "    def test_sparse_pca_fit_transform_accuracy(self, norm_comp):\n        # Test the transform method accuracy\n        n_samples = 100\n        n_components = 3\n        image_size = (10, 10)\n\n        Y, _, V = generate_toy_data(n_components, n_samples, image_size,\n                                     random_state=0)\n        if norm_comp:\n            sparse_pca = SparsePCA(n_components=n_components, normalize_components=True,\n                                   random_state=0)\n        else:\n            sparse_pca = SparsePCA(n_components=n_components,\n                                   random_state=0)\n        sparse_pca.fit(Y)\n        X_transformed = sparse_pca.transform(Y)\n\n        # Check that the reconstruction error is small\n        reconstructed = sparse_pca.inverse_transform(X_transformed)\n        assert_allclose(Y, reconstructed, rtol=1e-2)\n", "    def test_sparse_pca_fit_transform_ridge_alpha(self):\n        # Test if fitting and transforming with different ridge alpha values\n        # yields different results\n        X, _, _ = generate_toy_data(n_components=3, n_samples=10,\n                                    image_size=(10, 10), random_state=0)\n\n        spca = SparsePCA(n_components=3, alpha=0.1, ridge_alpha=0.1,\n                         random_state=0).fit(X)\n        X_transformed_01 = spca.transform(X)\n\n        spca = SparsePCA(n_components=3, alpha=0.1, ridge_alpha=0.5,\n                         random_state=0).fit(X)\n        X_transformed_05 = spca.transform(X)\n\n        assert_true(not np.allclose(X_transformed_01, X_transformed_05))\n\n", "    def test_sparse_pca_fit_transform(self):\n        # Test that transform works after fit, with and without ridge\n        rng = np.random.RandomState(0)\n        n_samples, n_features = 100, 20\n        X, _, _ = generate_toy_data(\n            n_components=3, n_samples=n_samples,\n            image_size=(10, 10), random_state=rng)\n        spca = SparsePCA(n_components=3, alpha=0.1, normalize_components=norm_comp, random_state=rng)\n        spca.fit(X)\n        X_t = spca.transform(X)\n        X_t_ridge = spca.transform(X, ridge_alpha=0.1)\n        assert_true(X_t.shape == (n_samples, 3))\n        assert_true(X_t_ridge.shape == (n_samples, 3))\n\n\n"], "sample_768": ["def test_check_cv_iterable():\n\n    cv = [(np.arange(10), np.arange(10, 20))]\n\n    checked_cv = check_cv(cv)\n    assert isinstance(checked_cv, _CVIterableWrapper)\n    for train, test in checked_cv:\n      assert_array_equal(train, np.arange(10))\n      assert_array_equal(test, np.arange(10, 20))\n\n", "def test_check_cv_predefined_split():\n    ps = PredefinedSplit(test_fold=[0, 1, 2, 0, 1, 2])\n    # Check if the number of splits is correct\n    assert_equal(ps.get_n_splits(X), 3)\n\n    # Check if the splits are created correctly\n    train_index, test_index = next(ps.split(X))\n    assert_array_equal(train_index, [2, 3, 4])\n    assert_array_equal(test_index, [0, 1])\n", "def test_stratified_shuffle_split_with_pandas_dataframe():\n    df = MockDataFrame({'col1': [1, 1, 2, 2, 3, 3],\n                         'col2': [1, 2, 3, 4, 5, 6]})\n    sss = StratifiedShuffleSplit(n_splits=2, test_size=0.5, random_state=42)\n    for train_index, test_index in sss.split(df, df['col1']):\n        assert_equal(len(train_index) + len(test_index), len(df))\n\n", "def test_cross_val_score_sparse_matrix():\n    \"\"\"Test cross_val_score with sparse matrices\"\"\"\n    clf = MockClassifier()\n    X = coo_matrix([[1, 2], [3, 4]])\n    y = np.array([0, 1])\n    scores = cross_val_score(clf, X, y, cv=KFold(2))\n    assert_array_equal(scores, [1/2, 1/2])\n\n    # Test with different sparse matrix formats\n    for fmt in ['coo', 'csc', 'csr']:\n        X = getattr(scipy.sparse, fmt + '_matrix')(X)\n        scores = cross_val_score(clf, X, y, cv=KFold(2))\n        assert_array_equal(scores, [1/2, 1/2])\n", "def test_stratifiedkfold_empty_classes():\n    # Test case for stratified k-fold when a class has 0 samples\n    y = np.array([0, 0, 1, 1, 2])\n    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n    with pytest.raises(ValueError, match=\"The least populated class in y has only 0 members\"):\n        list(skf.split(X, y))\n"], "sample_939": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_145": ["    def test_formset_options(self):\n        class MyInlineFormSet(BaseModelFormSet):\n            pass\n\n        class MyInline(TabularInline):\n            formset = MyInlineFormSet\n            model = Song\n\n        self.assertIsInvalid(MyInline, Band, \"The value of 'formset' must inherit from 'BaseModelFormSet'.\", id='admin.E206')\n", "    def test_get_list_display_links_has_invalid_field(self):\n        class MyAdmin(ModelAdmin):\n            model = ValidationTestModel\n            list_display = ('field1', 'field2')\n            list_display_links = ('field3',)\n\n        self.assertIsInvalid(\n            MyAdmin,\n            ValidationTestModel,\n            r\"The value of 'list_display_links\\[0\\]' refers to 'field3', which is not defined in 'list_display'.\",\n            id='admin.E111',\n        )\n", "    def test_list_display_links_invalid_item(self):\n        class MyAdmin(ModelAdmin):\n            model = ValidationTestModel\n            list_display_links = [\n                'id',\n                'nonexistent_field', \n            ]\n        self.assertIsInvalidRegexp(MyAdmin, ValidationTestModel,\n                                  r\"The value of 'list_display_links\\[1\\]' refers to 'nonexistent_field', which is not defined in 'list_display'.\",\n                                  id='admin.E111')\n\n", "    def test_validationtestmodel_inline(self):\n        class ValidationTestInlineAdmin(TabularInline):\n            model = ValidationTestInlineModel\n            extra = 1\n            fk_name = 'parent_model'\n\n        self.assertIsInvalid(\n            ValidationTestInlineAdmin,\n            ValidationTestModel,\n            # The error message\n            \"The value of 'fk_name' refers to 'parent_model', which is not a ForeignKey field.\"\n        )\n\n", "    def test_check_actions_uniqueness(self):\n        class TestModelAdmin(ModelAdmin):\n            actions = ['action1', 'action1']\n                pass\n\n        self.assertIsInvalidRegexp(\n            TestModelAdmin,\n            ValidationTestModel,\n            r\"__name__ attributes of actions defined in 'TestModelAdmin' must be \"\n            r\"unique. Name 'action1' is not unique.\",\n            id='admin.E130',\n        )\n"], "sample_1198": ["    def test_parse_derivative():\n        assert parse_mathematica(\"D[f[x],x]\") == Derivative(f(x), x)\n        assert parse_mathematica(\"D[f[x,y],x]\") == Derivative(f(x, y), x)\n        assert parse_mathematica(\"D[f[x,y],{x,2}]\") == Derivative(f(x, y), (x, 2))\n\n", "    def test_parse_mathematica_function_with_arguments():\n        parser = MathematicaParser()\n        expr = parser.parse(\"Sin[x + y]\")\n        assert expr == sin(x + y)\n\n", "    def test_parse_compound_expression_empty():\n        expr = \"CompoundExpression[]\"\n        assert parse_mathematica(expr) == []\n\n", "def test_parse_mathematica_string_function():\n    M = MathematicaParser()\n    expr = M.parse(\"Function[x, x^2]\")\n    assert expr == Function(x)(x**2)\n", "    def test_parse_mathematica_function_with_noargs(self):\n        expr = \"f[]\"\n        parsed_expr = parse_mathematica(expr)\n        assert parsed_expr == Function('f')()\n"], "sample_1043": ["    def test_print_Limit(self):\n        self.assertEqual(mcode(Limit(sin(x)/x, x, 0)),\n                         \"Hold[Limit[Sin[x]/x,x->0]]\")\n", "    def test_print_ImmutableDenseMatrix(self):\n        a = [[1, 2], [3, 4]]\n\n        self.assertEqual(mcode(sympy.Matrix(a)), '{{1, 2}, {3, 4}}')\n", "compilation error", "    def test_calculus(self):\n        assert mcode(f(x).diff(x)) == 'Hold[D[f[x], x]]'\n        assert mcode(Integral(sin(x), (x, 0, pi))) == 'Hold[Integrate[Sin[x], {x, 0, Pi}]]'\n        assert mcode(Sum(x**n, (n, 1, 5))) == 'Hold[Sum[x^n, {n, 1, 5}]]'\n        assert mcode(Derivative(sin(x), x)) == 'Hold[D[Sin[x], x]]'\n\n", "    def test_mcode_derivative_with_order(self):\n        expr = Derivative(sin(x)*cos(x), x, 2)\n        expected = \"Hold[D[Sin[x]*Cos[x],{x,2}]]\"\n        assert mcode(expr, order='lex') == expected\n"], "sample_596": ["    def test_concat_dataarrays_join(self, dim, expected_dims, join):\n        # test concatenation with different join types\n        da1 = DataArray(\n            np.arange(4).reshape((2, 2)), dims=['dim1', 'dim2'], name='data1'\n        )\n        da2 = DataArray(\n            np.arange(6).reshape((3, 2)), dims=['dim1', 'dim2'], name='data2'\n        )\n\n        ds1 = da1._to_temp_dataset()\n        ds2 = da2._to_temp_dataset()\n\n        result = concat([ds1, ds2], dim=dim, join=join)\n\n\n        assert_equal(result.dims, expected_dims)\n", "    def test_concat_dataarrays_with_different_coords():\n        da1 = DataArray(\n            np.arange(4).reshape((2, 2)), coords={\"x\": [0, 1], \"y\": [0, 1]}, dims=[\"x\", \"y\"]\n        )\n        da2 = DataArray(\n            np.arange(4).reshape((2, 2)) + 4,\n            coords={\"x\": [0, 1], \"z\": [0, 1]},\n            dims=[\"x\", \"z\"],\n        )\n\n        expected = concat(\n            [da1, da2], dim=\"new_dim\", coords=\"minimal\", compat=\"override\"\n        )\n        expected_coords = {\"x\": [0, 1, 0, 1], \"new_dim\": [0, 1]}\n        expected_data = np.array([\n            [0, 1],\n            [2, 3],\n            [4, 5],\n            [6, 7],\n        ])\n\n        assert_identical(\n            expected,\n            DataArray(\n                expected_data,\n                coords=expected_coords,\n                dims=[\"new_dim\", \"x\"],\n            ),\n        )\n", "    def test_concat_dataarray_override_compat():\n        da1 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"],\n                        attrs={\"a\": 1})\n        da2 = DataArray(np.arange(4, 8).reshape(2, 2), dims=[\"x\", \"y\"],\n                        attrs={\"a\": 2})\n\n        expected = concat([da1, da2], dim='x')\n        actual = concat([da1, da2], dim='x', compat='override')\n        assert_identical(expected, actual)\n", "    def test_concat_different_compat_coords():\n        da1 = DataArray(np.ones((2, 3)), dims=[\"x\", \"y\"], coords={\"x\": range(2)})\n        da2 = DataArray(np.zeros((2, 3)), dims=[\"x\", \"y\"], coords={\"x\": range(2, 4)})\n        with pytest.raises(ValueError):\n            concat([da1, da2], dim=\"x\", compat=\"identical\")\n        expected = concat([da1, da2], dim=\"x\", compat=\"override\")\n        actual = concat([da1, da2], dim=\"x\", compat=\"no_conflicts\")\n        assert_identical(expected, actual)\n", "    def test_concat_different_coords_with_conflicts():\n        # Test concatenation with different data variables and coordinates\n        # with conflicting values.\n        array1 = DataArray(\n            np.arange(4).reshape((2, 2)),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [0, 1], \"y\": [0, 1]},\n        )\n\n        array2 = DataArray(\n            np.arange(4).reshape((2, 2)) * 2,\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [0, 1], \"y\": [0, 2]},\n        )\n\n        expected_coords = {\"x\": [0, 1], \"y\": [0, 1, 2]}\n        expected_data = np.array(\n            [[0, 0], [2, 4], [4, 8]], dtype=np.int64\n        )\n\n        result = concat([array1, array2], dim=\"y\", compat=\"no_conflicts\")\n        assert_identical(result, DataArray(expected_data, dims=[\"x\", \"y\"], coords=expected_coords))\n"], "sample_494": ["    def test_serialize_operation_with_kwargs(self):\n        class MyOperation(migrations.Operation):\n                super().__init__()\n                self.arg1 = arg1\n                self.arg2 = arg2\n                self.kwargs = kwargs\n\n        operation = MyOperation(arg1=\"value1\", arg2=\"value2\", kwarg1=\"val1\", kwarg2=2)\n        writer = OperationWriter(operation)\n\n        expected_output = (\n            'MyOperation(arg1=\"value1\", arg2=\"value2\", kwarg1=\"val1\", kwarg2=2)'\n        )\n        self.assertEqual(writer.serialize(), (expected_output, set()))\n\n", "    def test_serialize_deconstructable_instances(self):\n        instance = DeconstructibleInstances()\n        serializer = migrations.Serializer(instance)\n        self.assertEqual(serializer.serialize()[0], \"DeconstructibleInstances()\")\n\n\n\n", "    def test_serialize_enum_intflag(self):\n        with mock.patch(\"django.db.migrations.serializer.PY311\", True):\n            operation = migrations.CreateModel(\n                name='MyModel',\n                fields=[\n                    ('foo', models.IntegerField())\n                ],\n                options={\n                    'flags': IntFlagEnum.A | IntFlagEnum.B,\n                },\n            )\n            writer = OperationWriter(operation)\n            serialized = writer.serialize()\n            self.assertEqual(serialized[0], 'migrations.CreateModel(\\n    name=\"MyModel\",\\n    fields=[(\\\\n        \"foo\", models.IntegerField(),\\\\n    )],\\n    options={\\\\n        \"flags\": IntFlagEnum.A | IntFlagEnum.B,\\\\n    },\\n)')\n\n", "    def test_serializer_int_flag_enum(self):\n        with self.settings(DEBUG=True):\n            class MyModel(models.Model):\n                field = models.SmallIntegerField(\n                    choices=[(IntFlagEnum.A, \"A\"), (IntFlagEnum.B, \"B\")]\n                )\n\n            operation = migrations.AddField(\n                model_name=\"mymodel\",\n                name=\"field\",\n                field=MyModel._meta.get_field(\"field\"),\n            )\n            writer = OperationWriter(operation, indentation=0)\n            expected = (\n                \"migrations.AddField(\\n    model_name=\\'mymodel\\',\\n    name=\\'field\\',\\n    \"\n                \"field=models.SmallIntegerField(choices=[(IntFlagEnum.A, 'A'), \"\n                \"(IntFlagEnum.B, 'B')]),\\n)\"\n            )\n            self.assertEqual(writer.serialize(), (expected, set()))\n\n", "    def test_serialize_datetime_aware(self):\n        tz = get_fixed_timezone(time.timedelta(hours=5))\n        dt = datetime.datetime.now(tz)\n\n        with mock.patch(\"django.db.migrations.serializer.datetime.datetime\") as dt_mock:\n            dt_mock.now.return_value = dt\n            migration = migrations.Migration(\"test\")\n            operation = migrations.CreateModel(\n                name=\"Foo\",\n                fields=([models.DateTimeField(default=dt)]),\n            )\n            migration.operations.append(operation)\n\n            writer = OperationWriter(migration, indentation=0).serialize()\n            self.assertIn(\n                'datetime.datetime(2023, 10, 26, 18, 42, 1, 123456, tzinfo=datetime.timezone(datetime.timedelta(seconds=18000)))',\n                writer[0],\n            )\n"], "sample_1005": ["def test_latex_complex_region():\n    cr = ComplexRegion(Interval(-1, 1), Interval(-1, 1))\n    assert latex(cr) == r\"\\left\\{ z \\in \\mathbb{C} \\, \\middle| \\, -1 \\le \\operatorname{Re}(z) \\le 1 \\text{ and } -1 \\le \\operatorname{Im}(z) \\le 1 \\right\\}\"\n\n\n\n", "def test_latex_printing_of_lowergamma():\n    assert latex(lowergamma(z, a)) == \"\\\\text{lowergamma}(z, a)\"\n\n", "def test_latex_UnevaluatedExpr():\n    x = Symbol('x')\n    expr = UnevaluatedExpr(sin(x)**2 + cos(x)**2)\n    assert latex(expr) == '\\\\sin^{2}{\\left (x \\right )} + \\\\cos^{2}{\\left (x \\right )}'\n", "def test_latex_SetExpr():\n    A = SetExpr([1, 2, 3])\n    assert latex(A) == '\\\\left\\{1, 2, 3\\\\right\\}'\n\n", "def test_latex_print_lowergamma():\n    assert latex(lowergamma(x, y)) == r'\\lowergamma(x, y)' \n"], "sample_278": ["    def test_filtered_relation_clone(self):\n        fr = FilteredRelation(\n            'employees',\n            condition=Q(salary__gt=10),\n        )\n        fr_clone = fr.clone()\n        # Check that cloning creates a new object with the same attributes\n        self.assertEqual(fr_clone.relation_name, 'employees')\n        self.assertEqual(fr_clone.condition, fr.condition)\n        self.assertIsNot(fr_clone, fr)\n", "    def test_filtered_relation(self):\n        query = Company.objects.annotate(\n            employees_with_high_salary=Count(\n                'employee_set',\n                filter=Q(employee__salary__gt=F('num_employees') / 10)\n            )\n        )\n        with CaptureQueriesContext(connection) as captured_queries:\n            list(query)\n        self.assertEqual(len(captured_queries), 2)\n        # Check the join condition is applied correctly.\n        self.assertIn('WHERE \"employee\".\"salary\" > (\"company\".\"num_employees\" / 10)', str(captured_queries[1]['sql']))\n", "    def test_filter_expression_with_when(self):\n        # Test filtering using Case/When expressions.\n        result = Employee.objects.filter(\n            When(salary__gt=20, then=\"High\").case(\n                When(salary__gt=10, then=\"Medium\").case(\n                    default=\"Low\"\n                )\n            ) == \"High\"\n        ).values(\"firstname\", \"lastname\")\n        self.assertCountEqual(result, [(\"Frank\", \"Meyer\")])\n", "    def test_filtered_relation_join(self):\n        # Test FilteredRelation functionality by filtering related objects.\n        query = (\n            Company.objects.annotate(\n                managers=Count(\n                    'employee_set',\n                    filter=Q(employee_set__position='manager'),\n                )\n            )\n        )\n\n        with CaptureQueriesContext(connection) as captured_queries:\n            list(query)\n\n        self.assertGreater(len(captured_queries), 1)\n        sql = captured_queries[1]['sql']\n\n        # Check for the existence of FilteredRelation in the WHERE clause\n        self.assertIn(\n            'WHERE (\"employee\".\"position\" = %s)',\n            sql,\n        )\n\n\n\n", "    def test_filtered_relation_clone(self):\n        filtered_relation = FilteredRelation('employees', condition=Q(salary__gt=10))\n        cloned_relation = filtered_relation.clone()\n        self.assertEqual(filtered_relation.relation_name, cloned_relation.relation_name)\n        self.assertEqual(filtered_relation.condition, cloned_relation.condition)\n        self.assertIsNone(cloned_relation.alias)\n        self.assertEqual(filtered_relation.path, cloned_relation.path)\n"], "sample_269": ["    def test_set_language_post(self):\n        rf = RequestFactory()\n        inactive_lang_code = self._get_inactive_language_code()\n        response = self.client.post(\n            reverse('set_language'),\n            {'language': inactive_lang_code, 'next': '/'},\n            HTTP_X_REQUESTED_WITH='XMLHttpRequest',\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/')\n        self.assertEqual(get_language(), inactive_lang_code)\n", "    def test_set_language_with_next_url(self):\n        rf = RequestFactory()\n        inactive_language_code = self._get_inactive_language_code()\n        request = rf.post(reverse('set_language'), {\n            'next': '/',\n            LANGUAGE_QUERY_PARAMETER: inactive_language_code,\n        })\n\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)  # Redirect\n        self.assertEqual(response['Location'], '/')\n        self.assertEqual(get_language(), inactive_language_code)\n        self.assertEqual(response.cookies[settings.LANGUAGE_COOKIE_NAME].value, inactive_language_code)\n", "    def test_set_language_post(self):\n        \"\"\"Test POST request to set_language view.\"\"\"\n        inactive_language = self._get_inactive_language_code()\n        rf = RequestFactory()\n        response = self.client.post(\n            reverse('set_language'), {'language': inactive_language, 'next': '/'},\n        )\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response.url, translate_url('/', inactive_language))\n        self.assertEqual(self.client.session.get(settings.LANGUAGE_SESSION_KEY), inactive_language)\n", "    def test_set_language_redirects_to_next_url(self):\n        factory = RequestFactory()\n\n        response = self.client.post(\n            reverse('set_language'),\n            {'next': '/some/url/', LANGUAGE_QUERY_PARAMETER: 'de'},\n        )\n        self.assertRedirects(response, '/some/url/')\n        self.assertEqual(get_language(), 'de')\n", "    def test_set_language_sets_cookie(self):\n        \"\"\"Test that set_language sets the language cookie.\"\"\"\n        rf = RequestFactory()\n        request = rf.post(reverse('set_language'), {'language': 'fr', 'next': '/'})\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertIn('LANGUAGE_COOKIE_NAME', response.cookies)\n        self.assertEqual(response.cookies['LANGUAGE_COOKIE_NAME'].value, 'fr')\n\n"], "sample_229": ["    def test_union_with_subquery_exists(self):\n        # Test union with subquery using Exists\n\n        numbers_with_even_other_num = Number.objects.filter(other_num__even)\n        subquery = Number.objects.filter(\n            other_num=OuterRef('other_num'),\n        ).exists()\n        queryset = Number.objects.filter(other_num__gt=3).union(\n            numbers_with_even_other_num.annotate(exists=subquery).filter(exists=True),\n        )\n\n        expected_numbers = [\n            Number(num=4, other_num=6),\n            Number(num=5, other_num=5),\n            Number(num=6, other_num=4),\n            Number(num=7, other_num=3),\n            Number(num=8, other_num=2),\n            Number(num=9, other_num=1),\n        ]\n        self.assertNumbersEqual(queryset, expected_numbers)\n\n", "    def test_union(self):\n        numbers1 = Number.objects.filter(num__lt=5)\n        numbers2 = Number.objects.filter(num__gt=7)\n        unioned = numbers1.union(numbers2)\n        self.assertNumbersEqual(unioned, [0, 1, 2, 3, 4, 8, 9])\n", "    def test_union_with_different_fields(self):\n        qs1 = Number.objects.values('num')\n        qs2 = Number.objects.values('other_num')\n        combined = qs1.union(qs2)\n        self.assertQuerysetEqual(combined, list(qs1) + list(qs2), lambda x: x['num'] if 'num' in x else x['other_num'])\n", "    def test_select_related_descendants_with_union(self):\n        # Test select_related() with descendant models when the main queryset\n        # involves a UNION.\n        q1 = Number.objects.filter(num__gt=5).select_related('reservedname')\n        q2 = Number.objects.filter(num__lt=5).select_related('reservedname')\n        queryset = q1.union(q2)\n        self.assertQuerysetEqual(\n            queryset.values('num', 'reservedname__name'),\n            [{'num': 6, 'reservedname__name': 'six'},\n             {'num': 7, 'reservedname__name': 'seven'},\n             {'num': 8, 'reservedname__name': 'eight'},\n             {'num': 9, 'reservedname__name': 'nine'},\n             {'num': 0, 'reservedname__name': 'zero'},\n             {'num': 1, 'reservedname__name': 'one'},\n             {'num': 2, 'reservedname__name': 'two'},\n             {'num': 3, 'reservedname__name': 'three'},\n             {'num': 4, 'reservedname__name': 'four'}],\n        )\n", "    def test_union_with_subquery(self):\n        # Test union with a subquery\n        subquery = Number.objects.filter(num__gt=5).values('num')\n        union_queryset = Number.objects.filter(num__lt=3).union(subquery)\n        expected_numbers = [Number(num=1), Number(num=2), Number(num=6), Number(num=7), Number(num=8), Number(num=9)]\n        self.assertNumbersEqual(union_queryset, expected_numbers)\n"], "sample_788": ["def test_transform_single_feature_constant(strategy, expected):\n    X = np.array([[-2], [-2], [-2], [-2]])\n    est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n    Xt = est.fit_transform(X)\n\n    assert_array_equal(Xt, np.array([[0], [0], [0], [0]]))\n", "    def test_constant_feature(self):\n        X = np.array([[10], [10], [10], [10]])\n        est = KBinsDiscretizer(n_bins=5, encode='onehot').fit(X)\n        Xt = est.transform(X)\n        assert_array_equal(Xt.toarray(), np.array([[1, 0, 0, 0, 0],\n                                                 [1, 0, 0, 0, 0],\n                                                 [1, 0, 0, 0, 0],\n                                                 [1, 0, 0, 0, 0]]))\n", "    def test_inverse_transform_constant_feature(self):\n        X = [[1, 0], [1, 0], [1, 0], [1, 0]]\n        est = KBinsDiscretizer(n_bins=2, encode='onehot')\n        Xt = est.fit_transform(X)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            X_inv = est.inverse_transform(Xt)\n        assert_array_equal(X_inv[:, 0], np.array([1, 1, 1, 1]))\n        assert_array_almost_equal(X_inv[:, 1], np.array([0, 0, 0, 0]))\n", "    def test_k_bins_discretizer_inverse(strategy):\n        est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy=strategy)\n        Xt = est.fit_transform(X)\n        Xinv = est.inverse_transform(Xt)\n        assert_array_almost_equal(X, Xinv)\n\n", "    def test_transform_n_bins_one(self):\n        X = [[10]]\n        est = KBinsDiscretizer(n_bins=1, encode='ordinal', strategy='uniform')\n        Xt = est.fit_transform(X)\n        assert_array_equal(Xt, [[0]])\n"], "sample_476": ["    def test_save_blank_image(self):\n        \"\"\"\n        Test saving a blank image and ensure dimensions are updated\n        correctly.\n        \"\"\"\n        data = {'name': 'blank', 'mugshot': ''}\n        person = self.PersonModel(**data)\n        person.save()\n\n        self.check_dimensions(person, None, None)\n", "    def test_save_and_update_dimensions(self):\n        person = self.PersonModel.objects.create(name=\"Fred\", age=25)\n        instance = person\n        self.check_dimensions(person, None, None)\n\n        person.mugshot = self.file1\n        person.save()\n        self.check_dimensions(person, 4, 8)\n\n        person.mugshot = self.file2\n        person.save()\n        self.check_dimensions(person, 8, 4)\n\n", "    def test_dimensions_update_on_save(self):\n        \"\"\"\n        #21957 - Test that dimensions are updated on save even when the\n        image field is not changed.\n        \"\"\"\n        p = self.PersonModel(name=\"John\")\n        p.mugshot = self.file1\n        p.save()\n        self.check_dimensions(p, 4, 8)\n        p.name = \"Jane\"\n        p.save()\n        self.check_dimensions(p, 4, 8)\n", "    def test_image_field_dimensions_update_with_post_init_signal(self):\n        # Create a Person instance with an image, this will trigger the\n        # post_init signal and update the dimension fields.\n        person = PersonWithHeightAndWidth.objects.create(\n            name=\"Test Person\",\n            mugshot=self.file1,\n        )\n\n        self.check_dimensions(person, 4, 8)\n", "    def test_save_form_data_clears_field(self):\n        p = self.PersonModel.objects.create()\n        self.assertIsNone(getattr(p, 'mugshot'))\n        form_data = {'mugshot': ''}\n        p.save_form_data(form_data)\n        p.refresh_from_db()\n        self.assertIsNone(getattr(p, 'mugshot'))\n"], "sample_79": ["    def test_pluralize(self):\n        self.check_values(\n            (0, 'votes'),\n            (1, 'vote'),\n            (2, 'votes'),\n\n            # Test with custom suffix\n            (0, 'classes', 'es'),\n            (1, 'class', 'es'),\n            (2, 'classes', 'es'),\n\n            # Test with custom singular/plural suffixes\n            (0, 'candies', 'y,ies'),\n            (1, 'candy', 'y,ies'),\n            (2, 'candies', 'y,ies'),\n        )\n", "    def test_pluralize_with_comma(self):\n        self.check_values(\n            (0, 'classes'),\n            (1, 'class'),\n            (2, 'classes'),\n        )\n", "    def test_pluralize_with_custom_suffix(self):\n        self.check_values(\n            (0, 'classes'),\n            (1, 'class'),\n            (2, 'classes'),\n        )\n", "    def test_pluralize_with_comma(self):\n        self.check_values(\n            (0, 'classes'),\n            (1, 'class'),\n            (2, 'classes'),\n        )\n", "    def test_pluralize(self):\n        self.check_values(\n            (0, 'votes'),\n            (1, 'vote'),\n            (2, 'votes'),\n        )\n"], "sample_1200": ["    def test_get_dimensional_expr(self):\n        u = SI.get_quantity_dimension(meter)\n        assert SI.get_dimensional_expr(meter) == u\n        assert SI.get_dimensional_expr(meter * second) == u * SI.get_quantity_dimension(second)\n        assert SI.get_dimensional_expr(meter/second) == u / SI.get_quantity_dimension(second)\n        assert SI.get_dimensional_expr(meter**2) == u**2\n        assert SI.get_dimensional_expr(meter / (second**2)) == u / SI.get_quantity_dimension(second)**2\n        x = Symbol('x')\n        assert SI.get_dimensional_expr(x*meter) == SI.get_quantity_dimension(meter)\n        assert SI.get_dimensional_expr(diff(x*meter,x)) == SI.get_quantity_dimension(meter)\n\n", "    def test_get_dimensional_expr(self):\n        x = Symbol('x')\n        t = Symbol('t')\n        v = Symbol('v')\n        a = SI.acceleration\n        s = SI.speed\n        m = SI.meter\n\n        # Test basic units\n        assert SI.get_dimensional_expr(m) == Dimension(length)\n\n        # Test derived units\n        assert SI.get_dimensional_expr(a) == Dimension(length) / Dimension(time)**2\n        assert SI.get_dimensional_expr(s) == Dimension(length) / Dimension(time)\n\n        # Test expressions with symbols\n        assert SI.get_dimensional_expr(v * t) == Dimension(length)\n        assert SI.get_dimensional_expr(x**2 / t**3) == Dimension(length)**2 / Dimension(time)**3\n\n        # Test expressions with functions\n        assert SI.get_dimensional_expr(sin(x)) ==Dimension(1)\n        assert SI.get_dimensional_expr(exp(t)) ==Dimension(1)\n", "    def test_get_dimensional_expr(self):\n        x = Symbol('x')\n        v = Symbol('v')\n        t = Symbol('t')\n        a = Symbol('a')\n\n        # Test for simple quantities\n        self.assertEqual(SI.get_dimensional_expr(m), Dimension(length))\n        self.assertEqual(SI.get_dimensional_expr(kg), Dimension(mass))\n        self.assertEqual(SI.get_dimensional_expr(s), Dimension(time))\n\n        # Test for products\n        self.assertEqual(SI.get_dimensional_expr(m*v), Dimension(length*\n                                                                    (length/time)))\n        self.assertEqual(SI.get_dimensional_expr(a*t), Dimension(\n            (length/(time**2)) * time))\n        self.assertEqual(SI.get_dimensional_expr(m*a), Dimension(mass *\n                                                                    (length/(time**2))))\n\n        # Test for powers\n        self.assertEqual(SI.get_dimensional_expr(v**2), Dimension((length/time)**2))\n        self.assertEqual(SI.get_dimensional_expr(t**2), Dimension(time**2))\n\n        # Test for division\n        self.assertEqual(SI.get_dimensional_expr(v/t), Dimension(length/time))\n        self.assertEqual(SI.get_dimensional_expr(m/a), Dimension(mass/(length/(time**2))))\n\n        # Test for functions\n        self.assertEqual(SI.get_dimensional_expr(sin(x)), S.One)\n        self.assertEqual(SI.get_dimensional_expr(exp(t)), S.One)\n\n        # Test for derivatives\n        self.assertEqual(SI.get_dimensional_expr(diff(x*v, t)),\n                         Dimension(length/time))\n        self.assertEqual(SI.get_dimensional_expr(diff(v**2, t)),\n                         Dimension(2*(length**2)/(time**3)))\n\n        # Test for integrals\n        self.assertEqual(SI.get_dimensional_expr(integrate(v, t)),\n                         Dimension((length**2)/time))\n\n", "    def test_derived_units(self):\n        A = SI.derived_units[Dimension(length**2)]\n        assert A == area\n        V = SI.derived_units[Dimension(length**3)]\n        assert V == volume\n\n", "    def test_get_dimensional_expr(self):\n        u = SI.get_quantity_dimension(meter)\n        assert SI.get_dimensional_expr(meter) == u\n        assert SI.get_dimensional_expr(Quantity('x', 'm')) == u\n\n        assert SI.get_dimensional_expr(speed_of_light * meter / second) ==  length / time\n        assert SI.get_dimensional_expr(speed_of_light ** 2) == length**2 / time**2\n        assert SI.get_dimensional_expr(kilo*meter) == u\n"], "sample_150": ["    def test_check(self):\n        with mock.patch('django.core.management.base.connections') as mock_connections:\n            mock_connections.databases = {'default': 'sqlite', 'other': 'mysql'}\n            cmd = CommandError\n            cmd.check()\n            mock_connections.close_all.assert_called_once_with()\n            self.assertEqual(cmd.stderr.getvalue(), '')\n", "    def test_check_migrations_no_migrations(self):\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.migration_plan', return_value=None):\n            from django.core.management.base import BaseCommand\n            command = BaseCommand()\n            command.check_migrations()\n            self.assertEqual(command.stdout.getvalue(), '')\n", "    def test_check_migrations_no_migrations(self):\n        \"\"\"\n        Test check_migrations() when there are no migrations to apply.\n        \"\"\"\n        with mock.patch('django.db.migrations.executor.MigrationExecutor') as mock_executor:\n            mock_executor.return_value.migration_plan.return_value = []\n            executor = mock_executor.return_value\n            command = BaseCommand()\n            command.check_migrations()\n\n            mock_executor.assert_called_once_with(connection.alias)\n            executor.migration_plan.assert_called_once_with(executor.loader.graph.leaf_nodes())\n            self.assertEqual(command.stdout.getvalue(), '')\n\n", "    def test_check_migrations_no_migrations(self):\n        with mock.patch('django.db.migrations.executor.MigrationExecutor') as mock_executor:\n            mock_executor.return_value.migration_plan.return_value = []\n            command = Command()\n            command.check_migrations()\n            self.assertFalse(command.stdout.write.called)\n", "    def test_check_database_backends(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            check_database_backends(connection.settings_dict)\n            mock_check.assert_called_once_with(connection.settings_dict)\n"], "sample_1081": ["    def test_is_mersenne_prime(self):\n        self.assertTrue(is_mersenne_prime(3))\n        self.assertTrue(is_mersenne_prime(7))\n        self.assertTrue(is_mersenne_prime(31))\n        self.assertFalse(is_mersenne_prime(6))\n        self.assertFalse(is_mersenne_prime(12))\n        self.assertFalse(is_mersenne_prime(20))\n\n", "    def test_multiproduct():\n        assert multiproduct({3:7, 2:5}, 4) == 279936\n        assert multiproduct([(3, 7), (2, 5)], 4) == 279936\n        assert multiproduct({3:7, 2:5}) == 279936\n        assert multiproduct([(3, 7), (2, 5)]) == 279936\n        assert multiproduct([]) == 1\n        assert multiproduct([]) == 1\n", "    def test_is_amicable(self):\n        assert is_amicable(220, 284)\n        assert is_amicable(1184, 1210)\n        assert is_amicable(2620, 2924)\n        assert not is_amicable(10, 12)\n", "def test_is_deficient():\n    assert is_deficient(15)\n    assert is_deficient(1)\n    assert not is_deficient(20)\n    assert not is_deficient(12)\n", "    def test_is_deficient(self):\n        self.assertTrue(is_deficient(15))\n        self.assertFalse(is_deficient(20))\n"], "sample_926": ["    def test_cpp_enum_scoped(self):\n        check('enum', 'enum class MyEnum {{ A, B, C }};',\n              {1: 'enum_MyEnum', 2: 'MyEnum'},\n              key='MyEnum',\n              output='enum class MyEnum { { A, B, C } };',\n              asTextOutput='MyEnum')\n", "    def test_cpp_enum_unnamed():\n        check('enum', 'enum { a, b, c }',\n            idDict={1: 'enum_unnamed_a', 2: 'unnamed_enum_a'},\n              output=\n              'enum { a, b, c }')\n\n", "def test_enum_identifier():\n    check('enum',\n          \"enum class {key}MyEnum {{ VALUE1, VALUE2 }}\",\n          {},\n          \"enum class MyEnum {{ VALUE1, VALUE2 }}\",\n          key='MyEnum',\n          asTextOutput=\"MyEnum\")\n", "def test_cpp_type_reference():\n\n    check('type', 'MyType',\n          idDict={1: 'MyType'},\n          output='MyType')\n\n", "    def test_cpp_namespace_push_pop(app, status, warning):\n        app.builder.build_all()\n\n        # First, test a plain function\n        content = \"\"\""], "sample_578": ["    def test_bars_width_mapping(self):\n        x = np.array([1, 2, 3])\n        y = np.array([2, 4, 6])\n        df = pd.DataFrame({\"x\": x, \"y\": y})\n\n        bars = self.plot_bars(\n            {\"x\": \"x\", \"y\": \"y\"}, {\"width\": \"y\"}, {\"data\": df}\n        )\n\n        assert_array_equal(\n            [bar.get_width() for bar in bars], [2, 4, 6]\n        )\n", "    def test_bar_edgewidth_auto(self):\n\n        data = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [4, 5, 6]})\n\n        bars = self.plot_bars(\n            {\"x\": \"x\", \"y\": \"y\"},\n            {\"edgewidth\": {\"auto\": True}},\n            {},\n        )\n\n        # Check edgewidths based on bar widths\n        for bar in bars:\n            width = bar.get_width()\n            assert bar.get_linewidth() == pytest.approx(0.1 * width)\n\n", "    def test_bars_width_mapping(self):\n\n        data = pd.DataFrame({'x': [1, 2, 3], 'y': [10, 20, 30]})\n        \n        bars = self.plot_bars(\n            variables={'x': 'x', 'y': 'y'},\n            mark_kws={'width': 'x'},\n            layer_kws={ 'height': 5}\n        )\n\n        self.check_bar(bars[0], 1, 0, 1, 10)\n        self.check_bar(bars[1], 2, 0, 2, 20)\n        self.check_bar(bars[2], 3, 0, 3, 30)\n", "    def test_bars_edgewidth_auto(self):\n\n        df = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [2, 4, 6]})\n\n        bars = self.plot_bars(\n            {\"x\": \"x\", \"y\": \"y\"},\n            {\"width\": .8, \"edgewidth\": \"auto\"},\n            {},\n        )\n\n        for bar in bars:\n            assert bar.get_linewidth() > 0\n", "    def test_bar_baseline(self):\n        data = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\n        bars = self.plot_bars({'x': 'x', 'y': 'y'}, {'baseline': 2}, {})\n\n        for bar, y in zip(bars, data['y']):\n            self.check_bar(bar, bar.get_x(), 2, 0.8, y - 2)\n"], "sample_182": ["    def test_select_related_prefetch_related_with_select_raw(self):\n        # Regression test for #29563:\n        # Ensure that prefetching works correctly with select_related when using\n        # raw SQL queries as the basis for a QuerySet.\n\n        # Select numbers with even 'num' values, prefetching the related\n        # ReservedName object.\n        qs = Number.objects.raw(\n            'SELECT * FROM queryset_set_operation_tests_number WHERE num % 2 = 0'\n        ).prefetch_related('reservedname_set').select_related('reservedname')\n\n        # Ensure that the related objects are populated correctly.\n        for number in qs:\n            self.assertEqual(number.reservedname_set.count(), 0)\n\n\n\n", "    def test_select_related_with_prefetch_related(self):\n        qs = Number.objects.select_related('id').prefetch_related('numbers')\n        self.assertQuerysetEqual(qs, Number.objects.all(), lambda x: x.id)\n", "    def test_select_related_with_union(self):\n        # Regression test for #28742\n        qs1 = Number.objects.select_related('related').filter(num=1)\n        qs2 = Number.objects.select_related('related').filter(num=2)\n        combined = qs1.union(qs2)\n        numbers = list(combined)\n        self.assertEqual(len(numbers), 2)\n        for num in numbers:\n            self.assertEqual(num.related, num.related)\n\n", "    def test_select_related_m2m_already_selected(self):\n        # Test a scenario where a m2m field is already selected\n        # due to a parent model's select_related().\n        r = ReservedName.objects.select_related('number').all().first()\n        self.assertEqual(r.number.num, 0)\n", "    def test_prefetch_related_with_empty_queryset(self):\n        # Regression test for #24774\n        empty_queryset = Number.objects.filter(num__gt=100)\n        self.assertQuerysetEqual(\n            Number.objects.prefetch_related('reservedname').filter(pk__in=empty_queryset),\n            [],\n        )\n"], "sample_848": ["    def test_multioutput_regressor_with_sparse_data(self):\n        # test MultiOutputRegressor with sparse data\n        X, y = make_classification(n_samples=50, n_features=10,\n                                   random_state=0,\n                                   n_informative=5, n_classes=3)\n        y = np.array([[i, i ** 2, i ** 3] for i in y])\n        X = sp.csr_matrix(X)\n        regressor = MultiOutputRegressor(Ridge())\n        regressor.fit(X, y)\n        y_pred = regressor.predict(X)\n        assert_array_almost_equal(y_pred, y, decimal=2)\n\n", "    def test_classifier_chain_partial_fit(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_classes=2, random_state=0)\n        chain = ClassifierChain(LogisticRegression())\n        chain.partial_fit(X[:50], y[:50])\n        chain.partial_fit(X[50:], y[50:])\n        assert_array_equal(chain.predict(X),\n                           MultiOutputClassifier(LogisticRegression()).fit(X,\n                                                                        y).predict(X))\n\n", "def test_multioutput_classifier_partial_fit():\n    # Test partial_fit for MultiOutputClassifier\n    X, y = make_classification(n_samples=100, n_features=10,\n                               n_informative=5, n_classes=3,\n                               random_state=0)\n    n_outputs = y.shape[1]\n    estimator = LogisticRegression()\n    multi_clf = MultiOutputClassifier(estimator)\n    # Check if partial_fit raises error when not fitted\n    with pytest.raises(NotFittedError):\n        multi_clf.predict(X[:10])\n\n    # Partial fit on the first half of the data\n    multi_clf.partial_fit(X[:50], y[:50])\n\n    # Check if predict works even with partial fit\n    y_pred = multi_clf.predict(X[50:])\n\n    assert y_pred.shape == (50, n_outputs)\n\n    # Check if partial_fit can be called again\n    multi_clf.partial_fit(X[50:], y[50:])\n    y_pred_2 = multi_clf.predict(X)\n\n    # Assert predictions are consistent\n    assert_array_equal(np.argmax(y_pred_2, axis=1), np.argmax(y, axis=1))\n", "    def test_multioutputclassifier_predict_proba():\n        # Test predict_proba for MultiOutputClassifier.\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_informative=5, n_classes=3,\n                                   random_state=0)\n        clf = MultiOutputClassifier(LogisticRegression(random_state=0))\n        clf.fit(X, y)\n        y_proba = clf.predict_proba(X)\n        # check that the output shape is (n_samples, n_classes)\n        assert y_proba.shape == (100, 3)\n\n", "    def test_multioutputregressor_fit_predict_sparse(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_informative=5, n_classes=3, random_state=42)\n        X = sp.csr_matrix(X)\n        multioutput_regressor = MultiOutputRegressor(\n            LinearSVC(random_state=42), n_jobs=1\n        )\n\n        multioutput_regressor.fit(X, y)\n        y_pred = multioutput_regressor.predict(X)\n\n        assert_array_equal(y_pred.shape, y.shape)\n\n\n\n"], "sample_1090": ["    def test_imaginary_unit():\n        assert S.I*S.I == -1\n        assert 1/S.I == -S.I\n", "def test_imaginary_unit():\n    assert S.ImaginaryUnit**2 == -1\n    assert S.ImaginaryUnit.as_base_exp() == (S.NegativeOne, S.Half)\n\n    assert (1 + S.ImaginaryUnit) * (1 - S.ImaginaryUnit) == 2\n", "    def test_imaginary_unit_power():\n        assert S.ImaginaryUnit**0 == 1\n        assert S.ImaginaryUnit**1 == S.ImaginaryUnit\n        assert S.ImaginaryUnit**2 == -1\n        assert S.ImaginaryUnit**3 == -S.ImaginaryUnit\n        assert S.ImaginaryUnit**4 == 1\n", "    def test_imaginary_unit():\n        assert S.ImaginaryUnit * S.ImaginaryUnit == -1\n        assert 1 / S.ImaginaryUnit == -S.ImaginaryUnit\n        assert S.ImaginaryUnit.is_imaginary == True\n", "    def test_GoldenRatio():\n        assert S.GoldenRatio.is_irrational is True\n        assert S.GoldenRatio > 1\n\n"], "sample_313": ["    def test_watch_for_template_changes(self):\n        autoreload.autoreload_started(None, None)\n        expected_dirs = [EXTRA_TEMPLATES_DIR]\n        with mock.patch('django.template.autoreload.get_template_directories') as mock_get_dirs:\n            mock_get_dirs.return_value = expected_dirs\n            autoreload.watch_for_template_changes(None, None)\n            mock_get_dirs.assert_called_once()\n            for dir in expected_dirs:\n                autoreload.watched_dirs[0].watch_dir.assert_any_call(dr, '**/*')\n", "    def test_watch_for_template_changes_is_called(\n        self, mock_watch_for_template_changes, mock_get_template_directories", "    def test_watch_for_template_changes(self):\n        with mock.patch('django.template.autoreload.get_template_directories') as mock_get_template_dirs:\n            mock_get_template_dirs.return_value = {EXTRA_TEMPLATES_DIR}\n            autoreload.watch_for_template_changes(None)\n            mock_get_template_dirs.assert_called_once()\n            self.assertEqual(len(autoreload._watched_dirs), 1)\n            self.assertIn(EXTRA_TEMPLATES_DIR, autoreload._watched_dirs)\n", "    def test_template_changed_triggers_reload(self):\n        mock_reset_loaders = self.patcher.mock\n        file_path = EXTRA_TEMPLATES_DIR / 'test.html'\n        autoreload.file_changed(file_path)\n        mock_reset_loaders.assert_called_once()\n", "    def test_watch_for_template_changes(self):\n        autoreload.autoreload_started()\n\n"], "sample_212": ["    def test_session_middleware(self):\n        request = HttpRequest()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'session_key'}\n        response = HttpResponse()\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        middleware.process_response(request, response)\n", "    def test_session_middleware_empty_session(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.COOKIES = {settings.SESSION_COOKIE_NAME: 'some-session-key'}\n        # Simulate empty session\n        request.session = self.SessionStore('')\n        middleware = SessionMiddleware()\n        middleware.process_response(request, response)\n        self.assertTrue(settings.SESSION_COOKIE_NAME not in response.cookies)\n", "    def test_session_middleware_empty(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.session = self.SessionStore('')\n        middleware = SessionMiddleware()\n        middleware.process_response(request, response)\n        self.assertNotIn(settings.SESSION_COOKIE_NAME, response.cookies)\n\n", "    def test_session_middleware_empty_session(self):\n        request = HttpRequest()\n        response = HttpResponse()\n        request.COOKIES[settings.SESSION_COOKIE_NAME] = 'test_session_key'\n        session_middleware = SessionMiddleware()\n        session_middleware.process_request(request)\n        request.session.clear()\n        response = session_middleware.process_response(request, response)\n        self.assertTrue(settings.SESSION_COOKIE_NAME not in response.cookies)\n\n", "    def test_session_middleware_empty_session(self):\n        request = HttpRequest()\n        response = HttpResponse()\n\n        middleware = SessionMiddleware()\n        middleware.process_request(request)\n        middleware.process_response(request, response)\n\n        self.assertFalse(settings.SESSION_COOKIE_NAME in response.cookies)\n\n"], "sample_467": ["    def test_value_from_datadict_empty_values(self):\n        data = {\n            \"date_form_year\": \"\",\n            \"date_form_month\": \"\",\n            \"date_form_day\": \"\",\n        }\n        form = Form({'date_form': DateField(widget=self.widget)})\n        self.assertEqual(form.fields['date_form'].widget.value_from_datadict(data, {}, 'date_form'), None)\n", "    def test_value_from_datadict_empty(self):\n        data = {'form-0-year': '', 'form-0-month': '', 'form-0-day': ''}\n        widget = SelectDateWidget()\n        self.assertIsNone(widget.value_from_datadict(data, {}, 'form-0'))\n", "    def test_empty_values(self):\n        self.check_html(\n            self.widget.render(\"mydate\", None),\n            '<select name=\"mydate_year\" id=\"id_mydate_year\"><option value=\"\">---</option>'\n            '<option value=\"2007\">2007</option>'\n            '<option value=\"2008\">2008</option>'\n            '<option value=\"2009\">2009</option>'\n            '<option value=\"2010\">2010</option>'\n            '<option value=\"2011\">2011</option>'\n            '<option value=\"2012\">2012</option>'\n            '<option value=\"2013\">2013</option>'\n            '<option value=\"2014\">2014</option>'\n            '<option value=\"2015\">2015</option>'\n            '<option value=\"2016\">2016</option></select>'\n            '<select name=\"mydate_month\" id=\"id_mydate_month\"><option value=\"\">---</option>'\n            '<option value=\"1\">January</option>'\n            '<option value=\"2\">February</option>'\n            '<option value=\"3\">March</option>'\n            '<option value=\"4\">April</option>'\n            '<option value=\"5\">May</option>'\n            '<option value=\"6\">June</option>'\n            '<option value=\"7\">July</option>'\n            '<option value=\"8\">August</option>'\n            '<option value=\"9\">September</option>'\n            '<option value=\"10\">October</option>'\n            '<option value=\"11\">November</option>'\n            '<option value=\"12\">December</option></select>'\n            '<select name=\"mydate_day\" id=\"id_mydate_day\"><option value=\"\">---</option>'\n            '<option value=\"1\">1</option>'\n            '<option value=\"2\">2</option>'\n            '<option value=\"3\">3</option>'\n            '<option value=\"4\">4</option>'\n            '<option value=\"5\">5</option>'\n            '<option value=\"", "    def test_select_date_widget_with_empty_label(self):\n        form = Form({'date': ''})\n        widget = SelectDateWidget(empty_label='---')\n        field = DateField(widget=widget)\n        self.assertEqual(widget.render(None, 'date'),\n                         '<div class=\"date_input\"><select name=\"date_year\" id=\"id_date_year\"><option value=\"\">---</option><option value=\"2007\">2007</option><option value=\"2008\">2008</option><option value=\"2009\">2009</option><option value=\"2010\">2010</option><option value=\"2011\">2011</option><option value=\"2012\">2012</option><option value=\"2013\">2013</option><option value=\"2014\">2014</option><option value=\"2015\">2015</option><option value=\"2016\">2016</option></select><select name=\"date_month\" id=\"id_date_month\"><option value=\"\">---</option><option value=\"1\">January</option><option value=\"2\">February</option><option value=\"3\">March</option><option value=\"4\">April</option><option value=\"5\">May</option><option value=\"6\">June</option><option value=\"7\">July</option><option value=\"8\">August</option><option value=\"9\">September</option><option value=\"10\">October</option><option value=\"11\">November</option><option value=\"12\">December</option></select><select name=\"date_day\" id=\"id_date_day\"><option value=\"\">---</option><option value=\"1\">1</option><option value=\"2\">2</option><option value=\"3\">3</option><option value=\"4\">4</option><option value=\"5\">5</option><option value=\"6\">6</option><option value=\"7\">7</option><option value=\"8\">8</option><option value=\"9\">9</option><option value=\"10\">10</option><option value=\"11\">11", "    def test_empty_values(self):\n        form = self.get_form({'mydate_year': '', 'mydate_month': '', 'mydate_day': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['mydate'], ['This field is required.'])\n\n"], "sample_267": ["    def test_regexp(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE RegexpTest (text TEXT)\")\n            cursor.execute(\"INSERT INTO RegexpTest (text) VALUES ('hello')\")\n            cursor.execute(\"SELECT * FROM RegexpTest WHERE text REGEXP 'hello'\")\n            self.assertEqual(cursor.fetchall(), [('hello',)])\n\n            cursor.execute(\"SELECT * FROM RegexpTest WHERE text REGEXP 'world'\")\n            self.assertEqual(cursor.fetchall(), [])\n", "    def test_sqlite_datetime_trunc(self):\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT django_date_trunc(\"year\", \"2023-10-26\")')\n            self.assertEqual(cursor.fetchone()[0], '2023-01-01')\n\n            cursor.execute('SELECT django_date_trunc(\"quarter\", \"2023-10-26\")')\n            self.assertEqual(cursor.fetchone()[0], '2023-10-01')\n\n            cursor.execute('SELECT django_date_trunc(\"month\", \"2023-10-26\")')\n            self.assertEqual(cursor.fetchone()[0], '2023-10-01')\n\n            cursor.execute('SELECT django_date_trunc(\"week\", \"2023-10-26\")')\n            self.assertEqual(cursor.fetchone()[0], '2023-10-23')\n\n            cursor.execute('SELECT django_date_trunc(\"day\", \"2023-10-26\")')\n            self.assertEqual(cursor.fetchone()[0], '2023-10-26')\n\n            cursor.execute('SELECT django_time_trunc(\"hour\", \"10:30:05\")')\n            self.assertEqual(cursor.fetchone()[0], '10:00:00')\n\n            cursor.execute('SELECT django_time_trunc(\"minute\", \"10:30:05\")')\n            self.assertEqual(cursor.fetchone()[0], '10:30:00')\n\n            cursor.execute('SELECT django_time_trunc(\"second\", \"10:30:05\")')\n            self.assertEqual(cursor.fetchone()[0], '10:30:05')\n", "    def test_sqlite_regexp(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT django_regexp('abc', '^a')\")\n            self.assertTrue(cursor.fetchone()[0])\n            cursor.execute(\"SELECT django_regexp('abc', '^z')\")\n            self.assertFalse(cursor.fetchone()[0])\n", "    def test_datetime_trunc_with_timezone(self):\n        with self.settings(TIME_ZONE='America/New_York'):\n            with transaction.atomic():\n                dt = timezone.make_naive(datetime(2023, 1, 1, 12, 0, 0), UTC)\n                Item.objects.create(name='item', created_at=dt)\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"SELECT django_date_trunc('day', created_at) FROM core_item\"\"\"\n            )\n            result = cursor.fetchone()[0]\n\n        self.assertEqual(result, '2023-01-01')\n", "    def test_aggregate(self):\n        \"\"\"\n        Tests aggregations on SQLite\n        \"\"\"\n        Item.objects.create(name='Widget A', price=10.0)\n        Item.objects.create(name='Widget B', price=20.0)\n\n            \n        with mock.patch('django.db.backends.sqlite3.base.DatabaseWrapper') as mock_wrapper:\n            mock_wrapper.return_value.connection.isolation_level = ''\n            with transaction.atomic():\n                \n                avg_price = Item.objects.aggregate(avg_price=Avg('price'))\n                self.assertEqual(avg_price['avg_price'], 15.0)\n\n                sum_price = Item.objects.aggregate(sum_price=Sum('price'))\n                self.assertEqual(sum_price['sum_price'], 30.0)\n\n                variance_price = Item.objects.aggregate(variance_price=Variance('price'))\n                self.assertEqual(variance_price['variance_price'], 50.0)\n\n                std_dev_price = Item.objects.aggregate(std_dev_price=StdDev('price'))\n                self.assertEqual(std_dev_price['std_dev_price'], 7.0710678118654755)\n\n"], "sample_868": ["    def test_fowlkes_mallows_score_empty(self):\n        assert fowlkes_mallows_score([], []) == 1.0\n", "    def test_silhouette_score_empty_clusters():\n        # Check if silhouette_score handles empty clusters gracefully\n        labels = np.array([0, 0, 1, 2, 2, 2])\n        X = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n        with pytest.warns(UserWarning):\n            silhouette_score(X, labels)\n", "    def test_silhouette_score_errors(self):\n        X = np.array([[1, 2], [1, 2], [2, 3], [3, 4]])\n        labels = np.array([0, 0, 1, 1])\n\n        # Test case: Labels should be 1 dimensional\n        with pytest.raises(ValueError):\n            silhouette_score(X, labels.reshape(-1, 1))\n\n        # Test case: Labels should be integers\n        with pytest.raises(TypeError):\n            silhouette_score(X, labels.astype(float))\n\n        # Test case: Metric should be string or callable\n        with pytest.raises(TypeError):\n            silhouette_score(X, labels, metric=1)\n\n", "def test_silhouette_score_empty_labels():\n    # Test silhouette_score with empty labels\n    with pytest.raises(ValueError):\n        silhouette_score([], [])\n    with pytest.raises(ValueError):\n        silhouette_score([0, 1, 2], [])\n", "    def test_silhouette_score_zero_samples(self):\n        X = np.zeros((0, 2))\n        labels = np.array([])\n        assert_allclose(silhouette_score(X, labels), 0.0)\n"], "sample_57": ["    def test_usercreationform_invalid_password(self):\n        form = UserCreationForm(data={'username': 'newuser', 'password1': 'password', 'password2': 'wrong'})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password2'], [_(\n            \"The two password fields didn't match.\")])\n\n", "    def test_create_user_with_empty_password(self):\n        form_data = {'username': 'emptypassworduser', 'password1': '', 'password2': ''}\n        form = UserCreationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['password1'], [\n            _(\"This field is required.\")\n\n        ])\n", "    def test_user_creation_form_with_custom_email_field(self):\n        class CustomEmailUserCreationForm(UserCreationForm):\n            class Meta:\n                model = CustomEmailField\n                fields = ('username', 'email')\n        \n        form = CustomEmailUserCreationForm()\n        self.assertIn('username', form.fields)\n        self.assertIn('email', form.fields)\n        self.assertEqual(form.fields['email'].widget.__class__, forms.EmailInput)\n", "    def test_create_user_with_custom_username_field(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.with_integer_username.IntegerUsernameUser'):\n            form = UserCreationForm({'username': 12345, 'password1': 'password', 'password2': 'password'})\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertIsInstance(user, IntegerUsernameUser)\n            self.assertEqual(user.username, 12345)\n", "    def test_save_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.custom_user.CustomUser'):\n            data = {'username': 'newuser', 'password1': 'newPassword', 'password2': 'newPassword'}\n            form = UserCreationForm(data=data)\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertEqual(user.username, 'newuser')\n            self.assertTrue(user.check_password('newPassword'))\n            self.assertIsInstance(user, CustomUser)\n"], "sample_429": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_336": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_279": ["    def test_checkconstraint_remove_sql(self):\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Product.objects.create(name=\"test\", price=10)\n        constraint = CheckConstraint(name='test_constraint', check=Q(price__gt=0))\n        Product._meta.constraints.append(constraint)\n        \n        sql = constraint.remove_sql(Product, connection.introspection)\n\n        with mock.patch('django.db.backends.utils.execute_sql') as mock_execute_sql:\n            mock_execute_sql.return_value = None\n            with atomic():\n                with connection.cursor() as cursor:\n                    cursor.execute(sql)\n\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertNotIn('test_constraint', [c[\"name\"] for c in constraints])\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            product = UniqueConstraintConditionProduct.objects.create(\n                name='Product A',\n                price=10,\n                status='available',\n            )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='Product B',\n                    price=10,\n                    status='available',\n                )\n            product_2 = UniqueConstraintConditionProduct.objects.create(\n                name='Product B',\n                price=10,\n                status='out_of_stock',\n            )\n\n            self.assertEqual(\n                UniqueConstraintConditionProduct.objects.count(), 2,\n            )\n\n", "    def test_check_constraint_remove(self):\n        with atomic():\n            Product.objects.create(name='foo', price=10)\n            constraint = CheckConstraint(check=Q(price__gt=0), name='positive_price')\n            constraint.create_sql(Product, connection.schema_editor())\n        with self.assertRaises(IntegrityError):\n            with atomic():\n                Product.objects.create(name='bar', price=-10)\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertTrue(any(c['name'] == 'positive_price' for c in constraints))\n        constraint.remove_sql(Product, connection.schema_editor())\n        constraints = get_constraints(Product._meta.db_table)\n        self.assertFalse(any(c['name'] == 'positive_price' for c in constraints))\n", "    def test_check_constraint_deconstruction(self):\n        constraint = CheckConstraint(name='test_constraint', check=models.Q(price__gt=10))\n        path, args, kwargs = constraint.deconstruct()\n        self.assertEqual(path, 'django.db.models.constraints.CheckConstraint')\n        self.assertEqual(args, ())\n        self.assertEqual(kwargs, {'name': 'test_constraint', 'check': models.Q(price__gt=10)})\n", "    def test_unique_constraint_create_and_drop(self):\n        with self.subTest('create'):\n            with atomic():\n                product = UniqueConstraintProduct.objects.create(\n                    name='Test product',\n                    description='Test description',\n                )\n                self.assertIsNotNone(product.pk)\n                constraints = get_constraints(\n                    UniqueConstraintProduct._meta.db_table\n                )\n                self.assertIn(\n                    'unique_constraint_product_name_description', constraints\n                )\n        with self.subTest('drop'):\n            with atomic():\n                UniqueConstraintProduct.objects.all().delete()\n                constraints = get_constraints(\n                    UniqueConstraintProduct._meta.db_table\n                )\n                self.assertNotIn(\n                    'unique_constraint_product_name_description', constraints\n                )\n"], "sample_382": ["    def test_template_changed(self, mock_reset_loaders):\n        sender = mock.Mock()\n        file_path = EXTRA_TEMPLATES_DIR / 'test.html'\n        autoreload.template_changed(\n            sender,\n            file_path=file_path,\n        )\n        mock_reset_loaders.assert_called_once()\n\n", "    def test_template_changed_triggers_reset_loaders(self, mock_get_template_directories, mock_reset_loaders):\n        mock_get_template_directories.return_value = {EXTRA_TEMPLATES_DIR}\n        autoreload.file_changed(None, str(EXTRA_TEMPLATES_DIR / 'test.html'))\n        mock_reset_loaders.assert_called_once()\n", "    def test_template_changed_resets_loaders(self):\n        mock_reset_loaders = mock.patch('django.template.autoreload.reset_loaders')\n        with mock_reset_loaders as mock_reset:\n            template_path = EXTRA_TEMPLATES_DIR / 'test.html'\n            template_changed(None, file_path=template_path)\n            mock_reset.assert_called_once()\n", "    def test_watch_for_template_changes(self):\n        with mock.patch('django.template.autoreload.get_template_directories') as mock_get_dir:\n            mock_get_dir.return_value = {EXTRA_TEMPLATES_DIR}\n            autoreload.autoreload_started('test')\n            mock_get_dir.assert_called_once()\n", "    def test_watch_for_template_changes(self, mock_get_template_directories, mock_reset_loaders):\n        mock_get_template_directories.return_value = {EXTRA_TEMPLATES_DIR}\n        autoreload.autoreload_started(None)\n        mock_get_template_directories.assert_called_once()\n        mock_reset_loaders.assert_not_called()\n"], "sample_960": ["def test_parse_annotation():\n    assert_node(\n        _parse_annotation(\"x: int\"),\n        desc_annotation('int', 'x:'),\n    )\n    assert_node(\n        _parse_annotation(\"x: typing.List[int]\"),\n        desc_annotation('typing.List[int]', 'x:'),\n    )\n    assert_node(\n        _parse_annotation(\"x: typing.List[int] = []\"),\n        desc_annotation('typing.List[int]', 'x:'),\n    )\n\n\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int'), ('int', None))\n        self.assertEqual(_parse_annotation('str'), ('str', None))\n        self.assertEqual(_parse_annotation('list[int]'), ('list', ('int',)))\n        self.assertEqual(_parse_annotation('Dict[str, int]'), ('Dict', ('str', 'int')))\n        self.assertEqual(_parse_annotation('Callable[[int], float]'), ('Callable', ((\n            'int',), 'float')))\n\n        with pytest.raises(ValueError):\n            _parse_annotation('invalid')\n", "def test_parse_annotation():\n    assert _parse_annotation('a: int') == ('a', 'int')\n    assert _parse_annotation('a:str') == ('a', 'str')\n    assert _parse_annotation('a: List[int]') == ('a', 'List[int]')\n    assert _parse_annotation('a: typing.List[int]') == ('a', 'typing.List[int]')\n\n    with pytest.raises(ValueError):\n        _parse_annotation(': int')\n", "    def test_parse_sig(sig, expected):\n        assert parse(sig) == expected\n", "def test_parse_annotation():\n    assert_node(_parse_annotation('int'), addnodes.desc_annotation, 'int', '', nodes.Text('int'))\n    assert_node(_parse_annotation('List[str]'), addnodes.desc_annotation, 'List[str]', '', nodes.Text('List[str]'))\n    assert_node(_parse_annotation('Optional[int]'), addnodes.desc_annotation, 'Optional[int]', '',\n                 nodes.Text('Optional[int]'))\n    assert_node(_parse_annotation('typing.Union[int, str]'), addnodes.desc_annotation, 'typing.Union[int, str]', '',\n                 nodes.Text('typing.Union[int, str]'))\n    assert_node(_parse_annotation('Dict[str, Any]'), addnodes.desc_annotation, 'Dict[str, Any]', '',\n                 nodes.Text('Dict[str, Any]'))\n"], "sample_43": ["    def test_regular_events_varying_dt():\n        np.random.seed(42)\n        t = np.arange(1000) * 0.1\n        x = np.zeros_like(t)\n        x[np.random.randint(0, len(t), len(t) // 10)] = 1\n        for dt in [0.1, 0.2, 0.5]:\n            edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n            assert len(edges) > 1\n", "def test_regular_events_repeat():\n    dt = 0.05\n    t = dt * np.arange(1000)\n    x = np.zeros(len(t))\n    x[np.random.randint(0, len(t), len(t) // 10)] = 1\n    # deliberately repeat some events\n    x[200:210] = 1\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n    assert len(edges) > 10  # make sure we find more than one block\n", "    def test_regular_events_repeated_times():\n        t = np.array([0, 0, 1, 2, 2, 2, 3, 4])\n        x = np.array([1, 0, 1, 1, 1, 0, 0, 1])\n        dt = 1\n\n        edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n        assert_allclose(edges, [0, 1, 2, 3, 4, 5])\n", "    def test_regular_events_empty_block(self):\n        # Test case where an empty block is possible\n        t = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n        x = np.array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0])\n\n        dt = 1\n        edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt)\n        assert_allclose(edges, [0, 1, 6, 9])\n", "def test_regular_events():\n    t = np.arange(100) * 0.1\n    x = np.zeros_like(t)\n    x[::5] = 1\n\n    edges = bayesian_blocks(t, x, fitness='regular_events', dt=0.5)\n    assert_allclose(edges, [0., 0.5, 5., 5.5, 10.])  # Note: only checking a subset of edges here\n\n"], "sample_509": ["    def test_locator_tick_values(self, locator, expected_ticks):\n        # Test that tick_values returns the expected tick values\n        start_date = datetime.datetime(2023, 1, 1)\n        end_date = datetime.datetime(2024, 1, 1)\n\n        ticks = locator.tick_values(mdates.date2num(start_date),\n                                   mdates.date2num(end_date))\n        assert np.allclose(ticks, expected_ticks)\n", "    def test_tick_values_with_interval(self, locator_class, expected_ticks):\n        dmin = datetime.datetime(2023, 10, 26, 0, 0, 0)\n        dmax = datetime.datetime(2023, 10, 26, 1, 0, 0)\n        locator = locator_class(interval=15)\n        ticks = locator.tick_values(dmin, dmax)\n        assert all(tick in expected_ticks for tick in np.round(ticks))\n", "    def test_concise_date_formatter_formats(self, fmt, expected):\n        dt = datetime.datetime(2023, 10, 27, 12, 0, 0)\n        formatter = mdates.ConciseDateFormatter(mdates.AutoDateLocator(),\n                                               formats=[fmt])\n        assert formatter.format_data(dt) == expected\n", "    def test_concise_date_formatter_with_formats():\n        # Test ConciseDateFormatter with custom formats for different date ranges\n        fig, ax = plt.subplots()\n\n        dates = [\n            datetime.datetime(2023, 1, 1),\n            datetime.datetime(2023, 1, 15),\n            datetime.datetime(2023, 2, 1),\n            datetime.datetime(2023, 2, 15),\n            datetime.datetime(2024, 1, 1),\n        ]\n\n        ax.plot(dates, [1] * len(dates))\n\n        fmt = ConciseDateFormatter(\n            formats=[\"%Y-%m-%d\", \"%b %d\", \"%Y\"],\n            zero_formats=[\"%Y\", \"%b\", \"%Y\"],\n            offset_formats=[\"%Y-%m-%d\", \"%b %d\", \"%Y\"],\n            show_offset=False,\n        )\n        ax.xaxis.set_major_formatter(fmt)\n        fig.autofmt_xdate()\n\n        # Assert that the date labels are formatted according to the provided\n        # formats.\n        # (This is a bit tricky to automate, so for now it's just manual\n        # inspection)\n\n        return fig\n\n\n\n", "    def test_date2num_tz(self, tzinfo):\n        dt = datetime.datetime(2000, 1, 1, 12, 0, 0, tzinfo=tzinfo)\n        assert mdates.date2num(dt) == mdates.date2num(\n            dt.astimezone(dateutil.tz.gettz('UTC'))\n        )\n"], "sample_237": ["    def test_user_model_required_fields_contains_username_field(self):\n        with self.assertRaises(checks.Error):\n            check_user_model(app_configs=self.apps)\n", "    def test_user_model_required_fields_empty_list(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserEmptyRequiredFields'):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 0)\n", "    def test_required_fields_is_empty_list(self):\n        class CustomUserEmptyRequiredFields(AbstractBaseUser):\n            username = models.CharField(max_length=30, unique=True)\n            email = models.EmailField(unique=True)\n\n            USERNAME_FIELD = 'username'\n            REQUIRED_FIELDS = []\n\n            objects = models.Manager()\n\n        with isolate_apps('auth_tests'):\n            with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserEmptyRequiredFields'):\n                errors = check_user_model()\n                self.assertEqual(len(errors), 0)\n", "    def test_user_model_required_fields_not_list(self):\n        with isolate_apps('auth_tests', attr_name='apps'):\n            with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserRequiredFieldsNotList'):\n                errors = check_user_model()\n                self.assertEqual(len(errors), 1)\n                self.assertEqual(errors[0].id, 'auth.E001')\n", "    def test_required_fields_tuple(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserTupleRequiredFields'):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertListEqual(errors, [])\n"], "sample_172": ["    def test_formfield_for_dbfield_empty_choices(self):\n        ff = self.assertFormfield(\n            model=School,\n            fieldname='head',\n            widgetclass=forms.Select,\n            # Ensure no choices are presented for an empty ManyToManyField\n        )\n        self.assertEqual(ff.choices, [])\n", "    def test_formfield_for_dbfield_manytomany(self):\n        ff = self.assertFormfield(Band, 'members', widgets.FilteredSelectMultiple)\n        self.assertEqual(ff.queryset, Member.objects.all())\n", "    def test_formfield_for_dbfield_empty_choices(self):\n        ff = self.assertFormfield(\n            MyFileField,\n            'file',\n            forms.fields.FileField,\n            formfield_for_foreignkey=lambda x: x\n        )\n\n", "    def test_formfield_for_dbfield_uuidfield(self):\n        ff = self.assertFormfield(Company, 'uuid', forms.UUIDField)\n\n\n", "    def test_formfield_for_dbfield_explicit_widget(self):\n        # Test that formfield_for_dbfield respects explicitly defined widgets\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {'widget': forms.TextInput},\n            }\n        ff = self.assertFormfield(Car, 'make', forms.TextInput, admin_overrides={'formfield_overrides': MyModelAdmin.formfield_overrides})\n        self.assertEqual(ff.widget.__class__, forms.TextInput)\n"], "sample_702": ["    def test_runpytest_subprocess_with_plugins(self, pytestconfig):\n        # Regression test for issue #5954\n        plugin = Plugin()\n\n        result = self.runpytest_subprocess(\n            \"-p\", f\"{plugin.__module__}:Plugin\", \"--capture=no\", monkeypatch=pytestconfig.monkeypatch\n        )\n        result.assert_outcomes(passed=1)\n        assert \"this is my plugin\" in result.stdout.str()\n", "    def test_chdir(pytester):\n        p = pytester.mkdir(\"mydir\")\n        p.join(\"test_hello.py\").write(\"def test_hello(): pass\")\n        pytester.chdir()\n        assert pytester.path.cwd() == pytester.path\n\n        pytester.chdir(p)\n        assert pytester.path.cwd() == p\n", "def test_runpytest_subprocess_with_python_flag(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            assert 1 == 1\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--python=python3.7\")\n    result.assert_outcomes(passed=1)\n\n", "    def test_getpathnode_dot_py_in_dir(self, testdir):\n        testdir.makepyfile(\n            test_foo = \"\"\"\n                pass\n            \"\"\"\n        )\n        node = testdir.getpathnode(\"test_foo.py\")\n        assert node.name == \"test_foo.py\"\n        assert node.parent.name == \"testdir\"\n\n", "    def test_spawn(self, testdir: Testdir) -> None:\n        testdir.makefile(\n            \".txt\", \"hello world\",\n        )\n        p = testdir.spawn(\"cat\", \"hello.txt\")\n        p.expect(\"hello world\")\n        p.sendline(\"exit\")\n        p.wait()\n        assert p.exitstatus == 0\n\n"], "sample_1208": ["def test_MatrixNormal_sample():\n    n, p = 3, 2\n    M = MatrixSymbol('M', n, p)\n    U = Matrix([[1, 0], [0, 1]])\n    V = Matrix([[2, 0], [0, 2]])\n    X = MatrixNormal('X', M, U, V)\n    pspace = MatrixPSpace(X, MatrixNormalDistribution(M, U, V), n, p)\n    samples = sample(X, size=(2,), library='scipy')\n    assert isinstance(samples, dict)\n    assert len(samples) == 1\n    assert samples[X].shape == (2, n, p)\n\n", "def test_matrix_gamma_sampling():\n    from sympy import Matrix, symbols\n    a, b = symbols('a b', positive=True)\n    M = MatrixGamma('M', a, b, [[2, 1], [1, 2]])\n    space = MatrixPSpace('M', M, 2, 2)\n    samples = sample(M, size=(2, 2), library='scipy', random_state=123)\n    assert isinstance(samples, dict)\n    assert len(samples) == 1\n    assert isinstance(samples[M], Matrix)\n    assert samples[M].shape == (2, 2)\n", "def test_MatrixStudentT_pdf():\n    n = symbols('n', positive=True)\n    p = symbols('p', positive=True)\n    M = MatrixSymbol('M', n, p)\n    Omega = MatrixSymbol('Omega', p, p)\n    Sigma = MatrixSymbol('Sigma', n, n)\n    nu = symbols('nu', positive=True)\n    X = MatrixSymbol('X', n, p)\n    T = MatrixStudentT('T', nu, M, Omega, Sigma)\n    # Test the pdf function\n    density_expr = density(T)(X)\n\n    # Simplify the expression\n\n    # TODO: Add assertions to check the simplified expression against the expected results.\n    # For example, check if the determinant term, gamma function terms, and other\n    # components are calculated correctly.\n\n", "def test_MatrixGamma_sampling():\n    from sympy import MatrixSymbol, symbols\n    a, b = symbols('a b', positive=True)\n    M = MatrixGamma('M', a, b, [[2, 1], [1, 2]])\n    pspace = MatrixPSpace('M', M, 2, 2)\n    samples = sample(M, size=(2, 2), library='scipy')\n    assert isinstance(samples, dict)\n    assert 'M' in samples\n    assert isinstance(samples['M'], Matrix)\n", "    def test_MatrixGamma_sample():\n        x, y = symbols('x y', positive=True)\n        M = MatrixGamma('M', x, y, [[1, 0], [0, 1]])\n        X = MatrixSymbol('X', 2, 2)\n        sample_M = sample(M, size=(2, 2))\n        assert isinstance(sample_M, dict)\n\n"], "sample_54": ["    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(mode='w+b', delete=False) as f:\n            f.write(b'This is some content')\n            f.seek(0)\n            response = FileResponse(f, filename='test.txt')\n            self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.txt\"')\n            self.assertEqual(response['Content-Length'], str(f.tell()))\n", "    def test_file_response_with_content_length(self):\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(b'Hello, world!')\n            f.seek(0)\n            response = FileResponse(open(f.name, 'rb'))\n            self.assertEqual(response['Content-Length'], '13')\n", "    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(b'Content')\n            f.flush()\n            filename = f.name\n\n        response = FileResponse(open(filename, 'rb'), as_attachment=True, filename='test.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n\n        response = FileResponse(open(filename, 'rb'), as_attachment=False, filename='test.txt')\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"test.txt\"')\n        self.assertEqual(response['Content-Type'], 'application/octet-stream')\n\n        os.unlink(filename)\n", "    def test_file_response_with_file_object(self):\n        with tempfile.NamedTemporaryFile(mode='w+t') as f:\n            f.write('Hello world')\n            f.seek(0)\n            response = FileResponse(f, as_attachment=True, filename='test.txt')\n\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n            self.assertEqual(response['Content-Length'], str(len('Hello world')))\n\n            content = io.BytesIO()\n            for chunk in response.streaming_content:\n                content.write(chunk)\n            self.assertEqual(content.getvalue(), b'Hello world')\n\n", "    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(mode='w+') as f:\n            f.write(b\"Hello, world!\")\n            f.seek(0)\n            response = FileResponse(f, filename='test.txt')\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n            self.assertEqual(response['Content-Type'], 'application/octet-stream')\n"], "sample_116": ["    def test_make_template_fragment_key(self):\n        key = make_template_fragment_key('test_fragment')\n        self.assertIsInstance(key, str)\n        self.assertIn('test_fragment', key)\n\n        key = make_template_fragment_key('test_fragment', vary_on=['arg1', 2, 'arg3'])\n        self.assertIsInstance(key, str)\n        self.assertIn('test_fragment', key)\n        self.assertIn('arg1', key)\n        self.assertIn('2', key)\n        self.assertIn('arg3', key)\n", "    def test_make_template_fragment_key(self):\n        key = make_template_fragment_key('test_fragment', vary_on=['arg1', 2, {'key': 'value'}])\n        self.assertIsInstance(key, str)\n        self.assertIn('template.cache.test_fragment.', key)\n", "    def test_make_template_fragment_key(self):\n        key = make_template_fragment_key('test_fragment')\n        self.assertEqual(\n            key,\n            'template.cache.test_fragment.d41d8cd98f00b204e9800998ecf8427e'\n        )\n        key = make_template_fragment_key('test_fragment', vary_on=['arg1', 'arg2'])\n        self.assertEqual(\n            key,\n            'template.cache.test_fragment.c25b27272e1491276111352822f73235'\n        )\n\n", "    def test_make_template_fragment_key(self):\n        self.assertEqual(\n            make_template_fragment_key('test_fragment'),\n            'template.cache.test_fragment.d41d8cd98f00b204e9800998ecf8427e'\n        )\n        self.assertEqual(\n            make_template_fragment_key('test_fragment', vary_on=['arg1', 'arg2']),\n            'template.cache.test_fragment.7c6920a02681293920edc8879753c7d9'\n        )\n", "    def test_make_template_fragment_key(self):\n        key = make_template_fragment_key('fragment_name')\n        self.assertIsNotNone(key)\n        key = make_template_fragment_key('fragment_name', vary_on=['arg1', 2, {'a': 'b'}])\n        self.assertIsNotNone(key)\n\n"], "sample_920": ["    def test_namedtuple_subclass(self):\n        doc = GoogleDocstring(NamedtupleSubclass.__doc__, self.config)\n        lines = doc.lines()\n        self.assertEqual(lines[0], 'Sample namedtuple subclass')\n        self.assertEqual(lines[2], ':param attr1: Quick description of attr1')\n        self.assertEqual(lines[4], ':param attr2: Quick description of attr2')\n        self.assertEqual(lines[6], ':param attr3: Type')\n        self.assertEqual(lines[7], ' ')\n        self.assertEqual(lines[8], '    Adds a newline after the type')\n\n", "    def test_namedtuple_subclass(self):\n        docstring = NamedtupleSubclass.__doc__\n        expected = dedent(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n\n        \"\"\")\n        config = Config({'napoleon_use_param': True})\n        self.assertEqual(GoogleDocstring(docstring, config=config).lines(),\n                         expected.splitlines())\n", "    def test_namedtuple_subclass_docstring(self):\n        docstring = cleandoc(NamedtupleSubclass.__doc__)\n        expected = [\n            'Sample namedtuple subclass',\n            '',\n            'Attributes',\n            '----------',\n            '*attr1*: Arbitrary type',\n            '    Quick description of attr1',\n            '*attr2*: Another arbitrary type',\n            '    Quick description of attr2',\n            '*attr3*: Type',\n            '',\n            '    Adds a newline after the type',\n        ]\n        self.assertEqual(GoogleDocstring(docstring).lines(), expected)\n", "    def test_namedtuple_subclass(self):\n        docstring = cleandoc(NamedtupleSubclass.__doc__)\n        config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n        doc = GoogleDocstring(docstring, config)\n        expected = dedent('''\\\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n            Adds a newline after the type\n\n        ''')\n        self.assertEqual(doc.lines(), expected.splitlines())\n\n", "    def test_namedtuple_subclass(self):\n        docstring = NamedtupleSubclass.__doc__\n        n = NumpyDocstring(docstring, config=Config())\n        expected = dedent(\"\"\"\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n        \"\"\")\n        self.assertEqual(str(n), expected)\n"], "sample_527": ["    def test_figure_rendering_with_no_axes(self, method):\n        fig = Figure()\n        # We should not raise any exception when trying to render a figure\n        # without Axes\n\n        if method == \"draw\":\n            fig.draw(RendererBase())\n        elif method == \"print_png\":\n\n            with io.BytesIO() as stream:\n                fig.print_png(stream)\n        elif method == \"savefig\":\n\n            with io.BytesIO() as stream:\n                fig.savefig(stream, format=\"png\")\n\n", "def test_ginput_with_axes_out_of_bounds():\n    fig, ax = plt.subplots()\n    fig.canvas.set_window_extent(\n        (0, 0, 100, 100)\n    )  # set fixed canvas size\n\n    # Create a point that is outside of the axes bounds\n    x = 10\n    y = 10\n    event = MouseEvent(\n        \"button_press_event\", fig.canvas, x, y, button=MouseButton.LEFT\n    )\n    ax.contains_point((x, y))  # Ensure point is outside axes\n\n    # call ginput, which should ignore the out-of-bounds click\n    points = fig.ginput(1, timeout=0.1)\n    assert points == []\n", "    def test_figure_add_axes_empty(self):\n        fig = Figure()\n        fig.add_axes((0, 0, 1, 1))\n        assert len(fig.axes) == 1\n", "    def test_constrained_layout(self):\n        fig, axs = plt.subplots(2, 2, constrained_layout=True)\n        fig.tight_layout()\n        # test that axes are spaced correctly\n        # this would be a good place to add assertions\n\n", "    def test_set_constrained_layout_pads(self):\n        fig, ax = plt.subplots()\n        fig.set_constrained_layout(True)\n        fig.set_constrained_layout_pads(w_pad=1.5, h_pad=0.5,\n                                        hspace=0.1, wspace=0.1)\n        pads = fig.get_constrained_layout_pads()\n        assert pads[0] == 1.5\n        assert pads[1] == 0.5\n        assert pads[2] == 0.1\n        assert pads[3] == 0.1\n\n"], "sample_879": ["    def test_one_hot_encoder_inverse_transform_empty_array():\n        # Test inverse_transform on empty array\n        X = np.array([], dtype=int).reshape(0, 2)\n        enc = OneHotEncoder(sparse=False).fit(np.array([[1, 2], [3, 4]]))\n        assert_array_equal(enc.inverse_transform(X), np.array([]).reshape(0, 2))\n", "    def test_unknown_value_and_encoded_missing_value_conflict():\n        enc = OrdinalEncoder(\n            handle_unknown=\"use_encoded_value\", unknown_value=1, encoded_missing_value=1\n        )\n        with pytest.raises(ValueError, match=\"already used\"):\n            enc.fit([[1, 2], [np.nan, 3]])\n", "    def test_ordinalencoder_sparse_input_with_missing_values():\n        X = sparse.csr_matrix([[1, np.nan], [2, 3]])\n        encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n                                 unknown_value=-1)\n        encoder.fit(X)\n\n        X_trans = encoder.transform(X)\n        assert_allclose(X_trans.toarray(), np.array([[0.0, -1.],\n                                                      [1.0, 2.]]))\n", "    def test_ordinalencoder_handle_unknown_use_encoded_value_int_input():\n        enc = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=3)\n        X = [['Male', 1], ['Female', 3], ['Female', 4]]\n        enc.fit(X)\n        X_trans = enc.transform(X)\n\n        assert_array_equal(\n            X_trans,\n            [[1, 0], [0, 2], [0, 3]],\n        )\n\n", "    def test_ordinal_encoder_unknown_value_dtype_mismatch():\n        X = [['Male', 1], ['Female', 3], ['Female', np.nan]]\n        enc = OrdinalEncoder(handle_unknown='use_encoded_value',\n                             unknown_value=np.nan, dtype=np.int64)\n        with pytest.raises(ValueError):\n            enc.fit_transform(X)\n"], "sample_733": ["    def test_tfidf_vectorizer_vocabulary_passing(self):\n        # Test that passing vocabulary overrides learning from data\n        v = TfidfVectorizer(vocabulary=['pizza', 'burger', 'beer'])\n        X = v.fit_transform(JUNK_FOOD_DOCS)\n\n        assert_array_equal(v.get_feature_names(), ['pizza', 'burger', 'beer'])\n        assert_equal(X.shape, (6, 3))\n\n        # ensure idf is not calculated\n        assert_true(v._tfidf.idf_ is None)\n", "    def test_tfidf_transformer_empty_vocabulary(self):\n        transformer = TfidfTransformer()\n        X = sp.csr_matrix((2, 0))\n        assert_raises(ValueError, transformer.fit, X)\n\n", "def test_tfidf_vocabulary_size():\n    # Test that the vocabulary size can be controlled using max_features\n\n    corpus = [\n        'this is the first document',\n        'this document is the second document',\n        'and this is the third one',\n        'is this the first document',\n    ]\n    vectorizer = TfidfVectorizer(max_features=3)\n    vectorizer.fit(corpus)\n    vocabulary = vectorizer.vocabulary_\n    assert_equal(len(vocabulary), 3)\n\n", "    def test_tfidf_vectorizer_vocabulary_pickle(self):\n        # Test pickling and unpickling TfidfVectorizer with a vocabulary.\n        tfidf = TfidfVectorizer(vocabulary={'a': 0, 'b': 1, 'c': 2})\n        tfidf2 = pickle.loads(pickle.dumps(tfidf))\n\n        assert_equal(tfidf.vocabulary, tfidf2.vocabulary)\n        assert_array_equal(tfidf.idf_, tfidf2.idf_)\n", "    def test_tfidf_vectorizer_vocabulary_intersection(self):\n        corpus = [\"foo bar baz\", \"baz quux foo\"]\n        vectorizer = TfidfVectorizer()\n        vectorizer.fit(corpus)\n        vocab = vectorizer.vocabulary_\n\n        # Create a new vectorizer with a pre-defined vocabulary\n        new_vectorizer = TfidfVectorizer(vocabulary=vocab)\n        new_vectorizer.fit(corpus)\n        new_vocab = new_vectorizer.vocabulary_\n\n        assert_equal(vocab, new_vocab)\n"], "sample_672": ["    def test_pformat_dispatch_with_long_string():\n        long_string = \"x\" * 1000\n        result = _pformat_dispatch(long_string)\n        assert \"...\" in result\n", "    def test_pformat_dispatch_with_custom_object():\n        class CustomObject:\n                return \"CustomObject()\"\n\n        result = _pformat_dispatch(CustomObject())\n        assert result == 'CustomObject()'\n", "    def test_saferepr_exception_in_repr():\n        class BrokenRepr:\n                raise ValueError(\"broken repr\")\n        obj = BrokenRepr()\n        result = saferepr(obj)\n        assert isinstance(result, str)\n        assert \"ValueError\" in result\n        assert \"broken repr\" in result\n        assert \"BrokenRepr\" in result\n", "    def test_pformat_dispatch_with_cyclic_object():\n        class Cyclic:\n                self.x = self\n        obj = Cyclic()\n        result = _pformat_dispatch(obj)\n        assert '...' in result\n", "    def test_pformat_dispatch_long_string():\n        class LongString:\n                return \"a\" * 1000\n        assert len(_pformat_dispatch(LongString())) < 1000 \n"], "sample_824": ["    def test_check_pairwise_arrays_error_precomputed(self):\n        with pytest.raises(ValueError):\n            check_pairwise_arrays(np.ones((5, 5)), np.ones((5, 2)),\n                                  precomputed=True)\n        with pytest.raises(ValueError):\n            check_pairwise_arrays([[1, 2], [3, 4]], [[5, 6]],\n                                  precomputed=True)\n\n", "    def test_pairwise_distances_chunked_with_reduce_func(self):\n        X = np.random.RandomState(0).rand(5, 3)\n        r = .2\n\n            neigh = [np.flatnonzero(d < r) for d in D_chunk]\n            return neigh\n\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n        neigh = next(gen)\n        assert_equal(neigh, [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])])\n\n\n", "    def test_pairwise_distances_chunked_sparse():\n        X = dok_matrix((5, 3), dtype=np.float64)\n        X[0, :] = [1, 2, 3]\n        X[1, :] = [4, 5, 6]\n        X[2, :] = [7, 8, 9]\n        X[3, :] = [10, 11, 12]\n        X[4, :] = [13, 14, 15]\n\n        n_jobs = 1\n        # Test with a metric that works with sparse matrices\n        D = pairwise_distances(\n            X, metric='euclidean', n_jobs=n_jobs\n        )\n        D_chunked = list(pairwise_distances_chunked(\n            X, metric='euclidean', n_jobs=n_jobs\n        ))\n\n        assert_array_equal(D, np.vstack(D_chunked))\n\n", "    def test_pairwise_distances_chunked_with_reduce_func(self):\n        X = np.random.RandomState(0).rand(5, 3)\n            return D_chunk.sum(axis=1)\n        gen = pairwise_distances_chunked(\n            X, reduce_func=reduce_func, working_memory=10)\n        result = next(gen)\n        assert_array_almost_equal(result,\n                                  pairwise_distances(X).sum(axis=1)\n                                  )\n\n\n", "    def test_pairwise_distances_chunked_memory_limit(self):\n        # Create a large matrix to force chunking\n        X = np.random.rand(1000, 10)\n        chunk_size = 100\n\n        # Use a low working_memory limit to guarantee chunking\n        working_memory = 1000\n\n        with config_context(working_memory=working_memory):\n            # Test that chunks are generated correctly\n            for chunk in pairwise_distances_chunked(X,\n                                                    chunk_size=chunk_size):\n                assert chunk.shape[0] == chunk_size\n\n            # Test that final chunk has the correct size\n            last_chunk_size = X.shape[0] % chunk_size\n            assert pairwise_distances_chunked(X, chunk_size=chunk_size,\n                                             reduce_func=None).next() \\\n                                             .shape[0] == last_chunk_size\n"], "sample_1091": ["def test_relational_rewrite():\n    from sympy.abc import x, y\n    assert (x + y).rewrite(Add) == x + y\n    assert (x + y).rewrite(Add, evaluate=False) == Add(x, y)\n    assert (x + y).rewrite(Add, evaluate=True) == x + y\n        \n\n", "    def test_eval_relation_complex():\n        assert (2 + 3*I) > (1 + I)\n        assert (2 + 3*I) >= (2 + 3*I)\n        assert not ((2 + 3*I) < (1 + I))\n        assert not ((2 + 3*I) <= (1 + I))\n        assert (1 + 2*I) != (1 + 3*I)\n        assert (1 + 2*I) == (1 + 2*I)\n", "    def test_relational_complex():\n        I = S.ImaginaryUnit\n        assert (I > 1) is False\n        assert (I < 1) is False\n        assert (I == 1) is False\n        assert (I != 1) is True\n        assert (I >= 1) is False\n        assert (I <= 1) is False\n\n        assert ((-I) > 1) is False\n        assert ((-I) < 1) is False\n        assert ((-I) == 1) is False\n        assert ((-I) != 1) is True\n        assert ((-I) >= 1) is False\n        assert ((-I) <= 1) is False\n\n", "def test_relational_empty_set():\n    assert Relational(x, EmptySet(), \"<\") == False\n    assert Relational(x, EmptySet(), \">\") == False\n    assert Relational(x, EmptySet(), \"==\") == False\n    assert Relational(x, EmptySet(), \"!=\") == True\n\n", "def test_relational_complex():\n    a = 1 + I\n    b = 2 + 2*I\n    assert (a > b) == False\n    assert (a < b) == True\n    assert (a == b) == False\n    assert (a >= b) == False\n    assert (a <= b) == True\n    assert (a != b) == True\n"], "sample_755": ["    def test_silhouette_samples_sparse(self):\n        X, y = datasets.make_blobs(n_samples=100, random_state=42)\n        X_sparse = csr_matrix(X)\n        silhouette_avg = silhouette_score(X, y)\n        silhouette_avg_sparse = silhouette_score(X_sparse, y)\n        assert_allclose(silhouette_avg, silhouette_avg_sparse)\n", "def test_silhouette_samples_sparse_diagonal():\n    # Test that silhouette_samples raises an error for sparse\n    # distance matrices with non-zero diagonal entries\n    X = csr_matrix([[0, 1], [1, 0]])\n    labels = np.array([0, 1])\n    assert_raises_regexp(\n        ValueError,\n        'The precomputed distance matrix contains non-zero '\n        'elements on the diagonal',\n        silhouette_samples, X, labels, metric='precomputed')\n", "    def test_silhouette_samples_sparse(self):\n        # Make sure silhouette_samples works with sparse matrices\n        X, y = datasets.make_blobs(n_samples=50, centers=3, random_state=42)\n        X_sparse = csr_matrix(X)\n        sil_dense = silhouette_samples(X, y)\n        sil_sparse = silhouette_samples(X_sparse, y)\n        assert_array_equal(sil_dense, sil_sparse)\n", "    def test_calinski_harabasz_score_sparse(self):\n        X, _ = datasets.make_blobs(n_samples=30, n_features=2, centers=3,\n                                   random_state=0)\n        X_sparse = csr_matrix(X)\n\n        labels = np.array([0] * 10 + [1] * 10 + [2] * 10)\n\n        score_dense = calinski_harabasz_score(X, labels)\n        score_sparse = calinski_harabasz_score(X_sparse, labels)\n\n        assert_allclose(score_dense, score_sparse)\n", "    def test_silhouette_samples_precomputed_sparse_matrix(self):\n        X, y = datasets.make_blobs(n_samples=10, random_state=42)\n        X_sparse = csr_matrix(X)\n\n        # Check if Silhouette Samples works with precomputed sparse distance\n        # matrix\n        dist_matrix = pairwise_distances(X)\n        silhouette_samples(dist_matrix, y, metric=\"precomputed\")\n"], "sample_176": ["    def test_autodetector_renames_with_m2m_through_table(self):\n        before = [\n            self.author_with_m2m,\n            self.contract,\n        ]\n        after = [\n            self.author_with_renamed_m2m_through,\n            self.contract_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(\n            changes, 'testapp', 0,\n            ['RenameModel', 'RenameField', 'AlterField', 'RenameModel']\n        )\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Contract', app_label='testapp')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='publishers', old_name='publishers')\n        self.assertOperationAttributes(changes, 'testapp', 0, 2, name='publisher', model_name='publisher')\n        self.assertOperationAttributes(changes, 'testapp', 0, 3, old_name='Contract', name='Deal')\n", "    def test_add_field_with_default(self):\n        before = [\n            self.author,\n        ]\n        after = [\n            self.author_name_default,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['AddField', 'AlterField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='name', field_type='CharField')\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, default='Unknown')\n\n", "    def test_rename_field_with_default(self):\n        # Case 1\n        before = [self.author_name_default_1]\n        after = [self.author_name_default_2]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name_old')\n\n        # Case 2\n        before = [self.author_name_default_2]\n        after = [self.author_name_default_1]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name_old', old_name='name')\n", "    def test_detect_changes_field_rename_with_default(self):\n        changes = self.get_changes(\n            [self.author_name],\n            [self.author_name_renamed],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", old_name=\"name_renamed\")\n", "    def test_rename_unique_together(self):\n        # Test if renaming a field involved in unique_together constraint is handled correctly\n\n        before_states = [\n            self.author_with_unique_together,\n            self.book_with_author_renamed,\n        ]\n        after_states = [\n            self.author_renamed_with_unique_together,\n            self.book_with_author_renamed,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField', 'AlterUniqueTogether'])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='new_name', model_name='author', field_name='name'\n        )\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 1, name='book_with_author_renamed', unique_together=[('new_name', 'title')]\n        )\n"], "sample_634": ["    def test_expand_modules_ignore_list_re(self) -> None:\n        ignore_list_re = [\n            re.compile(\".*test.*\"),\n        ]\n        files_or_modules = [\n            \"test_module1.py\",\n            \"module.py\",\n            \"package/test_module2.py\",\n            \"package/module2.py\",\n        ]\n        result, errors = expand_modules(\n            files_or_modules,\n            ignore_list=[],\n            ignore_list_re=ignore_list_re,\n            ignore_list_paths_re=[],\n        )\n        assert len(errors) == 0\n        expected_result = [\n            {\n                \"path\": \"module.py\",\n                \"name\": \"module\",\n                \"isarg\": True,\n                \"basepath\": \"module.py\",\n                \"basename\": \"module\",\n            },\n            {\n                \"path\": \"package/module2.py\",\n                \"name\": \"package.module2\",\n                \"isarg\": True,\n                \"basepath\": \"package/module2.py\",\n                \"basename\": \"module2\",\n            },\n        ]\n        self.assertEqual(result, expected_result)\n", "    def test_expand_modules_ignore_paths_re(self, tmpdir: Path) -> None:\n        # Test that ignore_list_paths_re correctly ignores paths containing patterns\n        ignore_pattern = re.compile(r\".*tests.*\")\n        files_or_modules = [\"path/to/module.py\", \"path/to/tests/test_module.py\"]\n        ignore_list = []\n        ignore_list_re = []\n        ignore_list_paths_re = [ignore_pattern]\n\n        result, errors = expand_modules(\n            files_or_modules, ignore_list, ignore_list_re, ignore_list_paths_re\n        )\n\n        assert len(result) == 1\n        assert result[0][\"path\"] == \"path/to/module.py\"\n", "    def test_expand_modules_ignore_re(self) -> None:\n        set_config(ignore_patterns=[\"^__init__\\.py$\"])\n        files_or_modules = [\"test.py\", \"test/__init__.py\"]\n        result, errors = expand_modules(\n            files_or_modules, [], [], [re.compile(r\"^__init__\\.py$\")]\n        )\n        assert result == [\n            {\n                \"path\": \"test.py\",\n                \"name\": \"test\",\n                \"isarg\": True,\n                \"basepath\": \"test.py\",\n                \"basename\": \"test\",\n            }\n        ]\n        assert not errors\n", "    def test_expand_modules_ignore_list_paths_re(\n        self,", "    def test_expand_modules_with_namespace_package(\n        self, checker_class: Type[BaseChecker],"], "sample_668": ["    def test_node_use_from_parent_warning(testdir):\n        with testdir.usefixture(\"testdir\"):\n            testdir.makepyfile(\n                \"\"\"\n                import pytest\n\n                    node = pytest.Node.from_parent(None)\n                    node.name = 'test_foo'\n                \"\"\"\n            )\n            result = testdir.runpytest()\n            assert result.ret == 0\n            result.stdout.fnmatch_lines(\n                [\n                    \"*test_node_use_from_parent_warning.py .\",\n                ]\n            )\n", "    def test_deprecated_node_use_from_parent():\n        with pytest.raises(deprecated.PytestDeprecationWarning) as excinfo:\n            nodes.Item.from_parent(None)\n        assert (\n            excinfo.value.args[0]\n            == \"Direct construction of Item has been deprecated, please use Item.from_parent.\\n\"\n            \"See https://docs.pytest.org/en/latest/deprecations.html#node-construction-changed-to-node-from-parent\"\n            \" for more details.\"\n        )\n\n", "def test_node_use_from_parent(monkeypatch):\n    monkeypatch.setattr(nodes.Collector, \"from_parent\", lambda self, parent: self)\n    with pytest.raises(PytestDeprecationWarning) as excinfo:\n        nodes.Collector()\n\n    assert str(excinfo.value).startswith(\n        \"Direct construction of Collector has been deprecated, please use Collector.from_parent.\"\n    )\n", "    def test_node_use_from_parent_warning(request):\n        with pytest.raises(PytestDeprecationWarning) as excinfo:\n            nodes.Collector.from_parent(request.node)\n        assert str(excinfo.value).endswith(\n            \"https://docs.pytest.org/en/latest/deprecations.html#node-construction-changed-to-node-from-parent\"\n        )\n", "    def test_deprecation_node_use_from_parent(pytester):\n        p = pytester.makepyfile(\n            \"\"\"\n            from _pytest.nodes import Collector\n\n            c = Collector.from_parent(None, name=\"foo\")\n            \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.assert_warnings(\n            [\n                pytest.PytestWarning(\n                    \"Direct construction of Collector has been deprecated, please use Collector.from_parent.\\n\"\n                    \"See \"\n                    \"https://docs.pytest.org/en/latest/deprecations.html#node-construction-changed-to-node-from-parent\"\n                    \" for more details.\"\n                ),\n            ]\n        )\n"], "sample_60": ["    def test_add_media_to_episode(self):\n        url = reverse('admin:generic_inline_admin_episode_add')\n        response = self.client.get(url)\n        self.assertContains(response, 'Add Episode')\n        self.assertContains(response, 'Media')\n\n        form_data = {\n            'name': 'New Episode',\n            'media-TOTAL_FORMS': '2',\n            'media-INITIAL_FORMS': '0',\n            'media-0-content_type': ContentType.objects.get_for_model(Episode).pk,\n            'media-0-object_id': self.episode_pk,\n            'media-0-url': 'http://example.com/new_episode.mp3',\n            'media-1-content_type': ContentType.objects.get_for_model(Episode).pk,\n            'media-1-object_id': self.episode_pk,\n            'media-1-url': 'http://example.com/new_episode_image.png',\n        }\n\n        response = self.client.post(url, form_data)\n        self.assertEqual(response.status_code, 302)  # Redirect to change list\n        self.assertRedirects(response, reverse('admin:generic_inline_admin_episode_changelist'))\n", "    def test_change_view_with_generic_inline(self):\n        response = self.client.get(reverse('admin:generic_inline_admin_episode_change', args=(self.episode_pk,)))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'This Week in Django')\n        self.assertContains(response, 'http://example.com/podcast.mp3')\n        self.assertContains(response, 'http://example.com/logo.png')\n", "    def test_generic_inline_add(self):\n        url = reverse('admin:generic_inline_admin_episode_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        formsets_data = {\n            'content_type': ContentType.objects.get_for_model(Media).id,\n            'object_id': self.episode_pk,\n            'form-0-url': 'http://example.com/new_media.mp3',\n        }\n\n        response = self.client.post(url, formsets_data)\n        self.assertEqual(response.status_code, 302)\n\n        self.assertEqual(Media.objects.count(), 3)\n", "    def test_add_media_to_episode(self):\n        url = reverse('admin:generic_inline_admin_episode_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        media_count_before = Media.objects.count()\n        data = {\n            'name': 'New Episode',\n            'media-INITIAL_FORMS': '0',\n            'media-TOTAL_FORMS': '1',\n            'media-MAX_NUM_FORMS': DEFAULT_MAX_NUM,\n            'media_0-url': 'http://example.com/new_episode.mp3',\n            'media_0-content_type': ContentType.objects.get_for_model(Episode).pk,\n            'media_0-object_pk': self.episode_pk,\n\n        }\n        response = self.client.post(url, data)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(Media.objects.count(), media_count_before + 1)\n\n", "    def test_add_media_to_episode(self):\n        url = reverse('admin:generic_inline_admin_episode_add')\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n\n        # Check for presence of the inline formset for Media\n        self.assertContains(response, '<input type=\"hidden\" name=\"form-TOTAL_FORMS\" value=\"2\">')\n\n        # Fill out the form with media data for a new episode\n        data = {\n            'episode_name': 'New Episode',\n            'media_0-url': 'http://example.com/newepisode.mp3',\n            'media_0-content_type': ContentType.objects.get_for_model(Episode).id,\n            'media_1-url': 'http://example.com/newepisode.jpg',\n            'media_1-content_type': ContentType.objects.get_for_model(Episode).id,\n        }\n\n        response = self.client.post(url, data)\n        # Redirect to the change list\n        self.assertEqual(response.status_code, 302)\n\n        # Check for successful creation\n        episode = Episode.objects.get(name='New Episode')\n        self.assertEqual(episode.media_set.count(), 2)\n"], "sample_774": ["    def test_onehotencoder_handle_unknown_ignore(self):\n        X = [['Male'], ['Female'], ['Male'], ['Female']]\n        encoder = OneHotEncoder(handle_unknown='ignore')\n        encoder.fit(X)\n        X_transformed = encoder.transform([['Male'], ['Female'], ['Unknown']])\n        assert_allclose(X_transformed.toarray(),\n                        [[1., 0.],\n                         [0., 1.],\n                         [0., 0.]])\n\n", "    def test_onehot_inverse_transform_raises_error_on_unknown_cats(self):\n        enc = OneHotEncoder(handle_unknown='error')\n        enc.fit([[0, 1], [1, 2]])\n        with pytest.raises(ValueError) as exc:\n            enc.inverse_transform([[0, 1, 0]])\n        assert \"Unknown category found in the input data.\" in str(exc.value)\n", "    def test_onehot_inverse_transform_unknown_categories(self):\n        onehot = OneHotEncoder(handle_unknown='ignore').fit(\n            [[0, 1], [1, 0]]\n        )\n        X = [[0, 2], [2, 0]]\n        X_trans = onehot.transform(X)\n        X_back = onehot.inverse_transform(X_trans)\n        assert_array_equal(\n            X_back, np.array([[0, None], [None, 0]], dtype=object)\n        )\n", "    def test_onehotencoder_drop_unknown_fit_transform(self):\n        # test fit_transform with drop argument and unknown categories\n        X = np.array([['apple', 'blue'], ['banana', 'red'],\n                      ['apple', 'yellow'], ['banana', 'green']],\n                     dtype=object)\n        enc = OneHotEncoder(handle_unknown='ignore', drop='first', sparse=False)\n        with assert_warns(UserWarning):\n            X_out = enc.fit_transform(X)\n\n        assert_array_equal(X_out,\n                           [[0., 1.],\n                            [1., 0.],\n                            [0., 1.],\n                            [1., 0.]])\n        assert_equal(enc.categories_[0], ['apple', 'banana'])\n        assert_equal(enc.categories_[1], ['blue', 'green', 'red', 'yellow'])\n\n", "    def test_onehot_empty_input(self):\n        # Test with an empty array for both fit and transform\n        enc = OneHotEncoder()\n        enc.fit([])\n        X_empty = []\n        assert array_equal(enc.transform(X_empty).toarray(), [])\n\n        enc = OneHotEncoder(handle_unknown='ignore')\n        enc.fit([])\n        X_empty = np.array([['unknown']]).astype(object)\n\n        assert_raises(ValueError, enc.transform, X_empty)\n\n"], "sample_823": ["    def test_pairwise_distances_chunked_small_chunksize(self):\n        X = np.random.RandomState(0).rand(5, 3)\n        # Test with chunksize smaller than n_samples\n        chunk_size = 2\n        # Make sure we iterate over all chunks\n        n_chunks = math.ceil(X.shape[0] / chunk_size)\n        for i, chunk in enumerate(pairwise_distances_chunked(X, chunk_size=chunk_size)):\n            assert chunk.shape == (chunk_size, X.shape[0])\n            if i == n_chunks - 1:\n                assert chunk.shape[0] == X.shape[0] % chunk_size\n        # test with reduce_func\n            return chunk.mean(axis=1)\n        results = [result for result in pairwise_distances_chunked(\n            X, chunk_size=chunk_size, reduce_func=reduce_func)]\n        expected_results = pairwise_distances(X).mean(axis=1)\n        assert_allclose(np.concatenate(results), expected_results)\n\n", "    def test_pairwise_distances_chunked_reduce_func_output(self):\n        X = np.random.RandomState(0).rand(5, 3)\n        r = .2\n\n            neigh = [np.flatnonzero(d < r) for d in D_chunk]\n            avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n            return neigh, avg_dist\n\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n        neigh, avg_dist = next(gen)\n\n        assert isinstance(neigh, list)\n        assert isinstance(avg_dist, np.ndarray)\n        assert len(neigh) == X.shape[0]\n        assert avg_dist.shape == (X.shape[0],)\n", "    def test_pairwise_distances_chunked_empty_matrix():\n        X = np.empty((0, 5))\n        D = pairwise_distances_chunked(X)\n        assert isinstance(D, GeneratorType)\n\n        # Iterate over the generator, check it's empty\n        chunk = next(D, None)\n        assert chunk is None\n\n        # Ensure no more chunks are generated\n        with pytest.raises(StopIteration):\n            next(D)\n", "    def test_pairwise_distances_with_boolean_arrays():\n        X = np.array([[True, True], [False, True], [True, False]])\n        Y = np.array([[True, False], [True, True], [False, False]])\n        for metric in PAIRWISE_BOOLEAN_FUNCTIONS:\n            dist = pairwise_distances(X, Y, metric=metric)\n            assert(dist.shape == (3, 3))\n", "    def test_pairwise_distances_chunked_empty_input(self):\n        X = np.array([])\n        result = list(pairwise_distances_chunked(X))\n        assert_equal(len(result), 0)\n"], "sample_213": ["    def test_filedescriptor_save_and_delete(self):\n        temp_dir = tempfile.mkdtemp()\n        storage = FileSystemStorage(location=temp_dir)\n\n        class CustomModel(models.Model):\n            file = FileField(storage=storage)\n\n        model = CustomModel()\n        filename = 'test.txt'\n        with open(filename, 'w') as f:\n            f.write('This is a test file')\n\n        with open(filename, 'rb') as f:\n            model.file.save(filename, f)\n\n        self.assertTrue(os.path.exists(os.path.join(temp_dir, filename)))\n\n        model.file.delete()\n        self.assertFalse(os.path.exists(os.path.join(temp_dir, filename)))\n", "    def test_fieldfile_close_method(self):\n        # Make sure that the close() method of FieldFile doesn't affect the underlying file.\n\n        # Create a temporary file\n        with tempfile.NamedTemporaryFile(suffix='.txt') as f:\n            f.write(b'Hello, World!')\n            # Create a FieldFile instance\n            field = FileField()\n            file = field.attr_class(None, field, f.name)\n            file.file = f\n\n            file.close()\n\n            # Try to read from the file after it's been closed through FieldFile\n            f.seek(0)\n            self.assertEqual(f.read(), b'Hello, World!')\n", "    def test_descriptor_access(self):\n        # Tests that the descriptor allows access to the FileField value.\n        tempfile.tempdir = tempfile.mkdtemp()\n        try:\n            with open(os.path.join(tempfile.tempdir, \"test.txt\"), 'w') as f:\n                f.write(\"test content\")\n            instance = Storage.objects.create(file=open(os.path.join(tempfile.tempdir, \"test.txt\"), 'rb'))\n\n            self.assertEqual(instance.file.name, 'test.txt')\n            self.assertEqual(instance.file.size, 11)\n            self.assertTrue(instance.file.open().read(5) == b\"test \")\n            self.assertTrue(instance.file.closed)\n        finally:\n            shutil.rmtree(tempfile.tempdir)\n", "    def test_file_descriptor_handles_various_file_types(self):\n        instance = Storage()\n        field = FileField()  \n\n        # Test with a string path\n        instance.file = 'path/to/file.txt'\n        descriptor = FileDescriptor(field)\n        file_obj = descriptor.__get__(instance, Storage)\n        self.assertTrue(isinstance(file_obj, FileField.attr_class))\n        self.assertEqual(file_obj.name, 'path/to/file.txt')\n\n        # Test with a File object\n        instance.file = SimpleUploadedFile('test.txt', b'content')\n        descriptor = FileDescriptor(field)\n        file_obj = descriptor.__get__(instance, Storage)\n        self.assertTrue(isinstance(file_obj, FileField.attr_class))\n        self.assertEqual(file_obj.file.read(), b'content')\n        self.assertFalse(file_obj._committed)\n\n", "    def test_file_descriptor_get_returns_fieldfile(self):\n        model = Storage.objects.create(file=SimpleUploadedFile('test.txt', b'content'))\n        field = model._meta.get_fields()[0]\n        descriptor = field.descriptor_class(field)\n        self.assertIsInstance(descriptor.__get__(model), field.attr_class)\n\n"], "sample_154": ["    def test_check_database_backends_with_no_databases(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            issues = check_database_backends()\n            self.assertEqual(issues, [])\n            mock_check.assert_not_called()\n", "    def test_check_database_backends_with_databases(self):\n        mock_check = mock.MagicMock()\n        mock_check.return_value = ['issue1', 'issue2']\n        with mock.patch('django.db.connections', {\n            'default': mock.MagicMock(validation=mock.MagicMock(check=mock_check)),\n            'other': mock.MagicMock(validation=mock.MagicMock(check=mock_check)),\n        }):\n            issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2', 'issue1', 'issue2'])\n", "    def test_check_database_backends_with_databases(self):\n        mock_check = mock.MagicMock()\n        mock_check.return_value = ['issue1', 'issue2']\n        with mock.patch('django.db.connections') as mock_connections:\n            mock_connections.return_value.__getitem__.side_effect = lambda alias: mock.MagicMock(validation=mock.MagicMock(check=mock_check))\n            issues = check_database_backends(databases=self.databases)\n        self.assertEqual(issues, ['issue1', 'issue2', 'issue1', 'issue2'])\n", "    def test_check_database_backends_no_databases(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            check_database_backends()\n            mock_check.assert_not_called()\n", "    def test_check_database_backends_with_databases(self):\n        patcher = mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check')\n        with patcher as mock_check:\n            mock_check.return_value = ['issue 1', 'issue 2']\n            issues = check_database_backends(databases=self.databases)\n            self.assertEqual(issues, ['issue 1', 'issue 2'])\n            mock_check.assert_has_calls([mock.call(**{})] * 2)\n"], "sample_1037": ["def test_matmul_with_piecewise():\n    M = MatrixSymbol('M', 2, 2)\n    x = symbols('x')\n    p = Piecewise((1, x > 0), (0, True))\n    expr = MatMul(p*M, M)\n    assert isinstance(expr, MatMul)\n    assert expr.args[0].is_Piecewise\n    assert expr.args[1] == M\n", "def test_MatMul_canonicalize():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    Z = MatrixSymbol('Z', 2, 2)\n    expr = X*X.T*Y.T\n    assert canonicalize(expr) == Z*Y.T*X.T*Y.T\n", "    def test_shape_error(self):\n        raises(ShapeError, lambda: MatMul(A, B, E))\n", "    def test_MatMul_transpose_apply_inverse_rules_issue_14577(self):\n        # https://github.com/sympy/sympy/issues/14577\n        X = MatrixSymbol('X', 2, 2)\n        expr = (X.T * X).I * X\n        expected = MatrixExpr.fromiter(\n            [\n                Inverse(MatrixExpr.fromiter([MatMul(X.T, X)])),\n                X,\n            ]\n        )\n\n        result = expr.doit()\n\n        assert result == expected\n\n", "def test_MatMul_inverse():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    I = Identity(2)\n    expr = MatMul(X, Y, X.inverse(), Y.inverse())\n    assert expand(expr) == I\n    assert expr.doit() == I\n"], "sample_1046": ["    def test_riemann_cyclic_replace(self):\n        Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n        i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n        R = tensorhead('R', [Lorentz]*4, [[2, 2]])\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t1 = riemann_cyclic_replace(t)\n        assert _is_equal(t1, 0)\n", "def test_canon_bp_issue_15679():\n    Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n    i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n    A = tensorhead('A', [Lorentz]*2, [[1]*2])\n    t = A(i, k)*A(-k, j)\n    t = canon_bp(t)\n\n    assert t.components == [S.One]\n    assert t.free == [(i, 0), (j, 1)]\n    assert t.dum == []\n", "def test_riemann_cyclic_replace():\n    i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n    R = tensorhead('R', [Lorentz]*4, [[2, 2]])\n    t = 2*R(i,j,k,l)\n    t1 = 2*riemann_cyclic_replace(R(i,j,k,l))\n    assert _is_equal(t, t1)\n", "    def test_riemann_cyclic_replace3(self):\n        Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n        i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n        R = tensorhead('R', [Lorentz]*4, [[2, 2]])\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t = riemann_cyclic_replace(t)\n        t = t.expand().canon_bp()\n        ans = 2*R(i, j, k, l)/3 - R(i, l, j, k)/3\n        assert _is_equal(t, ans)\n", "    def test_riemann_cyclic_replace_simple():\n        Lorentz = TensorIndexType('Lorentz', dummy_fmt='L')\n        i, j, k, l = tensor_indices('i, j, k, l', Lorentz)\n        R = tensorhead('R', [Lorentz]*4, [[2, 2]])\n        t = R(i,j,k,l)\n        tr = riemann_cyclic_replace(t)\n        ans = 2*R(i,j,k,l)/3 - R(i,l,j,k)/3 + R(i,k,j,l)/3\n        assert _is_equal(tr,ans)\n"], "sample_8": ["    def test_from_masked_ndarray(self):\n        masked_ndarray = MaskedNDArray(self.a, mask=self.mask_a)\n        a = Masked(masked_ndarray)\n        assert_masked_equal(a, masked_ndarray)\n", "    def test_masked_array_from_masked_array(self):\n        # Check that MaskedArray can be initialized from a MaskedArray.\n\n        ma = MaskedArray(self.a, mask=self.mask_a)\n        ma2 = MaskedArray(ma)\n        assert_masked_equal(ma, ma2)\n\n", "    def test_masked_array_creation_from_masked_array(self):\n        ma = MaskedNDArray(self.a, mask=self.mask_a)\n        m2 = MaskedArray(ma)\n        assert_masked_equal(ma, m2)\n", "    def test_masked_array_from_masked(self):\n        a = Masked(self.a, mask=self.mask_a)\n        assert_masked_equal(a, Masked.from_masked(a))\n        assert_masked_equal(Masked.from_masked(self.a),\n                            Masked(self.a, mask=np.zeros_like(self.a)))\n", "    def test_from_ndarray_masked_copy(self):\n        \"\"\"Test creating MaskedArray from ndarray with copy=True.\"\"\"\n        a = np.arange(6).reshape(2, 3)\n        mask = np.array([[True, False, False],\n                         [False, True, False]])\n        ma = MaskedArray.from_ndarray(a, mask=mask, copy=True)\n        assert not np.may_share_memory(a, ma.unmasked)\n        assert_masked_equal(ma, MaskedArray(a, mask=mask))\n"], "sample_1143": ["def test_numbers_equal_with_different_prec():\n    assert not (Float(3.14, 50) == Float(3.14, 20))\n\n\n", "def test_imaginary_unit_pow():\n    assert (I**10).n() == -1\n    assert (I**11).n() == -I\n    assert (I**20).n() == 1\n    assert (I**-1).n() == -I\n    assert (I**-2).n() == -1\n    assert (I**-3).n() == I\n", "def test__eval_power_complex():\n    assert (I**2).evalf() == -1\n    assert (I**4).evalf() == 1\n    assert srepr((1 + I)**2) == \"Complex(0, 2)\"\n    assert (1 + I)**2 == 2 * I\n", "def test_algebraic_number_evalf():\n    x = AlgebraicNumber(sqrt(2))\n    assert x.evalf() == sqrt(2).evalf()\n\n    y = AlgebraicNumber(cbrt(2))\n    assert y.evalf() == cbrt(2).evalf()\n", "def test_Float_eq():\n    assert Float(1.0) == Float(1.0)\n    assert Float(1.0) != Float(1.1)\n    assert Float(1.0) != pi\n    assert Float(1.0).__eq__(Float(1.0))\n    assert not Float(1.0).__eq__(Float(1.1))\n    assert not Float(1.0).__eq__(pi)\n\n"], "sample_29": ["    def test_write_latex_to_file(self, cosmo, tmpdir):\n        \"\"\"Test writing cosmology to LaTeX file.\"\"\"\n        file = tmpdir.join(\"cosmology.tex\")\n        write_latex(cosmo, file)\n        assert file.exists()\n", "    def test_latex(self, cosmo, tmp_path):\n        tmp_file = tmp_path / \"cosmo.tex\"\n        write_latex(cosmo, tmp_file)\n        with open(tmp_file) as f:\n            content = f.read()\n        assert r\"\\begin{table}\" in content\n        assert r\"\\end{table}\" in content\n\n", "    def test_latex_names(self, cosmo, tmp_path):\n        \"\"\"Test that writing to LaTeX uses LaTeX formatted parameter names.\"\"\"\n        filename = tmp_path / 'test.tex'\n        write_latex(cosmo, filename, latex_names=True)\n        with open(filename, 'r') as f:\n            latex_table = f.read()\n        for name in cosmo.__parameters__:\n            assert _FORMAT_TABLE.get(name, name) in latex_table\n", "    def test_latex_names(self, cosmo, tmp_path):\n        \"\"\"Test writing a LaTeX table with LaTeX formatted parameter names.\"\"\"\n        filename = tmp_path / \"cosmo.tex\"\n\n        write_latex(cosmo, filename, latex_names=True)\n\n        with open(filename, \"r\") as f:\n            content = f.read()\n\n        for param_name in cosmo.__parameters__:\n            latex_name = _FORMAT_TABLE.get(param_name, param_name)\n            assert latex_name in content\n", "    def test_write_latex_units(self, cosmo, tmp_path):\n        \"\"\"Test that units are correctly handled when writing to LaTeX.\"\"\"\n        filename = tmp_path / \"cosmology.tex\"\n        write_latex(cosmo, filename, cls=QTable)\n        table = QTable.read(filename, format=\"latex\")\n        for name in cosmo.__parameters__:\n            param = getattr(cosmo, name)\n            if isinstance(param, Parameter) and param.unit is not None:\n                assert table[name].unit == param.unit\n"], "sample_614": ["    def test_diff_array_repr_identical_with_attrs():\n        a = xr.DataArray(\n            [1, 2, 3], dims=\"x\", attrs={\"foo\": \"bar\", \"baz\": 1}\n        ).to_dataset(name=\"a\")\n        b = xr.DataArray(\n            [1, 2, 3], dims=\"x\", attrs={\"foo\": \"bar\", \"baz\": 1}\n        ).to_dataset(name=\"a\")\n        expected = dedent(\n            \"\"\"\n            Left and right Dataset objects are identical\n            Dimensions only on the left object:\n            Dimensions only on the right object:\n        \"\"\"\n        )\n        assert formatting.diff_dataset_repr(a, b, compat=\"identical\") == expected\n\n", "    def test_inline_sparse_repr(self):\n\n        data = np.array([1, 2, 0, 4, 5])\n        sparse_array = sparse.COO(data, shape=(5,))\n\n        expected = \"<sparse.COO: nnz=3, fill_value=0>\"\n\n        actual = formatting.inline_sparse_repr(sparse_array)\n\n        assert actual == expected\n\n", "    def test_inline_sparse_repr(self):\n        data = np.array([0, 1, 0, 0, 2, 0])\n        i = np.array([0, 1, 4])\n        j = np.array([0, 1, 2])\n        sparse_array = sparse.COO(\n            (data, (i, j)), shape=(5, 3)\n        )\n        var = xr.Variable([\"row\", \"col\"], sparse_array)\n        expected = \"<sparse.COO: nnz=3, fill_value=0>\"\n        assert formatting.inline_sparse_repr(var.data) == expected\n", "    def test_diff_array_repr_with_long_data(self):\n        a = xr.DataArray(np.arange(100), dims=\"x\")\n        b = xr.DataArray(np.arange(100) + 1, dims=\"x\")\n\n        with pytest.warns(UserWarning):\n            diff_repr = formatting.diff_array_repr(a, b, compat=\"equals\")\n        assert \"Differing values:\" in diff_repr\n", "    def test_array_repr_empty(self):\n        data = xr.DataArray(np.array([]), dims=['dim1'], coords={'dim1': []})\n        expected = dedent(\"\"\"\n        <xarray.DataArray (dim1: 0)>\n        []\n        Dimensions without coordinates: dim1\n        \"\"\").strip()\n        assert formatting.array_repr(data) == expected\n"], "sample_558": ["    def test_grid_label_mode(self):\n        fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n        grid = AxesGrid(fig, 111, nrows_ncols=(2, 2), axes_pad=0.1,\n                        label_mode=\"L\", share_all=True)\n        grid.axes_llc.set_xlabel(\"X-axis\")\n        grid.axes_llc.set_ylabel(\"Y-axis\")\n\n        for ax in grid.axes_all:\n            ax.tick_params(axis=\"both\", which=\"major\", length=0)\n\n        self.compare_figure_and_grid(fig, grid, \"test_grid_label_mode_L.png\")\n", "def test_label_mode(mode):\n    fig, axs = plt.subplots(2, 2)\n    grid = AxesGrid(fig, 111, nrows_ncols=(2, 2),\n                    axes_pad=0.2, label_mode=mode)\n\n    for ax in grid.axes_all:\n        ax.set_xticks([-1, 1])\n        ax.set_yticks([-1, 1])\n        ax.tick_params(length=5)\n\n    fig.canvas.draw()\n    for ax in grid.axes_all:\n        for tick in ax.xaxis.get_major_ticks():\n            tick.tick1line.set_visible(False)\n            tick.tick2line.set_visible(False)\n        for tick in ax.yaxis.get_major_ticks():\n            tick.tick1line.set_visible(False)\n            tick.tick2line.set_visible(False)\n\n\n\n", "    def test_imagegrid_label_mode(self, nrows, ncols, aspect):\n        fig, axs = plt.subplots(nrows=nrows, ncols=ncols,\n                               subplot_kw={'aspect': aspect})\n        grid = ImageGrid(fig, 111, (nrows, ncols),\n                        axes_pad=0.1, label_mode=['L', '1', 'all', 'keep'][0],\n                        share_all=True)\n        for ax in grid.axes_all:\n            ax.tick_params(axis='both', which='major', labelsize=8)\n        plt.draw()\n        # This test primarily checks for no errors during the process.\n", "    def test_axes_divider_add_axes(self, orientation):\n        fig, ax = plt.subplots()\n        divider = Divider(fig, 111, orientation=orientation)\n        ax2 = divider.add_axes(0.5, 0.5, 0.3, 0.3)\n        assert ax2.get_position() == Bbox.from_bounds(0.5, 0.5, 0.3, 0.3)\n        assert ax2.axes.get_figure() == fig\n\n", "    def test_imagegrid_cbar_axes_visibility():\n        fig, axs = plt.subplots(2, 2, figsize=(6, 6))\n        grid = ImageGrid(fig, 111, nrows_ncols=(2, 2),\n                         axes_pad=0.1, label_mode=\"L\",\n                         cbar_mode=\"each\", cbar_location=\"right\",\n                         cbar_size=\"5%\", cbar_pad=0.05)\n        for ax in grid.axes_all:\n            ax.imshow(np.random.rand(10, 10))\n        for cb_ax in grid.cbar_axes:\n            assert cb_ax.get_visible() == True\n"], "sample_892": ["    def test_ada_boost_regressor_sparse_input(self):\n        # Check if AdaBoostRegressor works correctly with sparse input.\n        X_sparse_csr = csr_matrix(diabetes.data)\n        y = diabetes.target\n\n        regressor = AdaBoostRegressor(n_estimators=10)\n        regressor.fit(X_sparse_csr, y)\n        y_pred = regressor.predict(X_sparse_csr)\n\n        assert_array_almost_equal(y_pred, regressor.predict(X_sparse_csr.toarray()))\n        assert_array_almost_equal(regressor.score(X_sparse_csr, y),\n                                  regressor.score(X_sparse_csr.toarray(), y))\n\n", "    def test_ada_boost_regressor_sparse_input(self):\n        # Check if AdaBoostRegressor can handle sparse input\n        sparse_X = csc_matrix(diabetes.data)\n\n        regressor = AdaBoostRegressor(random_state=rng).fit(sparse_X, diabetes.target)\n\n        assert_array_almost_equal(\n            diabetes.target, regressor.predict(sparse_X)\n        , decimal=2)\n\n        for fmt in [csr_matrix, coo_matrix, dok_matrix, lil_matrix]:\n            sparse_X = fmt(diabetes.data)\n            regressor = AdaBoostRegressor(\n                random_state=rng).fit(sparse_X, diabetes.target)\n\n            assert_array_almost_equal(\n                diabetes.target, regressor.predict(sparse_X)\n            , decimal=2)\n\n", "    def test_ada_regressor_sample_weight(self):\n        # test if sample_weight is correctly passed through estimators\n        est = AdaBoostRegressor(base_estimator=DummyRegressor(), n_estimators=1)\n        X = [[1, 2], [3, 4], [5, 6]]\n        y = [1, 2, 3]\n        sample_weight = np.array([0.5, 1.0, 0.5])\n\n        est.fit(X, y, sample_weight=sample_weight)\n\n        # assert that the dummy regressor received the correct sample weight\n        assert_array_equal(\n            est.estimators_[0].n_samples_seen_, np.array([0.5, 1.0, 0.5]) * 3\n        )\n\n\n\n", "    def test_ada_boost_regressor_sparse_input(self):\n        # Check AdaBoostRegressor with sparse input\n        X_sparse = csc_matrix(diabetes.data)\n        y = diabetes.target\n\n        # Use a simple estimator for quick training\n        estimator = DummyRegressor(strategy=\"constant\", constant=1.0)\n        regressor = AdaBoostRegressor(\n            base_estimator=estimator, n_estimators=10, random_state=0\n        )\n        regressor.fit(X_sparse, y)\n        y_pred = regressor.predict(X_sparse)\n\n        assert_array_equal(y_pred.shape, y.shape)\n\n", "def test_ada_boost_regressor_sparse_matrix():\n    # Test AdaBoostRegressor with sparse matrix input\n    X = csc_matrix(diabetes.data)\n    y = diabetes.target\n    regressor = AdaBoostRegressor(random_state=rng)\n    regressor.fit(X, y)\n    regressor.predict(X)\n\n"], "sample_787": ["def test_hinge_loss_multiclass():\n    # Test hinge loss for multiclass classification\n    rng = np.random.RandomState(42)\n    X = rng.randn(10, 5)\n    y_true = [0, 1, 2, 0, 1, 2, 0, 1, 2, 0]\n    y_true = np.array(y_true)\n    labels = np.unique(y_true)\n\n    # Create random decision function outputs\n    y_pred = rng.randn(10, len(labels))\n    \n    hinge_loss_result = hinge_loss(y_true, y_pred, labels=labels)\n    assert isinstance(hinge_loss_result, float)\n\n", "    def test_classification_report_output_labels(self):\n        y_true = [0, 1, 2, 0, 1, 2]\n        y_pred = [0, 2, 1, 0, 0, 1]\n        target_names = ['A', 'B', 'C']\n        report = classification_report(y_true, y_pred,\n                                       target_names=target_names, output_dict=True)\n        expected_report = {'A': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2},\n                         'B': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2},\n                         'C': {'precision': 0.5, 'recall': 0.5, 'f1-score': 0.5, 'support': 2},\n                         'micro avg': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 6},\n                         'macro avg': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 6},\n                         'weighted avg': {'precision': 0.3333333333333333, 'recall': 0.3333333333333333, 'f1-score': 0.3333333333333333, 'support': 6}}\n        assert_equal(report, expected_report)\n", "    def test_brier_score_loss_empty():\n        y_true = np.array([], dtype=int)\n        y_prob = np.array([], dtype=float)\n\n        score = brier_score_loss(y_true, y_prob)\n        assert_equal(score, 0.0)\n", "    def test_brier_score_loss_sparse_labels(self):\n        # Check that brier_score_loss works with sparse labels\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        y_true_sparse = label_binarize(y_true, classes=[0, 1])\n        assert_almost_equal(\n            brier_score_loss(y_true, y_prob),\n            brier_score_loss(y_true_sparse, y_prob)\n        )\n", "    def test_brier_score_loss_multiclass(self):\n        # Test brier score loss with multiclass predictions\n        y_true = np.array([0, 1, 2, 1, 0])\n        y_prob = np.array([[0.8, 0.1, 0.1],\n                          [0.2, 0.6, 0.2],\n                          [0.1, 0.1, 0.8],\n                          [0.1, 0.8, 0.1],\n                          [0.9, 0.05, 0.05]])\n\n        score = brier_score_loss(y_true, y_prob)\n        assert_almost_equal(score, 0.6)\n\n"], "sample_378": ["    def test_bulk_update_related(self):\n        tags = self.create_tags()\n        notes = Note.objects.all()\n\n        for i, note in enumerate(notes):\n            note.tags.add(tags[i])\n\n        notes = Note.objects.select_related('tags').all()\n        updated_notes = notes\n\n        with transaction.atomic():\n            updated_notes.update(misc='updated')\n            # Ensure that the related tags are also updated (if they have a related field)\n            tag = tags[0]\n            tag.name = 'updated tag'\n            tag.save()\n\n        updated_notes = Note.objects.select_related('tags').all()\n        for i, note in enumerate(updated_notes):\n            self.assertEqual(note.misc, 'updated')\n            self.assertEqual(note.tags.all()[0].name, 'updated tag')\n\n", "    def test_bulk_update_set_default(self):\n            return 'default_value'\n\n        Note.objects.bulk_update(misc=set_default)\n        for note in Note.objects.all():\n            self.assertEqual(note.misc, 'default_value')\n", "    def test_bulk_update_f_expression(self):\n        Note.objects.filter(pk__in=self.notes[:5]).update(note=F('pk') * 2)\n        self.assertEqual(Note.objects.get(pk=1).note, 2)", "    def test_bulk_update_with_related_objects(self):\n        self.create_tags()\n        Note.objects.bulk_update_related(\n            self.notes[:5],\n            update_fields=[\"tags\"],\n            tags=self.tags[:5],\n            related_name=\"notes\"\n        )\n        for i, note in enumerate(self.notes[:5]):\n            self.assertEqual(note.tags.all(), self.tags[:i+1])\n        \n        for note in self.notes[5:]:\n            self.assertEqual(note.tags.all(), [])\n\n", "    def test_bulk_update_select_related_related_fields(self):\n        for note in self.notes:\n            note.tags.add(self.tags[note.id % len(self.tags)])\n\n        updated_notes = Note.objects.select_related('tags').filter(id__in=[1, 2, 3]).update(\n            note=F('note') + ' updated'\n        )\n        self.assertEqual(updated_notes, 3)\n\n        for note in Note.objects.filter(id__in=[1, 2, 3]):\n            self.assertIn(' updated', note.note)\n            self.assertIsNotNone(note.tags.all())\n"], "sample_834": ["def test_nca_loss_grad_consistency():\n    n_samples = 100\n    n_features = 20\n    n_components = 5\n    X = rng.randn(n_samples, n_features)\n    y = rng.randint(0, 2, n_samples)\n\n    # Initialize NCA\n    nca = NeighborhoodComponentsAnalysis(random_state=rng,\n                                       n_components=n_components,\n                                       max_iter=1)\n\n    # Compute initial loss and gradient\n    loss, gradient = nca._loss_grad_lbfgs(\n        nca._initialize(X, y, 'random').ravel(), X,\n        (y[:, np.newaxis] == y[np.newaxis, :]).astype(int))\n\n    # Use scipy.optimize.check_grad to verify gradients\n    grad_check = check_grad(lambda x: nca._loss_grad_lbfgs(x, X,\n                                                           (y[:, np.newaxis]\n                                                            == y[np.newaxis,\n                                                            :]).astype(int))[0],\n                            nca._initialize(X, y, 'random').ravel(), gradient)\n    assert grad_check < EPS\n", "    def test_nca_warm_start(self):\n        # Test that warm_start works correctly.\n        nca1 = NeighborhoodComponentsAnalysis(n_components=2, init='random',\n                                            random_state=rng, warm_start=True)\n        nca1.fit(iris_data[:100], iris_target[:100])\n\n        nca2 = clone(nca1)\n        nca2.fit(iris_data, iris_target)  \n\n        assert_array_almost_equal(nca1.components_, nca2.components_)\n", "    def test_n_components_auto_n_classes_greater(self):\n        X, y = make_classification(n_samples=100, n_features=20, n_classes=5,\n                                  random_state=rng)\n        nca = NeighborhoodComponentsAnalysis(n_components='auto')\n        nca.fit(X, y)\n        assert_equal(nca.components_.shape[0], 5)\n", "def test_nca_callback():\n    # Test callback functionality\n    rng = check_random_state(0)\n    X = rng.rand(100, 10)\n    y = rng.randint(0, 4, size=100)\n\n    class Callback(object):\n            self.calls = 0\n            self.transformation = None\n\n            self.calls += 1\n            self.transformation = transformation\n\n    callback = Callback()\n    nca = NeighborhoodComponentsAnalysis(n_components=5, callback=callback)\n    nca.fit(X, y)\n\n    assert_equal(callback.calls, nca.n_iter_)\n    assert_array_equal(nca.components_, callback.transformation.reshape(-1, 10))\n", "def test_nca_init_pca_warm_start():\n    # Test warm_start with PCA initialization.\n    nca_pca = NeighborhoodComponentsAnalysis(n_components=2, init='pca',\n                                            random_state=rng)\n    nca_pca.fit(iris_data, iris_target)\n    components_pca = nca_pca.components_\n\n    nca_pca_warm = NeighborhoodComponentsAnalysis(n_components=2, init='pca',\n                                                 warm_start=True,\n                                                 random_state=rng)\n\n    nca_pca_warm.fit(iris_data, iris_target)\n    # The components should be the same because warm_start reuses\n    # the previous solution\n    assert_array_almost_equal(components_pca, nca_pca_warm.components_)\n"], "sample_866": ["    def test_affinity_propagation_empty_clusters():\n        # Test AffinityPropagation with empty clusters when all data points are\n        # identical.\n        X = np.zeros((10, 2))\n        ap = AffinityPropagation()\n        assert_warns_message(ConvergenceWarning,\n                             \"Affinity propagation did not converge, this model \"\n                             \"will not have any cluster centers.\",\n                             ap.fit, X)\n        assert_array_equal(ap.labels_, np.full(10, -1))\n        assert len(ap.cluster_centers_indices_) == 0\n\n        # Test with precomputed affinity matrix\n        X = np.zeros((10, 10))\n        ap = AffinityPropagation(affinity='precomputed')\n        assert_warns_message(ConvergenceWarning,\n                             \"Affinity propagation did not converge, this model \"\n                             \"will not have any cluster centers.\",\n                             ap.fit, X)\n        assert_array_equal(ap.labels_, np.full(10, -1))\n        assert len(ap.cluster_centers_indices_) == 0\n\n", "    def test_equal_similarities_and_preferences():\n        S = np.array([[0, 1, 1],\n                      [1, 0, 1],\n                      [1, 1, 0]])\n        preference = np.array([1, 1, 1])\n        assert _equal_similarities_and_preferences(S, preference)\n\n        S = np.array([[0, 2, 2],\n                      [2, 0, 2],\n                      [2, 2, 0]])\n        preference = np.array([1, 1, 1])\n        assert not _equal_similarities_and_preferences(S, preference)\n\n        S = np.array([[0, 1, 1],\n                      [1, 0, 1],\n                      [1, 1, 0]])\n        preference = np.array([2, 2, 2])\n        assert not _equal_similarities_and_preferences(S, preference)\n", "    def test_affinity_propagation_same_similarities_preferences():\n        # Test the behavior when all samples have equal similarities\n        # and preferences.\n        similarities = np.ones((5, 5))\n        preferences = np.array([1] * 5)\n\n        clusters, labels, n_iter = affinity_propagation(\n            similarities, preference=preferences, return_n_iter=True)\n\n        assert_array_equal(clusters, np.array([0]))\n        assert_array_equal(labels, np.array([0] * 5))\n        assert n_iter == 0\n\n        clusters, labels, n_iter = affinity_propagation(\n            similarities, preference=preferences * 2, return_n_iter=True)\n\n        assert_array_equal(clusters, np.array([0, 1, 2, 3, 4]))\n        assert_array_equal(labels, np.array([0, 1, 2, 3, 4]))\n        assert n_iter > 0\n\n", "    def test_affinity_propagation_precomputed_sparse(self):\n        X = csr_matrix(euclidean_distances(X))\n        ap = AffinityPropagation()\n        ap.fit(X)\n        assert ap.cluster_centers_.shape[0] == n_clusters\n\n", "    def test_affinity_propagation_equal_similarities_and_preferences():\n        np.random.seed(0)\n        S = np.ones((5, 5))\n        preference = 1\n        assert _equal_similarities_and_preferences(S, preference)\n        S[0, 1] = 2\n        assert not _equal_similarities_and_preferences(S, preference)\n        S = S + np.eye(5)\n        assert not _equal_similarities_and_preferences(S, preference)\n        preference = np.array([1, 1, 1, 1, 1])\n        assert _equal_similarities_and_preferences(S, preference)\n\n"], "sample_223": ["    def test_ticket_23605(self):\n        a = Ticket23605A.objects.create()\n        b = Ticket23605B.objects.create(a=a)\n        c = Ticket23605C.objects.create(b=b)\n        self.assertEqual(Ticket23605C.objects.get(pk=c.pk).b.a, a)\n", "    def test_select_related_with_deferred(self):\n        with ignore_warnings(category=RemovedInDjango40Warning):\n            a1 = Author.objects.defer('extra').select_related('extra').get(pk=self.a1.pk)\n        self.assertEqual(a1.extra.note.misc, 'foo')\n", "    def test_select_related_join_through_null(self):\n        # Test that select_related works correctly when there's a join through a\n        # nullable ForeignKey.\n        items = Item.objects.select_related('tags').all()\n        for item in items:\n            for tag in item.tags.all():\n                self.assertEqual(item.id, tag.item.id)\n\n", "    def test_reverse_related_manager_select_related(self):\n        report_comments = Report.objects.all().select_related('creator').values_list('creator__name', flat=True)\n        self.assertIn('a1', report_comments)\n        self.assertIn('a3', report_comments)\n", "    def test_select_related_delete(self):\n        with transaction.atomic():\n            item = Item.objects.select_related('creator').get(name='one')\n            self.assertEqual(item.creator_id, self.a1.id)\n            item.delete()\n        self.assertEqual(Author.objects.count(), 3)\n"], "sample_964": ["def test_parse_annotation_basic():\n    assert _parse_annotation(\"str\") == ('str', None)\n    assert _parse_annotation(\"int\") == ('int', None)\n    assert _parse_annotation(\"List[str]\") == ('List', ('str',))\n    assert _parse_annotation(\"Dict[str, int]\") == ('Dict', ('str', 'int'))\n    assert _parse_annotation(\"Tuple[A, B, C]\") == ('Tuple', ('A', 'B', 'C'))\n\n\n", "def test_parse_annotation():\n    assert_node(\n        _parse_annotation('int'),\n        desc_annotation('int', '', desc_sig_name('int'), '', '', '')\n    )\n    assert_node(\n        _parse_annotation('list[str]'),\n        desc_annotation(\n            'list[str]', '', desc_sig_name('list'), desc_sig_punctuation('[', '['),\n            desc_annotation('str', '', desc_sig_name('str'), '', '', ''),\n            desc_sig_punctuation(']', ']'), ''\n        )\n    )\n    assert_node(\n        _parse_annotation('Dict[str, int]'),\n        desc_annotation(\n            'Dict[str, int]', '', desc_sig_name('Dict'), desc_sig_punctuation('[', '['),\n            desc_annotation('str', '', desc_sig_name('str'), '', '', ''),\n            desc_sig_punctuation(',', ','),\n            desc_annotation('int', '', desc_sig_name('int'), '', '', ''),\n            desc_sig_punctuation(']', ']'), ''\n        )\n    )\n", "def test_parse_annotation_complex():\n    assert _parse_annotation('foo: int') == ('foo', 'int')\n    assert _parse_annotation('foo: list[int]') == ('foo', 'list[int]')\n    assert _parse_annotation('foo: typing.List[int]') == ('foo', 'typing.List[int]')\n    assert _parse_annotation('foo: Union[int, str]') == ('foo', 'Union[int, str]')\n    assert _parse_annotation('foo: Dict[str, int]') == ('foo', 'Dict[str, int]')\n    assert _parse_annotation('foo: Callable[[int, str], None]') == ('foo', 'Callable[[int, str], None]')\n", "def test_parse_annotation_complex():\n    assert_node(\n        _parse_annotation('List[Tuple[str, int]]'),\n        [desc_annotation('List', 'List[Tuple[str, int]]'),\n         desc_sig_punctuation('[', ''),\n         desc_sig_name('Tuple', 'Tuple'),\n         desc_sig_punctuation('[', ''),\n         desc_sig_name('str', 'str'),\n         desc_sig_punctuation(',', ','),\n         desc_sig_name('int', 'int'),\n         desc_sig_punctuation(']', ']'),\n         desc_sig_punctuation(']', ']')])\n", "def test_parse_annotation_with_complex_types(app: Sphinx):\n    result = _parse_annotation('List[Tuple[str, int]]')\n    assert result == [('List', 'List'), ('[', '['),\n                      ('Tuple', 'Tuple'), ('[', '['),\n                      ('str', 'str'), (',', ','), ('int', 'int'), (']', ']'),\n                      (']', ']')\n    ]\n"], "sample_620": ["    def test_concat_coords_different_in_some_datasets():\n        # Regression test for #1849\n\n        da1 = DataArray(np.arange(4).reshape((2, 2)),\n                      coords={'x': np.arange(2), 'y': [1, 2]})\n        da2 = DataArray(np.arange(4).reshape((2, 2)),\n                      coords={'x': np.arange(2), 'y': [3, 4]})\n        expected = concat([da1, da2], dim='y')\n        actual = concat([da1, da2], dim='y', coords='different')\n\n        assert_identical(expected, actual)\n", "    def test_concat_override_compat_with_different_coords():\n        da1 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [2, 3]})\n        da2 = DataArray(\n            np.arange(4, 8).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [4, 5]}\n        )\n        expected = DataArray(\n            np.arange(8).reshape(4, 2), dims=[\"x\", \"y\"], coords={\"x\": [0, 1, 0, 1], \"y\": [2, 3, 4, 5]}\n        )\n        actual = concat([da1, da2], dim=\"y\", compat=\"override\")\n        assert_identical(actual, expected)\n\n\n", "def test_concat_identical_coords(compat):\n    # Test concatenation with identical coordinates\n    \n    da1 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [2, 3]})\n    da2 = DataArray(np.arange(4, 8).reshape(2, 2), dims=[\"x\", \"y\"], coords={\"x\": [0, 1], \"y\": [2, 3]})\n    \n    expected = concat([da1, da2], dim=\"y\")\n\n    assert_identical(concat([da1, da2], dim=\"y\", compat=compat), expected)\n\n", "    def test_concat_dataarray_identical_attrs():\n        data = create_test_data()\n        da1 = data[\"var1\"].to_array()\n        da2 = data[\"var2\"].to_array().rename(\"var2\").copy()\n        d1 = da1.to_dataset()\n        d2 = da2.to_dataset()\n        da1.attrs[\"foo\"] = \"bar\"\n        d1.attrs[\"foo\"] = \"bar\"\n        assert_identical(concat([da1, da2], dim=\"new_dim\"), d1.concat(d2, dim=\"new_dim\"))\n\n", "def test_concat_indexes_same_data():\n    # Test concatenating two DataArrays with the same index data but different names\n    # to make sure the resulting index data is preserved correctly.\n\n    arr1 = DataArray(np.arange(4), coords={'x': ['a', 'b', 'c', 'd']}, dims='x')\n    arr2 = DataArray(np.arange(4) * 2, coords={'x': ['a', 'b', 'c', 'd']}, dims='x')\n\n    result = concat([arr1, arr2], dim='x')\n\n    assert_identical(result.indexes['x'], PandasIndex(['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd'], name='x'))\n    assert_array_equal(result.values, np.concatenate([arr1.values, arr2.values]))\n"], "sample_759": ["    def test_onehotencoder_transform_unknown():\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X = [['Male', 1], ['Female', 3], ['Female', 2], ['Male', 5]]\n        enc.fit(X)\n        X_t = np.array([['Female', 4], ['Male', 1]])\n        Xt = enc.transform(X_t)\n\n        rows = np.array([0, 0, 1, 1, 1], dtype=int)\n        cols = np.array([0, 1, 0, 1, 2], dtype=int)\n        data = np.array([1, 1, 1, 1, 1], dtype=float)\n\n        expected = sparse.csc_matrix((data, (rows, cols)), shape=(2, 6))\n        assert_allclose(Xt.toarray(), expected.toarray())\n        assert Xt.shape == (2, 6)\n", "    def test_onehotencoder_inverse_transform_ignore_unknown(self):\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X = np.array([[0, 1], [1, 2], [3, 0]]).reshape(-1, 2)\n        enc.fit(X)\n        X_trans = enc.transform(X)\n        X_inv = enc.inverse_transform(X_trans)\n        assert_array_equal(X_inv, np.array([[0, 1], [1, 2], [None, 0]]))\n", "    def test_onehotencoder_inverse_transform_errors(self):\n        enc = OneHotEncoder().fit([[0, 1], [1, 0]])\n        # Wrong number of features\n        assert_raises_regex(ValueError,\n                            \"Shape of the passed X data is not correct. \"\n                            \"Expected 2 columns, got 1.\",\n                            enc.inverse_transform, [[0]])\n        # Wrong data type, should be integer\n        assert_raises_regex(ValueError,\n                            \"Invalid input dtype: float64. \"\n                            \"Expected dtype: int\",\n                            enc.inverse_transform, np.array([[0.5, 0.5]]))\n", "    def test_onehot_transform_sparse_input_unknown_ignore(self):\n        X = sparse.csr_matrix([[0, 1], [2, 3]])\n        enc = OneHotEncoder(handle_unknown='ignore')\n        enc.fit(X)\n        Xt = enc.transform([[2, 5]])\n        assert_allclose(toarray(Xt), [[0, 1, 0, 0]])\n\n", "    def test_handle_unknown_ignore_with_sparse_input():\n        \"\"\"Test transforming new data with unknown categories\n\n        Specifically test the case of ignore and sparse input\n        \"\"\"\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=True)\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc.fit(X)\n        X_new = [['Female', 2], ['Male', 4]]\n        X_sparse = sparse.csr_matrix(X_new)\n        with assert_warns(UserWarning):\n            Xt = enc.transform(X_sparse)\n        expected_output = sparse.csr_matrix([[0, 1, 0, 0, 0],\n                                               [1, 0, 0, 0, 0]])\n        assert_allclose(toarray(Xt), toarray(expected_output))\n"], "sample_751": ["def test_sparse_input_adaboost_classifier():\n    # Check that AdaBoostClassifier works with sparse input\n    X_train, X_test = csc_matrix(iris.data[:100]), csc_matrix(iris.data[100:])\n    y_train, y_test = iris.target[:100], iris.target[100:]\n    clf = AdaBoostClassifier(n_estimators=100, algorithm='SAMME',\n                             random_state=rng)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    assert_equal(y_pred.shape, y_test.shape)\n\n", "    def test_sparse_input_regressor(self):\n        \"\"\"Check if AdaBoostRegressor works with sparse input.\"\"\"\n        for sparse_matrix_type in [csc_matrix, csr_matrix, coo_matrix,\n                                   dok_matrix, lil_matrix]:\n            X_sparse = sparse_matrix_type(boston.data)\n            y_sparse = boston.target\n            clf = AdaBoostRegressor(DecisionTreeRegressor(max_depth=1)).fit(\n                X_sparse, y_sparse)\n            y_pred = clf.predict(X_sparse)\n            assert_equal(len(y_pred), len(y_sparse))\n\n", "    def test_sparse_matrix_transform(self):\n        # Check if AdaBoostClassifier handles sparse matrices properly.\n        # This test checks if the transform method of AdaBoostClassifier\n        # can handle sparse matrices. It creates a sparse matrix 'X_sparse'\n        # and converts it to a dense matrix 'X_dense'. Then, it fits\n        # the classifier on 'X_dense' and transforms both 'X_sparse' and\n        # 'X_dense'. The transformed outputs are compared for equality\n\n        X_dense = np.array([[1, 2], [3, 4], [5, 6]])\n        X_sparse = csr_matrix(X_dense)\n\n        clf = AdaBoostClassifier(n_estimators=2)\n        clf.fit(X_dense, y_class[:3].copy())\n\n        output_sparse = clf.transform(X_sparse)\n        output_dense = clf.transform(X_dense)\n        assert_array_equal(output_sparse, output_dense)\n\n", "    def test_sparse_input_classification():\n        # Test AdaBoostClassifier with sparse input\n        clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                                 algorithm=\"SAMME\", n_estimators=10)\n        X_train = csc_matrix(X)\n        y_train = np.array(y_class)\n        clf.fit(X_train, y_train)\n        assert_equal(clf.predict(csc_matrix(T)), y_t_class)\n\n", "    def test_adaboost_sparse_input():\n        # Check that AdaBoostClassifier works with sparse input\n        clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(\n                                     max_depth=1),\n                                  n_estimators=10)\n        X = csc_matrix(iris.data[perm[:100]])\n        y = iris.target[perm[:100]]\n        clf.fit(X, y)\n        assert_equal(clf.predict(X).shape[0], 100)\n\n        clf = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(\n                                    max_depth=1),\n                                  n_estimators=10)\n        X = csr_matrix(boston.data[perm[:100]])\n        y = boston.target[perm[:100]]\n        clf.fit(X, y)\n        assert_equal(clf.predict(X).shape[0], 100)\n"], "sample_243": ["    def test_promote_joins_or_conditions_with_different_inner_votes(self):\n        author = Author.objects.create(name='Author Name')\n        item1 = Item.objects.create(name='Item 1', author=author)\n        item2 = Item.objects.create(name='Item 2', author=author)\n\n        query = Item.objects.filter(Q(author__name='Author Name') | Q(author__isnull=True))\n\n        # The query is designed to test the following scenario:\n        # We have an OR condition with two parts:\n        # 1. author__name='Author Name' (which will create INNER join to Author)\n        # 2. author__isnull=True (which doesn't require any join)\n        #\n        # If all joins were promoted to LOUTER as a result of the OR\n        # condition, we would return wrong results for the second part of\n        # the OR condition (items that don't have an associated author)\n\n        # For a single item, get the tables used in the query\n        tables = set()\n        for alias in query.alias_map:\n            tables.add(query.alias_map[alias].table_name)\n\n        self.assertIn('items', tables)\n        self.assertIn('authors', tables)\n", "    def test_join_promotion_or_with_related_filter(self):\n        author = Author.objects.create(name='test')\n        Item.objects.create(name='item1', author=author)\n        Item.objects.create(name='item2', author=author)\n        Item.objects.create(name='item3', author=None)\n        qs = Item.objects.filter(Q(author__name='test') | Q(name='item3'))\n        self.assertEqual(qs.count(), 3)\n        # The first part of the OR condition (author__name='test') requires\n        # a join to the Author table. The second part (name='item3') doesn't.\n        # The join_promotion logic should ensure that the join to the Author\n        # table is promoted to a LEFT OUTER JOIN so that we don't accidentally\n        # exclude items that have no author.\n", "    def test_join_promotion_complex_filter(self):\n        # This test verifies that join promotion works correctly with complex\n        # filter conditions.\n\n        # Define some sample data\n        author1 = Author.objects.create(name='Author 1')\n        author2 = Author.objects.create(name='Author 2')\n        item1 = Item.objects.create(name='Item 1', author=author1)\n        item2 = Item.objects.create(name='Item 2', author=author1)\n\n        # Create a QuerySet with a complex filter condition\n        qs = Item.objects.filter(\n            Q(author=author1) | Q(name__icontains='Item'),\n            author__name__startswith='Auth',\n        )\n\n        # Assert that the generated SQL query uses the expected join types\n        self.assertEqual(\n            str(qs.query.get_compiler(qs.db).as_sql()[0]),\n            'SELECT \"my_schema\".\"item\".\"id\", \"my_schema\".\"item\".\"name\", '\n            '\"my_schema\".\"item\".\"author_id\" FROM \"my_schema\".\"item\" '\n            'INNER JOIN \"my_schema\".\"author\" ON \"my_schema\".\"item\".\"author_id\" = '\n            '\"my_schema\".\"author\".\"id\" WHERE (\"my_schema\".\"item\".\"author_id\" = %s '\n            'OR \"my_schema\".\"item\".\"name\" LIKE %s) AND \"my_schema\".\"author\".\"name\" '\n            'LIKE %s',\n        )\n\n\n\n", "    def test_join_promotion_complex_or(self):\n        # This test covers join promotion in complex OR conditions.\n        author1 = Author.objects.create(name='Author 1')\n        author2 = Author.objects.create(name='Author 2')\n        item1 = Item.objects.create(name='Item 1', authors=[author1])\n        item2 = Item.objects.create(name='Item 2', authors=[author2])\n\n        # Construct a complex query with OR conditions involving related fields.\n        query = Item.objects.filter(\n            Q(authors__name='Author 1', price__gt=10) |\n            Q(authors__name='Author 2', price__lt=5)\n        )\n\n        # Check that the generated SQL correctly handles the join promotion.\n        self.assertQuerysetEqual(query, [item1])\n\n", "    def test_query_add_select(self):\n        query = Query(model=Author)\n        query.add_select_col(\"name\")\n        self.assertEqual(query.select, ('name',))\n\n        query.add_select_col(\"age\")\n        self.assertEqual(query.select, ('name', 'age'))\n\n        query.add_select_col(F('age'))\n        self.assertEqual(query.select, ('name', 'age', F('age')))\n"], "sample_328": ["    def test_bulk_update_f(self):\n        Note.objects.filter(pk__lt=5).update(note=F('misc'))\n        for i in range(5):\n            self.assertEqual(Note.objects.get(pk=i).note, str(i))\n        for i in range(5, 10):\n            self.assertNotEqual(Note.objects.get(pk=i).note, str(i))\n", "    def test_prefetch_related_with_select_related(self):\n        for note in self.notes:\n            note.tags.add(self.tags[0])\n\n        qs = Note.objects.select_related('tags').prefetch_related(\n            'tags__details',\n        )\n        # Manually evaluate the queryset to trigger prefetching\n        list(qs)\n\n        for note in self.notes:\n            # Make sure the 'details' are loaded for all tags.\n            self.assertEqual(len(note.tags.all()[0].details.all()), 0)\n\n", "    def test_update_with_subquery(self):\n        Note.objects.filter(\n            id__in=Note.objects.filter(id__gt=4).values('id')) \\\n            .update(note='modified')\n        self.assertEqual(Note.objects.filter(note='modified').count(), 5)\n\n", "    def test_update_bulk_related(self):\n        self.create_tags()\n        notes = self.notes[:5]\n        for note, tag in zip(notes, self.tags):\n            note.tags.add(tag)\n        Note.objects.filter(pk__in=[n.pk for n in notes]).update(misc=F('note'))\n        updated_notes = Note.objects.filter(pk__in=[n.pk for n in notes])\n        for note in updated_notes:\n            self.assertEqual(note.misc, note.note)\n\n", "    def test_bulk_update_notes_with_related_objects(self):\n        for i, note in enumerate(self.notes):\n            note.tags.add(self.tags[i])\n\n        Note.objects.filter(pk__in=[self.notes[i].pk for i in range(5)]).update(\n            note=F('note')  + Lower(F('misc'))\n        )\n\n        for i in range(5):\n            self.assertEqual(self.notes[i].note, str(i) + str(i).lower())\n"], "sample_670": ["        def test_nested_parentheses():\n            assert evaluate(\"((a and b) or (c and d))\", lambda x: x in \"abcd\") is True\n", "    def test_nested_parens():\n        assert evaluate(\"(a and (b or c))\", lambda x: x in (\"a\", \"c\")) is True\n", "    def test_nested_expressions():\n        assert evaluate(\"(a and b) or (c and d)\", lambda x: x == \"a\" or x == \"d\") is True\n", "    def test_nested_parentheses():\n        assert evaluate(\"(a and (b or c))\", lambda x: x in (\"a\", \"c\")) is True\n", "    def test_nested_parentheses():\n        assert evaluate(\"((a and b) or c)\", lambda x: x in ['a', 'b', 'c']) is True\n        assert evaluate(\"((a and b) or (c and d))\", lambda x: x in ['a', 'b', 'c', 'd']) is True\n        assert evaluate(\"((a and not b) or c)\", lambda x: x in ['a', 'c']) is True\n"], "sample_475": ["    def test_admin_raw_id_fields_no_nulls(self):\n        class MyModelAdmin(ModelAdmin):\n            raw_id_fields = (\"non_existent_field\",)\n\n        class MyModel(Model):\n            name = models.CharField(max_length=100)\n\n        self.assertIsInvalid(\n            MyModelAdmin, MyModel, r\"The value of 'raw_id_fields\\[0\\]' refers to 'non_existent_field', which is not a field of 'MyModel'.\" , id=\"admin.E121\"\n        )\n", "    def test_raw_id_fields_with_related_model_no_auto_add(self):\n        class TestRawIdAdmin(admin.ModelAdmin):\n            raw_id_fields = (\"related_band\",)\n            auto_add_related_fields = False\n\n        class RelatedModel(Model):\n            name = models.CharField(max_length=100)\n\n        class TestModel(Model):\n            related_band = models.ForeignKey(RelatedModel, on_delete=CASCADE)\n\n        self.assertIsInvalid(\n            TestRawIdAdmin, TestModel, \"The 'raw_id_fields' option 'related_band' is not a ForeignKey field.\",\n            id=\"admin.E017\",\n        )\n\n", "    def test_check_raw_id_fields_on_inline(self):\n        class CustomInline(TabularInline):\n            model = Song\n            raw_id_fields = ('band',)\n        \n        class CustomAdmin(ModelAdmin):\n            inlines = [CustomInline]\n\n        self.assertIsInvalid(\n            CustomAdmin,\n            ValidationTestModel,\n            msg=r\"The 'raw_id_fields' option is deprecated for Inlines.\",\n            id=\"admin.E131\",\n        ) \n", "    def test_raw_id_fields_check(self):\n        class MyModelAdmin(ModelAdmin):\n            raw_id_fields = ('nonexistent_field',)\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            msg='The value of \\'raw_id_fields\\' refers to \\'nonexistent_field\\', which is not a field of \\'ValidationTestModel\\'.',\n            id='admin.E121',\n        )\n", "    def test_readonly_fields_callable(self):\n        class MyAdmin(ModelAdmin):\n            readonly_fields = ['get_readonly_field']\n\n                return obj.name\n        \n        self.assertIsInvalid(\n            MyAdmin,\n            ValidationTestModel,\n            r\"The value of 'readonly_fields\\['get_readonly_field'\\]' must be a field name, \"\n            r\"not 'get_readonly_field'.\",\n            id=\"admin.E035\",\n        )\n"], "sample_103": ["    def test_aggregate_float_field(self):\n        with Approximate(Decimal('30.00'), Decimal('0.01')):\n            self.assertEqual(Book.objects.aggregate(Avg('price')).get('price__avg'), Decimal('30.00'))\n        with Approximate(Decimal('39.56'), Decimal('0.01')):\n            self.assertEqual(Book.objects.filter(contact=self.a1).aggregate(Avg('price')).get('price__avg'), Decimal('39.56'))\n", "    def test_aggregate_as_subquery(self):\n        with CaptureQueriesContext(self.request) as queries:\n            sub_query = Book.objects.values('id').annotate(\n                avg_rating=Avg('rating')\n            ).filter(avg_rating__gt=3.5)\n            books = Publisher.objects.filter(book__in=sub_query)\n        self.assertQuerySetEqual(books, ['Apress', 'Sams', 'Prentice Hall', 'Morgan Kaufmann'], transform=lambda o: o.name)\n", "    def test_aggregate_window_functions(self):\n        qs = Book.objects.annotate(\n            rank=Window(\n                expression=Rank(),\n                partition_by=[F('publisher__name')],\n                order_by=[F('pubdate').desc()],\n            )\n        ).filter(rank=1)\n        self.assertQuerysetEqual(\n            qs,\n            [\n                'The Definitive Guide to Django: Web Development Done Right',\n                'Python Web Development with Django',\n                'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',\n            ],\n            transform=lambda b: b.name,\n        )\n\n", "    def test_aggregate_with_filter_on_aggregate(self):\n        with CaptureQueriesContext(connection) as ctx:\n            qs = Book.objects.annotate(\n                avg_rating=Avg('rating'),\n            ).filter(avg_rating__gt=3.5)\n\n            self.assertQuerySetEqual(\n                qs.values_list('isbn', flat=True),\n                ['159059725', '159059996', '013235613', '013790395', '155860191']\n            )\n\n            # 2 SQL queries: one to calculate avg_rating for each book,\n            # the other to filter on avg_rating\n            self.assertEqual(len(ctx.captured_queries), 2)\n\n", "    def test_aggregate_window_functions(self):\n        with CaptureQueriesContext(connection) as queries:\n            queryset = Book.objects.annotate(\n                avg_rating=Window(Avg('rating'), partition_by=[F('publisher__name')]),\n            ).order_by('publisher__name', '-avg_rating')\n        \n        self.assertEqual(len(queries), 2)\n"], "sample_538": ["    def test_TransformedPath_affine_only(self):\n        # Test that TransformedPath handles affine-only transforms correctly\n        path = Path([(0, 0), (1, 0), (1, 1), (0, 0)])\n        affine_transform = mtransforms.Affine2D().rotate(np.pi / 4)\n        transformed_path = mtransforms.TransformedPath(path, affine_transform)\n        # Get the transformed points using the affine transform\n        transformed_points = affine_transform.transform(path.vertices)\n        assert_allclose(transformed_path.vertices, transformed_points)\n", "    def test_affine_delta_transform(self):\n        t = mtransforms.Affine2D.from_values(1, 2, 3, 4, 5, 6)\n        at = mtransforms.AffineDeltaTransform(t)\n\n        p = np.array([[0, 0], [1, 1]]) \n        tp = at.transform(p)\n\n        assert_array_almost_equal(tp, t.transform(p) - t.transform([0, 0]))\n", "    def test_offset_copy():\n        fig, ax = plt.subplots()\n        t = mtransforms.Affine2D()\n        t_offset = offset_copy(t, fig=fig, x=1, y=2)\n        assert_allclose(t_offset.transform([0, 0]), [1, 2])\n        t_offset = offset_copy(t, fig=fig, x=1, y=2, units='points')\n        assert_allclose(t_offset.transform([0, 0]), [1/72, 2/72])\n", "    def test_nonsingular():\n        assert_allclose(*nonsingular(1, 1), (1, 1))\n        assert_allclose(*nonsingular(1e-300, 1e-300),\n                        (-1e-3, 1e-3))\n        assert_allclose(*nonsingular(1, -1), (-1, 1))\n        assert_allclose(*nonsingular(np.inf, 1), (-1e-3, 1e-3))\n        assert_allclose(*nonsingular(1, np.inf), (-1e-3, 1e-3))\n        assert_allclose(*nonsingular(np.nan, 1), (-1e-3, 1e-3))\n        assert_allclose(*nonsingular(1, np.nan), (-1e-3, 1e-3))\n\n\n", "    def test_transformpath():\n        verts = np.array([[1, 1], [2, 1], [2, 2], [1, 2]], float)\n        codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n        path = Path(verts, codes)\n\n        trans = mtransforms.Affine2D().translate(10, 10)\n\n        transformed_path = trans.transform_path(path)\n\n        verts = np.array([[11, 11], [12, 11], [12, 12], [11, 12]], float)\n        assert_allclose(transformed_path.vertices, verts)\n\n"], "sample_1144": ["    def test_requires_partial():\n        x, y, t = symbols('x y t')\n        assert requires_partial(Derivative(x*y, x)) is True\n        assert requires_partial(Derivative(x*y, x, y)) is True\n        assert requires_partial(Derivative(x*y, y)) is True\n        assert requires_partial(Derivative(x, x)) is False\n        assert requires_partial(x*y) is False\n        assert requires_partial(Integral(x*y, x)) is False\n        assert requires_partial(exp(x*y)) is False\n        assert requires_partial(cos(x*y)) is True\n        assert requires_partial(besselj(0, x)) is False\n        assert requires_partial(legendre(1, x)) is False\n\n\n\n", "    def test_split_super_sub_complex():\n        assert split_super_sub('a_x^1__y^2') == ('a', ['1', '2'], ['x', 'y'])\n", "    def test_requires_partial_integral():\n        x, y = symbols('x y')\n        assert requires_partial(Integral(exp(x + y), (x, 0, 1)))\n", "    def test_requires_partial_derivative():\n        x, y = symbols('x y')\n        assert requires_partial(Derivative(exp(x*y), x)) is True\n        assert requires_partial(Derivative(exp(x*y), x, y)) is True\n        assert requires_partial(Derivative(exp(x*y), y)) is True\n        assert requires_partial(Derivative(exp(x), x)) is False\n", "    def test_requires_partial_derivative_expr():\n        x, y = symbols('x y')\n        expr = Derivative(exp(x*y), x, y)\n        assert requires_partial(expr)\n"], "sample_449": ["    def test_log_message_handles_ssl_handshake(self):\n        request = self.request_factory.get('/')\n        request.META['HTTPS'] = 'on'\n        request.META['SSL_PROTOCOL'] = '\\x16\\x03'\n\n        handler = WSGIRequestHandler(Stub())\n        handler.log_message('%(request)s', request, '404', 'Not Found')\n\n        self.assertIn(\n            \"You're accessing the development server over HTTPS, but it only supports HTTP.\",\n            captured_stderr().getvalue()\n        )\n", "    def test_log_message_logs_correct_status_code(self):\n        with captured_stderr() as stderr:\n            request = self.request_factory.get(\"/\")\n            request.method = \"GET\"\n            request.environ[\"REQUEST_METHOD\"] = \"GET\"\n            request.environ[\"SERVER_PROTOCOL\"] = \"HTTP/1.1\"\n            request.environ[\"SERVER_NAME\"] = \"localhost\"\n            request.environ[\"SERVER_PORT\"] = \"8000\"\n            request.environ[\"REMOTE_ADDR\"] = \"127.0.0.1\"\n            request.environ[\"REMOTE_PORT\"] = \"50000\"\n            request.environ[\"PATH_INFO\"] = \"/\"\n            request.environ[\"QUERY_STRING\"] = \"\"\n            handler = WSGIRequestHandler(Stub(makefile=lambda: UnclosableBytesIO()),\n                                        Stub(),\n                                        Stub())\n            handler.log_message(\"Test message\",\n                                \"127.0.0.1\", \"200 OK\", request)\n\n            output = stderr.getvalue()\n            self.assertIn(\"200\", output)\n", "    def test_log_message_with_status_code(self):\n        request = self.request_factory.get('/')\n        request.META['CONTENT_LENGTH'] = '0'\n        handler = WSGIRequestHandler(Stub(makefile=lambda: UnclosableBytesIO()),\n                                     client_address=('127.0.0.1', 8000))\n\n        handler.log_message(\"Some message: %s %s\", request.path, \"200\")\n        output = handler.makefile('rb').getvalue().decode('utf-8')\n        self.assertIn('status_code=200', output)\n\n        handler.log_message(\"Some message: %s %s\", request.path, \"404\")\n        output = handler.makefile('rb').getvalue().decode('utf-8')\n        self.assertIn('status_code=404', output)\n\n        handler.log_message(\"Some message: %s %s\", request.path, \"500\")\n        output = handler.makefile('rb').getvalue().decode('utf-8')\n        self.assertIn('status_code=500', output)\n\n", "    def test_close_connection_on_error(self):\n        request = self.request_factory.get('/')\n        request._body = b''\n        with captured_stderr() as stderr:\n            handler = WSGIRequestHandler(Stub(makefile=UnclosableBytesIO))\n            handler.rfile = BytesIO(b'GET / HTTP/1.1\\r\\nHost: example.com\\r\\n\\r\\n')\n            handler.requestline = b'GET / HTTP/1.1'\n            handler.request_version = 'HTTP/1.1'\n            handler.command = 'GET'\n            handler.parse_request()\n\n            # Simulate an error during request processing.\n            handler.get_environ = lambda: {'REQUEST_METHOD': 'GET', 'PATH_INFO': '/', 'wsgi.errors': stderr}\n            handler.run(lambda environ, start_response: (\n                start_response('500 Internal Server Error', [('Content-Type', 'text/plain')]),\n                [b'Internal Server Error']\n            ))\n        self.assertIn(\"Broken pipe from\", stderr.getvalue())\n", "    def test_handle_request_with_head_method(self):\n        request = self.request_factory.head('/')\n\n        with captured_stderr() as stderr:\n            handler = WSGIRequestHandler(Stub(makefile=UnclosableBytesIO()))\n            handler.raw_requestline = request.as_bytes()\n            handler.headers = {}\n            handler.parse_request()\n            handler.get_environ()\n            handler.run(lambda *a, **kw: None)\n\n        # Assert that HEAD doesn't include Content-Length\n        self.assertNotIn('Content-Length', handler.headers)\n        self.assertEqual(stderr.getvalue(), '')\n"], "sample_849": ["    def test_gridsearchcv_refit_incompatible_with_n_jobs(self):\n        clf = SVC()\n        parameters = {'kernel': ['rbf', 'linear']}\n        grid_search = GridSearchCV(clf, parameters, scoring='accuracy',\n                                   n_jobs=2, refit=True)\n        with pytest.raises(ValueError):\n            grid_search.fit(digits.data[:100], digits.target[:100])\n\n", "    def test_train_test_split_pandas(self):\n        df = MockDataFrame({'col1': range(10), 'col2': [0] * 5 + [1] * 5})\n        train, test = train_test_split(df, test_size=0.3, random_state=42)\n        assert_array_equal(train['col1'].values, [0, 1, 2, 3, 4, 6, 7, 8, 9])\n        assert_array_equal(test['col1'].values, [5])\n\n\n\n", "def test_shuffle_split_random_state():\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    y = rng.randint(0, 2, size=10)\n\n    cv = ShuffleSplit(test_size=0.2, random_state=0)\n    splits = list(cv.split(X, y))\n    train_indices_1, test_indices_1 = splits[0]\n\n    cv = ShuffleSplit(test_size=0.2, random_state=0)\n    splits = list(cv.split(X, y))\n    train_indices_2, test_indices_2 = splits[0]\n\n    assert_array_equal(train_indices_1, train_indices_2)\n    assert_array_equal(test_indices_1, test_indices_2)\n", "    def test_shuffle_split_integer_n_splits():\n        n_samples = 100\n        for n_splits in range(1, 11):\n            ss = ShuffleSplit(n_splits=n_splits, test_size=0.25,\n                              random_state=0)\n            expected_n_splits = n_splits\n            assert ss.n_splits == expected_n_splits\n            n_splits_from_split = sum(1 for _ in ss.split(X))\n            assert n_splits_from_split == expected_n_splits\n\n", "def test_cross_val_score_sparse_matrix():\n    # Test cross_val_score with sparse matrices\n    clf = MockClassifier(allow_nd=True)\n    X = coo_matrix(digits.data)\n    y = digits.target\n    score = cross_val_score(clf, X, y, cv=5)\n    assert_allclose(score.mean(), 1 / (1 + np.abs(clf.a)),\n                    rtol=1e-4)  # Check score is as expected\n"], "sample_248": ["    def test_python_shell_executes_provided_command(self):\n        with captured_stdin(), captured_stdout():\n            call_command('shell', command=self.script_globals)\n            self.assertEqual(captured_stdout.getvalue().strip(), 'True')\n", "    def test_interactive_shell_with_command_argument(self):\n        \"\"\"Test that a command can be passed as an argument and executed.\"\"\"\n        with captured_stdout() as stdout:\n            call_command('shell', command=self.script_globals)\n        self.assertIn('True', stdout.getvalue())\n", "    def test_executes_stdin_when_available(self, mock_select, mock_isatty):\n        out = captured_stdout(lambda: call_command('shell', '-c', self.script_globals))\n        self.assertEqual(out.getvalue().strip(), 'True')\n\n", "    def test_stdin_execution(self):\n        \"\"\"Tests execution of code from stdin.\"\"\"\n        with captured_stdin():\n            sys.stdin.write(\"print('Hello from stdin')\\n\")\n            sys.stdin.seek(0)  # Reset file pointer\n            call_command('shell', stdout=captured_stdout())\n            self.assertEqual(\n                captured_stdout().getvalue().strip(), 'Hello from stdin'\n            )\n", "    def test_shell_command_no_startup(self):\n        with captured_stdout() as stdout:\n            call_command('shell', no_startup=True)\n        self.assertEqual(stdout.getvalue(), '')\n\n"], "sample_1127": ["    def test_coset_as_list(self):\n        a = Permutation(1, 2)\n        b = Permutation(0, 1)\n        G = PermutationGroup([a, b])\n        cst1 = Coset(a, G, dir=\"+\")\n        cst2 = Coset(a, G, dir=\"-\")\n        assert sorted([str(x) for x in cst1.as_list()]) == sorted(['(0 1)', '(1 2)'])\n        assert sorted([str(x) for x in cst2.as_list()]) == sorted(['(1 2)', '(0 1)'])\n\n", "def test_coset():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([b])\n    cst = Coset(a, H, dir=\"+\")\n    assert cst.is_right_coset\n    assert cst.as_list() == [Permutation(1, 2), Permutation(0, 2, 1)]\n\n", "def test_coset_identity():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    H = PermutationGroup([a])\n    c = Coset(G.identity, H)\n    assert c.as_list() == [G.identity]\n", "def test_coset():\n    G = SymmetricGroup(4)\n    a = Permutation(1, 2)\n    H = G.subgroup([a])\n    coset = Coset(a*a, H, G, dir=\"+\")\n    assert coset.is_right_coset\n    assert coset.as_list() == [Permutation((0, 1, 2, 3)), Permutation((1, 2, 0, 3))]\n    coset = Coset(a*a, H, G, dir=\"-\")\n    assert coset.is_left_coset\n    assert coset.as_list() == [Permutation((0, 1, 2, 3)), Permutation((1, 2, 0, 3))]\n", "    def test_coset(self):\n        a = Permutation(1, 2)\n        b = Permutation(0, 1)\n        G = PermutationGroup([a, b])\n        cst = Coset(a, G, dir=\"-\")\n        assert cst.is_left_coset\n        assert not cst.is_right_coset\n        cst = Coset(a, G, dir=\"+\")\n        assert not cst.is_left_coset\n        assert cst.is_right_coset\n\n"], "sample_117": ["    def test_creation_form_with_custom_user_model(self):\n        User.objects.all().delete()  # Ensure we start with an empty database\n        class CustomUserCreationForm(UserCreationForm):\n            class Meta:\n                model = CustomUser\n        with override_settings(AUTH_USER_MODEL='auth_tests.models.custom_user.CustomUser'):\n            form = CustomUserCreationForm(data={'username': 'newuser', 'password1': 'password', 'password2': 'password'})\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertEqual(user.username, 'newuser')\n            self.assertTrue(user.is_active)\n", "    def test_create_user_with_custom_field(self):\n        \"\"\"Test creating user with custom email field: CustomEmailField\"\"\"\n        with self.assertNumQueries(1):\n            form = UserCreationForm({'username': 'testuser', 'password1': 'password', 'password2': 'password'})\n            form.save()\n", "    def test_password2_invalid_password(self):\n        # Test with a password that doesn't meet the validation rules\n        form_data = {\n            'username': self.u1.username,\n            'password1': 'short',\n            'password2': 'short',\n        }\n        form = UserCreationForm(data=form_data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('password2', form.errors)\n", "    def test_user_creation_form_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.custom_user.CustomUser'):\n            form = UserCreationForm()\n            self.assertTrue(isinstance(form.fields['username'].widget, forms.TextInput))\n            self.assertFalse(form.fields['username'].required)\n\n\n\n", "    def test_create_user_with_empty_password(self):\n        form = UserCreationForm(data={'username': 'empty_password'})\n        self.assertFalse(form.is_valid())\n        self.assertIn('This field is required.', form.errors['password1'])\n"], "sample_207": ["    def test_key_transform_isnull_with_null_key(self):\n        model = JSONModel.objects.create(data={'name': 'John', 'age': None})\n        self.assertTrue(JSONModel.objects.filter(data__age__isnull=True).exists())\n        self.assertFalse(JSONModel.objects.filter(data__age__isnull=False).exists())\n", "    def test_jsonfield_key_transform_isnull(self):\n        with self.assertRaises(IntegrityError):\n            JSONModel.objects.create(data={'a': 1, 'b': 2})\n        obj = JSONModel.objects.create(data=None)\n        self.assertIsNone(KeyTransform('a').resolve_expression(obj, None, None))\n        self.assertFalse(obj.data__a__isnull)\n        self.assertTrue(obj.data__b__isnull)\n\n", "    def test_keytransform_isnull(self):\n        with ConnectionHandler(self.database) as connection:\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"INSERT INTO json_model (data) VALUES ('{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": null}' )\"\n                )\n                cursor.execute(\"INSERT INTO json_model (data) VALUES ('{\\\"key1\\\": \\\"value1\\\", \\\"key2\\\": \\\"value2\\\"}' )\")\n\n        obj1 = JSONModel.objects.get(id=1)\n        obj2 = JSONModel.objects.get(id=2)\n\n        self.assertTrue(JSONModel.objects.filter(key__isnull=True).exists())\n        self.assertFalse(JSONModel.objects.filter(key2__isnull=False).exists())\n        self.assertTrue(\n            JSONModel.objects.filter(key2__isnull=True).exclude(key1='value1').exists()\n        )\n", "    def test_key_transform_isnull_lookup_with_non_existing_key(self):\n        \"\"\"Test that KeyTransformIsNull lookup returns True for non-existing keys.\"\"\"\n        obj = JSONModel.objects.create(data={'key1': 'value1'})\n        self.assertTrue(\n            JSONModel.objects.filter(data__non_existing_key__isnull=True).exists()\n        )\n", "    def test_key_transform_isnull_true(self):\n        obj = JSONModel.objects.create(data={'key': 'value'})\n        qs = JSONModel.objects.filter(data__key__isnull=True)\n        self.assertQuerysetEqual(qs, [])\n"], "sample_880": ["    def test_type_of_target_with_non_array_like(self):\n        for target in NON_ARRAY_LIKE_EXAMPLES:\n            with pytest.raises(ValueError):\n                type_of_target(target)\n\n", "    def test_unique_labels_multilabel_sequences(self):\n        # Check if unique_labels works correctly with multilabel sequences\n        for labels in MULTILABEL_SEQUENCES:\n            unique = unique_labels(labels)\n            assert_array_equal(\n                unique, np.array([0, 1, 2], dtype=object)\n            )\n\n", "    def test_type_of_target_multilabel_sequences(self):\n        for multilabel_sequence in MULTILABEL_SEQUENCES:\n            assert type_of_target(multilabel_sequence) == \"multilabel-indicator\"\n\n", "def test_is_multilabel_empty_array():\n    assert not is_multilabel(np.array([]))\n    assert not is_multilabel(np.array([], dtype=object))\n\n", "def test_unique_labels_multilabel_sequences():\n    for sequence in MULTILABEL_SEQUENCES:\n        with pytest.raises(ValueError):\n            unique_labels(sequence)\n\n\n"], "sample_833": ["    def test_logistic_regression_cv_warm_start(self):\n        # Test warm start functionality\n        X, y = make_classification(n_samples=100, n_features=10, random_state=0)\n        clf = LogisticRegressionCV(cv=5, solver='lbfgs', max_iter=100,\n                                   warm_start=True)\n        clf.fit(X, y)\n        initial_coef = clf.coef_\n        clf.fit(X, y)\n        assert_array_almost_equal(clf.coef_, initial_coef)\n\n", "    def test_logistic_regression_cv_l1_ratios_gridsearchcv(self):\n        # check that LogisticRegressionCV works with GridSearchCV\n        # with l1_ratios parameter\n        X, y = make_classification(n_samples=100, random_state=42)\n\n        param_grid = {'penalty': ['elasticnet'],\n                      'l1_ratios': [0, 0.5, 1]}\n        clf = GridSearchCV(LogisticRegressionCV(Cs=5, cv=3),\n                           param_grid=param_grid,\n                           scoring='accuracy',\n                           refit=True)\n        clf.fit(X, y)\n", "    def test_multi_class_sparse_input_cv(self):\n        X, y = make_classification(n_samples=100, n_features=10, n_classes=3,\n                                   random_state=0)\n        X_sp = sp.csr_matrix(X)\n        clf = LogisticRegressionCV(cv=StratifiedKFold(2, shuffle=True,\n                                                    random_state=0),\n                                   multi_class='multinomial',\n                                   solver='lbfgs', random_state=0)\n        clf.fit(X_sp, y)\n        check_predictions(clf, X_sp, y)\n", "def test_logistic_regression_cv_l1_ratios_warm_starting():\n    # Test that warm-starting works correctly with l1_ratios in\n    # LogisticRegressionCV. This checks if the coefficients from\n    # previous folds are properly used as initializations.\n    X, y = make_classification(n_samples=100, n_features=20, random_state=0)\n\n    l1_ratios = [0.1, 0.5, 0.9]\n\n    # Fit with warm starting\n    clf_warm_start = LogisticRegressionCV(\n        Cs=[1.0], penalty='elasticnet', solver='saga', l1_ratios=l1_ratios,\n        max_iter=100, random_state=0, cv=3, warm_start=True\n    ).fit(X, y)\n\n    # Fit without warm starting\n    clf_no_warm_start = LogisticRegressionCV(\n        Cs=[1.0], penalty='elasticnet', solver='saga', l1_ratios=l1_ratios,\n        max_iter=100, random_state=0, cv=3, warm_start=False\n    ).fit(X, y)\n\n    # Check if the warm start improves the convergence speed\n    assert clf_warm_start.n_iter_ < clf_no_warm_start.n_iter_\n", "    def test_logistic_regression_multiclass_sparse_input(self):\n        X = sp.csr_matrix(make_classification(n_samples=100,\n                                            n_features=20,\n                                            n_informative=10,\n                                            random_state=0))\n        y = np.array([0, 1] * 50)\n        clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',\n                                 max_iter=100, tol=1e-4)\n        check_predictions(clf, X, y)\n"], "sample_770": ["    def test_silhouette_score_sparse_input():\n        X, y = datasets.make_blobs(n_samples=50, random_state=42)\n        X_csr = csr_matrix(X)\n        score = silhouette_score(X_csr, y)\n        assert_greater(score, 0.5)\n\n", "    def test_silhouette_samples_sparse(self):\n        X, y = datasets.make_blobs(n_samples=100, random_state=42)\n        X_csr = csr_matrix(X)\n        labels = np.zeros(100)\n        labels[:50] = 1\n\n        silhouette_values = silhouette_samples(X_csr, labels, metric='euclidean')\n        assert_equal(silhouette_values.shape[0], 100)\n        assert_greater(silhouette_values.mean(), 0.5)\n\n", "    def test_silhouette_samples_sparse_warn(self):\n        # check warning is raised for sparse distance matrices\n        X, y = datasets.make_blobs(n_samples=10, centers=3, random_state=42)\n        X_sparse = csr_matrix(X)\n        with pytest.warns(Warning, match='Sparse matrix'):\n            silhouette_samples(X_sparse, y)\n\n", "    def test_silhouette_samples_sparse_input(self):\n        # test silhouette_samples with sparse input\n        X, y = datasets.make_blobs(n_samples=100, random_state=42)\n        X_sparse = csr_matrix(X)\n        sil_dense = silhouette_samples(X, y)\n        sil_sparse = silhouette_samples(X_sparse, y)\n        assert_array_equal(sil_dense, sil_sparse)\n", "    def test_silhouette_samples_empty_cluster(self):\n        X = np.array([[0, 0], [1, 1], [2, 2]])\n        labels = np.array([0, 0, 1])\n        sil_samples = silhouette_samples(X, labels)\n        assert_equal(sil_samples[2], 0.0)\n"], "sample_59": ["    def test_model_set_order(self):\n        department = Department.objects.create(name='foo')\n        worker1 = Worker.objects.create(name='worker1', department=department)\n        worker2 = Worker.objects.create(name='worker2', department=department)\n        worker3 = Worker.objects.create(name='worker3', department=department)\n\n        self.assertEqual(list(department.get_worker_order()), [worker1.pk, worker2.pk, worker3.pk])\n        department.set_worker_order([worker3.pk, worker1.pk, worker2.pk])\n        self.assertEqual(list(department.get_worker_order()), [worker3.pk, worker1.pk, worker2.pk])\n", "    def test_deferred_field_creation(self):\n        # Check that deferred fields are created when accessed for the first time.\n        deferred_model = Article\n        self.assertFalse(hasattr(deferred_model, 'content'))\n        content = getattr(deferred_model, 'content')\n        self.assertTrue(hasattr(deferred_model, 'content'))\n        self.assertIsInstance(content, models.TextField)\n\n", "    def test_get_order_with_respect_to(self):\n        worker1 = Worker.objects.create(name=\"worker1\")\n        worker2 = Worker.objects.create(name=\"worker2\")\n        event1 = Event.objects.create(worker=worker1, description=\"event1\", date=datetime.date(2023, 1, 1))\n        event2 = Event.objects.create(worker=worker1, description=\"event2\", date=datetime.date(2023, 1, 2))\n        event3 = Event.objects.create(worker=worker2, description=\"event3\", date=datetime.date(2023, 1, 3))\n\n        self.assertEqual(\n            list(Event.objects.filter(worker=worker1).order_by('date').values_list('id', flat=True)),\n            [event1.id, event2.id],\n        )\n", "    def test_get_order(self):\n        # Creating instances of related model\n        event1 = Event.objects.create(name=\"Event 1\")\n        event2 = Event.objects.create(name=\"Event 2\")\n        event3 = Event.objects.create(name=\"Event 3\")\n\n        # Create instances of Department related to events\n        dept1 = Department.objects.create(name=\"Dept 1\", event=event1)\n        dept2 = Department.objects.create(name=\"Dept 2\", event=event2)\n        dept3 = Department.objects.create(name=\"Dept 3\", event=event3)\n\n        # Accessing order of departments\n        order = dept1.get_event_order(event1)\n\n        # Asserting order\n        self.assertEqual(list(order), [dept1.pk])\n\n        # Changing the order\n        event1.set_department_order([dept3.pk, dept2.pk, dept1.pk])\n\n        # Asserting the new order\n        order = dept1.get_event_order(event1)\n        self.assertEqual(list(order), [dept3.pk, dept2.pk, dept1.pk])\n", "    def test_order_with_respect_to_ordering(self):\n        \"\"\"\n        Test ordering with respect to a related field.\n        \"\"\"\n        worker1 = Worker.objects.create(name='Alice', department=Department.objects.create(name='Finance'))\n        worker2 = Worker.objects.create(name='Bob', department=Department.objects.create(name='Sales'))\n        worker3 = Worker.objects.create(name='Charlie', department=worker1.department)\n\n        # Check initial order\n        workers = Worker.objects.all().order_by('department__name')\n\n        self.assertEqual(list(workers), [worker1, worker3, worker2])\n\n        # Change order\n        worker3.department = worker2.department\n        worker3.save()\n\n        # Check new order\n        workers = Worker.objects.all().order_by('department__name')\n\n        self.assertEqual(list(workers), [worker2, worker3, worker1])\n"], "sample_1104": ["def test_printmethod():\n    class A(Expr):\n            super().__init__()\n\n            return 'A'\n\n    a = A()\n    assert sstr(a) == 'A'\n", "def test_print_Tr():\n    M = Matrix([[1, 2], [3, 4]])\n    assert sstr(Tr(M)) == 'Tr(Matrix([[1, 2], [3, 4]]))'\n\n", "    def test_printmethod(self):\n        class A(Expr):\n                return \"A()\"\n        a = A()\n        assert sstr(a) == \"A()\"\n        assert sstr(a, printmethod=\"sympy\") == \"A()\"\n\n", "def test_printmethod():\n    from sympy.abc import x\n\n    class MyClass(Expr):\n            self.arg = arg\n\n            return f\"%s*MyClass\" % self.arg\n\n    a = MyClass(x)\n    assert sstr(a) == 'x*MyClass'\n", "def test_print_EmptySet():\n    assert sstr(EmptySet()) == 'EmptySet'\n\n"], "sample_697": ["    def test_getbasetemp_with_given_basetemp(self, tmp_path_factory_with_given_basetemp: TempPathFactory) -> None:\n        basetemp = tmp_path_factory_with_given_basetemp.getbasetemp()\n        assert basetemp == self.given_basetemp\n        assert basetemp.is_dir()\n", "    def test_tmp_path_factory_numbered(self, tmp_path_factory: TempPathFactory) -> None:\n        p1 = tmp_path_factory.mktemp(\"foo\", numbered=True)\n        p2 = tmp_path_factory.mktemp(\"foo\", numbered=True)\n        assert str(p1) != str(p2)\n        assert p1.name.startswith(\"foo-\")\n        assert p2.name.startswith(\"foo-\")\n        assert int(p1.name.split(\"-\")[1]) < int(p2.name.split(\"-\")[1])\n", "def test_get_user(monkeypatch):\n        return \"testuser\"\n\n    monkeypatch.setattr(getpass, \"getuser\", mock_getuser)\n    assert get_user() == \"testuser\"\n\n    monkeypatch.setattr(getpass, \"getuser\", lambda: None)\n    assert get_user() is None\n", "    def test_mktemp_with_numbered_dir(self, tmp_path_factory: TempPathFactory) -> None:\n        p1 = tmp_path_factory.mktemp(\"foo-\", numbered=True)\n        p2 = tmp_path_factory.mktemp(\"foo-\", numbered=True)\n        assert p1.name.startswith(\"foo-\")\n        assert p2.name.startswith(\"foo-\")\n        assert p1.name != p2.name\n", "    def test_tmpdirhandler_getbasetemp_with_given_basetemp(self, tmpdir) -> None:\n        config = tmpdir.config\n\n        config._tmp_path_factory = TempPathFactory(\n            given_basetemp=Path(tmpdir.join(\"given_basetemp\")),\n            trace=(),\n            _ispytest=True,\n        )\n\n        t = TempdirFactory(config._tmp_path_factory, _ispytest=True)\n        config._tmpdirhandler = t\n        assert t.getbasetemp().resolve() == Path(tmpdir.join(\"given_basetemp\")).resolve()\n"], "sample_238": ["    def test_round_function(self):\n        self.assertQuerysetEqual(\n            Book.objects.annotate(rounded_price=Round('price')),\n            [\n                {'rounded_price': Decimal('30'), 'isbn': '159059725'},\n                {'rounded_price': Decimal('23'), 'isbn': '067232959'},\n                {'rounded_price': Decimal('30'), 'isbn': '159059996'},\n                {'rounded_price': Decimal('30'), 'isbn': '013235613'},\n                {'rounded_price': Decimal('83'), 'isbn': '013790395'},\n                {'rounded_price': Decimal('75'), 'isbn': '155860191'},\n            ],\n            transform=lambda x: x.values('rounded_price', 'isbn')\n        )\n\n", "    def test_round_function(self):\n        self.assertQuerySetEqual(\n            Book.objects.annotate(rounded_price=Round('price', 0)).order_by('rounded_price'),\n            [\n                {'rounded_price': 30}, {'rounded_price': 23}, {'rounded_price': 30},\n                {'rounded_price': 30}, {'rounded_price': 83}, {'rounded_price': 75},\n            ],\n            transform(lambda x: x['rounded_price'])\n        )\n", "    def test_function_window(self):\n        with CaptureQueriesContext(connection) as queries:\n            result = (\n                Book.objects.annotate(\n                    rank=Window(\n                        ExpressionWrapper(\n                            Rank()\n                            .over(partition_by=F('publisher__name'), order_by=F('pubdate').asc()),\n                            output_field=IntegerField()),\n                    )\n                )\n                .filter(rank=1)\n                .values_list('name', flat=True)\n            )\n            self.assertEqual(\n                list(result),\n                [\n                    'The Definitive Guide to Django: Web Development Done Right',\n                    'Sams Teach Yourself Django in 24 Hours',\n                    'Python Web Development with Django',\n                ]\n            )\n        self.assertEqual(queries.count(), 2)\n", "    def test_window_functions(self):\n        # Test various window functions\n        with CaptureQueriesContext(connection) as captured_queries:\n            # Test ROW_NUMBER()\n            result = (\n                Book.objects.annotate(\n                    row_num=Window(\n                        expression=RowNumber(),\n                        partition_by=F('publisher__name'),\n                    )\n                )\n                .order_by('publisher__name', 'row_num')\n            ).values_list('name', 'row_num', 'publisher__name')\n            self.assertSequenceEqual(\n                [\n                    ('The Definitive Guide to Django: Web Development Done Right', 1, 'Apress'),\n                    ('Practical Django Projects', 2, 'Apress'),\n                    ('Sams Teach Yourself Django in 24 Hours', 1, 'Sams'),\n                    ('Python Web Development with Django', 1, 'Prentice Hall'),\n                    ('Artificial Intelligence: A Modern Approach', 2, 'Prentice Hall'),\n                    ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1, 'Morgan Kaufmann'),\n                ],\n                list(result)\n            )\n\n            # Test RANK()\n            result = (\n                Book.objects.annotate(\n                    page_rank=Window(\n                        expression=Rank(),\n                        order_by=F('pages').desc(),\n                    )\n                )\n                .order_by('page_rank')\n            ).values_list('name', 'page_rank', 'pages')\n            self.assertEqual(\n                [('Artificial Intelligence: A Modern Approach', 1, 1132),\n                 ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 2, 946),\n                 ('Sams Teach Yourself Django in 24 Hours', 3, 528),\n                 ('The Definitive Guide to Django: Web Development Done Right', 4, 447),\n                 ('Python Web Development with Django', 5, 350),\n                 ('Practical Django Projects', 6, 300)],\n                list(result)\n            )\n\n            # Test DENSE_RANK()\n            result = (\n                Book.objects.annotate(\n                    dense", "    def test_window_functions(self):\n        with Approximate(Decimal('29.69'), Decimal('0.01')) as approx:\n            query = Book.objects.annotate(\n                avg_price=Window(\n                    expression=Avg('price'),\n                    partition_by=F('contact'),\n                    order_by=F('pubdate'),\n                )\n            ).filter(avg_price=approx)\n            self.assertQuerysetEqual(query, [self.b3], [])\n\n"], "sample_1151": ["    def test_mod_mod():\n        assert Mod(x**2 % y, y) == Mod(x**2, y)\n        assert Mod(Mod(x, y), y) == Mod(x, y)\n", "    def test_mod_issue_21373(self):\n        # See issue 21373: https://github.com/sympy/sympy/issues/21373\n        # Mod should not raise PolynomialError for non-polynomial arguments\n        with warns_deprecated_sympy():\n            m = Mod(x**2 + 1, x + 1)\n        assert isinstance(m, Mod)\n", "    def test_mod_gcd():\n        a = 2*x + 4*y\n        b = 2*x + 2*y\n        assert Mod(a,b).args == (a,b)\n\n        # GCD simplification should occur\n        assert Mod(a,b).expand() == Mod(2*y, 2*x + 2*y)\n", "def test_mod_Mul():\n    assert Mod(Mul(2, x), 3) == Mod(2*x, 3)\n    assert Mod(Mul(2, x + 1), 3) == Mod(2*(x + 1), 3)\n    assert Mod(Mul(2, x, y), 3) == Mod(2*x*y, 3)\n\n", "    def test_mod_simplification():\n        p = Mod(x**2 + 2*x, y)\n        q = Mod(x**2, y) + Mod(2*x, y)\n        assert p.equals(q) \n"], "sample_393": ["    def test_ignore_patterns(self):\n        copytree(\n            os.path.join(self.django_source_dir, 'tests', 'fixtures', 'makemessages'),\n            self.workdir\n        )\n        with open(os.path.join(self.workdir, 'settings.py'), 'a') as f:\n            f.write(\"\\nIGNORE_PATTERNS = ['.*templates/']\\n\")\n        out, po_contents = self._run_makemessages(domain='django', locale=[LOCALE])\n        self.assertNotMsgId('Template error message', po_contents)\n        self.assertMsgId('Admin index')\n", "    def test_makemessages_no_location(self):\n        copytree(\n            os.path.join(\n                os.path.dirname(os.path.abspath(__file__)), \"test_project\", \"myapp\"\n            ),\n            os.path.join(self.tmpdir, \"myapp\"),\n        )\n        with open(os.path.join(self.tmpdir, \"myapp\", \"views.py\"), \"a\") as f:\n            f.write(\n                \"\\n\"\n                'gettext_lazy(\"This string should be extracted.\")\\n'\n                'pgettext(\"context\", \"This string should also be extracted.\")'\n            )\n        output, po_contents = self._run_makemessages(\n            no_location=True, verbosity=2\n        )\n        self.assertMsgStr('\"This string should be extracted.\"', po_contents)\n        self.assertMsgStr('\"This string should also be extracted.\"', po_contents)\n        self.assertLocationCommentNotPresent(self.PO_FILE, \"views.py\", \"12\")\n", "    def test_ignores_files_in_ignored_paths(self):\n        copytree(\n            os.path.join(self.django_data_path, \"testapp\"), os.path.join(self.tmpdir, \"testapp\")\n        )\n        with open(os.path.join(self.tmpdir, \"testapp\", \"templates\", \"ignore.txt\"), \"w\"):\n            pass\n        with open(\n            os.path.join(\n                self.tmpdir, \"testapp\", \"static\", \"testapp\", \"ignore.html\"\n            ),\n            \"w\",\n        ):\n            pass\n        # Create a settings file that ignores certain paths\n        settings_file = os.path.join(self.tmpdir, \"settings.py\")\n        with open(settings_file, \"w\") as f:\n            f.write(\n                \"\"\"\n            INSTALLED_APPS = ['testapp']\n            MEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n            STATIC_ROOT = os.path.join(BASE_DIR, 'static')\n            IGNORE_PATTERNS = [\n                'testapp/templates/ignore.txt',\n                'testapp/static/testapp/ignore.html',\n            ]\n            \"\"\"\n            )\n        # Execute makemessages with the custom settings\n        execute_from_command_line([\n            \"makemessages\",\n            \"--settings=%s\" % settings_file,\n            \"--locale=%s\" % LOCALE,\n            \"--verbosity=2\",\n        ])\n\n        # Check that no translation files were created for ignored paths\n\n        self.assertFalse(os.path.exists(os.path.join(self.tmpdir, LOCALE, 'LC_MESSAGES', 'django.po')))\n", "    def test_add_location_with_full(self):\n        \"\"\"\n        Test the --add-location=full option to include both filename and line number.\n        \"\"\"\n        out, po_contents = self._run_makemessages(add_location=\"full\")\n        self.assertLocationCommentPresent(self.PO_FILE, 11, \"templates\", \"project\", \"base.html\")\n        self.assertMsgStr('Hello world!', po_contents)\n", "    def test_makemessages_no_locale_path(self):\n        with mock.patch('django.core.management.commands.makemessages.settings.LOCALE_PATHS', []):\n            with captured_stderr() as stderr:\n                out, po_contents = self._run_makemessages()\n                self.assertIn(\"invalid locale %s\" % LOCALE, stderr.getvalue())\n                self.assertNotIn(\n                    \"processing file %s\" % os.path.join(\"commands\", \"test_template.html\"),\n                    out,\n                )\n                self.assertFalse(os.path.exists(self.PO_FILE))\n"], "sample_857": ["    def test_sparse_input_regressor(self):\n        for tree_class in REG_TREES.values():\n            for Xs, ys in DATASETS.items():\n                if Xs.startswith(\"sparse\"):\n                    reg = tree_class(random_state=0)\n                    reg.fit(Xs, ys[\"y\"])\n                    assert reg.tree_.n_features == Xs.shape[1]\n                    pred = reg.predict(Xs)\n                    score = mean_squared_error(ys[\"y\"], pred)\n                    assert score < 1.0\n", "    def test_tree_sparse_matrix_feature_values(self):\n        # Test that trees can handle sparse matrices correctly,\n        # especially for feature_values\n        X = sparse_random_matrix(20, 10, density=0.25,\n                                  random_state=0)\n        y = np.arange(20) % 2  # Binary classification\n\n        # Fit a decision tree\n        clf = DecisionTreeClassifier()\n        clf.fit(X, y)\n\n        # Check if feature_values array is not empty\n        assert clf.tree_.feature_values.size > 0\n\n", "    def test_sparse_input_extra_trees(self):\n        # Check ExtraTreeClassifier and ExtraTreeRegressor work with sparse input\n\n        for TreeEstimator in [ExtraTreeClassifier, ExtraTreeRegressor]:\n            clf = TreeEstimator().fit(DATASETS[\"sparse-mix\"][\"X_sparse\"],\n                                       DATASETS[\"sparse-mix\"][\"y\"])\n            clf_dense = TreeEstimator().fit(DATASETS[\"sparse-mix\"][\"X\"],\n                                           DATASETS[\"sparse-mix\"][\"y\"])\n\n            assert_array_equal(clf.predict(DATASETS[\"sparse-mix\"][\"X_sparse\"]),\n                               clf_dense.predict(DATASETS[\"sparse-mix\"][\"X\"]))\n\n", "    def test_sparse_input_extra_tree_classifier(self):\n        for sparse_format in ['csc', 'csr', 'coo']:\n            X_sparse = DATASETS[\"clf_small\"][\"X_sparse\"].toformat(sparse_format)\n            y = DATASETS[\"clf_small\"][\"y\"]\n\n            clf = ExtraTreeClassifier(random_state=0).fit(X_sparse, y)\n            clf_dense = ExtraTreeClassifier(random_state=0).fit(\n                DATASETS[\"clf_small\"][\"X\"], y)\n\n            assert_array_equal(clf.predict(X_sparse), clf_dense.predict(\n                DATASETS[\"clf_small\"][\"X\"]))\n", "def test_tree_sparse_input():\n    for name, data in DATASETS.items():\n        if name in ('digits', \"iris\", \"boston\"):\n            continue\n\n        X = data['X_sparse']\n        y = data['y']\n\n        for tree_class in ALL_TREES.values():\n            if tree_class in (ExtraTreeClassifier, ExtraTreeRegressor):\n                continue\n\n            tree = tree_class()\n            tree.fit(X, y)\n\n            # Check that the predict method can handle both sparse and dense input\n            y_pred_sparse = tree.predict(X)\n            y_pred_dense = tree.predict(data['X'])\n            assert_array_equal(y_pred_sparse, y_pred_dense)\n"], "sample_179": ["    def test_long_column_names(self):\n        # Skip if database doesn't have a name length limit\n        allowed_len, db_alias = get_max_column_name_length()\n        if allowed_len is None:\n            return\n\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}), self.assertRaises(\n                Error\n        ):\n            class LongColumnNameModel(models.Model):\n                name = models.CharField(max_length=255)\n                long_field_name = models.CharField(\n                    max_length=255,\n                    db_column=f'{\"a\" * (allowed_len + 1)}'", "    def test_index_together_with_lookups(self):\n        class MyModel(models.Model):\n            field1 = models.CharField(max_length=100)\n            field2 = models.CharField(max_length=100)\n\n            class Meta:\n                index_together = [('field1', 'field2__lower')]\n\n        with override_settings(DATABASE_ROUTERS=[EmptyRouter]):\n            register_lookup(Lower, 'lower', lookup_name='lower')\n            with self.assertRaisesMessage(\n                 TypeError,\n                 \"Index together must be a list of field names, but got 'field2__lower'\"\n            ):\n                MyModel._meta.indexes\n\n", "    def test_check_index_together_invalid_types(self):\n        errors = Model._check_index_together()\n        expected_errors = [\n            checks.Error(\n                \"'index_together' must be a list or tuple.\",\n                obj=Model,\n                id='models.E008',\n            )\n        ]\n        self.assertEqual(errors, expected_errors)\n", "    def test_index_together_with_invalid_field_names(self):\n        class A(models.Model):\n            name = models.CharField(max_length=100)\n            some_field = models.IntegerField()\n        with self.assertRaisesMessage(\n            TypeError,\n            \"The 'index_together' option must be a list or tuple of tuples. \"\n            \"Got: <class 'str'>.\",\n        ):\n            A._meta.index_together = 'name'\n        with self.assertRaisesMessage(\n            ValueError,\n            \"'A' has no field named 'invalid_field'.\",\n        ):\n            A._meta.index_together = [('name', 'invalid_field')]\n", "    def test_check_index_together_with_functions(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            value = models.IntegerField()\n\n            class Meta:\n                index_together = [('name', Lower('name'))]\n\n        errors = MyModel.check(databases=['default'])\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn(\"Index together doesn't allow function expressions in field names.\", errors[0].msg)\n"], "sample_689": ["def test_deprecated_fillfuncargs(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import pytest\n\n            funcargs = pyfuncitem.funcargs\n            funcargs['arg'] = 42\n            return funcargs\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert arg == 42\n        \"\"\"\n    )\n    with pytest.warns(deprecated.FILLFUNCARGS):\n        pytester.runpytest()\n", "    def test_deprecated_collect_module(self, pytester: Pytester) -> None:\n        pytester.parseconfig(\"--collect-only\")\n        pytester.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n        with warnings.catch_warnings(record=True) as record:\n            pytester.runpytest(\"--collect-only\")\n        for warning in record:\n            assert (\n                re.search(\n                    r\"pytest\\.collect\\.(.+?) was moved to pytest\\.\\1\",\n                    str(warning.message),\n                )\n                is not None\n            )\n", "    def test_deprecated_collect_module(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-v\", p)\n        result.assert_outcomes(passed=1)\n        assert re.search(\n            r\"pytest\\.collect\\.test_hello was moved to pytest\\.test_hello\",\n            result.stdout.str(),\n        )\n", "    def test_deprecated_fillfuncargs(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n                request._fillfixtures()\n\n        \"\"\"\n        )\n        with pytest.warns(deprecated.FILLFUNCARGS):\n            pytester.runpytest(\"-c\", \"conftest.py\")\n", "    def test_pytest_collect_module(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            \"\"\"\n        )\n        with warnings.catch_warnings(record=True) as record:\n            testdir.runpytest(\"--collect-only\")\n        assert any(\n            deprecated.PYTEST_COLLECT_MODULE.match(warning.message)\n            for warning in record\n        )\n"], "sample_132": ["    def test_callable_setting_wrapper_str(self):\n        wrapped_setting = CallableSettingWrapper(lambda: 1)\n        self.assertEqual(str(wrapped_setting), str(callable))\n", "    def test_callable_setting_wrapper_repr(self):\n        class Dummy():\n                return 'Dummy()'\n        wrapped = CallableSettingWrapper(Dummy())\n        self.assertEqual(repr(wrapped), 'Dummy()')\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'some_value')\n        self.assertEqual(repr(wrapped_callable), repr(lambda: 'some_value'))\n", "    def test_callable_setting_wrapper_doesnt_call_callable(self):\n        wrapped_callable = mock.MagicMock()\n        wrapper = CallableSettingWrapper(wrapped_callable)\n        self.assertEqual(str(wrapper), str(wrapped_callable))\n        wrapped_callable.assert_not_called()\n", "    def test_callable_setting_wrapper_repr(self):\n        class Setting:\n                return '<Setting>'\n\n        wrapper = CallableSettingWrapper(Setting())\n        self.assertEqual(repr(wrapper), repr(Setting()))\n"], "sample_921": ["    def test_stringify_signature_with_return_annotation(self):\n            pass\n\n        sig = inspect.signature(func)\n        result = stringify_signature(sig, show_annotation=True)\n        assert result == '(a: int, b: str) -> datetime.datetime'\n\n\n", "    def test_stringify_signature_empty_return_annotation(self):\n        sig = inspect.Signature(lambda: None, has_retval=False)\n        assert stringify_signature(sig, show_return_annotation=False) == '()'\n        assert stringify_signature(sig, show_return_annotation=True) == '()'\n\n", "    def test_is_builtin_class_method(self):\n        class MyClass:\n                pass\n\n        assert inspect.is_builtin_class_method(int, '__init__')\n        assert inspect.is_builtin_class_method(MyClass, '__init__') is False\n\n", "    def test_is_builtin_class_method_with_cpython_descriptor(self):\n        # Test case for CPython bug in descriptor for int.__init__\n        class IntSubclass(int):\n            pass\n\n        assert inspect.is_builtin_class_method(IntSubclass, '__init__')\n", "    def test_is_builtin_class_method_cpython_implementation(self):\n        class MyClass:\n                pass\n\n        assert inspect.is_builtin_class_method(MyClass, '__init__') is False\n"], "sample_568": ["def test_poly3dcollection_zsort(plt):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    verts = [[(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n             [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)]]\n    col = art3d.Poly3DCollection(verts, facecolors=['r', 'b'])\n    col.set_zsort('min')\n    ax.add_collection3d(col)\n\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_zlim([0, 1])\n\n    ax.view_init(elev=30., azim=45)\n\n    # Check that the polygons are sorted by their minimum z-value\n    z_order = [p.get_zorder() for p in col.get_patches()]\n    assert z_order[0] < z_order[1]\n\n    # Test that setting the sort zpos to a fixed value overrides the\n    # automatic sorting.\n    col.set_sort_zpos(0.5)\n\n    # After setting a fixed zpos, the order should be determined based on that\n    # value\n    z_order = [p.get_zorder() for p in col.get_patches()]\n    assert z_order[0] == z_order[1]\n", "def test_poly3dcollection_shade():\n    # Test that shade works correctly for Poly3DCollection objects. \n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    verts = [\n        [[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0], [0, 0, 0]],\n        [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1], [0, 0, 1]],\n    ]\n\n    pc = art3d.Poly3DCollection(verts, facecolors='blue',\n                              edgecolors='black', shade=True, alpha=0.8)\n    ax.add_collection3d(pc)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_zlim([0, 1])\n    \n    return fig\n\n", "def test_poly3dcollection_shade():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    verts = [\n        #[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0],\n        [0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1],\n    ]\n    # Create a Poly3DCollection with shading\n    pc = art3d.Poly3DCollection(\n        verts,\n        shade=True,\n        lightsource=mpl.colors.LightSource(\n            azdeg=45, altdeg=30\n        ),\n    )\n    ax.add_collection(pc)\n    ax.set_xlim([0, 2])\n    ax.set_ylim([0, 2])\n    ax.set_zlim([0, 2])\n    return fig\n", "    def test_poly3dcollection_shade(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        verts = [\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n            [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n        ]\n        poly = art3d.Poly3DCollection(verts, facecolors=['r', 'g'],\n                                      edgecolors='k',\n                                      shade=True,\n                                      lightsource=mcolors.LightSource(azdeg=135, altdeg=30))\n        ax.add_collection(poly)\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        ax.set_zlim([0, 1])\n        plt.draw()\n", "def test_poly3dcollection_shade(ax):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    verts = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n        [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n    ]\n        \n    col = art3d.Poly3DCollection(verts, shade=True)\n    ax.add_collection(col)\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    return fig\n"], "sample_23": ["    def test_longitude_wrap_angle_setter():\n        lon = Longitude(180 * u.deg)\n        assert lon.wrap_angle == Angle(360 * u.deg)\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == Angle(180 * u.deg)\n        assert_allclose(lon, 180 * u.deg)\n\n        lon = Longitude(180 * u.deg, wrap_angle=180 * u.deg)\n        assert lon.wrap_angle == Angle(180 * u.deg)\n", "    def test_longitude_wrap_angle(self):\n        # Test setting wrap angle and wrapping\n        l = Longitude(np.array([300, 350, -10]) * u.deg)\n\n        l.wrap_angle = 180 * u.deg\n        assert_allclose(l.degree, [300 - 360, 350 - 360, -10], atol=1e-10)\n\n        l.wrap_angle = -180 * u.deg\n        assert_allclose(l.degree, [300 - 360, 350 - 360, -10], atol=1e-10)\n\n        l = Longitude(np.array([300, 350, -10]) * u.deg, wrap_angle=180 * u.deg)\n        assert_allclose(l.degree, [300 - 360, 350 - 360, -10], atol=1e-10)\n\n        l = Longitude(\n            np.array([300, 350, -10]) * u.deg, wrap_angle=\"180d\"\n        )  # Test string input\n        assert_allclose(l.degree, [300 - 360, 350 - 360, -10], atol=1e-10)\n", "def test_longitude_wrap_angle():\n    l = Longitude(np.arange(720) * u.deg, wrap_angle=180 * u.deg)\n    assert_allclose(l.degree, np.arange(720) % 360 - 180)\n    l = Longitude(np.arange(720) * u.deg, wrap_angle=-180 * u.deg)\n    assert_allclose(l.degree, np.arange(720) % 360 - 180)\n\n", "def test_wrap_angle_property():\n    a = Angle('10d')\n    assert a.wrap_angle == 360 * u.deg\n\n    a.wrap_angle = 180 * u.deg\n    assert a.wrap_angle == 180 * u.deg\n\n    a = Longitude('10d')\n    assert a.wrap_angle == 360 * u.deg\n\n    a.wrap_angle = 180 * u.deg\n    assert a.wrap_angle == 180 * u.deg\n", "    def test_angle_to_string_format_latex_inline(self):\n        a = Angle(1.234567, unit=u.rad)\n        s = a.to_string(format='latex_inline')\n        assert s == r'$1.234567 \\; \\mathrm{rad}$'\n"], "sample_1199": ["    def test_tensor_product_simp_trace(self):\n        A = Matrix([[1, 2], [3, 4]])\n        B = Matrix([[5, 6], [7, 8]])\n\n        tp = TensorProduct(A, B)\n        tr = Tr(tp)\n        self.assertEqual(tr.doit(), 70)\n\n", "def test_TP_simp_trace():\n    a = Qubit('0')\n    b = Qubit('1')\n    tp = TensorProduct(a, b)\n    assert tensor_product_simp(Tr(tp)) == Tr(a)*Tr(b)\n\n", "    def test_tensorproduct_simp_matrix(self):\n        tp1 = TensorProduct(mat1,mat2)\n        tp2 = TensorProduct(mat2,mat1)\n        self.assertEqual(tensor_product_simp(tp1*tp2), tp2*tp1)\n", "    def test_tensorproduct_simp_comm(self):\n        A, B = symbols('A,B', commutative=False)\n        C = symbols('C', commutative=True)\n        tp = TensorProduct(A, B)*C\n        tp_simp = tensor_product_simp(tp)\n        self.assertEqual(tp_simp, C*TensorProduct(A, B))\n", "    def test_TensorProduct_Dagger_simplification(self):\n        tp = TensorProduct(A, B)\n        self.assertEqual(Dagger(tp), Dagger(B)xDagger(A))\n        tp = TensorProduct(Dagger(A), B)\n        self.assertEqual(Dagger(tp), Dagger(B)xDagger(Dagger(A)))\n        tp = TensorProduct(A, Dagger(B))\n        self.assertEqual(Dagger(tp), Dagger(Dagger(B))xDagger(A))\n\n"], "sample_750": ["def test_omp_gram_return_path():\n    # Test that orthogonal_mp_gram returns the correct coefficient path\n    rng = check_random_state(0)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    G, Xy = np.dot(X.T, X), np.dot(X.T, y)\n\n    coef, n_iter = orthogonal_mp_gram(G, Xy, n_nonzero_coefs=3,\n                                        return_path=True)\n\n    assert_equal(coef.shape, (n_features, n_targets, 3))\n", "    def test_omp_gram_empty_Xy(self):\n        \"\"\"Test that orthogonal_mp_gram raises ValueError for empty Xy.\"\"\"\n        rng = check_random_state(0)\n        X = rng.randn(10, 5)\n        G = np.dot(X.T, X)\n        Xy = np.array([])\n        assert_raises(ValueError, orthogonal_mp_gram, G, Xy, 2)\n\n", "    def test_orthogonal_mp_gram_n_targets(self):\n        for n_targets in [1, 3]:\n            y_ = y[:, :n_targets]\n            coef = orthogonal_mp_gram(\n                G, Xy[:, :n_targets], n_nonzero_coefs=2,\n            )\n            assert (coef.shape[0] == n_features)\n            assert (coef.shape[1] == n_targets)\n", "def test_orthogonal_mp_gram_tol():\n    # Test that orthogonal_mp_gram works with tol\n    rng = check_random_state(0)\n    X = rng.randn(n_samples, n_features)\n    y = rng.randn(n_samples, n_targets)\n    Gram = np.dot(X.T, X)\n    Xy = np.dot(X.T, y)\n    norms_squared = np.sum(y ** 2, axis=0)\n\n    coef_tol = orthogonal_mp_gram(Gram, Xy, tol=0.1,\n                                  norms_squared=norms_squared)\n    coef_n_coefs = orthogonal_mp_gram(Gram, Xy, n_nonzero_coefs=5,\n                                      norms_squared=norms_squared)\n    assert_array_equal(coef_tol.shape, coef_n_coefs.shape)\n\n", "    def test_omp_gram_tol(self):\n        with ignore_warnings():\n            coefs, _ = orthogonal_mp_gram(G, Xy, tol=10, norms_squared=np.sum(y ** 2, axis=0))\n        assert_equal(coefs.shape, (n_features, n_targets))\n"], "sample_908": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_parse_function_def(source, expected):\n    node = ast.parse(source)\n    assert unparse(node) == expected\n"], "sample_1040": ["def test_function():\n    a = Function('f')(x)\n    assert mathml(a) == '<apply><ci type=\"function\">f</ci><ci>x</ci></apply>'\n\n", "def test_MatrixSymbol():\n    A = MatrixSymbol('A', 2, 3)\n    assert mathml(A) == '<ci>A</ci>'\n    assert mathml(x*A) == '<apply><times/><ci>x</ci> <ci>A</ci></apply>'\n\n", "def test_print_MatrixSymbol():\n    ms = MatrixSymbol('A', 2, 3)\n    assert mathml(ms) == '<ci>A</ci>'\n    assert mathml(ms, printer='presentation') == '<mi>A</mi>'\n\n", "def test_print_matrixsymbol():\n    assert mathml(MatrixSymbol('A', 2, 2), printer='presentation') == '<mi>A</mi>'\n\n", "def test_print_matrixsymbol():\n    m = MatrixSymbol('M', 2, 3)\n    assert mathml(m) == '<ci>M</ci>'\n\n"], "sample_188": ["    def test_window_expression_as_sql(self):\n        sql, params = Window(ExpressionWrapper(F('salary'), output_field=IntegerField()), order_by=F('salary')).as_sql(connection.compiler(), connection)\n        self.assertEqual(sql, 'salary OVER (ORDER BY salary)')\n\n", "    def test_subquery_ordering(self):\n        subquery = Employee.objects.filter(salary__gt=20).values('id')\n        outer_query = Company.objects.annotate(\n            high_salary_employees=Count('employees__id', filter=Q(employees__id__in=subquery))\n        ).order_by('high_salary_employees')\n        self.assertQuerysetEqual(\n            outer_query,\n            [\n                '<Company: Test GmbH>',\n                '<Company: Example Inc.>',\n                '<Company: Foobar Ltd.>',\n            ],\n            transform=lambda company: company\n        )\n", "    def test_when_with_lookups(self):\n        qs = Employee.objects.annotate(\n            is_high_salary=When(Q(salary__gt=15), then=True),\n        )\n        self.assertEqual(\n            list(qs.values_list('is_high_salary', flat=True)),\n            [False, False, True],\n        )\n\n", "    def test_json_field_lookup(self):\n        # Create a company with JSON field\n        company = Company.objects.create(\n            name=\"JSONCompany\",\n            json_data='{\"key\": \"value\"}'\n        )\n\n        # Look up the company using a JSON field lookup\n        found_company = Company.objects.filter(json_data__key=\"value\").first()\n\n        self.assertEqual(found_company, company)\n", "    def test_case_when_with_nulls(self):\n        with self.assertRaisesMessage(\n            DatabaseError, \"This backend does not support NULLS FIRST/LAST in ORDER BY\"\n        ):\n            list(\n                Company.objects.annotate(\n                    rank=Case(\n                        When(num_employees__gt=100, then=1),\n                        default=2,\n                        output_field=IntegerField(),\n                    )\n                ).order_by(\n                    'rank', \"num_chairs\", nulls_first=True\n                )\n            )\n\n"], "sample_638": ["    def test_run_with_invalid_format(self, mock_subprocess, mock_graphviz):\n        with mock.patch(\"pylint.pyreverse.writer.DiagramWriter\") as mock_writer:\n            main.Run([\"test_module.py\", \"--output=XYZ\"]).run([])\n            mock_subprocess.run.assert_called_once_with(\n                [\"/usr/bin/dot\", \"-TXYZ\", \"-o\", \"test_module.XYZ\"],\n                capture_output=True,\n                text=True,\n            )\n            mock_writer.assert_not_called()\n", "def test_pyreverse_main_with_unsupported_format(\n    mock_subprocess, mock_graphviz, tmpdir", "def test_main_with_unsupported_format(mock_subprocess, mock_graphviz, capsys):\n    main.Run([\"test_module\", \"-o\", \"XYZ\"]).run([])\n    captured = capsys.readouterr()\n    assert \"Format XYZ is not supported natively.\" in captured.err\n", "def test_main_with_invalid_format(mock_subprocess, mock_graphviz):\n    \"\"\"Test main function with an unsupported format.\"\"\"\n    with fix_import_path([]):\n        with pytest.raises(SystemExit):\n            main.Run([\"--output\", \"XYZ\", \"test_module\"])\n\n\n", "def test_run_with_unsupported_format(mock_subprocess, mock_graphviz):\n    with mock.patch(\"pylint.pyreverse.writer.DiagramWriter\"):\n        main.Run([\"module1\", \"--output-format=XYZ\"]).run([])\n        mock_subprocess.run.assert_called_with(\n            [\"/usr/bin/dot\", \"-TXYZ\", \"-o\", \"module1.XYZ\", \"module1.dot\"],\n            check=True,\n        )\n\n"], "sample_595": ["def test_str_strip(dtype):\n    arr = xr.DataArray([\"  foo  \", \" bar \", \"baz   \", None], dtype=dtype)\n    expected = xr.DataArray([\"foo\", \"bar\", \"baz\", None], dtype=dtype)\n    assert_equal(arr.str.strip(), expected)\n", "def test_str_pad(dtype):\n    arr = xr.DataArray([\"foo\", \"bar\", \"baz\"], dtype=dtype)\n    result = arr.str.pad(width=5, side=\"right\", fillchar=\"*\")\n    expected = xr.DataArray([\"foo**\", \"bar**\", \"baz**\"], dtype=dtype)\n    assert_equal(result, expected)\n\n    result = arr.str.pad(width=5, side=\"left\", fillchar=\"*\")\n    expected = xr.DataArray([\"**foo\", \"**bar\", \"**baz\"], dtype=dtype)\n    assert_equal(result, expected)\n", "    def test_str_accessor_encode_decode(self, dtype):\n        # GH 18063\n        data = [\n            \"caf\u00e9\",\n            \"restaurant\",\n            \"\u6c49\u8bed\",\n        ]\n        arr = xr.DataArray(\n            np.array(data, dtype=dtype), dims=[\"x\"], coords={\"x\": range(len(data))}\n        )\n        encoded = arr.str.encode(\"utf-8\")\n        decoded = encoded.str.decode(\"utf-8\")\n        assert_equal(decoded, arr)\n", "    def test_replace_regex_callable(self):\n        # GH 19311\n            return match.group(0).upper()\n\n        result = xr.DataArray([\"abc\", \"def\"], dtype=object).str.replace(\n            \"b|e\", repl, regex=True\n        )\n        expected = xr.DataArray([\"aBc\", \"dEf\"], dtype=object)\n        assert_equal(result, expected)\n", "def test_str_accessor_contains(dtype):\n    arr = xr.DataArray([\"foo\", \"bar\", \"baz\"], dtype=dtype)\n    assert_equal(arr.str.contains(\"o\"), [True, False, True])\n"], "sample_975": ["    def test_nsolve_issue11768():\n        x, y = symbols('x y')\n        f = Eq(x**2 + y**2 - 1, 0)\n        sol = nsolve(f, (x, y), (0.5, 0.5))\n        assert len(sol) == 2\n", "    def test_issue_11768():\n        x = Symbol('x')\n        f = lambda x: sin(x)**2 - 1/2\n        sol = nsolve(Eq(f(x), 0), 0, solver='bisect')\n        assert abs(sol - pi/2) < 1e-5\n", "def test_unrad_sqrt_add_two_terms():\n    x = Symbol('x')\n    eq = sqrt(x) + sqrt(x + 1) - 2\n    sol = unrad(eq)\n    assert sol is not None\n    eq, cov = sol\n    assert eq.is_polynomial(x)\n    assert cov == []\n\n", "    def test_nsolve_issue_8069():\n        x = Symbol('x')\n        f = Eq(sqrt(x + 1) - x, 0)\n        sol = nsolve(f, 0)\n        assert abs(sol - 1.618033988749895) < 1e-6\n", "    def test_unrad_poly_system():\n        x, y = symbols('x y')\n        eqs = [\n            sqrt(x) + root(y, 3) - 2,\n            x**2 + y**2 - 1,\n        ]\n        sol = nsolve(eqs, [x, y], [0, 0])\n        assert len(sol) == 2\n"], "sample_847": ["    def test_multi_task_lasso_cv_predict(self):\n        X, y = load_boston(return_X_y=True)\n        X = X[:, 1:4]  # take only 3 features to reduce computation time\n        y = np.vstack((y, y)).T\n        n_samples, n_features = X.shape\n\n        clf = MultiTaskLassoCV(cv=3).fit(X, y)\n        \n\n        # Test prediction\n        y_pred = clf.predict(X[:5])\n        assert_array_equal(y_pred.shape, (5, 2))\n", "    def test_multi_task_elastic_net_cv_warm_start(self):\n        X, y = load_boston(True)\n        n_samples, n_features = X.shape\n        y = np.column_stack((y, y))  # Two identical target columns\n        \n        clf = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .9],\n                                     cv=3, warm_start=True, random_state=42)\n        clf.fit(X, y)\n", "    def test_multitask_lasso_cv_n_jobs(self):\n        X, y = self.make_regression(n_samples=50, n_targets=2, n_features=10,\n                                    noise=4, random_state=0)\n        lasso_cv = MultiTaskLassoCV(cv=5, n_jobs=2, random_state=0)\n        lasso_cv.fit(X, y)\n", "    def test_multi_task_lasso_cv_copy_X(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1], [2], [3]])\n        clf = MultiTaskLassoCV(copy_X=True)\n        clf.fit(X, y)\n        assert(clf.coef_.shape == (1, 2))\n        assert(clf._get_param_names().count('X') == 0)\n", "    def test_multi_task_elastic_net_cv_copy_x(self):\n        X, y = load_boston(return_X_y=True)\n        y = np.column_stack((y, y))  # Create a multi-task problem\n        X = check_array(X, copy=True)\n        clf = MultiTaskElasticNetCV(copy_X=True, random_state=0)\n        clf.fit(X, y)\n        assert_array_equal(X.data, X.data.copy())\n\n\n\n"], "sample_966": ["def test_parse_annotation():\n    assert _parse_annotation('None') == ('None', [])\n    assert _parse_annotation('int') == ('int', [])\n    assert _parse_annotation('list[str]') == ('list', [('str', [])])\n    assert _parse_annotation('dict[str, int]') == ('dict', [('str', []), ('int', [])])\n    assert _parse_annotation('Tuple[A, B]') == ('Tuple', [('A', []), ('B', [])])\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', [([], ['int']), ('str', [])])\n    \n", "    def test_parse_annotation(self):\n        assert _parse_annotation('str') == ('str', None)\n        assert _parse_annotation('List[str]') == ('List', 'str')\n        assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n        assert _parse_annotation('Callable[[int, str], bool]') == ('Callable',\n                                                               ((int, str), bool))\n        assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n        assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n        assert _parse_annotation('Tuple[int, str, ...]') == ('Tuple', (int, str, '...'))\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('str...') == ('str', '...')\n    assert _parse_annotation('list[int]') == ('list', 'int')\n    assert _parse_annotation('dict[str, int]') == ('dict', 'str, int')\n    assert _parse_annotation('Tuple[str, int]') == ('Tuple', 'str, int')\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', '[[int], str]')\n\n    # Test for cases where the type annotation contains nested brackets\n    assert _parse_annotation('List[Dict[str, int]]') == ('List', 'Dict[str, int]')\n\n    # Test for cases where the type annotation contains whitespace\n    assert _parse_annotation('List[ Dict[str, int] ]') == ('List', 'Dict[str, int]')\n", "    def test_parse_annotation_complex(self):\n        result = _parse_annotation('List[Tuple[str, int]]')\n\n        assert result == [('List', None, 'List'),\n                         ('Tuple', None, 'Tuple'),\n                         ('str', None, 'str'),\n                         ('int', None, 'int')]\n\n", "def test_parse_annotation_complex():\n    # Test parsing complex annotations with nested types and generics\n    annotation = \"List[Tuple[str, int]]\"\n    result = _parse_annotation(annotation)\n    assert result == [\n        addnodes.desc_annotation('List', 'List'),\n        addnodes.desc_sig_punctuation('[', '['),\n        addnodes.desc_annotation('Tuple', 'Tuple'),\n        addnodes.desc_sig_punctuation('[', '['),\n        addnodes.desc_annotation('str', 'str'),\n        addnodes.desc_sig_punctuation(',', ','),\n        addnodes.desc_annotation('int', 'int'),\n        addnodes.desc_sig_punctuation(']', ']'),\n        addnodes.desc_sig_punctuation(']', ']'),\n    ]\n"], "sample_757": ["    def test_fit_transform_object_dtype(self):\n        X = np.array([['Male', 1], ['Female', 3], ['Female', 2]],\n                     dtype=object)\n        enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        Xt = enc.fit_transform(X)\n        assert_allclose(Xt, [[0., 1., 0., 1., 0.],\n                             [1., 0., 1., 0., 0.],\n                             [1., 0., 0., 0., 1.]])\n\n\n", "    def test_onehotencoder_inverse_transform_unknown(self):\n        enc = OneHotEncoder(handle_unknown='ignore')\n        X = [['Male', 'A'], ['Female', 'B'], ['Female', 'C']]\n        enc.fit(X)\n\n        X_new = [['Male', 'A'], ['Female', 'D']]\n        X_tr = enc.inverse_transform(enc.transform(X_new))\n        assert_equal(X_tr, [['Male', None], ['Female', None]])\n\n        X_tr = enc.inverse_transform(enc.transform(X))\n        assert_equal(X_tr, [['Male', 'A'], ['Female', 'B'], ['Female', 'C']])\n", "    def test_one_hot_encoder_inverse_transform_unknown(self):\n        enc = OneHotEncoder(handle_unknown='ignore')\n        enc.fit([[1, 0], [2, 1], [3, 0], [0, 0]])\n        X_t = enc.transform([[1, 2]])\n        with pytest.warns(UserWarning):\n            X_inv = enc.inverse_transform(X_t)\n        assert_array_equal(X_inv, [[1, None]])\n", "    def test_onehotencoder_legacy_mode_ignore_unknown():\n        # Test OneHotEncoder with 'ignore' for handle_unknown and legacy mode\n        X = np.array([[0, 1], [1, 2], [3, 0]])\n        enc = OneHotEncoder(handle_unknown='ignore', categorical_features='all')\n        with assert_warns(FutureWarning):\n            enc.fit(X)\n        X_transformed = enc.transform(np.array([[0, 4], [1, 2], [3, 0]]))\n        expected_result = sparse.csr_matrix(\n            [[1., 0.],\n             [0., 1.],\n             [0., 0.],\n             [1., 0.],\n             [0., 1.],\n             [0., 0.]])\n        assert_allclose(toarray(X_transformed), toarray(expected_result))\n\n\n", "    def test_inverse_transform_unknown_categories_sparse(self):\n        enc = OrdinalEncoder(handle_unknown='ignore')\n        X = [['a', 'b'], ['b', 'c'], ['c', 'd']]\n        enc.fit(X)\n        X_new = [['b', 'e'], ['a', 'c']]\n        X_trans = enc.transform(X_new)\n        X_inv = enc.inverse_transform(X_trans)\n        assert_equal(X_inv[0], ['b', None])\n        assert_equal(X_inv[1], ['a', 'c'])\n"], "sample_747": ["    def test_quantile_transform_errors(self):\n        X = [[1, 2], [3, 4]]\n        qt = QuantileTransformer(n_quantiles=1)\n        assert_raises(ValueError, qt.fit, X)\n        qt = QuantileTransformer(output_distribution=\"foo\")\n        assert_raises(ValueError, qt.fit, X)\n", "def test_quantile_transform_n_quantiles():\n    # Test if quantile_transform works correctly with different n_quantiles\n    # values\n\n    n_quantiles_list = [2, 5, 10, 100]\n    for n_quantiles in n_quantiles_list:\n\n        X = np.array([-2., -1., 0., 1., 2.], dtype=np.float64)\n        X_transformed = quantile_transform(X.reshape(-1, 1),\n                                           n_quantiles=n_quantiles,\n                                           random_state=0)\n\n        # Assert that the output array has the correct shape\n        assert X_transformed.shape == (5, 1)\n\n", "    def test_quantile_transform_output():\n        X = np.array([[-1, 1], [0, 2], [1, 1]])\n        n_quantiles = 3\n        transformer = QuantileTransformer(n_quantiles=n_quantiles,\n                                          random_state=0)\n\n        X_trans = transformer.fit_transform(X)\n        assert_array_equal(X_trans.shape, X.shape)\n        assert_almost_equal(\n            X_trans, np.array([[0., 0.5], [0.5, 1.], [1., 0.5]]), 2)\n\n        # Output should be in the range [0, 1]\n        assert_greater_equal(X_trans.min(), 0)\n        assert_less_equal(X_trans.max(), 1)\n\n\n", "    def test_categorical_encoder_inverse_transform_unknown(self):\n        # Test inverse_transform with 'ignore' on unseen categories\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc = CategoricalEncoder(handle_unknown='ignore')\n        enc.fit(X)\n        X_trans = enc.transform([['Female', 1], ['Male', 4]])\n        X_inv = enc.inverse_transform(X_trans)\n\n        # Check that unknown category is None\n        assert_equal(X_inv[1, 1], None)\n", "    def test_categorical_encoder_inverse_transform_unknown_categories(self):\n        # Test inverse transform with unknown categories, ignore option\n        enc = CategoricalEncoder(handle_unknown='ignore')\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc.fit(X)\n        X_trans = enc.transform([['Female', 1], ['Male', 4]])\n        X_inv = enc.inverse_transform(X_trans)\n        assert_array_equal(X_inv, [['Female', 1], [None,  None]])\n\n\n"], "sample_17": ["    def test_shape(self):\n        self.check(np.shape, self.q)\n", "    def test_shape(self):\n        self.check(np.shape, self.q)\n", "    def test_concatenate(self):\n        self.check(np.concatenate, (self.q, self.q))\n\n", "    def test_array_equiv(self):\n        self.check(np.array_equiv, self.q, self.q, dtype=float)\n", "    def test_reshape(self):\n        self.check(np.reshape, (4, 4, 3),\n                   order='C')\n"], "sample_115": ["    def test_callable_setting_wrapper(self):\n        simple_lazy_obj = SimpleLazyObject(lambda: 'lazy_value')\n        wrapper = CallableSettingWrapper(simple_lazy_obj)\n        self.assertEqual(repr(wrapper), repr(simple_lazy_obj))\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'test')\n        self.assertEqual(repr(wrapped_callable), \"<function <lambda> at 0x...> wrapped\")\n", "    def test_get_default_exception_reporter_filter(self):\n        self.assertIsInstance(get_default_exception_reporter_filter(), CallableSettingWrapper)\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'wrapped')\n        self.assertIsInstance(wrapped_callable, CallableSettingWrapper)\n        self.assertEqual(repr(wrapped_callable), '<CallableSettingWrapper object>')\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'test_value')\n        self.assertEqual(repr(wrapped_callable), '<bound method lambda of <object object at ...>>(...)')\n"], "sample_1147": ["    def test_latex_vector_ops(self):\n        N = CoordSys3D('N')\n        v1 = N.i + 2*N.j + 3*N.k\n        v2 = 2*N.i - N.j + N.k\n        assert latex(v1) == r'\\mathbf{i} + 2 \\mathbf{j} + 3 \\mathbf{k}'\n        assert latex(v2) == r'2 \\mathbf{i} - \\mathbf{j} + \\mathbf{k}'\n        assert latex(v1.dot(v2)) == r'\\mathbf{N}[\\mathbf{i}] \\cdot \\mathbf{N}[\\mathbf{i}] + 2 \\mathbf{N}[\\mathbf{j}] \\cdot \\mathbf{N}[\\mathbf{i}] + 3 \\mathbf{N}[\\mathbf{k}] \\cdot \\mathbf{N}[\\mathbf{i}]'\n\n        assert latex(Cross(v1, v2)) == latex(v1.cross(v2))\n\n", "def test_latex_printing_of_transferfunction():\n    s = symbols('s')\n    tf = TransferFunction(1, s + 1)\n    assert latex(tf) == r'\\frac{1}{s + 1}'\n\n    tf2 = TransferFunction(2*s + 1, s**2 + 2*s + 1)\n    assert latex(tf2) == r'\\frac{2 s + 1}{s^{2} + 2 s + 1}'\n\n    series = Series(tf, tf2)\n    assert latex(series) == r'\\frac{1}{s + 1} \\circ \\frac{2 s + 1}{s^{2} + 2 s + 1}'\n\n    parallel = Parallel(tf, tf2)\n    assert latex(parallel) == r'\\frac{1}{s + 1} \\parallel \\frac{2 s + 1}{s^{2} + 2 s + 1}'\n\n    feedback = Feedback(tf, tf2)\n    assert latex(feedback) == r'\\frac{1}{s + 1} \\bullet \\frac{2 s + 1}{s^{2} + 2 s + 1}'\n", "def test_latex_Product():\n    i = Symbol('i')\n    n = Symbol('n')\n    expr = Product(i**2, (i, 1, n))\n    assert latex(expr) == r\"\\prod_{i=1}^{n} i^{2}\"\n", "def test_latex_SetExpr():\n    S = SetExpr('S', symbols('x'))\n    assert latex(S) == '\\\\left\\{x \\\\right\\}'\n    assert latex(Union(S, symbols('y'))) == '\\\\left\\{x, y \\\\right\\}'\n    assert latex(Intersection(S, symbols('y'))) == '\\\\left\\{x \\\\cap y \\\\right\\}'\n    assert latex(Complement(S, symbols('y'))) == '\\\\left\\{x \\\\setminus y \\\\right\\}'\n    assert latex(SymmetricDifference(S, symbols('y'))) == '\\\\left\\{x \\\\triangle y \\\\right\\}'\n    assert latex(ProductSet(S, symbols('y'))) == '\\\\left\\{x \\\\times y \\\\right\\}'\n", "def test_latex_MatrixSymbol():\n    A = MatrixSymbol('A', 3, 3)\n    assert latex(A) == r'A'\n    assert latex(A, mat_symbol_style='bold') == r'\\mathbf{A}'\n"], "sample_380": ["    def test_variance_empty_result_set(self):\n        with CaptureQueriesContext(using=self.db_name) as captured_queries:\n            self.assertEqual(Book.objects.filter(isbn='nonexistent').aggregate(variance=Variance('price')), {'variance': None})\n        self.assertQuerySetEqual(captured_queries, [])\n\n", "    def test_aggregate_in_union(self):\n        with CaptureQueriesContext(connection) as queries:\n            # Test Avg with union\n            result = Book.objects.filter(price__gt=Decimal('25')).annotate(\n                avg_price=Avg('price')\n            ).union(\n                Book.objects.filter(price__lt=Decimal('25')).annotate(\n                    avg_price=Avg('price')\n                )\n            ).order_by('avg_price')\n\n        self.assertQuerySetEqual(result, [\n            Book.objects.get(isbn='159059996'),\n            Book.objects.get(isbn='013235613'),\n            Book.objects.get(isbn='067232959'),\n            Book.objects.get(isbn='159059725'),\n            Book.objects.get(isbn='013790395'),\n            Book.objects.get(isbn='155860191'),\n        ], transform=lambda book: book.isbn)\n        self.assertEqual(len(queries), 3)\n\n", "    def test_aggregate_filter_clause(self):\n        # Test that aggregate function with filter clause works correctly\n        # when database backend supports it.\n        with CaptureQueriesContext(connection) as queries:\n            result = Book.objects.filter(rating__gt=3).aggregate(Avg('price', filter=Q(pages__gt=300)))\n        self.assertEqual(result['price__avg'], Decimal('36.34'))\n        self.assertEqual(len(queries), 1)\n\n", "    def test_aggregate_with_filter_clause(self):\n        with CaptureQueriesContext(connection) as queries:\n            books = Book.objects.annotate(\n                avg_rating=Avg('rating', filter=Q(price__gt=Decimal('25.00'))),\n            ).filter(avg_rating__isnull=False).order_by('avg_rating').values('name', 'avg_rating')\n", "    def test_variance_with_filter(self):\n        variance_age = Author.objects.annotate(\n            variance_age=Variance('age', sample=True)\n        ).filter(name__startswith='J').values('variance_age').get()\n        self.assertAlmostEqual(variance_age['variance_age'], 49.5, places=1)\n"], "sample_924": ["    def test_enum(self):\n        check(\"enum\",\n              \"enum MyEnum {{ VALUE1, VALUE2 = 2, VALUE3 }}\",\n              {1: 'myenum'},\n              output=\"enum MyEnum {{ VALUE1, VALUE2 = 2, VALUE3 }}\",\n              key=\"MyEnum\")\n", "    def test_enum_scoped_enum_with_nested_class():\n        check(\n            'class',\n            \"enum class {key}Enum {{ VALUE}}; struct S {{ {key}Enum e; }};\",\n            {1: \"Enum\", 2: \"S::Enum\"},\n            \"\"\"\n                \\", "    def test_function_template_args_no_name(self):\n        check('function',\n              \"void foo(typename T, int I) {{}}\",\n              {1: 'foo__typename_T_int_I', 2: 'foo_T_I'},\n              output=\"void foo(typename T, int I) {{}}\",\n              key='foo')\n", "    def test_function_template(self):\n        check('function', 'void func(int a,typename T) { }',\n              idDict={1: 'func'},\n              output='void func(int a, typename T) { }')\n", "    def test_function_template_args():\n        check('function', 'void foo(int a, typename T)',\n              idDict={1: 'foo-void-int-typename-T'},\n              output='void foo(int a, typename T)')\n"], "sample_186": ["    def test_inlines_with_invalid_model(self):\n        class InvalidInline(admin.TabularInline):\n            model = 'nonexistent_model'\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [InvalidInline]\n\n        admin_site = AdminSite()\n        admin_site.register(Author, MyAdmin)\n        checks = admin_site._check_checks()\n        self.assertEqual(len(checks), 2)\n        self.assertEqual(checks[0].id, 'admin.E104')\n        self.assertEqual(checks[1].id, 'admin.E103')\n", "    def test_check_middleware_classes(self):\n        admin_site = AdminSite()\n        admin_site.register(Book, ValidFields)\n        middleware = admin_site.get_middleware()\n\n        # Check that the required middleware classes are present.\n\n        self.assertIn(AuthenticationMiddlewareSubclass, middleware)\n        self.assertIn(MessageMiddlewareSubclass, middleware)\n        self.assertIn(SessionMiddlewareSubclass, middleware)\n\n        # Ensure the middleware is in the correct order.\n        self.assertEqual(\n            middleware.index(AuthenticationMiddlewareSubclass),\n            middleware.index(SessionMiddlewareSubclass) - 1\n        )\n        self.assertEqual(\n            middleware.index(SessionMiddlewareSubclass),\n            middleware.index(MessageMiddlewareSubclass) - 1\n        )\n\n\n\n", "    def test_check_formset_is_subclass_of_BaseModelFormSet(self):\n        class MyFormSet(forms.ModelForm):\n            pass\n\n        class MyInline(admin.TabularInline):\n            formset = MyFormSet\n\n        errors = AdminSite(name='test').check()\n        self.assertEqual(\n            [e.id for e in errors if e.msg.startswith(\"The value of 'formset' must inherit\")],\n            ['admin.E206']\n        )\n", "    def test_check_middleware(self):\n        admin_site = AdminSite()\n        admin_site.register(Song, MyAdmin)\n        errors = admin_site.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.W101')\n\n", "    def test_check_exclude_of_parent_model_error(self):\n        class CustomInline(admin.TabularInline):\n            model = Song\n            fk_name = 'album'\n            exclude = ('album',)\n\n        class CustomAdmin(admin.ModelAdmin):\n            inlines = [CustomInline]\n\n        admin_site = AdminSite()\n        admin_site.register(Album, CustomAdmin)\n        errors = admin_site.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E201')\n"], "sample_397": ["    def test_get_template_context_processors(self):\n        # Ensure the context processors are imported correctly\n        engine = Engine(context_processors=['django.template.context_processors.request',\n                                             'template_tests.test_response.test_processor_name'])\n        processors = engine.template_context_processors\n        self.assertIn(test_processor_name, processors)\n", "    def test_get_default(self):\n        from django.template import engines\n\n        with override_settings(TEMPLATES=[\n            {\n                'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                'DIRS': [],\n                'OPTIONS': {},\n            },\n        ]):\n            engine = Engine.get_default()\n            self.assertIsInstance(engine, Engine)\n            self.assertEqual(len(engines.all()), 1)\n\n        with self.assertRaises(ImproperlyConfigured):\n            with override_settings(TEMPLATES=[]):\n                Engine.get_default()\n", "    def test_template_context_processors(self):\n        \"\"\"\n        Test that template context processors are correctly loaded and executed.\n        \"\"\"\n        context_processors = [\n            'django.template.context_processors.request',\n            'template_tests.test_response.test_processor_name',\n        ]\n\n        engine = Engine(context_processors=context_processors)\n        template = engine.from_string(\"{{ request.resolver_match.url_name }} {{ test_name }}\")\n        context = RequestContext(self.request_factory.get('/'), current_app=self.backend_name)\n        rendered = template.render(context)\n\n        self.assertIn('my_url_name', rendered)\n        self.assertIn('test_value', rendered)\n", "    def test_get_default(self):\n        with override_settings(TEMPLATES=[\n                {\n                    'BACKEND': 'django.template.backends.django.DjangoTemplates',\n                    'DIRS': [],\n                    'APP_DIRS': True,\n                    'OPTIONS': {\n                        'context_processors': [test_processor_name],\n                    },\n                },\n            ]):\n            engine = Engine.get_default()\n            self.assertIsInstance(engine, Engine)\n            self.assertEqual(engine.context_processors, [test_processor_name])\n\n", "    def test_get_template_builtins(self):\n        engine = Engine(builtins=[\"django.template.defaulttags\"])\n        builtins = engine.get_template_builtins(engine.builtins)\n        self.assertEqual(len(builtins), 1)\n        self.assertIsNotNone(builtins[0])\n"], "sample_825": ["def test_plsca_singular_matrix():\n    # Test PLS-CA with a singular covariance matrix\n    rng = check_random_state(42)\n    X = rng.rand(10, 2)\n    X[:, 1] = X[:, 0]  # Make the second column a linear combination of the first\n    Y = rng.rand(10, 2)\n    plsca = PLSCanonical(n_components=1)\n    plsca.fit(X, Y)\n    assert_warns(ConvergenceWarning, plsca.fit, X, Y)\n\n", "    def test_pls_regression_fit_transform(self):\n        rng = check_random_state(0)\n        n_samples = 100\n        X = rng.randn(n_samples, 5)\n        Y = rng.randn(n_samples, 2)\n\n        pls2 = pls_.PLSRegression(n_components=2)\n        X_t, Y_t = pls2.fit_transform(X, Y)\n        assert_equal(X_t.shape, (n_samples, 2))\n        assert_equal(Y_t.shape, (n_samples, 2))\n", "    def test_plssvd_fit_transform_output(self):\n        X, y = load_linnerud(return_X_y=True)\n        pls = PLSSVD(n_components=2)\n        Xs, Ys = pls.fit_transform(X, y)\n        assert Xs.shape == (y.shape[0], 2)\n        assert Ys.shape == (y.shape[0], 2)\n\n\n\n", "    def test_pls_regression_n_components_too_large(self):\n        X, y = load_linnerud(return_X_y=True)\n        pls = pls_.PLSRegression(n_components=X.shape[1] + 1)\n        with pytest.raises(ValueError):\n            pls.fit(X, y)\n", "    def test_pls_regression_predict_deprecation_warning(self):\n        X, y = load_linnerud(return_X_y=True)\n        pls = pls_.PLSRegression(n_components=2)\n        with pytest.warns(FutureWarning, match=\"The `predict` method is\"):\n            pls.fit(X, y).predict(X)\n"], "sample_706": ["    def test_nested_parens(self) -> None:\n        assert evaluate(\"(a and (b or c))\", lambda x: x in (\"a\", \"c\")) is True\n", "    def test_not_parentheses(self) -> None:\n        assert not evaluate(\"not (hello)\", lambda x: x == \"world\")\n", "    def test_not_expr_parentheses():\n        assert evaluate(\"(not a)\", lambda s: s == \"b\") is True\n", "    def test_double_negation(self):\n        matcher = lambda x: x == \"a\"\n\n        assert evaluate(\"not not a\", matcher)\n", "    def test_complex_with_parentheses():\n        assert evaluate(\n            \"(not foo and bar) or (baz and not quux)\", lambda x: x in [\"bar\", \"baz\"]\n        ) is True\n\n"], "sample_35": ["    def test_find_mod_objs_onlylocals_list(tmp_path):\n        # Create a dummy package structure\n        pkg_dir = tmp_path / 'mypkg'\n        pkg_dir.mkdir()\n        (pkg_dir / '__init__.py').touch()\n        (pkg_dir / 'mod1.py').write_text('x = 1\\ndef foo(): pass')\n\n        import sys\n        sys.path.append(str(tmp_path))\n\n        localnames, fqnames, objs = introspection.find_mod_objs('mypkg.mod1', onlylocals=['mypkg.mod1'])\n        assert localnames == ['x', 'foo']\n        assert fqnames == ['mypkg.mod1.x', 'mypkg.mod1.foo']\n\n        localnames, fqnames, objs = introspection.find_mod_objs('mypkg.mod1', onlylocals=['mypkg'])\n        assert localnames == []\n        assert fqnames == []\n        assert objs == []\n", "    def test_find_mod_objs_onlylocals(tmp_path):\n        test_mod_path = tmp_path / 'test_mod.py'\n        test_mod_path.write_text(\"\"\"\n        a = 1\n        b = 2\n\n            pass\n        \"\"\")\n\n        importlib.import_module(str(test_mod_path).replace('.py', ''))\n        localnames, fqnames, objs = introspection.find_mod_objs(str(test_mod_path).replace('.py', ''), onlylocals=True)\n\n        assert localnames == ['a', 'b', 'func']\n        assert fqnames == [str(test_mod_path).replace('.py', '') +'.'+ x for x in localnames]\n        assert objs == [1, 2, introspection.resolve_name(str(test_mod_path).replace('.py', ''), 'func')]\n\n", "def test_find_mod_objs_onlylocals():\n    \"\"\"\n    This test makes sure that find_mod_objs correctly filters objects,\n    including subpackages/modules and private attributes.\n    \"\"\"\n    Point = namedtuple('Point', ['x', 'y'])\n    modname = 'astropy.utils.introspection'\n\n    # Define a submodule in the same namespace to test\n    modname_sub = modname + '.test_utils'\n\n    # Define a function in the same namespace,\n    # but outside the module, to test\n        pass\n\n    # Add some attributes to the module namespace and submodule namespace:\n    #   - Public simple variable\n    globals()[modname_sub] = Point(1, 2)\n    #   - Public function (defined outside module, referenced inside)\n    globals()[modname + '.not_in_mod'] = not_in_mod\n    #   - Private variable\n\n    # Check output for various combinations of 'onlylocals'\n\n    localnames, fqnames, objs = introspection.find_mod_objs(modname)\n    assert len(localnames) > 0\n    assert all(l not in dir(introspection) for l in localnames)\n\n    localnames, fqnames, objs = introspection.find_mod_objs(\n                               modname, onlylocals=[modname])\n\n    assert len(localnames) > 0\n    assert all(fq.startswith(modname) for fq in fqnames)\n\n\n\n", "    def test_find_mod_objs_onlylocals_submodule():\n        mod = introspection.resolve_name(__name__)\n        localnames, fqnames, objs = introspection.find_mod_objs(\n            __name__, onlylocals=[__name__])\n        assert len(localnames) == len(fqnames) == len(objs)\n\n        # this assumes the test file has at least one function defined\n        assert len(localnames) > 0\n\n        for ln, fq, obj in zip(localnames, fqnames, objs):\n            if ln != 'test_find_mod_objs_onlylocals_submodule':\n                assert fq.startswith(__name__ + '.')\n", "    def test_isinstancemethod_staticmethod(recwarn):\n        class MyClass:\n            @staticmethod\n                pass\n\n        assert not introspection.isinstancemethod(MyClass, MyClass.my_staticmethod)\n        assert len(recwarn) == 0\n"], "sample_483": ["    def test_check_ModelAdmin_admin_site_required(self):\n        class MyModelAdmin(admin.ModelAdmin):\n            pass\n\n        site = AdminSite()\n        with self.assertRaisesMessage(checks.Error, \"ModelAdmin.admin_site must be set.\"):\n            MyModelAdmin().check(admin_site=None)\n\n        MyModelAdmin.admin_site = site\n        self.assertEqual(MyModelAdmin().check(admin_site=site), [])\n", "    def test_check_middleware(self):\n        admin_site = AdminSite()\n        admin_site.register(Album, ValidFields)\n\n        # Correct middleware setup\n\n        middleware = [SessionMiddlewareSubclass, AuthenticationMiddlewareSubclass, MessageMiddlewareSubclass]\n        admin_site.check(middleware=middleware)\n        self.assertEqual(admin_site.check(middleware=middleware), [])\n\n        # Incorrect middleware setup - missing AuthenticationMiddleware\n\n        middleware = [SessionMiddlewareSubclass, MessageMiddlewareSubclass]\n        errors = admin_site.check(middleware=middleware)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, \"admin.E012\")\n\n", "    def test_check_middleware(self):\n        site = AdminSite()\n        valid_admin = ValidFields(Album, admin_site=site)\n        admin_checks = admin.checks.ModelAdminChecks(valid_admin)\n        errors = admin_checks.check(valid_admin)\n        self.assertEqual(errors, [])\n\n        site.middleware.append(AuthenticationMiddlewareSubclass)\n        site.middleware.append(MessageMiddlewareSubclass)\n        site.middleware.append(SessionMiddlewareSubclass)\n        errors = admin_checks.check(valid_admin)\n        self.assertEqual(errors, [])\n\n\n\n\n", "    def test_middleware_checks(self):\n        admin_site = AdminSite()\n        admin_site.register(Song, ValidFields)\n\n        middleware = [\n            AuthenticationMiddlewareSubclass,\n            SessionMiddlewareSubclass,\n            MessageMiddlewareSubclass,\n        ]\n\n        errors = admin_site.check(middleware=middleware, apps=[])\n        self.assertEqual(len(errors), 0)\n\n", "    def test_check_auth_middleware_subclass(self):\n        site = AdminSite()\n        site.register(Album, MyAdmin)\n        site.register(Author, ValidFields)\n        site.register(Book, ValidFormFieldsets)\n\n        errors = site._check_middleware_classes()\n\n        # AuthenticationMiddleware should not be overridden\n        self.assertNotIn(\n            \"AuthenticationMiddleware must be exactly\",\n            [e.msg for e in errors],\n        )\n\n        # Other middleware can be subclassed\n        self.assertEqual(len(errors), 0)\n\n"], "sample_174": ["    def test_returning_clause_not_supported(self):\n        with self.assertRaises(NotSupportedError):\n            self.ops.returning_clause(None)\n\n\n", "    def test_prep_for_like_query_unicode(self):\n        self.assertEqual(self.ops.prep_for_like_query('Django'), 'Django')\n        self.assertEqual(self.ops.prep_for_like_query('Caf\u00e9'), 'Caf\\\\xe9')\n", "    def test_year_lookup_bounds_for_datetime_field(self):\n        with self.assertRaisesMessage(\n            NotImplementedError,\n            f'{self.may_require_msg % \"year_lookup_bounds_for_datetime_field\"}'\n        ):\n            self.ops.year_lookup_bounds_for_datetime_field(2023)\n", "    def test_cast_char_field_without_max_length(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'cast_char_field_without_max_length'):\n            self.ops.cast_char_field_without_max_length\n", "    def test_year_lookup_bounds_for_date_field(self):\n        with self.assertRaisesMessage(NotImplementedError, self.may_require_msg % 'year_lookup_bounds_for_date_field'):\n            self.ops.year_lookup_bounds_for_date_field(2023)\n\n"], "sample_997": ["    def test_convert_equals_signs_nested(self):\n        self.assertEqual(parse_expr(\"(1=2)=(3=4)\",\n                                  transformations=(\n                                      standard_transformations +\n                                      (convert_equals_signs,))),\n                       Eq(Eq(1, 2), Eq(3, 4)))\n        self.assertEqual(parse_expr(\"1=2=(3=4)\",\n                                  transformations=(\n                                      standard_transformations +\n                                      (convert_equals_signs,))),\n                       Eq(1, Eq(2, 3, 4)))\n", "    def test_convert_equals_signs_nested(self):\n        expr = parse_expr(\"(1=2)=(3=4)\",\n                         transformations=(standard_transformations +\n                                           (convert_equals_signs,)))\n        assert expr == Eq(Eq(1, 2), Eq(3, 4))\n\n        expr = parse_expr(\"((1=2)=3)=(4=5)\",\n                         transformations=(standard_transformations +\n                                           (convert_equals_signs,)))\n        assert expr == Eq(Eq(Eq(1, 2), 3), Eq(4, 5))\n", "    def test_factorial2_notation(self):\n        # Test the factorial2 notation with implicit multiplication\n        expr = parse_expr(\"x!!\", transformations=standard_transformations + (implicit_multiplication,))\n        assert expr == factorial2(Symbol('x'))\n        \n        expr = parse_expr(\"2x!!\", transformations=standard_transformations + (implicit_multiplication,))\n        assert expr == 2*factorial2(Symbol('x'))\n\n        expr = parse_expr(\"x!!+y\", transformations=standard_transformations + (implicit_multiplication,))\n        assert expr == factorial2(Symbol('x')) + Symbol('y')\n\n", "def test_convert_equals_signs():\n    assert parse_expr(\"1=2\", transformations=(standard_transformations + (convert_equals_signs,))) == Eq(1, 2)\n    assert parse_expr(\"1*2=x\", transformations=(standard_transformations + (convert_equals_signs,))) == Eq(2, x)\n    assert parse_expr(\"(1*2)=x\", transformations=(standard_transformations + (convert_equals_signs,))) == Eq(2, x)\n    assert parse_expr(\"(1=2)=False\", transformations=(standard_transformations + (convert_equals_signs,))) == Eq(Eq(1, 2), False)\n    assert parse_expr(\"x=y=z\", transformations=(standard_transformations + (convert_equals_signs,))) == Eq(x, Eq(y, z))\n", "    def test_convert_equals_signs_nested():\n        assert parse_expr(\"(1=2)=(3=4)\", transformations= (standard_transformations + (convert_equals_signs,))) == Eq(Eq(1, 2), Eq(3, 4))\n        assert parse_expr(\"((1=2)=3)=(4=5)\", transformations= (standard_transformations + (convert_equals_signs,))) == Eq(Eq(Eq(1, 2), 3), Eq(4, 5))\n        assert parse_expr(\"(1 = (2 = 3))\", transformations= (standard_transformations + (convert_equals_signs,))) == Eq(1, Eq(2, 3))\n\n"], "sample_616": ["def test_apply_ufunc_scalars(func, expected_result):\n    da = xr.DataArray(10.0)\n    result = apply_ufunc(func, da)\n    assert result.values == expected_result\n\n\n", "    def test_where_aligned_coords(self, dim):\n        data = {'x': np.arange(5),\n                'y': np.arange(3)}\n        coords = {'x': np.arange(5), 'y': np.arange(3)}\n        ds = xr.Dataset(data, coords=coords)\n        condition = ds['x'] > 2\n\n        result = xr.where(condition, ds['x'], 0)\n        expected = xr.DataArray(\n            [0, 0, 0, 3, 4], dims=['x'], coords={'x': np.arange(5)\n        )\n        assert_identical(result, expected)\n\n", "    def test_broadcast_compat_data_no_dim_overlap(self, dim_order):\n        x = xr.DataArray(np.arange(4).reshape((2, 2)), dims=[\"x\", \"y\"],\n                        coords={\"x\": [0, 1], \"y\": [0, 1]})\n        y = xr.DataArray(np.arange(6).reshape((3, 2)), dims=[dim_order[0], dim_order[1]])\n\n        with pytest.raises(ValueError):\n            broadcast(x, y)\n\n", "    def test_unify_chunks():\n        da1 = xr.DataArray(\n            np.arange(12).reshape(3, 4),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [0, 1, 2], \"y\": [0, 1, 2, 3]},\n            chunks={\"x\": 2, \"y\": 2},\n        )\n        da2 = xr.DataArray(\n            np.arange(12).reshape(3, 4),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": [0, 1, 2], \"y\": [4, 5, 6, 7]},\n            chunks={\"x\": 1, \"y\": 4},\n        )\n\n        da3, da4 = unify_chunks(da1, da2)\n\n        assert da3.chunks == ({'x': (2, 1), 'y': (2, 2)},)\n        assert da4.chunks == ({'x': (2, 1), 'y': (2, 2)},)\n", "    def test_polyval_dask():\n        da = xr.DataArray(np.arange(10), dims=[\"x\"])\n        coeffs = xr.DataArray(np.ones((3,)), dims=[\"degree\"])\n        assert_identical(\n            xr.polyval(da, coeffs),\n            xr.DataArray(\n                [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]* (1 + 1 + 1), dims=[\"x\"]\n            ),\n        )\n\n"], "sample_587": ["    def test_merge_empty_datasets(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({'foo': ('x', [1, 2])})\n\n        actual = merge([ds1, ds2])\n        expected = xr.Dataset({'foo': ('x', [1, 2])})\n        assert_identical(actual, expected)\n", "    def test_merge_conflicting_data_variables(self):\n        data1 = create_test_data(data={'foo': [1, 2, 3]})\n        data2 = create_test_data(data={'foo': [4, 5, 6]})\n        with raises_regex(\n            ValueError, \"conflicting values for variable foo on objects to be combined\"):\n            merge([data1, data2], compat=\"equals\")\n", "    def test_merge_coords_minimal(self) -> None:\n        data = xr.Dataset(\n            {\"foo\": ((\"x\",), [1, 2]), \"bar\": ((\"x\",), [3, 4])}, coords={\"x\": [0, 1]}\n        )\n        other = xr.Dataset({\"foo\": ((\"x\",), [5, 6])}, coords={\"x\": [0, 1]})\n\n        expected = xr.Dataset(\n            {\"foo\": ((\"x\",), [1, 2]), \"bar\": ((\"x\",), [3, 4])}, coords={\"x\": [0, 1]}\n        )\n        actual = data.merge(other, compat=\"minimal\")\n        assert_identical(actual, expected)\n", "    def test_merge_variable_no_coords_conflict(self):\n        data1 = create_test_data(\n            [(\"var1\", np.arange(4)), (\"var2\", [1, 2, 3, 4])],\n            dims=[(\"dim1\", 4)],\n        )\n        data2 = create_test_data(\n            [(\"var1\", [5, 6, 7, 8]), (\"var3\", [5, 6, 7, 8])],\n            dims=[(\"dim1\", 4)],\n        )\n\n        actual = merge([data1, data2], compat=\"override\")\n        expected = xr.Dataset(\n            {\n                \"var1\": ((\"dim1\",), np.array([5, 6, 7, 8])),\n                \"var2\": ((\"dim1\",), np.array([1, 2, 3, 4])),\n                \"var3\": ((\"dim1\",), np.array([5, 6, 7, 8])),\n            },\n            coords={\"dim1\": np.arange(4)},\n        )\n        assert_identical(actual, expected)\n", "    def test_merge_priority_vars(self):\n        ds1 = create_test_data(dim=True)\n        ds2 = create_test_data(dim=True, var_name=\"foo\", new_var=\"bar\")\n        expected = ds2.copy()\n        actual = merge([ds1, ds2], priority_arg=1)\n        assert_identical(expected, actual)\n"], "sample_292": ["    def test_csrf_cookie_http_only_flag(self):\n        \"\"\"\n        Test that the CSRF cookie is set with the 'HttpOnly' attribute.\n        \"\"\"\n        middleware = CsrfViewMiddleware()\n        req = self._get_GET_csrf_cookie_request()\n        res = HttpResponse()\n        middleware.process_response(req, res)\n\n        self.assertTrue('HttpOnly' in res['Set-Cookie'])\n", "    def test_csrf_cookie_reset_on_login(self):\n        \"\"\"\n        Ensure CSRF cookie is reset upon successful login if CSRF_USE_SESSIONS is\n        enabled and a new session is created.\n        \"\"\"\n        with self.settings(CSRF_USE_SESSIONS=True):\n            request = self._get_GET_csrf_cookie_request()\n            request.session.save()\n            middleware = CsrfViewMiddleware()\n            response = HttpResponse()\n            middleware.process_response(request, response)\n            # Make sure the cookie exists with a valid token\n            csrf_cookie = response.cookies.get(settings.CSRF_COOKIE_NAME)\n            self.assertIsNotNone(csrf_cookie)\n\n            # Simulate a login that creates a new session\n            request.session = SessionStore()\n            request.session.save()\n            middleware.process_request(request)\n\n            response = HttpResponse()\n            middleware.process_response(request, response)\n\n            # Check if the cookie has changed\n            new_csrf_cookie = response.cookies.get(settings.CSRF_COOKIE_NAME)\n            self.assertIsNotNone(new_csrf_cookie)\n            self.assertNotEqual(csrf_cookie.value, new_csrf_cookie.value)\n", "    def test_csrf_cookie_set_with_referer(self):\n        request = self._get_GET_csrf_cookie_request()\n        request.META['HTTP_REFERER'] = 'https://example.com/'\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_response(request, HttpResponse())\n        self.assertTrue(response.csrf_cookie_set)\n\n", "    def test_origin_checking_with_trusted_origin(self):\n        request = self._get_GET_csrf_cookie_request()\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertIsNone(response)\n", "    def test_csrf_token_is_sent_in_cookie_and_hidden_field(self):\n        response = self.middleware.process_response(self._get_GET_csrf_cookie_request(), HttpResponse())\n        self.assertIn('Set-Cookie', response['headers'])\n        self.assertIn(settings.CSRF_COOKIE_NAME, response['Set-Cookie'])\n        self.assertEqual(response.csrf_cookie_set, True)\n        self._check_GET_request(response)"], "sample_1050": ["def test_print_noncommutative_Mul():\n    expr = x*y*z\n    assert pycode(expr, standard=\"python\") == \"x*y*z\"\n", "def test_numpy_array_constant():\n    assert NumPyPrinter().doprint(MatrixSymbol('A', 2, 2)) == 'numpy.array([[A_0_0, A_0_1], [A_1_0, A_1_1]])'\n\n    \n", "    def test_codegen_array_tensorproduct(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        expr = A.T @ B\n        code = NumPyPrinter().doprint(expr)\n        assert code == \"numpy.dot(A.T, B)\"\n", "    def test_scipy_dense(self):\n        a = sympy.Matrix([[1, 2], [3, 4]])\n        code = SciPyPrinter().doprint(a)\n        assert code == \"scipy.array([[1, 2], [3, 4]])\"\n", "    def test_print_Assignment(self):\n        from sympy.codegen import Assignment\n        a = symbols('a')\n        Assignment(a, x + y)\n        assert self.p.doprint(Assignment(a, x + y)) == 'a = x + y'\n"], "sample_463": ["    def test_rabbit_model_circular_fk_index(self, rabbit):\n        with self.assertRaises(IndexError):\n            rabbit.objects.create(knight_id=1, parent_id=1)\n\n", "    def test_unique_together_circular_fk(self):\n        knight = Knight.objects.create()\n        with pytest.raises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight.rabbit_set.create(knight=knight))\n", "    def test_unique_together_with_circular_foreign_keys(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_unique_together_field_order_change(self):\n        self.assertTableNamesEqual(self.author_unique_together, self.author_unique_together_2)\n        self.assertColumnNamesEqual(\n            self.author_unique_together, self.author_unique_together_2\n        )\n        self.assertColumnNamesEqual(\n            self.author_unique_together_3, self.author_unique_together_4\n        )\n", "    def test_knight_rabbit_circular_fk_indexes(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n"], "sample_438": ["    def test_get_choices_for_field(self):\n        answer_choices = Answer.objects.create(\n            question=Question.objects.create(text=\"What's your favorite color?\"),\n            text=\"Blue\",\n        ).text\n        self.assertEqual(\n            list(Answer._meta.get_field(\"content_type\").get_choices()),\n            [(ct.id, ct.name) for ct in ContentType.objects.filter(app_label=\"contenttypes_tests\", model=\"answer\")],\n        )\n        self.assertEqual(\n            Answer._meta.get_field(\"object_id\").get_choices(answer_choices),\n            [(answer_choices.id, str(answer_choices))],\n        )\n\n", "    def test_generic_foreign_key_ordering(self):\n        Question.objects.create(text='What is the meaning of life?')\n        post = Post.objects.create(title='A Post')\n        Answer.objects.create(content='42', question=post)\n        question = Question.objects.get(pk=1)\n        answer = Answer.objects.get(pk=1)\n        self.assertEqual(question.answer_set.order_by('id')[0], answer)\n", "    def test_genericforeignkey_ordering_relations_preserved(self):\n        \"\"\"Verify ordering from related objects that use a generic foreign key is preserved.\"\"\"\n        q1 = Question.objects.create(text=\"Question 1\")\n        q2 = Question.objects.create(text=\"Question 2\")\n\n        a1 = Answer.objects.create(question=q1, text=\"Answer 1\")\n        a2 = Answer.objects.create(question=q1, text=\"Answer 2\")\n        a3 = Answer.objects.create(question=q2, text=\"Answer 3\")\n        a4 = Answer.objects.create(question=q2, text=\"Answer 4\")\n\n        self.assertEqual(\n            list(Answer.objects.filter(question__in=[q1, q2]).values_list(\"text\", flat=True)),\n            [\"Answer 1\", \"Answer 2\", \"Answer 3\", \"Answer 4\"],\n        )\n\n", "    def test_model_unpickle(self):\n        question = Question.objects.create(content=\"Test question\")\n        answer = Answer.objects.create(question=question, content=\"Test answer\")\n\n        serialized_answer = json.dumps(answer)\n        unpickled_answer = json.loads(serialized_answer, object_hook=model_unpickle)\n\n        self.assertEqual(unpickled_answer.__class__, Answer)\n        self.assertEqual(unpickled_answer.question, question)\n", "    def test_generic_relation_ordering(self):\n        q1 = Question.objects.create(text='Question 1')\n        q2 = Question.objects.create(text='Question 2')\n\n        a1 = Answer.objects.create(question=q1, text='Answer 1')\n        a2 = Answer.objects.create(question=q2, text='Answer 2')\n        a3 = Answer.objects.create(question=q1, text='Answer 3')\n\n        self.assertEqual(list(Answer.objects.all().order_by('question')), [a1, a3, a2])\n\n        # Test ordering by a related field on the related model\n        self.assertEqual(\n            list(Answer.objects.all().order_by('question__text')),\n            [a1, a3, a2],\n        )\n"], "sample_989": ["     def test_golden_ratio_methods():\n         assert S.GoldenRatio.expand(func=True) == Rational(1, 2) + sqrt(5)/2\n         assert GoldenRatio.approximation_interval(Integer) == (Integer(1), Integer(2))\n         assert GoldenRatio.approximation_interval(Rational) is None\n", "def test_catalan_constant():\n    assert Catalan > 0\n    assert Catalan < 1\n    assert Catalan.is_irrational is None\n\n\n", "def test_imaginary_unit_power():\n    assert (I**2) == -1\n    assert (I**3) == -I\n    assert (I**4) == 1\n    assert (I**5) == I\n    assert (I**-1) == -I\n\n", "    def test_Integer_conversion():\n        assert Integer(123) == 123\n        assert Integer(-42) == -42\n        assert Integer(0) == 0\n        assert Integer(long(123)) == 123\n        assert Integer(float(123)) == 123\n        assert Integer(decimal.Decimal(\"123.0\")) == 123\n        assert Integer(Fraction(123, 1)) == 123\n    \n\n", "def test_Catalan():\n    assert Catalan > 0\n    assert Catalan < 1\n    assert Catalan.is_irrational is None\n    assert Catalan.is_Transcendental is False\n    assert Catalan.is_algebraic is True\n    assert (Catalan + Catalan)**2 == 4*Catalan**2\n    assert Catalan.approximation_interval(Rational) == (Rational(9,10), S.One)\n"], "sample_760": ["def test_make_scorer_with_callable():\n    scorer = make_scorer(lambda y_true, y_pred: -np.mean(y_true == y_pred))\n    assert_equal(scorer(LinearSVC(), X_mm, y_mm),\n                 -np.mean(y_mm == LinearSVC().fit(X_mm, y_mm).predict(X_mm)))\n", "def test_make_scorer_with_kwargs():\n    # Test make_scorer with additional keyword arguments\n    scorer = make_scorer(fbeta_score, beta=0.5, pos_label=1)\n    estimator = LinearSVC()\n    X, y = make_classification(random_state=0)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n    estimator.fit(X_train, y_train)\n    score = scorer(estimator, X_test, y_test)\n    assert isinstance(score, numbers.Number)\n\n", "def test_make_scorer_with_dummy_estimator():\n    dummy_estimator = EstimatorWithFitAndPredict()\n    scorer = make_scorer(accuracy_score)\n    # Should not raise an error as it uses predict\n    scorer(dummy_estimator, X_mm, y_mm) \n", "    def test_check_scoring_no_score_method_when_allow_none_is_false(self):\n        estimator = EstimatorWithoutFit()\n        with pytest.raises(TypeError):\n            check_scoring(estimator, allow_none=False)\n", "def test_check_multimetric_scoring_empty_dict():\n    estimator = LinearSVC()\n    scoring = {}\n    err_msg_generic = (\"scoring should either be a single string or \"\n                       \"callable for single metric evaluation or a \"\n                       \"list/tuple of strings or a dict of scorer name \"\n                       \"mapped to the callable for multiple metric \"\n                       \"evaluation. Got {} of type {}\"\n                       .format(repr(scoring), type(scoring)))\n    with pytest.raises(ValueError, match=err_msg_generic):\n        _check_multimetric_scoring(estimator, scoring=scoring)\n"], "sample_325": ["    def test_boundfield_label_tag(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        bound_field = form['first_name']\n        label_tag = bound_field.label_tag()\n\n        # Check if the label has the correct id attribute\n        self.assertIn('for=\"first_name_id\"', label_tag)\n\n        # Check if the label contains the field's label text\n        self.assertIn('first_name', label_tag)\n", "    def test_boundfield_label_tag(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        bound_field = form['first_name']\n\n        # Test with no label suffix\n        label = bound_field.label_tag()\n        self.assertIn('<label for=\"first_name_id\">first_name</label>', label)\n\n        # Test with custom label suffix\n        label = bound_field.label_tag(label_suffix='*')\n        self.assertIn('<label for=\"first_name_id\">first_name*</label>', label)\n\n        # Test with label ending in punctuation\n        bound_field.label = 'Mr. Doe'\n        label = bound_field.label_tag()\n        self.assertIn('<label for=\"first_name_id\">Mr. Doe</label>', label)\n", "    def test_boundfield_label_tag_suffix_override(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        field = form['first_name']\n        form.label_suffix = ':)'\n        label = field.label_tag()\n        self.assertEqual(\n            label,\n            '<label for=\"first_name_id\">John:)</label>',\n        )\n        label = field.label_tag(label_suffix=None)\n        self.assertEqual(\n            label,\n            '<label for=\"first_name_id\">John</label>',\n        )\n        label = field.label_tag(label_suffix='!')\n        self.assertEqual(\n            label,\n            '<label for=\"first_name_id\">John!</label>',\n        )\n\n", "    def test_boundfield_html_name(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        bound_field = form['first_name']\n        self.assertEqual(bound_field.html_name, 'first_name')\n", "    def test_boundfield_label_tag_with_custom_label_suffix(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        bound_field = form['first_name']\n        label_tag = bound_field.label_tag(label_suffix='*')\n        self.assertIn('John*', label_tag)\n\n"], "sample_815": ["def test_brier_score_loss_multiclass():\n    # Check that Brier score loss raises an error for multiclass problems\n    y_true = np.array([0, 1, 2, 0])\n    y_prob = np.array([[0.1, 0.2, 0.7],\n                       [0.8, 0.1, 0.1],\n                       [0.3, 0.6, 0.1],\n                       [0.9, 0.05, 0.05]])\n    with pytest.raises(ValueError):\n        brier_score_loss(y_true, y_prob)\n", "def test_brier_score_loss_multiclass():\n    # Test brier score loss for multiclass classification.\n    y_true = np.array([0, 1, 2, 0])\n    y_prob = np.array([[0.8, 0.1, 0.1],\n                       [0.2, 0.6, 0.2],\n                       [0.1, 0.2, 0.7],\n                       [0.7, 0.2, 0.1]])\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, 0.26, decimal=2)\n", "    def test_brier_score_loss_all_weights_are_zero():\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        sample_weight = np.zeros_like(y_true)\n        assert_almost_equal(brier_score_loss(y_true, y_prob,\n                                             sample_weight=sample_weight), 0)\n", "def test_brier_score_loss_binary_with_sample_weights():\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    sample_weight = np.array([1, 2, 1, 1])\n    score = brier_score_loss(y_true, y_prob, sample_weight=sample_weight)\n    assert_almost_equal(score, 0.05, 2)\n\n", "    def test_matthews_corrcoef_binary():\n        y_true, _, _ = make_prediction(binary=True)\n        # Test with perfect prediction\n        y_pred = y_true.copy()\n        assert_allclose(matthews_corrcoef(y_true, y_pred), 1)\n\n        # Test with anti-perfect prediction\n        y_pred = 1 - y_true\n        assert_allclose(matthews_corrcoef(y_true, y_pred), -1)\n"], "sample_129": ["    def test_floatformat01(self):\n        with localcontext() as ctx:\n            ctx.prec = 2\n            a = Decimal('1234.56789')\n            b = Decimal('9876.54321')\n            output = self.engine.render_to_string('floatformat01', {'a': a, 'b': b})\n            self.assertEqual(output, '1234.57 9876.54')\n\n", "    def test_floatformat_arg(self):\n        with self.settings(USE_I18N=True):\n            output = self.engine.render_to_string('floatformat01', {'a': 1.23456789, 'b': Decimal('1.23456789')})\n            self.assertEqual(output, '1.23456789 1.23456789')\n", "    def test_floatformat01(self):\n        template = self.engine.get_template('floatformat01')\n        result = template.render(context={\n            'a': Decimal('12345678901234567890'),\n            'b': 1.234567890123456789\n        })\n        self.assertEqual(result, '12345678901234567890 1.2345678901234568')\n", "    def test_floatformat01(self):\n        with self.settings(USE_L10N=True):\n            output = self.engine.render_to_string('floatformat01', {'a': 1.2345, 'b': Decimal('1.2345')})\n            self.assertEqual(output, '1.2345 1.2345')\n", "    def test_floatformat01(self):\n        template = self.engine.get_template('floatformat01')\n        with localcontext() as ctx:\n            ctx['a'] = Decimal('1234.56789')\n            ctx['b'] = Decimal('9876.54321')\n            output = template.render(ctx)\n            self.assertEqual(output, '1234.56789 9876.54321')\n"], "sample_860": ["    def test_check_scalar_errors_with_bounds():\n        # Test check_scalar with different types and bounds\n        for x, name, target_type, min_val, max_val in product(\n                [1, 1.5, 'a', [1]],\n                ['param_name'],\n                [int, float],\n                [0, None],\n                [10, None]):\n            if isinstance(x, target_type) and \\\n               (min_val is None or x >= min_val) and \\\n               (max_val is None or x <= max_val):\n                continue\n\n            with assert_raises(TypeError if not isinstance(x, target_type) else ValueError):\n                check_scalar(x, name, target_type, min_val, max_val)\n\n", "    def test_check_sample_weight_dtype():\n        X = np.array([[1, 2], [3, 4]])\n        # Test dtype preservation for array-like input\n        sample_weight = np.array([1., 2.], dtype=np.float32)\n        result = _check_sample_weight(sample_weight, X)\n        assert result.dtype == np.float32\n        # Test dtype casting for non-array-like input\n        sample_weight = 2.\n        result = _check_sample_weight(sample_weight, X)\n        assert result.dtype == np.float64\n        # Test default dtype for None input\n        sample_weight = None\n        result = _check_sample_weight(sample_weight, X)\n        assert result.dtype == np.float64\n\n", "def test_check_sample_weight_dtype():\n    # Test dtype handling in _check_sample_weight\n\n    X = np.array([[1, 2], [3, 4]])\n    sw = np.array([1.0, 2.0], dtype=np.float32)\n\n    sw_out = _check_sample_weight(sw, X, dtype=np.float64)\n    assert sw_out.dtype == np.float64\n    assert_allclose(sw_out, sw)\n\n    sw_out = _check_sample_weight(sw, X, dtype=None)\n    assert sw_out.dtype == np.float32\n    assert_allclose(sw_out, sw)\n\n    sw_out = _check_sample_weight(1.5, X, dtype=np.float32)\n    assert sw_out.dtype == np.float32\n    assert_allclose(sw_out, [1.5, 1.5])\n\n    with pytest.raises(TypeError):\n        _check_sample_weight(sw, X, dtype=np.int32)\n\n\n", "    def test_check_sample_weight_dtype(self):\n        X = np.array([[1, 2], [3, 4]])\n        # Test default dtype\n        sample_weight = _check_sample_weight(None, X)\n        assert sample_weight.dtype == np.float64\n        # Test preservation of input dtype\n        sample_weight = np.array([1.0, 2.0], dtype=np.float32)\n        sample_weight = _check_sample_weight(sample_weight, X)\n        assert sample_weight.dtype == np.float32\n        # Test explicit dtype\n        sample_weight = _check_sample_weight(\n            np.array([1.0, 2.0]), X, dtype=np.float64\n        )\n        assert sample_weight.dtype == np.float64\n        # Test invalid dtype\n        with pytest.raises(ValueError):\n            _check_sample_weight(np.array([1, 2]), X, dtype=int)\n\n\n\n", "    def test_check_symmetric_sparse_formats():\n        # test check_symmetric with various sparse matrix formats\n        X = sparse_random_matrix(10, 10, density=0.5, format='csr')\n        for fmt in ['csr', 'csc', 'lil', 'dok', 'coo']:\n            X_fmt = X.toformat(fmt)\n            check_symmetric(X_fmt, tol=1e-10)\n            X_fmt[0, 1] = 2\n            assert_raises(AssertionError, check_symmetric, X_fmt, tol=1e-10)\n\n"], "sample_187": ["    def test_unescape_entities(self):\n        with override(gettext_lazy('This &lt; is a &quot;test&quot;')):\n            self.assertEqual(text.unescape_entities('This &lt; is a &quot;test&quot;'), 'This < is a \"test\"')\n", "    def test_unescape_entities(self):\n        self.assertEqual(text.unescape_entities('&amp;'), '&')\n        self.assertEqual(text.unescape_entities('&lt;'), '<')\n        self.assertEqual(text.unescape_entities('&gt;'), '>')\n        self.assertEqual(text.unescape_entities('&quot;'), '\"')\n        self.assertEqual(text.unescape_entities('&#39;'), \"'\")\n        self.assertEqual(text.unescape_entities('&#x27;'), \"'\")\n        self.assertEqual(text.unescape_entities('&#65;'), 'A')\n        self.assertEqual(text.unescape_entities('&nbsp;'), ' ')\n", "    def test_smart_split(self):\n        self.assertEqual(list(text.smart_split('This is \"a person\\'s\" test.')),\n                         ['This', 'is', '\"a person\\\\\\'s\"', 'test.'])\n        self.assertEqual(list(text.smart_split(\"Another 'person\\'s' test.\")),\n                         ['Another', \"'person\\\\'s'\", 'test.'])\n        self.assertEqual(list(text.smart_split('A \"\\\"funky\\\" style\" test.')),\n                         ['A', '\"\\\\\"funky\\\\\" style\"', 'test.'])\n", "    def test_smart_split(self):\n        self.assertEqual(list(text.smart_split('This is \"a person\\'s\" test.')),\n                         ['This', 'is', '\"a person\\\\\\'s\"', 'test.'])\n        self.assertEqual(list(text.smart_split(\"Another 'person\\'s' test.\")),\n                         ['Another', \"'person\\\\'s'\", 'test.'])\n        self.assertEqual(list(text.smart_split('A \"\\\"funky\\\" style\" test.')),\n                         ['A', '\"\\\\\"funky\\\\\" style\"', 'test.'])\n", "    def test_smart_split(self):\n        self.assertEqual(list(text.smart_split(r'This is \"a person\\'s\" test.')),\n                         ['This', 'is', '\"a person\\\\\\'s\"', 'test.'])\n        self.assertEqual(list(text.smart_split(r\"Another 'person\\'s' test.\")),\n                         ['Another', \"'person\\\\'s'\", 'test.'])\n        self.assertEqual(list(text.smart_split(r'A \"\\\"funky\\\" style\" test.')),\n                         ['A', '\"\\\\\"funky\\\\\" style\"', 'test.'])\n"], "sample_678": ["    def test_fnmatch_ex_double_star(self, match):\n        assert match(\"**/test_*.py\", \"tests/foo/bar/test_foo.py\")\n        assert not match(\"**/test_*.py\", \"tests/foo/bar/baz.py\")\n        assert match(\"**/doc/test*.py\", \"tests/foo/bar/doc/test_foo.py\")\n", "    def test_fnmatch_ex(self, match):\n        assert match(\"tests/**/doc/test*.py\", \"tests/foo/bar/doc/test_foo.py\") == True\n        assert match(\"tests/**/doc/test*.py\",\n                     \"tests/foo/bar/doc/test_foo.py\") == True\n        assert match(\"*\", \"foo.py\") == True\n        assert match(\"*.txt\", \"foo.txt\") == True\n", "    def test_fnmatch_ex(self, match):\n        assert match(\"*.py\", \"foo.py\")\n        assert match(\"*.py\", \"bar/foo.py\")\n        assert match(\"test_*.py\", \"foo/test_foo.py\")\n        assert match(\"tests/*.py\", \"tests/foo.py\")\n        assert match(self.drv1 + \"/*.py\", self.drv1 + \"/foo.py\")\n        assert match(self.drv1 + \"/foo/*.py\", self.drv1 + \"/foo/foo.py\")\n        assert match(\"tests/**/test*.py\", \"tests/foo/test_foo.py\")\n        assert match(\"tests/**/doc/test*.py\", \"tests/foo/bar/doc/test_foo.py\")\n        assert match(\"tests/**/doc/**/test*.py\", \"tests/foo/doc/bar/test_foo.py\")\n", "    def test_fnmatch_ex(self, match):\n        match_ = match\n        assert match_('*.py', 'foo.py')\n\n        assert match_('*.py', 'bar/foo.py')\n        assert not match_('tests/*.py', 'bar/foo.py')\n        assert match_('tests/*.py', 'tests/foo.py')\n        assert not match_('tests/*.py', 'tests/foo/bar/foo.py')\n        assert match_('tests/**/test*.py', 'tests/foo/bar/test_foo.py')\n", "    def test_fnmatch_ex(self, match):\n        for pattern, path in [\n            (\"tests/**/doc/test*.py\", \"tests/foo/bar/doc/test_foo.py\"),\n            (\"tests/**/doc/test*.py\", \"tests/foo/doc/test_foo.py\"),\n            (drv1 + \"/**/*py\", drv1 + \"/foo/bar.py\"),\n        ]:\n            assert match(pattern, path), f\"pattern {pattern} should match {path}\"\n"], "sample_276": ["    def test_model_detail_view_no_docstring(self):\n        class NoDocstringModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        admin.site.register(NoDocstringModel)\n        response = self.client.get(reverse('admindocs:model_detail', args=['no_docstring_model', 'admin']))\n        self.assertEqual(response.status_code, 200)\n        self.assertIn('<title>Model: NoDocstringModel</title>', response.content.decode())\n        self.assertNotIn('<p><b>Summary:</b></p>', response.content.decode()) \n", "    def test_get_return_data_type(self):\n        self.assertEqual(get_return_data_type('get_person_list'), 'List')\n        self.assertEqual(get_return_data_type('get_person_count'), 'Integer')\n        self.assertEqual(get_return_data_type('get_company_details'), '')\n", "    def test_view_detail_view_with_view_method(self):\n        # Create a simple view function.\n            return HttpResponse(\"Hello, world!\")\n\n        # Add the view to the URL patterns.\n        urlconf = {\n            path('my-view/', views.SimpleAdminDocsView.as_view()),\n        }\n\n        with override_settings(ROOT_URLCONF=urlconf):\n            response = self.client.get('%s/admin/doc/view/my-view/' % self.admin_url)\n            self.assertEqual(response.status_code, 200)\n            self.assertContains(response, 'my-view')\n            self.assertContains(response, 'SimpleAdminDocsView')\n", "    def test_model_detail_view_with_foreign_key(self):\n        model_name = 'company'\n        url = reverse('admin:admindocs-detail', kwargs={'app_label': 'test_app', 'model_name': model_name})\n\n        response = self.client.get(url)\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, _('related `test_app.Person` objects'))\n        self.assertContains(response, 'List')\n        self.assertContains(response, 'Integer')\n", "    def test_get_model_methods(self):\n        \"\"\"\n        Test that get_model_methods returns the correct data for a model.\n        \"\"\"\n        model = Person\n        methods = views.ModelDetailView.get_model_methods(model)\n        # Check for the existence of expected methods\n        self.assertIn('get_full_name', methods)\n        self.assertIn('get_absolute_url', methods)\n        self.assertIn('save', methods)\n        self.assertIn('delete', methods)\n"], "sample_1": ["    def test_cm12():\n        cm = (rot & sh1) | (scl1 & scl2)\n        expected_is_separable = (np.array([False, False, True, True]),\n                                  np.array([[True, True, False, False],\n                                            [True, True, False, False],\n                                            [False, False, True, False],\n                                            [False, False, False, True]]))\n        assert_allclose(is_separable(cm), expected_is_separable[0])\n        assert_allclose(separability_matrix(cm), expected_is_separable[1])\n", "def test_separability_matrix_compound_model_with_mapping():\n    m = map1 & sh1 | rot | p2\n    sep_matrix = separability_matrix(m)\n    assert_allclose(sep_matrix, np.array([[True, False], [True, False],\n                                         [False, True], [False, True]]))\n\n", "def test_separability_matrix_compound_model_cm12():\n    cm = rot & sh1 & map1 & p2\n    is_sep, sep_mat = is_separable(cm), separability_matrix(cm)\n    assert_allclose(is_sep, np.array([False, False]))\n    assert_allclose(sep_mat, np.array([[True, True], [True, True]]))\n", "    def test_compound_model_separability_with_custom_model():\n\n        @custom_model\n            return x ** 2\n\n        model1 = my_model()\n        model2 = models.Shift(1)\n        model = model1 & model2\n        result = is_separable(model)\n        assert_allclose(result, np.array([True]))\n\n", "    def test_cm12(self):\n        cm = (map1 & sh1 & sh2) | (map1 & p1 & p2)\n        expected_separable = (np.array([False, False, True, True]),\n                               np.array([[True, False, False, False],\n                                         [True, False, False, False],\n                                         [False, True, False, False],\n                                         [False, False, True, False]]))\n        result = separability_matrix(cm)\n        assert_allclose(result, expected_separable[1])\n        result = is_separable(cm)\n        assert_allclose(result, expected_separable[0])\n"], "sample_143": ["    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces(\"ThisIsATest\"), \"this is a test\")\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces(\"HelloWorld\"), \"hello world\")\n        self.assertEqual(text.camel_case_to_spaces(\"helloWorld\"), \"hello world\")\n        self.assertEqual(text.camel_case_to_spaces(\"helloWorld \"), \"hello world\")\n        self.assertEqual(text.camel_case_to_spaces(\" helloWorld \"), \"hello world\")\n        self.assertEqual(text.camel_case_to_spaces(\"SomePDFName\"), \"some pdf name\")\n", "    def test_unescape_entities(self):\n        self.assertEqual(text.unescape_entities(\"This &amp; that &lt;br/&gt;\"),\n                         \"This & that <br/>\")\n        self.assertEqual(text.unescape_entities(\"This is &quot;a quote&quot;\"),\n                         \"This is \\\"a quote\\\"\")\n        self.assertEqual(text.unescape_entities(\"&#39;\"), \"'\")\n        self.assertEqual(text.unescape_entities(\"&#x27;\"), \"'\")\n", "    def test_get_valid_filename(self):\n        self.assertEqual(text.get_valid_filename(\"john's portrait in 2004.jpg\"),\n                         'johns_portrait_in_2004.jpg')\n        self.assertEqual(text.get_valid_filename(\"john's portrait in 2004.jpg \").strip(),\n                         'johns_portrait_in_2004.jpg')\n        self.assertEqual(text.get_valid_filename(\"john's portrait in 2004.\"),\n                         'johns_portrait_in_2004')\n        self.assertEqual(text.get_valid_filename(\"john's portrait in 2004\"),\n                         'johns_portrait_in_2004')\n", "    def test_camel_case_to_spaces(self):\n        self.assertEqual(text.camel_case_to_spaces('someRandomString'), 'some random string')\n        self.assertEqual(text.camel_case_to_spaces('SomeRandomString'), 'some random string')\n        self.assertEqual(text.camel_case_to_spaces('someRandomStringWithNumbers123'), 'some random string with numbers 123')\n"], "sample_214": ["    def test_transform_with_null_value(self):\n        with self.assertRaises(ValueError):\n            JSONModel.objects.filter(data__nullable_key=KeyTransform('key')).get()\n\n        # Test with a nullable field\n        NullableJSONModel.objects.create(data={'key': None})\n        self.assertEqual(\n            NullableJSONModel.objects.filter(data__nullable_key__isnull=True).count(),\n            1,\n        )\n", "    def test_jsonfield_key_transform_in_with_empty_list(self):\n        model = JSONModel.objects.create(data={'list': []})\n        with self.assertRaises(ValidationError):\n            JSONModel.objects.create(data={'list': []}, validator=self.validator)\n        with CaptureQueriesContext(connection) as captured_queries:\n            list_in = Q(data__list__in=[])\n            self.assertFalse(JSONModel.objects.filter(list_in).exists())\n            self.assertEqual(len(captured_queries), 1)\n\n", "    def test_key_transform_with_f_expression(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT id, data FROM json_model\")\n            rows = cursor.fetchall()\n        data = {row[0]: json.loads(row[1]) for row in rows}\n        self.assertEqual(JSONModel.objects.filter(data__0__name='Alice').count(), 0)\n        self.assertEqual(JSONModel.objects.filter(data__0__name__icontains='lice').count(), 1)\n", "    def test_key_transform_null_lhs(self):\n        data = {'key': None}\n        obj = JSONModel.objects.create(data=data)\n        with self.assertRaises(IntegrityError):\n            JSONModel.objects.create(data={'key': 'value'})\n        \n        with self.assertRaises(OperationalError):\n            self.assertIsNone(JSONModel.objects.filter(data__key__exact='value').exists())\n\n        self.assertQuerysetEqual(JSONModel.objects.filter(data__key__isnull=True), [obj])\n\n", "    def test_json_field_transform_key_lookup_sqlite(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"PRAGMA foreign_keys = OFF;\")\n\n        json_data = {\n            'age': 30,\n            'name': 'John Doe',\n            'address': {\n                'city': 'New York',\n                'zip': 10001\n            }\n        }\n\n        model = JSONModel.objects.create(data=json_data)\n        q = JSONModel.objects.filter(data__address__city='New York')\n        self.assertQuerysetEqual(q, [model])\n\n        q = JSONModel.objects.filter(data__address__city__exact='New York')\n        self.assertQuerysetEqual(q, [model])\n\n        q = JSONModel.objects.filter(data__address__city__isnull=False)\n        self.assertQuerysetEqual(q, [model])\n        \n        q = JSONModel.objects.filter(data__address__city__isnull=True)\n        self.assertEqual(q.count(), 0)\n\n\n\n"], "sample_881": ["def test_top_k_accuracy_score_multiclass_sample_weights():\n    # Test with sample weights for multi-class\n    y_true = np.array([0, 1, 2, 2])\n    predicted_scores = np.array([[0.5, 0.2, 0.2],\n                               [0.3, 0.4, 0.2],\n                               [0.2, 0.4, 0.3],\n                               [0.7, 0.2, 0.1]])\n    labels = np.array([0, 1, 2])\n    sample_weights = np.array([1, 2, 1, 1])\n    assert_almost_equal(\n        top_k_accuracy_score(y_true,\n                             predicted_scores,\n                             k=2,\n                             labels=labels,\n                             sample_weight=sample_weights,\n                             normalize=True),\n        0.5)\n", "    def test_top_k_accuracy_score_multiclass_labels(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],\n                           [0.3, 0.4, 0.2],\n                           [0.2, 0.4, 0.3],\n                           [0.7, 0.2, 0.1]])\n        labels = np.array(['a','b','c'])\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_allclose(score, 0.75)\n\n", "def test_dcg_score_errors():\n    # Check for errors when input is not 2d\n    y_true = np.array([1, 0, 0, 1])\n    y_score = np.array([0.8, 0.2, 0.6, 0.9])\n    with pytest.raises(ValueError):\n        dcg_score(y_true, y_score[:, None])\n    with pytest.raises(ValueError):\n        dcg_score(y_true[:, None], y_score)\n\n    # Check for errors when y_true and y_score have different lengths\n    y_true = np.array([1, 0, 0, 1])\n    y_score = np.array([0.8, 0.2, 0.6])\n    with pytest.raises(ValueError):\n        dcg_score(y_true, y_score)\n\n    # Check for errors when k is negative or not an integer\n    with pytest.raises(ValueError):\n        dcg_score(y_true, y_score, k=-1)\n    with pytest.raises(TypeError):\n        dcg_score(y_true, y_score, k=1.5)\n\n    # Check for errors when sample_weight is not 1d\n\n    sample_weight = np.array([[1, 2, 3], [4, 5, 6]])\n\n    with pytest.raises(ValueError):\n        dcg_score(y_true, y_score, sample_weight=sample_weight)\n", "def test_label_ranking_average_precision_score_empty():\n    \"\"\"Check that label_ranking_average_precision_score handles empty inputs\n    correctly\"\"\"\n    y_true = np.array([], dtype=int)\n    y_score = np.array([], dtype=float)\n    with pytest.raises(ValueError):\n        label_ranking_average_precision_score(y_true, y_score)\n", "    def test_top_k_accuracy_score_multiclass_labels_provided():\n        # Test case with provided labels for multiclass classification.\n\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],\n                           [0.3, 0.4, 0.2],\n                           [0.2, 0.4, 0.3],\n                           [0.7, 0.2, 0.1]])\n        labels = [2, 1, 0]  # Labels in a different order\n\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_almost_equal(score, 0.5)\n\n"], "sample_205": ["    def test_validationerror_error_dict_property(self):\n        error_dict = {\"field1\": [\"Error 1\", \"Error 2\"], \"field2\": [\"Error 3\"]}\n        error = ValidationError(error_dict)\n        self.assertEqual(error.message_dict, error_dict)\n", "    def test_validationerror_from_dict(self):\n        error_dict = {'field1': ['Error 1', 'Error 2'], 'field2': 'Error 3'}\n        ve = ValidationError(error_dict)\n\n        self.assertEqual(ve.error_dict, error_dict)\n        self.assertEqual(list(ve), ['Error 1', 'Error 2', 'Error 3'])\n        self.assertEqual(ve.messages, ['Error 1', 'Error 2', 'Error 3'])\n", "    def test_validate_error_update(self):\n        error_dict = {}\n        message = ValidationError(\"Error message\")\n        message.update_error_dict(error_dict)\n        self.assertEqual(error_dict.get(NON_FIELD_ERRORS), [message])\n", "    def test_validation_error_from_dict(self):\n        error_dict = {'field1': ['Error message 1', 'Error message 2'],\n                      'field2': ValidationError('Error message 3')}\n        ve = ValidationError(error_dict)\n        self.assertEqual(ve.error_dict, {'field1': ['Error message 1', 'Error message 2'],\n                                       'field2': ValidationError('Error message 3')})\n", "    def test_validationerror_from_dict(self):\n        error_dict = {\n            'name': ['This field is required.'],\n            'age': ['This field must be an integer.'],\n        }\n        ve = ValidationError(error_dict)\n        self.assertEqual(ve.error_dict, error_dict)\n        self.assertEqual(list(ve.messages), [\n            'This field is required.',\n            'This field must be an integer.',\n        ])\n"], "sample_841": ["    def test_ridge_classifier_cv_sparse(self):\n        for solver, X_filter in product([\"sparse_cg\", \"sag\"],\n                                        [SPARSE_FILTER, DENSE_FILTER]):\n            X = X_filter(X_diabetes)\n            clf = RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), solver=solver)\n            clf.fit(X, y_diabetes)\n            assert clf.coef_.shape == (1, X.shape[1])\n", "def test_ridge_solver_vs_scipy(solver, X_filter, tol=1e-5):\n    X = X_filter(X_diabetes)\n    y = y_diabetes\n\n    # Test with Ridge and compare with scipy.optimize.minimize\n    alpha = 1.0\n    ridge = Ridge(alpha=alpha, solver=solver, random_state=42)\n    ridge.fit(X, y)\n    coef_ridge = ridge.coef_\n\n        return 0.5 * np.mean((X @ coef - y)**2) + 0.5 * alpha * np.sum(\n            coef**2)\n\n    res = linalg.minimize(loss_func, np.zeros(X.shape[1]))\n    coef_scipy = res.x\n\n    assert_allclose(coef_ridge, coef_scipy, rtol=tol, atol=tol)\n", "    def test_ridgecv_sparse_data_gcv(self):\n        X = sp.csr_matrix(np.array([[1, 2], [3, 4], [5, 6]]))\n        y = np.array([1, 2, 3])\n        for solver in ('auto', 'svd', 'eigen'):\n            ridgecv = RidgeCV(alphas=[1, 2], fit_intercept=False,\n                              gcv_mode=solver)\n            ridgecv.fit(X, y)\n\n", "    def test_ridge_cv_sparse_multioutput(self):\n        rng = np.random.RandomState(0)\n        X = rng.rand(20, 5)\n        y = rng.rand(20, 3)\n        X_sparse = sp.csr_matrix(X)\n        clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1], cv=3, fit_intercept=False)\n\n        clf.fit(X_sparse, y)\n        coef_dense = clf.coef_\n\n        clf.fit(X, y)\n        coef_sparse = clf.coef_\n\n        assert_allclose(coef_dense, coef_sparse)\n", "    def test_ridge_cv_scoring(self, X_filter, scoring, solver):\n        X = X_filter(X_diabetes)\n        y = y_diabetes\n        ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1], scoring=scoring,\n                          solver=solver)\n        ridgecv.fit(X, y)\n        # Check that the best_score_ attribute is correctly set\n        assert ridgecv.best_score_ == -ridgecv.best_estimator_.score(X, y)\n\n"], "sample_1112": ["def test_digits_negative_base():\n    assert digits(-123, b=-2) == [-2, 1, 1, 1, 1]\n", "    def test_digits_negative_base():\n        assert digits(-10, -2) == [-2, 1, 0, 1, 0]\n\n", "    def test_digits_negative_base():\n        raises(ValueError, lambda: digits(10, b=-2))\n", "    def test_digits_negative_base():\n        assert digits(-10, b=-2) == [-2, 1, 0]\n", "    def test_is_palindromic_negatives():\n        assert is_palindromic(-121) == True\n        assert is_palindromic(-11, 2) == True\n        assert is_palindromic(-101, 10) == True\n"], "sample_286": ["    def test_save_model_with_default_pk(self):\n        obj = PrimaryKeyWithDefault.objects.create()\n        self.assertIsNotNone(obj.pk)\n        self.assertEqual(obj.pk, 1)\n", "    def test_get_or_create_with_defaults(self):\n        with mock.patch('django.db.models.Model._default_manager') as mock_manager:\n            mock_manager.get_or_create.return_value = (Article.objects.create(headline='Test'), True)\n            article, created = Article.objects.get_or_create(headline='Test', defaults={'content': 'Test content'})\n            self.assertEqual(article.headline, 'Test')\n            self.assertEqual(article.content, 'Test content')\n            self.assertTrue(created)\n\n", "    def test_select_on_save_default(self):\n        obj = ArticleSelectOnSave.objects.create(headline='Test')\n        self.assertEqual(obj.pub_date, None)\n", "    def test_deferred_fields_saving(self):\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            data = models.JSONField(default=dict)\n\n        obj = MyModel(name='test')\n        obj.save()\n        self.assertEqual(obj.data, {})\n\n", "    def test_model_with_custom_pk_default(self):\n        instance = PrimaryKeyWithDefault.objects.create(name='test')\n        self.assertEqual(instance.pk, 1)\n"], "sample_976": ["def test_symbols_exclude():\n    x, y = symbols('x y')\n    w = Wild('w', exclude=[x, y])\n    assert w.matches(pi) == {w: pi}\n    assert w.matches(x) is None\n    d = Dummy()\n    assert w.matches(d) == {w: d}\n", "def test_symbols_commutative():\n    x, y = symbols('x y', commutative=False)\n    assert x.is_commutative is False\n    assert y.is_commutative is False\n    assert (x*y != y*x) == True\n\n    x, y = symbols('x y')\n    assert x.is_commutative is True\n    assert y.is_commutative is True\n    assert (x*y == y*x) == True\n", "    def test_symbol_name_already_defined():\n        x = Symbol('x')\n        raises(ValueError, lambda: Symbol('x'))\n", "def test_symbols_exclude():\n    x, y = symbols('x y')\n    a = Wild('a', exclude=[x])\n    b = Wild('b', exclude=[y])\n\n    assert a.match(x + y) is None\n    assert b.match(x + y) is None\n    assert a.match(y + 1) is not None\n    assert b.match(x + 1) is not None", "def test_symbols_range_single_digit():\n    assert symbols('x1:3') == (Symbol('x1'), Symbol('x2'))\n"], "sample_379": ["    def test_safe_string_concatenation(self):\n        self.assertIsInstance(mark_safe(\"Hello\") + mark_safe(\" world\"), SafeString)\n        self.assertIsInstance(mark_safe(\"Hello\") + \" world\", str)\n        self.assertIsInstance(\"Hello\" + mark_safe(\" world\"), str)\n        self.assertEqual(mark_safe(\"Hello\") + mark_safe(\" world\"), \"Hello world\")\n        self.assertEqual(mark_safe(\"Hello\") + \" world\", \"Hello world\")\n        self.assertEqual(\"Hello\" + mark_safe(\" world\"), \"Hello world\")\n", "    def test_safe_string_concatenation(self):\n        self.assertIsInstance(mark_safe('foo') + mark_safe('bar'), SafeString)\n        self.assertIsInstance(mark_safe('foo') + 'bar', str)\n        with self.assertRaises(TypeError):\n            mark_safe('foo') + 123\n        self.assertEqual(mark_safe('foo') + mark_safe('bar'), 'foobar')\n        self.assertEqual(mark_safe('foo') + 'bar', 'foobar')\n", "    def test_safe_string_addition(self):\n        s1 = mark_safe('<p>Hello')\n        s2 = ' World</p>'\n        self.assertIsInstance(s1 + s2, SafeString)\n        self.assertEqual(str(s1 + s2), '<p>Hello World</p>')\n\n        s3 = mark_safe('<p>')\n        s4 = SafeString('Hello')\n        self.assertIsInstance(s3 + s4, SafeString)\n        self.assertEqual(str(s3 + s4), '<p>Hello')\n\n        s5 = '<p>'\n        s6 = mark_safe('Hello')\n        self.assertIsNotInstance(s5 + s6, SafeString)\n        self.assertEqual(str(s5 + s6), '<p>Hello')\n\n", "    def test_safe_add(self):\n        self.assertEqual(mark_safe('<b>') + mark_safe('</b>'), '<b></b>')\n\n        with self.assertRaises(TypeError):\n            mark_safe('<b>') + '</b>'\n        with self.assertRaises(TypeError):\n            '<b>' + mark_safe('</b>')\n", "    def test_safe_string_concatenation(self):\n        s1 = SafeString('<script>')\n        s2 = SafeString('alert(\"XSS\")</script>')\n        self.assertEqual(s1 + s2, '<script>alert(\"XSS\")</script>')\n        self.assertTrue(isinstance(s1 + s2, SafeString))\n        s3 = '<script>'\n        self.assertEqual(s1 + s3, '<script><script>')\n        self.assertFalse(isinstance(s1 + s3, SafeString))\n"], "sample_300": ["    def test_join_promotion_and_negated_filters(self):\n        # Test that join promotion works correctly with negated filters,\n        # ensuring that the correct join types are used.\n\n        a1 = Author.objects.create(name='Author 1')\n        a2 = Author.objects.create(name='Author 2')\n        Item.objects.create(name='Item 1', author=a1)\n        Item.objects.create(name='Item 2', author=a2)\n\n        # Case 1: AND with negated filter\n        qs = Item.objects.filter(author__name__icontains='Author', ~Q(author__name='Author 1'))\n        self.assertQuerysetEqual(qs, [Item.objects.get(pk=2)])\n\n        # Case 2: OR with negated filter\n        qs = Item.objects.filter(Q(author__name='Author 1') | ~Q(author__name='Author 2'))\n        self.assertQuerysetEqual(qs, [Item.objects.get(pk=1)])\n", "    def test_promote_joins_with_related_foreign_keys(self):\n        # Test that joins are promoted correctly when using related foreign keys\n        # in the WHERE clause.\n        author = Author.objects.create(name='Alice')\n        item1 = Item.objects.create(name='Item 1', author=author)\n        item2 = Item.objects.create(name='Item 2', author=author)\n\n        # Query using a related foreign key in the WHERE clause.\n        qs = Item.objects.filter(author__name='Alice')\n\n        # Check that the join between Item and Author has been promoted to an\n        # INNER JOIN.\n        self.assertEqual(qs[0].author, author)\n        self.assertEqual(qs[1].author, author)\n        self.assertQuerysetEqual(qs, [item1, item2])\n", "    def test_get_loaded_field_names(self):\n        author = Author.objects.defer('email', 'date_joined').select_related('best_ranking').only('name', 'best_ranking__rank')\n\n        loaded_fields = author.get_loaded_field_names()\n        self.assertEqual(loaded_fields, {\n            Author: {'name'},\n            Ranking: {'rank'}\n        })\n\n        author = Author.objects.defer('email').select_related('best_ranking')\n\n        loaded_fields = author.get_loaded_field_names()\n        self.assertEqual(loaded_fields, {\n            Author: {'name', 'date_joined'},\n            Ranking: {'rank'}\n        })\n", "    def test_split_exclude_and_null_like_expressions(self):\n        query = Query(\n            model=Item,\n            where=(\n                Q(title__isnull=True) |\n                Q(title__icontains='test')\n            ),\n        )\n        query.set_group_by(allow_aliases=True)\n        split_query = query.split_exclude(\n            exclude_cols=['title'],\n        )\n        self.assertEqual(split_query.select, [F('title')])\n        self.assertEqual(split_query.where.children[0].connector, 'OR')\n        self.assertTrue(isinstance(\n            split_query.where.children[0].children[0],\n            IsNull,\n        ))\n        self.assertTrue(\n            isinstance(split_query.where.children[0].children[1],\n                       Exact),\n        )\n", "    def test_join_promotion_complex(self):\n        a = Author.objects\n        r = Ranking.objects\n        q = a.filter(\n            Q(rankings__score__gt=5) | Q(rankings__score__lt=1)\n        ).filter(\n            Q(rankings__name='foo') & Q(rankings__name='bar')\n        )\n        promoted_joins = q.query.join_cache.keys()\n        self.assertIn('rankings', promoted_joins)\n\n"], "sample_94": ["    def test_createsuperuser_interactive_with_custom_user_model(self, *args):\n        call_command('createsuperuser', verbosity=2, interactive=True, database='test',\n                     stdin=MockTTY())\n\n        user = CustomUser.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertEqual(user.password, 'password')\n", "    def test_createsuperuser_with_password_validation(self):\n        self.user_model = apps.get_model('auth', 'User')\n        with override_settings(AUTH_PASSWORD_VALIDATORS=[\n                {'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator'},\n        ]):\n            call_command('createsuperuser', interactive=True, username='alice',\n                         stdin=MockTTY(), stdout=StringIO())\n            self.assertEqual(User.objects.count(), 1)\n            user = User.objects.get(username='alice')\n            self.assertTrue(user.is_superuser)\n\n", "    def test_createsuperuser_interactive_with_password(self):\n        mock_stdout = StringIO()\n        call_command('createsuperuser',\n            interactive=True, stdout=mock_stdout,\n            stdin=MockTTY())\n\n        self.assertIn('Superuser created successfully.', mock_stdout.getvalue())\n\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.username, 'alice')\n        self.assertTrue(user.check_password('password'))\n\n", "    def test_createsuperuser_with_password_validation(self):\n        \"\"\"\n        Test that createsuperuser prompts for password confirmation and validates\n        it against the password policy.\n        \"\"\"\n        out = StringIO()\n        err = StringIO()\n        calls = []\n        with override_settings(PASSWORD_HASHERS=['django.contrib.auth.hashers.MD5PasswordHasher']), \\\n                mock.patch('django.contrib.auth.password_validation.validate_password', side_effect=lambda password, user: calls.append(password)), \\\n                mock.patch('sys.stdout', out), \\\n                mock.patch('sys.stderr', err):\n            call_command('createsuperuser', username='alice', verbosity=1,stdin=MockTTY())\n        self.assertIn('Superuser created successfully.', out.getvalue())\n        self.assertEqual(calls, ['password'])\n", "    def test_createsuperuser_email_and_password(self):\n        # Test creating a superuser with email and password.\n        out = StringIO()\n        err = StringIO()\n        sys.stdout = out\n        sys.stderr = err\n        call_command('createsuperuser', interactive=True, verbosity=1,\n                     stdin=MockTTY())\n        sys.stdout = sys.__stdout__\n        sys.stderr = sys.__stderr__\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertEqual(user.password, User.objects.make_random_password())\n        self.assertIn(\"Superuser created successfully.\", out.getvalue())\n\n"], "sample_49": ["    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = [TextInput(), TextInput()]\n                super().__init__(widgets, attrs)\n\n        class MyForm(Form):\n            myfield = MyMultiWidget()\n\n        form = MyForm()\n        self.assertEqual(form.media.render_js(), '')\n        self.assertEqual(\n            form.media.render_css(),\n            '<link href=\"http://media.example.com/static/django/css/widgets.css\" rel=\"stylesheet\" type=\"text/css\">'\n        )\n", "    def test_media_from_multiwidget(self):\n        class TestMultiWidget(MultiWidget):\n                super().__init__(widgets)\n\n        class MyForm(Form):\n            my_field = CharField(widget=TestMultiWidget([TextInput(), TextInput()]))\n\n        form = MyForm()\n        self.assertEqual(\n            form.media.render_js(),\n            '<script src=\"http://media.example.com/static/__/django/forms.js\"></script>',\n        )\n", "    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(), TextInput())\n                super().__init__(widgets, attrs)\n\n        class MyForm(Form):\n            my_field = MyMultiWidget()\n\n        form = MyForm()\n        self.assertEqual(form.media.render_html(), '<link rel=\"stylesheet\" type=\"text/css\" href=\"http://media.example.com/static/admin/css/widgets.css\" />')\n\n", "    def test_multiwidget_media(self):\n        class TestMultiWidget(MultiWidget):\n                widgets = (TextInput(), TextInput())\n                super().__init__(widgets, attrs)\n\n        class TestForm(Form):\n            field = TestMultiWidget()\n\n        form = TestForm()\n        self.assertEqual(form.media, Media(js=[\n            'http://media.example.com/static/django/forms/widgets.js',\n        ]))\n", "    def test_media_property(self):\n        class CustomWidget(TextInput):\n            media = Media(css={'all': ('custom.css',)})\n\n        class MyForm(Form):\n            field1 = CharField(widget=CustomWidget)\n            field2 = CharField()\n\n        form = MyForm()\n        self.assertEqual(form.media, Media(css={'all': ('custom.css',)}) + Media())\n\n        # Test inheritance of media\n        class MyMultiWidget(MultiWidget):\n                super().__init__([CustomWidget(), CustomWidget()], attrs)\n        class AnotherForm(Form):\n            field3 = CharField(widget=MyMultiWidget())\n\n        form = AnotherForm()\n        self.assertEqual(form.media, Media(css={'all': ('custom.css',)}) * 2)\n\n"], "sample_987": ["def test_evalf_piecewise():\n    p = Piecewise((x, x < 1), (x**2, x >= 1))\n    assert NS(p.evalf(2), 2, subs={x: 0.5}) == '0.50'\n    assert NS(p.evalf(2), 2, subs={x: 2}) == '4.00'\n", "def test_evalf_piecewise():\n    x = Symbol('x')\n    p = Piecewise((0, x < 0), (1, x >= 0))\n    assert NS(p.subs(x, -1)) == '0'\n    assert NS(p.subs(x, 0)) == '1'\n    assert NS(p.subs(x, 1)) == '1'\n\n    p = Piecewise((x**2, x < 0), (x, True))\n    assert NS(p.subs(x, -2)) == '4'\n    assert NS(p.subs(x, 0)) == '0'\n    assert NS(p.subs(x, 2)) == '2'\n", "    def test_evalf_log_prec():\n        assert NS(log(E, 10), 10) == '0.434294481903'\n        assert NS(log(7, 10), 15) == '0.845098040014'\n        assert NS(log(2, 3), 10) == '0.630929753571'\n\n", "    def test_evalf_abs(self):\n        assert NS(Abs(5)) == '5'\n        assert NS(Abs(-5)) == '5'\n        assert NS(Abs(S.ImaginaryUnit*5)) == '5'\n        assert NS(Abs(S.ComplexInfinity)) == 'inf'\n        assert NS(Abs(Abs(5))) == '5'\n        assert NS(Abs(5+4*I)) == '6.403'\n\n", "    def test_evalf_piecewise():\n        p = Piecewise((x, x > 0), (-x, True))\n        assert NS(p.evalf(2, subs={x: 2})) == '2.00'\n        assert NS(p.evalf(2, subs={x: -2})) == '2.00'\n        assert NS(p.evalf(2, subs={x: 0})) == '0.00'\n"], "sample_226": ["    def test_test_db_signature(self):\n        test_connection = get_connection_copy()\n        original_settings = copy.deepcopy(test_connection.settings_dict)\n        # Test that the signature includes the database name\n        self.assertTupleEqual(\n            BaseDatabaseCreation(test_connection).test_db_signature(),\n            (original_settings['HOST'], original_settings['PORT'],\n             original_settings['ENGINE'],\n             TEST_DATABASE_PREFIX + original_settings['NAME']),\n        )\n        # Test that changing the database name changes the signature\n        test_connection.settings_dict['NAME'] = 'new_name'\n        self.assertTupleEqual(\n            BaseDatabaseCreation(test_connection).test_db_signature(),\n            (original_settings['HOST'], original_settings['PORT'],\n             original_settings['ENGINE'],\n             TEST_DATABASE_PREFIX + 'new_name'),\n        )\n", "    def test_test_db_signature(self):\n        connection = get_connection_copy()\n        test_db_creation = BaseDatabaseCreation(connection)\n        signature1 = test_db_creation.test_db_signature()\n        # Modify some settings\n        connection.settings_dict['NAME'] = 'test_db_name'\n        signature2 = test_db_creation.test_db_signature()\n        self.assertNotEqual(signature1, signature2)\n", "    def test_test_db_signature_includes_specific_elements(self):\n        # Test that test_db_signature includes specific elements from the\n        # settings_dict.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['HOST'] = 'test_host'\n        test_connection.settings_dict['PORT'] = 1234\n        test_connection.settings_dict['ENGINE'] = 'test_engine'\n        test_connection.settings_dict['NAME'] = 'test_name'\n\n        creation = BaseDatabaseCreation(test_connection)\n        signature = creation.test_db_signature()\n        self.assertIn('test_host', signature)\n        self.assertIn(1234, signature)\n        self.assertIn('test_engine', signature)\n        self.assertIn(TEST_DATABASE_PREFIX + 'test_name', signature)\n", "    def test_test_db_signature(self):\n        conn = get_connection_copy()\n        creation = BaseDatabaseCreation(conn)\n        signature = creation.test_db_signature()\n        # Assert that the signature contains the expected elements\n        self.assertIn(conn.settings_dict['HOST'], signature)\n        self.assertIn(conn.settings_dict['PORT'], signature)\n        self.assertIn(conn.settings_dict['ENGINE'], signature)\n        self.assertIn(TEST_DATABASE_PREFIX + conn.settings_dict['NAME'], signature)\n", "    def test_test_db_signature(self):\n        conn = get_connection_copy()\n        creation = BaseDatabaseCreation(conn)\n        signature = creation.test_db_signature()\n\n        # Check that signature reflects the expected values\n        self.assertIn(conn.settings_dict['HOST'], signature)\n        self.assertIn(conn.settings_dict['PORT'], signature)\n        self.assertIn(conn.settings_dict['ENGINE'], signature)\n        self.assertIn(TEST_DATABASE_PREFIX + conn.settings_dict['NAME'], signature)\n"], "sample_574": ["    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n        # TODO more comprehensive tests of formatter output\n        # (dependent on locale, tick interval, etc.)\n", "    def test_temporal_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n\n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n\n        # TODO more specific assertions about the formatter output?\n", "    def test_label_formatter_function(self, x):\n        a, locs = self.setup_labels(x, formatter=lambda x: f'{x:.0f}')\n        labels = a.major.formatter.format_ticks(locs)\n        assert labels == ['1', '3', '9']\n\n", "    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n\n\n", "    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n\n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n        assert len(formatter.format_ticks(locs)) == len(locs)\n\n"], "sample_1192": ["def test_symbols_disambiguate():\n    x, y = symbols('x y')\n    x_1, x_2 = Dummy('x'), Dummy('x')\n    assert disambiguate(x, y, x_1, x_2) == (x, y, x_1, x_2)\n    assert disambiguate(x + y, x_1 + x_2) == (x + y, x_1 + x_2)\n", "def test_disambiguate_symbols():\n    x, y, z = symbols('x y z')\n    a = Symbol('x')\n    b = Dummy('x')\n    c = Dummy('x')\n    assert tuple(disambiguate(x, y, z, a, b, c)) == (x, y, z, x_1, x, x_2)\n", "def test_disambiguate_empty():\n    assert disambiguate() == ()\n", "def test_undefined_function():\n    x = Symbol('x')\n    f = UndefinedFunction('f')\n    assert f(x) == f(x)\n    assert f(x) != f(Symbol('y'))\n    assert f(x).free_symbols == {x}\n    assert f(x).args == (x,)\n    assert (f(x) + 1).free_symbols == {x}\n    assert f(x, y=x).free_symbols == {x}\n    assert f(x, y=x).args == (x, {'y': x})\n\n    raises(TypeError, lambda: f(x, y=2))\n", "    def test_disambiguate_with_mapping():\n        x = Symbol('x')\n        y = Symbol('y')\n        z = Dummy('z')\n        expr = x + y + z\n        free = expr.free_symbols\n        mapping = dict(zip(free, disambiguate(*free)))\n        assert expr.xreplace(mapping) == x + y + z\n"], "sample_391": ["    def test_add_and_remove_same_field_with_different_args(self):\n        operations = [\n            migrations.AddField(\n                model_name='unicodemodel', name='foo', field=models.CharField(max_length=10)\n            ),\n            migrations.RemoveField(model_name='unicodemodel', name='foo'),\n        ]\n        self.assertOptimizesTo(\n            operations,\n            [],\n        )\n", "    def test_rename_model_with_m2m_field_with_through_table(self):\n        \"\"\"\n        #28106 - Renaming a model with a M2M field that has a through table should\n        generate the correct migrations.\n        \"\"\"\n        class MyModel(models.Model):\n            pass\n\n        class AnotherModel(models.Model):\n            pass\n\n        class MyThroughModel(models.Model):\n            my_model = models.ForeignKey(MyModel, on_delete=models.CASCADE)\n            another_model = models.ForeignKey(AnotherModel, on_delete=models.CASCADE)\n\n        initial_state = {\n            'models': {\n                'migrations': {\n                    'MyModel': {\n                        'fields': {},\n                    },\n                    'AnotherModel': {\n                        'fields': {},\n                    },\n                    'MyThroughModel': {\n                        'fields': {\n                            'my_model': {\n                                'null': 'False',\n                                'on_delete': 'CASCADE',\n                                'to': 'migrations.MyModel',\n                            },\n                            'another_model': {\n                                'null': 'False',\n                                'on_delete': 'CASCADE',\n                                'to': 'migrations.AnotherModel',\n                            },\n                        },\n                    },\n                },\n            },\n        }\n\n        old_model_name = 'MyModel'\n        new_model_name = 'RenamedModel'\n\n        operations = [\n            migrations.RenameModel(old_model_name, new_model_name),\n        ]\n\n        expected_operations = [\n            operations.RenameModel(old_model_name, new_model_name),\n            operations.AlterModelTable(new_model_name, 'migrations_%s' % new_model_name.lower())\n        ]\n\n        self.assertOptimizesTo(operations, expected_operations)\n", "    def test_remove_non_existent_field_after_add(self):\n        # Removing a non-existent field after adding it should be optimized away\n        operations = [\n            operations.AddField(\n                model_name='UnicodeModel',\n                name='new_field',\n                field=models.CharField(max_length=255),\n            ),\n            operations.RemoveField(\n                model_name='UnicodeModel',\n                name='new_field',\n            ),\n        ]\n        self.assertOptimizesTo(operations, [])\n\n", "    def test_rename_model_with_field_references(self):\n        before_operations = [\n            migrations.CreateModel(\n                name=\"Foo\",\n                fields=[\n                    models.CharField(max_length=10, name=\"bar\"),\n                ],\n            ),\n            migrations.CreateModel(\n                name=\"Baz\",\n                fields=[\n                    models.ForeignKey(\n                        models.CharField(max_length=10),\n                        to=\"migrations.Foo\",\n                        name=\"foo\",\n                    ),\n                ],\n            ),\n        ]\n        after_operations = [\n            migrations.RenameModel(\n                old_name=\"Foo\",\n                new_name=\"FooBar\",\n            ),\n            migrations.AlterField(\n                model_name=\"baz\",\n                name=\"foo\",\n                field=models.ForeignKey(\n                    models.CharField(max_length=10),\n                    to=\"migrations.FooBar\",\n                    name=\"foo\",\n                ),\n            ),\n        ]\n        self.assertOptimizesTo(before_operations, after_operations)\n", "    def test_optimize_rename_model_with_m2m(self):\n        class Post(models.Model):\n            pass\n\n        class Tag(models.Model):\n            pass\n\n        Post.objects.create()\n        Tag.objects.create()\n\n        initial_state = migrations.MigrationState(\n            apps=[\n                migrations.App(\"test_app\"),\n            ],\n        )\n\n        initial_state.models = {\n            \"test_app\": {\n                \"post\": initial_state.add_model(\n                    \"test_app\", \"Post\", Post, (\"id\",)\n                ),\n                \"tag\": initial_state.add_model(\n                    \"test_app\", \"Tag\", Tag, (\"id\",)\n                ),\n            },\n        }\n\n        initial_state.m2m.update(\n            {\n                \"test_app\": {\n                    \"post_tags\": initial_state.add_m2m(\n                        \"test_app\",\n                        \"Post\",\n                        \"Tag\",\n                        \"post_tags\",\n                        (\"post\", \"tag\"),\n                    ),\n                },\n            },\n        )\n\n        # Rename Post to Article\n        post_rename = operations.RenameModel(\n            \"Post\", \"Article\"\n        )\n\n        self.assertOptimizesTo(\n            [post_rename],\n            [\n                operations.RenameModel(\"Post\", \"Article\"),\n                operations.AlterModelTable(\"Article\", \"article_post\"),\n                operations.AlterUniqueTogether(\n                    \"Article\",\n                    unique_together=[\n                        (\"id\", \"post_tags\"),\n                    ],\n                ),\n            ],\n            less_than=2,\n        )\n\n\n"], "sample_986": ["    def test_evalf_piecewise():\n        p = Piecewise((x**2, x < 0), (x, x >= 0))\n        assert NS(p.evalf(subs={x: -2})) == '-4.00000000000000'\n        assert NS(p.evalf(subs={x: 2})) == '2.00000000000000'\n        assert NS(p.evalf(subs={x: 0})) == '0.00000000000000'\n\n", "    def test_evalf_piecewise():\n        piecewise = Piecewise((x, x < 0), (x**2, x >= 0))\n        assert NS(piecewise.subs(x, -1)) == '-1.000000000000000'\n        assert NS(piecewise.subs(x, 1)) == '1.000000000000000'\n        assert NS(piecewise.subs(x, 0)) == '0.000000000000000'\n", "def test_evalf_piecewise():\n    q = Piecewise((0, x < 0), (1, x >= 0))\n    assert NS(q, 10) == '0.0000000000'\n    assert NS(q.subs(x, 1), 10) == '1.0000000000'\n    assert NS(q.subs(x, -1), 10) == '0.0000000000'\n", "def test_evalf_piecewise():\n    p = Piecewise((x**2, x < 0), (x, True))\n    assert N(p.subs(x, -1)) == 1\n    assert N(p.subs(x, 1)) == 1\n", "def test_evalf_add_with_infinities():\n    assert NS(oo + 1) =='inf'\n    assert NS(1 + oo) == 'inf'\n    assert NS(-oo + 1) == '-inf'\n    assert NS(1 + (-oo)) == '-inf'\n    assert NS(oo + (-oo)) == 'nan'\n\n"], "sample_1029": ["def test_srepr_Matrix_with_symbols():\n    A = Matrix([[x, y], [1, x + y]])\n    sT(A, \"Matrix([[x, y], [Integer(1), Add(x, y)]])\")\n", "    def test_srepr_FractionField():\n        f = field(\"x\", QQ, lex)\n        sT(f, \"FractionField(QQ, [x], lex)\")\n", "    def test_srepr_PolyRing():\n        R = ring('x', ZZ)\n        sT(R, \"PolyRing('x', ZZ, lex)\")\n", "compilation error", "compilation error"], "sample_662": ["    def test_collectreport_serialization(self):\n        report = CollectReport(\n            nodeid=\"hello\", outcome=\"failed\", longrepr=\"somefailure\", result=[\"a\"]\n        )\n        serialized = pytest_report_to_serializable(report)\n        recreated = pytest_report_from_serializable(serialized)\n        assert recreated.nodeid == report.nodeid\n        assert recreated.outcome == report.outcome\n        assert recreated.longrepr == report.longrepr\n        assert recreated.result == report.result\n", "    def test_serialize_report_with_exception_chain(self, tmpdir):\n        excinfo = pytest.raises(ValueError, lambda: 1/0)\n        exception_info = ExceptionChainRepr([\n            (\n                ExceptionChainRepr.from_exception(excinfo).reprtraceback,\n                ExceptionChainRepr.from_exception(excinfo).reprcrash,\n                \"First error\"\n            )\n        ])\n        report = TestReport.from_item_and_call(\n            item=pytest.Item.from_parent(session=None, name=\"test_foo\", nodeid=\"test_foo\"),\n            call=pytest.CallInfo(when=\"call\", excinfo=excinfo, start=0, stop=1),\n        )\n        report.longrepr = exception_info\n        serialized_report = pytest_report_to_serializable(report)\n        deserialized_report = pytest_report_from_serializable(serialized_report)\n        assert isinstance(deserialized_report.longrepr, ExceptionChainRepr)\n        assert deserialized_report.longrepr.chain == report.longrepr.chain\n\n", "    def test_report_serialization_exception_chain(self):\n        rep = TestReport(\n            nodeid=\"test_func\",\n            location=(Path(\"test_file.py\"), 10, \"test_func\"),\n            keywords={\"hello\": \"world\"},\n            outcome=\"failed\",\n            longrepr=ExceptionChainRepr([\n                (\n                    ReprTraceback(\n                        reprentries=[\n                            ReprEntry(\n                                lines=[\"some_traceback_entry_1\"],\n                                style=\"short\"\n                            )\n                        ]\n                    ),\n                    ReprFileLocation(\"path\", 12, \"lineno\", \"domain\"),\n                    \"Description 1\",\n                ),\n                (\n                    ReprTraceback(\n                        reprentries=[\n                            ReprEntry(\n                                lines=[\"some_traceback_entry_2\"],\n                                style=\"short\"\n                            )\n                        ]\n                    ),\n                    ReprFileLocation(\"path2\", 13, \"lineno\", \"domain\"),\n                    \"Description 2\",\n                ),\n            ]),\n            when=\"call\",\n            duration=1.23,\n        )\n\n        serialized = pytest_report_to_serializable(rep)\n        deserialized_report = pytest_report_from_serializable(serialized)\n        assert deserialized_report.longrepr.count == 2\n        assert deserialized_report.longrepr.chain[0][2] == \"Description 1\"\n\n", "    def test_collect_report_deserialization_with_exception(self, tmpdir):\n        root = tmpdir.mkdir(\"root\")\n        test_file = root.join(\"test_file.py\")\n        test_file.write(\"def test_foo(): raise ValueError('oh no')\")\n        pytest.main([\"-v\", str(root)])\n        collected_reports = self.config.pluginmanager.getplugin(\"terminalreporter\").stats.get(\n            \"passed\"\n        )\n        assert len(collected_reports) == 0\n        failed_reports = self.config.pluginmanager.getplugin(\"terminalreporter\").stats.get(\n            \"failed\"\n        )\n        assert len(failed_reports) == 1\n        \n        # serialize the report\n        serialized_report = pytest_report_to_serializable(failed_reports[0])\n        # deserialize it\n        deserialized_report = pytest_report_from_serializable(serialized_report)\n        assert isinstance(deserialized_report, TestReport)\n        assert deserialized_report.outcome == \"failed\"\n        assert isinstance(deserialized_report.longrepr, ExceptionChainRepr)\n        assert len(deserialized_report.longrepr.chain) == 1\n        assert isinstance(deserialized_report.longrepr.chain[0][1], ReprFileLocation)\n        assert deserialized_report.longrepr.chain[0][1].path == str(test_file)\n\n", "    def test_report_to_json_and_back_exception_chain(self, tmpdir):\n        report = TestReport(\n            nodeid=\"foo\", location=(\"foo.py\", 1, \"foo\"), outcome=\"failed\", longrepr=ExceptionChainRepr([]), when=\"call\"\n        )\n        json_report = pytest_report_to_serializable(report)\n        \n        reconstructed_report = pytest_report_from_serializable(json_report)\n"], "sample_296": ["    def test_cookie_storage_not_finished_sentinel(self):\n        storage = self._get_storage()\n        messages = [Message('test', constants.DEBUG)] * 5\n\n        # Set cookie with messages and not_finished sentinel\n        set_cookie_data(storage, messages + [CookieStorage.not_finished])\n\n        # Retrieve messages\n        retrieved_messages, all_retrieved = storage._get()\n\n        self.assertEqual(retrieved_messages, messages)\n        self.assertFalse(all_retrieved)\n\n", "    def test_cookie_storage_encode_empty(self):\n        storage = self.storage_class(request=self.request)\n        encoded_data = storage._encode([], encode_empty=True)\n        self.assertTrue(encoded_data)\n        decoded_data = storage._decode(encoded_data)\n        self.assertEqual(decoded_data, [])\n", "    def test_cookie_storage_max_cookie_size_with_multiple_messages(self):\n        storage = self.storage_class(self.request)\n\n        # Create a list of messages to exceed the max cookie size\n        messages = [Message(level=constants.INFO, message=get_random_string(100)) for _ in range(20)]\n        set_cookie_data(storage, messages)\n\n        response = self.request.build_response()\n        self.assertEqual(stored_cookie_messages_count(storage, response), storage.max_cookie_size // 100)\n", "    def test_cookie_storage_invalid_data(self):\n        storage = self.storage_class(\n            request=self.request\n        )\n        messages = [\n            Message(\n                'This is a warning message.', constants.WARNING,\n                extra_tags=['important']\n            ),\n            Message(\n                'This is an error message.', constants.ERROR,\n                extra_tags=['urgent']\n            ),\n        ]\n        set_cookie_data(storage, messages, invalid=True)\n        response = self.get_response()\n        self.assertEqual(stored_cookie_messages_count(storage, response), 0)\n", "    def test_encode_decode_with_safedata(self):\n        \"\"\"\n        Ensure that messages containing SafeData are encoded and decoded\n        correctly.\n        \"\"\"\n        storage = self.storage_class()\n        message = Message(mark_safe('<b>Hello, world!</b>'), constants.INFO)\n        messages = [message]\n\n        # Encode and decode the message\n        encoded_data = storage._encode(messages)\n        decoded_messages = storage._decode(encoded_data)\n\n        # Assert that the decoded message is the same as the original\n        self.assertEqual(len(decoded_messages), 1)\n        self.assertEqual(decoded_messages[0].message, '<b>Hello, world!</b>')\n        self.assertIsInstance(decoded_messages[0].message, SafeData)\n"], "sample_535": ["    def test_table_edges(self):\n        fig, ax = plt.subplots()\n        table = Table(ax, loc='center')\n        table.add_cell(0, 0, width=0.2, height=0.1, text='A')\n        table.edges = 'horizontal'\n        table.auto_set_column_width(0)\n        fig.canvas.draw()\n        renderer = fig.canvas.get_renderer()\n        for cell in table.get_celld().values():\n            for path in cell.get_path().get_paths():\n                self.assertEqual(len(path.vertices), 5)\n        \n", "    def test_table_empty_cell(self, fig_test, fig_ref):\n        ax = fig_test.add_subplot(111)\n        table = Table(ax)\n\n        # Add an empty cell\n        cell = table.add_cell(0, 0, width=0.2, height=0.2)\n\n        # Add a cell with content\n        table.add_cell(0, 1, width=0.2, height=0.2, text=\"Hello\")\n\n        ax.add_table(table)\n        fig_test.canvas.draw()\n", "    def test_table_cell_visibility(self, fig_test, fig_ref):\n        ax = fig_test.add_subplot(111)\n        table = Table(ax, loc='upper left', bbox=[0.1, 0.1, 0.8, 0.8])\n\n        for i in range(3):\n            for j in range(3):\n                facecolor = 'lightgrey'\n                if i == j:\n                    facecolor = 'lightgreen'\n\n                cell = CustomCell(np.array([0.1, 0.1]), width=0.2, height=0.2,\n                                  facecolor=facecolor,\n                                  visible_edges=f'TRB') if i != j else 'open'\n                table[i, j] = cell\n\n        ax.add_table(table)\n\n        ax = fig_ref.add_subplot(111)\n        table = Table(ax, loc='upper left', bbox=[0.1, 0.1, 0.8, 0.8])\n\n        for i in range(3):\n            for j in range(3):\n                facecolor = 'lightgrey'\n                if i == j:\n                    facecolor = 'lightgreen'\n\n                cell = CustomCell(np.array([0.1, 0.1]), width=0.2, height=0.2,\n                                  facecolor=facecolor,\n                                  visible_edges='open')\n                table[i, j] = cell\n\n        ax.add_table(table)\n", "def test_auto_set_column_width():\n    fig, ax = plt.subplots()\n    data = [['A', 'B', 'C', 'D'],\n            ['E', 'F', 'G', 'H'],\n            ['I', 'J', 'K', 'L']]\n    table = Table(ax, bbox=[0.1, 0.1, 0.8, 0.8])\n    for row in range(len(data)):\n        for col in range(len(data[row])):\n            table.add_cell(row, col, width=0.1, height=0.1,\n                           text=data[row][col], loc='center')\n    table.auto_set_column_width(0)\n    ax.add_table(table)\n    ax.set_axis_off()\n    return fig\n\n", "    def test_bbox(self):\n        fig, ax = plt.subplots()\n        bbox = [0.2, 0.2, 0.4, 0.4]\n        table(ax, cellText=np.arange(9).reshape((3, 3)), bbox=bbox)\n        plt.show()\n"], "sample_875": ["def test_brier_score_loss_with_pos_label_specified():\n    y_true = np.array([0, 1, 0, 1])\n    y_prob = np.array([0.2, 0.8, 0.3, 0.9])\n    pos_label = 1\n\n    score = brier_score_loss(y_true, y_prob, pos_label=pos_label)\n    assert_almost_equal(score, 0.0625)\n", "    def test_brier_score_loss_multilabel():\n        # Test multilabel case\n        random_state = check_random_state(42)\n        y_true = label_binarize([\n            [0, 1],\n            [1, 0],\n            [0, 1],\n        ], classes=[0, 1])\n        y_prob = random_state.rand(3, 2)\n        score = brier_score_loss(y_true, y_prob)\n        assert isinstance(score, float)\n        assert score >= 0\n        assert score <= 1\n", "    def test_brier_score_loss_with_pos_label_string():\n        # Test brier score loss when pos_label is a string\n        y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        with pytest.raises(ValueError):\n            brier_score_loss(y_true, y_prob)\n        brier_score = brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n        assert_almost_equal(brier_score, 0.037, decimal=3)\n", "    def test_brier_score_loss_pos_label_object_dtype():\n        y_true = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        with pytest.raises(ValueError, match=\"pos_label should be specified\"):\n            brier_score_loss(y_true, y_prob)\n\n        score = brier_score_loss(y_true, y_prob, pos_label=\"ham\")\n        assert_almost_equal(score, 0.037, decimal=3)\n", "    def test_multilabel_confusion_matrix_micro_macro_weighted(self):\n        \"\"\"Test multilabel confusion matrix with micro, macro and weighted averaging.\n\n        Test that the returned metrics for the matrix are correct\n        for micro, macro and weighted averaging.\n        \"\"\"\n        y_true = np.array([[0, 1, 0], [1, 1, 0], [0, 0, 1]])\n        y_pred = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1]])\n        labels = [0, 1, 2]\n\n        mcm = multilabel_confusion_matrix(y_true, y_pred, labels=labels)\n        \n        # For the micro averaging\n        expected_micro_f1 = f1_score(y_true, y_pred, average='micro')\n        \n        np.testing.assert_almost_equal(\n            f1_score(mcm.ravel(), y_true.ravel()),\n            expected_micro_f1\n        )\n\n        # For the macro averaging\n        expected_macro_f1 = f1_score(y_true, y_pred, average='macro')\n\n        np.testing.assert_almost_equal(\n\n            np.mean([f1_score(y_true[:, i], y_pred[:, i]) for i in range(len(labels))]),\n            expected_macro_f1\n        )\n        \n        # For the weighted averaging\n        expected_weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n\n        np.testing.assert_almost_equal(\n\n            np.sum([(f1_score(y_true[:, i], y_pred[:, i])) * np.sum(y_true[:, i]) for i in range(len(labels))]) / np.sum(y_true),\n            expected_weighted_f1\n        )\n\n"], "sample_934": ["def test_cpp_function_template():\n    check('function',\n          'void foo<typename T>(T a, T b)',\n          {1: 'foo', 2: 'foo_T', 3: 'foo_T_T', 4: 'foo_T_T_T'},\n          'void foo<typename T>(T a, T b)',\n          key='foo')\n", "    def test_union_alias():\n        check(\n            'union',\n            \"union uName {{ int i; }};\",\n            {1: \"uName\"}, \"union uName {int i;};\",\n        )\n", "def test_cpp_enum_member():\n    check('enum member',\n          'MyEnum::MEMBER{key}',\n          {1: 'MyEnum_MEMBER'},\n          'MyEnum::MEMBER',\n          key='MyEnum',\n          asTextOutput='`MyEnum::MEMBER`')\n", "def test_function_params_with_defaults():\n    check(\"function\", \"int foo(int a = 10, const char* b = \\\"hello\\\")\",\n          {\n              1: \"foo\",  \n              2: \"cpp:func:foo\"\n          },\n          key = \"foo\",\n          output = \"int foo({0}int a = 10{0}const char* b = \\\"hello\\\"{0})\")\n", "    def test_cpp_enum_scoped():\n        check(\n            'enum',\n            \"enum {key}ONE, TWO, THREE}\",\n            {1: 'ONE', 2: 'TWO', 3: 'THREE'},\n            output=\"enum {key}ONE, TWO, THREE\",\n            key='Scoped'\n        )\n"], "sample_442": ["    def test_signature_expired(self):\n        with freeze_time(\"2023-01-01\"):\n            signer = signing.TimestampSigner(salt=\"test_salt\")\n            signed_data = signer.sign_object({\"key\": \"value\"})\n        with freeze_time(\"2023-01-02\"):\n            with self.assertRaises(signing.SignatureExpired):\n                signer.unsign_object(signed_data, max_age=1)\n\n", "    def test_signer_with_unicode_key(self):\n        signer = signing.Signer(key=\"key\ud83d\udd11\")\n        self.assertEqual(signer.sign(\"message\"), signer.sign(\"message\"))\n\n", "        def test_signing_with_timestamp_and_max_age(self):", "    def test_fallback_keys(self):\n        signing_backend = \"django.core.signing.Signer\"\n        with override_settings(SECRET_KEY=\"secret_key\", SIGNING_BACKEND=signing_backend):\n\n            signer = signing.get_cookie_signer(\"secret_key_fallback\")\n            original_data = {\"key\": \"value\"}\n            signed = signer.dumps(original_data)\n            self.assertEqual(signer.loads(signed), original_data)\n\n            # Test that when the primary key fails, the fallback key is used.\n            original_data = {\"key\": \"value\"}\n            signed = signing.get_cookie_signer(\n                \"wrong_key\"\n            ).dumps(original_data)\n            with self.assertRaises(BadSignature):\n                signing.get_cookie_signer(\"wrong_key\").loads(signed)\n\n            signer = signing.get_cookie_signer()\n            self.assertEqual(signer.loads(signed), original_data)\n", "    def test_timestamp_signer_max_age(self):\n        signer = signing.TimestampSigner()\n        signed_data = signer.sign_object({'foo': 'bar'})\n\n        with freeze_time(datetime.datetime(2023, 1, 1)):\n            # Check signature is valid within 10 seconds\n            signer.unsign_object(signed_data, max_age=10)\n\n        with freeze_time(datetime.datetime(2023, 1, 1) + datetime.timedelta(seconds=11)):\n            with self.assertRaises(signing.SignatureExpired):\n                signer.unsign_object(signed_data, max_age=10)\n\n"], "sample_810": ["def test_pipeline_memory():\n    memory = Memory(location=mkdtemp())\n    clf = Pipeline([\n        ('scaler', StandardScaler()),\n        ('svc', SVC())\n    ], memory=memory)\n\n    X, y = load_iris(return_X_y=True)\n    clf.fit(X, y)\n\n    # Check if the first step has been cached\n    with open(memory.cachedir + '/scaler.pkl', 'r') as f:\n        assert f.read()\n\n    # Check if the cache is reused\n    clf.fit(X, y)\n    assert clf.named_steps['scaler'].timestamp_ == clf.named_steps['scaler'].timestamp_\n\n    shutil.rmtree(memory.cachedir)\n", "    def test_pipeline_cache_fit_params(self):\n        # Test caching of fit_params in Pipeline\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n\n        # Create a Pipeline with a transformer that captures fit_params\n        transformer = TransfFitParams()\n        pipeline = Pipeline([\n            ('transformer', transformer)\n        ])\n\n        # Fit the pipeline with fit_params\n        pipeline.fit(X, y, transformer__fit_params={'a': 1})\n\n        # Check that fit_params were captured\n        assert_equal(transformer.fit_params, {'a': 1})\n\n        # Fit the pipeline again with a different set of fit_params\n        pipeline.fit(X, y, transformer__fit_params={'a': 2})\n\n        # Check that fit_params were updated\n        assert_equal(transformer.fit_params, {'a': 2})\n", "    def test_pipeline_with_steps_that_return_self(self):\n        pipe = Pipeline([('t', Transf()), ('m', Mult())]).fit(X, y)\n        assert_array_equal(pipe.transform(X), X * 2)\n        assert_equal(pipe[:-1], pipe.named_steps['t'])\n", "    def test_memory_cache_fit_params(self):\n        # Test that fit_params are correctly passed to the estimators when\n        # using memory caching\n        memory = Memory(location=mkdtemp(), verbose=0)\n        X, y = load_iris(return_X_y=True)\n        est = Pipeline([\n            ('transf', TransfFitParams()),\n            ('class', FitParamT())\n        ], memory=memory)\n\n        # First fit, should populate cache\n        est.fit(X, y, should_succeed=True, fit_params={'transf__mult': 2})\n\n        # Second fit, should load from cache, with the same fit_params\n        est.fit(X, y, should_succeed=True, fit_params={'transf__mult': 2})\n        assert est.named_steps['transf'].fit_params == {'mult': 2}\n\n        # Clean up tempfile\n        shutil.rmtree(memory.cachedir)\n\n", "    def test_pipeline_fit_params(self):\n        # Test that fit_params are passed correctly to transformers.\n\n        # Create a pipeline with a transformer that takes fit_params\n        pipeline = Pipeline([\n            ('transformer', TransfFitParams()),\n            ('classifier', DummyClassifier())\n        ])\n\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([0, 1])\n\n        # Fit the pipeline with fit_params\n        pipeline.fit(X, y, transformer__fit_params={'a': 1, 'b': 2})\n\n        # Check that the fit_params were passed correctly\n        assert_equal(pipeline.named_steps['transformer'].fit_params,\n                     {'a': 1, 'b': 2})\n"], "sample_69": ["    def test_watch_dir(self):\n        with tempfile.TemporaryDirectory() as tmpdir, extend_sys_path(tmpdir):\n            test_file = self.temporary_file('test_module.py')\n            test_file.write_text('')\n            self.clear_autoreload_caches()\n            r = autoreload.WatchmanReloader()\n            r.watch_dir(Path(tmpdir), '*.py')\n\n            self.assertFileFound(test_file)\n\n\n", "    def test_module_from_zip_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'my_archive.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module.py', 'def my_func(): pass')\n            sys.path.append(tmpdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n", "    def test_zipfile_module(self):\n        # Create a zip archive and import a module from it.\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('my_module.py', 'def my_func(): pass')\n            sys.path.append(str(tmpdir))\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path)\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_path = Path(tempdir) / 'my.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('module.py', 'pass')\n            sys.path.append(tempdir)\n            self.import_and_cleanup('my')\n            self.assertFileFound(zip_path)\n", "    def test_iter_modules_and_files_zipfile(self):\n        with self.temporary_file('my_module.py') as module_path:\n            with open(module_path, 'w') as f:\n                f.write('x = 1')\n            with zipfile.ZipFile('my_zip.zip', 'w') as zf:\n                zf.write(module_path, 'my_module.py')\n            sys.path.append(os.getcwd())\n            self.import_and_cleanup('my_zip')\n            self.assertFileFound(Path('my_zip.zip') / 'my_module.py')\n"], "sample_416": ["    def test_runshell_with_password(self):\n        with mock.patch(\"subprocess.check_call\") as mock_check_call:\n            with mock.patch.dict(os.environ, {\"PGPASSWORD\": \"testpassword\"}):\n                settings_dict = {\n                    \"NAME\": \"testdb\",\n                    \"USER\": \"testuser\",\n                    \"PASSWORD\": \"testpassword\",\n                    \"OPTIONS\": {\"sslmode\": \"require\"},\n                }\n                client = DatabaseClient(settings_dict)\n                client.runshell([\"some\", \"parameters\"])\n                mock_check_call.assert_called_once_with(\n                    [\n                        \"psql\",\n                        \"-U\",\n                        \"testuser\",\n                        \"-c\",\n                        \"some parameters\",\n                        \"testdb\",\n                    ],\n                    env=os.environ,\n                )\n", "    def test_runshell_sigint(self, mock_runshell):\n        with mock.patch('django.db.backends.postgresql.client.signal'):\n            DatabaseClient().runshell([])\n            mock_runshell.assert_called_once()\n", "    def test_runshell_sigint(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value.wait.return_value = 0\n            # Create a mock signal handler\n                pass\n\n            # Patch signal.signal and signal.getsignal\n            with mock.patch('signal.signal'), mock.patch('signal.getsignal') as mock_getsignal:\n                mock_getsignal.return_value = mock_sigint_handler\n                DatabaseClient().runshell([])\n\n            # Assert that signal.signal was called with SIG_IGN\n            mock_signal.assert_called_once_with(signal.SIGINT, signal.SIG_IGN)\n", "    def test_runshell_handles_sigint(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value.communicate.return_value = (b'', b'')\n            with mock.patch('signal.signal') as mock_signal:\n                DatabaseClient().runshell([])\n                mock_signal.assert_called()\n", "    def test_runshell_SIGINT_not_ignored(self):\n        with mock.patch(\"subprocess.Popen\") as mock_popen:\n            mock_popen.return_value.communicate.return_value = (\n                b\"\",\n                b\"\",\n            )\n            # Simulate a SIGINT signal interruption\n            signal.signal(signal.SIGINT, signal.SIG_IGN)\n            try:\n                DatabaseClient().runshell([\"-c\", \"SELECT 1\"])\n            except KeyboardInterrupt:\n                pass\n            signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n"], "sample_498": ["    def test_legend_with_bbox_to_anchor_and_loc(self):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend([line], [\"Line\"], bbox_to_anchor=(1, 1), loc='upper left')\n        fig.canvas.draw()\n\n        # Check if bbox_to_anchor overrides loc\n        bbox = line.get_tightbbox(ax.transData)\n        bbox_to_anchor = ax.transData.transform((1, 1))\n        assert_allclose(bbox.transformed(ax.transData).xy[0],\n                        bbox_to_anchor,\n                        err_msg='bbox_to_anchor should override loc')\n\n", "    def test_legend_handler_tuple():\n        fig, ax = plt.subplots()\n        x = np.arange(0, 10, 0.1)\n        line1, = ax.plot(x, np.sin(x), label='sin')\n        line2, = ax.plot(x, np.cos(x), label='cos')\n        ax.plot(x, x, label='linear')\n        handles, labels = ax.get_legend_handles_labels()\n\n        # Create a tuple of handles\n        tuple_handle = (line1, line2)\n        handles.append(tuple_handle)\n        labels.append('sin & cos')\n\n        legend = ax.legend(handles, labels)\n\n        assert legend.get_children()[0]._text.get_text() == 'sin'\n        assert legend.get_children()[1]._text.get_text() == 'cos'\n        assert legend.get_children()[2]._text.get_text() == 'linear'\n        assert legend.get_children()[3]._text.get_text() == 'sin & cos'\n", "    def test_legend_labels_with_unicode(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label='\u4f60\u597d')\n        ax.plot([1, 2, 3], label='\u4e16\u754c')\n        ax.legend()\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        line, = ax.plot([1, 2, 3], [1, 2, 3])\n        ax.legend([line], loc=loc)\n        fig.canvas.draw()\n        # check if legend is in the specified location\n        legend = ax.findobj(type(mlegend.Legend))[0]\n        bbox = legend.get_window_extent()\n        ax_bbox = ax.get_window_extent()\n\n        if loc.startswith('upper'):\n            assert bbox.y0 <= ax_bbox.y0\n        elif loc.startswith('lower'):\n            assert bbox.y1 >= ax_bbox.y1\n        if loc.endswith('right'):\n            assert bbox.x1 >= ax_bbox.x1\n        elif loc.endswith('left'):\n            assert bbox.x0 <= ax_bbox.x0\n\n", "def test_legend_tuple_handler():\n    # Test that legend handles are properly processed.\n    fig, ax = plt.subplots()\n    x = np.arange(10)\n    y1 = x**2\n    y2 = x**3\n\n    # Create a tuple of lines to be handled as a single legend entry\n    line1, = ax.plot(x, y1, label='Data 1')\n    line2, = ax.plot(x, y2, label='Data 2')\n    handles = (line1, line2)\n\n    # Test legend creation with a tuple of handles\n    legend = ax.legend(handles=handles)\n    assert len(legend.texts) == 1\n    assert legend.texts[0].get_text() == 'Data 1, Data 2'\n\n    # Test legend creation with a tuple of handles and labels\n    legend = ax.legend(handles=handles,\n                       labels=['Combined Data'])\n    assert len(legend.texts) == 1\n    assert legend.texts[0].get_text() == 'Combined Data'\n"], "sample_719": ["    def test_tfidf_vectorizer_lowercase_parameter(self):\n        corpus = ['This is THE title', 'another TITLE', 'tHis is NOT']\n        vectorizer = TfidfVectorizer(lowercase=True)\n        vectorizer.fit(corpus)\n\n        # Expected vocabulary since lowercase=True\n        expected_vocabulary = {'this': 0, 'is': 1, 'the': 2, 'title': 3,\n                               'another': 4, 'not': 5}\n\n        assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n\n        vectorizer = TfidfVectorizer(lowercase=False)\n        vectorizer.fit(corpus)\n\n        # Expected vocabulary since lowercase=False\n        expected_vocabulary = {'This': 0, 'is': 1, 'THE': 2, 'title': 3,\n                               'another': 4, 'TITLE': 5, 'tHis': 6, 'NOT': 7}\n        assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n", "def test_tfidf_vectorizer_vocabulary_deprecation():\n    vectorizer = TfidfVectorizer(vocabulary=['apple', 'banana', 'cherry'])\n    with warnings.catch_warnings(record=True) as w:\n        vectorizer.fit(['apple banana'], vocabulary=['apple', 'banana', 'cherry'])\n        assert len(w) == 1\n        assert \"The 'vocabulary' attribute is deprecated in favor of \" in str(w[0].message)\n\n\n", "    def test_tfidf_vectorizer_vocabulary_deprecation_warning(self):\n        vect = TfidfVectorizer(vocabulary=[\"pizza\", \"burger\", \"beer\"])\n        with warnings.catch_warnings(record=True) as w:\n            vect.transform([\"the pizza beer copyright\"])\n        assert_equal(len(w), 1)\n        assert_true(\n            issubclass(w[-1].category, FutureWarning) and\n            \"The `vocabulary` parameter\" in str(w[-1].message)\n        )\n", "    def test_stop_words_parameter_vocabulary(self):\n        corpus = [\n            'This is the first document.',\n            'This document is the second document.',\n            'And this is the third one.',\n            'Is this the first document?',\n        ]\n        vectorizer = TfidfVectorizer(stop_words='english')\n        vectorizer.fit(corpus)\n        vocabulary = vectorizer.vocabulary_\n        stop_words = ENGLISH_STOP_WORDS\n        for word in stop_words:\n            assert word not in vocabulary\n\n        vectorizer = TfidfVectorizer(stop_words=['is', 'the', 'this'])\n        vectorizer.fit(corpus)\n        vocabulary = vectorizer.vocabulary_\n        assert 'is' not in vocabulary\n        assert 'the' not in vocabulary\n        assert 'this' not in vocabulary\n\n\n", "def test_tfidf_vectorizer_max_features():\n    text = [\"This is a story about a cat.\",\n            \"This is a story about a dog.\",\n            \"This is a story about a bird.\"]\n\n    vectorizer = TfidfVectorizer(max_features=2)\n    X = vectorizer.fit_transform(text)\n\n    assert X.shape[1] == 2\n\n"], "sample_704": ["    def test_iterparentnodeids_empty(self, pytester: Pytester) -> None:\n        item = pytester.getnode(\"emptydir\")\n        ids = list(nodes.iterparentnodeids(item.nodeid))\n        assert ids == [\"\"]\n\n", "    def test_iterparentnodeids_empty(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\"\")\n        node = testdir.getnode(None)\n        assert list(nodes.iterparentnodeids(node.nodeid)) == [\"\"]\n", "    def test_iterparentnodeids_empty(self) -> None:\n        node = nodes.Node(name=\"test\")\n        self.assertEqual(list(nodes.iterparentnodeids(node.nodeid)), [])\n", "compilation error", "compilation error"], "sample_262": ["    def test_lazy_object_init(self):\n        class MyObject:\n                self.value = value\n        lazy_obj = lazy(MyObject, str)(5)\n        self.assertEqual(lazy_obj._wrapped, empty)\n        lazy_obj._setup()\n        self.assertEqual(lazy_obj._wrapped.value, 5)\n\n", "    def test_lazy_str(self):\n            return 'hello world'\n        lazy_str = lazystr(func)\n        self.assertIsInstance(lazy_str, str)\n        self.assertEqual(lazy_str, 'hello world')\n", "    def test_lazystr(self):\n        @lazystr\n            return 'Hello, world!'\n        self.assertEqual(myfunc, 'Hello, world!')\n", "    def test_lazy_str(self):\n        lazy_str = lazystr('hello')\n        self.assertIsInstance(lazy_str, str)\n        self.assertEqual(lazy_str, 'hello')\n", "    def test_lazy_str(self):\n        @lazystr\n            return str(value)\n\n        lazy_str_obj = my_lazy_str(\"hello\")\n        self.assertEqual(lazy_str_obj, \"hello\")\n        self.assertIsInstance(lazy_str_obj, str)\n"], "sample_253": ["    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'test.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module.py', 'def my_func(): pass')\n\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        zip_file = self.temporary_file('my_app.zip')\n        with zipfile.ZipFile(zip_file, 'w') as z:\n            z.writestr('my_app/__init__.py', '')\n\n        self.import_and_cleanup('my_app')\n        self.assertFileFound(zip_file)\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filename = Path(tmpdir) / 'my_module.zip'\n            with zipfile.ZipFile(filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n                zf.writestr('my_module/some_file.py', '')\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(filename)\n\n", "    def test_file_watching_with_zip_import(self):\n        with self.temporary_file('my_module.zip') as zip_path:\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('my_module/__init__.py', 'x = 1')\n            py_compile.compile(zip_path, dfile=zip_path, optimize=0)\n\n        with extend_sys_path(zip_path.parent):\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_path)\n", "    def test_zipfile_module(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zf:\n                zf.writestr('mymodule.py', 'x = 1')\n            sys.path.append(str(tmpdir))\n            self.import_and_cleanup('mymodule')\n            self.assertFileFound(zip_path)\n"], "sample_819": ["    def test_voting_classifier_predict_proba_weights(self):\n        clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial',\n                                  random_state=1)\n        clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n        clf3 = GaussianNB()\n        eclf1 = VotingClassifier(estimators=[\n            ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft',\n            weights=[1, 2, 1])\n        eclf1.fit(X, y)\n        probas = eclf1.predict_proba(X)\n        assert_array_almost_equal(probas.sum(axis=1), np.ones(len(X)))\n", "    def test_voting_classifier_single_estimator():\n        # Test case for a voting classifier with just a single estimator\n\n        estimator = LogisticRegression(solver='lbfgs',\n                                      multi_class='multinomial',\n                                      random_state=0)\n        eclf = VotingClassifier([('lr', estimator)])\n        eclf = eclf.fit(X, y)\n        assert_array_equal(eclf.predict(X), estimator.fit(X, y).predict(X))\n\n\n\n", "    def test_voting_classifier_empty_estimators(self):\n        with pytest.raises(AttributeError):\n            VotingClassifier(estimators=None).fit(X, y)\n\n        with pytest.raises(AttributeError):\n            VotingClassifier(estimators=[]).fit(X, y)\n", "def test_voting_classifier_predict_proba_empty():\n    \"\"\"Test predict_proba with empty input.\"\"\"\n    clf1 = LogisticRegression()\n    clf2 = RandomForestClassifier()\n    eclf = VotingClassifier([('lr', clf1), ('rf', clf2)], voting='soft')\n    eclf.fit(X, y)\n    assert_raise_message(ValueError,\n                         \"cannot compute probabilities for an empty input\",\n                         eclf.predict_proba, [])\n\n\n", "    def test_voting_regressor_empty_input():\n        # Test that VotingRegressor raises ValueError for empty estimator list\n        with pytest.raises(ValueError):\n            VotingRegressor([])\n\n\n\n"], "sample_22": ["def test_rotation_matrix_angle_unit():\n    \"\"\"Test rotation_matrix() with unit support.\"\"\"\n    # Check that specifying angle with unit works as expected\n    angle = 45.0 * u.deg\n    mat = rotation_matrix(angle, axis='x')\n    assert_allclose(mat, rotation_matrix(np.deg2rad(45.0), axis='x'))\n\n    with pytest.raises(u.UnitsError):\n        rotation_matrix(angle, axis='x', unit=u.m)\n", "    def test_rotation_matrix_quantity():\n        angle = 45 * u.degree\n        matrix = rotation_matrix(angle, 'z')\n        assert isinstance(matrix, np.ndarray)\n        assert matrix.shape == (3, 3)\n        assert_allclose(matrix @ matrix.T, np.identity(3), atol=1e-12)\n", "    def test_is_rotation_improper():\n        # Test cases\n        # Proper Rotation\n        rot_mat = rotation_matrix(np.pi / 2, axis=\"z\")  \n        assert is_rotation(rot_mat)\n\n        # Improper Rotation (reflection + rotation)\n        improper_rot_mat = np.array([[-1, 0, 0],\n                                   [0, 1, 0],\n                                   [0, 0, -1]])\n\n        assert is_rotation(improper_rot_mat, allow_improper=True)\n        assert not is_rotation(improper_rot_mat) \n", "    def test_rotation_matrix_axis_array():\n        \"\"\"Test the rotation matrix function with axis specified as an array.\"\"\"\n        axis = np.array([1, 0, 0])\n        angle = np.deg2rad(90)\n        expected = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])\n        actual = rotation_matrix(angle, axis=axis)\n        assert_allclose(actual, expected)\n", "    def test_is_rotation_improper():\n        # Test case for improper rotations (determinant -1)\n        R = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])\n\n        # Check if it's a rotation, including improper ones\n        assert is_rotation(R, allow_improper=True)\n\n        # Check if it's a proper rotation (should be False)\n        assert not is_rotation(R, allow_improper=False)\n"], "sample_640": ["    def test_is_builtin(name: str, expected: bool) -> None:\n        node = astroid.parse(f\"import {name}\")\n        assert utils.is_builtin(node.get_children()[0]) == expected\n", "    def test_is_builtin_name(self, name: str, expected: bool) -> None:\n        node = astroid.parse(f\"a = {name}\")\n        assert utils.is_builtin_name(node.body[0].right) == expected\n", "    def test_is_builtin_name(self, name: str, expected: bool) -> None:\n        node = astroid.Name(name, lineno=1, col_offset=1)\n        assert utils.is_builtin_name(node) is expected\n", "    def test_is_builtin(self, name, expected):\n        node = astroid.parse(f\"import {name}\\n{name}.something\").body[0].names[0]\n        assert utils.is_builtin(node) == expected\n", "    def test_is_builtin(self, name, expected):\n        self.assertEqual(utils.is_builtin(name), expected)\n"], "sample_801": ["    def test_logisticregressioncv_repr(self):\n        lr = LogisticRegressionCV(penalty='l1')\n        expected = (\n            \"LogisticRegressionCV(Cs=10, class_weight=None, cv='warn', dual=False,\"\n            \" fit_intercept=True, intercept_scaling=1, l1_ratios=np.array([0.1,\"\n            \" 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), max_iter=100,\"\n            \" multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\"\n            \" refit=True, scoring=None, solver='warn', tol=0.0001, verbose=0)\"\n        )\n        assert repr(lr)[:len(expected)] == expected\n", "    def test_estimator_pretty_printer_pipeline(self):\n        pipe = make_pipeline(\n            StandardScaler(), SelectKBest(chi2, k=2),\n            LogisticRegressionCV()\n        )\n        pp = _EstimatorPrettyPrinter()\n        repr_ = pp.pformat(pipe)\n        assert len(repr_.splitlines()) > 1\n        assert 'StandardScaler()' in repr_\n        assert 'SelectKBest' in repr_\n        assert 'LogisticRegressionCV' in repr_\n\n", "def test_estimator_repr_large_n_params():\n    n_features = 100\n    reg = make_pipeline(\n        SelectKBest(chi2, k=10),\n        LogisticRegressionCV(\n            penalty='l2', solver='lbfgs', max_iter=500,\n            random_state=0, cv=5,\n            Cs=np.linspace(0.1, 10, 20),\n            class_weight='balanced'\n        )\n    )\n    # This should not raise an error\n    repr(reg)\n", "    def test_estimator_repr_long_line(self):\n        # Test repr for estimators with long lines\n        # This ensures the repr doesn't overflow the console\n        very_long_name = 'this_is_a_very_long_parameter_name'\n        estimator = LogisticRegression(\n            penalty=very_long_name,                               \n            dual=False, tol=1e-4, C=1.0, fit_intercept=True,\n            intercept_scaling=1, class_weight=None, random_state=None, \n            solver='warn', max_iter=100, multi_class='warn', verbose=0,\n            warm_start=False, n_jobs=None, l1_ratio=None)\n        repr_str = repr(estimator)\n        self.assertTrue('penalty=' in repr_str)\n        self.assertTrue(very_long_name in repr_str)\n\n        # Test with nested estimators\n        pipeline = make_pipeline(\n            StandardScaler(copy=True, with_mean=True, with_std=True),\n            SelectKBest(chi2, k=10),\n            LogisticRegression(\n                penalty=very_long_name,                               \n                dual=False, tol=1e-4, C=1.0, fit_intercept=True,\n                intercept_scaling=1, class_weight=None, random_state=None, \n                solver='warn', max_iter=100, multi_class='warn', verbose=0,\n                warm_start=False, n_jobs=None, l1_ratio=None)\n        )\n        repr_str = pipeline.set_params()._repr_html_()\n        self.assertTrue('penalty=' in repr_str)\n        self.assertTrue(very_long_name in repr_str)\n\n", "    def test_pprint_pipeline(self):\n        # Test pprint with Pipeline\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        expected = (\n            'Pipeline(memory=None, steps=[('\n            '\\'standardscaler\\', StandardScaler(copy=True, with_mean=True, '\n            'with_std=True)), (\\'logisticregression\\', LogisticRegression('\n            'C=1.0, class_weight=None, dual=False, fit_intercept=True, '\n            'intercept_scaling=1, l1_ratio=None, max_iter=100, '\n            'multi_class=\\'warn\\', n_jobs=None, penalty=\\'l2\\', '\n            'random_state=None, solver=\\'warn\\', tol=1e-04, verbose=0, '\n            'warm_start=False))])'\n        )\n        actual = repr(pipe)\n        assert re.match(expected, actual)\n\n"], "sample_1187": ["def test_integration_reduction_dynamic():\n    p1 = Polygon(Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1))\n    p2 = Polygon(Point(0, 0), Point(1, 0), Point(1, 2), Point(0, 2))\n    expr = x**2 + y**2\n    result = integration_reduction_dynamic(p1, p2, expr)\n    assert result == S(10)/3\n\n\n\n", "def test_integration_reduction_dynamic_3D():\n    cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n             (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n            [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n            [3, 1, 0, 2], [0, 4, 6, 2]]\n    expr = x**2 + y**2 + z**2\n    integrals = integration_reduction_dynamic(expr, cube)\n    assert integrals == [S(1250/3), S(1250/3), S(1250/3), S(1250/3),\n                         S(1250/3), S(1250/3)]\n\n", "def test_integration_reduction_dynamic():\n    p = Polygon(Point(0, 0), Point(1, 0), Point(1, 1), Point(0, 1))\n    expr = x**2 * y + x * y**2\n\n    result = integration_reduction_dynamic(p, expr)\n    assert result == 1/6 * (x**3 * y + x * y**3) + S.One\n\n\n\n", "def test_integration_reduction_dynamic():\n    p = Polygon(Point(1,0), Point(2, 0), Point(2, 2), Point(1,2))\n    expr = x**2 + y\n    integral = integration_reduction_dynamic(p, expr)\n\n    assert integral == 14/3\n\n\n\n", "    def test_integration_reduction_dynamic_3D():\n        cube = [[(0, 0, 0), (0, 0, 5), (0, 5, 0), (0, 5, 5), (5, 0, 0),\n                 (5, 0, 5), (5, 5, 0), (5, 5, 5)],\n                [2, 6, 7, 3], [3, 7, 5, 1], [7, 6, 4, 5], [1, 5, 4, 0],\n                [3, 1, 0, 2], [0, 4, 6, 2]]\n        facets = cube[1:]\n        vertices = cube[0]\n        expr = x**2 + y**2 + z**2\n        result = integration_reduction_dynamic(facets, vertices, expr,\n                                                3, True)\n        assert result == 250/3\n"], "sample_1058": ["    def test_print_MatrixSymbol_pythoncode(self):\n        A = MatrixSymbol('A', 2, 2)\n        assert pycode(A, standard='python3') == 'A'\n\n", "def test_print_Mul():\n    assert PythonCodePrinter().doprint(x*y) == 'x*y'\n    assert PythonCodePrinter().doprint(Rational(1, 2)*x) == 'x/2'\n    assert PythonCodePrinter().doprint(-x*y) == '-x*y'\n    assert PythonCodePrinter().doprint(2*x*y) == '2*x*y'\n\n", "    def test_print_Piecewise_default(self):\n        p = Piecewise((x, x > 0), (0, True))\n        expected = (\n            'numpy.select([(x > 0)], [x], default=0)'\n        )\n        self.assertEqual(NumPyPrinter().doprint(p), expected)\n", "    def test_Piecewise_numpy(self):\n        # Test Piecewise with NumPyPrinter\n        p = Piecewise((x, x < 0), (x**2, x >= 0))\n        code = NumPyPrinter().doprint(p)\n        expected = 'numpy.select([(x < 0),], [x], default=x**(2))'\n        assert code == expected\n", "    def test_print_SparseMatrix(self):\n        m = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        self.assertEqual(\n            NumPyPrinter().doprint(m),\n            'scipy.sparse.coo_matrix([(0, 0, 1), (1, 1, 2), (2, 2, 3)], shape=(3, 3))'\n        )\n\n"], "sample_152": ["    def test_cascade_delete_with_transaction(self):\n        DeleteTop.objects.create()\n        DeleteBottom.objects.create(delete_top=DeleteTop.objects.get())\n        with transaction.atomic():\n            DeleteTop.objects.all().delete()\n        self.assertEqual(DeleteTop.objects.count(), 0)\n        self.assertEqual(DeleteBottom.objects.count(), 0)\n", "    def test_can_fast_delete_with_generic_relations(self):\n        \"Check if can_fast_delete function correctly identifies fast-deletable objects with generic relations\"\n        b1 = B1.objects.create()\n        b2 = B2.objects.create()\n        GenericB1.objects.create(content_object=b1)\n        GenericB2.objects.create(content_object=b2)\n\n        collector = Collector(self.using)\n        self.assertTrue(collector.can_fast_delete([b1, b2]))\n\n\n\n", "    def test_fast_delete_with_m2m_fields(self):\n        # Create some objects\n        a1 = A.objects.create()\n        a2 = A.objects.create()\n        b1 = B.objects.create(a=a1)\n        b2 = B.objects.create(a=a2)\n\n        # Create a m2m relationship\n        b1.m2m_related.add(a1, a2)\n        b2.m2m_related.add(a1)\n\n        # Delete objects using the Collector\n        collector = Collector(using='default')\n        collector.collect([a1])\n", "    def test_cascade_delete_with_restrict_on_related_model(self):\n        \"\"\"\n        Test cascading deletes when a model has a FK to a restricted model and\n        the related model's on_delete behavior is RESTRICT.\n        \"\"\"\n        r1 = R.objects.create(name='R1')\n        rchild1 = RChild.objects.create(r=r1, name='RChild1')\n        rchild2 = RChild.objects.create(r=r1, name='RChild2')\n        rchildchild1 = RChildChild.objects.create(rchild=rchild1, name='RChildChild1')\n        rchildchild2 = RChildChild.objects.create(rchild=rchild2, name='RChildChild2')\n\n        # Create a restricted model (M2MFrom) with a relation to R\n        m2mfrom = M2MFrom.objects.create()\n        m2mfrom.r.add(r1)\n\n        # Ensure that deleting the restricted object (M2MFrom) raises\n        # a RestrictedError\n\n        with self.assertRaises(RestrictedError) as cm:\n            Collector(using='default').collect([m2mfrom])\n        self.assertEqual(\n            cm.exception.restricted_objects,\n            {\n                \"'M2MFrom.r'\": {r1},\n            },\n        )\n", "    def test_delete_cascade_with_restricted_objects(self):\n        # Test that cascading deletion raises a RestrictedError when there are\n        # restricted foreign key constraints.\n        mr = MR.objects.create()\n        p = P.objects.create()\n        p.m.add(mr)\n        b1 = B1.objects.create(parent=p)\n\n        with transaction.atomic():\n            with self.assertRaises(RestrictedError):\n                p.delete()\n\n        # Ensure that the RestrictedError contains the correct information.\n        self.assertIsInstance(p.related_objects(), list)\n\n\n\n"], "sample_75": ["    def test_prefetch_related_field_name(self):\n        with CaptureQueriesContext(connection) as queries:\n            readers = Reader.objects.prefetch_related('books_read__authors').all()\n            self.assertEqual(len(queries.captured_queries), 3)\n", "    def test_prefetch_related_with_reverse_m2m(self):\n        with CaptureQueriesContext(connection) as queries:\n            prefetch = Prefetch('books_read__authors')\n            readers = Reader.objects.prefetch_related(prefetch).all()\n            self.assertEqual(len(queries.recorded), 3)\n\n            for reader in readers:\n                for book in reader.books_read.all():\n                    self.assertEqual(len(reader.books_read__authors), book.authors.count())\n\n\n\n", "    def test_prefetch_related_with_multiple_objects_and_distinct(self):\n        with CaptureQueriesContext(connection) as queries:\n            prefetch = Prefetch('books_read__authors', to_attr='authors_from_books')\n            readers = Reader.objects.prefetch_related('books_read', prefetch).all()\n            self.assertEqual(len(readers), 2)\n            for reader in readers:\n                self.assertEqual(len(reader.authors_from_books), 2)\n        # Ensure we only executed a query for books once\n        self.assertEqual(len(queries.captured_queries), 3)\n\n", "    def test_prefetch_related_m2m_reverse_order_by(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            authors = Author.objects.prefetch_related(\n                Prefetch('books', queryset=Book.objects.order_by('-title'))\n            ).all()\n        for author in authors:\n            # Ensure book ordering respects the Prefetch ordering.\n            self.assertEqual(author.books.order_by('-title'), author.books.all())\n\n        self.assertGreater(len(captured_queries), 1)\n\n\n", "    def test_prefetch_related_reverse_m2m_with_complex_queryset_filter(self):\n        # Test prefetching a reverse M2M field when the initial queryset\n        # uses a complex filter with joins. This ensures that the prefetch\n        # query doesn't get confused by the joins in the initial query.\n        queryset = Book.objects.filter(authors__name__startswith='A').annotate(\n            author_count=Count('authors')\n        )\n        with CaptureQueriesContext(connection) as queries:\n            prefetch_related_objects(queryset, Prefetch('authors'))\n        self.assertEqual(len(queries), 2)\n        # The first query should be the initial query with the join and\n        # annotation.\n        self.assertWhereContains(queries[0]['sql'], \"authors__name LIKE %s\")\n\n        # The second query should be the prefetch query, which should not\n        # contain the join or annotation from the initial query.\n\n"], "sample_222": ["    def test_file_locking(self):\n        with tempfile.TemporaryFile('w+b') as f:\n            f.write(b'some data')\n            f.seek(0)\n\n            with locks.lock(f, locks.LOCK_EX):\n                self.assertTrue(locks.lock(f, locks.LOCK_SH))\n                self.assertFalse(locks.lock(f, locks.LOCK_EX))\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n", "        def test_lock_unlock_named_temporary_file(self):\n            with NamedTemporaryFile(delete=False) as f:\n                filename = f.name\n            try:\n                with open(filename, 'r+b') as f:\n                    self.assertTrue(locks.lock(f, locks.LOCK_EX))\n                    self.assertTrue(locks.unlock(f))\n            finally:\n                os.unlink(filename)\n", "    def test_lock_unlock_file(self):\n        f = NamedTemporaryFile(delete=False)\n        f.close()\n        try:\n            with open(f.name, 'w') as opened_file:\n                self.assertTrue(locks.lock(opened_file, locks.LOCK_EX))\n                self.assertTrue(locks.unlock(opened_file))\n        finally:\n            os.unlink(f.name)\n", "    def test_lock_unlock(self):\n        f = NamedTemporaryFile(delete=False)\n        try:\n            with open(f.name, 'w') as f:\n                f.write('test')\n            with open(f.name, 'r+') as f:\n                self.assertTrue(locks.lock(f, locks.LOCK_EX))\n                self.assertTrue(locks.unlock(f))\n        finally:\n            os.unlink(f.name)\n", "    def test_lock_unlock_file(self):\n        with NamedTemporaryFile() as f:\n            f.write(b'test')\n            f.seek(0)\n            self.assertTrue(locks.lock(f, locks.LOCK_EX))\n            self.assertTrue(locks.unlock(f))\n"], "sample_783": ["def test_missing_indicator_features_all():\n    rng = np.random.RandomState(42)\n    X = rng.randn(5, 3)\n    X[0, 1] = np.nan\n    X[1, 2] = np.nan\n\n    indicator = MissingIndicator(features='all', sparse=False)\n    indicator.fit(X)\n\n    Xt = indicator.transform(X)\n    assert_array_equal(Xt.shape, (5, 3))\n    assert_allclose(Xt, [[False, True, False],\n                         [False, False, True],\n                         [False, False, False],\n                         [False, False, False],\n                         [False, False, False]])\n", "    def test_missing_indicator_all_features(self):\n        X_dense = np.array([[1, np.nan, 2],\n                            [np.nan, 3.0, 4],\n                            [5, 6, np.nan]])\n\n        missing_indicator = MissingIndicator(features='all', sparse=False)\n        Xt_dense = missing_indicator.fit_transform(X_dense)\n        assert_array_equal(Xt_dense,\n                           np.array([[False, True, False],\n                                     [True, False, False],\n                                     [False, False, True]]))\n\n        missing_indicator = MissingIndicator(features='all', sparse=True)\n        Xt_sparse = missing_indicator.fit_transform(X_dense)\n        assert_allclose_dense_sparse(Xt_dense, Xt_sparse)\n\n        X_sparse = sparse.csr_matrix(X_dense)\n        missing_indicator = MissingIndicator(features='all', sparse='auto')\n        Xt_auto = missing_indicator.fit_transform(X_sparse)\n        assert_allclose_dense_sparse(Xt_dense, Xt_auto)\n\n", "    def test_MissingIndicator_error_on_new(self):\n        X1 = np.array([[np.nan, 1, 3],\n                       [4, 0, np.nan],\n                       [8, 1, 0]])\n        X2 = np.array([[5, 1, np.nan],\n                       [np.nan, 2, 3],\n                       [2, 4, 0]])\n\n        indicator = MissingIndicator(error_on_new=True,\n                                     features='missing-only')\n        indicator.fit(X1)\n        with pytest.raises(ValueError):\n            indicator.transform(X2)\n\n\n\n", "def test_simple_imputer_constant_sparse():\n    \"\"\"Test SimpleImputer with constant strategy on sparse matrices.\"\"\"\n    X = sparse.csc_matrix([[np.nan, 1, 3],\n                           [4, 0, np.nan],\n                           [8, 1, 0]])\n    X_true = np.array([[0, 1, 3],\n                       [4, 0, 0],\n                       [8, 1, 0]])\n    impute = SimpleImputer(missing_values=np.nan, strategy=\"constant\",\n                           fill_value=0)\n\n    impute.fit(X)\n\n    assert_allclose_dense_sparse(impute.transform(X), X_true)\n\n", "    def test_missing_indicator_sparse_zero(self):\n        X = sparse.csr_matrix([[1, 0, 2],\n                               [0, 2, 0],\n                               [3, 0, 4]])\n        indicator = MissingIndicator(missing_values=0, features='all',\n                                     sparse='auto')\n        Xt = indicator.fit_transform(X)\n        assert_array_equal(Xt.toarray(),\n                           np.array([[False, True, False],\n                                     [True, False, True],\n                                     [False, True, False]]))\n"], "sample_897": ["    def test_pdp_categorical_features(self, clf_diabetes, diabetes):\n        feature_names = [\"sex\"]\n        transformer = make_column_transformer(\n            (OneHotEncoder(), [\"sex\"]),\n        )\n        X = transformer.fit_transform(diabetes.data)\n        clf = make_pipeline(transformer, clf_diabetes)\n\n        disp = PartialDependenceDisplay.from_estimator(\n            clf,\n            X,\n            features=feature_names,\n            kind=\"average\",\n            grid_resolution=self.grid_resolution,\n        )\n\n        assert disp.bounding_ax_ is None\n        assert disp.axes_.size == 1\n        assert_allclose(disp.lines_[0, 0].get_ydata(), [0.0], atol=1e-2)\n\n", "def test_partial_dependence_categorical_features(clf_diabetes, diabetes):\n    # test that PDP works with categorical features\n\n    X = np.array(diabetes.data, dtype=object)\n    X[:, 2] = X[:, 2].astype('category')\n    column_transformer = make_column_transformer(\n        (OneHotEncoder(handle_unknown='ignore'), [2]),\n        remainder='passthrough')\n    clf = make_pipeline(\n        column_transformer,\n        clf_diabetes)\n    clf.fit(X, diabetes.target)\n\n    features = [2]\n    display = PartialDependenceDisplay.from_estimator(clf, X, features)\n    assert display.figure_ is not None\n\n\n", "def test_plot_two_way_categorical(clf_diabetes, diabetes):\n    encoded_data = OneHotEncoder(handle_unknown='ignore').fit_transform(diabetes.data).toarray()\n    clf = GradientBoostingRegressor(n_estimators=10, random_state=1)\n    clf.fit(encoded_data, diabetes.target)\n    pdp_display = PartialDependenceDisplay.from_estimator(clf, encoded_data, [0, 1], kind='average',\n                                                        grid_resolution=10, \n                                                        is_categorical=[True, True])\n    assert pdp_display.axes_.shape == (1, 1)\n    assert pdp_display.contours_[0, 0] is None\n", "    def test_partial_dependence_categorical(self, clf_diabetes, diabetes):\n        features = [0, 2]\n        pdp_display = PartialDependenceDisplay.from_estimator(\n            clf_diabetes,\n            diabetes.data[:, features],\n            features,\n            kind=\"average\",\n            grid_resolution=grid_resolution,\n        )\n        assert pdp_display.figure_ is not None\n        assert len(pdp_display.axes_) == len(features)\n\n", "    def test_pdp_categorical_feature_multiclass(self, iris):\n        # Make sure PDP works with categorical features and multiclass problems\n        X = iris.data\n        y = iris.target\n        clf = GradientBoostingClassifier(n_estimators=10, random_state=1)\n        clf.fit(X, y)\n\n        transformer_1 = make_pipeline(\n            OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n        )\n\n        transformer_2 = make_pipeline(\n            # No transformation needed for numerical features\n        )\n        preprocessor = make_column_transformer(\n            (transformer_1, [0]),\n            (transformer_2, [1, 2, 3]),\n        )\n\n        clf = make_pipeline(preprocessor, clf)\n        clf.fit(X, y)\n        features = [0, 1]\n        display = PartialDependenceDisplay.from_estimator(\n            clf, X, features, kind=\"both\", grid_resolution=grid_resolution\n        )\n        assert display.figure_ is not None\n"], "sample_14": ["    def test_longitude_wrap_angle():\n        lon = Longitude(365 * u.deg)\n        assert lon.wrap_angle == 360 * u.deg\n        assert_allclose(lon.to_value(u.deg), 5 * u.deg)\n\t\t\n        lon = Longitude(185 * u.deg, wrap_angle=180 * u.deg)\n        assert lon.wrap_angle == 180 * u.deg\n        assert_allclose(lon.to_value(u.deg), -5 * u.deg)\n", "    def test_angle_pickling():\n        a = Angle('1h2m3s')\n        b = pickle.loads(pickle.dumps(a))\n        assert_allclose(a.deg, b.deg)\n\n", "    def test_longitude_wrap_angle():\n        lon = Longitude(300 * u.deg)\n        assert lon.wrap_angle == 360 * u.deg\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == 180 * u.deg\n        assert_allclose(lon.deg, -60)\n\n        # Test wrapping around 180 deg wrap_angle.\n        lon = Longitude([300, 350, 400, -10] * u.deg, wrap_angle=180 * u.deg)\n        assert_allclose(lon.deg, [-60, -10, 20, -10])\n\n        # Test assignment (which should wrap)\n        lon[0] = 370 * u.deg\n        assert_allclose(lon.deg, [-10, -10, 20, -10])\n\n", "def test_longitude_wrap_angle():\n    lon = Longitude('180d', wrap_angle='180d')\n    assert_allclose(lon.wrap_angle.value, 180.)\n\n    lon = Longitude([-190, 170, 350] * u.deg, wrap_angle='90d')\n    assert_allclose(lon.wrap_angle.value, 90.)\n    assert_allclose(lon.degree, [170, 170, 10.0])\n\n    lon = Longitude([-190, 170, 350] * u.deg, wrap_angle='180d')\n    assert_allclose(lon.wrap_angle.value, 180.)\n    assert_allclose(lon.degree, [-170, 170, -10.0])\n", "    def test_wrap_at_inplace_wrap_angle_changed(self):\n        a = Angle([10, 200, 350] * u.deg)\n        a.wrap_at(180 * u.deg, inplace=True)\n        a.wrap_angle = 360 * u.deg\n        assert_allclose(a.degree, [10, -160, -10])\n"], "sample_6": ["    def test_longitude_wrap_angle():\n        lon = Longitude(np.array([30, 150, 390]), unit=u.deg)\n        assert_allclose(lon.wrap_angle, Angle(360 * u.deg))\n\n        lon.wrap_angle = Angle(180 * u.deg)\n        assert_allclose(lon, Angle([-150, 150, -180], unit=u.deg))\n\n        lon = Longitude([30, 150, 390], unit=u.deg, wrap_angle=Angle(180 * u.deg))\n        assert_allclose(lon, Angle([30, 150, -180], unit=u.deg))\n\n", "def test_angle_representation_subclassing():\n    class MyAngle(Angle):\n        pass\n    a = MyAngle(10.0*u.deg)\n    assert isinstance(a, Angle)\n    assert isinstance(a, MyAngle)\n", "    def test_latitude_wrap_angle(self):\n        lat = Latitude('45d')\n        with pytest.raises(ValueError):\n            lat.wrap_angle = 180 * u.deg\n\n", "    def test_longitude_wrap_angle_setter():\n        lon = Longitude('10h')\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == 180 * u.deg\n        # Should wrap to -170d\n        assert lon.degree == -170.0\n\n        lon = Longitude('350d')\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == 180 * u.deg\n        # Should wrap to 10d\n        assert lon.degree == 10.0\n", "    def test_wrap_at_inplace_with_negative_wrap_angle(self):\n       # test that in-place wrapping works correctly for negative wrap angles\n       a = Angle([-170, 10, 350] * u.deg)\n       a.wrap_at(-180 * u.deg, inplace=True)\n       assert_allclose(a.degree, [10, 10, -10])\n"], "sample_675": ["    def test_caplog_records_with_autoindent_fixed(self, testdir: Testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.basicConfig(\n                    level=logging.DEBUG,\n                    format=\"%(levelname)s: %(message)s\",\n                    auto_indent=10\n                )\n                logging.debug(\"first line\")\n                logging.debug(\"second line\")\n\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n        log = result.stdout.str()\n        assert re.findall(r\"DEBUG: first line\", log)\n        assert re.findall(r\"DEBUG: second line\", log)\n        assert log.count(\"DEBUG:\") == 2\n\n", "    def test_log_handler_reset(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.info(\"first message\")\n                caplog = pytest.caplog\n                caplog.clear()\n                logging.info(\"second message\")\n                assert len(caplog.records) == 1\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_set_log_path(self, testdir: Testdir) -> None:\n        config = testdir.parseconfig()\n        plugin = LoggingPlugin(config)\n        log_file = os.path.join(testdir.tmpdir, \"test.log\")\n        plugin.set_log_path(log_file)\n        assert plugin.log_file_handler.stream.name == log_file\n\n", "    def test_log_level_setting(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n                logging.info(\"this is a log message\")\n\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--log-level=WARNING\")\n        assert result.ret == 0\n        result.stdout.fnmatch_lines(\"*this is a log message*\")\n", "    def test_log_level_setting(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n            @pytest.mark.parametrize(\"log_level\", [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"])\n                logging.getLogger().setLevel(log_level)\n                logging.root.debug(\"debug message\")\n                logging.root.info(\"info message\")\n                assert \"debug message\" in caplog.text if log_level in [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] else \"debug message\" not in caplog.text\n                assert \"info message\" in caplog.text if log_level in [\"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] else \"info message\" not in caplog.text\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n"], "sample_92": ["    def test_get_all_permissions(self):\n        with self.settings(PASSWORD_HASHERS=[\n            {'NAME': 'django.contrib.auth.hashers.MD5PasswordHasher',}\n        ]):\n            self.assertEqual(\n                self.backend.get_all_permissions(self.user),\n                {'user_perm', 'group_perm'}\n            )\n", "    def test_user_can_authenticate(self):\n        backend = SimpleBackend()\n        self.assertTrue(backend.user_can_authenticate(self.user))\n", "    def test_get_all_permissions_empty(self):\n        with self.settings(AUTH_BACKENDS=[]):\n            request = HttpRequest()\n            request.session = {}\n            with mock.patch('django.contrib.auth.backends.UserModel') as mock_user_model:\n                mock_user_model.objects.get.return_value = self.user\n                permissions = ModelBackend().get_all_permissions(self.user, None)\n                self.assertEqual(permissions, set())\n", "    def test_has_perm_with_none(self):\n        self.assertFalse(SimpleBackend().has_perm(None, 'perm'))\n", "    def test_base_backend_methods(self):\n        self.assertEqual(SimpleBackend().authenticate(None, username='test', password='test'), None)\n        self.assertEqual(SimpleBackend().get_user(self.user.pk), None)\n\n        perms = SimpleBackend().get_user_permissions(self.user)\n        self.assertEqual(perms, {'user_perm'})\n        perms = SimpleBackend().get_group_permissions(self.user)\n        self.assertEqual(perms, {'group_perm'})\n\n        all_perms = SimpleBackend().get_all_permissions(self.user)\n        self.assertEqual(all_perms, {'user_perm', 'group_perm'})\n"], "sample_767": ["    def test_column_transformer_sparse_output(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        ct = ColumnTransformer(\n            transformers=[\n                ('num', StandardScaler(), [0]),\n                ('cat', OneHotEncoder(handle_unknown='ignore'), [1]),\n            ],\n            remainder='drop', sparse_threshold=0.3\n        )\n\n        X_transformed = ct.fit_transform(X)\n\n        assert isinstance(X_transformed, sparse.csr_matrix)\n        assert X_transformed.shape == (3, 3)\n\n\n\n", "    def test_column_transformer_sparse_output(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        ct = ColumnTransformer(\n            transformers=[\n                ('num', StandardScaler(), [0]),\n                ('txt', OneHotEncoder(), [1])\n            ],\n            remainder='drop', sparse_threshold=0.5\n        )\n        X_t = ct.fit_transform(X)\n        assert sparse.issparse(X_t)\n        assert_equal(X_t.shape, (3, 3))\n\n", "    def test_column_transformer_sparse_output(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        X_sparse = sparse.csr_matrix(X)\n        transformers = [\n            ('scaler', StandardScaler(), [0]),\n            ('onehot', OneHotEncoder(sparse=True), [1]),\n        ]\n        ct = ColumnTransformer(transformers, sparse_threshold=0.5)\n        ct.fit(X)\n        Xt = ct.transform(X_sparse)\n        assert_isinstance(Xt, sparse.csr_matrix)\n\n        ct = ColumnTransformer(transformers, sparse_threshold=0)\n        ct.fit(X)\n        Xt = ct.transform(X_sparse)\n        assert_isinstance(Xt, np.ndarray)\n", "def test_column_transformer_sparse_output():\n    # Test sparse_output_ attribute and its behavior\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    y = np.array([1, 2, 3])\n\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('onehot', OneHotEncoder(), [1])\n    ], remainder='passthrough', sparse_threshold=0.5)\n\n    ct.fit(X, y)\n    X_t = ct.transform(X)\n    assert ct.sparse_output_ is True\n\n    ct = ColumnTransformer([\n        ('scaler', StandardScaler(), [0]),\n        ('onehot', OneHotEncoder(), [1])\n    ], remainder='passthrough', sparse_threshold=0)\n\n    ct.fit(X, y)\n    X_t = ct.transform(X)\n    assert ct.sparse_output_ is False\n\n    ct = ColumnTransformer([\n        ('trans', Trans(), [0, 1]),\n    ])\n    ct.fit(X, y)\n    X_t = ct.transform(X)\n    assert ct.sparse_output_ is False\n", "    def test_sparse_output(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n        Xt = ct.fit_transform(X)\n        assert sparse.issparse(Xt)\n        assert_array_equal(Xt.toarray(), X)\n\n        # Test with sparse input\n        X_sparse = sparse.csr_matrix(X)\n        ct = ColumnTransformer([('trans', Trans(), [0, 1])])\n        Xt = ct.fit_transform(X_sparse)\n        assert sparse.issparse(Xt)\n        assert_array_equal(Xt.toarray(), X)\n\n        # Test with sparse output\n        X = np.array([[1, 2], [0, 0], [5, 6]])\n        ct = ColumnTransformer([('trans', Trans(), [0, 1])],\n                               sparse_threshold=0.1)\n        Xt = ct.fit_transform(X)\n        assert sparse.issparse(Xt)\n        \n\n"], "sample_679": ["    def test_mark_evaluates_condition_with_globals(self, tmpdir):\n        tmpfile = tmpdir.join(\"test_module.py\")\n        tmpfile.write(\"def test_func(): pass\")\n\n        mod_name = tmpfile.basename\n        \n            return \"hello\"\n\n        \n        with mock.patch('builtins.__dict__', {'custom_func': custom_func}):\n            col = Collector.from_name(f\"{mod_name}::test_func\")\n            mark = Mark(\n                \"mymark\", \n                {\"condition\": \"custom_func() == 'hello'\"}\n            )\n            me = MarkEvaluator(col.nodeid, mark)\n\n            assert me.istrue()\n", "    def test_mark_evaluator_invalidraise_with_no_raises(self, monkeypatch):\n        # Test that invalidraise returns None when there are no raises markers.\n        item = Node(None)\n        mark_evaluator = MarkEvaluator(item, \"raises\")\n        monkeypatch.setattr(mark_evaluator, \"get\", lambda self, attr: None)\n        assert mark_evaluator.invalidraise(Exception) is None\n", "    def test_mark_invalid_condition(self, request, monkeypatch) -> None:\n        monkeypatch.setattr(sys, 'argv', ['pytest', '-v', '-s', 'test_mark.py', '-k', 'test_mark_invalid_condition'])\n        item = Collector(\n            name=\"test_mark_invalid_condition\",\n            parent=Node(name=\"test_mark.py\"),\n            fspath=\"test_mark.py\",\n        )\n        mark = Mark(\"condition\", args=(\n            'os.path.exists(\"this_file_does_not_exist.txt\")',))\n        item._markers.add(mark)\n\n        mark_evaluator = MarkEvaluator(item, \"condition\")\n        mark_evaluator.istrue()\n\n        assert not mark_evaluator.result\n", "    def test_mark_evaluator_get_with_no_mark(self, mocker: mocker.MockFixture):\n        item = mocker.create_autospec(Node)\n        me = MarkEvaluator(item, \"mark\")\n        assert me.get(\"attr\") is None\n", "    def test_mark_condition_eval_false(self, monkeypatch):\n        monkeypatch.setattr(sys, \"argv\", [\"pytest\", \"test_module.py\"])\n        collector = Collector(None, name=\"test_module\", config=pytest.Config())\n        item = Node(parent=collector, name=\"test_function\")\n        item.config = pytest.Config()\n\n        mark = Mark(\"condition\", args=(\"1 == 2\",))\n        item._add_marker(mark)\n\n        mark_evaluator = MarkEvaluator(item, \"condition\")\n        assert not mark_evaluator.istrue()\n        assert mark_evaluator.getexplanation() == \"condition: 1 == 2\"\n\n"], "sample_251": ["    def test_window_function_filtering(self):\n        qs = Book.objects.annotate(\n            rank=Window(\n                F('rating'),\n                partition_by=F('publisher'),\n                order_by=F('rating').desc(),\n            ).row_number()\n        ).filter(rank=1)\n        self.assertQuerysetEqual(qs, [\n            'The Definitive Guide to Django: Web Development Done Right',\n        ], transform=lambda b: b.name)\n", "    def test_when_with_nulls(self):\n        qs = Book.objects.annotate(\n            price_category=Case(\n                When(price__gt=Decimal('30'), then=Value('expensive')),\n                When(price__isnull=True, then=Value('unknown')),\n                default=Value('affordable'),\n                output_field=CharField()\n            )\n        ).filter(price_category='expensive')\n        self.assertQuerysetEqual(qs, [\n            'The Definitive Guide to Django: Web Development Done Right', 'Artificial Intelligence: A Modern Approach',\n        ], lambda b: b.name)\n\n\n\n", "    def test_coalesce(self):\n        from django.db.models.functions import Coalesce\n        # Test coalesce on different types\n        self.assertQuerysetEqual(\n            Book.objects.annotate(\n                price_or_zero=Coalesce('price', Value(0, output_field=IntegerField()))\n            ).order_by('price_or_zero'), [\n                'b1', 'b2', 'b3', 'b4', 'b5', 'b6'\n            ], transform=lambda obj: obj.isbn\n        )\n\n        self.assertQuerysetEqual(\n            Book.objects.annotate(\n                name_or_isbn=Coalesce('name', F('isbn'))\n            ).order_by('name_or_isbn'), [\n                'b1', 'b2', 'b3', 'b4', 'b5', 'b6'\n            ], transform=lambda obj: obj.name_or_isbn\n        )\n", "    def test_row_number_over_partition_by(self):\n        qs = (\n            Author.objects.annotate(rn=Window(\n                F('id'),\n                partition_by=[F('age')]\n            ).row_number())\n        )\n        # Check that only authors with the same age get grouped together\n        self.assertEqual(\n            qs.order_by('age', 'rn').values_list('name', 'age', 'rn'),\n            [\n                ('James Bennett', 29, 1),\n                ('Paul Bissex', 29, 2),\n                ('Wesley J. Chun', 25, 1),\n                ('Adrian Holovaty', 34, 1),\n                ('Jacob Kaplan-Moss', 35, 1),\n                ('Jeffrey Forcier', 37, 1),\n                ('Brad Dayley', 45, 1),\n                ('Stuart Russell', 46, 1),\n                ('Peter Norvig', 57, 1),\n            ]\n        )\n\n", "    def test_coalesce(self):\n        coalesce = Coalesce(F('age'), Value(0))\n        result = list(Author.objects.annotate(coalesced_age=coalesce).values_list('coalesced_age', flat=True))\n        self.assertEqual(result, [34, 35, 45, 29, 37, 29, 25, 57, 46])\n\n"], "sample_872": ["    def test_roc_auc_score_multiclass(self, drop):\n        y_true = np.array([0, 0, 1, 1, 2, 2, 0, 1, 2])\n        y_score = np.array([[0.9, 0.1, 0.0],\n                            [0.8, 0.15, 0.05],\n                            [0.1, 0.2, 0.7],\n                            [0.3, 0.6, 0.1],\n                            [0.0, 0.4, 0.6],\n                            [0.1, 0.2, 0.7],\n                            [0.95, 0.02, 0.03],\n                            [0.2, 0.6, 0.2],\n                            [0.05, 0.1, 0.85]])\n        if drop:\n            y_score = y_score[y_true != 2]\n            y_true = y_true[y_true != 2]\n\n        expected = roc_auc_score(y_true, y_score, multi_class='ovr')\n        result = roc_auc_score(y_true, y_score, multi_class='ovr', drop_intermediate=drop)\n        assert_allclose(result, expected)\n\n", "    def test_top_k_accuracy_score_multiclass_labels_provided(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                           [0.3, 0.4, 0.2],  # 1 is in top 2\n                           [0.2, 0.4, 0.3],  # 2 is in top 2\n                           [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n        labels = [2, 0, 1]\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_almost_equal(score, 0.5)\n", "    def test_top_k_accuracy_score_multi_class(self):\n        # Check for correct behaviour with multi-class labels\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array(\n            [\n                [0.5, 0.2, 0.2],  # 0 is in top 2\n                [0.3, 0.4, 0.2],  # 1 is in top 2\n                [0.2, 0.4, 0.3],  # 2 is in top 2\n                [0.7, 0.2, 0.1],\n            ]  # 2 isn't in top 2\n        )\n        for k in [1, 2, 3]:\n            score = top_k_accuracy_score(y_true, y_score, k=k)\n            if k == 1:\n                assert score == 0.5\n            elif k == 2:\n                assert score == 0.75\n\n        # test with labels\n        labels = [2, 1, 0]\n        for k in [1, 2, 3]:\n            score = top_k_accuracy_score(\n                y_true, y_score, k=k, labels=labels\n            )\n            if k == 1:\n                assert score == 0.25\n            elif k == 2:\n                assert score == 0.5\n            elif k == 3:\n                assert score == 0.75\n", "    def test_roc_auc_score_multiclass(self):\n        y_true = np.array([0, 0, 1, 1, 2, 2, 2, 2, 2])\n        y_score = np.array([[0.1, 0.2, 0.7],\n                            [0.3, 0.4, 0.3],\n                            [0.6, 0.2, 0.2],\n                            [0.7, 0.1, 0.2],\n                            [0.5, 0.2, 0.3],\n                            [0.8, 0.05, 0.15],\n                            [0.9, 0.05, 0.05],\n                            [0.85, 0.05, 0.1],\n                            [0.95, 0.01, 0.04]])\n\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            assert_almost_equal(roc_auc_score(y_true, y_score, average='macro'),\n                                0.6666666666666666, 7)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            assert_almost_equal(roc_auc_score(y_true, y_score, average='weighted'),\n                                0.7458333333333334, 7)\n\n        with pytest.raises(ValueError):\n            roc_auc_score(y_true, y_score, average='samples')\n\n", "    def test_top_k_accuracy_score_multiclass_labels_provided(self):\n        y_true = np.array([0, 1, 2, 2])\n        y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n                           [0.3, 0.4, 0.2],  # 1 is in top 2\n                           [0.2, 0.4, 0.3],  # 2 is in top 2\n                           [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n        labels = [0, 1, 2]\n\n        score = top_k_accuracy_score(y_true, y_score, k=2, labels=labels)\n        assert_almost_equal(score, 0.75)\n\n"], "sample_948": ["def test_cpp_namespace_nested():\n    check(\n        \"namespace\",\n        \"namespace Test {{ int a; }}\",\n        {},\n        \"Test {int a; }\")\n\n", "def test_cpp_union_id():\n    check(\n        'union',\n        \"union {int a; int b;};\",\n        {\n            1: 'union',\n            2: 'union',\n            3: 'union',\n        },\n        output=\"union  {int a; int b;};\",\n    )\n", "def test_cpp_class_template_default_argument():\n    check('class',\n          'template <typename T = int> class MyClass {};',\n          {},\n          'template <typename T = int> class MyClass {};',\n          key='MyClass',\n          asTextOutput='MyClass')\n\n", "def test_cpp_nested_template_parameter():\n    check(\n        \"class\",\n        \"\"\"\n        template <typename T>\n        class Foo {\n        public:\n            template <typename U>\n            void bar(const U& u);\n        };\n        \"\"\",\n        {1: \"foo\", 2: \"foo_1\"},\n        \"\"\"\n        template <typename T>\n        class Foo {\n        public:\n            template <typename U>\n            void bar(const U& u);\n        }\n        \"\"\",\n        key=\"Foo\",\n    )\n\n", "def test_function_template_param_default():\n    check(\n        \"function\",\n        \"void foo(int a = 5, typename T = std::string);\",\n        {1: \"cpp:function:TestDoc:42:foo\"},\n        key=\"foo(\",\n        asTextOutput=\"foo(int a = 5, typename T = std::string)\",\n    )\n"], "sample_665": ["    def test_collect_with_venv(self, tmpdir):\n        \"\"\"Test that collect handles virtual environments.\"\"\"\n        venv_path = tmpdir.ensure(\"venv\", dir=True)\n        os.environ[\"VIRTUAL_ENV\"] = str(venv_path)\n\n        # Create a test file inside the \"virtual\" environment\n        test_file = venv_path.join(\"test_something.py\")\n        test_file.write(\"def test_func(): pass\")\n\n        # Use pytest to collect tests, ignoring any errors\n        session = Session.from_commandline([str(test_file)])\n        session.main()\n\n        # Ensure that tests were collected and that pytest didn't fail\n        assert len(session._items) == 1\n        assert session.exitstatus == ExitCode.OK\n\n", "    def test_collect_function_with_args_and_kwargs(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n                return request.param\n\n                return request.param\n            \"\"\"\n        )\n\n        testdir.create_test_file(\n            \"\"\"\n                assert arg == 1\n                assert kwarg == 42\n\n            test_func(1, kwarg=42)\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_pytest_collect_module_with_no_tests(self, tmpdir):\n        p = tmpdir.mkdir(\"sub\").join(\"test_no_tests.py\")\n        p.write(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        col = Session.pytest_session_prepare(\n            config=self.config,\n            args=[\"-v\", tmpdir.strpath],\n        )\n        col.perform_collect()\n        assert len(col.items) == 1\n", "    def test_collect_function_with_docstring(self, tmpdir):\n        p = tmpdir.join(\"test_func.py\")\n        p.write(\n            textwrap.dedent(\n                \"\"\"\n            '''This is a docstring.'''\n            pass\n        \"\"\"\n            )\n        )\n        items = pytest.collect(str(p))\n        assert len(items) == 1\n        item = items[0]\n        assert item.name == \"test_something\"\n\n", "    def test_simple_collection(self, tmpdir):\n        p = tmpdir.join(\"test_module.py\")\n        p.write(\"def test_func(): pass\")\n        reprec = self.runpytest(str(p))\n        reprec.assertoutcome(passed=1)\n"], "sample_18": ["    def test_copy_info(self):\n        q2 = self.q.copy()\n        assert_info_equal(self.q, q2)\n        q2.info.name = \"v2\"\n        assert_info_equal(self.q, q2, ignore={\"name\"})\n        q2 = copy.copy(self.q)\n        assert_info_equal(self.q, q2)\n", "    def test_array_operations_preserve_info(self):\n        q1 = copy.deepcopy(self.q)\n        q2 = copy.deepcopy(self.q)\n\n        q3 = q1 + q2\n        assert_info_equal(q1, q3)\n\n        q3 = q1 * 2\n        assert_info_equal(q1, q3)\n\n        q3 = q1.clip(min=1, max=3)\n        assert_info_equal(q1, q3)\n\n        q3 = q1.reshape((2, 2))\n        assert_info_equal(q1, q3)\n\n", "    def test_view_info(self):\n        v = self.q.view(self.q.dtype)\n        assert_no_info(v)\n        v.info = self.q.info.copy()\n        assert_info_equal(self.q, v)\n        v = self.q.view(np.ndarray)\n        assert_no_info(v)\n", "    def test_array_operation(self):\n        q2 = self.q * 2\n        assert_info_equal(self.q, q2)\n", "    def test_quantity_copy(self):\n        qc = copy.copy(self.q)\n        assert_info_equal(self.q, qc)\n\n        qd = copy.deepcopy(self.q)\n        assert_info_equal(self.q, qd)\n"], "sample_523": ["    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot(range(10), label='test')\n        ax.legend(loc=loc)\n        plt.close(fig)\n\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.legend(loc=loc)\n        fig.canvas.draw()\n\n\n", "    def test_legend_handler_tuple(self):\n        lines = [plt.plot([1, 2, 3], [4, 5, 6])[0] for i in range(3)]\n        fig, ax = plt.subplots()\n        labels = ['Line {}'.format(i) for i in range(3)]\n        ax.legend(lines, labels)\n\n        h = HandlerTuple([mlines.Line2D()])\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            ax.legend(lines, labels, handler_map={tuple: h})\n        fig.canvas.draw()\n\n        # Check that the legend entries have the right colors.\n\n        # TODO: Fix the failing test in case of multiple entries\n\n\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label='test')\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            legend = ax.legend(loc=loc)\n        assert legend.get_bbox_to_anchor().get_points() is not None\n\n", "def test_legend_with_linecollection():\n    fig, ax = plt.subplots()\n    # Create a LineCollection\n    x = np.linspace(0, 10, 10)\n    y = np.sin(x)\n    segments = [(x[i], y[i], x[i+1], y[i+1]) for i in range(len(x)-1)]\n    lc = mcollections.LineCollection(segments, label='LineCollection')\n    ax.add_collection(lc)\n    ax.legend()\n    plt.draw()\n\n\n"], "sample_682": ["    def test_evaluate_skip_marks(self) -> None:\n        testdir = Testdir()\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == 'win32', reason=\"skip on windows\")\n                pass\n\n            @pytest.mark.skip(reason=\"always skip\")\n                pass\n\n                pass\n            \"\"\"\n        )\n        result = runtestprotocol(testdir, \"test_skipif_false\")\n        assert result.rep_count == 1\n        assert result.passed == 1\n\n        result = runtestprotocol(testdir, \"test_skipif\")\n        assert result.rep_count == 0\n\n        result = runtestprotocol(testdir, \"test_skip\")\n        assert result.rep_count == 0\n\n        skip_reason = evaluate_skip_marks(testdir.getitem(\"test_skip\")).reason\n        assert skip_reason == \"always skip\"\n\n        skipif_reason = evaluate_skip_marks(testdir.getitem(\"test_skipif\")).reason\n        assert skipif_reason == \"skip on windows\"\n", "    def test_evaluate_skip_marks_condition_str_true(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import sys\n\n                assert True\n            pytest.mark.skipif(\"sys.platform == 'win32'\", reason=\"skip on windows\")\n            (test_function)\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n\n", "    def test_xfail_with_condition_and_reason(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(condition=\"sys.platform == 'win32'\", reason=\"unsupported on windows\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        assert result.stdout.str() == \"xfailed test_function\\n\"\n\n", "    def test_evaluate_skip_marks_skipif_false(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == 'win32', reason=\"skip on windows\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.assert_outcomes(passed=1)\n", "    def test_evaluate_xfail_marks_condition_raises_string(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                xfail(\"condition\", raises=ValueError)\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\n                \"*xfail*\",\n                \"*ValueError*\",\n                \"*condition*\",\n                \"*1 passed*\",\n                \"*1 xfailed*\",\n            ]\n        )\n"], "sample_311": ["    def test_changelist_with_filter(self):\n        response = self.client.get(reverse('admin:myapp_article_changelist'))\n        self.assertContains(response, '<option value=\"\">All</option>')\n        self.assertContains(response, '<option value=\"2008\">2008</option>')\n        self.assertContains(response, '<option value=\"2009\">2009</option>')\n        self.assertContains(response, '<option value=\"2000\">2000</option>')\n\n        response = self.client.get(reverse('admin:myapp_article_changelist') + '?date__year=2008')\n        self.assertContains(response, 'Article 1')\n        self.assertNotContains(response, 'Article 2')\n        self.assertNotContains(response, 'Article 3')\n", "    def test_change_list_preserve_filters(self):\n        response = self.client.get(reverse('admin:articles_article_changelist'))\n        self.assertContains(response, 'Article 1')\n        self.assertContains(response, 'Article 2')\n        self.assertContains(response, 'Article 3')\n\n        # Add a filter\n        response = self.client.get(reverse('admin:articles_article_changelist') + '?content=Middle')\n        self.assertContains(response, 'Article 1')\n        self.assertNotContains(response, 'Article 2')\n        self.assertNotContains(response, 'Article 3')\n\n        # Navigate to another page\n        response = self.client.get(add_preserved_filters(response.request['GET'], reverse('admin:articles_article_changelist') + '?p=2'))\n        self.assertContains(response, 'Article 1')\n        self.assertNotContains(response, 'Article 2')\n        self.assertNotContains(response, 'Article 3')\n", "    def test_add_related_object_with_foreign_key_to_self(self):\n        response = self.client.get(reverse('admin:admin_models_person_add'))\n        self.assertContains(response, 'Name')\n        self.assertContains(response, 'Friends')\n        self.assertContains(response, 'Guardian')\n\n        # Create a new person\n        data = {\n            'name': 'John Doe',\n            'friends': [self.p1.pk],\n            'guardian': self.p1.pk,\n        }\n        response = self.client.post(reverse('admin:admin_models_person_add'), data)\n        self.assertRedirects(response, reverse('admin:admin_models_person_changelist'))\n\n        john = Person.objects.get(name='John Doe')\n        self.assertEqual(john.friends.count(), 1)\n        self.assertEqual(john.guardian, self.p1)\n", "    def test_change_list_view_filtering_by_foreign_key(self):\n        response = self.client.get(reverse('admin:books_book_changelist'))\n        self.assertContains(response, 'Book 1')\n        self.assertContains(response, 'Book 2')\n        response = self.client.get(reverse('admin:books_book_changelist') + '?promo__name=Promo%201')\n        self.assertContains(response, 'Book 1')\n        self.assertNotContains(response, 'Book 2')\n\n", "    def test_changelist_escaped_object_titles(self):\n        book = Book.objects.create(name='<script>alert(\"XSS\")</script>')\n\n        response = self.client.get(reverse('admin:books_book_changelist'))\n\n        self.assertContains(response, '&lt;script&gt;alert(\"XSS\")&lt;/script&gt;')\n"], "sample_273": ["    def test_duplicate_db_table_names(self):\n        class DuplicateModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = 'duplicate_table'\n                app_label = 'check_framework'\n        \n        class DuplicateModel2(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = 'duplicate_table'\n                app_label = 'check_framework'\n\n        # Simulate a database connection.\n        with mock.patch('django.db.connections') as mock_connections:\n            mock_connections.databases = {'default': {'ENGINE': 'dummy'}}\n            errors = checks.run_checks(\n                [checks.model_checks.check_duplicate_db_tables]\n            )\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertIn(\"Duplicate table name 'duplicate_table' found\", errors[0].msg)\n", "    def test_duplicate_db_table_checks(self):\n        class DuplicateModel(models.Model):\n            class Meta:\n                app_label = 'check_framework'\n                db_table = 'duplicate_table'\n\n        with mock.patch('django.db.routers.Router.allow_migrate', new=lambda *args: True):\n            with override_settings(DATABASE_ROUTERS=[EmptyRouter]):\n                errors = checks.run_checks(list(self.apps.get_models()))\n                self.assertEqual(\n                    len([e for e in errors if isinstance(e, Error) and 'duplicate' in str(e)]),\n                    1,\n                )\n", "    def test_duplicate_db_table_different_apps(self):\n        with mock.patch('django.apps.apps.get_app_configs', \n                        return_value=self.apps.get_app_configs()):\n            check = checks.model_checks.check_duplicate_db_tables\n            # Duplicate tables in different apps should not trigger an error\n            model_a = type('ModelA', (models.Model,), {'Meta': type('Meta', (), {'db_table': 'my_table'})}),\n\n            model_b = type('ModelB', (models.Model,), {'Meta': type('Meta', (), {'db_table': 'my_table', 'app_label': 'other_app'})}),\n\n            self.assertEqual(len(check(self.apps, [model_a, model_b])), 0)\n\n\n\n", "    def test_duplicate_db_tables(self):\n        class Meta(models.Model):\n            app_label = \"check_framework\"\n            db_table = \"duplicate_table\"\n\n        class DuplicateTable(models.Model):\n            name = models.CharField(max_length=100)\n            meta = Meta\n\n        with self.assertRaisesMessage(ValueError, 'Model \"check_framework.DuplicateTable\" has already been registered with the database identifier \"duplicate_table\"'):\n            DuplicateTable._meta.get_db_table(using='default')\n", "    def test_duplicate_db_table(self):\n        with self.assertRaises(checks.Error) as cm:\n            class DuplicateModel(models.Model):\n                pass\n\n        # Make sure the error message is clear and helpful.\n        self.assertIn(\n            \"models.E001: Duplicate model definition for table 'check_framework_duplicatemodel'\",\n            str(cm.exception)\n        )\n\n"], "sample_893": ["def test_export_text_with_weights():\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=0)\n    clf.fit(X, y)\n\n    # Export the tree as text with weights\n    report = export_text(clf, show_weights=True)\n\n    # Check if the report contains the weights\n    assert \"weights: \" in report\n\n", "    def test_export_text_multi_output(self):\n        # Test export_text with multi-output regression\n        clf = DecisionTreeRegressor()\n        clf.fit(X, y2, sample_weight=w)\n        tree_str = export_text(clf)\n        # Check if output contains both target values\n        assert ('value: [[-1.00, 1.00]]' in tree_str and\n                'value: [[1.00, 2.00]]' in tree_str)\n\n", "    def test_export_text_max_depth(self):\n        clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n        report = export_text(clf, max_depth=2)\n        assert \"...\" not in report\n        report = export_text(clf, max_depth=1)\n        assert \"...\" in report\n", "    def test_export_text_custom_weights(self):\n        clf = DecisionTreeClassifier(random_state=0).fit(X, y, sample_weight=w)\n        dot_data = export_text(clf, show_weights=True, feature_names=['feature 1', 'feature 2'])\n        # Check if weights are displayed correctly\n        assert search(r'weights: \\[1\\.00, 3\\.00\\]', dot_data) is not None\n\n", "    def test_export_text_max_depth(self):\n        clf = DecisionTreeClassifier(max_depth=3, random_state=0).fit(X, y)\n\n        # Test with max_depth equal to the actual depth\n        report = export_text(clf, max_depth=3)\n        assert len(report.splitlines()) == 7\n\n        # Test with max_depth less than the actual depth\n        report = export_text(clf, max_depth=2)\n        assert len(report.splitlines()) == 5\n        assert \"truncated branch\" in report\n\n"], "sample_245": ["    def test_no_location(self):\n        \"\"\"\n        Test --no-location option.\n        \"\"\"\n        self._create_file(\n            'myapp/templates/index.html',\n            '{% trans \"Hello, world!\" %}',\n        )\n\n        out, po_contents = self._run_makemessages(no_location=True)\n\n        self.assertMsgId('Hello, world!', po_contents)\n        self.assertLocationCommentNotPresent(self.PO_FILE, None, 'myapp', 'templates', 'index.html')\n", "    def test_locale_dir(self):\n        # Test if makemessages creates the locale directory if it doesn't exist.\n        shutil.rmtree('locale', ignore_errors=True)\n        output, po_contents = self._run_makemessages()\n        self.assertTrue(os.path.exists('locale'))\n        self.assertTrue(os.path.isdir('locale'))\n        self.assertTrue(os.path.exists(self.PO_FILE))\n", "    def test_makemessages_disabled_i18n(self):\n        expected_output = (\n            'Django version 4.2, using settings '\n            \"'tests.settings'\\n\"\n            'processing locale de\\n'\n            'no translation files found for locale de\\n\"\n        )\n        with captured_stdout() as stdout:\n            self._run_makemessages()\n        self.assertEqual(stdout.getvalue(), expected_output)\n        self.assertFalse(os.path.exists(self.PO_FILE))\n\n        # Now turn i18n on and check it works\n        with override_settings(USE_I18N=True):\n            with captured_stdout() as stdout:\n                self._run_makemessages()\n            self.assertTrue(os.path.exists(self.PO_FILE))\n", "    def test_makemessages_disabled_i18n(self):\n        \"\"\"\n        Make sure makemessages doesn't generate anything if USE_I18N is False\n        \"\"\"\n        self.copytree(os.path.join(self.project_dir, 'templates'), self.work_dir)\n\n        with captured_stdout() as stdout, captured_stderr() as stderr:\n            output, po_contents = self._run_makemessages()\n\n        self.assertEqual(output, '')\n        self.assertFalse(os.path.exists(self.PO_FILE))\n", "    def test_makemessages_no_location(self):\n        \"\"\"\n        Test that location comments are suppressed when the --no-location flag\n        is used.\n\n        \"\"\"\n        with captured_stdout() as out:\n            self._run_makemessages(no_location=True)\n\n        output = out.getvalue()\n        self.assertNotRegex(output, r'#\\:.*')\n        with open(self.PO_FILE) as fp:\n            po_contents = fp.read()\n        self.assertNotRegex(po_contents, r'^\\#\\:.*')\n\n"], "sample_0": ["def test_pickle(UncertClass):\n    data = np.arange(10)\n    uncert = UncertClass(data)\n    uncert_pickled = pickle.loads(pickle.dumps(uncert))\n    assert isinstance(uncert_pickled, UncertClass)\n    assert_array_equal(uncert_pickled.array, data)\n", "def test_uncertainty_pickle(UncertClass):\n    # Test pickling and unpickling of uncertainty objects\n    data = np.array([1, 2, 3])\n    uncertainty = UncertClass(data, unit='m')\n    unpickled_uncertainty = pickle.loads(pickle.dumps(uncertainty))\n    assert unpickled_uncertainty.array_equal(uncertainty)\n    assert unpickled_uncertainty.unit == uncertainty.unit\n", "def test_pickle(UncertClass):\n    data = np.array([1, 2, 3])\n    uncertainty = UncertClass([0.1, 0.2, 0.3])\n    nddata = NDData(data, uncertainty=uncertainty)\n\n    unpickled_nddata = pickle.loads(pickle.dumps(nddata))\n    assert_allclose(unpickled_nddata.uncertainty.array, uncertainty.array)\n    assert unpickled_nddata.uncertainty.__class__ == UncertClass\n", "def test_unknown_uncertainty_conversion(UncertClass):\n    \"\"\"Test if conversion to another uncertainty type raises TypeError.\"\"\"\n    uncert = UncertClass([1, 2, 3], unit=u.m)\n\n    if UncertClass in uncertainty_types_without_conversion_support:\n        with pytest.raises(TypeError):\n            uncert.represent_as(StdDevUncertainty([1, 2, 3]))\n    else:\n        uncert.represent_as(StdDevUncertainty([1, 2, 3]))\n\n", "    def test_nduncertainty_pickle(self, UncertClass):\n        data = np.array([1, 2, 3])\n        uncert = UncertClass(data, unit=u.m)\n        pickled = pickle.dumps(uncert)\n        unpickled = pickle.loads(pickled)\n        assert unpickled.array_equal(uncert)\n"], "sample_321": ["    def test_csrf_reject_bad_origin_https(self):\n        request = self._get_POST_csrf_cookie_request()\n        request.META['HTTP_ORIGIN'] = 'http://example.com'\n        request.is_secure = lambda : True\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(request, token_view, (), {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(REASON_BAD_ORIGIN % 'http://example.com', str(response.content))\n", "    def test_csrf_cookie_is_not_httponly_when_settings_disable_it(self):\n        response = self.process_request(self._get_GET_csrf_cookie_request())\n        self.assertFalse(\n            'HttpOnly' in response.cookies[settings.CSRF_COOKIE_NAME]['__class__'].__str__(),\n            'CSRF cookie should not be HttpOnly when settings.CSRF_COOKIE_HTTPONLY is False'\n        )\n\n", "    def test_csrf_cookie_is_set_when_csrf_exempt(self):\n        request = self._get_GET_csrf_cookie_request()\n        response = post_form_view(request)\n        self.assertFalse('csrf_token' in response.cookies)\n\n        request = self._get_POST_csrf_cookie_request()\n        response = post_form_view(request)\n        self.assertFalse('csrf_token' in response.cookies)\n\n\n\n", "    def test_csrf_cookie_domain(self):\n        \"\"\"\n        Verify that CSRF cookie is set with the configured domain when\n        CSRF_COOKIE_DOMAIN is set.\n        \"\"\"\n        request = self._get_GET_csrf_cookie_request(cookie=self._csrf_id_cookie)\n        request.get_host = lambda: 'www.example.com'\n        middleware = CsrfViewMiddleware()\n        response = HttpResponse()\n        middleware.process_response(request, response)\n        self.assertEqual(response.cookies['csrftoken']['domain'], '.example.com')\n\n", "    def test_csrf_invalid_cookie_format(self):\n        # Test case for a cookie with invalid characters.\n        req = self._get_POST_csrf_cookie_request(\n            cookie='invalid-csrf-cookie'\n        )\n        response = self.middleware.process_view(req, post_form_view, (), {})\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(REASON_CSRF_TOKEN_INCORRECT, response.content.decode())\n"], "sample_633": ["    def test_similar_multiple_files(self):\n        linter = PyLinter()\n        reporter = Reporter()\n        linter.set_reporter(reporter)\n        linter.add_checker(similar.SimilarChecker(linter))\n        linter.check(SIMILAR1)\n        linter.check(SIMILAR2)\n        assert 'R0801' in reporter.messages_\n", "    def test_similar_with_imports(self):\n        linter = PyLinter(reporter=Reporter())\n        with redirect_stdout(StringIO()):\n            linter.check([HIDE_CODE_WITH_IMPORTS])\n        stats = linter.stats\n        assert stats[\"nb_duplicated_lines\"] == 0\n        assert stats[\"percent_duplicated_lines\"] == 0.0\n", "    def test_similar_empty_functions(self):\n        linter = PyLinter()\n        reporter = Reporter()\n\n        with redirect_stdout(StringIO()):\n            linter.set_reporter(reporter)\n            linter.check([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n        self.assertEqual(reporter.results[0].line, 1)\n        self.assertEqual(reporter.results[0].column, 0)\n\n", "    def test_similar_empty_functions(self):\n        linter = PyLinter()\n        reporter = Reporter()\n        linter.reporter = reporter\n        linter.check([EMPTY_FUNCTION_1, EMPTY_FUNCTION_2])\n        output = reporter.messages\n        assert len(output) == 1\n", "    def test_similar_ignore_imports_no_duplicates(self):\n        linter = PyLinter()\n        reporter = Reporter()\n        linter.set_reporter(reporter)\n        linter.check(\n            [HIDE_CODE_WITH_IMPORTS],\n            settings={\"ignore-imports\": True},\n        )\n        self.assertEqual(len(reporter.stats[\"duplicated_lines\"]), 0)\n\n"], "sample_491": ["    def test_boundfield_auto_id_with_string(self):\n        form = FrameworkForm()\n        bf = form['name']\n        self.assertEqual(bf.auto_id, \"name\")\n", "    def test_boundfield_auto_id(self):\n        form = FrameworkForm()\n        self.assertEqual(form[\"name\"].auto_id, \"name\")\n        with self.settings(DEBUG=True):\n            self.assertEqual(form[\"language\"].auto_id, \"language\")\n        self.assertEqual(form[\"name\"].auto_id, \"name\")\n\n", "    def test_as_hidden_without_initial(self):\n        form = FrameworkForm({'name': 'test'})\n        self.assertEqual(form.as_hidden(), '<input type=\"hidden\" name=\"name\" value=\"test\" id=\"id_name\">')\n\n", "    def test_boundfield_as_text(self):\n        form = FrameworkForm({'name': 'John Doe', 'language': 'P'})\n        bound_field = form['name']\n        self.assertEqual(bound_field.as_text(), '<input type=\"text\" name=\"name\" value=\"John Doe\" required id=\"id_name\">')\n        self.assertEqual(\n            bound_field.as_text(attrs={'class': 'special'}),\n            '<input type=\"text\" name=\"name\" value=\"John Doe\" required id=\"id_name\" class=\"special\">',\n        )\n\n", "    def test_bound_field_auto_id(self):\n        form = FrameworkForm({'name': 'John'})\n        bound_field = form['name']\n        self.assertEqual(bound_field.auto_id, 'name')\n        \n        form = FrameworkForm({'name': 'John'}, auto_id='%s_field' % name)\n        bound_field = form['name']\n        self.assertEqual(bound_field.auto_id, 'name_field')\n\n        form = FrameworkForm({'name': 'John'}, auto_id=False)\n        bound_field = form['name']\n        self.assertEqual(bound_field.auto_id, '')\n"], "sample_546": ["    def test_tight_layout_no_change(self):\n        fig, ax = plt.subplots()\n        original_bbox = fig.bbox\n        fig.tight_layout()\n        assert fig.bbox == original_bbox\n", "def test_figure_add_axes():\n    fig = Figure()\n    ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert ax.get_figure() is fig\n    assert ax.axes.get_position() == (0.1, 0.1, 0.8, 0.8)\n\n", "    def test_tight_layout(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        fig.tight_layout()\n        fig.canvas.draw()\n        # Check that the axes are not overlapping with the edges of the figure\n        assert fig.axes[0].get_position().x0 >= 0.1\n        assert fig.axes[0].get_position().y0 >= 0.1\n\n", "    def test_tight_layout_with_constrained_layout(self):\n        fig, ax = plt.subplots(constrained_layout=True)\n        ax.plot([1, 2, 3], [1, 2, 3])\n        fig.tight_layout()\n\n", "def test_figaspect():\n    # Basic usage\n    w, h = Figure.figaspect(2)\n    assert w > 0 and h > 0\n\n    # Array input\n    arr = np.arange(12).reshape(3, 4)\n    w, h = Figure.figaspect(arr)\n    assert w > 0 and h > 0\n\n    # Check against expected values\n    assert Figure.figaspect(1) == pytest.approx((6.4, 6.4))\n    assert Figure.figaspect([1, 2]) == pytest.approx((9.6, 4.8))\n"], "sample_749": ["    def test_fit_transform_with_sparse_input():\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        transformer = ColumnTransformer(\n            transformers=[\n                ('scaler', StandardScaler(), [0]),\n                ('sparse_matrix', SparseMatrixTrans(), [1]),\n            ])\n\n        Xt = transformer.fit_transform(X)\n        assert_true(sparse.issparse(Xt[:, 1]))\n        assert_allclose_dense_sparse(Xt[:, 0], StandardScaler().fit_transform(X[:, 0]))\n\n\n\n", "    def test_passthrough_sparse(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        ct = ColumnTransformer([('passthrough', 'passthrough', 0)])\n        Xt = ct.fit_transform(X)\n        assert_allclose_dense_sparse(Xt, X[:, 0])\n        assert_true(isinstance(Xt, sparse.csr_matrix))\n\n", "    def test_fit_transform_remainder_sparse(self):\n        X = sparse.csr_matrix([[1, 2, 3], [4, 5, 6]])\n        ct = ColumnTransformer([('scaler', StandardScaler(), [0, 1]),\n                                ('normalizer', Normalizer(), [2])],\n                              remainder='passthrough')\n        Xt = ct.fit_transform(X)\n        assert_true(sparse.issparse(Xt))\n        assert_allclose_dense_sparse(ct.transform(X), Xt)\n", "    def test_remainder_estimator_no_fit_params(self):\n        # check that passing an estimator as remainder raises an error\n        # if it doesn't accept fit_params\n        with assert_raises(TypeError):\n            ColumnTransformer([('trans', Trans(), [0]),\n                               ('trans', Trans(), [1])],\n                              remainder=TransNo2D())\n", "    def test_column_transformer_invalid_transformer(self):\n        X = np.array([[1, 2],\n                      [3, 4],\n                      [5, 6]])\n        ct = ColumnTransformer([('trans', TransRaise(), [0])],\n                               remainder='passthrough')\n\n        with pytest.raises(ValueError, match=\"specific message\"):\n            ct.fit(X)\n"], "sample_707": ["    def test_iterparentnodeids_empty(self, pytester: Pytester) -> None:\n        item = pytester.makeTestItem(\"test_empty.py\", \"\")\n        expected = [\"\"]\n        actual = list(nodes.iterparentnodeids(item.nodeid))\n        assert actual == expected\n\n", "    def test_iterparentnodeids_empty_nodeid(self, pytester: Pytester) -> None:\n        item = pytester.makepyfile(\n            \"\"\"\n                pass\n            \"\"\"\n        ).collect()[0]\n\n        parent_ids = list(nodes.iterparentnodeids(item.nodeid))\n        assert parent_ids == [\"\"]\n", "    def test_iterparentnodeids_empty(self, pytester: Pytester) -> None:\n        item = pytester.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        node = pytester.getnode(item.path)\n        assert list(nodes.iterparentnodeids(node.nodeid)) == [\"\"]\n", "    def test_iterparentnodeids_empty_nodeid(self, pytester: Pytester):\n        item = pytester.Item(name=\"test_func\", parent=None, nodeid=\"\")\n        ids = list(nodes.iterparentnodeids(item.nodeid))\n        assert ids == [\"\"]\n", "    def test_iterparentnodeids_empty(self, pytester: Pytester) -> None:\n        item = pytester.makepyfile(\n            \"\"\"\n            pass\n        \"\"\"\n        ).collect()\n        assert list(nodes.iterparentnodeids(item.nodeid)) == [\"\"]\n"], "sample_935": ["def test_cpp_template_arguments():\n    check(\n        'class',\n        'template <typename T>\\nclass MyClass {{ }}',\n        {1: 'classMyClass'},\n        output='template <typename T> class MyClass {{ }}',\n        key='MyClass',\n    )\n", "    def test_function_template_args(self):\n        check('function',\n              \"\"\"\n              void foo(\n                  int a,\n                  T b\n              ) {}\n              \"\"\",\n              {1: 'cpp:func-foo'},\n              key='foo',\n              output=\"\"\"\n              void foo(int a, T b)\n              \"\"\")\n", "    def test_cpp_enum_scoped():\n        check('enum',\n              'enum { CONST1, CONST2 = 2, CONST3 };',\n              { 1: 'enum'},\n              'enum { CONST1, CONST2 = 2, CONST3 };',\n              key='MyEnum',\n              asTextOutput='MyEnum { CONST1, CONST2 = 2, CONST3 };')\n\n", "def test_cpp_enum_unscoped():\n    check(\"enum\",\n          \"enum MyEnum {{ VALUE1, VALUE2 }}\",\n          {1: 'MyEnum', 2: 'MyEnum', 3: 'MyEnum', 4: 'MyEnum'},\n          \"enum MyEnum { VALUE1, VALUE2 }\",\n          key='MyEnum')\n\n", "def test_cpp_nested_class():\n    check('class',\n          \"template <typename T> class OuterClass {{ public: class InnerClass {}; }};\",\n          {},\n          \"template <typename T> class OuterClass {{ public: class InnerClass {}; }};\",\n          key='OuterClass::InnerClass')\n"], "sample_612": ["    def test_groupby_rolling_window(self):\n        ds = self.dataset\n        ds['rolling_mean'] = ds.foo.rolling(x=2, center=True).mean()\n        expected = ds.groupby('x').apply(lambda x: x.foo.rolling(x=2, center=True).mean())\n        assert_identical(ds.groupby('x').rolling_window(x=2, center=True).mean(), expected)\n", "    def test_groupby_apply_on_empty_groups(self):\n        ds = self.dataset.isel(x=[0, 2])\n        actual = ds.groupby(\"x\").apply(lambda x: x.sum())\n        expected = ds.groupby(\"x\").sum()\n        assert_identical(actual, expected)\n", "    def test_groupby_quantile(self, dataset):\n        actual = dataset.groupby(\"x\").quantile(0.5, dim=\"y\")\n\n        expected_quantiles = dataset[\"foo\"].groupby(\"x\").mean(dim=\"y\")\n        assert_identical(actual, expected_quantiles)\n", "    def test_groupby_apply_with_multiple_dimensions_and_single_coordinate(\n        self, dataset: Dataset", "def test_groupby_assign_coords(dataset):\n    ds = dataset.copy()\n    ds_grouped = ds.groupby(\"x\")\n    ds_grouped = ds_grouped.assign_coords(new_coord=((\"x\",), np.array([1, 2, 3])))\n    assert_identical(\n        ds_grouped.coords[\"new_coord\"],\n        xr.DataArray(np.array([1, 2, 3]), dims=(\"x\"), coords={\"x\": ds.coords[\"x\"]}),\n    )\n\n"], "sample_621": ["def test_get_unique():\n    indexes = Indexes({\n        'x': PandasIndex(pd.Index([1, 2, 3]), 'x'),\n        'y': PandasIndex(pd.Index([4, 5, 6]), 'y'),\n        'z': PandasIndex(pd.Index([1, 2, 3]), 'z'),\n    }, {})\n    unique_indexes = indexes.get_unique()\n    assert len(unique_indexes) == 2\n    assert unique_indexes[0].equals(indexes['x']) \n    assert unique_indexes[1].equals(indexes['y']) \n", "    def test_indexes_to_pandas_indexes(self, index, expected_index):\n        indexes = Indexes({0: index}, {})\n        pandas_indexes = indexes.to_pandas_indexes()\n        assert_identical(pandas_indexes[0], expected_index)\n\n", "    def test_indexes_all_equal():\n        index1 = PandasIndex(pd.Index([0, 1, 2]), \"dim1\")\n        index2 = PandasIndex(pd.Index([0, 1, 2]), \"dim1\")\n        index3 = PandasIndex(pd.Index([0, 1, 3]), \"dim1\")\n        index4 = PandasMultiIndex(pd.MultiIndex.from_tuples([(0, 'a'), (1, 'b'), (2, 'c')]), \"dim1\")\n\n        assert indexes_all_equal([(index1, {\"dim1\": Variable(\"dim1\", [0, 1, 2])})])\n        assert indexes_all_equal(\n            [(index1, {\"dim1\": Variable(\"dim1\", [0, 1, 2])}),\n             (index2, {\"dim1\": Variable(\"dim1\", [0, 1, 2])})]\n        )\n        assert not indexes_all_equal(\n            [(index1, {\"dim1\": Variable(\"dim1\", [0, 1, 2])}),\n             (index3, {\"dim1\": Variable(\"dim1\", [0, 1, 3])})]\n        )\n        assert not indexes_all_equal(\n            [(index1, {\"dim1\": Variable(\"dim1\", [0, 1, 2])}),\n             (index4, {\"dim1\": Variable(\"dim1\", [0, 1, 2])})]\n        )\n\n", "    def test_indexes_copy_indexes(self) -> None:\n        indexes = Indexes({\n            'x': PandasIndex(pd.Index(range(5), name='x'), 'x'),\n            'y': PandasIndex(pd.Index(range(5), name='y'), 'y'),\n            'z': PandasMultiIndex.from_arrays([\n                pd.Index(['a', 'a', 'b', 'b', 'b'], name='z1'),\n                pd.Index(range(5), name='z2'),\n            ], names=['z1', 'z2']),\n        }, {'x': Variable('x', range(5)), 'y': Variable('y', range(5)), 'z1': Variable('z1', ['a', 'a', 'b', 'b', 'b']), 'z2': Variable('z2', range(5))})\n\n        new_indexes, new_index_vars = indexes.copy_indexes()\n        assert new_indexes['x'] is not indexes['x']\n        assert new_indexes['y'] is not indexes['y']\n        assert new_indexes['z'] is not indexes['z']\n        assert new_indexes['z'].equals(indexes['z'])\n\n        assert_identical(new_index_vars['x'], indexes._variables['x'])\n        assert_identical(new_index_vars['y'], indexes._variables['y'])\n        assert_identical(new_index_vars['z1'], indexes._variables['z1'])\n        assert_identical(new_index_vars['z2'], indexes._variables['z2'])\n", "    def test_asarray_tuplesafe(index_data, expected):\n        result = _asarray_tuplesafe(index_data)\n        assert_identical(result, np.array(expected))\n\n"], "sample_398": ["    def test_password_reset_confirm_token_expiry(self):\n        # Token expiry test\n        self.u1.set_password(\"password\")\n        self.u1.save()\n        with self.assertRaises(ImproperlyConfigured):\n            PasswordResetConfirmView.as_view()(\n                self.request, uidb64=urlsafe_base64_encode(str(self.u1.id).encode()).decode(), token=\"some-invalid-token\"\n            )\n        \n        with mock.patch('django.contrib.auth.tokens.default_token_generator.make_token') as mock_make_token:\n            mock_make_token.return_value =  'some-token'\n        \n            response = self.client.post(\n                reverse('password_reset_confirm', kwargs={'uidb64': urlsafe_base64_encode(str(self.u1.id).encode()).decode(), 'token': 'some-token'}),\n                {'new_password1': 'newpassword', 'new_password2': 'newpassword'}\n            )\n            self.assertEqual(response.status_code, 200)\n\n            self.assertTrue(mock_make_token.called)\n", "    def test_password_reset_confirm_invalid_token(self):\n        self.client.force_login(self.u1)\n        self.logout()\n        # Generate a token that will expire\n        token = self.u1.generate_password_reset_token()\n        # Sleep to make sure token expires\n        time.sleep(settings.PASSWORD_RESET_TIMEOUT + 1)\n        response = self.client.get(\n            reverse(\n                \"password_reset_confirm\", kwargs={\"uidb64\": urlsafe_base64_encode(str(self.u1.id).encode()).decode(), \"token\": token}\n            )\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertTemplateUsed(response, \"registration/password_reset_confirm.html\")\n", "    def test_password_reset_confirm_with_token_in_session(self):\n        self.u1.set_password(\"newpassword\")\n        self.u1.save()\n        token = self.client.get(\"/reset/confirm/?uidb64={}&token={}\".format(\n            urlsafe_base64_encode(str(self.u1.pk).encode()).decode(),\n            self.client.session.get(INTERNAL_RESET_SESSION_TOKEN),\n        ))\n        self.assertEqual(token.status_code, 200)\n        self.assertTemplateUsed(token, \"registration/password_reset_confirm.html\")\n\n", "    def test_logout_then_login_redirect(self):\n        # Log in as a user\n        self.login()\n        # Get the next page URL from the session data\n        next_page = self.client.session.get(REDIRECT_FIELD_NAME)\n        self.assertIsNotNone(next_page)\n        # Log out the user and redirect to the login page\n        response = logout_then_login(self.client.session)\n        # Assert the redirect URL is correct\n        self.assertRedirects(response, reverse(\"login\") + \"?next=\" + quote(next_page))\n", "    def test_redirect_to_login_without_redirect_field_name(self):\n        self.client.force_login(self.u1)\n        response = self.client.get(\"/secured/\")\n        self.assertRedirects(response, settings.LOGIN_REDIRECT_URL)\n"], "sample_1171": ["def test_complexregion_contains_empty_interval():\n    r = Interval(0, 0)\n    theta = Interval(0, 2*S.Pi)\n    c = ComplexRegion(r * theta, polar=True)\n    assert 0 in c\n    assert not any(z in c for z in [1, 1 + I, -1])\n\n", "def test_normalize_theta_set_interval_multiples_of_pi():\n    from sympy import Interval\n    assert normalize_theta_set(Interval(0, 6*pi)) == Interval(0, 2*pi)\n    assert normalize_theta_set(Interval(-2*pi, 4*pi)) == Interval(0, 2*pi)\n    assert normalize_theta_set(Interval(-3*pi, pi)) == Union(Interval(pi, 2*pi), Interval(0, pi))\n\n\n\n", "def test_complexregion_polar_productset():\n    r = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n    c = ComplexRegion(r*theta, polar=True)\n    assert c.sets == ProductSet(Interval(0, 1), Interval.Ropen(0, 2*S.Pi))\n    assert c.polar\n    assert isinstance(c, PolarComplexRegion)\n\n    c2 = ComplexRegion(r*theta, polar=False)\n    assert c2.sets == ProductSet(Interval(0, 1), Interval.Ropen(0, 2*S.Pi))\n    assert not c2.polar\n    assert isinstance(c2, CartesianComplexRegion)\n\n", "def test_complex_region_intersection():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = Interval(1, 2)\n    d = Interval(S.Pi/2, 3*S.Pi/2)\n\n    c1 = ComplexRegion(a*b, polar=True)  \n    c2 = ComplexRegion(c*d, polar=True) \n\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(ProductSet(Interval(1, 2), Interval(S.Pi/2, 3*S.Pi/2)), polar=True)\n", "    def test_ComplexRegion_polar_intersection():\n        a = Interval(0, 1)\n        b = Interval(0, 2*S.Pi)\n        c = Interval(0, S.Pi/2)\n        unit_disk = ComplexRegion(ProductSet(a, b), polar=True)\n        upper_quadrant = ComplexRegion(ProductSet(a, c), polar=True)\n        intersection = unit_disk.intersect(upper_quadrant)\n        assert intersection == ComplexRegion(ProductSet(a, c), polar=True)\n"], "sample_1023": ["    def test_primorial():\n        assert primorial(4) == 210\n        assert primorial(4, nth=False) == 6\n        assert primorial(1) == 2\n        assert primorial(1, nth=False) == 1\n        assert primorial(sqrt(101), nth=False) == 210\n", "    def test_cycle_length_values():\n        func = lambda i: (i**2 + 1) % 51\n        n = cycle_length(func, 4,  values=True)\n        assert list(n) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n", "    def test_cycle_length_values():\n        f = lambda i: (i**2 + 1) % 51\n        L = list(cycle_length(f, 4, values=True))\n        assert L == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n", "    def test_cycle_length_values():\n        f = lambda i: (i**2 + 1) % 51\n        gen = cycle_length(f, 4, values=True)\n        assert list(gen) == [17, 35, 2, 5, 26, 14, 44, 50, 2, 5, 26, 14]\n\n", "    def test_compositepi():\n        assert compositepi(25) == 15\n        assert compositepi(1000) == 831\n        assert compositepi(1) == 0\n        assert compositepi(2) == 0\n        assert compositepi(3) == 0\n\n"], "sample_882": ["    def test_mlp_classifier_partial_fit(self, X, y, seed):\n        if len(np.unique(y)) > 2:\n            return\n        mlp = MLPClassifier(\n            hidden_layer_sizes=(10,),\n            activation=\"relu\",\n            solver=\"sgd\",\n            batch_size=10,\n            max_iter=10,\n            random_state=seed,\n            learning_rate=\"constant\",\n            learning_rate_init=0.1,\n        )\n        mlp.partial_fit(X[:50], y[:50])\n        mlp.partial_fit(X[50:], y[50:])\n        score = mlp.score(X, y)\n        assert score > 0.5\n\n", "    def test_mlp_regression_solver(dataset, solver):\n        X, y = dataset\n        mlp = MLPRegressor(solver=solver, max_iter=1000, random_state=42).fit(X, y)\n        assert mlp.score(X, y) > 0\n\n", "def test_mlp_classifier_partial_fit(dataset):\n    X, y = dataset\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(5,), solver=\"lbfgs\", batch_size=10, max_iter=10\n    )\n    mlp.partial_fit(X[:50], y[:50], classes=np.unique(y))\n    mlp.partial_fit(X[50:], y[50:])\n    mlp.predict_proba(X[:2])\n", "def test_mlp_classifier_accuracy(solver, dataset, expected_score):\n    X, y = dataset\n    mlp = MLPClassifier(solver=solver, max_iter=500, random_state=42)\n    mlp.fit(X, y)\n    score = mlp.score(X, y)\n    assert score == expected_score\n", "    def test_mlp_classifier_early_stopping(self, dataset):\n        X, y = dataset\n        mlp = MLPClassifier(\n            hidden_layer_sizes=(10,), solver=\"sgd\", early_stopping=True, random_state=42\n        )\n        mlp.fit(X, y)\n        assert mlp.n_iter_ < mlp.max_iter\n\n"], "sample_580": ["    def test_variable_type_mixed_numeric_categorical():\n        data = pd.Series([1, 2, 'a', 'b'])\n        assert variable_type(data) == VarType(\"categorical\")\n", "    def test_variable_type_datetime64(self):\n        series = pd.Series(pd.to_datetime([\"2023-10-26\", \"2023-10-27\"]))\n        assert variable_type(series) == VarType(\"datetime\")\n", "    def test_variable_type_datetime64():\n        vector = pd.Series(pd.to_datetime([\"2023-03-01\", \"2023-03-02\", \"2023-03-03\"]))\n        assert variable_type(vector) == VarType(\"datetime\")\n", "    def test_variable_type_boolean_strict(\n        self,", "    def test_variable_type_mixed_numeric_and_string():\n        vector = pd.Series([1, 2, 'a', 4, 'b'])\n        assert variable_type(vector) == VarType(\"categorical\")\n"], "sample_351": ["    def test_modelchoicefield_empty_label(self):\n        form = AuthorForm(\n            {\n                'name': 'Test author',\n                'friends': '1',\n            },\n            empty_label=None\n        )\n        self.assertTrue(form.is_valid())\n\n", "    def test_modelchoicefield_blank_choices(self):\n        form_data = {'category': ''}\n        form = CategoryForm(data=form_data)\n        self.assertTrue(form.is_valid())\n        self.assertIsNone(form.cleaned_data['category'])\n", "    def test_modelchoicefield_iterator_empty_label(self):\n        class EmptyLabelForm(forms.Form):\n            category = forms.ModelChoiceField(\n                queryset=Category.objects.all(), empty_label='Select a category'\n            )\n\n        form = EmptyLabelForm()\n        choices = list(form.fields['category'].choices)\n        self.assertEqual(choices[0][0], '')\n        self.assertEqual(choices[0][1], 'Select a category')\n        self.assertEqual(choices[1][0], str(self.c1.pk)),\n        self.assertEqual(choices[1][1], self.c1.name)\n\n", "    def test_modelchoicefield_choices(self):\n        field = ModelChoiceField(Category.objects.all())\n\n        # Verify the choices generated by the iterator\n        choices = list(field.choices)\n        self.assertEqual(len(choices), 3)\n        self.assertIn((self.c1.pk, self.c1.name), choices)\n        self.assertIn((self.c2.pk, self.c2.name), choices)\n\n        # Test with a limit_choices_to clause\n        field = ModelChoiceField(Category.objects.all(), limit_choices_to={'url__contains': 'test'})\n        choices = list(field.choices)\n        self.assertEqual(len(choices), 2)\n        self.assertIn((self.c2.pk, self.c2.name), choices)\n        self.assertIn((self.c3.pk, self.c3.name), choices)\n", "    def test_modelchoicefield_label_from_instance(self):\n        field = forms.ModelChoiceField(queryset=Category.objects.all())\n        self.assertEqual(field.label_from_instance(self.c1), str(self.c1))\n        self.assertEqual(field.label_from_instance(self.c2), str(self.c2))\n"], "sample_1052": ["def test_codegen_multioutput_mixed():\n    x, y, a = symbols('x y a')\n    r = make_routine('test', [Eq(a, x + y), a * x])\n    source = get_string(CCodeGen.dump_c, [r], header=False, empty=False)\n    expected = \"\"\"", "def test_codegen_output_argument_mixed_types():\n    x, y = symbols('x y')\n    f, g = symbols('f g', cls=Dummy)\n    r = make_routine(\n        'fcn', [Eq(f, 2*x), Eq(g, y), Matrix([[x, 2]])],\n    )\n    code = get_string(CCodeGen().dump_c, [r], header=False)\n    assert '\\tdouble fcn(double x, double y, double *f, double *g, double out_8598435338387848786[2]) {' in code\n\n\n", "def test_codegen_multiple_return_values():\n    x, y = symbols('x y')\n    f, g = symbols('f g', cls=Function)\n    routine = make_routine('two_outputs', [Eq(f, x + y), Eq(g, x*y)])\n\n    code = get_string(CCodeGen().dump_c, [routine])\n    assert 'double two_outputs(double x, double y, double *f, double *g)' in code\n    assert '(*f) = x + y;' in code\n    assert '(*g) = x*y;' in code\n\n", "def test_codegen_matrixsymbol():\n    x, y = symbols('x y')\n    A = MatrixSymbol('A', 2, 2)\n    expr = Eq(A[0, 0], x)\n    routine = make_routine('test', expr)\n    code = get_string(CCodeGen().dump_c, [routine])\n    assert \"#include \\\"test.h\\\"\" in code\n    assert \"void test(double x, double A[2][2]) {\" in code\n    assert \"A[0][0] = x;\" in code\n\n", "def test_codegen_indexed_output():\n    x, y = symbols('x y')\n    i = symbols('i', integer=True)\n\n    A = MatrixSymbol('A', 3, 3)\n    code_gen = CCodeGen()\n    r = code_gen.routine('test', [Eq(A[i, i], x*y)],\n                          argument_sequence=[i, x, y])\n    \n    source = get_string(code_gen.dump_c, [r], header=False)\n    \n    assert \"A[i, i] = x*y;\" in source\n"], "sample_1079": ["def test_point3d_intersection():\n    p1, p2 = Point3D(0, 0, 0), Point3D(1, 1, 1)\n    p3, p4 = Point3D(2, 2, 2), Point3D(1, 2, 3)\n\n    assert p1.intersection(p2) == []\n    assert p1.intersection(p1) == [p1]\n    assert p1.intersection(Line3D(p2, p3)) == []\n    assert p2.intersection(Line3D(p1, p3)) == [p2]\n    assert p1.intersection(Plane(p2, p3, p4)) == []\n    assert p2.intersection(Plane(p1, p3, p4)) == [p2]\n\n", "def test_point3d_intersection():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(4, 5, 6)\n    assert p1.intersection(p2) == []\n    assert p1.intersection(p1) == [p1]\n", "def test_point3d_direction_cosine():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    assert p1.direction_cosine(p2) == [sqrt(6)/6, sqrt(6)/6, sqrt(6)/3]\n\n", "    def test_Point3D_rotate():\n        p = Point3D(1, 0, 0)\n        r = p.rotate(pi/2, (0, 0, 0))\n        assert r == Point3D(0, 1, 0)\n", "def test_point3d_direction_cosine():\n    p1 = Point3D(1, 2, 3)\n    p2 = Point3D(2, 3, 5)\n    assert p1.direction_cosine(p2) == [sqrt(6)/6, sqrt(6)/6, sqrt(6)/3]\n"], "sample_282": ["    def test_multivaluefield_with_required_field_in_multiwidget(self):\n        form = PartiallyRequiredForm({'f_0': 'foo', 'f_1': 'bar'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'foo,bar')\n", "    def test_complex_field_initial(self):\n        form = ComplexFieldForm({\n            'field1_0': 'hello',\n            'field1_1': 'J,P',\n            'field1_2_0': '2023',\n            'field1_2_1': '01',\n            'field1_2_2': '01',\n            'field1_2_3': '10',\n            'field1_2_4': '00',\n            'field1_2_5': '00',\n        })\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['field1'], ('hello', ['J', 'P'], datetime(2023, 1, 1, 10, 0, 0)))\n\n", "    def test_partially_required_field_validation(self):\n        form = PartiallyRequiredForm({'f': 'a,b'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], 'a,b')\n\n        form = PartiallyRequiredForm({'f': 'a,'})\n        self.assertTrue(form.is_valid())\n\n        form = PartiallyRequiredForm({'f': ','})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['f'], '')\n\n        form = PartiallyRequiredForm({'f': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['f'], [\n            'This field is required.',\n        ])\n", "    def test_multivaluefield_errors(self):\n        form = ComplexFieldForm({'field1_0': 'a', 'field1_1': 'J,P', 'field1_2_0': '2023', 'field1_2_1': '01', 'field1_2_2': '01'})\n        self.assertTrue(form.is_valid())\n        form = ComplexFieldForm({'field1_0': 'a', 'field1_1': 'J', 'field1_2_0': '2023', 'field1_2_1': '01', 'field1_2_2': '01'})\n        self.assertFalse(form.is_valid())\n\n\n\n", "    def test_complex_multi_widget_with_required_field(self):\n        form = ComplexFieldForm({'field1_0': 'value1', 'field1_1': ['J', 'P'], 'field1_2_0': '2023', 'field1_2_1': '01', 'field1_2_2': '01', 'field1_2_3': '00', 'field1_2_4': '00', 'field1_2_5': '00'})\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data['field1'], ('value1', ['J', 'P'], datetime(2023, 1, 1, 0, 0, 0)))\n"], "sample_500": ["    def test_colorbar_ticks_and_formatters():\n        # Create a figure and axes.\n        fig, ax = plt.subplots()\n        # Generate some data.\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        # Create a scatter plot.\n        sc = ax.scatter(x, y, c=x)\n        # Create a colorbar.\n        cbar = plt.colorbar(sc)\n\n        # Test default ticks and formatters\n        assert isinstance(cbar.ax.yaxis.get_major_locator(),\n                          mticker.MaxNLocator)\n        assert isinstance(cbar.ax.yaxis.get_major_formatter(),\n                          mticker.ScalarFormatter)\n\n        # Test custom ticks and formatters\n        ticks = [0, 2, 4, 6, 8, 10]\n        formatter = mticker.FormatStrFormatter('%1.0f')\n        cbar.set_ticks(ticks)\n        cbar.set_ticklabels(ticks)\n        cbar.ax.yaxis.set_major_formatter(formatter)\n\n        # Assert that the ticks and formatters are applied correctly\n        assert np.allclose(cbar.ax.get_yticks(), ticks)\n        assert cbar.ax.yaxis.get_major_formatter() == formatter\n\n        # Test ticks with LogNorm\n        norm = mcolors.LogNorm(vmin=0.1, vmax=10)\n        cbar = plt.colorbar(sc, norm=norm)\n        assert isinstance(cbar.ax.yaxis.get_major_locator(),\n                          mticker.LogLocator)\n        assert isinstance(cbar.ax.yaxis.get_major_formatter(),\n                          mticker.LogFormatterSciNotation)\n\n        \n", "    def test_colorbar_ticks_with_extend(self):\n        # Test that ticks are correctly positioned when extend is set.\n        cmap = cm.get_cmap(\"viridis\", lut=5)\n        clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n        norm = BoundaryNorm(clevs, len(clevs) - 1)\n        fig, ax = plt.subplots()\n        cax = fig.add_axes([0.1, 0.1, 0.2, 0.8])\n        cbar = Colorbar(cax, cmap=cmap, norm=norm,\n                        extend='both', spacing='proportional',\n                        orientation='vertical')\n        cbar.set_ticks(clevs[1:-1])\n\n        # Create a simple line plot to ensure there are no side-effects\n        ax.plot([0, 1], [0, 1])\n        fig.canvas.draw()\n        ticks = cbar.ax.get_yticklabels()\n        tick_pos = [tick.get_position()[1] for tick in ticks]\n        # Assert that the tick positions are not all the same\n        assert len(set(tick_pos)) > 1\n\n", "    def test_colorbar_extension_shape_with_lognorm(self):\n        \"\"\"\n        Test the shape of colorbar extensions when using a LogNorm.\n\n        The shape of the extensions should be rectangular, even when using\n        a LogNorm.\n        \"\"\"\n        cmap = cm.get_cmap(\"RdBu\", lut=5)\n        # Define levels, including zero, to cover both positive and negative\n        # values in log scale.\n        clevs = [0.1, 0.5, 1, 5, 10]\n\n        fig, axs = plt.subplots(2, 2)\n\n        for i, extend in enumerate(('neither', 'min', 'max', 'both')):\n            ax = axs.flat[i]\n            norm = LogNorm(vmin=0.1, vmax=10)\n            Colorbar(ax, cmap=cmap, norm=norm,\n                     boundaries=clevs, values=clevs,\n                     extend=extend, extendrect=True,\n                     orientation='horizontal')\n            ax.tick_params(left=False, labelleft=False,\n                            bottom=False, labelbottom=False)\n        return fig\n", "def test_colorbar_extension_length():\n    return _colorbar_extension_length(spacing='uniform')\n", "def test_colorbar_ticks_locations():\n    # Test ticks locations and labels for a colorbar.\n    cmap = cm.get_cmap(\"viridis\", lut=10)\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape(10, 10), cmap=cmap)\n    \n    # Test default tick locations\n    cb = fig.colorbar(im)\n    plt.testing.assert_array_equal(cb.get_ticks(),np.linspace(0, 99, 11))\n    \n    # Test manual tick locations\n    cb.set_ticks([0, 25, 50, 75, 100])\n    plt.testing.assert_array_equal(cb.get_ticks(), [0, 25, 50, 75, 100])\n\n    # Test tick locations with BoundaryNorm\n    norm = BoundaryNorm([0, 25, 50, 75, 100], cmap.N)\n    cb = fig.colorbar(im, norm=norm)\n    plt.testing.assert_array_equal(cb.get_ticks(), [0, 25, 50, 75, 100])\n"], "sample_859": ["    def test_multitask_elastic_net_cv_warm_start(self):\n        X, y = make_regression(n_targets=2, noise=4, random_state=0)\n        n_samples, n_features = X.shape\n\n        clf = MultiTaskElasticNetCV(cv=5, warm_start=True, random_state=0)\n        clf.fit(X, y)\n        initial_coef = clf.coef_.copy()\n\n        # Fit again but on a subset of the data\n        X_subset = X[:n_samples // 2]\n        y_subset = y[:n_samples // 2]\n        clf.fit(X_subset, y_subset)\n\n        # Warm start should reuse the previous coefficients\n        assert_array_almost_equal(clf.coef_, initial_coef)\n\n\n", "    def test_multitask_lasso_cv_fit_intercept(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1], [2], [3]])\n        # Test with fit_intercept=True\n        clf = MultiTaskLassoCV(fit_intercept=True, cv=2)\n        clf.fit(X, y)\n        coef_true = np.array([[0.46194268], [0.53805732]])\n        intercept_true = np.array([0.25238095])\n        assert_array_almost_equal(clf.coef_, coef_true)\n        assert_array_almost_equal(clf.intercept_, intercept_true)\n\n        # Test with fit_intercept=False\n        clf = MultiTaskLassoCV(fit_intercept=False, cv=2)\n        clf.fit(X, y)\n        coef_true = np.array([[0.39741695], [0.60258305]])\n        assert_array_almost_equal(clf.coef_, coef_true)\n        assert_array_almost_equal(clf.intercept_, np.zeros(1))\n", "    def test_multitask_lasso_cv_fit_intercept(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1, 2], [3, 4], [5, 6]])\n        clf = MultiTaskLassoCV(fit_intercept=True).fit(X, y)\n\n        # Make sure intercept is not zero\n        assert clf.intercept_.sum() != 0\n\n        clf = MultiTaskLassoCV(fit_intercept=False).fit(X, y)\n\n        # Make sure intercept is zero\n        assert_array_equal(clf.intercept_, np.zeros(y.shape[1]))\n\n", "    def test_multitasklasso_convergence_warn(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([[1, 2], [3, 4], [5, 6]])\n        clf = MultiTaskLasso(max_iter=1, warm_start=False)\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            clf.fit(X, y)\n            assert len(w) == 1\n            assert isinstance(w[0].message, ConvergenceWarning)\n\n", "    def test_multitask_lasso_cv_no_convergence(self):\n        X = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n        y = np.array([[0, 0], [1, 1], [0, 0], [1, 1]])\n        clf = MultiTaskLassoCV(max_iter=1, cv=2)\n        clf.fit(X, y)\n        assert clf.n_iter_ == 1\n"], "sample_210": ["    def test_view_as_view(self):\n        view = SimpleView.as_view()\n        request = self.rf.get('/')\n        response = view(request)\n        self._assert_simple(response)\n", "    def test_view_as_view_initkwargs_invalid(self):\n        with self.assertRaises(TypeError) as e:\n            views.View.as_view(invalid_argument=True)\n        self.assertIn('as_view only accepts arguments that are already attributes', str(e.exception))\n", "    def test_customizable_view(self):\n        view = CustomizableView()\n        view.parameter = {'key': 'value'}\n        response = view.as_view()(self.rf.get('/'))\n        self.assertEqual(response.status_code, 200)\n", "    def test_view_as_view(self):\n        view = SimpleView.as_view()\n        response = view(self.rf.get('/'))\n        self._assert_simple(response)\n        self.assertTrue(hasattr(view, 'is_decorated'))\n", "    def test_view_as_view(self):\n        view = SimpleView.as_view()\n        request = self.rf.get('/')\n        response = view(request)\n        self._assert_simple(response)\n"], "sample_963": ["    def test_restify_broken_type():\n        assert restify(BrokenType) == 'BrokenType'\n", "def test_restify_invalid_builtin_classes():\n    assert restify(Struct) == ':py:class:`struct.Struct`'\n    assert restify(TracebackType) == ':py:class:`types.TracebackType`'\n", "    def test_restify_broken_type(self):\n        assert restify(BrokenType) == repr(BrokenType)\n", "    def test_restify_forwardref():\n        from sphinx.util.typing import ForwardRef\n\n        cls = ForwardRef('MyClass')\n\n        assert restify(cls) == ':py:class:`MyClass`'\n", "    def test_restify_forwardref():\n        assert restify(ForwardRef('MyClass1')) == ':py:class:`MyClass1`'\n        assert restify(ForwardRef('MyClass2', is_argument=False)) == ':py:class:`<MyClass2>`'\n"], "sample_89": ["    def test_import_from_zipfile(self):\n        with tempfile.NamedTemporaryFile(suffix='.zip') as zip_file:\n            zipfile_path = zip_file.name\n\n            with zipfile.ZipFile(zip_file, 'w') as zipf:\n                zipf.writestr('test_module.py', 'print(\"Hello from zipfile\")')\n\n            self.addCleanup(lambda: shutil.rmtree(zipfile_path))\n            self.import_and_cleanup('zipfile_test')\n\n            self.assertFileFound(Path(zipfile_path))\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_filename = Path(tmpdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n            sys.path.append(tmpdir)\n            self.addCleanup(sys.path.remove, tmpdir)\n\n            self.assertFileFound(zip_filename)\n\n            # Make sure the file is found even after reloading\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'my_module.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('my_module/__init__.py', '')\n            sys.path.append(tempdir)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zip_filename)\n\n", "    def test_module_in_zipfile(self):\n        with tempfile.NamedTemporaryFile(suffix='.zip') as temp:\n            zipfile_path = Path(temp.name)\n            with zipfile.ZipFile(zipfile_path, 'w') as zf:\n                zf.writestr('my_module.py', 'pass')\n            temp.seek(0)\n            sys.path.append(zipfile_path.parent)\n            self.addCleanup(sys.path.remove, zipfile_path.parent)\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(zipfile_path)\n", "    def test_iter_modules_and_files_zipfile(self):\n\n        with tempfile.NamedTemporaryFile('w', delete=False) as f:\n            f.write('__init__.py')\n        zip_filename = f.name + '.zip'\n        with zipfile.ZipFile(zip_filename, 'w') as zf:\n            zf.write(f.name)\n        self.addCleanup(os.remove, zip_filename)\n\n        sys_path = sys.path[:]\n        try:\n            sys_path.append(os.path.dirname(zip_filename))\n            with extend_sys_path(sys_path):\n                self.import_and_cleanup('test_zipfile')\n                self.assertFileFound(Path(zip_filename))\n        finally:\n            sys.path = sys_path\n"], "sample_505": ["    def test_date2num_tz(self, tzinfo):\n        dt = datetime.datetime(2020, 1, 1, 12, 0, 0, tzinfo=tzinfo)\n        num = mdates.date2num(dt)\n        assert np.isfinite(num)\n\n        # Check roundtrip\n        dt2 = mdates.num2date(num)\n        assert dt2 == dt\n\n", "    def test_locator_interval(self, locator_class, kwargs):\n        fig, ax = plt.subplots()\n        start_date = datetime.datetime(2023, 12, 1)\n        end_date = datetime.datetime(2023, 12, 31)\n\n        # Create a DateLocator with the specified interval\n        locator = locator_class(**kwargs)\n\n        ax.plot([start_date, end_date], [1, 2])\n        ax.xaxis.set_major_locator(locator)\n        ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n\n        fig.autofmt_xdate()\n        # Check if the tick positions are correct\n        ticks = ax.get_xticks()\n        expected_ticks = locator.tick_values(start_date, end_date)\n        assert np.allclose(ticks, expected_ticks)\n\n\n\n", "    def test_date_converter_axisinfo(self, tz):\n        converter = mdates.DateConverter()\n        info = converter.axisinfo(unit=tz, axis=plt.gca())\n        assert isinstance(info.majloc, mdates.AutoDateLocator)\n        assert isinstance(info.majfmt, mdates.AutoDateFormatter)\n\n", "    def test_concise_date_formatter(self, tz):\n        # Test ConciseDateFormatter with various timezones and zero/offset\n        # formatting\n        dates = np.array([\n            datetime.datetime(2023, 1, 1, 0, 0, tzinfo=tz),\n            datetime.datetime(2023, 1, 1, 12, 0, tzinfo=tz),\n            datetime.datetime(2023, 1, 2, 0, 0, tzinfo=tz),\n        ])\n        locator = mdates.AutoDateLocator(tz=tz)\n        formatter = mdates.ConciseDateFormatter(\n            locator, tz=tz,\n            formats=[\"%Y-%m-%d\", \"%H:%M\"],\n            zero_formats=[\"%Y-%m-%d\"],\n            offset_formats=[\"%H:%M\"],\n            show_offset=True\n        )\n        labels = formatter.format_ticks(dates)\n        # Format should be consistent for the same date but different\n        # hours\n        assert labels[0] == labels[1].split(\",\")[0]\n        assert labels[1].endswith(\", 12:00\")\n\n        # Check that zero and offset formats are applied correctly\n        dates = np.array([\n            datetime.datetime(2023, 1, 1, 0, 0, tzinfo=tz),\n            datetime.datetime(2023, 1, 1, 12, 0, tzinfo=tz),\n            datetime.datetime(2023, 1, 2, 0, 0, tzinfo=tz),\n        ])\n\n", "def test_date_ticker_factory():\n    fig, ax = plt.subplots()\n\n    # Test that date_ticker_factory returns a locator and formatter\n    locator, formatter = mdates.date_ticker_factory(1)\n    assert isinstance(locator, mdates.Locator)\n    assert isinstance(formatter, mdates.DateFormatter)\n\n    # Test that date_ticker_factory generates different\n    # locators and formatters based on date span\n\n    locator, formatter = mdates.date_ticker_factory(0.5)\n    assert isinstance(locator, mdates.HourLocator)\n\n    locator, formatter = mdates.date_ticker_factory(30)\n    assert isinstance(locator, mdates.MonthLocator)\n\n    locator, formatter = mdates.date_ticker_factory(365 * 2)\n    assert isinstance(locator, mdates.YearLocator)\n"], "sample_417": ["    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            output = self.engine.render_to_string(\n                \"floatformat01\", {\"a\": Decimal(\"1.23456\"), \"b\": 1.23456}\n            )\n            self.assertEqual(output, \"1.23 1.23\")\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):\n            self.assertEqual(\n                self.render(\"floatformat01\", {\"a\": 1234.56789, \"b\": Decimal(1234.56789)}),\n                \"1234.56789 1234.56789\",\n            )\n", "    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": 12.3456, \"b\": Decimal(\"34.5678\")})\n            self.assertEqual(output, '12.346 34.568')\n", "    def test_floatformat_with_decimal(self):\n        with localcontext() as ctx:\n            ctx.prec = 6\n            decimal_value = Decimal(\"123.456789\")\n            self.assertEqual(\n                self.engine.render_to_string(\n                    \"floatformat01\", {\"a\": decimal_value, \"b\": decimal_value}\n                ),\n                \"123.456789 123.456789\",\n            )\n", "    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": \"1.23456\", \"b\": Decimal(\"1.23456\")})\n            self.assertEqual(output, \"1.23456 1.23456 \")\n"], "sample_696": ["    def test_fillfuncargs(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n            pytest.fillfuncargs(test_func, {'arg': 1})\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_contains_warning(deprecated.FILLFUNCARGS.message)\n\n", "    def test_pytest_collect_module(self) -> None:\n        with mock.patch(\"warnings.warn\", new=lambda *args, **kwargs: None) as mock_warn:\n            deprecated.PYTEST_COLLECT_MODULE.issue(name=\"collect\")\n            assert mock_warn.call_count == 1\n            assert re.search(r\"pytest\\.collect\\.collect was moved to pytest\\.collect\", mock_warn.call_args[0][0])\n", "def test_deprecated_node_fspath(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            node = pytest.nodes.Node.from_parent(None, \"test_mytest.py\")\n            print(node.fspath)\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines([\"*DeprecationWarning: Node.fspath is deprecated*\"])\n    assert result.ret == 0\n", "def test_deprecation_warning_for_pytest_collect_module(pytester: Pytester) -> None:\n    p = pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pytest.collect.something()\n        \"\"\"\n    )\n    result = pytester.runpytest(p, \"-s\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*PytestDeprecationWarning: pytest.collect.something was moved to pytest.something*\",\n        ]\n    )\n", "    def test_deprecation_warning_for_pytest_collect_module(self, pytester: Pytester) -> None:\n        pytester.mark(\n            \".\", deprecated.PYTEST_COLLECT_MODULE\n        )\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n            pytest.collect.test_func(test_func)\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines(\n            [\n                \"*pytest.collect.test_func was moved to pytest.test_func*\",\n            ]\n        )\n"], "sample_122": ["    def test_get_cache_key_with_vary_header(self):\n        request = HttpRequest()\n        request.method = 'GET'\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-US'\n        request.META['HTTP_ACCEPT_ENCODING'] = 'gzip, deflate'\n\n        response = HttpResponse('Hello world')\n        response['Vary'] = 'Accept-Language, Accept-Encoding'\n\n        key_prefix = 'test_prefix'\n        cache_key = learn_cache_key(request, response, key_prefix=key_prefix)\n        self.assertIsNotNone(cache_key)\n        self.assertIn(key_prefix, cache_key)\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'fr-FR'\n        different_cache_key = learn_cache_key(request, response, key_prefix=key_prefix)\n        self.assertNotEqual(cache_key, different_cache_key)\n", "    def test_cache_key_with_special_characters(self):\n        request = HttpRequest()\n        request.META['HTTP_HOST'] = 'example.com'\n        request.META['PATH_INFO'] = '/foo?'\n        request.META['QUERY_STRING'] = 'bar=baz&qux=quux'\n\n        # Cache key with special characters should raise a warning\n        with warnings.catch_warnings(record=True) as w:\n            cache_key = get_cache_key(request)\n        self.assertEqual(len(w), 1)\n        self.assertIsInstance(w[-1].message, CacheKeyWarning)\n        self.assertIn(\n            KEY_ERRORS_WITH_MEMCACHED_MSG % cache_key,\n            str(w[-1].message)\n        )\n", "    def test_learn_cache_key_with_vary_accept_language(self):\n        request = HttpRequest()\n        request.META['HTTP_ACCEPT_LANGUAGE'] = 'en-us,en;q=0.5'\n\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Language'\n        learn_cache_key(request, response)\n        cache_key = get_cache_key(request)\n        self.assertIn('.en-us', cache_key)\n", "    def test_get_cache_key_with_vary_header(self):\n        request = HttpRequest()\n        request.META = {'HTTP_ACCEPT_LANGUAGE': 'en-US'}\n        response = HttpResponse()\n        response['Vary'] = 'Accept-Language'\n\n        cache_key = get_cache_key(request, method='GET', cache=caches['default'])\n        self.assertIsNotNone(cache_key)\n", "    def test_learn_cache_key_with_vary_header(self):\n        request = HttpRequest()\n        request.META['HTTP_HOST'] = 'example.com'\n        request.method = 'GET'\n\n        response = HttpResponse('Hello, world!')\n        response['Vary'] = 'Accept-Language'\n\n        cache_key = learn_cache_key(request, response)\n        self.assertTrue(cache_key.startswith('views.decorators.cache.cache_header'))\n        self.assertIn('Accept-Language', cache_key)\n"], "sample_890": ["    def test_backward_selection_with_groups():\n        X, y = make_classification(n_samples=100, n_features=10, n_informative=5,\n                                   n_redundant=0, random_state=42)\n        groups = np.array([0] * 50 + [1] * 50)\n        estimator = KNeighborsClassifier(n_neighbors=3)\n        selector = SequentialFeatureSelector(estimator, n_features_to_select=3,\n                                           direction='backward',\n                                           cv=LeaveOneGroupOut())\n        selector.fit(X, y, groups=groups)\n        assert selector.n_features_to_select_ == 3\n\n", "def test_sequantial_feature_selector_with_groups():\n    X, y = make_regression(n_samples=100, n_features=10, random_state=0)\n    groups = np.repeat([1, 2, 3], 33)\n    estimator = LinearRegression()\n    selector = SequentialFeatureSelector(estimator, n_features_to_select=5)\n\n    selector = make_pipeline(StandardScaler(), selector)\n    scores = cross_val_score(\n        selector, X, y, cv=LeaveOneGroupOut(), groups=groups, scoring=\"r2\"\n    )\n    assert np.mean(scores) > 0.8\n\n\n", "    def test_sfs_backward_selection_multiclass():\n        X, y = make_classification(n_samples=50, n_features=10, n_classes=3,\n                                   random_state=42)\n        estimator = KNeighborsClassifier(n_neighbors=3)\n        sfs = SequentialFeatureSelector(estimator, direction='backward',\n                                        n_features_to_select=5)\n        sfs.fit(X, y)\n        assert sfs.n_features_to_select_ == 5\n        assert_array_equal(sfs.support_, [True, True, False, True, True,\n                                         False, True, False, False, True])\n", "    def test_sequentialselector_backward_selection_with_scoring(self):\n        X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n\n        estimator = LinearRegression()\n        selector = SequentialFeatureSelector(\n            estimator,\n            n_features_to_select=5,\n            direction=\"backward\",\n            scoring=\"neg_mean_squared_error\",\n        )\n        selector.fit(X, y)\n        assert selector.n_features_to_select_ == 5\n        assert selector.support_.sum() == 5\n", "    def test_sfs_groups(self):\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        groups = np.array([0] * 50 + [1] * 50)\n\n        sfs = SequentialFeatureSelector(\n            estimator=LogisticRegression(penalty=\"none\"),\n            n_features_to_select=5,\n            cv=LeaveOneGroupOut(),\n        )\n        sfs.fit(X, y, groups)\n        assert sfs.n_features_to_select_ == 5\n\n"], "sample_1184": ["    def test_conjugate_gauss_beams_f():\n        l, w_i, w_o = symbols('l w_i w_o')\n        s_in, s_out, f = conjugate_gauss_beams(1, w_i, w_o, f=5)\n        assert streq(s_in, \n                     5*(1 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(25*l**2))))\n        assert streq(s_out, \n                     5*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 - pi**2*w_i**4/(25*l**2)))/w_i**2)\n        assert streq(f, 5)\n", "compilation error", "    def test_conjugate_gauss_beams_dist():\n        l, wi, wo, f = symbols('l wi wo f')\n        s_in, s_out, f = conjugate_gauss_beams(l, wi, wo, dist=l)\n        assert streq(s_in, f*(1 - sqrt(wi**2/wo**2 - pi**2*wi**4/(f**2*l**2))))\n        assert streq(s_out, f*wo**2*(wi**2/wo**2 - sqrt(wi**2/wo**2 - pi**2*wi**4/(f**2*l**2)))/wi**2)\n", "def test_conjugate_gauss_beams():\n    l, w_i, w_o, f = symbols('l w_i w_o f')\n    s_in, s_out, f = conjugate_gauss_beams(10, 1, 2, f=f)\n    assert streq(str(factor(s_in)),'f*(1 - sqrt(w_i**2/w_o**2 - 100*pi**2*w_i**4/(f**2*l**2)))')\n    assert streq(str(factor(s_out)),'f*w_o**2*(w_i**2/w_o**2 - sqrt(w_i**2/w_o**2 - 100*pi**2*w_i**4/(f**2*l**2)))/w_i**2')\n    assert streq(str(f), 'f')\n\n", "compilation error"], "sample_488": ["    def test_get_app_config(self):\n        with isolate_apps(SOME_INSTALLED_APPS):\n            apps.populate(SOME_INSTALLED_APPS)\n            self.assertEqual(apps.get_app_config(\"MyAuth\").name, \"apps.apps.MyAuth\")\n            self.assertEqual(apps.get_app_config(\"django.contrib.auth\").name, \"django.contrib.auth\")\n\n            with self.assertRaises(LookupError) as cm:\n                apps.get_app_config(\"nonexistent_app\")\n            self.assertIn(\"No installed app with label 'nonexistent_app'.\", str(cm.exception))\n\n            apps.get_app_config(\"django.contrib.auth\")\n            self.assertTrue(apps.get_app_config(\"django.contrib.auth\").ready)\n", "    def test_get_swappable_settings_name_for_swapped_model(self):\n        with override_settings(INSTALLED_APPS=SOME_INSTALLED_APPS):\n            self.assertEqual(apps.get_swappable_settings_name(\"django.contrib.auth.user\"), \"AUTH_USER_MODEL\")\n", "    def test_get_containing_app_config_with_nested_apps(self):\n        \"\"\"\n        Regression test for #24601: ensure get_containing_app_config works\n        correctly when an object is nested within multiple app configs.\n        \"\"\"\n        with extend_sys_path(HERE):\n            with isolate_apps(\n                installed_apps=[\n                    \"apps.two_configs_one_default_app\",\n                    \"apps.my_nested_app\",\n                ]\n            ):\n                self.assertEqual(\n                    apps.get_containing_app_config(\"apps.two_configs_one_default_app.nested.ModelA\"),\n                    apps.get_app_config(\"apps.two_configs_one_default_app\"),\n                )\n", "    def test_get_registered_model(self):\n        apps.populate(SOME_INSTALLED_APPS)\n        with self.assertRaises(LookupError):\n            apps.get_registered_model(\"django.contrib.auth\", \"NoSuchModel\")\n        self.assertEqual(apps.get_registered_model(\"django.contrib.auth\", \"User\"),\n                         LogEntry._meta.model)\n", "    def test_get_app_configs_from_empty_registry(self):\n        empty_registry = Apps(installed_apps=[])\n        with self.assertRaises(AppRegistryNotReady):\n            empty_registry.check_apps_ready()\n        with self.assertRaises(AppRegistryNotReady):\n            list(empty_registry.get_app_configs())\n        self.assertEqual(empty_registry.get_app_config(\"nonexistent\"), None)\n"], "sample_123": ["    def test_urlencode_multivaluedict(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        expected = 'a=1&a=2&b=3'\n        self.assertEqual(urlencode(query), expected)\n", "    def test_urlencode_multivaluedict(self):\n        query = MultiValueDict({'a': ['b', 'c'], 'd': 'e'})\n        self.assertEqual(urlencode(query), 'a=b&a=c&d=e')\n", "    def test_urlencode_multivaluedict(self):\n        d = MultiValueDict({'a': ['1', '2'], 'b': '3'})\n        self.assertEqual(urlencode(d), 'a=1&a=2&b=3')\n        self.assertEqual(urlencode(d, doseq=True), 'a=1&a=2&b=3')\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({'a': ['1', '2'], 'b': ['3']})\n        self.assertEqual(\n            urlencode(data),\n            'a=1&a=2&b=3',\n        )\n", "    def test_urlencode_multivaluedict(self):\n        query = MultiValueDict({'a': ['1', '2'], 'b': ['3', '4']})\n        self.assertEqual(urlencode(query), 'a=1&a=2&b=3&b=4')\n\n        query = MultiValueDict({'a': '', 'b': ['3', '4']})\n        self.assertEqual(urlencode(query), 'a=&b=3&b=4')\n\n"], "sample_105": ["    def test_view_dispatch(self):\n        view = SimplePostView()\n        request = self.rf.post('/')\n        response = view(request)\n        self._assert_simple(response)\n", "    def test_decorated_dispatch_view(self):\n        view = DecoratedDispatchView()\n        request = self.rf.get('/')\n        response = view.dispatch(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(getattr(view, 'is_decorated'))\n", "    def test_dispatch_view_with_custom_http_methods(self):\n        view = SimplePostView()\n        request = self.rf.post('/test/')\n        response = view(request)\n        self._assert_simple(response)\n", "    def test_redirectview_permanent(self):\n        view = RedirectView.as_view(url='/permanent/')\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 308)\n        self.assertEqual(response['Location'], '/permanent/')\n\n", "    def test_view_as_view_accepts_initkwargs(self):\n        view = SimpleView.as_view(parameter='value')\n        self.assertEqual(view.view_initkwargs['parameter'], 'value')\n        instance = view(self.rf.get('/'))\n        self.assertEqual(instance.parameter, 'value')\n"], "sample_1155": ["    def test_construct_domain_complex_with_algebraic():\n        K, elements = construct_domain([sqrt(2) + 1, 2*I])\n        assert K == CC\n        assert elements == [CC(sqrt(2) + 1), CC(2*I)]\n\n", "    def test_construct_domain_complex_extension():\n        K, elements = construct_domain([sqrt(2) + I], extension=True)\n        assert K == QQ.algebraic_field((sqrt(2), sqrt(2)))\n        assert elements[0] == sqrt(2) + I\n", "def test_construct_domain_algebraic_extension():\n    K = QQ.algebraic_field((sqrt(2), S.Zero))\n    a = sqrt(2).as_algebraic_number(K).rep\n    assert construct_domain([sqrt(2) + 1], extension=True)[0].gen == a.gen  \n", "    def test_construct_domain_complex_algebraic():\n        domain, coeffs = construct_domain([sqrt(2) + 1j, sqrt(3) - I])\n\n        assert domain.dtype == domain.algebraic_field(\n            QQ[sqrt(2)], [sqrt(2) + 1j]\n        )[0] \n", "    def test_construct_domain_field_extension_imaginary():\n        from sympy.polys.numberfields import primitive_element\n        from sympy.abc import x\n        a = sqrt(2) + I\n        domain, coeffs = construct_domain([a], extension=True)\n        assert domain.is_AlgebraicField\n        assert domain.extension.gen == a\n\n"], "sample_1028": ["def test_mod_mul_symbols():\n    assert Mod(x*y, z) == Mod(x * y, z)\n    assert Mod(x*y, z).expand() == Mod(x * y, z)\n", "    def test_Mod_gcd():\n        p = Mod(x**2 + 2*x + 1, x + 1)\n        assert p.expand() == Mod(x(x + 2) + 1, x + 1)\n\n", "    def test_mod_nested():\n        assert Mod(Mod(x, y), z) == Mod(x, y*z)\n\n", "def test_mod_mul():\n    assert Mod(Mul(2, x), 3) == Mod(2*x, 3)\n    assert Mod(Mul(2, x, y), 3) == Mod(2*x*y, 3)\n    assert Mod(Mul(2, x, y), y) == Mod(2*x*y, y)\n", "    def test_mod_non_integer_divisor():\n        assert Mod(x, 2.0) == Mod(x, 2)\n        assert Mod(x, 2.5) == Mod(x, Rational(5, 2))\n        assert Mod(5, 2.0) == 1\n"], "sample_34": ["    def test_composite_unit_equality():\n        assert u.m / u.s == u.m / u.s\n        assert u.m / u.s != u.m * u.s\n\n        assert u.m / u.s != u.km / u.s\n        assert u.m / u.s != u.m / u.ms\n\n        assert u.m / u.s == u.cm / (u.ms / 1000)\n", "    def test_scaling_in_arithmetic():\n        m = u.m\n        km = u.km\n        # Check multiplication\n        assert (2 * m).to(km).value == 0.002\n        assert (2 * km * 1000).to(m).value == 2000000\n\n        # Check division\n        assert (1000 * m / km).value == 1000\n        assert (1000 * m / 2 / km).value == 500\n\n\n\n", "    def test_unit_equivalencies():\n        u.def_unit('special_unit', u.m / u.s)\n\n        with pytest.raises(u.UnitsError):\n            u.Quantity(1, 'special_unit').to(u.s)\n\n        eq = u.Equivalency(\n            [u.m / u.s, u.km / u.hour],\n            [u.m / u.s],\n        )\n        q = u.Quantity(1000, 'special_unit')\n\n        with catch_warnings() as w:\n            # ignore UserWarning: UnitsWarning\n            q.to(u.km / u.hour, equivalencies=eq)\n            assert len(w) == 1\n            assert w[0].category == UserWarning\n            assert w[0].message.args[0] == (\n                \"The unit 'special_unit' does not have a defined\"\n                \" conversion to 'km/hour'. Converting anyway based\"\n                \" on equivalency.\"\n            )\n\n        assert q.to(u.km / u.hour, equivalencies=eq) == 3.6\n\n", "    def test_unit_format_roundtrip():\n        u.Unit('m/s').format()\n        with catch_warnings(u.UnitsWarning) as w:\n            u.Unit('m/s', format='fits').format()\n        assert len(w) == 1\n", "    def test_quantity_creation_with_unit_str():\n        \"\"\"Test creating a Quantity directly from a string.\"\"\"\n        q = 10 * u.m\n        assert q.value == 10\n        assert q.unit == u.m\n        q2 = u.Quantity('10 m')\n        assert q2.value == 10\n        assert q2.unit == u.m\n"], "sample_820": ["    def test_voting_classifier_fit_raises_error_if_all_estimators_none(self):\n        # Test if fit raises an error if all estimators are None\n        estimators = [('lr', None), ('rf', None)]\n        clf = VotingClassifier(estimators=estimators, voting='hard')\n        with pytest.raises(ValueError):\n            clf.fit(X, y)\n", "    def test_voting_regressor_transform_with_weights():\n        clf1 = LinearRegression()\n        clf2 = RandomForestRegressor(n_estimators=5, random_state=1)\n        eclf = VotingRegressor(estimators=[('lr', clf1), ('rf', clf2)],\n                               weights=[2, 1])\n        eclf.fit(X_r, y_r)\n        predictions = eclf.transform(X_r)\n        assert predictions.shape == (len(y_r), 2)\n        assert_array_almost_equal(\n            np.average(predictions, axis=1, weights=[2, 1]),\n            eclf.predict(X_r))\n", "    def test_voting_classifier_predict_proba_hard_voting(self):\n        clf1 = LinearRegression()\n        clf2 = DummyRegressor()\n        eclf = VotingRegressor([('lr', clf1), ('dr', clf2)])\n        eclf.fit(X_r, y_r)\n        with pytest.raises(AttributeError) as exc:\n            eclf.predict_proba(X_r)\n        assert \"predict_proba is not available when voting='hard'\" in str(exc.value)\n", "    def test_voting_regressor_with_sample_weight(self):\n        # Test if sample weights are passed correctly to the estimators for\n        # VotingRegressor\n        clf1 = LinearRegression()\n        clf2 = RandomForestRegressor(n_estimators=10, random_state=1)\n        vr = VotingRegressor([('lr', clf1), ('rf', clf2)])\n\n        # Create sample weights\n        sample_weight = np.ones(X_r.shape[0])\n        sample_weight[:10] = 2\n\n        vr.fit(X_r, y_r, sample_weight=sample_weight)\n\n        # Check if each estimator was fitted with the correct sample weights\n        for est in vr.estimators_:\n            if hasattr(est, 'sample_weight') and est.sample_weight is not None:\n                assert_array_equal(est.sample_weight, sample_weight)\n\n        # Check if prediction works with sample weights\n        pred_with_weights = vr.predict(X_r)\n\n        # Predict without sample weights to compare\n        vr.fit(X_r, y_r)\n        pred_without_weights = vr.predict(X_r)\n\n        assert_array_almost_equal(pred_with_weights, pred_without_weights,\n                                decimal=2)\n", "    def test_voting_regressor_empty_input():\n        # Test VotingRegressor with empty estimators list\n        ereg1 = LinearRegression()\n        ereg2 = RandomForestRegressor(n_estimators=10, random_state=1)\n        with pytest.raises(AttributeError, match='Invalid `estimators`'):\n            VotingRegressor([]).fit(X_r, y_r)\n\n"], "sample_591": ["    def test_merge_different_chunks(self):\n        ds1 = create_test_data(\n            dims=[\"x\", \"y\"],\n            sizes={\"x\": 5, \"y\": 10},\n            chunks={\"x\": 2, \"y\": 5},\n        )\n        ds2 = create_test_data(\n            dims=[\"x\", \"y\"],\n            sizes={\"x\": 5, \"y\": 10},\n            chunks={\"x\": 3, \"y\": 5},\n        )\n        with pytest.raises(ValueError):\n            merge([ds1, ds2])\n", "    def test_merge_dataset_empty(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({'a': (['x'], [1, 2])})\n        actual = merge([ds1, ds2])\n        expected = ds2.copy()\n        assert_identical(actual, expected)\n\n", "    def test_merge_attrs(self):\n        ds1 = create_test_data(dims={\"x\": 10})\n        ds2 = create_test_data(dims={\"x\": 5}, attrs={\"foo\": \"bar\"})\n\n        actual = merge([ds1, ds2])\n\n        assert actual.attrs == ds1.attrs\n        assert \"foo\" in actual[ds2.data_vars[0].name].attrs\n        assert actual[ds2.data_vars[0].name].attrs[\"foo\"] == \"bar\"\n", "    def test_merge_empty(self):\n        ds1 = xr.Dataset()\n        ds2 = xr.Dataset({'a': (('x',), range(5))})\n        merged = merge([ds1, ds2])\n        assert_identical(merged, ds2)\n\n", "    def test_merge_conflicting_coords(self):\n        ds1 = xr.Dataset({'a': (('x',), [1, 2]),\n                          'b': (('x',), [3, 4])},\n                         coords={'x': [0, 1], 'c': ('x', [5, 6])})\n        ds2 = xr.Dataset({'a': (('x',), [7, 8]),\n                          'c': ('x', [9, 10])},\n                         coords={'x': [0, 1]})\n        with pytest.raises(ValueError, match=r\"conflicting values for coord.* 'c'\"):\n            merge([ds1, ds2])\n"], "sample_846": ["    def test_fit_transform_error_transform_no2d(self):\n        X = np.array([1, 2, 3])\n        transformer = ColumnTransformer(\n            transformers=[('trans', TransNo2D(), [0])],\n            remainder='passthrough')\n        with assert_raises(ValueError, match=_ERR_MSG_1DCOLUMN):\n            transformer.fit_transform(X)\n\n", "    def test_fit_transform_sparse_output(self):\n        n_samples = 10\n        n_features = 5\n\n        X = sparse.csr_matrix((n_samples, n_features))\n        trans1 = StandardScaler()\n        trans2 = OneHotEncoder()\n\n        ct = make_column_transformer(\n            (trans1, [0, 1]),\n            (trans2, [2, 3]),\n            remainder='passthrough'\n        )\n\n        X_t = ct.fit_transform(X)\n\n        assert (X_t.shape[0] == n_samples)\n        assert (X_t.shape[1] == n_features)\n        assert isinstance(X_t, sparse.csr_matrix)\n\n        X = sparse.csc_matrix((n_samples, n_features))\n        X_t = ct.fit_transform(X)\n\n        assert (X_t.shape[0] == n_samples)\n        assert (X_t.shape[1] == n_features)\n        assert isinstance(X_t, sparse.csc_matrix)\n\n", "    def test_column_transformer_sparse_output_threshold(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4], [5, 6]])\n        ct = ColumnTransformer([\n            ('trans1', Trans(), [0]),\n            ('trans2', Trans(), [1])\n        ], sparse_threshold=0.5)\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, sparse.csr_matrix)\n\n        ct = ColumnTransformer([\n            ('trans1', Trans(), [0]),\n            ('trans2', Trans(), [1])\n        ], sparse_threshold=0.1)\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, np.ndarray)\n", "    def test_column_transformer_sparse_output_threshold():\n        # Test the sparse_threshold parameter for ColumnTransformer.\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        sparse_X = sparse.csr_matrix(X)\n\n        # Transforms to dense output,\n        ct = ColumnTransformer(\n            transformers=[('trans1', StandardScaler(), [0]),\n                          ('trans2', OneHotEncoder(handle_unknown='ignore'), [1])],\n            sparse_threshold=0.0,\n            remainder='passthrough'\n        )\n        dense_output = ct.fit_transform(X)\n        assert isinstance(dense_output, np.ndarray)\n\n        # Transforms to sparse output\n        ct = ColumnTransformer(\n            transformers=[('trans1', StandardScaler(), [0]),\n                          ('trans2', OneHotEncoder(handle_unknown='ignore'), [1])],\n            sparse_threshold=0.1,\n            remainder='passthrough'\n\n        )\n        sparse_output = ct.fit_transform(sparse_X)\n        assert isinstance(sparse_output, sparse.csr_matrix)\n\n        # Test with only one transformer outputing sparse\n        ct = ColumnTransformer(\n            transformers=[('trans1', StandardScaler(), [0]),\n                          ('trans2', OneHotEncoder(handle_unknown='ignore'), [1])],\n            sparse_threshold=0.1,\n            remainder='drop'\n        )\n        sparse_output = ct.fit_transform(sparse_X)\n        assert isinstance(sparse_output, sparse.csr_matrix)\n\n", "    def test_fit_transform_sparse_input_remainder_passthrough(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        transformer = make_column_transformer((StandardScaler(), [0]),\n                                              remainder='passthrough')\n        Xt = transformer.fit_transform(X)\n        expected_Xt = np.array([[-1.22474487, 2],\n                                [ 1.22474487, 4]])\n        assert_allclose_dense_sparse(Xt, expected_Xt)\n"], "sample_512": ["    def test_xlabel():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3, 4])\n        ax.set_xlabel('My XLabel')\n        assert ax.get_xlabel() == 'My XLabel'\n        ax.set_xlabel('New Label')\n        assert ax.get_xlabel() == 'New Label'\n", "    def test_plot_date():\n        fig, ax = plt.subplots()\n        dates = [\n            datetime.datetime(2023, 10, 26),\n            datetime.datetime(2023, 10, 27),\n            datetime.datetime(2023, 10, 28),\n        ]\n        values = [10, 15, 12]\n        ax.plot_date(dates, values, fmt=\"o-\", linestyle=\"-\")\n        plt.xticks(rotation=45)\n        # Save the plot to a temporary file\n        tmpfile = Path(\"test.png\")\n        fig.savefig(tmpfile)\n        # Compare the generated plot to a reference image\n        subprocess.run([\"convert\", tmpfile, \"test.png\"], check=True)\n\n        with open(\"test.png\", \"rb\") as f:\n            actual = f.read()\n        with open(os.path.join(os.path.dirname(__file__), \"baseline\", \"test_plot_date.png\"), \"rb\") as f:\n            expected = f.read()\n        assert actual == expected\n\n        os.remove(tmpfile)\n", "    def test_figure_figsize_defaults():\n        fig, ax = plt.subplots()\n        assert fig.get_size_inches() == (6.4, 4.8)\n", "    def test_plot_data_argument():\n        x = np.arange(5)\n        y = x**2\n        fig, ax = plt.subplots()\n        ax.plot(x, y, data={'x': x, 'y': y})\n\n        assert len(ax.lines) == 1\n        line = ax.lines[0]\n        assert np.allclose(line.get_xdata(), x)\n        assert np.allclose(line.get_ydata(), y)\n\n", "    def test_set_title():\n        fig, ax = plt.subplots()\n        ax.set_title('Hello')\n        assert ax.title.get_text() == 'Hello'\n\n"], "sample_736": ["    def test_logisticregressioncv_error_handling_with_sample_weights(self):\n\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        sample_weight = np.array([0, 1, 0])  \n\n        clf = LogisticRegressionCV()\n        with assert_raises(ValueError):\n            clf.fit(X, y, sample_weight=sample_weight)\n", "    def test_logistic_regression_cv_multiclass_predict_proba(self):\n        # Test predict_proba for multiclass LogisticRegressionCV\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_classes=3,\n                                   random_state=0)\n        model = LogisticRegressionCV(multi_class='multinomial',\n                                      solver='lbfgs',\n                                      cv=StratifiedKFold(5),\n                                      random_state=0).fit(X, y)\n        proba = model.predict_proba(X)\n        assert_almost_equal(proba.sum(axis=1), np.ones(len(X)))\n        assert_equal(proba.shape, (len(X), 3))\n", "    def test_multi_class_ovr_cv_fit(self):\n        # Test multi-class OvR with custom cross-validation\n        # This test checks if LogisticRegressionCV works correctly\n        # when given a custom cross-validator and a multi-class problem\n\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   n_informative=10, n_classes=3, random_state=0)\n        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n        clf = LogisticRegressionCV(multi_class='ovr', solver='lbfgs', cv=cv,\n                                   random_state=0)\n        clf.fit(X, y)\n\n        assert_equal(len(clf.coef_), 3)  # One coef for each class\n        assert_equal(clf.coef_.shape[1], 20)\n        assert_array_equal(clf.classes_, np.array([0, 1, 2]))\n\n", "    def test_predict_proba_multiclass_ovr(self):\n        # Test predict_proba for multiclass with ovr\n        clf = LogisticRegression(multi_class='ovr', solver='liblinear')\n        clf.fit(X, Y2)\n        probs = clf.predict_proba(X)\n        assert_almost_equal(probs.sum(axis=1), np.ones(len(X)))\n", "    def test_solver_liblinear_multiclass_ovr(self):\n        # Test liblinear solver with multiclass='ovr'\n        X, y = make_classification(n_samples=100, n_features=20, random_state=42,\n                                   n_classes=3, n_informative=10)\n\n        # Check with different Cs\n        for C in [1e-4, 1e-2, 1, 10]:\n            clf = LogisticRegression(solver='liblinear', multi_class='ovr',\n                                    C=C, random_state=42)\n            clf.fit(X, y)\n            check_predictions(clf, X, y)\n\n        # Test with penalty='l1'\n        clf = LogisticRegression(solver='liblinear', penalty='l1',\n                                    multi_class='ovr', random_state=42)\n        clf.fit(X, y)\n        check_predictions(clf, X, y)\n"], "sample_955": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_717": ["def test_fetch_lfw_people_min_faces_per_person():\n    # Test that the min_faces_per_person parameter works correctly\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                  funneled=True,\n                                  min_faces_per_person=2)\n    assert lfw_people.data.shape[0] == sum(\n        count for count in counts.values() if count >= 2\n    )\n    assert len(lfw_people.target_names) == len(\n        [name for name, count in six.iteritems(counts) if count >= 2]\n    )\n\n\n\n", "def test_fetch_lfw_people_min_faces_per_person():\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                 funneled=True,\n                                 min_faces_per_person=2,\n                                 download_if_missing=True)\n\n    assert len(lfw_people.images) > 0\n    assert len(lfw_people.target_names) == len(FAKE_NAMES)\n\n    # Check only people with more than 1 face are retained\n    for name in lfw_people.target_names:\n        assert counts[name] >= 2\n", "def test_fetch_lfw_people_min_faces_per_person():\n    \"Test that fetch_lfw_people filters people with too few pictures\"\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                 funneled=True,\n                                 min_faces_per_person=2)\n    # At least one fake person has less than 2 images\n    assert len(lfw_people.target_names) < len(FAKE_NAMES)\n\n\n", "def test_lfw_people_min_faces_per_person():\n    # Smoke test for the min_faces_per_person argument,\n    # make sure that the dataset loader excludes people with few pictures\n\n    lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                 min_faces_per_person=2, color=False)\n    assert lfw_people.target.shape[0] < len(FAKE_NAMES) * 5\n\n", "    def test_fetch_lfw_people_min_faces_per_person(self):\n        lfw_people = fetch_lfw_people(data_home=SCIKIT_LEARN_DATA,\n                                     min_faces_per_person=2)\n        assert_equal(len(lfw_people.target_names), 4)\n        self.assertGreater(len(lfw_people.data), 0)\n"], "sample_454": ["    def test_exclusion_constraint_spgist_include(self):\n        with atomic():\n            Product.objects.create(name='Product 1', price=10.00)\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(name='Product 1', price=10.00)\n\n", "    def test_exclusion_constraint_with_condition(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(name=\"A\", price=10)\n            with self.assertRaises(ValidationError):\n                UniqueConstraintProduct.objects.create(name=\"a\", price=10)\n            UniqueConstraintProduct.objects.create(name=\"B\", price=20)\n        with self.assertNumQueries(0):\n            constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n        self.assertEqual(len(constraints), 2)\n\n\n", "    def test_exclusion_constraint_include_spgist(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE test_include_spgist (\n                    id SERIAL PRIMARY KEY,\n                    geom GEOMETRY(POINT, 4326),\n                    other_column TEXT\n                )\n            \"\"\"\n            )\n        model = type(\n            'TestIncludeSpgist',\n            (models.Model,),\n            {'objects': models.Manager.from_queryset_and_model(\n                get_queryset_klass(),\n                model=None,\n            ),\n             'id': models.AutoField(primary_key=True),\n             'geom': models.PointField(srid=4326),\n             'other_column': models.TextField(),\n            },\n        )\n\n        constraint = ExclusionConstraint(\n            name='test_include_constraint',\n            expressions=[('geom', '&&')],\n            index_type='SPGIST',\n            include=['other_column'],\n        )\n        model._meta.constraints = [constraint]\n        model._meta._constraint_list = [constraint]\n\n        model.objects.create(geom='POINT(0 0)', other_column='test')\n        model.objects.create(geom='POINT(1 1)', other_column='test2')\n\n        # Verify the constraint was created with include\n        table_constraints = get_constraints(model._meta.db_table)\n        constraint = [\n            c for c in table_constraints\n            if c['name'] == 'test_include_constraint'\n        ][0]\n        self.assertIn('INCLUDE (other_column)', constraint['sql'])\n", "    def test_exclusion_constraint_include(self):\n        with atomic():\n            UniqueConstraintInclude.objects.create(name='product_a', value=1)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintInclude.objects.create(name='product_a', value=1)\n            \n            UniqueConstraintInclude.objects.create(name='product_b', value=2)\n\n", "    def test_exclusion_constraint_with_include(self):\n        with self.atomic():\n            UniqueConstraintInclude.objects.create(\n                col1=\"foo\", col2=\"bar\", col3=\"baz\"\n            )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintInclude.objects.create(\n                    col1=\"foo\", col2=\"bar\", col3=\"baz\"\n                )\n            constraints = get_constraints(UniqueConstraintInclude._meta.db_table)\n            self.assertEqual(len(constraints), 1)\n            constraint = constraints[0]\n            self.assertEqual(constraint[\"name\"], \"unique_constraint_include\")\n            self.assertEqual(constraint[\"type\"], \"UNIQUE\")\n            self.assertIn(\"INCLUDE (col3)\", constraint[\"sql\"])\n"], "sample_125": ["    def test_set_cookie_expires_naive_datetime(self):\n        with freeze_time(datetime(2023, 10, 26, 12, 0, 0)):\n            response = HttpResponse()\n            response.set_cookie(\n                'test_cookie',\n                expires=datetime(2023, 10, 27, 12, 0, 0),\n            )\n            self.assertEqual(response.cookies['test_cookie']['expires'], http_date(\n                datetime(2023, 10, 27, 12, 0, 0).replace(tzinfo=utc)\n            ))\n", "    def test_samesite_cookie(self):\n        response = HttpResponse()\n        response.set_cookie('foo', samesite='lax')\n        self.assertEqual(response.cookies['foo']['samesite'], 'Lax')\n        response.set_cookie('bar', samesite='strict')\n        self.assertEqual(response.cookies['bar']['samesite'], 'Strict')\n        with self.assertRaisesMessage(ValueError, 'samesite must be \"lax\", \"none\", or \"strict\".'):\n            response.set_cookie('baz', samesite='invalid')\n", "    def test_set_cookie_expires_naive_datetime(self):\n        response = HttpResponse()\n        with freeze_time(datetime(2023, 10, 27, 10, 0, 0)):\n            response.set_cookie('mycookie', expires=datetime.now() + timedelta(days=1))\n        self.assertEqual(response.cookies['mycookie']['expires'], http_date(datetime(2023, 10, 28, 10, 0, 0)))\n", "    def test_set_cookie_expires(self):\n        response = HttpResponse()\n        now = datetime.now(utc)\n        response.set_cookie(\n            'test', 'value', expires=now + timedelta(days=1)\n        )\n        self.assertEqual(response['Set-Cookie'],\n                         f'test=value; expires={http_date(now + timedelta(days=1))}')", "    def test_set_cookie_expires_datetime(self):\n        response = HttpResponse()\n        now = datetime.now(utc)\n        expires = now + timedelta(days=1)\n        response.set_cookie('mycookie', expires=expires)\n        cookie = response.cookies['mycookie']\n        self.assertEqual(cookie.value, '')\n        self.assertEqual(cookie['expires'], http_date(expires.timestamp()))\n"], "sample_1174": ["def test_unpolarify_principal_branch():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert unpolarify(principal_branch(x, 2*pi)) == x\n    assert unpolarify(principal_branch(y*I*x, 2*pi)) == y*I*x\n    assert unpolarify(principal_branch(x*exp_polar(2*pi*I*y), 2*pi)) == x*exp(2*pi*I*y)\n", "    def test_periodic_argument_positive_period():\n        x = Symbol('x', real=True)\n        assert periodic_argument(exp(2*pi*I*x), 2*pi) == 2*pi*x\n\n", "def test_principal_branch_complexification():\n    z = Symbol('z')\n    assert unpolarify(principal_branch(z*exp_polar(I*pi/2), 2*pi), exponents_only=True) == z*exp_polar(I*pi/2)\n", "    def test_periodic_argument_basic():\n        x = symbols('x')\n        assert periodic_argument(exp(2*pi*I*x), 2*pi) == 0\n        assert periodic_argument(exp(4*pi*I*x), 2*pi) == 0\n        assert periodic_argument(exp(pi*I*x), 2*pi) == pi*x\n\n        assert periodic_argument(exp(2*pi*I*x), pi) == 0\n        assert periodic_argument(exp(4*pi*I*x), pi) == 0\n        assert periodic_argument(exp(pi*I*x), pi) == pi*x % pi  \n\n", "def test_polar_lift():\n    x = Symbol('x')\n    y = Symbol('y', polar=True)\n    assert polar_lift(2) == 2*exp_polar(0)\n    assert polar_lift(-2) == 2*exp_polar(I*pi)\n    assert polar_lift(x) == polar_lift(x)\n    assert polar_lift(y) == y\n\n"], "sample_863": ["    def test_pipeline_with_fitintransform_transformer():\n        # Make sure Pipeline works with transformers that implement\n        # fit_transform instead of fit and transform\n        clf = Pipeline([\n            ('scaler', StandardScaler()),\n            ('svd', TruncatedSVD(n_components=2)),\n            ('clf', LogisticRegression())\n        ])\n        clf.fit(iris.data, iris.target)\n        clf.predict(iris.data)\n\n\n", "def test_pipeline_fit_predict_with_fit_params():\n    # Test that fit_predict passes fit_params to the estimator\n\n    clf = FitParamT()\n    pipe = Pipeline([('clf', clf)])\n\n    # Should succeed\n    pipe.fit_predict(iris.data, iris.target, should_succeed=True)\n    assert clf.successful\n\n    # Should fail\n    pipe.fit_predict(iris.data, iris.target, should_succeed=False)\n    assert not clf.successful\n", "    def test_pipeline_cache(self):\n        # Test caching of fitted transformers within a pipeline\n        # Make sure the transformer is fitted only once when using 'cachedir'\n\n        # Create a pipeline with a transformer that stores a timestamp on fit\n        pipe = Pipeline([('dummy', DummyTransf())])\n\n        # First fit, should trigger a cache write\n        pipe.fit(iris.data)\n        timestamp1 = pipe.named_steps['dummy'].timestamp_\n\n        # Second fit with the same cachedir, should load from cache\n        pipe.fit(iris.data)\n        timestamp2 = pipe.named_steps['dummy'].timestamp_\n        assert timestamp1 == timestamp2\n\n        # Now, change the cachedir, should trigger refitting\n        cachedir = mkdtemp()\n        pipe = Pipeline([('dummy', DummyTransf())], memory=cachedir)\n        pipe.fit(iris.data)\n        timestamp3 = pipe.named_steps['dummy'].timestamp_\n        assert timestamp1 != timestamp3\n        shutil.rmtree(cachedir)\n", "def test_pipeline_with_cache():\n    \"\"\"Test pipeline with caching.\n\n    Checks if caching work properly, including the case of fitted transformers.\n    \"\"\"\n    # Memory caching\n    cachedir = mkdtemp()\n    mem = joblib.Memory(cachedir=cachedir, verbose=0)\n    pipeline = Pipeline([('scaler', StandardScaler()),\n                         ('clf', SVC())], memory=mem)\n\n    pipeline.fit(iris.data, iris.target)\n\n    # Check if the fitted scaler is cached\n    assert isinstance(pipeline.named_steps['scaler'].__getstate__()[\n                      'memory'], joblib.Memory)\n    assert pipeline.named_steps['scaler'].__getstate__()['memory'].cachedir == cachedir\n    shutil.rmtree(cachedir)\n\n", "def test_pipeline_caching():\n    # Test that fit() method caches the fitted transformers.\n\n    # Define a transformer that stores a timestamp on fit.\n    transformer = DummyTransf()\n\n    # Define a pipeline with the transformer\n    pipeline = Pipeline([('dummy', transformer)])\n\n    # Fit the pipeline once\n    pipeline.fit(iris.data)\n\n    # Store the timestamp of the first fit\n    initial_timestamp = transformer.timestamp_\n\n    # Fit the pipeline again\n    pipeline.fit(iris.data)\n\n    # The timestamp should not have changed, indicating the transformer\n    # was not refitted\n    assert transformer.timestamp_ == initial_timestamp\n"], "sample_605": ["    def test_groupby_sum_keep_attrs(array):\n        expected = array.sum(dim=\"x\", keep_attrs=True)\n        grouped = array.groupby(\"x\")\n        result = grouped.sum(keep_attrs=True)\n        assert_identical(result, expected)\n", "def test_groupby_size(dataset):\n    expected_size = dataset.groupby(\"x\").size()\n    actual_size = dataset.groupby(\"x\").apply(lambda x: len(x))\n    assert_identical(expected_size, actual_size)\n\n", "def test_groupby_reduce_empty(dataset):\n    # Check that reduce operations work correctly on empty groups\n    ds = dataset.where(dataset.x != 'a')\n    expected = ds.groupby('x').sum()\n\n    # Test that groupby on an empty dataset returns an empty dataset.\n    actual = ds.groupby('x').sum()\n\n    assert expected.equals(actual)\n\n", "def test_groupby_assign_coords(dataset):\n    grouped = dataset.groupby(\"x\")\n    expected = grouped.map(lambda ds: ds.assign_coords(a=1))\n    expected[\"a\"] = 1\n    actual = grouped.assign_coords(a=1)\n    assert_identical(actual, expected)\n", "def test_groupby_reduce_empty_groups(array):\n    array['u'] = (\n        (\"x\", \"y\"),\n        [\n            [1, 2, 3],\n            [4, 5, 6],\n            [7, 8, 9],\n        ],\n    )\n    grouped = array.groupby(\"u\")\n    result = grouped.sum()\n    expected = array.sum(dim=[\"y\", \"x\"])\n    assert_identical(result, expected)\n"], "sample_234": ["    def test_union(self):\n        queryset1 = Number.objects.filter(num__lt=5)\n        queryset2 = Number.objects.filter(num__gt=5)\n        union_queryset = queryset1.union(queryset2)\n        self.assertNumbersEqual(union_queryset, Number.objects.all())\n", "    def test_union_with_annotation(self):\n        qs1 = Number.objects.annotate(total=F('num') + F('other_num'))\n        qs2 = Number.objects.annotate(total=F('num') * F('other_num'))\n        union_qs = qs1.union(qs2)\n\n        self.assertQuerysetEqual(\n            union_qs,\n            Number.objects.values('id', 'num', 'other_num').annotate(total=F('num') + F('other_num')).order_by('id'),\n            operator.attrgetter('num', 'other_num', 'total'),\n        )\n", "    def test_union_with_different_orders(self):\n        number_set1 = Number.objects.all().order_by('num')\n        number_set2 = Number.objects.all().order_by('-num')\n        combined = number_set1.union(number_set2)\n        self.assertNumbersEqual(combined, list(range(10)), ordered=False)\n", "    def test_union_with_different_fields(self):\n        numbers1 = Number.objects.all()[:5].values('num')\n        numbers2 = Number.objects.all()[5:].values('other_num')\n        combined = numbers1.union(numbers2)\n        self.assertEqual(combined.count(), 10)\n        self.assertNumbersEqual(combined, [\n            Number(num=0), Number(num=1), Number(num=2), Number(num=3), Number(num=4),\n            Number(other_num=5), Number(other_num=4), Number(other_num=3), Number(other_num=2), Number(other_num=1),\n        ], ordered=False)\n\n", "    def test_union_with_same_model(self):\n        qs1 = Number.objects.filter(num__lt=5)\n        qs2 = Number.objects.filter(num__gt=5)\n        self.assertNumbersEqual(qs1.union(qs2), [Number.objects.get(num=i) for i in range(10) if i < 5 or i > 5])\n"], "sample_903": ["    def test_tsne_sparse_input(self):\n        X = sp.csr_matrix(make_blobs(n_samples=50, n_features=10,\n                              random_state=0)[0])\n        tsne = TSNE(n_components=2, method='exact')\n        tsne.fit(X)\n\n", "    def test_perplexity(self):\n        X = make_blobs(n_samples=100, random_state=42)[0]\n        # Test binary search perplexity\n        perplexity_list = [5, 10, 20, 40]\n        for perplexity in perplexity_list:\n            n = X.shape[0]\n            perplexity_estimate = _binary_search_perplexity(X, perplexity,\n                                                          n_components=2,\n                                                          random_state=42)\n            assert_almost_equal(perplexity_estimate, perplexity, decimal=1)\n\n", "    def test_perplexity_search_small_dataset(self):\n        X = make_blobs(n_samples=20, centers=2, random_state=0)[0]\n        # Test with a small perplexity, should result in a single neighbor for\n        # most points.\n        perplexity = 2\n        P = _joint_probabilities(\n            squareform(pdist(X)), perplexity, verbose=False)\n        # Test that the sum of each row is close to 1\n        assert_almost_equal(np.sum(P, axis=1), np.ones(X.shape[0]), decimal=2)\n        # Test that the sum of all probabilities is close to 2 * number of\n        # points as we expect a single neighbor for most points\n\n        assert_almost_equal(np.sum(P), 2 * X.shape[0] , decimal=2)\n\n\n\n", "    def test_barnes_hut_tsne_sparse(self):\n        # Test that TSNE with sparse input works with barnes_hut.\n        X = sp.csr_matrix(make_blobs(n_samples=100, n_features=2,\n                                   random_state=42)[:, :2].astype(float))\n        tsne = TSNE(n_components=2, method='barnes_hut', random_state=42)\n        tsne.fit(X)\n\n", "    def test_fit_transform_sparse(self):\n        # Test fit_transform with sparse input\n        random_state = check_random_state(0)\n        X = random_state.rand(10, 10)\n        X_sparse = sp.csr_matrix(X)\n\n        tsne = TSNE(n_components=2, metric='euclidean',\n                    method='exact', random_state=random_state)\n        # The check for sparse input is done in the _fit method.\n        embedding = tsne.fit_transform(X_sparse)\n        assert_equal(embedding.shape, (10, 2))\n\n"], "sample_1173": ["def test_convert_equals_signs_nested():\n    assert parse_expr('(1=2)=False', transformations=(standard_transformations + (convert_equals_signs,))) == Eq(Eq(1,2), False)\n", "    def test_parse_expr_evaluate_false():\n        expr = parse_expr('2**3', evaluate=False)\n        assert isinstance(expr, ast.Call)\n        assert expr.func.id == 'Pow'\n        assert expr.args[0].n == 2\n        assert expr.args[1].n == 3\n", "    def test_parse_expr_nested_functions_evaluate_false():\n        expr = parse_expr('sin(exp(x))', evaluate=False)\n        assert isinstance(expr, Function)\n        assert expr.func.name == 'sin'\n        assert isinstance(expr.args[0], Function)\n        assert expr.args[0].func.name == 'exp'\n        assert isinstance(expr.args[0].args[0], Symbol)\n\n", "    def test_convert_equals_signs_nested():\n        assert parse_expr(\"(1*2=x)=False\", transformations=(\n            standard_transformations + (convert_equals_signs,))) == Eq(Eq(2, x), False)\n", "def test_factorial2():\n    assert parse_expr(\"5!!\") == factorial2(5)\n    assert parse_expr(\"(5)!!\") == factorial2(5)\n    assert parse_expr(\"x!!\") == factorial2(Symbol('x'))\n    assert parse_expr(\"2x!!\") == Mul(2, factorial2(Symbol('x')))\n\n"], "sample_656": ["    def test_capture_binary(self, tmpdir):\n        capman = CaptureManager(None)\n        with tmpdir.as_cwd():\n            with open(\"hello\", \"wb\") as f:\n                f.write(b\"hello\")\n            # Simulate a subprocess call\n            proc = subprocess.Popen([\"cat\", \"hello\"], stdout=subprocess.PIPE)\n            out, _ = proc.communicate()\n            assert out == b\"hello\"\n\n            capman.start_capturing()\n            try:\n                proc = subprocess.Popen([\"cat\", \"hello\"], stdout=subprocess.PIPE)\n                out, _ = proc.communicate()\n                assert out == b\"hello\"\n            finally:\n                capman.stop_capturing()\n            captured = capman.read_global_capture()\n            assert captured.out == b\"hello\"\n", "    def test_stdcapture_binary(self, tmpdir):\n        s = tmpdir.join(\"out\")\n        os.environ[\"FOO\"] = \"bar\"\n        code = textwrap.dedent(\n            \"\"\"\n            import os\n            print(\"stdout\")\n            print(\"stderr\", file=sys.stderr)\n            with open(\"%s\", \"wb\") as f:\n                f.write(b\"binarycontent\")\n        \"\"\"\n        ) % s\n        res = subprocess.run([sys.executable, \"-c\", code], capture_output=True)\n        res = capture.CaptureResult(*res.stdout.splitlines())  \n        assert res.out == b\"stdout\\n\"\n        assert res.err == b\"stderr\\n\"\n        with open(s, \"rb\") as f:\n            assert f.read() == b\"binarycontent\"\n\n", "    def test_capture_manager_setup_teardown(self, tmpdir):\n        tmpdir.join(\"foo.py\").write(textwrap.dedent(\"\"\"\n            import sys\n            print(\"hello\")\n            sys.exit(42)\n        \"\"\"))\n        config = pytest.Config()\n        config.option.capture = \"sys\"\n        capture.patchsysdict = {'stdout': 'stdout'}\n\n            capturemanager = CaptureManager(config)\n            capturemanager.start_capturing()\n            return capturemanager\n\n            capturemanager.stop_capturing()\n\n        # simulate setup/teardown inside pytest run\n        capturemanager = my_setup()\n        try:\n            ret = subprocess.check_output([sys.executable, str(tmpdir.join(\"foo.py\"))],\n                                         stderr=subprocess.STDOUT)\n        finally:\n            my_teardown(capturemanager)\n\n        assert capturemanager.is_globally_capturing() is False\n        out, err = capturemanager.read_global_capture()\n        assert out.endswith(b\"hello\\n\")\n        assert err == b\"\"\n        assert ret.endswith(b\"Exit code: 42\\n\")\n\n", "    def test_capture_manager_global_capture(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                print(\"hello\")\n                assert capsys.readouterr().out == \"hello\\\\n\"\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == EXIT_NOTESTSCOLLECTED\n", "    def test_capture_output_is_a_namedtuple(self, tmpdir):\n        with StdCapture() as cap:\n            print(\"hello\")\n        out, err = cap.readouterr()\n        assert isinstance(cap.readouterr(), capture.CaptureResult)\n        assert out == \"hello\\n\"\n        assert err == \"\"\n\n"], "sample_585": ["    def test_groupby_apply_empty_groups():\n        # regression test for https://github.com/pydata/xarray/issues/2600\n        ds = xr.Dataset({'a': (['x', 'y'], np.arange(4).reshape(2, 2))})\n        ds = ds.assign_coords({'x': [0, 1], 'y': ['a', 'b']})\n        expected = ds.groupby('y').apply(lambda ds: ds['a'] * 2)\n\n        # make sure we handle empty groups correctly\n        ds = ds.where(ds['x'] > 0)\n        result = ds.groupby('y').apply(lambda ds: ds['a'] * 2)\n        assert_identical(result, expected)\n", "    def test_groupby_apply_reduce():\n        ds = xr.Dataset(\n            {'foo': (('x', 'y'), np.arange(12).reshape(3, 4)),\n             'bar': (('x', 'y'), np.arange(12).reshape(3, 4) * 2),\n             'baz': ('x', list('abc')),\n            },\n            coords={'x': np.arange(3), 'y': np.arange(4)},\n        )\n\n        expected = ds.groupby('baz').mean()\n        result = ds.groupby('baz').apply(lambda x: x.mean(dim=['x', 'y']))\n\n        assert_identical(result, expected)\n", "    def test_groupby_reduce_with_empty_groups(self):\n        data = {'x': [1, 2, 3, 4, 5],\n                'y': [6, 7, 8, 9, 10]}\n        ds = xr.Dataset(data, coords={'labels': [\n            'a', 'a', 'b', 'c', 'c']})\n\n        expected = ds.groupby('labels').sum()\n        actual = ds.groupby('labels').reduce(\n            lambda x: x.sum(), dim='labels', allow_lazy=True).compute()\n\n        assert_identical(expected, actual)\n", "def test_groupby_apply_shortcut_with_coords():\n    arr = np.arange(2 * 3 * 4).reshape((2, 3, 4))\n    ds = xr.Dataset({'foo': (['x', 'y', 'z'], arr)},\n                    coords={'x': [0, 1], 'y': range(3), 'z': range(4)})\n\n        return ds.foo + 1\n\n    expected = ds.apply(func)\n    actual = ds.groupby('x').apply(func, shortcut=True)\n    assert_identical(expected, actual)\n\n", "    def test_groupby_reduce_with_existing_dim(self, dim):\n        ds = xr.Dataset({'foo': (['time', 'dim_2'], np.random.randn(4, 3))},\n                         coords={'time': pd.date_range('2000-01-01', periods=4),\n                                 'dim_2': np.arange(3)})\n        expected = ds.groupby('time').mean(dim='dim_2')\n        actual = ds.groupby('time').reduce(np.mean, dim=dim, allow_lazy=True).compute()\n        assert_identical(actual, expected)\n"], "sample_479": ["    def test_rename_index_then_remove(self):\n        # Reverse these operations, as the optimizer shouldn't care about the\n        # order\n        remove_index = operations.RemoveIndex('app', 'Model', 'idx_name')\n        rename_index = operations.RenameIndex(\n            'app', 'new_name', old_name='idx_name'\n        )\n        self.assertOptimizesTo(\n            [remove_index, rename_index], [remove_index], app_label=\"app\"\n        )\n", "    def test_rename_index_with_fields(self):\n        # Test that renaming an index with fields specified works as expected\n        before = [\n            operations.CreateModel(\n                name='UnicodeModel',\n                fields=[\n                    models.CharField(name='name', max_length=100),\n                ],\n                options={'db_table': 'unicode_model'},\n            ),\n            operations.CreateIndex(\n                model_name='UnicodeModel',\n                index=models.Index(fields=['name'], name='my_index'),\n            )\n        ]\n\n        after = [\n            operations.RenameIndex(\n                model_name='UnicodeModel',\n                new_name='new_index_name',\n                old_fields=['name'],\n            )\n        ]\n\n        self.assertOptimizesTo(before, after)\n", "    def test_index_rename_then_delete(self):\n        # Test case: Rename an index, then delete it. The delete should stand alone.\n        before = [\n            operations.RenameIndex(\n                \"UnicodeModel\", new_name=\"new_index\", old_name=\"old_index\"\n            ),\n            operations.RemoveIndex(\"UnicodeModel\", name=\"new_index\"),\n        ]\n        after = [operations.RemoveIndex(\"UnicodeModel\", name=\"new_index\")]\n\n        self.assertOptimizesTo(before, after)\n\n", "    def test_alter_unique_together_empty(self):\n        # Regression test for #24905\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            other_field = models.CharField(max_length=100)\n\n        before = [\n            migrations.CreateModel(\n                name='MyModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=100)),\n                    ('other_field', models.CharField(max_length=100)),\n                ],\n            ),\n        ]\n        after = [\n            migrations.CreateModel(\n                name='MyModel',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=100)),\n                    ('other_field', models.CharField(max_length=100)),\n                ],\n                options={'unique_together': {('name', 'other_field')}},\n            ),\n        ]\n        self.assertOptimizesTo(\n            operations.operations_from_migration_plan(before, after, 'test'),\n            [operations.AlterModelOptions(\n                name='MyModel',\n                options={'unique_together': set([('name', 'other_field')])},\n            )],\n            app_label='migrations',\n        )\n", "    def test_rename_index_with_old_fields(self):\n        before = [\n            operations.CreateModel(\n                name='UnicodeModel',\n                fields=[\n                    models.CharField(name='name', max_length=255),\n                    models.CharField(name='slug', max_length=255, unique=True),\n                ],\n            ),\n            operations.RunPython(\n                code=lambda apps, schema_editor: schema_editor.create_unique_index(\n                    UnicodeModel._meta.db_table,\n                    [UnicodeModel._meta.get_field('name'), UnicodeModel._meta.get_field('slug')]\n                )\n            ),\n        ]\n        after = [\n            operations.RenameIndex(\n                model_name='UnicodeModel',\n                new_name='unique_name_slug',\n                old_fields=['name', 'slug'],\n            ),\n        ]\n        self.assertOptimizesTo(before, after)\n\n"], "sample_1088": ["def test_symmetrize_formal():\n    x, y = symbols('x y')\n    f = x**2 + y**2\n    result = symmetrize(f, formal=True)\n    assert result[0] == (x + y)**2 - 2*x*y\n    assert result[2] == [(x + y, s1), (x*y, s2)]\n\n", "    def test_symmetrize_formal(self):\n        x, y = symbols('x y')\n        f = x**2 + 2*x*y + y**2\n        symm, polys = symmetrize(f, formal=True)\n        assert symm == (s1**2, 0)\n        assert polys == [(s1, x + y), (s2, x*y)]\n        assert symm[0].subs(polys[0][0], polys[0][1]) == f\n", "def test_viete_non_constant():\n    x, r1, r2 = symbols('x r1 r2')\n    f = x**2 + 2*x + 1\n    assert viete(f, [r1, r2], x) == [(r1 + r2, -2), (r1*r2, 1)]\n", "def test_symmetrize_formal():\n    x, y = symbols('x y')\n    F = x**2 + y**2 + 2*x*y\n\n    sym, polys = symmetrize(F, x, y, formal=True)\n    assert sym == (\n        S(s1**2 - 2*s2), S(0), [(s1, x + y), (s2, x*y)])\n\n", "    def test_symmetrize_formal(self):\n        x, y = symbols('x y')\n        F = x**2 + y**2\n        sym, _ = symmetrize(F, formal=True)\n        assert sym == ( -2*x*y + (x + y)**2, 0)\n\n\n\n"], "sample_549": ["    def test_masked_array_2d(self):\n        x = np.linspace(0, 10, 11)\n        y = np.linspace(0, 10, 11)\n        xp, yp = np.meshgrid(x, y)\n        z = xp * yp\n        z[3, 4] = np.ma.masked\n        z[0, 0] = np.ma.masked\n        \n        xp, yp, zp = delete_masked_points(xp, yp, z)\n        assert xp.shape == (10, 10)\n        assert yp.shape == (10, 10)\n        assert zp.shape == (10, 10)\n\n\n", "    def test_delete_masked_points_empty_input(self):\n        x = np.array([])\n        y = np.array([])\n        mask = np.array([], dtype=bool)\n        x_new, y_new = delete_masked_points(x, y, mask)\n        assert_array_equal(x_new, [])\n        assert_array_equal(y_new, [])\n", "    def test_delete_masked_points_ndarray(self):\n        x = np.array([1, 2, 3, 4, 5])\n        y = np.array([1, 2, np.ma.masked, 4, 5])\n        xm, ym = delete_masked_points(x, y)\n        assert_array_equal(xm, [1, 2, 4, 5])\n        assert_array_equal(ym, [1, 2, 4, 5])\n", "    def test_delete_masked_points_empty_arrays(self):\n        x = np.array([])\n        y = np.array([])\n        m = np.array([], dtype=bool)\n        assert_array_equal(delete_masked_points(x, y, m), (x, y))\n", "    def test_delete_masked_points_empty_input(self):\n        x = np.array([])\n        y = np.array([])\n        m = np.array([], dtype=bool)\n        x_new, y_new = delete_masked_points(x, y, m)\n        assert_array_equal(x_new, np.array([]))\n        assert_array_equal(y_new, np.array([]))\n"], "sample_730": ["    def test_multitask_elasticnet_cv_warm_start(self):\n        X, y = make_regression(n_samples=50, n_features=10,\n                               n_targets=2, random_state=42, noise=10)\n        clf = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .9],\n                                   random_state=42,\n                                   n_jobs=1,\n                                   warm_start=True)\n        clf.fit(X, y)\n        # Test that warm_start is used\n        coef_old = clf.coef_.copy()\n        clf.fit(X, y)\n        assert_array_equal(clf.coef_, coef_old)\n", "    def test_multi_task_lasso_multi_output(self):\n        # Test MultiTaskLasso with multi-output data\n        X = np.random.randn(10, 5)\n        y = np.random.randn(10, 3)\n\n        mtl = MultiTaskLasso(alpha=0.1)\n        mtl.fit(X, y)\n", "    def test_multi_task_lasso_path(self):\n        X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n        Y = np.array([[1], [2], [3], [4]])\n        alphas = [0.1, 0.5, 1]\n        coefs = MultiTaskLasso.path(X, Y, alphas=alphas)\n        assert_equal(coefs.shape, (3, 2, 2))\n", "    def test_multi_task_lasso_warm_start(self):\n        # Test warm_start functionality for MultiTaskLasso\n        X, y = load_boston(return_X_y=True)\n        X = np.array(X, dtype=np.float64)\n        y = np.array(y, dtype=np.float64).reshape(-1, 1)\n        X = np.column_stack((X, X[:, 0:2]))\n        n_samples = X.shape[0]\n        n_features = X.shape[1]\n\n        # Create a MultiTaskLasso instance with warm_start=True\n        clf = MultiTaskLasso(alpha=0.1, warm_start=True, max_iter=1000)\n\n        # First fit\n        clf.fit(X, y)\n        initial_coef = clf.coef_\n        initial_n_iter = clf.n_iter_\n\n        # Second fit with the same data (warm start should be used)\n        clf.fit(X, y)\n        second_coef = clf.coef_\n        second_n_iter = clf.n_iter_\n\n        assert_array_equal(initial_coef, second_coef)\n        assert_true(second_n_iter < initial_n_iter)\n\n        # Third fit with different data\n        X, y = load_boston(return_X_y=True)\n        X = np.array(X, dtype=np.float64)\n        y = np.array(y, dtype=np.float64).reshape(-1, 1)\n        clf.fit(X, y)\n        third_coef = clf.coef_\n\n        assert_false(np.array_equal(second_coef, third_coef))\n\n", "compilation error"], "sample_534": ["    def test_contour_algorithm(self, algorithm):\n        # Ensure all algorithms produce the same contours for a simple case\n        x = np.arange(-5, 5, 0.25)\n        y = np.arange(-5, 5, 0.25)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig, ax = plt.subplots()\n        with rc_context({'contour.algorithm': algorithm}):\n            cs = ax.contour(X, Y, Z)\n\n        # Check that all contours are closed polygons\n        for path in cs.allsegs:\n            for polygon in path:\n                assert np.allclose(polygon[0], polygon[-1])\n\n", "    def test_contour_corner_mask():\n        x = np.arange(-1, 2)\n        y = np.arange(-2, 1)\n        z = np.zeros((len(y), len(x)))\n        z[1:-1, 1:-1] = np.arange(1, 4).reshape(3, 1)\n        z = np.ma.masked_where(np.random.rand(z.shape) < 0.2, z)\n\n        fig, ax = plt.subplots()\n        cs = ax.contour(x, y, z, levels=[1, 2, 3], corner_mask=True)\n        assert_array_almost_equal(cs.allsegs[0][0][0], [-0.5, 0.5])\n        assert_array_almost_equal(cs.allsegs[1][0][0], [0.5, 1.5])\n        assert len(cs.allsegs[2][0]) == 0\n\n        cs = ax.contour(x, y, z, levels=[1, 2, 3], corner_mask=False)\n        assert len(cs.allsegs[2][0]) > 0\n", "    def test_corner_mask(self):\n        # Test the effect of corner_mask on masked arrays\n        x = np.arange(5)\n        y = np.arange(5)\n        z = np.ones((5, 5))\n        z[1, 1] = np.ma.masked\n        fig, ax = plt.subplots()\n        cs = ax.contour(x, y, z, corner_mask=True)\n        assert len(cs.collections[0].get_paths()) == 8\n        cs = ax.contour(x, y, z, corner_mask=False)\n        assert len(cs.collections[0].get_paths()) == 4\n", "    def test_corner_mask(self):\n        mask = np.zeros((10, 10), dtype=bool)\n        mask[2:4, 2:4] = True\n        z = np.arange(100).reshape((10, 10))\n        fig, ax = plt.subplots()\n        ax.contourf(z, corner_mask=True, levels=10)\n        ax.set_title('corner_mask=True')\n", "    def test_contour_empty(self):\n        fig, ax = plt.subplots()\n        x = np.array([])\n        y = np.array([])\n        z = np.array([])\n        with pytest.warns(UserWarning):\n            cs = ax.contour(x, y, z)\n        assert cs.collections == []\n\n"], "sample_772": ["compilation error", "def check_regressor_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X_large[:10], y_large[:10])\n    prediction = reg.predict(X_large[11:12])\n    assert_almost_equal(prediction[0], y_large[11], decimal=1)\n\n", "    def test_sparse_input(self):\n        # Test that sparse input is handled correctly\n        # Regression test for: #12881\n        X_sparse = csr_matrix(X)\n        clf = RandomForestClassifier(random_state=0)\n\n        clf.fit(X_sparse, y)\n        pred = clf.predict(X_sparse)\n        assert_array_equal(pred, clf.predict(X))\n\n", "def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X, y)\n    assert_almost_equal(reg.predict(T), [ -1.,  1.,  1.], decimal=1)\n", "    def test_oob_score_toy(self):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n\n        clf = ForestClassifier(n_estimators=10, oob_score=True,\n                               random_state=1)\n        clf.fit(X_large, y_large)\n        assert_greater(clf.oob_score_, 0.0)\n"], "sample_784": ["def test_calibration_classifiercv_fit_predict_proba():\n    X, y = make_classification(n_samples=100, n_features=20, random_state=42)\n    clf = RandomForestClassifier(random_state=42)\n    calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv=LeaveOneOut())\n    calibrated_clf.fit(X, y)\n    y_prob = calibrated_clf.predict_proba(X)\n    assert_equal(y_prob.shape, (100, 2))\n    assert_almost_equal(np.sum(y_prob, axis=1), np.ones(100), decimal=5)\n\n", "    def test_calibration_curve_multiclass():\n          X, y = make_blobs(n_samples=200, centers=3, random_state=0)\n          clf = MultinomialNB()\n          clf.fit(X, y)\n          probas = clf.predict_proba(X)\n\n          prob_true, prob_pred = calibration_curve(y, probas[:, 1],\n                                                  n_bins=10)\n          assert_equal(prob_true.size, prob_pred.size)\n          assert prob_true.size <= 10\n\n", "    def test_calibration_curve_log_loss(self):\n        X, y = make_classification(n_samples=500, n_features=20, random_state=42)\n        clf = LinearSVC(random_state=42)\n        calibrated_clf = CalibratedClassifierCV(clf, method='sigmoid', cv='prefit')\n        calibrated_clf.fit(X, y)\n\n        # check if the calibrated classifier improves log loss\n        y_prob = clf.predict_proba(X)[:, 1]\n        calibrated_y_prob = calibrated_clf.predict_proba(X)[:, 1]\n        assert_greater(log_loss(y, calibrated_y_prob), log_loss(y, y_prob))\n\n", "    def test_calibration_curve_multiclass():\n        X, y = make_classification(n_samples=100, n_classes=3, random_state=0)\n        clf = MultinomialNB()\n        clf.fit(X, y)\n        prob_pos = clf.predict_proba(X)\n        prob_true, prob_pred = calibration_curve(y, prob_pos[:, 1],\n                                                n_bins=10, strategy='quantile')\n        assert_equal(len(prob_true), len(prob_pred))\n", "    def test_fit_predict_proba_sparse_input():\n        # Test with sparse input\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        X_sparse = sparse.csr_matrix(X)\n        clf = CalibratedClassifierCV(MultinomialNB(), cv=LeaveOneOut())\n        clf.fit(X_sparse, y)\n        y_pred_proba = clf.predict_proba(X_sparse)\n        assert_equal(y_pred_proba.shape, (100, 2))\n"], "sample_745": ["    def test_transform_inverse_transform_check_inverse_warning(self):\n        X = np.array([[1, 2], [3, 4]])\n        func = lambda x: x + 1\n        inv_func = lambda x: x - 1\n        transformer = FunctionTransformer(func, inv_func, check_inverse=True)\n        transformer.fit(X)\n        with assert_warns_message(UserWarning,\n                                  \"The provided functions are not strictly inverse of each other\"):\n            transformer.fit(X)\n", "def test_function_transformer_sparse():\n    X = sparse.csr_matrix([[0, 1], [1, 0]])\n    transformer = FunctionTransformer(accept_sparse=True)\n    assert_allclose_dense_sparse(X, transformer.fit_transform(X))\n", "    def test_inverse_transform_check():\n        X = np.array([[1, 2], [3, 4]])\n        func = lambda X: X ** 2\n        inv_func = lambda X: np.sqrt(X)\n\n        transformer = FunctionTransformer(func, inv_func=inv_func)\n        transformer.fit(X)\n        X_trans = transformer.transform(X)\n        X_trans_inv = transformer.inverse_transform(X_trans)\n        assert_allclose_dense_sparse(X, X_trans_inv)\n", "def test_functiontransformer_inverse_check():\n    \"\"\"Check the behavior of check_inverse parameter.\"\"\"\n    X = np.array([[1, 2], [3, 4]])\n    transformer = FunctionTransformer(\n        func=lambda x: x * 2, inverse_func=lambda x: x / 2, check_inverse=True\n    )\n    transformer.fit(X)\n    assert_no_warnings(transformer.transform, X)\n    transformer = FunctionTransformer(\n        func=lambda x: x * 2, inverse_func=lambda x: x / 3, check_inverse=True\n    )\n    with pytest.warns(UserWarning, match=\"The provided functions are not strictly inverse\"):\n        transformer.fit(X)\n", "    def test_function_transformer_sparse_input():\n        X = sparse.eye(5)\n        transformer = FunctionTransformer(lambda x: x + 1, accept_sparse=True)\n        assert_allclose_dense_sparse(transformer.fit_transform(X),\n                                     X + 1)\n"], "sample_645": ["def test_log_level_with_no_console(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n\n            logger.debug(\"debug message\")\n            logger.info(\"info message\")\n            logger.warning(\"warning message\")\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=WARNING\")\n    result.assert_outcomes(passed=1)\n    assert \"debug message\" not in result.stdout.str()\n    assert \"info message\" not in result.stdout.str()\n    assert \"warning message\" in result.stdout.str()\n", "    def test_log_records_at_level(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n            logger = logging.getLogger(__name__)\n\n                logger.debug('debug message')\n                logger.info('info message')\n                logger.warning('warning message')\n\n                with caplog.at_level(logging.INFO):\n                    test_function()\n                assert len(caplog.records) == 2\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n\n", "    def test_log_level(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            logger.warning('This is a warning')\n        \"\"\"\n        )\n        result = testdir.runpytest(\"--log-level\", \"WARNING\")\n        result.assert_outcomes(passed=1)\n        assert \"This is a warning\" in result.stdout.str()\n\n        result = testdir.runpytest(\"--log-level\", \"ERROR\")\n        result.assert_outcomes(passed=1)\n        assert \"This is a warning\" not in result.stdout.str()\n\n", "def test_log_level(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import logging\n\n        pytest_plugins = \"pytest-logging\"\n\n            config.pluginmanager.get_plugin(\"logging\").log_level = logging.DEBUG\n    \"\"\"\n    )\n    pytester.runpytest_subprocess('-v',\"test_module.py\")\n\n\n\n", "def test_caplog_records_with_sublogger(pytester: Pytester) -> None:\n    pytester.makeconftest(\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n        sublogger = logging.getLogger(__name__ + \".baz\")\n        \"\"\"\n    )\n    pytester.write_test(\n        \"test_example.py\",\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(__name__)\n        sublogger = logging.getLogger(__name__ + \".baz\")\n\n            logger.info('message from root logger')\n            sublogger.info('message from sublogger')\n\n            logger.debug('debug message from root logger')\n        \"\"\",\n    )\n    result = pytester.runpytest()\n    assert result.ret == 0\n    item = result.getitem(\"test_example.py::test_something\")\n    assert item.report.passed\n    assert item.stash[caplog_records_key][\"call\"] == [\n        logging.LogRecord(\n            name=__name__,\n            level=logging.INFO,\n            msg=\"message from root logger\",\n            args=(),\n        ),\n        logging.LogRecord(\n            name=__name__ + \".baz\",\n            level=logging.INFO,\n            msg=\"message from sublogger\",\n            args=(),\n        ),\n    ]\n"], "sample_58": ["    def test_is_multipart(self):\n        form = Form()\n        self.assertFalse(form.is_multipart())\n\n        form = Form({'file': FileField()})\n        self.assertTrue(form.is_multipart())\n", "    def test_form_initial(self):\n        initial = {'first_name': 'John', 'last_name': 'Smith'}\n        person = Person(initial=initial)\n        self.assertEqual(person.initial, initial)\n        self.assertEqual(person.fields['first_name'].initial, 'John')\n        self.assertEqual(person.fields['last_name'].initial, 'Smith')\n", "    def test_form_renderers(self):\n        class CustomRenderer(DjangoTemplates):\n                return 'rendered'\n\n        get_default_renderer.cache_clear()\n        get_default_renderer.set(CustomRenderer)\n\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        self.assertEqual(form.as_p(), 'rendered')\n\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        form.renderer = CustomRenderer()\n        self.assertEqual(form.as_p(), 'rendered')\n\n        get_default_renderer.cache_clear()\n", "    def test_initial_for_field(self):\n        person = Person({'first_name': 'John'})\n        self.assertEqual(person.get_initial_for_field(person.fields['first_name'], 'first_name'), 'John')\n        person.initial = {'first_name': lambda: 'Jane'}\n        self.assertEqual(person.get_initial_for_field(person.fields['first_name'], 'first_name'), 'Jane')\n", "    def test_non_field_errors_after_full_clean(self):\n        form = Person(data={'first_name': 'John', 'last_name': 'Doe', 'birthday': 'invalid'})\n\n        form.full_clean()\n\n        self.assertEqual(len(form.errors), 1)\n        self.assertEqual(NON_FIELD_ERRORS in form.errors, True)\n\n\n"], "sample_353": ["    def test_createsuperuser_with_custom_fields(self):\n        # Test creating a superuser with custom fields.\n\n        # Ensure the model has the expected fields.\n        self.assertTupleEqual(\n            tuple(CustomUserWithM2M._meta.get_fields()),\n            (\n                CustomUserWithM2M._meta.pk,\n                CustomUserWithM2M.USERNAME_FIELD,\n                CustomUserWithM2M.PASSWORD_FIELD,\n                CustomUserWithM2M.email,\n                CustomUserWithM2M.date_of_birth,\n                CustomUserWithM2M.first_name,\n                CustomUserWithM2M.organizations,\n            ),\n        )\n\n        # Create a superuser.\n        call_command('createsuperuser',\n                     stdin=StringIO(), verbosity=0, interactive=True)\n        \n        # Check that the superuser was created.\n        self.assertEqual(CustomUserWithM2M.objects.count(), 1)\n        superuser = CustomUserWithM2M.objects.get(username='alice')\n        self.assertEqual(superuser.email, 'alice@example.com')", "    def test_createsuperuser_with_email_and_password(self):\n        call_command('createsuperuser', interactive=True, database='default', stdin=MockTTY())\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n        self.assertEqual(user.check_password('password'), True)\n\n", "    def test_createsuperuser_interactive_password_validation_bypass(self):\n        with mock.patch('django.db.models.Model.save') as mock_save:\n            call_command('createsuperuser', interactive=True, stdin=MockTTY())\n            mock_save.assert_called_once()\n", "    def test_createsuperuser_with_email_field(self):\n        call_command('createsuperuser', verbosity=0, interactive=True)\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='alice')\n        self.assertEqual(user.email, 'alice@example.com')\n", "    def test_createsuperuser_with_email_field(self):\n        # Test creating a superuser with an email field.\n        # This checks that the email field is properly handled during creation.\n        with override_settings(AUTH_USER_MODEL='tests.CustomUserWithEmail'):\n            call_command('createsuperuser', interactive=True, stdin=MockTTY())\n            user = CustomUserWithEmail.objects.get(username='alice')\n            self.assertEqual(user.email, 'alice@example.com')  \n"], "sample_62": ["    def test_register_twice(self):\n        with self.assertRaises(AlreadyRegistered):\n            self.site.register(Person)\n            self.site.register(Person)\n", "    def test_register_unregister(self):\n        register_model_methods = [\n            [Person],\n            [Person, Location],\n            [Traveler, Place],\n        ]\n\n        for models in register_model_methods:\n            for model in models:\n                self.assertFalse(model in self.site._registry)\n            [self.site.register(model) for model in models]\n\n            for model in models:\n                self.assertTrue(model in self.site._registry)\n            [self.site.unregister(model) for model in models]\n            for model in models:\n                self.assertFalse(model in self.site._registry)\n\n", "    def test_register_unregister(self):\n        self.site.register(Location)\n        self.assertIn(Location, self.site._registry)\n        self.site.unregister(Location)\n        self.assertNotIn(Location, self.site._registry)\n", "    def test_register_model_twice(self):\n        with self.assertRaises(AlreadyRegistered):\n            self.site.register(Location, NameAdmin)\n            self.site.register(Location, NameAdmin)\n\n", "    def test_register_model_twice(self):\n        with self.assertRaises(AlreadyRegistered):\n            self.site.register(Location)\n            self.site.register(Location)\n"], "sample_876": ["    def test_mlp_classifier_sparse_input_multilabel():\n        # Test multilabel classification with sparse input\n        X, y = make_multilabel_classification(\n            n_samples=100, n_features=20, random_state=42\n        )\n        X_sparse = csr_matrix(X)\n        clf = MLPClassifier(hidden_layer_sizes=(10,),\n                            max_iter=1000, random_state=42).fit(X_sparse, y)\n        y_pred = clf.predict(X_sparse)\n        assert (y_pred.shape[0] == y.shape[0])\n\n", "def test_mlp_regressor_loss(loss):\n    X, y = make_regression(n_samples=100, n_features=10, random_state=42)\n    mlp = MLPRegressor(hidden_layer_sizes=(10,),\n                       solver='sgd', max_iter=200,\n                       learning_rate='constant', learning_rate_init=0.01,\n                       loss=loss, random_state=42)\n    mlp.fit(X, y)\n", "    def test_mlp_regressor_sparse_input(self):\n        # Check if MLPRegressor can handle sparse input\n        mlp = MLPRegressor(random_state=42)\n        X_sparse = csr_matrix(X_reg)\n        mlp.fit(X_sparse, y_reg)\n        mlp.predict(X_sparse[:5])\n", "def test_mlp_classifier_sparse_input(X, y, solver):\n    mlp = MLPClassifier(solver=solver, random_state=42)\n    mlp.fit(csr_matrix(X), y)\n\n", "    def test_mlp_classifier_predict_proba(dataset):\n        X, y = dataset\n        mlp = MLPClassifier(\n            hidden_layer_sizes=(10,),\n            activation='relu',\n            solver='adam',\n            max_iter=2000,\n            random_state=42\n        )\n\n        mlp.fit(X, y)\n        y_proba = mlp.predict_proba(X)\n\n        assert y_proba.shape == (X.shape[0], np.unique(y).size)\n        assert_almost_equal(y_proba.sum(axis=1), np.ones(X.shape[0]))\n"], "sample_1205": ["    def test_quo_ground(self):\n        x = self.ring.gens[0]\n\n        p = self.ring(f'x**2 + 2*x + 1')\n\n        assert p.quo_ground(2) == self.ring(f'(x**2 + 2*x + 1)/2')\n\n        assert p.quo_ground(1) == p\n\n        raises(ZeroDivisionError, lambda: p.quo_ground(0))\n\n", "    def test_trunc_ground(self):\n        R, x = ring(\"x\", ZZ)\n        p = 3*x**2 + 2*x - 1\n        q = p.trunc_ground(7)\n        assert q == R.from_dict({(2,), 5},ZZ)\n", "    def test_trunc_ground(self):\n        R = ring('x,y', ZZ)\n        f = 3*x + 5*y + 2\n        p = 7\n\n        g = f.trunc_ground(p)\n        assert g == 3*x + 5*y + 2\n\n        f = 11*x + 8*y + 20\n        g = f.trunc_ground(p)\n        assert g == 4*x + y + 6\n", "    def test_PolyElement__call__(self):\n        R = ring(\"x,y\", ZZ)\n        x, y = R.gens\n\n        p = x**2*y + 2*x + y**3\n\n        assert p(1, 2) == 1*2*2 + 2*1 + 2**3 == 12\n        assert p(x, y) == p\n", "def test_poly_evaluate():\n    R, x = ring(\"x\", ZZ)\n    f = 2*x**3 - x**2 + 5*x\n\n    assert f.evaluate(2) == 35\n    assert f.evaluate(x) == 2*x**3 - x**2 + 5*x\n    assert f.evaluate((x, 2)) == 35\n    assert f.evaluate([x, 2]) == 35\n"], "sample_991": ["def test_product_rewrite_as_sum():\n    P = Product(x, (x, 1, 5))\n    assert P.rewrite(Sum).simplify() == exp(Sum(log(x), (x, 1, 5)))\n\n", "def test_product_symbolic_with_free_symbol():\n    p = product(x**2, (k, 1, n))\n    assert p.doit() == x**(2*n)\n\n", "def test_product_empty():\n    assert product(k, (k, n, n - 1)) == 1\n", "def test_product_empty():\n    assert product(1, (n, 5, 4)) == 1\n\n", "def test_product_rewrite_as_Sum():\n    assert Product(1/x, (x, 1, n))._eval_rewrite_as_Sum() == exp(Sum(log(1/x), (x, 1, n)))\n"], "sample_346": ["    def test_method_decorator_classonlymethod(self):\n        class MyClass:\n            @method_decorator(classonlymethod)\n                return 'Hello from my_method'\n\n        obj = MyClass()\n        self.assertRaises(AttributeError, obj.my_method)\n        self.assertEqual(MyClass.my_method(), 'Hello from my_method')\n", "    def test_method_decorator(self):\n                return view_func(request) + \" decorated\"\n            return wrapper\n        \n        @method_decorator(my_decorator)\n            return \"hello\"\n\n        self.assertEqual(my_view(HttpRequest()).content, b\"hello decorated\")\n", "    def test_method_decorator(self):\n        @method_decorator(login_required)\n            return HttpResponse('OK')\n        self.assertEqual(my_view.__name__, 'my_view')\n        self.assertEqual(my_view.__original_func__.__name__, 'my_view')\n\n        # Make sure the decorator updates the docstring and __dict__ of the decorated function.\n        self.assertEqual(my_view.__doc__, login_required.__doc__)\n        self.assertEqual(my_view.__dict__, login_required.__wrapped__.__dict__)\n\n        @method_decorator(login_required, name='my_method')\n        class MyClass:\n                return HttpResponse('OK')\n        self.assertTrue(hasattr(MyClass, 'my_method'))\n        self.assertEqual(MyClass.my_method.__doc__, login_required.__doc__)\n        self.assertEqual(MyClass.my_method.__dict__, login_required.__wrapped__.__dict__)\n\n        with self.assertRaises(ValueError):\n            @method_decorator(login_required, name='nonexistent_method')\n            class MyClass:\n                pass\n", "    def test_method_decorator(self):\n        class MyView:\n                return HttpResponse(\"OK\")\n\n        @method_decorator(login_required)\n            return HttpResponse(\"OK\")\n\n        decorated_view = method_decorator(login_required)(MyView.my_method)\n\n        self.assertEqual(decorated_view.__name__, 'login_required(<function MyView.my_method at 0x...>)')\n        self.assertEqual(my_decorated_method.__name__, 'my_decorated_method')\n\n        instance = MyView()\n        self.assertEqual(instance.my_method.__name__, 'my_method')\n\n        # Test that it works with class methods\n        @classmethod\n        @method_decorator(login_required)\n            return HttpResponse(\"OK\")\n\n        self.assertEqual(my_class_method.__name__, 'login_required(<classmethod object at 0x...>)')\n\n\n\n", "    def test_method_decorator_name_required(self):\n        class C:\n            pass\n\n        with self.assertRaisesMessage(ValueError,\n                                     \"The keyword argument `name` must be the name of a method of the decorated class: <class 'DecoratorsTest.test_method_decorator_name_required.<locals>.C'>. Got '' instead.\"):\n            method_decorator(staff_member_required)('')\n"], "sample_350": ["    def test_union_with_no_objects(self):\n        queryset1 = Number.objects.filter(num=11)\n        queryset2 = Number.objects.filter(num=12)\n        combined = queryset1.union(queryset2)\n        self.assertEmpty(combined)\n", "    def test_union_ordering(self):\n        queryset1 = Number.objects.filter(num__lt=5).order_by('num')\n        queryset2 = Number.objects.filter(num__gte=5).order_by('num')\n        combined = queryset1.union(queryset2)\n        self.assertNumbersEqual(combined, range(10))\n\n", "    def test_union_with_existing_queryset(self):\n        queryset1 = Number.objects.filter(num__lt=5)\n        queryset2 = Number.objects.filter(num__gt=5)\n\n        combined = queryset1 | queryset2\n        self.assertNumbersEqual(combined, Number.objects.all())\n", "    def test_union_with_exists(self):\n        # Test union with subqueries involving EXISTS.\n        exists_queryset = Number.objects.filter(\n            Exists(ReservedName.objects.filter(name=OuterRef('other_num')).values('id'))\n        )\n        union_queryset = Number.objects.filter(num__lt=5).union(exists_queryset)\n        self.assertNumbersEqual(union_queryset, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n", "    def test_union_with_subquery(self):\n        # Test union with a subquery that has a LIMIT clause\n        limited_nums = Number.objects.filter(num__lt=5).order_by('num')[:3]\n        union_queryset = Number.objects.filter(num__gte=5).union(limited_nums)\n        self.assertNumbersEqual(union_queryset, list(range(5, 10)) + list(range(3)), ordered=False)\n"], "sample_110": ["    def test_raw_sql_as_arg(self):\n        qs = Container.objects.annotate(\n            event_count=RawSQL(\n                'SELECT COUNT(*) FROM events WHERE container_id = %s',\n                (models.OuterRef('pk'),),\n                output_field=models.IntegerField(),\n            )\n        )\n        self.assert_pickles(qs)\n\n", "    def test_pickle_expression(self):\n        event = Event.objects.create(\n            name='Test Event',\n            timestamp=datetime.datetime(2023, 1, 1),\n            container=Container.objects.create()\n        )\n        qs = Event.objects.filter(name=\"Test Event\").annotate(\n            event_date=self.expression\n        )\n        self.assert_pickles(qs)\n\n", "    def test_pickle_expression_list(self):\n        events = Event.objects.all()\n        self.assert_pickles(events.values_list('name'))\n", "    def test_annotate_with_subquery(self):\n        container = Container.objects.create()\n        Event.objects.create(container=container, timestamp=datetime.datetime(2023, 10, 26, 10, 0, 0))\n        Event.objects.create(container=container, timestamp=datetime.datetime(2023, 10, 26, 11, 0, 0))\n        qs = Container.objects.annotate(\n            latest_event=Subquery(Event.objects.filter(container=OuterRef('pk')).order_by('-timestamp').values('timestamp')[:1])\n        )\n        self.assert_pickles(qs)\n", "    def test_pickled_expression_tree(self):\n        Event.objects.create(\n            name='Test',\n            happening=Happening.objects.get()\n        )\n\n        qs = Event.objects.annotate(\n            next_event=models.F('happening__events').order_by('date').filter(date__gt=models.F('date')).first()\n        )\n        self.assert_pickles(qs)\n"], "sample_593": ["    def test_dataset_repr_multiindex(multiindex):\n        expected = fh.dataset_repr(multiindex)\n        assert \"level_1\" in expected\n        assert \"level_2\" in expected\n\n\n\n", "def test_dataset_repr_multiindex(multiindex):\n    expected = fh.dataset_repr(multiindex)\n    assert (\n        fh.CSS_STYLE in expected\n    ), \"CSS should be injected into the HTML repr\"\n    assert (\n        fh.ICONS_SVG in expected\n    ), \"Icons SVG should be injected into the HTML repr\"\n    assert 'class=\"xr-has-index\"' in expected, \"MultiIndex coords should be marked as indexed\"\n", "def test_summarize_coord_multiindex(multiindex):\n    actual = fh.summarize_coord(\"x\", multiindex[\"x\"])\n    expected = {\n        \"x\": \"<div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>MultiIndex</div><div class='xr-var-preview'>(level_1, level_2)</div><input id='attrs-a3aa4376-e385-4046-8540-9c9667b61a57' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a3aa4376-e385-4046-8540-9c9667b61a57' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-e094b99c-a85c-4367-9952-277011a7c02a' class='xr-var-data-in' type='checkbox'><label for='data-e094b99c-a85c-4367-9952-277011a7c02a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'></div><div class='xr-var-data'></div>\"\n    }\n    assert actual == expected\n", "def test_multiindex_coords(multiindex):\n    expected_output = (\n        \"<div>\"\n        \"<svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg>\"\n        \"<svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg>\"\n        \"<pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\\nDimensions:\\n  level_1: 2\\n  level_2: 2\\nCoordinates:\\n  * x       (level_1, level_2) &lt;U1 (a, 1)\\nData variables:\\n    *empty*</pre>\"\n        \"<div class='xr-wrap' hidden>\"\n        \"<div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div>\"\n        \"<ul class='xr-sections'>\"\n        \"<li class='xr-section-item'><div class='xr-section-summary' title='Expand/collapse section'>Dimensions:<span>(2, 2)</span></div>\"\n        \"<div class='xr-section-inline-details'></div>\"\n        \"<div class='xr-section-details'></div></li>\"\n        \"<li class='xr-section-item'><div class='xr-section-summary' title='Expand/collapse section'>Coordinates: (2) </div>\"\n        \"<div class='xr-section-inline-details'>\"\n        \"<ul class='xr-var-list'>\"\n        \"<li class='xr-var-item'><div class='xr-var-name'><span>x</span></div><div class='xr-var-dims'>(<span>level_1, level_2</span>)</div><div class='xr-var-dtype'>&lt;U1</div><div class='xr-var-preview xr-preview'>&apos;a&apos; &apos;b&apos;</div><div class='xr-var-attrs'></div><div class='xr-var-data'></div></li></ul></div>\"\n        \"<div class='xr-section-details'></div></li>\"\n        \"<li class='xr-section-item'><div class='xr-", "    def test_summarize_variable_multiindex(multiindex):\n        expected = (\n            \"<div class='xr-var-name'><span>x</span></div>\"\n            \"<div class='xr-var-dims'>()</div>\"\n            \"<div class='xr-var-dtype'>object</div>\"\n            \"<div class='xr-var-preview'>('a', 1), ('a', 2), ('b', 1), ('b', 2)</div>\"\n            \"<input id='attrs-...' class='xr-var-attrs-in' type='checkbox' disabled>\"\n            \"<label for='attrs-...'\"\n            \" title='Show/Hide attributes'>\"\n            \"<svg class='icon xr-icon-file-text2'>\"\n            \"<use xlink:href='#icon-file-text2'>\"\n            \"</use>\"\n            \"</svg>\"\n            \"</label>\"\n            \"<input id='data-...'\"\n            \" class='xr-var-data-in' type='checkbox'>\"\n            \"<label for='data-...' title='Show/Hide data repr'>\"\n            \"<svg class='icon xr-icon-database'>\"\n            \"<use xlink:href='#icon-database'>\"\n            \"</use>\"\n            \"</svg>\"\n            \"</label>\"\n            \"<div class='xr-var-attrs'></div>\"\n            \"<div class='xr-var-data'><pre>...</pre></div>\"\n        )\n\n        result = fh.summarize_variable(\"x\", multiindex[\"x\"], is_index=True)\n\n"], "sample_598": ["    def test_format_array_flat_simple(self, array):\n        # Test basic formatting for various array shapes and types\n        expected = formatting.format_array_flat(array, max_width=20)\n        actual = str(array) \n        assert expected == actual\n\n", "    def test_diff_array_repr_scalar(self, compat):\n        a = xr.DataArray(np.array(1), dims=[\"x\"])\n        b = xr.DataArray(np.array(2), dims=[\"x\"])\n        expected = dedent(\n            f\"\"\"\n            Left and right DataArray objects are not {compat}\n            Differing values:\n                L\n                <xarray.DataArray (x: 1)>\n                array([1])\n                Dimensions without coordinates: x\n                R\n                <xarray.DataArray (x: 1)>\n                array([2])\n                Dimensions without coordinates: x\n            \"\"\"\n        )\n        assert formatting.diff_array_repr(a, b, compat=compat) == expected\n\n", "    def test_summarize_coord_multiindex(self):\n        data = np.arange(12).reshape(3, 4)\n        coords = {\"x\": pd.MultiIndex.from_tuples([(\"a\", 1), (\"a\", 2), (\"b\", 1)]),\n                   \"y\": np.arange(4)}\n        ds = xr.Dataset(\n            {\"foo\": ((\"x\", \"y\"), data)},\n            coords=coords,\n        )\n\n        expected = dedent(\n            \"\"\"\n            Coordinates:\n              * x     MultiIndex\n              * y     (y) int64 0 1 2 3\n            \"\"\"\n        )\n        assert formatting.coords_repr(ds.coords) == expected\n", "    def test_format_timedelta(self):\n        td1 = pd.Timedelta(days=1, hours=2, minutes=30)\n        td2 = pd.Timedelta(days=1, hours=2)\n        assert formatting.format_timedelta(td1) == \"1 days 02:30:00\"\n        assert formatting.format_timedelta(td1, timedelta_format=\"date\") == \"1 days\"\n        assert formatting.format_timedelta(td2, timedelta_format=\"time\") == \"02:00:00\"\n", "    def test_summarize_coord_level_names(self):\n        coord = xr.IndexVariable(\"level1\", [1, 2, 3], dims=[\"level1\"])\n        ds = xr.Dataset({\"foo\": (\"level1\", [0, 1, 2])}, coords={\"level1\": coord})\n        expected = dedent(\n            \"\"\"\n            Coordinates:\n              * level1  (level1) int64 1 2 3\n            \"\"\"\n        )\n        assert formatting.coords_repr(ds.coords) == expected\n\n        # Test with multi-level\n        level2 = xr.IndexVariable(\"level2\", [\"a\", \"b\", \"c\"], dims=[\"level2\"])\n        \n        ds = xr.Dataset(\n            {\"foo\": ((\"level1\", \"level2\"), np.arange(9).reshape(3, 3))},\n            coords={\"level1\": coord, \"level2\": level2},\n        )\n        expected = dedent(\n            \"\"\"\n            Coordinates:\n              * level1  (level1) int64 1 2 3\n              * level2  (level2) <U1 'a' 'b' 'c'\n            \"\"\"\n        )\n        assert formatting.coords_repr(ds.coords) == expected\n"], "sample_373": ["    def test_model_detail_view_property(self):\n        response = self.client.get(reverse('admin:admindocs-model_detail', args=['company', 'Company']))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'get_name')\n        self.assertContains(response, 'List')\n", "    def test_view_detail_view_for_view_with_args(self):\n        view_name = 'my_view'\n\n            return None\n\n        self.urls.append(path(f'test/{view_name}/', my_view, name=view_name))\n\n        response = self.client.get(reverse('admin:admindocs_view_detail', args=[view_name]))\n        self.assertEqual(response.status_code, 200)\n", "    def test_model_detail_view_with_related_objects(self):\n        response = self.client.get(reverse('admin:admindocs_model_detail', args=[Company._meta.app_label, Company._meta.model_name]))\n        self.assertEqual(response.status_code, 200)\n\n        self.assertContains(response, 'Person')\n        self.assertContains(response, '<a href=\"/admin/admindocs/model_detail/auth/person/\">Person</a>')\n\n", "    def test_view_detail_view_model_method(self):\n        response = self.client.get(reverse('admin:admindocs_model_detail', kwargs={'app_label': 'tests', 'model_name': 'person'}))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Person')\n", "    def test_view_detail_view_handles_view_not_found(self):\n        \"\"\" Test case for when view_detail cannot locate the given view. \"\"\"\n        with captured_stderr() as stderr:\n            response = self.client.get(reverse('admin:admindocs_view_detail', args=['nonexistent_view']))\n        self.assertEqual(response.status_code, 404)\n        self.assertIn(\"View 'nonexistent_view' not found\", stderr.getvalue())\n"], "sample_667": ["    def test_mktemp_numbered_unique(tmpdir):\n        p1 = tmpdir.mkdir(\"foo\")\n        p2 = tmpdir.mkdir(\"foo\")\n        assert p1.basename != p2.basename\n", "    def test_getbasetemp_with_given_basetemp(tmpdir_factory):\n        given_basetemp = pathlib.Path(tmpdir_factory._given_basetemp)\n        ensure_reset_dir(given_basetemp)\n        assert tmpdir_factory.getbasetemp() == given_basetemp\n", "    def test_tmpdirhandler_getbasetemp_when_basetemp_is_set(tmp_path_factory, monkeypatch):\n        monkeypatch.setattr(tmp_path_factory, \"_given_basetemp\", Path(\"/some/path\"))\n        assert tmp_path_factory.getbasetemp() == Path(\"/some/path\")\n", "    def test_ensure_relative_to_basetemp(self, tmp_path_factory):\n        with pytest.raises(ValueError):\n            tmp_path_factory._ensure_relative_to_basetemp(\"..\")\n        tmp_path_factory._ensure_relative_to_basetemp(\"hello\")\n\n", "def test_tmp_path_factory_getbasetemp_with_given_basetemp(tmp_path_factory):\n    # create a temporary directory to be used as basetemp\n    given_basetemp = tmp_path_factory.mktemp(\"given_basetemp\")\n    # recreate the TempPathFactory with the given basetemp\n    tmp_path_factory = TempPathFactory(given_basetemp=given_basetemp, trace=False)\n    # assert that getbasetemp returns the given basetemp\n    assert tmp_path_factory.getbasetemp() == given_basetemp\n"], "sample_1201": ["    def test_cgs_gauss_conversions():\n\n        assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10 * speed_of_light * statcoulomb\n        assert convert_to(coulomb, statcoulomb, cgs_gauss) == 1 / (10 * speed_of_light) * statcoulomb\n        assert convert_to(statvolt, volt, cgs_gauss) == 10**6 / speed_of_light * statvolt\n        assert convert_to(volt, statvolt, cgs_gauss) == speed_of_light / 10**6 * statvolt\n        assert convert_to(meter, centimeter, cgs_gauss) == 100 * centimeter\n        assert convert_to(centimeter, meter, cgs_gauss) == 1 / 100 * meter\n\n", "    def test_cgs_gauss_conversions():\n        assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*statcoulomb\n        assert convert_to(coulomb, statcoulomb, cgs_gauss) == coulomb/(10*speed_of_light*statcoulomb)\n        assert convert_to(coulomb_constant, coulomb_constant, cgs_gauss) == 1\n", "def test_conversion_cgs_gauss():\n    assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*statcoulomb\n", "    def test_cgs_gauss_conversions():\n        assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*statcoulomb\n        assert convert_to(coulomb, statcoulomb, cgs_gauss) == 1/(10*speed_of_light)*statcoulomb\n        assert convert_to(statvolt, volt, cgs_gauss) == 10**(-6)*speed_of_light*statvolt\n        assert convert_to(volt, statvolt, cgs_gauss) == 1/(10**(-6)*speed_of_light)*statvolt\n        assert convert_to(statampere, ampere, cgs_gauss) == 10*speed_of_light*statampere\n        assert convert_to(ampere, statampere, cgs_gauss) == 1/(10*speed_of_light)*statampere\n", "    def test_cgs_gauss_conversions():\n        assert convert_to(statcoulomb, coulomb, cgs_gauss) == 10*speed_of_light*statcoulomb\n        assert convert_to(coulomb, statcoulomb, cgs_gauss) == statcoulomb/(10*speed_of_light)\n        assert convert_to(ampere, statampere, cgs_gauss) == 10*speed_of_light\n"], "sample_206": ["    def test_filefield_save_form_data_clears_field(self):\n        doc = Document.objects.create(file=None)\n        # Simulate a form submission where the file field is left blank (not submitted)\n        doc.save_form_data({'file': ''})\n        doc.save()\n        self.assertIsNone(doc.file)\n", "    def test_filefield_save_form_data_empty_string(self):\n        doc = Document.objects.create(description=\"\")\n        self.assertEqual(doc.file.name, \"\")\n", "    def test_filefield_pickle(self):\n        doc = Document.objects.create(myfile=ContentFile(b'Some data'))\n        pickled_doc = pickle.dumps(doc)\n        unpickled_doc = pickle.loads(pickled_doc)\n        self.assertEqual(unpickled_doc.myfile.read(), b'Some data')\n", "    def test_filefield_force_update_dimensions(self):\n        \"\"\"Tests that update_dimension_fields correctly updates dimensions\n\n        even if the ImageField is assigned a previously saved image.\"\"\"\n        # Create a temporary file\n        with temp.NamedTemporaryFile() as f:\n            f.write(b'Image data')\n            f.seek(0)\n            image_file = File(f)\n            image_file.name = 'test.jpg'\n\n        # Create a new Document\n        doc = Document.objects.create(file=image_file)\n        doc.save()\n        self.assertTrue(doc.file.width > 0)\n        self.assertTrue(doc.file.height > 0)\n\n        # Simulate replacing the image with a previously saved one\n        doc.file = doc.file\n        doc.save()\n        \n        # Dimensions should be updated\n        self.assertEqual(doc.file.width, doc.width)\n        self.assertEqual(doc.file.height, doc.height)\n", "    def test_save_through_field_twice(self):\n        doc = Document.objects.create(myfile=ContentFile(\"hello\".encode('utf-8'), name='test.txt'))\n        self.assertTrue(doc.myfile)\n\n        # Save again with the same content\n        doc.myfile = ContentFile(\"hello\".encode('utf-8'), name='test.txt')\n        doc.save()\n\n        # File should still exist\n        self.assertTrue(doc.myfile)\n\n        with self.assertRaises(IntegrityError):\n            # Save again with a different name, this should fail due to unique constraint\n            doc.myfile.name = 'test2.txt'\n            doc.save()\n"], "sample_307": ["    def test_time_format_with_timezone(self):\n        dt = make_aware(datetime(2008, 1, 2, 10, 30, 0, tzinfo=utc)).astimezone(get_default_timezone())\n        self.assertEqual(dateformat.time_format(dt, 'e'), get_default_timezone().tzname(dt))\n", "    def test_format_iso_week_number(self):\n        d = date(2023, 12, 25)\n        self.assertEqual(format(d, 'W'), '52')\n", "    def test_format_timezone_aware_datetime(self):\n        dt = datetime(2023, 10, 27, 12, 0, 0, tzinfo=utc)\n        # Test that timezone information is included in the output\n        self.assertEqual(format(dt, 'O'), '+0000')\n        self.assertEqual(format(dt, 'T'), 'UTC')\n        # Test that the timezone-aware datetime is correctly formatted\n        self.assertEqual(format(dt, 'c'), '2023-10-27T12:00:00+00:00')\n", "    def test_format_timezone_aware_datetime(self):\n        dt = make_aware(datetime(2008, 1, 1, 10, 30, 0, tzinfo=utc))\n        formatted = format(dt, 'c')\n        self.assertEqual(formatted, '2008-01-01T10:30:00+00:00')\n", "    def test_format_time_with_timezone(self):\n        d = datetime(2008, 1, 2, 10, 30, 0, tzinfo=utc)\n        # The default timezone is Europe/Copenhagen, so the output should be\n        # '2008-01-02 12:30:00', 2 hours ahead of UTC.\n        self.assertEqual(format(d, 'Y-m-d H:i:s'), '2008-01-02 12:30:00')\n        \n        tz = get_fixed_timezone(1)\n        d = datetime(2008, 1, 2, 10, 30, 0, tzinfo=tz)\n        self.assertEqual(format(d, 'Y-m-d H:i:s'), '2008-01-02 11:30:00')\n"], "sample_272": ["    def test_migrate_forwards_with_fake(self):\n        \"\"\"\n        Test running migrations forwards with a fake=True setting for\n        the initial migration to simulate the initial schema creation.\n\n        The fake initial migration test must create tables in the database.\n        The test itself verifies that the tables have been created, but\n        doesn't perform any data migration operations. This ensures that\n        the initial migration setup is correctly handled without running\n        any actual data migrations related to the initial migration.\n\n        Skip this test if the database doesn't support schema introspection\n\n        \"\"\"\n        with mock.patch('django.db.connection.DatabaseOperations.execute_sql') as mock_execute:\n            with self.connection.cursor() as cursor:\n                with self.assertRaises(DatabaseError):\n                    cursor.execute(\"SELECT 1\")\n            self.loader.loader.project_state = mock.MagicMock(return_value=ProjectState(\n                apps=['migrations', 'migrations2'],\n                models={'migrations': {}, 'migrations2': {}},\n            ))\n            executor = MigrationExecutor(self.connection)\n\n            # Get the initial migration for 'migrations' app\n            initial_migration = self.loader.graph.leaf_nodes()[0][1]\n            executor.migrate([('migrations', initial_migration.name)], fake=True)\n            mock_execute.assert_called_once()\n            # The database doesn't have any tables initially\n            self.assertFalse(self.connection.introspection.table_names(self.connection.cursor()))\n\n", "    def test_migrate_empty_database(self):\n        # Migrate from a completely empty database\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT 1\")\n\n        graph = MigrationGraph(self.migrate_app, self.migrate_app)\n        executor = MigrationExecutor(self.connection)\n        executor.migrate(graph.leaf_nodes())\n        # Assuming the test database schema matches the default one for\n        # the migrations test app (with a single content type table)\n        with connection.cursor() as cursor:\n            rows = cursor.execute(\"SELECT COUNT(*) FROM django_content_type\").fetchone()\n            self.assertEqual(rows[0], 2)\n", "    def test_migrate_empty(self):\n        # Regression test for #24814 - ensure that migrate doesn't raise an error\n        # on an empty database.\n        with connection.cursor() as cursor:\n            with self.assertRaises(DatabaseError):\n                cursor.execute(\"SELECT 1\")\n        # Create a MigrationRecorder instance\n        recorder = MigrationRecorder(connection)\n        # Create a MigrationExecutor instance\n        executor = MigrationExecutor(connection)\n        # Ensure the schema is present\n        recorder.ensure_schema()\n        # Apply migrations\n        executor.migrate([])\n\n\n", "    def test_migrate_no_targets(self):\n        # Check that migrating with no targets behaves correctly.\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE migrations_test_model (id INTEGER PRIMARY KEY)\")\n        self.assertEqual(self.get_migration_state('migrations2').apps.get_model('migrations2', 'Model').objects.all().count(), 0)\n        executor = MigrationExecutor(connection)\n        executor.migrate([], fake=True)\n        self.assertEqual(self.get_migration_state('migrations2').apps.get_model('migrations2', 'Model').objects.all().count(), 0)\n        # Make sure the migration recorder doesn't record anything\n        self.assertEqual(executor.recorder.applied_migrations(), set())\n", "    def test_migrate_with_dependencies(self):\n        # Create a basic setup\n        with self.createConnection():\n            self._create_tables()\n\n            # Define migrations\n            migration1 = self._create_migration('migrations', '0001_initial',\n                                              \"Migration 0001\")\n            migration2 = self._create_migration('migrations2', '0001_initial',\n                                              \"Migration 0002\", depends_on=migration1)\n\n            # Install migrations\n            self.migrate(migration1)\n            self.migrate(migration2)\n\n            # Verify tables exist\n            self.assertTableExists('migrations_migration')\n            self.assertTableExists('migrations2_migration2')\n"], "sample_1038": ["def test_matrix_symbol_diff():\n    assert diff(A[0,0], B[0,0]) == 0\n    assert diff(A[0,0], A[0,0]) == 1\n\n    assert diff(A*B, A).doit() == B\n    assert diff(A*B, B).doit() == A.T\n\n    assert diff(A*B*C, A).doit() == B*C\n    assert diff(A*B*C, B).doit() == A*C.T\n    assert diff(A*B*C, C).doit() == A.T*B.T\n", "    def test_generic_zero_matrix_eval_transpose():\n        assert Transpose(GenericZeroMatrix()) == GenericZeroMatrix()\n", "    def test_matrix_derivative_scalar():\n        assert diff(A*3, x) == ZeroMatrix(n, m)\n        assert diff(3*A, x) == ZeroMatrix(n, m)\n", "def test_MatrixElement():\n    A = MatrixSymbol('A', 3, 3)\n    M = MatrixElement(A, 1, 2)\n    assert M.parent is A\n    assert M.i == 1\n    assert M.j == 2\n    assert M.shape == (3, 3)\n    assert M.is_symbol\n    assert M.is_commutative\n\n    assert MatrixElement(A, 1, 2) == MatrixElement(A, 1, 2)\n    assert M != MatrixElement(A, 2, 1)\n    assert M != MatrixElement(B, 1, 2)\n", "def test_matrix_element_diff():\n    X = MatrixSymbol('X', 2, 2)\n    i, j = symbols('i j', integer=True)\n    assert diff(X[i, j], X[0, 0]) == KroneckerDelta(i, 0)*KroneckerDelta(j, 0)\n\n"], "sample_695": ["    def test_iterparentnodeids_with_relative_path(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\n            \"\"\"\n                pass\n\n                pass\n        \"\"\"\n        )\n        nodeids = list(nodes.iterparentnodeids(testdir.getpath(\"test_1.py::test_foo\")))\n        expected = [\"\", \"test_1.py\", \"test_1.py::test_foo\"]\n        assert nodeids == expected\n\n", "    def test_iterparentnodeids_root(self, pytester: Pytester) -> None:\n        path = Path(self.testdir.mkdir(\"testdir\"))\n        item = nodes.Item.from_parent(parent=nodes.FSCollector(path=path))\n        ids = list(nodes.iterparentnodeids(item.nodeid))\n        expected = [\"\", str(path.name)]\n        assert ids == expected\n\n", "compilation error", "def test_iterparentnodeids(nodeid: str, expected: List[str]):\n    result = list(nodes.iterparentnodeids(nodeid))\n    assert result == expected\n", "    def test_iterparentnodeids_empty(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\"\")\n        item = testdir.getnode(testdir.root)\n        parents = list(nodes.iterparentnodeids(item.nodeid))\n        assert parents == [\"\"]\n"], "sample_280": ["    def test_aggregate_window_functions(self):\n        with self.assertQuerySetEqual(\n            Book.objects.annotate(\n                num_authors=Count('authors', distinct=True)\n            ).order_by('num_authors'),\n            [\n                {'isbn': '159059725', 'name': 'The Definitive Guide to Django: Web Development Done Right', 'pages': 447, 'rating': Decimal('4.50'),\n                 'price': Decimal('30.00'), 'contact_id': 1, 'publisher_id': 1, 'pubdate': datetime.date(2007, 12, 6), 'num_authors': 2},\n                {'isbn': '067232959', 'name': 'Sams Teach Yourself Django in 24 Hours', 'pages': 528, 'rating': Decimal('3.00'),\n                 'price': Decimal('23.09'), 'contact_id': 3, 'publisher_id': 2, 'pubdate': datetime.date(2008, 3, 3), 'num_authors': 1},\n                {'isbn': '159059996', 'name': 'Practical Django Projects', 'pages': 300, 'rating': Decimal('4.00'),\n                 'price': Decimal('29.69'), 'contact_id': 4, 'publisher_id': 1, 'pubdate': datetime.date(2008, 6, 23), 'num_authors': 1},\n                {'isbn': '013235613', 'name': 'Python Web Development with Django', 'pages': 350, 'rating': Decimal('4.00'), 'price': Decimal('29.69'),\n                 'contact_id': 5, 'publisher_id': 3, 'pubdate': datetime.date(2008, 11, 3), 'num_authors': 3},\n                {'isbn': '013790395', 'name': 'Artificial Intelligence: A Modern Approach',", "    def test_aggregate_with_window_function(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            result = list(\n                Book.objects.annotate(\n                    avg_price=Avg('price')\n                        .over(partition_by=[F('publisher__name')])\n                )\n                .values('publisher__name', 'avg_price')\n                .distinct()\n            )\n\n        self.assertEqual(len(captured_queries), 1)\n        self.assertEqual(\n            result,\n            [\n                {'publisher__name': 'Apress', 'avg_price': Decimal('30.00')},\n                {'publisher__name': 'Sams', 'avg_price': Decimal('23.09')},\n                {'publisher__name': 'Prentice Hall', 'avg_price': Decimal('34.70')},\n                {'publisher__name': 'Morgan Kaufmann', 'avg_price': Decimal('75.00')},\n                {'publisher__name': \"Jonno's House of Books\", 'avg_price': None},\n            ]\n        )\n\n", "    def test_aggregate_with_window_function(self):\n        with CaptureQueriesContext(connection) as queries:\n            # Test aggregate function with OVER clause (window function)\n            qs = Book.objects.annotate(\n                avg_rating=Window(Avg('rating'), partition_by=F('publisher__name'))).values(\n                'publisher__name', 'avg_rating'\n            )\n            self.assertQuerySetEqual(qs, [\n                {'publisher__name': 'Apress', 'avg_rating': Decimal('4.25')},\n                {'publisher__name': 'Sams', 'avg_rating': Decimal('3.00')},\n                {'publisher__name': 'Prentice Hall', 'avg_rating': Decimal('4.00')},\n                {'publisher__name': 'Morgan Kaufmann', 'avg_rating': Decimal('5.00')},\n                {'publisher__name': \"Jonno's House of Books\", 'avg_rating': None},\n            ], transform=lambda x: {'avg_rating': Approximate(x['avg_rating'])})\n            self.assertEqual(len(queries), 1)\n", "    def test_aggregate_with_window_function(self):\n        with CaptureQueriesContext(self.request) as captured_queries:\n            qs = Book.objects.annotate(\n                rank=Window(\n                    expression=Rank(),\n                    partition_by=F('contact'),\n                    order_by=F('rating').desc()\n                )\n            ).filter(rank__lte=2)\n            self.assertQuerySetEqual(qs, [\n                'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp',\n                'Artificial Intelligence: A Modern Approach',\n                'The Definitive Guide to Django: Web Development Done Right',\n                'Sams Teach Yourself Django in 24 Hours',\n                'Practical Django Projects',\n            ], lambda b: b.name)\n\n        self.assertEqual(len(captured_queries), 1)\n\n", "    def test_stddev_variance_sample(self):\n        with self.assertNumQueries(1):\n            self.assertAlmostEqual(\n                Book.objects.annotate(std_dev=StdDev('price', sample=True)).values('std_dev').get('std_dev'),\n                Decimal('19.245000'),\n                places=4\n            )\n            self.assertAlmostEqual(\n                Book.objects.annotate(variance=Variance('price', sample=True)).values('variance').get(\n                    'variance'\n                ),\n                Decimal('370.4999'),\n                places=4\n            )\n"], "sample_525": ["def test_constrained_layout():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    fig.set_constrained_layout(True)\n    fig.tight_layout()\n\n", "def test_figure_tight_layout_rect():\n    fig, ax = plt.subplots()\n    fig.tight_layout(rect=[0.1, 0.1, 0.9, 0.9])\n    assert ax.get_position().to_list() == [0.1, 0.1, 0.8, 0.8]\n", "def test_figure_constrained_layout():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1], [0, 1])\n    fig.tight_layout()\n    assert fig.get_layout_engine() == ConstrainedLayoutEngine\n\n\n", "def test_figure_savefig_transparent():\n    fig = Figure()\n    fig.patch.set_alpha(0)\n    ax = fig.add_subplot()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    with io.BytesIO() as buf:\n        fig.savefig(buf, transparent=True)\n        buf.seek(0)\n        img = Image.open(buf)\n        assert img.mode == 'RGBA'\n", "def test_figaspect():\n    # Test for basic figaspect functionality with a float\n    w, h = figaspect(2.)\n    assert w / h == 1 / 2\n\n    # Test for figaspect with an array\n    A = np.random.rand(5, 3)  \n    w, h = figaspect(A)\n    assert w / h == 3 / 5\n\n"], "sample_829": ["    def test_partial_fit_n_samples(self):\n        # Test that partial_fit raises an error when n_samples in a batch is\n        # less than n_components\n        ipca = IncrementalPCA(n_components=3, batch_size=2)\n        X = np.random.rand(5, 4)\n        assert_raises_regex(ValueError,\n                            \"n_components=3 must be less or equal to \"\n                            \"the batch number of samples 2.\",\n                            ipca.partial_fit, X)\n", "    def test_sparse_input_partial_fit_error(self):\n        ipca = IncrementalPCA()\n        X = sparse.csr_matrix(iris.data)\n        with assert_raises_regex(TypeError,\n                                 \"IncrementalPCA.partial_fit does not \"\n                                 \"support sparse input\"):\n            ipca.partial_fit(X)\n", "    def test_incremental_fit_sparse_input(self):\n        \"\"\"Check that IncrementalPCA handles sparse input correctly.\"\"\"\n\n        # Generate a sparse matrix\n        X = sparse.csr_matrix(np.random.rand(100, 10))\n\n        # Test fit with sparse input\n        ipca = IncrementalPCA(n_components=2, batch_size=20)\n        ipca.fit(X)\n        projected = ipca.transform(X)\n\n        # Test transform with sparse input\n        assert_raises_regex(TypeError,\n                            \"IncrementalPCA.partial_fit does not support\",\n                            ipca.partial_fit, X)\n\n        # Compare with PCA on dense input\n        pca = PCA(n_components=2)\n        pca.fit(X.toarray())\n        assert_almost_equal(pca.explained_variance_,\n                            ipca.explained_variance_)\n        assert_allclose_dense_sparse(pca.transform(X.toarray()),\n                                       projected)\n\n", "    def test_sparse_input(self):\n        X = sparse.csr_matrix(iris.data)\n        ipca = IncrementalPCA(n_components=2, batch_size=50)\n        ipca.fit(X)\n        X_transformed = ipca.transform(X)\n\n        pca = PCA(n_components=2)\n        pca.fit(iris.data)\n        X_transformed_dense = pca.transform(iris.data)\n\n        assert_allclose_dense_sparse(X_transformed, X_transformed_dense)\n", "    def test_sparse_fit_transform(self):\n        # Test fit_transform with sparse data.\n        X_sparse = sparse.csr_matrix(iris.data)\n        ipca = IncrementalPCA(n_components=2, batch_size=50)\n        X_transformed = ipca.fit_transform(X_sparse)\n        assert_allclose_dense_sparse(X_transformed,\n                                     PCA(n_components=2).fit_transform(iris.data))\n"], "sample_604": ["    def test_format_timedelta_timedelta_format_date_with_NaT():\n        data = np.array([pd.Timedelta(\"1 days\"), pd.NaT, pd.Timedelta(\"2 days\")])\n        arr = xr.DataArray(data, dims=[\"time\"])\n        assert formatting.format_array_flat(arr, 20) == \"1 days  2 days\"\n", "    def test_format_item_with_timedelta(self):\n        t = np.timedelta64(1, 'D')\n        assert formatting.format_item(t) == '1 days'\n        assert formatting.format_item(t, timedelta_format='date') == '1 days'\n        assert formatting.format_item(t, timedelta_format='time') == '0 days'\n", "    def test_wrap_indent():\n        text = \"This is a long line of text that will be wrapped to multiple lines.\"\n        expected = dedent(\n            \"\"\"\n            This is a long line of text that will be\n            wrapped to multiple lines.\n            \"\"\"\n        )\n        assert formatting.wrap_indent(text) == expected\n\n", "    def test_format_timedelta_with_timedelta_format(self):\n        t = pd.Timedelta(days=1, hours=2, minutes=30)\n        assert formatting.format_timedelta(t) == \"1 days 02:30:00\"\n        assert formatting.format_timedelta(t, timedelta_format=\"date\") == \"1 days\"\n        assert formatting.format_timedelta(t, timedelta_format=\"time\") == \"02:30:00\"\n", "    def test_format_timedelta(self):\n        # test formatting of Timedelta\n\n        # test basic formatting\n        assert formatting.format_timedelta(pd.Timedelta(\"1 days\")) == \"1 days\"\n        assert formatting.format_timedelta(pd.Timedelta(\"12:34:56\")) == \"12:34:56\"\n\n        # test with different timedelta_formats\n        assert formatting.format_timedelta(\n            pd.Timedelta(\"1 days 12:34:56\"), timedelta_format=\"datetime\"\n        ) == \"1 days 12:34:56\"\n        assert formatting.format_timedelta(\n            pd.Timedelta(\"1 days 12:34:56\"), timedelta_format=\"date\"\n        ) == \"1 days\"\n        assert formatting.format_timedelta(\n            pd.Timedelta(\"1 days 12:34:56\"), timedelta_format=\"time\"\n        ) == \"12:34:56\"\n\n        # test with NaT\n        assert formatting.format_timedelta(pd.NaT) == \"NaT\"\n"], "sample_7": ["    def test_masked_column_insert(self):\n        col = table.MaskedColumn([1, 2, 3], name='test', dtype=int)\n        col.mask = [False, True, False]\n\n        # Test inserting at the beginning\n        new_col = col.insert(0, [0], mask=[False])\n        assert_array_equal(new_col.data, [0, 1, 2, 3])\n        assert_array_equal(new_col.mask, [False, True, False, False])\n\n        # Test inserting in the middle\n        new_col = col.insert(1, [4], mask=[False])\n        assert_array_equal(new_col.data, [1, 4, 2, 3])\n        assert_array_equal(new_col.mask, [False, False, True, False])\n\n        # Test inserting at the end\n        new_col = col.insert(3, [5], mask=[False])\n        assert_array_equal(new_col.data, [1, 2, 3, 5])\n        assert_array_equal(new_col.mask, [False, True, False, False])\n", "    def test_masked_column_copy(self):\n        col = table.Column([1, 2, 3], name='a', dtype=int, mask=[True, False, False],\n                           unit=u.km/u.s)\n        col2 = col.copy()\n        assert_array_equal(col2.mask, [True, False, False])\n        assert col2.unit == col.unit\n        assert col2.name == col.name\n", "    def test_masked_column_insert(self):\n        t = table.Table()\n        t['a'] = table.Column([1, 2, 3], name='a', dtype=int)\n\n        # Test inserting a single element\n        t['a'].insert(1, 4)\n        assert_array_equal(t['a'], [1, 4, 2, 3])\n\n        # Test inserting multiple elements\n        t['a'].insert(2, [5, 6])\n        assert_array_equal(t['a'], [1, 4, 5, 6, 2, 3])\n\n        # Test inserting at the beginning\n        t['a'].insert(0, 0)\n        assert_array_equal(t['a'], [0, 1, 4, 5, 6, 2, 3])\n\n        # Test inserting with mask\n        t['b'] = table.MaskedColumn([1, 2, 3], name='b', dtype=int, mask=[False, True, False])\n        t['b'].insert(1, 4, mask=True)\n        assert_array_equal(t['b'].data, [1, 4, 2, 3])\n        assert_array_equal(t['b'].mask, [False, True, False, False])\n\n", "    def test_masked_column_insert_mixed_types(self):\n        t = table.Table()\n        t['a'] = table.Column([1, 2, 3], dtype=int)\n        t['b'] = table.Column(['a', 'b', 'c'], dtype='S1')\n        t['c'] = table.MaskedColumn([1.0, 2.0, 3.0], mask=[False, True, False], dtype=float)\n\n        t.insert_row(1, [4, 'd', 4.0])\n        assert_array_equal(t['a'], [1, 4, 2, 3])\n        assert_array_equal(t['b'], ['a', 'd', 'b', 'c'])\n        assert_array_equal(t['c'].data, [1., 4., 2., 3.])\n        assert_array_equal(t['c'].mask, [False, False, True, False])\n", "    def test_column_masked_column_insert(self):\n        t = table.Table({'a': [1, 2, 3], 'b': [4, 5, 6]}, masked=True)\n        t['a'][1] = np.ma.masked\n        assert np.any(t['a'].mask)\n        t.add_column(table.Column([7,8,9], name='c', masked=True))\n        t['c'][1] = np.ma.masked\n        assert np.any(t['c'].mask)\n\n        t.insert_row(1)\n        assert len(t) == 4\n        assert np.all(t['a'] == [1, np.ma.masked, 2, 3])\n        assert np.all(t['b'] == [4, np.ma.masked, 5, 6])\n        assert np.all(t['c'] == [7, np.ma.masked, 8, 9])\n"], "sample_765": ["def test_classification_report_labels():\n    y_true = np.array([0, 1, 2, 0, 1, 2])\n    y_pred = np.array([0, 2, 1, 0, 0, 2])\n\n    report = classification_report(y_true, y_pred, output_dict=True)\n\n    assert set(report.keys()) == {\n        '0',\n        '1',\n        '2',\n        'accuracy',\n        'macro avg',\n        'weighted avg'\n    }\n", "    def test_brier_score_loss_errors():\n        # Test errors for brier_score_loss\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n        with pytest.raises(ValueError):\n            brier_score_loss(y_true, y_prob, pos_label=-1)\n        with pytest.raises(ValueError):\n            brier_score_loss(y_true, y_prob, pos_label=2)\n        y_prob_bad = np.array([0.4, 5, 0.8, 0.3])\n        with pytest.raises(ValueError):\n            brier_score_loss(y_true, y_prob_bad)\n\n        y_prob_bad = np.array([0.4, -0.1, 0.8, 0.3])\n        with pytest.raises(ValueError):\n            brier_score_loss(y_true, y_prob_bad)\n\n", "def test_brier_score_loss_binary():\n    # Test brier score loss for binary classification\n    y_true = np.array([0, 1, 1, 0])\n    y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n    score = brier_score_loss(y_true, y_prob)\n    assert_almost_equal(score, 0.0375, decimal=4)\n", "    def test_brier_score_loss_unchanged_warning():\n\n        # check if a warning is raised when y_prob contains unchanged values\n        y_true = [0, 1, 1, 0]\n        y_prob = [0.2, 0.2, 0.2, 0.2]\n        with pytest.warns(UndefinedMetricWarning):\n            brier_score_loss(y_true, y_prob)\n", "    def test_multilabel_classification_report_empty_labels():\n        y_true = np.array([[0, 1], [1, 0]])\n        y_pred = np.array([[0, 1], [0, 0]])\n        with pytest.warns(UndefinedMetricWarning):\n            report = classification_report(y_true, y_pred, labels=[0, 1])\n\n"], "sample_930": ["    def test_create_index_with_triple(self, app, env):\n        env.domains['index'].entries = {\n            'dummy.txt': [\n                ('triple', 'foo bar baz', 'target1', 'main1', None),\n            ],\n        }\n        entries = IndexEntries(env).create_index(app.builder)\n        assert entries[0][1][0][0] == ('main1', '#target1')\n        assert entries[0][1][1][0][0] == ('foo bar baz', '')\n", "def test_create_index_single(app, env):\n    env.domaindata['index']['entries'] = {\n        'index.rst': [('single', 'entry1', 'entry1_id', 'document', None)],\n    }\n    entries = IndexEntries(env).create_index(app.builder)\n    assert entries == [\n        ('entry1', [\n            (['document', None], {}, None)\n        ])\n    ]\n", "    def test_index_entries_with_see(app, env):\n        env.get_domain('index').entries = [\n            ('path', 'single', 'entry_id', 'main', 'key'),\n            ('path', 'see', 'entry_id', 'see also main', 'key'),\n            ('path', 'index', 'entry_id', 'main1', 'key'),\n            ('path', 'seealso', 'entry_id', 'see also main1', 'key'),\n        ]\n        indexentries = IndexEntries(env)\n\n        index = indexentries.create_index(app.builder)\n        assert index[0][1][0][0][0] == 'main'\n        assert index[0][1][1][0][0] == 'see also main'\n        assert index[0][1][2][0][0] == 'main1'\n        assert index[0][1][3][0][0] == 'see also main1'\n\n", "        def test_indexentries_single(self, app):\n            app.env.domaindata['index']['entries'] = [\n                ('dummy.rst',\n                 ('single', 'entry', 'target', 'Main', None))\n            ]\n            entries = IndexEntries(app.env).create_index(app.builder)\n            assert entries == [\n                ('E', [('entry', [('Main', '')])]),\n            ]\n", "def test_indexentries_seealso(app, status, warning):\n    app.env.get_domain('py').directives['function'].object_type = 'function'\n    app.env.domaindata['py']['objects'] = {'key': [('function', 'module', 'key', 1)]}\n    app.build()\n\n    expected_index = [\n        ('seealso', [\n            ('key', 'genindex.html#key'),\n        ]),\n    ]\n\n    assert 'seealso' in app.env.indexentries\n    assert app.env.indexentries['seealso'] == expected_index\n"], "sample_562": ["    def test_set_data_empty(self):\n        l = mlines.Line2D([], [])\n        assert_array_equal(l.get_xdata(), [])\n        assert_array_equal(l.get_ydata(), [])\n        l.set_data(np.arange(10), np.arange(10))\n        assert_array_equal(l.get_xdata(), np.arange(10))\n        assert_array_equal(l.get_ydata(), np.arange(10)) \n", "    def test_get_dash_pattern(self, linestyle, expected):\n\n        assert mlines.Line2D._get_dash_pattern(linestyle) == expected\n", "    def test_set_markevery(self, fig_test, fig_ref):\n        ax_test = fig_test.subplots()\n        ax_ref = fig_ref.subplots()\n        x = np.arange(100)\n        y = np.sin(x)\n        line_test = ax_test.plot(x, y, markevery=10)[0]\n        line_ref = ax_ref.plot(x, y, marker='o', markevery=10)[0]\n        # Make sure the lines are identical\n        assert (line_test.get_xdata() == line_ref.get_xdata()).all()\n        assert (line_test.get_ydata() == line_ref.get_ydata()).all()\n\n", "    def test_linestyle(self, ls):\n        line = mlines.Line2D([], [], linestyle=ls)\n        assert line.get_linestyle() == ls\n\n", "    def test_markerstyle(self, marker, expected):\n        result = mlines.MarkerStyle(marker)\n        assert result == expected\n\n"], "sample_283": ["    def test_runshell_with_environment_variables(self):\n        with mock.patch('django.db.backends.postgresql.client.subprocess.Popen', autospec=True) as Popen:\n            connection.settings_dict = {\n                'HOST': os.environ.get('PGHOST'),\n                'NAME': os.environ.get('PGDATABASE'),\n                'USER': os.environ.get('PGUSER','django'),\n            }\n            DatabaseClient().runshell(['-c', 'SELECT version()'])\n            Popen.assert_called_with(\n                ['psql', '-U', 'django', '-h', os.environ.get('PGHOST'),\n                os.environ.get('PGDATABASE'), '-c', 'SELECT version()'],\n                env={'PGPASSWORD': os.environ.get('PGPASSWORD')}\n            )\n", "    def test_runshell_sigint(self):\n        with mock.patch('django.db.backends.postgresql.client.subprocess.Popen') as mock_popen:\n            mock_popen.return_value.communicate.return_value = (b'', b'')\n            original_handler = signal.getsignal(signal.SIGINT)\n            try:\n                DatabaseClient().runshell(['some_param'])\n            finally:\n                signal.signal(signal.SIGINT, original_handler)\n            mock_popen.assert_called_once()\n", "    def test_runshell_with_ssl_options(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            settings_dict = {\n                'NAME': 'test_db',\n                'USER': 'test_user',\n                'HOST': 'test_host',\n                'PORT': 5432,\n                'OPTIONS': {\n                    'sslmode': 'require',\n                    'sslrootcert': 'test_root_cert',\n                    'sslcert': 'test_cert',\n                    'sslkey': 'test_key',\n                },\n            }\n            client = DatabaseClient(settings_dict)\n            client.runshell([])\n            mock_popen.assert_called_once_with(\n                ['psql', '-U', 'test_user', '-h', 'test_host', '-p', '5432', 'test_db'],\n                env={\n                    'PGPASSWORD': os.environ.get('PGPASSWORD', ''),\n                    'PGSSLMODE': 'require',\n                    'PGSSLROOTCERT': 'test_root_cert',\n                    'PGSSLCERT': 'test_cert',\n                    'PGSSLKEY': 'test_key',\n                },\n            )\n", "    def test_runshell_sigint(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            mock_popen.return_value.communicate = mock.Mock(\n                return_value=('Output', '')\n            )\n            with mock.patch('signal.signal') as mock_signal:\n                DatabaseClient().runshell(['dummy'])\n                mock_signal.assert_has_calls([\n                    mock.call(signal.SIGINT, signal.SIG_IGN),\n                    mock.call(signal.SIGINT, mock.ANY),\n                ])\n\n\n\n", "    def test_runshell_with_ssl_options(self):\n        with mock.patch('subprocess.Popen') as mock_popen:\n            settings_dict = {\n                'NAME': 'test_db',\n                'USER': 'test_user',\n                'HOST': 'test_host',\n                'PORT': 5432,\n                'OPTIONS': {\n                    'sslmode': 'require',\n                    'sslrootcert': 'test_root_cert.pem',\n                    'sslcert': 'test_client_cert.pem',\n                    'sslkey': 'test_client_key.pem',\n                },\n            }\n            DatabaseClient.runshell([], settings_dict)\n            mock_popen.assert_called_once_with(\n                ['psql', '-U', 'test_user', '-h', 'test_host', '-p', '5432', 'test_db'],\n                env={\n                    'PGSSLMODE': 'require',\n                    'PGSSLROOTCERT': 'test_root_cert.pem',\n                    'PGSSLCERT': 'test_client_cert.pem',\n                    'PGSSLKEY': 'test_client_key.pem',\n                },\n            )\n"], "sample_1102": ["def test_Poly_add():\n    R = QQ\n    x = Symbol('x')\n\n    f = Poly(x**2 + 2*x + 1, x, domain=R)\n    g = Poly(x - 1, x, domain=R)\n\n    assert f + g == Poly(x**2 + 3*x, x, domain=R)\n\n    f = Poly(y**2 + 2*y + 1, y, domain=R)\n    assert f.add(g) == Poly(y**2 + 2*y + x - 1, x, y, domain=R)\n\n    # Test adding a constant\n    assert f + 2 == Poly(y**2 + 2*y + 3, y, domain=R)\n\n    # Test adding with different domains\n    raises(CoercionFailed, lambda: f + Poly(x, x, domain=ZZ))\n\n    # Test adding with incompatible variables\n    raises(MultivariatePolynomialError, lambda: f + Poly(x, x, domain=R))\n\n", "    def test_poly_from_expr_rational_coeffs(self):\n        f = Poly(x**2 + 2*x - y/2, x, y)\n        g = Poly(x**2 + 2*x - Rational(1, 2)*y, x, y)\n        assert f == g\n", "    def test__torational_factor_list(self):\n        f = Poly(x**3 + 2*x + 1, x)\n        factors = _torational_factor_list(f)\n        assert factors == [(x + 1), (x**2 - x + 1)]\n\n        f = Poly(x**4 - 1, x)\n        factors = _torational_factor_list(f)\n        assert factors == [(x - 1), (x + 1), (x**2 + 1)]\n", "def test_Poly_from_expr_imaginary():\n    assert Poly(I*x, x).as_expr() == I*x\n    assert Poly(x + I, x).as_expr() == x + I\n    assert Poly(x*I, x).as_expr() == I*x\n", "def test_poly_reduce():\n    # Test cases for poly.reduce function\n    f = 2*x**4 - x**2 + y**3 + y**2\n    G = groebner([x**3 - x, y**3 - y])\n    q, r = poly.reduce(f, *G)\n    assert _strict_eq(r, x**2 + y**2 + y)\n\n    f = x**2*y + 2*x*y**2 - 3*x\n    G = groebner([x**2 + y**2 - 1, x*y - 2])\n    q, r = poly.reduce(f, *G)\n    assert _strict_eq(r, x**2*y + 2*x*y**2 - 3*x)\n\n"], "sample_76": ["    def test_check_setting_languages_bidi(self):\n        # Test with valid and invalid values in settings.LANGUAGES_BIDI\n        settings.LANGUAGES_BIDI = self.valid_tags[:3]\n        self.assertEqual(check_setting_languages_bidi(None, **{}), [])\n\n        settings.LANGUAGES_BIDI = list(self.invalid_tags)\n        errors = check_setting_languages_bidi(None, **{})}\n        self.assertEqual(len(errors), len(self.invalid_tags))\n        for error in errors:\n            self.assertIsInstance(error, Error)\n", "    def test_check_setting_language_code_invalid(self):\n        settings.LANGUAGE_CODE = 123\n        errors = check_setting_language_code(None, None)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n", "    def test_check_setting_languages(self):\n        for invalid_tag in self.invalid_tags:\n            with self.subTest(invalid_tag=invalid_tag):\n                settings.LANGUAGES = [\n                    (invalid_tag, 'Invalid language'),\n                    ('en', 'English'),\n                ]\n                errors = check_setting_languages(None, **{})\n                self.assertEqual(len(errors), 1)\n                self.assertIsInstance(errors[0], Error)\n                self.assertEqual(errors[0].id, 'translation.E002')\n                self.assertIn(str(invalid_tag), errors[0].msg)\n", "    def test_check_setting_language_code_invalid(self):\n        for tag in self.invalid_tags:\n            with self.settings(LANGUAGE_CODE=tag):\n                errors = check_setting_language_code(None, **{})\n                self.assertEqual(len(errors), 1)\n                self.assertIsInstance(errors[0], Error)\n\n", "    def test_check_setting_languages_bidi(self):\n        settings.LANGUAGES_BIDI = list(self.valid_tags)\n        self.assertEqual(check_setting_languages_bidi(None, **{}), [])\n        settings.LANGUAGES_BIDI = ['en', 'not_a_valid_language_code']\n        errors = check_setting_languages_bidi(None, **{})\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'translation.E003')\n"], "sample_1118": ["    def test_MatPow_transpose():\n        assert MatPow(A, 2).T == MatPow(A.T, 2)\n", "    def test_MatPow_derivative_symbolic_negative_exponent():\n        X = MatrixSymbol('X', 2, 2)\n    \n        x = symbols('x')\n        expr = MatPow(X, -2) \n        result = expr._eval_derivative(x)\n        assert isinstance(result, MatMul)\n        assert isinstance(result.args[0], Inverse) \n        assert isinstance(result.args[1], MatPow) \n        assert result.args[1].base == X\n        assert result.args[1].exp == -3\n", "def test_MatPow_inverse():\n    assert MatPow(C.I, 2).doit() == Identity(n)\n    assert MatPow(Inverse(C), 2).doit() == Identity(n)\n\n\n", "def test_MatPow_derivative_matrix_lines():\n    a = symbols('a')\n    X = MatrixSymbol('X', 2, 2)\n\n    # Test derivative of a power of a symbol\n    derivative = (X**a)._eval_derivative_matrix_lines(a)\n    assert isinstance(derivative, MatMul)\n    assert len(derivative.args) == 2\n    assert isinstance(derivative.args[0], X) and isinstance(derivative.args[1], MatPow)\n\n    # Test derivative of a SymbolicMatrix power\n    derivative = (X**2)._eval_derivative_matrix_lines(X)\n    assert isinstance(derivative, MatMul)\n    assert len(derivative.args) == 2\n    assert isinstance(derivative.args[0], Identity) and isinstance(derivative.args[1], MatMul)\n\n    # Test derivative of identity power\n    derivative = (Identity(2)**2)._eval_derivative_matrix_lines(a)\n    assert derivative == ZeroMatrix(2, 2)\n", "    def test_MatPow_transpose():\n        assert (MatPow(A, 2).T).doit() == MatPow(A.T, 2).doit()\n"], "sample_602": ["def test_to_netcdf_unlimited_dims():\n    ds = xr.Dataset(\n        {\"foo\": ((\"time\", \"x\"), np.arange(12).reshape(3, 4))},\n        coords={\"time\": np.arange(3), \"x\": np.arange(4)},\n    )\n    ds.attrs[\"some_attr\"] = \"some value\"\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        filepath = os.path.join(tmpdir, \"test.nc\")\n        ds.to_netcdf(filepath, mode=\"w\", unlimited_dims=[\"time\"])\n\n        # Reopen and check\n        ds_reopened = xr.open_dataset(filepath)\n        assert_identical(ds, ds_reopened)\n        assert ds_reopened.dims[\"time\"] == 3\n", "def test_to_netcdf_multifile_engine():\n    ds = xr.Dataset(\n        {\"foo\": ((\"x\", \"y\"), np.random.randn(5, 6))}, coords={\"x\": np.arange(5), \"y\": np.arange(6)}\n    )\n    paths = [\"foo.nc\", \"bar.nc\"]\n    with pytest.warns(UserWarning):\n        xr.to_netcdf(ds[:3], paths[0], engine=\"scipy\")\n        xr.to_netcdf(ds[3:], paths[1], engine=\"scipy\")\n    with xr.open_dataset(paths[0], engine='netcdf4') as ds1, xr.open_dataset(paths[1], engine='netcdf4') as ds2:\n        assert_identical(ds[:3], ds1)\n        assert_identical(ds[3:], ds2)\n\n    for path in paths:\n        os.remove(path)\n\n", "    def test_to_netcdf_invalid_netcdf(self):\n        ds = xr.Dataset({\"a\": (\"x\", [1, 2, 3])})\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf\"):\n            ds.to_netcdf(path=\"test.nc\", engine=\"scipy\", invalid_netcdf=True)\n\n", "    def test_to_zarr_append_with_region(\n        self,", "    def test_to_netcdf_multifile_engine(self):\n        ds = xr.Dataset({\"foo\": (\"x\", np.arange(5))})\n        paths = [\"file1.nc\", \"file2.nc\"]\n        with tmpdir() as tmp:\n            for engine in [\"netcdf4\", \"scipy\", \"h5netcdf\"]:\n                save_mfdataset(\n                    [ds, ds], [str(Path(tmp) / p) for p in paths], engine=engine\n                )\n                for p in paths:\n                    with xr.open_dataset(str(Path(tmp) / p)) as ds_loaded:\n                        assert_identical(ds, ds_loaded)\n"], "sample_1160": ["def test_intersection_sets_complexregion_empty():\n    c1 = ComplexRegion(Interval(0, 2*S.Pi), Interval(0, 1))\n    c2 = ComplexRegion(Interval(S.Pi, 3*S.Pi/2), Interval(0, 1))\n    assert intersection_sets(c1, c2) == S.EmptySet\n", "def test_intersection_complexregion_conditionset():\n    R = Symbol('R', real=True, positive=True)\n    theta = Symbol('theta')\n    s = ComplexRegion(Interval(0, R), Interval(0, 2*pi))\n    c = ConditionSet(I*R, Eq(R*cos(theta), 1))\n    i = intersection_sets(s, c)\n    assert isinstance(i, ConditionSet)\n    assert i.base_set == s\n    assert i.condition == Eq(R*cos(theta), 1)\n\n", "def test_intersection_complexregion_emptyset():\n    cr = ComplexRegion(Interval(0, 1)*Interval(0, 1))\n    assert intersection_sets(cr, S.EmptySet) == S.EmptySet\n", "def test_intersection_sets_complexregion_interval():\n    cr = ComplexRegion(Interval(0, 1)*Interval(0, pi/2))\n    i = Interval(0.5, 1)\n    assert intersection_sets(cr, i) == ComplexRegion(\n        Interval(0.5, 1)*Interval(0, pi/2))\n", "def test_intersection_range_interval_non_integer_step():\n    r = Range(1, 5, Rational(1, 2))\n    i = Interval(1.5, 4, True, True)\n    assert intersection_sets(r, i) == FiniteSet(Rational(3, 2), 2, Rational(5, 2), 3)\n\n\n\n"], "sample_427": ["    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            absolute_max=2,\n            max_num=3,\n            extra=1,\n            initial_forms=1,\n        )\n        formset = formset_class()\n        self.assertEqual(formset.absolute_max, 2)\n        self.assertEqual(formset.max_num, 3)\n\n        formset = formset_class(data={\n            'form-0-choice': 'A',\n            'form-0-votes': 1,\n            'form-1-choice': 'B',\n            'form-1-votes': 2,\n            'form-2-choice': 'C',\n            'form-2-votes': 3,\n            f'{formset.prefix}-TOTAL_FORMS': 3,\n        })\n\n        self.assertEqual(formset.errors, [])\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 3)\n\n\n\n", "    def test_formset_factory_with_renderer(self):\n        class MyForm(Form):\n            name = CharField()\n\n        class MyFormSet(formsets.BaseFormSet):\n                super().__init__(*args, **kwargs)\n                self.renderer = get_default_renderer()\n\n        MyFormSet = formset_factory(MyForm, formset=MyFormSet)\n        formset = MyFormSet()\n        self.assertEqual(formset.renderer, get_default_renderer())\n\n\n", "    def test_formset_factory_absolute_max_less_than_max_num(self):\n        with self.assertRaisesMessage(ValueError, \"'absolute_max' must be greater or equal to 'max_num'.\"):\n            formset_factory(\n                Choice,\n                max_num=10,\n                absolute_max=5,\n            )\n", "    def test_formset_factory_can_delete_extra(self):\n        formset = formset_factory(\n            FavoriteDrinkForm,\n            can_delete=True,\n            can_delete_extra=False,\n            extra=2,\n        )\n        formset_instance = formset(data={'choices-0-name': 'Water', 'choices-1-name': 'Soda', 'choices-2-name': 'Juice'})\n\n        self.assertFalse(formset_instance.errors)\n        self.assertTrue(formset_instance.is_valid())\n        self.assertGreater(len(formset_instance.cleaned_data), 2)\n\n        formset_instance = formset(data={'choices-0-DELETE': 'on','choices-1-name': 'Soda', 'choices-2-name': 'Juice'})\n        self.assertFalse(formset_instance.errors)\n        self.assertTrue(formset_instance.is_valid())\n\n        self.assertEqual(len(formset_instance.cleaned_data), 2)\n\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            max_num=3,\n            absolute_max=5,\n        )\n        formset = formset_class({'choices-TOTAL_FORMS': '5', 'choices-INITIAL_FORMS': '0'})\n        self.assertEqual(formset.total_form_count(), 3)\n        \n        formset = formset_class({'choices-TOTAL_FORMS': '6', 'choices-INITIAL_FORMS': '0'})\n        self.assertEqual(formset.total_form_count(), 3)\n\n"], "sample_1078": ["def test_indexed_bounds_negative():\n    i = Idx('i', -3)\n    assert i.lower == -3\n    assert i.upper == -2\n\n\n", "def test_idx_comparison():\n    i = Idx('i', 5)\n    j = Idx('j', 3)\n    k = Idx('k', (0, 2))\n    l = Idx('l')\n\n    assert i <= j is False\n    assert i >= j is True\n    assert i <= k\n    assert i >= k is False\n    assert k <= i\n    assert k >= i is False\n    assert l <= i is None\n    assert l >= i is None\n    assert i < j is False\n    assert i > j is True\n    assert i < k\n    assert i > k is False\n\n    assert i == i\n    assert j == j\n\n\n", "def test_IndexedBase_getitem_multiple_indices():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(3, 3))\n    assert A[i, j] == Indexed('A', i, j)\n\n    for n in range(3):\n        for m in range(3):\n            assert A[n, m] == Indexed('A', n, m)\n\n    raises(IndexException, lambda: A[i, j, k])\n\n", "def test_indexed_free_symbols():\n    i = Idx('i')\n    x = Symbol('x')\n\n    assert Indexed('A', i).free_symbols == {i}\n    assert Indexed('A', x).free_symbols == {x}\n    assert Indexed('A', i, i).free_symbols == {i}\n    assert Indexed('A', i, x).free_symbols == {i, x}\n    assert IndexedBase('B', shape=(x,)).free_symbols == {x}\n    assert Indexed('A', x, Idx('j', 4)).free_symbols == {x}\n", "def test_Indexed_shape():\n    i = Idx('i', 3)\n    j = Idx('j', 5)\n    x = IndexedBase('x')\n    assert x[i, j].shape == Tuple(3, 5)\n    assert IndexedBase('y', shape=(2, 2))[i, j].shape == Tuple(2, 2)\n"], "sample_1139": ["def test_polar_complex_region_measure():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = ComplexRegion(a*b, polar=True)\n    assert c.measure == 2*S.Pi\n\n", "def test_complexregion_polar_intersection():\n    r = Interval(0, 1)\n    theta = Interval(0, 2*S.Pi)\n    c1 = ComplexRegion(ProductSet(r, theta), polar=True)  # Unit Disk\n\n    r2 = Interval(0, 0.5)\n    theta2 = Interval(S.Pi/4, 3*S.Pi/4)\n    c2 = ComplexRegion(ProductSet(r2, theta2), polar=True)  # Sector\n\n    intersection = c1.intersect(c2)\n    expected = ComplexRegion(ProductSet(Interval(0, 0.5), Interval(S.Pi/4, 3*S.Pi/4)), polar=True)\n    assert intersection == expected\n", "def test_complex_region_polar_intersection():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = Interval(S.Pi/2, 3*S.Pi/2)\n    c1 = ComplexRegion(a*b, polar=True)\n    c2 = ComplexRegion(a*c, polar=True)\n    intersection = c1.intersect(c2)\n    assert intersection == ComplexRegion(ProductSet(a, b.intersect(c)), polar=True)\n\n", "def test_intersection():\n    a = Interval(0, 1)\n    b = Interval(1, 2)\n    c = Interval(0.5, 1.5)\n    assert Intersection(a, b).is_empty\n    assert Intersection(a, c) == Interval(0.5, 1)\n    assert Intersection(b, c) == Interval(1, 1.5)\n    assert Intersection(ComplexRegion(a*b, polar=True), ComplexRegion(a*c, polar=True)) == ComplexRegion(a*Intersection(b,c), polar=True)\n", "def test_complexregion_polar_normalize_theta():\n    r = Interval(1, 2)\n    theta = Interval(-3*pi/2, pi/2)\n    C = ComplexRegion(r*theta, polar=True)\n    assert C.sets == ProductSet(r, normalize_theta_set(theta))\n    assert normalize_theta_set(theta) == Union(Interval(pi/2, 3*pi/2),\n                                             Interval.Ropen(0, 2*pi))\n    assert C.expr == x + I*y\n"], "sample_929": ["    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', [])\n        assert _parse_annotation('list[str]') == ('list', ['str'])\n        assert _parse_annotation('Dict[str, int]') == ('Dict', ['str', 'int'])\n        assert _parse_annotation('Tuple[int, ...]') == ('Tuple', ['int', '...'])\n        assert _parse_annotation('Union[str, int, float]') == ('Union', ['str', 'int', 'float'])\n        assert _parse_annotation('Callable[[int], str]') == ('Callable', [[int], 'str'])\n\n        # Error cases\n        with pytest.raises(ValueError):\n            _parse_annotation('invalid')\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('List[str]') == ('List', 'str')\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', (None, 'int'))\n    assert _parse_annotation('Callable[[int, str], bool]') == (\n        'Callable', (('int', 'str'), 'bool'))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n", "    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation('int'), 'int')\n        self.assertEqual(_parse_annotation('str'), 'str')\n        self.assertEqual(_parse_annotation('typing.List[int]'), 'typing.List[int]')\n        self.assertEqual(_parse_annotation('list'), 'list')\n        self.assertEqual(_parse_annotation('None'), 'None')\n        self.assertEqual(_parse_annotation('dict'), 'dict')\n        self.assertEqual(_parse_annotation('tuple'), 'tuple')\n        self.assertEqual(_parse_annotation('Optional[str]'), 'Optional[str]')\n        self.assertEqual(_parse_annotation('Union[int, str]'), 'Union[int, str]')\n        self.assertEqual(_parse_annotation('Callable[[int], str]'), 'Callable[[int], str]')\n", "def test_parse_annotation(app):\n    env = app.builder.env\n    env.temp_data = {'reflist': []}\n\n    node = _parse_annotation('str', env)\n    assert isinstance(node, desc_annotation)\n\n    node = _parse_annotation(':None: ', env)\n    assert isinstance(node, desc_annotation) and node.astext() == ':None:'\n\n    node = _parse_annotation('List[int]', env)\n    assert isinstance(node, desc_annotation)\n    assert len(node) == 1\n    assert isinstance(node[0], addnodes.literal)\n\n    node = _parse_annotation('A.B', env)\n    assert isinstance(node, desc_annotation)\n    assert len(node) == 2\n    assert isinstance(node[0], addnodes.literal)\n\n    node = _parse_annotation('a.b.C', env)\n    assert isinstance(node, desc_annotation)\n    assert len(node) == 3\n    assert isinstance(node[0], addnodes.literal)\n\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('List[str]') == ('List', ['str'])\n    assert _parse_annotation('Optional[int]') == ('Optional', ['int'])\n    assert _parse_annotation('Union[int, str]') == ('Union', ['int', 'str'])\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ['str', 'int'])\n    assert _parse_annotation('Callable[[int, str], bool]') == ('Callable', [[\n        'int', 'str'\n    ], 'bool'])\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', ['int', '...'])\n    assert _parse_annotation('Any') == ('Any', None)\n\n"], "sample_315": ["    def test_language_prefix_redirect(self):\n        request = RequestFactory().get('/some/path/')\n        middleware = LocaleMiddleware()\n        response = middleware.process_request(request)\n        self.assertIsNone(response)\n\n        response = middleware.process_response(request, HttpResponse())\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.status_code, 308)\n        self.assertEqual(response['Location'], '/en/some/path/')\n", "    def test_redirect_to_language_prefix(self):\n        factory = RequestFactory()\n\n        # Request a URL without a language prefix.\n        request = factory.get('/test/')\n        middleware = LocaleMiddleware()\n        response = middleware.process_request(request)\n        self.assertEqual(response.status_code, 200)\n\n        request = factory.get('/test/')\n        response = middleware.process_response(request, HttpResponse())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Language'], 'en-us')\n\n        # Check that the redirect is 301 (permanent)\n        middleware = PermanentRedirectLocaleMiddleWare()\n        response = middleware.process_response(request, HttpResponse())\n        self.assertEqual(response.status_code, 301) \n\n", "    def test_redirect_to_language_url(self):\n        request_factory = RequestFactory()\n\n        # Test redirect from a path without language prefix to a URL with\n        # language prefix when USE_I18N is True and\n        # LANGUAGES contains the language.\n\n        request = request_factory.get('/test/')\n        response = LocaleMiddleware().process_response(request, HttpResponse(''))\n        self.assertEqual(response.status_code, 301)\n        self.assertEqual(response['Location'], '/en/test/')\n", "    def test_redirect_to_language_prefix_url(self):\n        request = RequestFactory().get('/some/url/')\n        middleware = LocaleMiddleware()\n        response = middleware.process_request(request)\n\n        self.assertEqual(translation.get_language(), settings.LANGUAGE_CODE)\n\n        response = middleware.process_response(request, HttpResponse())\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['Content-Language'], settings.LANGUAGE_CODE)\n\n        request = RequestFactory().get('/some/url')\n        response = middleware.process_request(request)\n        self.assertEqual(translation.get_language(), settings.LANGUAGE_CODE)\n\n        response = middleware.process_response(request, HttpResponse())\n\n        redirect_url = response.url\n        self.assertEqual(response.status_code, 301)\n        self.assertTrue(redirect_url.startswith(f'/{settings.LANGUAGE_CODE}/'))\n        self.assertIn('/some/url', redirect_url)\n        self.assertEqual(response.headers['Vary'], 'Accept-Language, Cookie')\n", "    def test_language_prefix_redirect(self):\n        # Test if a redirect happens when the default language is absent from the URL\n        # and i18n_patterns are used with a prefixed default language.\n        request = RequestFactory().get('/some/page/')\n        request.LANGUAGE_CODE = translation.get_language()\n        middleware = LocaleMiddleware()\n        middleware.process_request(request)\n        response = middleware.process_response(request, HttpResponse())\n        self.assertIsInstance(response, HttpResponsePermanentRedirect)\n        self.assertEqual(response.status_code, 301)\n        self.assertIn('en-us/some/page/', response['Location'])\n"], "sample_96": ["    def test_list_editable_with_list_display_links(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = ['name']\n            list_display_links = ['name']\n            list_editable = ['name']\n\n        self.assertIsInvalid(MyModelAdmin, User,\n                             \"The value of 'list_editable[0]' cannot be in both 'list_editable' and 'list_display_links'.\",\n                             id='admin.E123')\n", "    def test_ModelAdmin_list_filter_invalid_field(self):\n        class InvalidModelAdmin(ModelAdmin):\n            model = ValidationTestModel\n            list_filter = ['invalid_field']\n\n        self.assertIsInvalid(\n            InvalidModelAdmin,\n            ValidationTestModel,\n            r\"The value of 'list_filter\\[0\\]' refers to 'invalid_field', which does not refer to a Field.\",\n            id='admin.E116',\n        )\n\n", "    def test_check_list_editable_with_fk_in_list_display_links(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = ('name', 'band', 'song')\n            list_display_links = ('name', 'band')\n            list_editable = ('name', 'band')\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            Band,\n            r\"The value of 'list_editable\\[2\\]' refers to 'song', which is not contained in 'list_display'.\",\n            id='admin.E122',\n        )\n\n", "    def test_inline_list_display_links(self):\n        class MyInline(TabularInline):\n            model = Song\n            list_display_links = ['title']\n\n        class MyAdmin(ModelAdmin):\n            inlines = [MyInline]\n\n        self.assertIsInvalid(MyAdmin, Band,\n                             msg=r\"The value of 'list_display_links' refers to 'title', which is not defined in 'list_display'.\", id='admin.E111')\n\n", "    def test_raw_id_fields_with_callable(self):\n        class MyModelAdmin(ModelAdmin):\n            list_display = ('name', 'get_formatted_date')\n            raw_id_fields = ('band',)\n\n                return obj.date.strftime('%Y-%m-%d')\n\n        class MyModel(Model):\n            name = models.CharField(max_length=100)\n            date = models.DateField()\n            band = models.ForeignKey(Band, on_delete=models.CASCADE)\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            MyModel,\n            r\"The value of 'raw_id_fields' should only contain Field objects, not callables\",\n        )\n"], "sample_1033": ["def test_as_content_primitive_radical():\n    from sympy import sqrt\n    assert (2*sqrt(2) + 4*sqrt(10)).as_content_primitive(radical=True) == (2, sqrt(2)*(1 + 2*sqrt(5)))\n", "def test_as_content_primitive_rational():\n    assert (3*x + 6*y).as_content_primitive() == (3, x + 2*y)\n    assert (2*x/3 + 4*y/9).as_content_primitive() == (2/9, 3*x + 2*y)\n    assert (2*x/3 + 4.2*y).as_content_primitive() == (1/3, 2*x + 12.6*y)\n\n\n\n", "def test_add_evalf():\n    assert (x + 1).evalf() == x + 1\n    assert (x + 1).evalf(subs={x: 2}) == 3\n    assert (2 + 3*I).evalf() == 2 + 3*I\n", "def test_as_content_primitive_radical():\n    # test the extraction of radical content\n    e = 2*sqrt(2) + 4*sqrt(10)\n    con, prim = e.as_content_primitive(radical=True)\n    assert con == 2 and prim == sqrt(2)*(1 + 2*sqrt(5))\n", "def test_as_content_primitive_radical():\n    from sympy import sqrt\n    assert (2*sqrt(2) + 4*sqrt(10)).as_content_primitive(\n        radical=True) == (2, sqrt(2)*(1 + 2*sqrt(5)))\n"], "sample_261": ["    def test_parse_datetime_with_timezone_offset(self):\n        self.assertEqual(\n            parse_datetime('2023-10-26T12:34:56+02:00'),\n            datetime(2023, 10, 26, 12, 34, 56, tzinfo=get_fixed_timezone(120)),\n        )\n        self.assertEqual(\n            parse_datetime('2023-10-26T12:34:56-05:30'),\n            datetime(2023, 10, 26, 12, 34, 56, tzinfo=get_fixed_timezone(-330)),\n        )\n", "    def test_parse_datetime_with_timezone(self):\n        # Test parsing datetime strings with time zone offsets.\n        dt_str = '2023-10-27T12:34:56+02:00'\n        expected_dt = datetime(2023, 10, 27, 12, 34, 56, tzinfo=get_fixed_timezone(120))\n        self.assertEqual(parse_datetime(dt_str), expected_dt)\n\n        dt_str = '2023-10-27T12:34:56-05:30'\n        expected_dt = datetime(2023, 10, 27, 12, 34, 56, tzinfo=get_fixed_timezone(-330))\n        self.assertEqual(parse_datetime(dt_str), expected_dt)\n", "    def test_parse_datetime_with_tzinfo(self):\n        dt = parse_datetime('2023-10-26T12:34:56.789+02:00')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.microsecond, 789000)\n        self.assertEqual(dt.tzinfo.utcoffset(dt), timedelta(hours=2))\n", "    def test_parse_datetime_with_offset(self):\n        dt = parse_datetime(\"2023-10-27T12:34:56+01:00\")\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(60))\n\n", "    def test_parse_datetime_tzinfo_postgres(self):\n        \"\"\"Test parsing datetime strings with time zone offsets.\"\"\"\n        dt = parse_datetime('2023-10-27 14:30:00+02:00')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n\n"], "sample_533": ["    def test_contour_algorithm(self, algorithm):\n        # Regression test for https://github.com/matplotlib/matplotlib/issues/19941\n        if contourpy.__version__ < '1.0.0':\n            pytest.skip(f\" ContourPy version {contourpy.__version__} is too old for this test.\")\n\n        with rc_context({'contour.algorithm': algorithm}):\n            x = np.array([1, 2, 3, 4, 5])\n            y = np.array([1, 2, 3, 4, 5])\n            z = np.array([[1, 2, 3, 4, 5],\n                          [2, 3, 4, 5, 6],\n                          [3, 4, 5, 6, 7],\n                          [4, 5, 6, 7, 8],\n                          [5, 6, 7, 8, 9]])\n            fig, ax = plt.subplots()\n            contour = ax.contour(x, y, z)\n            assert len(contour.collections) > 0\n", "    def test_contour_locator(self):\n        x = np.arange(-5, 5, 0.25)\n        y = np.arange(-5, 5, 0.25)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n\n        fig, ax = plt.subplots()\n        cs = ax.contour(X, Y, Z, locator=plt.MultipleLocator(0.5))\n        ax.clabel(cs, inline=True, fontsize=8)\n        # Make sure the test passes even if plot layout changes\n\n        ax.set_aspect('equal')\n\n", "    def test_contour_filled_logscale(self):\n        fig, ax = plt.subplots()\n\n        with np.errstate(divide='ignore'):\n            x = np.linspace(1, 10, 100)\n            y = np.linspace(1, 10, 100)\n            X, Y = np.meshgrid(x, y)\n            Z = np.log(X * Y)\n\n        contour = ax.contourf(X, Y, Z, 10, locator=plt.LogLocator(),\n                            cmap='viridis', norm=LogNorm())\n        fig.colorbar(contour)\n", "    def test_manual_levels(self):\n        x = np.arange(-5, 5, 0.25)\n        y = np.arange(-5, 5, 0.25)\n        X, Y = np.meshgrid(x, y)\n        Z = np.sin(np.sqrt(X**2 + Y**2))\n        fig, ax = plt.subplots()\n        CS = ax.contour(X, Y, Z, levels=[-1, -0.5, 0, 0.5, 1])\n        ax.clabel(CS, inline=True, fontsize=8)\n", "def test_contour_algorithm(algorithm):\n    if algorithm == 'mpl2005' and contourpy.__version__ < '1.0.0':\n        pytest.skip(\"mpl2005 algorithm requires contourpy>=1.0.0\")\n    x = np.arange(0, 10, 0.1)\n    y = np.arange(0, 10, 0.1)\n    X, Y = np.meshgrid(x, y)\n    Z = np.sin(X) * np.cos(Y)\n    fig, ax = plt.subplots()\n    with rc_context({'contour.algorithm': algorithm}):\n        contour = ax.contour(X, Y, Z)\n    ax.clabel(contour)\n\n\n"], "sample_894": ["    def test_classification_toy_oob_score(self):\n        \"\"\"Check oob_score on a toy dataset.\"\"\"\n        ForestClassifier = FOREST_CLASSIFIERS[\"RandomForestClassifier\"]\n\n        clf = ForestClassifier(n_estimators=10, oob_score=True, random_state=1)\n        clf.fit(X_large, y_large)\n        assert clf.oob_score_ >= 0\n        assert clf.oob_score_ <= 1\n\n", "def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X_reg, y_reg)\n    assert mean_squared_error(y_reg, reg.predict(X_reg)) < 0.1\n\n    reg = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    reg.fit(X_reg, y_reg)\n    assert mean_squared_error(y_reg, reg.predict(X_reg)) < 0.1\n", "    def test_classification_toy_sparse_input(self):\n        \"\"\"Check classification on a toy dataset with sparse input.\"\"\"\n        X_sparse = csr_matrix(X)\n        ForestClassifier = FOREST_CLASSIFIERS[\"ExtraTreesClassifier\"]\n        clf = ForestClassifier(n_estimators=10, random_state=1)\n        clf.fit(X_sparse, y)\n        assert_array_equal(clf.predict(csr_matrix(T)), true_result)\n        assert 10 == len(clf)\n", "    def test_oob_score_classification(self, name):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        for n_estimators in [1, 10]:\n            clf = ForestClassifier(\n                n_estimators=n_estimators,\n                oob_score=True,\n                random_state=0\n            ).fit(X_large, y_large)\n            assert hasattr(clf, \"oob_score_\")\n            assert clf.oob_score_ >= 0.0\n            assert clf.oob_score_ <= 1.0\n\n", "def check_regression_toy(name):\n    \"\"\"Check regression on a toy dataset.\"\"\"\n    ForestRegressor = FOREST_REGRESSORS[name]\n\n    reg = ForestRegressor(n_estimators=10, random_state=1)\n    reg.fit(X_reg, y_reg)\n    score = mean_squared_error(y_reg, reg.predict(X_reg))\n    assert score < 10\n\n    reg = ForestRegressor(n_estimators=10, max_features=1, random_state=1)\n    reg.fit(X_reg, y_reg)\n    score = mean_squared_error(y_reg, reg.predict(X_reg))\n    assert score < 10  \n"], "sample_270": ["    def test_index_together_references_invalid_field(self):\n        with self.assertRaisesMessage(\n            Error,\n            \"'index_together' refers to the nonexistent field 'invalid_field'.\"\n        ):\n            _check_constraints(InvalidModel, databases=['default'])\n\n", "    def test_index_together_with_expression(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.CharField(max_length=100)\n            genre = models.CharField(max_length=100)\n\n            class Meta:\n                index_together = (('author', F('genre')),)\n\n        errors = _check_indexes(Book)\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Warning)\n        self.assertEqual(\n            errors[0].msg,\n            '%s does not support indexes on expressions.' % connection.display_name\n        )\n\n", "    def test_index_together_invalid_field(self):\n        class InvalidModel(models.Model):\n            name = models.CharField(max_length=100)\n            nonexistent_field = models.CharField(max_length=100)\n            \n            class Meta:\n                index_together = [\n                    ('name', 'nonexistent_field'),\n                ]\n\n        with self.assertRaisesRegex(\n            FieldError,\n            r\"The 'index_together' option refers to the nonexistent field 'nonexistent_field'.\",\n        ):\n            InvalidModel.objects.all()\n\n", "    def test_index_together_invalid_field(self):\n        with self.assertRaisesMessage(\n            ValueError, \"The field 'invalid_field' does not exist in model 'Model'.\"\n        ):\n            class Model(models.Model):\n                name = models.CharField(max_length=100)\n                invalid_field = models.CharField(max_length=1)\n\n                class Meta:\n                    index_together = ('name', 'invalid_field',)\n\n", "    def test_check_index_together_with_expressions(self):\n        class BadModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                index_together = [('name', Abs('id'))]\n\n        errors = BadModel._meta._check_index_together()\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(\n            errors[0].msg,\n            \"Index together fields must be database columns.\",\n        )\n\n"], "sample_654": ["    def test_fixture_lookup_error_for_nonexistent_fixture():\n        pytest.raises(FixtureLookupError, lambda: fixtures.FixtureRequest(None, \"function\", None, None).getfixturevalue(\"does_not_exist\"))\n\n", "    def test_fixture_request_get_fixturevalue(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 42\n        \"\"\"\n        )\n        testdir.create_testfile(\n            \"\"\"", "    def test_issue813_fixture_with_scope_module(testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(scope=\"module\")\n                return 42\n\n                assert my_fixture == 42\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_fixture_function_name_shadowing(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            \n                return 'myfixture'\n            \n            @pytest.fixture(name='myfixture')\n                return 'myfixture_shadowed'\n\n                assert myfixture == 'myfixture_shadowed'\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_fixture_lookuperror_ordering(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return \"setup_value\"\n\n                assert teardown_fixture == \"teardown_value\"\n\n            @pytest.fixture\n                return \"teardown_value\"\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n"], "sample_701": ["    def test_fillfuncargs(self) -> None:\n        with warnings.catch_warnings(record=True) as record:\n            deprecated.FILLFUNCARGS.warn(name=\"fillfuncargs\")\n        assert len(record) == 1\n        assert str(record[0].message) == (\n            \"fillfuncargs is deprecated, use \"\n            \"function._request._fillfixtures() instead if you cannot avoid reaching into internals.\"\n        )\n\n", "    def test_deprecated_fillfuncargs(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n            @pytest.fixture\n                return 42\n\n            pytest.fillfunc_args(test_func, myfixture)\n        \"\"\"\n        )\n        with pytest.warns(deprecated.FILLFUNCARGS):\n            pytester.runpytest()\n", "    def test_pytest_collect_module(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n            pytest.collect.test_func\n        \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.assert_outcomes(failed=1)\n        result.stdout.fnmatch_lines(\n            [\n                \"*DeprecationWarning: pytest.collect.test_func was moved to pytest.test_func*\",\n            ]\n        )\n", "    def test_deprecated_fillfuncargs(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n                return 1\n\n                request.fillfixtures(arg=1)\n                pytest.deprecated.FILLFUNCARGS.issue(\n                    name=\"request.fillfixtures\",\n                )\n                assert request._fillfixtures() == {\"arg\": 1}\n\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n        assert result.stderr.fnmatch(\"*deprecated*request.fillfixtures*\")\n", "    def test_deprecated_collect_module(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        pytester.runpytest(\n            \"-v\", \"-W\", \"ignore::pytest.PytestDeprecationWarning\"\n        ).assert_outcomes(passed=1)\n        result = pytester.runpytest('-v')\n        result.stdout.fnmatch_lines(\n            [\n                \"*pytest.collect.pytest_collect_module was moved to pytest.pytest_collect_module*\",\n            ]\n        )\n        assert result.ret == 1\n\n"], "sample_913": ["def test_parse_annotation():\n    assert _parse_annotation('str') == ('str', None)\n    assert _parse_annotation('list[int]') == ('list', 'int')\n    assert _parse_annotation('Dict[str, float]') == ('Dict', 'str, float')\n    assert _parse_annotation('Tuple[Any, ...]') == ('Tuple', 'Any, ...')\n    assert _parse_annotation('Union[int, str]') == ('Union', 'int, str')\n    assert _parse_annotation('Optional[str]') == ('Optional', 'str')\n\n", "def test_parse_annotation_with_default():\n    node = _parse_annotation('x: int = 5')\n    assert isinstance(node, desc_annotation)\n    assert node.astext() == 'x: int = 5'\n\n", "    def test_parse_annotation(app, env):\n        ret_ann = _parse_annotation('-> int')\n        assert ret_ann == 'int'\n\n        arg_ann = _parse_annotation('x: str')\n        assert arg_ann == 'str'\n\n        arg_ann = _parse_annotation('x: typing.List[str]')\n        assert arg_ann == 'typing.List[str]'\n", "def test_parse_annotation():\n    assert _parse_annotation('x: int') == ('x', 'int')\n    assert _parse_annotation('x: typing.List[int]') == ('x', 'typing.List[int]')\n    assert _parse_annotation('x: Union[int, str]') == ('x', 'Union[int, str]')\n    assert _parse_annotation('x') == ('x', None)\n    assert _parse_annotation(':int') == (None, 'int')\n    assert _parse_annotation('') == (None, None)\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", None)\n    assert _parse_annotation(\"Optional[str]\") == (\"Optional\", [\"str\"])\n    assert _parse_annotation(\"List[Tuple[int, float]]\") == (\"List\", [(\"Tuple\", [\"int\", \"float\"])])\n    assert _parse_annotation(\"Callable[[int], str]\") == (\"Callable\", [[(\"int\",)], \"str\"])\n\n\n\n"], "sample_434": ["    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse()\n\n        self.assertTrue(AsyncView.view_is_async)\n\n        class SyncView(View):\n                return HttpResponse()\n\n        self.assertFalse(SyncView.view_is_async)\n\n        class MixedView(View):\n            async def get(self, request):\n                return HttpResponse()\n\n                return HttpResponse()\n\n        with self.assertRaises(ImproperlyConfigured):\n            MixedView.view_is_async\n", "    def test_view_is_async(self):\n        class AsycView(View):\n            async def get(self, request, *args, **kwargs):\n                pass\n\n        view = AsycView.as_view()\n        self.assertTrue(view.view_class.view_is_async)\n\n", "    def test_view_dispatch_options(self):\n        view = View()\n        request = RequestFactory().options('/')\n        response = view.dispatch(request)\n        self.assertIsInstance(response, HttpResponse)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers['Allow'], 'GET, POST, PUT, PATCH, DELETE, HEAD, OPTIONS, TRACE')\n", "    def test_http_method_not_allowed_async(self):\n        request = RequestFactory().get('/')\n        view = View.as_view()(\n            post=async_to_sync(lambda *args, **kwargs: asyncio.sleep(1) )\n        )\n        with self.assertRaises(TypeError):\n            view(request)\n", "    def test_view_is_async(self):\n        class AsyncView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n        self.assertTrue(AsyncView.view_is_async)\n\n        class SyncView(View):\n                return HttpResponse(\"Hello\")\n\n        self.assertFalse(SyncView.view_is_async)\n\n        class MixedView(View):\n            async def get(self, request):\n                return HttpResponse(\"Hello\")\n\n                return HttpResponse(\"World\")\n\n        with self.assertRaises(ImproperlyConfigured):\n            MixedView.view_is_async\n\n"], "sample_318": ["compilation error", "    def test_resolve_reverse_lookup_with_special_chars(self):\n        # Test resolving URLs with special characters like ':', '.', and '@'.\n        url = reverse('special', kwargs={'chars': 'some:resource.with@'})\n        self.assertEqual(url, '/special_chars/some%3Aresource.with%40/')\n\n", "compilation error", "    def test_resolve_url_with_trailing_slash(self):\n\n        with override_script_prefix('/'):\n            resolved_path = reverse('people3', kwargs={'state': 'il', 'name': 'adrian'})\n\n            self.assertEqual(resolved_path, '/people/il/adrian/')\n\n            resolved_path = reverse('people3', kwargs={'state': 'il', 'name': 'adrian'}, url_format='path')\n\n            self.assertEqual(resolved_path, '/people/il/adrian/')\n", "compilation error"], "sample_1065": ["    def test_binomial_Mod(self):\n        n, k, q = symbols('n k q', integer=True)\n        assert binomial(3, 2).Mod(5) == 3\n        assert binomial(5, 2).Mod(7) == 1\n        assert binomial(10, 5).Mod(11) == 1\n        assert binomial(n, k).Mod(q) == binomial(n, k) % q\n\n        assert binomial(-1, 1).Mod(10) == 0\n        assert binomial(3, 3).Mod(5) == 1\n        assert binomial(10, 0).Mod(2) == 1\n        assert binomial(10, 10).Mod(3) == 1\n\n        assert binomial(100, 50).Mod(101) == 1\n", "    def test_binomial_Mod_2():\n        n, k, q = symbols('n k q', integer=True)\n        assert binomial(n, k).Mod(q) == binomial(n, k) % q\n", "    def test_binomial_Mod():\n        assert binomial(4, 2).Mod(7) == 6\n        assert binomial(7, 3).Mod(5) == 3 \n        assert binomial(10, 5).Mod(2) == 0\n        assert binomial(100, 50).Mod(101) == 1\n        assert binomial(-1, 1).Mod(5) == 0 \n        assert binomial(2, -1).Mod(5) == 0\n        assert binomial(5, 2).Mod(100) == 10\n", "    def test_binomial_mod_lucas(self):\n        n, k, q = symbols('n k q', integer=True)\n        assert binomial(n, k) % q == binomial(n % q, k % q) % q\n\n\n", "    def test_binomial_Mod_prime(self):\n        n = 10**5\n        k = 5*10**4\n        q = 17\n        assert binomial(n, k).Mod(q) == binomial(n, k) % q\n\n"], "sample_439": ["    def test_field_order(self):\n        class OrderedForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderedForm({'field3': 'c', 'field1': 'a', 'field2': 'b'})\n\n        self.assertEqual(list(form.fields), ['field1', 'field2', 'field3'])\n        self.assertEqual(form.cleaned_data, {'field1': 'a', 'field2': 'b', 'field3': 'c'})\n\n        class OrderedForm2(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = OrderedForm2({'field3': 'c', 'field1': 'a', 'field2': 'b'})\n        form.field_order = ['field2', 'field1', 'field3']\n        self.assertEqual(list(form.fields), ['field2', 'field1', 'field3'])\n        self.assertEqual(form.cleaned_data, {'field2': 'b', 'field1': 'a', 'field3': 'c'})\n\n", "    def test_form_unicode_representation(self):\n        form = FrameworkForm({'name': 'Django', 'language': 'P'})\n        self.assertEqual(repr(form), \"<FrameworkForm bound=True, valid=True, fields=name,language>\")\n\n", "    def test_declared_fields(self):\n        class MyForm(Form):\n            name = CharField()\n\n        self.assertEqual(MyForm.base_fields[\"name\"].label, \"Name\")\n\n        class MySubForm(MyForm):\n            email = EmailField()\n\n        self.assertEqual(MySubForm.base_fields[\"name\"].label, \"Name\")\n        self.assertEqual(MySubForm.base_fields[\"email\"].label, \"Email\")\n", "    def test_form_hidden_fields(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        self.assertEqual(\n            form.as_p(),\n            '<p><label for=\"id_first_name\">First name:</label> <input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\"> </p>\\n<p><label for=\"id_last_name\">Last name:</label> <input type=\"text\" name=\"last_name\" value=\"Doe\" id=\"id_last_name\"> </p><p><label for=\"id_birthday\">Birthday:</label> <input type=\"text\" name=\"birthday\" id=\"id_birthday\"> </p>'\n        )\n\n", "    def test_field_order(self):\n        class OrderedForm(Form):\n            name = CharField()\n            email = EmailField()\n\n        form = OrderedForm(field_order=['email', 'name'])\n        self.assertEqual(list(form.fields), ['email', 'name'])\n\n        form = OrderedForm()\n        self.assertEqual(list(form.fields), ['name', 'email'])\n"], "sample_756": ["    def test_ordering_and_reachability(self):\n        X = generate_clustered_data(n_samples=100, n_features=2,\n                                    n_clusters=3, cluster_std=0.5,\n                                    random_state=0)\n        optics = OPTICS().fit(X)\n        assert_allclose(optics.reachability_[optics.ordering_],\n                        pairwise_distances(X[optics.ordering_],\n                                          metric='minkowski'))\n", "    def test_optics_cluster_tree(self):\n        # Test if OPTICS cluster tree builds correctly\n        # based on the reachability plot.\n\n        optics = OPTICS(min_samples=3, max_eps=np.inf, metric='euclidean',\n                        cluster_method='xi').fit(X)\n        reachability_plot = optics.reachability_\n        ordering = optics.ordering_\n        root_node = _cluster_tree(None, None, [],\n                                  reachability_plot, ordering,\n                                  min_cluster_size=10,\n                                  maxima_ratio=.75,\n                                  rejection_ratio=.7,\n                                  similarity_threshold=0.4,\n                                  significant_min=.003)\n", "def test_optics_extraction():\n    # Generate clustered data\n    X, y_true = generate_clustered_data(n_samples=100, n_features=2,\n                                        n_clusters=4, random_state=0)\n\n    # Fit OPTICS\n    optics = OPTICS(min_samples=5, max_eps=np.inf)\n    optics.fit(X)\n\n    # Extract clusters using DBSCAN with varying epsilon values\n    eps_values = [0.2, 0.5, 1.0, 2.0]\n    for eps in eps_values:\n        labels = optics.extract_dbscan(eps)\n\n        # Evaluate clustering performance\n        cm = contingency_matrix(y_true, labels)\n        # Assert that the contingency matrix is not empty or all zeros\n        assert np.any(cm), f\"Clustering failed for eps={eps}\"\n\n", "def test_optics_clustering_variable_density():\n    # Test OPTICS clustering with variable density data.\n\n    # Generate data with different densities\n    X = generate_clustered_data(n_samples=50, n_features=2,\n                                centers=[[0, 0], [3, 4], [6, 0]],\n                                cluster_std=[0.5, 1.2, 2.0],\n                                random_state=0)\n\n    # Fit OPTICS with a large max_eps to capture all clusters\n    optics = OPTICS(min_samples=5, max_eps=10, metric='euclidean',\n                    cluster_criterion='dbscan')\n    optics.fit(X)\n\n    # Extract clusters\n    labels = optics.labels_\n\n    # Check that 3 clusters were found\n    n_clusters = len(np.unique(labels[labels >= 0]))\n    assert_equal(n_clusters, 3)\n\n    # Check cluster connectivity\n    connectivity = pairwise_distances(X, metric='euclidean') < 5\n    contingency_mat = contingency_matrix(labels[labels >= 0],\n                                         np.arange(n_clusters))\n\n\n", "    def test_optics_extract_dbscan_different_eps():\n        X, true_labels = generate_clustered_data(n_samples=100,\n                                                n_features=2,\n                                                n_clusters=3,\n                                                random_state=0)\n        optics_model = OPTICS(min_samples=5, max_eps=100).fit(X)\n        labels_05 = optics_model.extract_dbscan(eps=0.5)\n        labels_1 = optics_model.extract_dbscan(eps=1)\n        labels_2 = optics_model.extract_dbscan(eps=2)\n        assert_equal(len(np.unique(labels_05[1])), 3)\n        assert_equal(len(np.unique(labels_1[1])), 3)\n        assert_equal(len(np.unique(labels_2[1])), 2)\n\n        # Check that changing eps results in different clusterings\n        assert_not_equal(labels_05[1], labels_1[1])\n        assert_not_equal(labels_1[1], labels_2[1])\n\n"], "sample_478": ["    def test_raw_id_fields_foreign_key_field(self):\n        class RawIdModelAdmin(ModelAdmin):\n            raw_id_fields = ('band',)\n\n        self.assertIsInvalid(\n            RawIdModelAdmin,\n            Song,\n            \"The value of 'raw_id_fields' refers to 'band', which is not a \"\n            \"Foreign Key field.\",\n            id=\"admin.E020\",\n        )\n", "    def test_raw_id_fields_filter_vertical(self):\n        \"\"\"\n        Test that `ModelAdmin.raw_id_fields` can only be used with\n        `ModelAdmin.filter_horizontal` when `ModelAdmin.list_display` is\n        set to `VERTICAL`.\n        \"\"\"\n        class TestAdmin(ModelAdmin):\n            raw_id_fields = (\"band\",)\n            filter_horizontal = (\"members\",)\n            list_display = (\"name\",)\n\n        self.assertIsInvalid(\n            TestAdmin, Band, \"raw_id_fields can only be used with filter_horizontal when list_display is set to VERTICAL.\"\n        )\n\n        class TestAdminVertical(ModelAdmin):\n            raw_id_fields = (\"band\",)\n            filter_horizontal = (\"members\",)\n            list_display = (VERTICAL,)\n\n        self.assertIsValid(TestAdminVertical, Band)\n\n\n\n", "    def test_model_admin_raw_id_fields_with_invalid_relation_fields(self):\n        class InvalidModelAdmin(ModelAdmin):\n            raw_id_fields = (\"nonexistent_field\", )\n\n        self.assertIsInvalid(\n            InvalidModelAdmin,\n            ValidationTestModel,\n            r\"The value of 'raw_id_fields' refers to 'nonexistent_field', which does not refer to a Field\\.\",\n            id=\"admin.E022\",\n        )\n\n", "    def test_raw_id_fields_check(self):\n        class MyAdmin(ModelAdmin):\n            raw_id_fields = ['nonexistent_field']\n\n        self.assertIsInvalid(\n            MyAdmin,\n            ValidationTestModel,\n            r\"The value of 'raw_id_fields' refers to 'nonexistent_field', which is not a field of 'ValidationTestModel'.\",\n            id=\"admin.E107\",\n        )\n", "    def test_raw_id_fields_on_inline_model(self):\n        class ValidationInline(TabularInline):\n            model = ValidationTestInlineModel\n            raw_id_fields = ['band']\n\n        class ValidationAdmin(ModelAdmin):\n            inlines = [ValidationInline]\n\n        self.assertIsInvalid(\n            ValidationAdmin,\n            ValidationTestModel,\n            msg=\"Inline models cannot have raw_id_fields.\",\n            id=\"admin.E104\",\n        )\n"], "sample_776": ["    def test_lars_path_residues_duplicate_columns(self):\n        # Test that _lars_path_residues handles duplicate columns in X\n        X = np.array([[1, 2, 3],\n                      [4, 5, 6],\n                      [7, 8, 9]])\n        y = np.array([10, 11, 12])\n        # Add a duplicate column\n        X = np.hstack((X, X[:, 0:1]))\n        with pytest.raises(ValueError):\n            _lars_path_residues(X, y, Gram=None, copy=False, method='lar',\n                                verbose=0, normalize=False,\n                                fit_intercept=True, max_iter=500, eps=np.finfo(np.float).eps,\n                                positive=False)\n\n", "    def test_lars_lars_ic_warning(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        model = linear_model.LassoLarsIC()\n        with pytest.warns(ConvergenceWarning):\n            model.fit(X, y)\n", "    def test_lars_path_residues(self):\n        rng = np.random.RandomState(42)\n        n_samples = 30\n        n_features = 10\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        X_test = rng.randn(n_samples, n_features)\n        y_test = rng.randn(n_samples)\n        Gram = np.dot(X.T, X)\n\n        for method in (\"lar\", \"lasso\"):\n            for fit_intercept in (True, False):\n                for positive in (True, False):\n                    alphas, active, _, residues = _lars_path_residues(\n                        X, y, X_test, y_test, Gram=Gram, copy=False,\n                        method=method, verbose=0, fit_intercept=fit_intercept,\n                        normalize=False, max_iter=500, eps=np.finfo(np.float).eps,\n                        positive=positive)\n                    residues.shape == (n_samples, len(alphas))\n\n", "    def test_lars_path_residues_positive_alpha(self):\n        n_samples, n_features = 5, 3\n        rng = np.random.RandomState(0)\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        # Test with a positive alpha\n        alpha = 0.1\n\n        # Fit the Lars path\n        _, _, _, residues = _lars_path_residues(\n            X, y, Xy=None, Gram=None, alpha=alpha, copy_X=True,\n            positive=True, verbose=False, max_iter=100\n        )\n\n        assert_greater(residues[0, 0], 0)\n\n", "    def test_lars_path_residues(self, random_state=42):\n        rng = np.random.RandomState(random_state)\n        X = rng.randn(100, 10)\n        y = rng.randn(100)\n        X_test = rng.randn(20, 10)\n        y_test = rng.randn(20)\n\n        # Test for different Gram computation options\n        for Gram in [None, 'auto', X.T @ X]:\n            # Test for different precomputed Gram handling\n            for precompute_ in [True, False]:\n                path, _, _, residues = _lars_path_residues(\n                    X, y, X_test, y_test, Gram=Gram, copy=False,\n                    method='lar', verbose=0, normalize=False,\n                    fit_intercept=False, max_iter=5, eps=1e-3, positive=False,\n                    precompute=precompute_)\n                # Check that residues are computed correctly\n                for i in range(len(path[1])):\n                    coef = path[1][i]\n                    assert_allclose(residues[i],\n                                    y_test - X_test @ coef)\n"], "sample_1122": ["def test_polar_lift():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    z = Symbol('z', real=True)\n    assert polar_lift(4) == 4*exp_polar(0)\n    assert polar_lift(-4) == 4*exp_polar(I*pi)\n    assert polar_lift(-I) == exp_polar(-I*pi/2)\n    assert polar_lift(I + 2) == polar_lift(2 + I)\n\n    assert polar_lift(4*x) == 4*polar_lift(x)\n    assert polar_lift(4*x*y) == 4*polar_lift(x*y)\n    assert polar_lift(4*x*z) == 4*polar_lift(x)*polar_lift(z)\n    assert polar_lift(4*x*y*z) == 4*polar_lift(x)*polar_lift(y)*polar_lift(z)\n", "    def test_unpolarify_complex():\n        x, y = symbols('x y', polar=True)\n        assert unpolarify(x + I*y) == x + y*I\n        assert unpolarify(exp_polar(2*pi*I)) == 1\n        assert unpolarify(exp_polar(I*pi/3) ) == exp(I*pi/3)\n        assert unpolarify(exp_polar(2*pi*I)*x) == x\n\n", "    def test_unbranched_argument():\n        x = symbols('x', real=True)\n        assert unbranched_argument(exp(I*pi)) == pi\n        assert unbranched_argument(exp_polar(I*pi*3/2)) == 3*pi/2\n        assert unbranched_argument(exp_polar(5*I*pi)) == 5*pi\n        assert unbranched_argument(exp_polar(I*pi/2)) == pi/2\n        assert unbranched_argument(x*exp(I*pi)) == pi\n        assert unbranched_argument(x*exp_polar(I*pi/2)) == pi/2\n\n", "def test_principal_branch_issue_3062():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    expr = principal_branch(x + I*y, 2*pi)\n    assert expr.expand() == x + I*y\n", "    def test_principal_branch_simple():\n        z = Symbol('z')\n        assert principal_branch(exp(2*pi*I), 2*pi) == 1\n        assert principal_branch(exp(2*pi*I*z), 2*pi) == 1\n        assert principal_branch(exp(2*pi*I*z)*2, 2*pi) == 2\n"], "sample_684": ["    def test_reprfunc_args_empty(self) -> None:\n        args = ReprFuncArgs([])\n        output = str(args)\n        assert output == \"\"\n", "    def test_reprfunc_args(self) -> None:\n            pass\n\n        frame = Frame(myfunc)\n        args = ReprFuncArgs([(arg, val) for arg, val in zip(\"xy\", (2, 3))])\n        assert args.lines == [\"x = 2\", \"y = 3\"]\n", "    def test_reprfuncargs(self):\n        f = mock.Mock(name=\"func\")\n        f.place_as = f\n\n        f.args = ((1, 2, 3), \"abcde\")\n\n        info = ExceptionInfo()\n        info._getreprcrash = mock.Mock(return_value=None)\n\n        reprentry = ReprEntry(\n            [\"   def func(a, b, c):\"],\n            ReprFuncArgs([(\"a\", 1), (\"b\", 2), (\"c\", 3)]),\n            None,\n            None,\n            \"short\",\n        )\n\n        output = io.StringIO()\n        tw = TerminalWriter(file=output)\n\n        reprentry.toterminal(tw)\n\n        output = output.getvalue()\n        assert \"a = 1\" in output\n\n\n", "def test_getfslineno_no_source(monkeypatch):\n        pass\n\n    with monkeypatch.setattr(inspect, \"getsourcefile\", lambda obj: None):\n        with monkeypatch.setattr(inspect, \"getfile\", lambda obj: None):\n            fspath, lineno = getfslineno(myfunc)\n            assert fspath == \"\"\n            assert lineno == -1\n", "    def test_getfslineno_no_source(self):\n        class NoSource:\n            pass\n\n        _, lineno = getfslineno(NoSource())\n        assert lineno == -1\n"], "sample_215": ["    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'test')\n        self.assertEqual(repr(wrapped_callable), '<callable>')\n", "    def test_callable_setting_wrapper(self):\n        wrapped = CallableSettingWrapper(lambda: 'foo')\n        self.assertEqual(repr(wrapped), '<bound method <lambda> of <object object at 0x...>>')\n", "    def test_callable_setting_wrapper_repr(self):\n        wrapped_callable = CallableSettingWrapper(lambda: True)\n        self.assertEqual(repr(wrapped_callable), '<bound method CallableSettingWrapper.__call__ of CallableSettingWrapper object at 0x...>'.replace('0x...', id(wrapped_callable).to_bytes(16, byteorder='big').hex()))\n", "    def test_callable_setting_wrapper_repr(self):\n        wrapped = CallableSettingWrapper(lambda: None)\n        self.assertEqual(repr(wrapped), '<CallableSettingWrapper: <function <lambda> at 0x...>>')\n", "    def test_callable_setting_wrapper_repr(self):\n            pass\n        wrapper = CallableSettingWrapper(example_callable)\n        self.assertEqual(repr(wrapper), repr(example_callable))\n"], "sample_465": ["    def test_get_form_for_get_fields(self):\n        class MyModelAdmin(ModelAdmin):\n                return MyForm\n\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = Band\n                fields = ['name']\n\n        admin_class = MyModelAdmin(Band, self.site)\n        form = admin_class._get_form_for_get_fields(request)\n        self.assertEqual(form, MyForm)\n\n", "    def test_inline_formset_can_delete(self):\n        class TestInline(TabularInline):\n            model = Song\n            formset = forms.inlineformset_factory(Band, Song, extra=1)\n            can_delete = True\n\n        class TestAdmin(ModelAdmin):\n            inlines = [TestInline]\n\n        admin = TestAdmin(Song, self.site)\n        formset = admin.get_formset(request, self.band)\n        self.assertTrue(formset.can_delete)\n", "    def test_get_formset_exclude(self):\n        class MyModelAdmin(ModelAdmin):\n            model = Band\n            exclude = ('sign_date',)\n        \n        admin = MyModelAdmin(Band, self.site)\n        formset = admin.get_formset(request)\n        self.assertNotIn('sign_date', formset.form._meta.fields)\n", "    def test_get_formset_extra(self):\n        class MyInline(TabularInline):\n            model = Song\n            extra = 2\n\n        admin = ModelAdmin(Song, self.site)\n        formset = admin.get_formset(request, inst=self.band, inline=MyInline)\n        self.assertEqual(formset.extra, 2)\n", "    def test_get_formset_exclude(self):\n        class TestAdmin(ModelAdmin):\n            model = Band\n            exclude = ['sign_date']\n\n        admin = TestAdmin(Band, self.site)\n        formset = admin.get_formset(request)\n        self.assertNotIn('sign_date', formset.form.declared_fields)\n"], "sample_447": ["    def test_functions_coalesce(self):\n        # Test coalesce function with different argument types\n        coalesced_value = Book.objects.annotate(\n            coalesced_price=Coalesce(\n                F(\"price\"),\n                Value(Decimal(\"0.00\"), output_field=DecimalField()),\n            )\n        ).values(\"coalesced_price\").get()\n        self.assertEqual(coalesced_value[\"coalesced_price\"], Decimal(\"30.00\"))\n\n        # Test coalesce with all None values\n        book_without_price = Book.objects.create(\n            isbn=\"978-3-16-148410-0\", name=\"The Book Without a Price\"\n        )\n        coalesced_value = Book.objects.filter(pk=book_without_price.pk).annotate(\n            coalesced_price=Coalesce(F(\"price\"), Value(None))\n        ).values(\"coalesced_price\").get()\n        self.assertIsNone(coalesced_value[\"coalesced_price\"])\n", "    def test_coalesce_expressions(self):\n        result = Author.objects.annotate(\n            coalesced_age=Coalesce(\n                F(\"age\"), Value(0), output_field=IntegerField()\n            )\n        ).values_list(\"name\", \"coalesced_age\")\n        self.assertSequenceEqual(\n            list(result),\n            [\n                (\"Adrian Holovaty\", 34),\n                (\"Jacob Kaplan-Moss\", 35),\n                (\"Brad Dayley\", 45),\n                (\"James Bennett\", 29),\n                (\"Jeffrey Forcier\", 37),\n                (\"Paul Bissex\", 29),\n                (\"Wesley J. Chun\", 25),\n                (\"Peter Norvig\", 57),\n                (\"Stuart Russell\", 46),\n            ],\n        )\n", "    def test_window_frame_clause(self):\n        # Test the RowRange and ValueRange frame clauses for window functions\n        qs = (\n            Book.objects.annotate(\n                row_number=RowNumber()\n                .over(partition_by=F(\"publisher\"), order_by=F(\"pubdate\").desc())\n            )\n            .filter(row_number=1)\n            .values(\"name\", \"publisher__name\")\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                {\"name\": \"Artificial Intelligence: A Modern Approach\", \"publisher__name\": \"Prentice Hall\"},\n                {\"name\": \"The Definitive Guide to Django: Web Development Done Right\", \"publisher__name\": \"Apress\"},\n                {\"name\": \"Sams Teach Yourself Django in 24 Hours\", \"publisher__name\": \"Sams\"},\n                {\"name\": \"Practical Django Projects\", \"publisher__name\": \"Apress\"},\n            ],\n            transform=lambda x: {\n                \"publisher__name\": x[\"publisher__name\"],\n                \"name\": x[\"name\"],\n            },\n        )\n\n\n", "    def test_non_aggregate_annotation_with_window_function(self):\n        qs = Book.objects.annotate(\n            avg_price=Window(\n                Avg(\"price\"), partition_by=F(\"publisher__name\"), order_by=F(\"pubdate\").desc()\n            )\n        )\n        self.assertQuerysetEqual(\n            qs.order_by(\"publisher__name\", \"pubdate\"),\n            [\n                (\"013235613\", \"Python Web Development with Django\", Decimal(\"29.69\"), Decimal(\"29.69\")),\n                (\"013790395\", \"Artificial Intelligence: A Modern Approach\", Decimal(\"82.80\"), Decimal(\"82.80\")),\n                (\"155860191\", \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", Decimal(\"75.00\"), Decimal(\"75.00\")),\n                (\"159059725\", \"The Definitive Guide to Django: Web Development Done Right\", Decimal(\"30.00\"), Decimal(\"30.00\")),\n                (\"159059996\", \"Practical Django Projects\", Decimal(\"29.69\"), Decimal(\"29.69\")),\n                (\"067232959\", \"Sams Teach Yourself Django in 24 Hours\", Decimal(\"23.09\"), Decimal(\"23.09\")),\n            ],\n        )\n", "    def test_case_when_then_value(self):\n        # Test Case with a simple WHEN/THEN/ELSE, checking the case of a value.\n        result = list(\n            Book.objects.annotate(\n                is_expensive=Case(\n                    When(price__gt=Decimal(\"30.00\"), then=Value(True)),\n                    default=Value(False),\n                )\n            ).values(\"is_expensive\")\n        )\n        self.assertEqual(result[0][\"is_expensive\"], True)\n        self.assertEqual(result[1][\"is_expensive\"], False)\n"], "sample_495": ["    def test_paginator_empty_first_page(self):\n        self.check_paginator(\n            (Article.objects.none(), 10, 0, True),\n            (0, 0, range(0)),\n        )\n", "    def test_pagination_with_empty_first_page(self):\n        \"\"\"\n        Tests pagination when allow_empty_first_page is True,\n        and the first page has no items.\n        \"\"\"\n        objects = []\n        paginator = Paginator(objects, 10, allow_empty_first_page=True)\n        self.assertEqual(paginator.count, 0)\n        self.assertEqual(paginator.num_pages, 0)\n        self.assertEqual(list(paginator.page_range), [])\n", "    def test_paginator_empty_page(self):\n        # Tests that paginator handles empty pages correctly.\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UnorderedObjectListWarning)\n            paginator = Paginator([], 2)\n        self.check_paginator([[], 2], [0, 0, range(0)])\n        with self.assertRaises(EmptyPage):\n            paginator.page(1)\n", "    def test_paginator_with_empty_first_page(self):\n        \"\"\"\n        Test that Paginator correctly handles allow_empty_first_page.\n        \"\"\"\n        paginator = Paginator([], 2, allow_empty_first_page=True)\n        self.assertEqual(paginator.num_pages, 0)\n        paginator = Paginator([1], 2, allow_empty_first_page=True)\n        self.assertEqual(paginator.num_pages, 1)\n        paginator = Paginator([1, 2], 2, allow_empty_first_page=True)\n        self.assertEqual(paginator.num_pages, 1)\n", "    def test_page_has_other_pages(self):\n        \"\"\"\n        Test the has_other_pages property of a Page object.\n        \"\"\"\n        paginator = Paginator([1, 2, 3, 4, 5, 6], 2)\n        page1 = paginator.page(1)\n        page2 = paginator.page(2)\n        page3 = paginator.page(3)\n        self.assertTrue(page1.has_other_pages())\n        self.assertTrue(page2.has_other_pages())\n        self.assertFalse(page3.has_other_pages())\n"], "sample_78": ["    def test_dance_command(self):\n        out = StringIO()\n        err = StringIO()\n        with mock.patch('sys.stdout', new=out), mock.patch('sys.stderr', new=err):\n            management.call_command('dance', 'waltz', 'tango')\n        self.assertIn('Dancing the waltz', out.getvalue())\n        self.assertIn('Dancing the tango', out.getvalue())\n", "    def test_base_command_stealth_options(self):\n        class MyCommand(BaseCommand):\n            stealth_options = ('foo', 'bar')\n\n        command = MyCommand()\n        self.assertEqual(command.base_stealth_options + command.stealth_options, command._stealth_options)\n", "    def test_no_translations(self):\n        \"\"\"\n        Test that a command decorated with `@no_translations` runs\n        with translations deactivated.\n        \"\"\"\n        with override_settings(LANGUAGE_CODE='fr-fr'):\n            translation.activate('fr-fr')\n            out = StringIO()\n            err = StringIO()\n            with captured_stderr(err):\n                management.call_command('dance', stdout=out, stderr=err)\n            self.assertEqual(err.getvalue(), '')\n            self.assertEqual(translation.get_language(), None)\n", "    def test_dance_command(self):\n        out = StringIO()\n        err = StringIO()\n        with captured_stderr(err):\n            with self.settings(INSTALLED_APPS=['django.contrib.auth', 'user_commands']):\n                management.call_command('dance', stdout=out, stderr=err)\n\n        self.assertEqual(out.getvalue(), \"Time to dance!\\n\")\n        self.assertEqual(err.getvalue(), '')\n", "    def test_command_with_stealth_options(self):\n        class CustomCommand(BaseCommand):\n            stealth_options = ('custom_option',)\n\n                self.assertEqual(options.get('custom_option'), 'some_value')\n\n        manager = management.CallCommand(CustomCommand())\n        manager.handle(custom_option='some_value')\n"], "sample_1176": ["def test_GoldenRatio_expand():\n    assert GoldenRatio.expand(func=True) == S.Half + S.Half*sqrt(5)\n", "def test_complex_arithmetic():\n    assert (3 + 4j) + 2j == (3 + 6j)\n    assert (3 + 4j) - 2j == (3 + 2j)\n    assert (3 + 4j) * 2 == (6 + 8j)\n    assert (3 + 4j) / 2 == (1.5 + 2j)\n    assert 2 * (3 + 4j) == 6 + 8j\n    assert (3 + 4j) * (1 + 2j) == (-5 + 10j)\n\n", "def test_integer_nthroot_imaginary():\n    assert integer_nthroot(-1, 2) == I\n    assert integer_nthroot(-8, 3) == -2*I\n", "def test_is_integer():\n    assert Integer(3).is_integer is True\n    assert Rational(3, 2).is_integer is False\n    assert Float(3.0).is_integer is True\n    assert Float(3.1).is_integer is False\n", "def test_Float_from_mpmath_with_prec():\n    x = mpmath.mpf('1.0')\n    y = Float._from_mpmath(x, 53)\n    assert y.mantissa == 1000000000000000000000000000000000000000000000000000\n    assert y._prec == 53\n\n"], "sample_312": ["    def test_add_squash_same_connector(self):\n        node3 = Node(connector='AND', children=[('c', 3)])\n        result = self.node1.add(node3, 'AND')\n        self.assertEqual(result.connector, 'AND')\n        self.assertEqual(result.children, self.node1_children + node3.children)\n", "    def test_add_squashes_nodes_with_same_connector(self):\n        node3 = Node(connector='AND', children=[('c', 3)])\n        self.node1.add(node3, 'AND')\n        self.assertEqual(self.node1.children, self.node1_children + node3.children)\n", "    def test_add_squash(self):\n        node2_children = [('c', 3), ('d', 4)]\n        node2 = Node(node2_children)\n        self.node1.add(node2, 'AND')\n        self.assertEqual(self.node1.children, self.node1_children + node2_children)\n", "    def test_add_squash(self):\n        node3 = Node([('c', 3), ('d', 4)])\n        node4 = self.node1.add(node3, 'AND')\n        self.assertEqual(len(self.node1.children), 4)\n        self.assertIn(('a', 1), self.node1.children)\n        self.assertIn(('b', 2), self.node1.children)\n        self.assertIn(('c', 3), self.node1.children)\n        self.assertIn(('d', 4), self.node1.children)\n        self.assertEqual(node4, node3)\n", "    def test_add_squash(self):\n        node3 = Node(connector='AND', children=[('c', 3)])\n        self.node1.add(node3, 'AND')\n        self.assertEqual(self.node1.children, self.node1_children + [('c', 3)])\n"], "sample_436": ["    def test_runserver_ipv6_address(self):\n        self.write_settings(\"settings.py\", extra='USE_IPV6=True')\n        os.chdir(self.test_dir)\n        # Redirect stdout to bypass the colorized server output,\n        # which can differ between Python versions and platforms\n        with captured_stdout() as stdout:\n            call_command(\"runserver\", \"--ipv6\", addrport=\"::1:8000\")\n        self.assertIn(\"Starting development server at http://[::1]:8000/\", stdout.getvalue())\n", "    def test_runserver_with_ipv6(self):\n        self.write_settings(\"settings.py\")\n        os.chdir(self.test_dir)\n\n        try:\n            with captured_stderr() as stderr:\n                call_command(\n                    \"runserver\",\n                    \"--ipv6\",\n                    \"--noreload\",\n                    stdout=StringIO(),\n                )\n        except SystemExit as e:\n            self.assertEqual(e.code, 0)\n\n        self.assertEqual(stderr.getvalue(), \"\")\n\n        # Check if IPv6 address is in the server output\n        server_output = stderr.getvalue()\n        self.assertRegex(server_output, r\"Starting development server at http://\\[::1\\]:8000/\")\n", "    def test_runserver_with_allowed_hosts(self):\n        out, err = captured_stdout_and_stderr(\n            lambda: call_command(\"runserver\", addrport=\"127.0.0.1:8000\")\n        )\n        self.assertIn(\"Starting development server\", out.getvalue())\n        self.assertIn(\"http://127.0.0.1:8000/\", out.getvalue())\n", "    def test_runserver_nothreading(self):\n        \"\"\"\n        # Tests that the runserver command with the --nothreading option works correctly\n        \"\"\"\n        self.write_settings(\"settings.py\")\n        os.chdir(self.test_dir)\n        with captured_stdout() as stdout, captured_stderr() as stderr:\n            result = subprocess.run(\n                [sys.executable, \"manage.py\", \"runserver\", \"--nothreading\"],\n                capture_output=True,\n                text=True,\n            )\n        self.assertEqual(result.returncode, 0)\n\n        # Check for the expected output and no errors\n        self.assertIn(\"Development server is running at\", stdout.getvalue())\n        self.assertEqual(stderr.getvalue(), \"\")\n", "    def test_runserver_with_ipv6(self):\n        \"\"\"\n        Test running the development server with IPv6.\n        \"\"\"\n        # Ensure IPv6 is supported\n        if not socket.has_ipv6:\n            self.skipTest(\"IPv6 is not supported on this system.\")\n        with override_settings(ALLOWED_HOSTS=[\"::1\"]):\n            with captured_stderr() as stderr:\n                # Start the server with IPv6\n                execute_from_command_line([\"runserver\", \"--ipv6\"])\n            # Check for IPv6 address in the output\n            self.assertIn(\"Starting development server at http://[::1]:8000/\", stderr.getvalue())\n\n\n\n"], "sample_136": ["    def test_get_full_path_info_with_trailing_slash(self):\n        request = HttpRequest()\n        request.path_info = '/path/to/resource/'\n        self.assertEqual(request.get_full_path_info(), '/path/to/resource/')\n        self.assertEqual(request.get_full_path_info(force_append_slash=True), '/path/to/resource/')\n", "    def test_get_full_path_info_with_trailing_slash(self):\n        request = HttpRequest()\n        request.path_info = '/test/'\n        self.assertEqual(request.get_full_path_info(), '/test/')\n        self.assertEqual(request.get_full_path_info(force_append_slash=True), '/test/')\n\n", "    def test_get_host_localhost_variations(self):\n        request = HttpRequest()\n        request.META = {'HTTP_HOST': 'localhost'}\n        self.assertEqual(request.get_host(), 'localhost')\n\n        request.META = {'HTTP_HOST': 'localhost:8080'}\n        self.assertEqual(request.get_host(), 'localhost:8080')\n\n        request.META = {'HTTP_HOST': '[::1]'}\n        self.assertEqual(request.get_host(), '[::1]')\n        \n        request.META = {'HTTP_HOST': '[::1]:8080'}\n        self.assertEqual(request.get_host(), '[::1]:8080')\n\n", "    def test_querydict_mutable(self):\n        q = HttpRequest.QueryDict('', mutable=True)\n        q['key'] = 'value'\n        self.assertEqual(q['key'], 'value')\n\n        q = HttpRequest.QueryDict('', mutable=False)\n        with self.assertRaises(AttributeError):\n            q['key'] = 'value'\n        self.assertNotIn('key', q)\n\n", "    def test_http_headers_with_underscores(self):\n        environ = {\n            'CONTENT_TYPE': 'text/html',\n            'HTTP_ACCEPT_LANGUAGE': 'en-us,en;q=0.5',\n            'HTTP_USER_AGENT': 'Mozilla/5.0',\n            'CONTENT_LENGTH': '1024',\n        }\n        headers = HttpHeaders(environ)\n        self.assertEqual(headers['content-type'], 'text/html')\n        self.assertEqual(headers['accept-language'], 'en-us,en;q=0.5')\n        self.assertEqual(headers['user-agent'], 'Mozilla/5.0')\n        self.assertEqual(headers['content-length'], '1024')\n"], "sample_579": ["    def test_clustermap_colors(self):\n\n        data = self.df_norm.copy()\n        data[\"colors\"] = [\"red\", \"red\", \"blue\", \"blue\"]\n        colors = [\"red\", \"blue\"]\n\n        cbar_ax = plt.axes([0.00, 0.1, 0.05, 0.8])\n        with pytest.warns(UserWarning):\n\n            mat.clustermap(data, col_colors=data[\"colors\"].values,\n                           cbar_ax=cbar_ax, colors_ratio=0.05)\n\n            # Add test to make sure colors are correctly applied to dendrogram\n            # and heatmap\n\n", "    def test_clustermap_cbar_positioning(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            fig, ax = plt.subplots(figsize=(6, 6))\n            plotter = mat.clustermap(\n                self.df_norm, cbar_pos=(.7, .3, .15, .4),\n                figsize=(6, 6), cbar_kws={'label': 'Colorbar label'},\n                xticklabels=False, yticklabels=False\n            )\n\n            plotter.savefig(f\"{tmpdir}/test.png\")\n            plt.close(fig)\n\n            fig, ax = plt.subplots(figsize=(6, 6))\n            plotter = mat.clustermap(\n                self.df_norm, cbar_pos=None, figsize=(6, 6),\n                xticklabels=False, yticklabels=False\n            )\n            plotter.savefig(f\"{tmpdir}/test_no_cbar.png\")\n            plt.close(fig)\n\n", "    def test_clustermap_row_colors(self):\n        rs = np.random.RandomState(42)\n        data = rs.randn(5, 5)\n        row_colors = pd.DataFrame(rs.randn(5, 3),\n                                 index=pd.Index([\"A\", \"B\", \"C\", \"D\", \"E\"], name=\"letters\"))\n        g = mat.clustermap(data, row_colors=row_colors)\n\n        # Test that the row colors on the dendrogram match the provided DataFrame\n        assert_colors_equal(g.ax_row_colors.get_children()[0].get_facecolors(),\n                           row_colors.values)\n", "    def test_clustermap_with_row_colors(self):\n        rs = np.random.RandomState(1)\n        data = rs.randn(10, 5)\n        row_colors = np.array(['red', 'blue'] * 5)\n\n        with plt.figure() as fig:\n            g = mat.clustermap(data, row_colors=row_colors)\n        assert len(g.ax_row_colors.get_children()) == 1\n        assert g.ax_row_colors.get_children()[0].get_label() == 'Colorbar'\n\n        # Test with color DataFrame with a name\n        row_colors = pd.DataFrame({'color': row_colors})\n        with plt.figure() as fig:\n            g = mat.clustermap(data, row_colors=row_colors)\n        assert len(g.ax_row_colors.get_children()) == 1\n        assert g.ax_row_colors.get_children()[0].get_label() == 'Colorbar'\n\n", "    def test_clustermap_mask(self):\n        # Test that clustermap correctly masks values\n        df = self.df_norm.copy()\n        mask = np.zeros_like(df, dtype=bool)\n        mask[0, 0] = True\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            g = mat.clustermap(df, mask=mask)\n\n            # Assert that the masked cell has no color\n\n        ax = g.ax_heatmap\n        img = ax.get_images()[0]\n        data = img.get_array()\n        assert data[0, 0] == mpl.cm.get_cmap(\"viridis\").N\n\n"], "sample_98": ["    def test_server_handles_broken_pipe_error(self):\n        \"\"\"\n        Test server gracefully handles broken pipe errors.\n        \"\"\"\n        with self.assertRaises(BrokenPipeError):\n            conn = HTTPConnection(self.live_server_url.split(':')[0],\n                                   int(self.live_server_url.split(':')[1]))\n            conn.request(\"GET\", \"/\")\n            conn.close()\n\n", "    def test_custom_host_header(self):\n        \"\"\"\n        Test if the server correctly handles a custom Host header.\n        \"\"\"\n        with self.subTest('Valid Host header'):\n            conn = HTTPConnection(host='localhost', port=self.server_thread.port)\n            conn.request('GET', '/')\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(response.read().decode(), 'Hello World!')\n            conn.close()\n\n        with self.subTest('Invalid Host header'):\n            conn = HTTPConnection(host='invalidhost', port=self.server_thread.port)\n            conn.request('GET', '/')\n            response = conn.getresponse()\n            self.assertEqual(response.status, 400)\n            conn.close()\n", "    def test_broken_pipe_error(self):\n        with self.assertRaises(HTTPError) as cm:\n            self.urlopen('/nonexistent_page/')\n        self.assertEqual(cm.exception.code, 500)\n        self.assertIn(\n            \"Broken pipe from\",\n            logging.getLogger('django.server').handlers[0].buffer.getvalue().decode()\n        )\n\n\n\n", "    def test_server_closes_connection_on_content_length_zero(self):\n        # Test that the server closes the connection when Content-Length is 0.\n        with HTTPConnection(self.live_server_url_test[0]) as conn:\n            conn.request(\"GET\", \"/\")\n            response = conn.getresponse()\n            self.assertEqual(response.status, 200)\n            self.assertEqual(conn.sock.recv(1024), b\"\")\n\n", "    def test_broken_pipe_error(self):\n        with self.assertRaises(HTTPError) as cm:\n            conn = HTTPConnection(self.live_server_url_test[0])\n            conn.request('GET', '/broken_pipe/')\n            conn.getresponse()\n        self.assertEqual(cm.exception.code, 500)\n\n"], "sample_294": ["    def test_bad_origin_cross_site_request(self):\n        req = TestingHttpRequest()\n        req.method = 'POST'\n        req.META['HTTP_ORIGIN'] = 'http://attacker.com'\n        req.META['HTTP_REFERER'] = 'http://example.com'\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(req, lambda r: HttpResponse(), (), {})\n        self.assertIsNotNone(response)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn(REASON_BAD_ORIGIN % 'http://attacker.com', str(response.content))\n", "    def test_csrf_exempt_view(self):\n        req = TestingHttpRequest()\n        req.method = 'POST'\n        with self.settings(CSRF_USE_SESSIONS=False):\n            response = CsrfViewMiddleware().process_view(req, post_form_view, (), {})\n        self.assertEqual(response, None)\n", "    def test_csrf_cookie_domain_with_session_middleware(self):\n        req = self._get_GET_csrf_cookie_request()\n        req.COOKIES[settings.CSRF_COOKIE_NAME] = self._csrf_id_cookie\n        req.META['HTTP_HOST'] = 'www.example.com'\n        middleware = CsrfViewMiddleware()\n        middleware.process_request(req)\n        self.assertEqual(req.session.get(CSRF_SESSION_KEY), self._csrf_id_cookie)\n", "    def test_csrf_exempt_view_allows_requests_without_token(self):\n        request = self._get_POST_no_csrf_cookie_request()\n        response = self.middleware.process_view(request, csrf_exempt(token_view), (), {})\n        self.assertEqual(response, None)\n", "    def test_csrf_cookie_httponly_samesite(self):\n        # Ensure CSRF cookie is set with HttpOnly and SameSite=Lax flags\n        req = self._get_GET_csrf_cookie_request()\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_response(req, HttpResponse())\n        self.assertTrue(response.cookies['csrftoken'].httponly)\n        self.assertEqual(response.cookies['csrftoken']['samesite'], 'Strict')\n"], "sample_255": ["    def test_handle_one_request_with_long_requestline(self):\n        request_handler = WSGIRequestHandler(Stub())\n        request_handler.rfile = BytesIO(b'A' * 65537)\n\n        request_handler.handle_one_request()\n\n        self.assertEqual(request_handler.command, '')\n        self.assertEqual(request_handler.request_version, '')\n        self.assertEqual(request_handler.raw_requestline, b'')\n        self.assertEqual(captured_stderr().getvalue(), b'Invalid request line: HTTP request too long\\n')\n", "    def test_handle_one_request_with_long_request_line(self):\n        request_line = 'GET / HTTP/1.1\\r\\n' + 'A' * 65536\n        request = Stub(makefile=lambda mode: BytesIO(request_line.encode()))\n        handler = WSGIRequestHandler(request, None, None, None)\n        handler.handle_one_request()\n        self.assertEqual(\n            handler.wfile.getvalue().decode().strip(),\n            'HTTP/1.1 414 Request-URI Too Long\\r\\n\\r\\n',\n        )\n", "    def test_log_message_ssl_handshake_error(self):\n        request = self.request_factory.get('/')\n        request.META['SERVER_PORT'] = '443'\n        request.META['REQUEST_METHOD'] = 'GET'\n        request.META['PATH_INFO'] = '/'\n        request.META['RAW_URI'] = '/'\n\n        with captured_stderr() as stderr:\n            handler = WSGIRequestHandler(Stub())\n            handler.client_address = ('127.0.0.1', 5000)\n            handler.request = request\n            handler.log_message('Test log message', '1603', '404')\n            handler.log_message('Test log message', '1603', '500')\n        self.assertIn('You\\'re accessing the development server over HTTPS', stderr.getvalue())\n", "    def test_handle_one_request_with_broken_pipe(self):\n        request = self.request_factory.get('/')\n        request.method = 'GET'\n        environ = request._environ\n        environ['REQUEST_METHOD'] = 'GET'\n        environ['PATH_INFO'] = '/'\n        environ['SERVER_PROTOCOL'] = 'HTTP/1.1'\n        environ['CONTENT_LENGTH'] = '0'\n\n        # Mock socket and stderr to simulate broken pipe error\n        sock = Stub()\n        stderr = captured_stderr()\n\n        handler = WSGIRequestHandler(sock, BytesIO(), stderr, environ)\n        handler.close_connection = False\n\n        # Trigger broken pipe error during write\n        try:\n            handler.handle_one_request()\n        except BrokenPipeError:\n            pass\n\n        self.assertEqual(stderr.getvalue(), '')\n", "    def test_handler_cleanup_headers(self):\n        request = self.request_factory.get('/')\n        request.META['CONTENT_LENGTH'] = '100'\n        handler = WSGIRequestHandler(Stub(), Stub(), Stub(), request.META)\n        handler.headers = {}\n        handler.cleanup_headers()\n        self.assertEqual(handler.headers.get('Connection'), 'close')\n"], "sample_185": ["    def test_get_format_modules_reverse(self):\n        with patch_formats('de', DATE_FORMAT='%d.%m.%Y', TIME_FORMAT='%H:%M'):\n            modules = get_format_modules('de', reverse=True)\n            self.assertEqual(modules[0].DATE_FORMAT, '%d.%m.%Y')\n            self.assertEqual(modules[0].TIME_FORMAT, '%H:%M')\n", "    def test_get_format_modules_reverse(self):\n        with patch_formats('de', DATE_FORMAT='Y-m-d'):\n            modules = get_format_modules('de', reverse=True)\n            self.assertEqual(get_format('DATE_FORMAT', 'de'), 'Y-m-d')\n            for i, module in enumerate(reversed(modules)):\n                self.assertIsNotNone(getattr(module, 'DATE_FORMAT', None))\n", "    def test_get_format_with_reverse_modules(self):\n        with patch_formats('de',\n                          DATE_FORMAT='%d.%m.%Y',\n                          DATETIME_FORMAT='%d.%m.%Y %H:%M'):\n            with mock.patch('django.utils.formats.get_format_modules') as mock_format_modules:\n                mock_format_modules.return_value = [\n                    AppModuleStub(DATE_FORMAT='%Y-%m-%d'),\n                    AppModuleStub(DATE_FORMAT='%d.%m.%Y'),\n                ]\n                self.assertEqual(get_format('DATE_FORMAT', reverse=True), '%Y-%m-%d')\n", "    def test_localize_input_with_default_format(self):\n        with self.settings(USE_L10N=True, LANGUAGES=[('de', 'German')]):\n            activate('de')\n            self.assertEqual(localize_input('2023-10-27'), '27.10.2023')\n            deactivate()\n", "    def test_localize_input_with_default_format(self):\n        with self.settings(LANGUAGE_CODE='de'):\n            self.assertEqual(\n                localize_input('2023-10-27'),\n                '27.10.2023'\n            )\n            self.assertEqual(\n                localize_input('10:30'),\n                '10:30'\n            )\n            self.assertEqual(\n                localize_input('10:30:15'),\n                '10:30:15'\n            )\n        with self.settings(LANGUAGE_CODE='en', USE_THOUSAND_SEPARATOR=True):\n            self.assertEqual(\n                localize_input('1234567'),\n                '1,234,567'\n            )"], "sample_649": ["    def test_get_log_level_for_setting(self, config: Config) -> None:\n        config.option.log_level = \"WARNING\"\n        assert get_log_level_for_setting(config, \"log_level\") == logging.WARNING\n\n        config.option.log_level = \"INFO\"\n        assert get_log_level_for_setting(config, \"log_level\") == logging.INFO\n\n        with pytest.raises(UsageError):\n            get_log_level_for_setting(config, \"log_level\", \"nonexistent_level\")\n", "    def test_logging_config_from_cmdline(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n\n                logging.info(\"hello from test function\")\n        \"\"\"\n        )\n        result = pytester.runpytest(\n            \"--log-level=DEBUG\", \"--log-format=%(levelname)s:%(message)s\"\n        )\n        result.assert_outcomes(passed=1)\n        assert \"DEBUG:hello from test function\" in result.stdout.lines  \n\n", "    def test_set_log_path(self, pytester: Pytester) -> None:\n        config = pytester.parseconfig([\"--log-file\", \"test.log\"])\n        plugin = config.pluginmanager.get_plugin(\"logging-plugin\")\n        assert plugin is not None\n\n        log_file_path = Path(\"test.log\")\n        assert log_file_path.exists() is False\n\n        plugin.set_log_path(log_file_path.name)\n        assert log_file_path.exists() is True\n\n        log_file_path.unlink()\n", "    def test_log_level_overrides_config(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n            @pytest.mark.parametrize(\"log_level\", [\"INFO\", \"DEBUG\", \"CRITICAL\"])\n                logging.getLogger().setLevel(log_level)\n                logging.info(\"This is an info message\")\n                logging.debug(\"This is a debug message\")\n                assert caplog.records[0].levelname == log_level\n        \"\"\"\n        )\n        pytester.runpytest(\"--log-level=WARNING\")\n        result = pytester.stdout.str()\n        assert \"This is an info message\" in result\n        assert \"This is a debug message\" not in result\n", "    def test_log_cli_level(self, pytester: Pytester) -> None:\n        pytester.makeconftest(\n            \"\"\"\n            log_cli_level = DEBUG\n        \"\"\"\n        )\n        pytester.runpytest(\"--log-cli-level\", \"WARNING\")\n        result = pytester.parseoutcomes()\n        assert result.passed == 1\n        result = pytester.runpytest()\n        assert result.ret == ExitCode.OK\n        assert \"WARNING\" in result.stdout.str()\n"], "sample_344": ["    def test_get_related_models_recursive(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        state = ProjectState()\n        state.models['app_label_a', 'A'] = ModelState.from_model(A)\n        state.models['app_label_b', 'B'] = ModelState.from_model(B)\n        state.models['app_label_c', 'C'] = ModelState.from_model(C)\n        related_models = get_related_models_recursive(state, ('app_label_c', 'C'))\n        self.assertEqual(\n            set([('app_label_a', 'A'), ('app_label_b', 'B'), ('app_label_c', 'C')]),\n            related_models\n        )\n", "    def test_state_get_related_models_recursive(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        state = ProjectState()\n        state.models = {\n            (\"app\", \"a\"): ModelState.from_model(A),\n            (\"app\", \"b\"): ModelState.from_model(B),\n            (\"app\", \"c\"): ModelState.from_model(C),\n        }\n\n        related_models = get_related_models_recursive(state, (\"app\", \"c\"))\n        self.assertIn((\"app\", \"b\"), related_models)\n        self.assertIn((\"app\", \"a\"), related_models)\n\n", "    def test_related_models_recursive(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(A))\n        state.add_model(ModelState.from_model(B))\n        state.add_model(ModelState.from_model(C))\n\n        related_models = get_related_models_recursive(state, A)\n        self.assertEqual(related_models, {A, B, C})\n\n", "    def test_get_related_models_recursive(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(A))\n        state.add_model(ModelState.from_model(B))\n        state.add_model(ModelState.from_model(C))\n\n        related_models = get_related_models_recursive(state.models[('app', 'C')])\n        self.assertEqual(\n            {\n                'app.A': state.models[('app', 'A')],\n                'app.B': state.models[('app', 'B')],\n                'app.C': state.models[('app', 'C')],\n            },\n            related_models\n        )\n", "    def test_related_models_recursive(self):\n        with isolate_apps('migrations_tests'):\n            class A(models.Model):\n                pass\n            class B(models.Model):\n                a = models.ForeignKey(A, on_delete=models.CASCADE)\n            class C(models.Model):\n                b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n            project_state = ProjectState()\n\n            project_state.add_model(ModelState.from_model(A))\n            project_state.add_model(ModelState.from_model(B))\n            project_state.add_model(ModelState.from_model(C))\n            related_models = get_related_models_recursive(project_state.models[( 'migrations_tests', 'c')].fields['b'])\n            self.assertEqual(\n                related_models,\n                {\n                    ('migrations_tests', 'a'),\n                    ('migrations_tests', 'b'),\n                }\n            )\n\n\n\n"], "sample_444": ["    def test_hashed_file_name(self):\n        test_file = \"css/style.css\"\n        content = (\n            \".selector { background-image: url('/images/bg.jpg'); }\"\n        )\n\n        with self.settings(STATIC_URL=\"/static/\"):\n            with tempfile.TemporaryDirectory() as temp_dir:\n                self.create_static_file(test_file, content, temp_dir)\n                os.makedirs(os.path.join(temp_dir, \"images\"))\n                self.create_static_file(\"images/bg.jpg\", \"image data\", temp_dir)\n\n                settings.STATIC_ROOT = temp_dir\n\n                call_command(\"collectstatic\", \"--no-input\", verbosity=0)\n\n                hashed_file = storage.staticfiles_storage.stored_name(test_file)\n                self.assertNotEqual(hashed_file, test_file)\n                self.assertTrue(\n                    os.path.isfile(os.path.join(temp_dir, hashed_file))\n                )\n                self.assertPostCondition()\n\n", "    def test_manifest_strict_missing_entry(self):\n        \"\"\"\n        Test that the ManifestFilesMixin raises an error for a missing manifest entry\n        when manifest_strict is True.\n        \"\"\"\n        path = 'stylesheets/style.css'\n        self.create_file(path, '/* hello */')\n        with mock.patch.object(settings, 'STATICFILES_STORAGE', 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            with self.assertRaises(ValueError) as err:\n                call_command('collectstatic')\n        self.assertIn(f\"Missing staticfiles manifest entry for '{path}'\", str(err.exception))\n\n", "    def test_hashed_file_path_different_extension(self):\n        # Test that hashed filenames are generated correctly for different file extensions.\n        test_file = \"test.css\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            test_path = Path(tmpdir) / test_file\n            with open(test_path, \"w\") as f:\n                f.write(\"body { background-color: red; }\")\n            storage.staticfiles_storage.save(test_path.name, ContentFile(\n                f.read().encode()\n            ))\n            hashed_path = self.hashed_file_path(self, test_file)\n            self.assertTrue(hashed_path.endswith(\".css\"))\n            self.assertPostCondition()\n", "    def test_manifest_strict_mode(self):\n        \"\"\"\n        Test the manifest mode with strict mode enabled.\n        \"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            settings.STATIC_ROOT = tmpdir\n            settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n\n            # Create a file that will be missing from the manifest.\n            path = os.path.join(settings.STATIC_ROOT, 'test.txt')\n            with open(path, 'w') as f:\n                f.write('Test content')\n\n            # Generate the manifest\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            # Attempt to access the file with strict mode enabled\n            with override_settings(STATICFILES_STORAGE=f'{settings.STATICFILES_STORAGE}?strict=True'):\n                with self.assertRaises(ValueError) as e:\n                    storage.staticfiles_storage.url('test.txt')\n                self.assertEqual(str(e.exception), \"Missing staticfiles manifest entry for 'test.txt'\")\n\n", "    def test_manifest_strict(self):\n        settings.STATICFILES_DIRS = [os.path.join(TEST_ROOT, 'static')]\n        settings.STATIC_ROOT = tempfile.mkdtemp()\n\n        with mock.patch('django.contrib.staticfiles.storage.ManifestFilesMixin.hashed_name') as mock_hashed_name:\n            mock_hashed_name.side_effect = lambda name, content=None, filename=None: name\n            storage.staticfiles_storage = storage.ManifestStaticFilesStorage(manifest_storage=storage.FileSystemStorage(location=settings.STATIC_ROOT))\n\n            # Create a couple of files.\n            with open(os.path.join(settings.STATICFILES_DIRS[0], 'style.css'), 'w') as f:\n                f.write(\"body { background: #fff }\")\n            with open(os.path.join(settings.STATICFILES_DIRS[0], 'script.js'), 'w') as f:\n                f.write('console.log(\"Hello World!\");')\n\n            # Run collectstatic\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            manifest_data = storage.staticfiles_storage.read_manifest()\n            manifest = json.loads(manifest_data)\n\n            # Assert that the manifest contains the expected entries\n            self.assertIn('style.css', manifest['paths'])\n            self.assertIn('script.js', manifest['paths'])\n\n            # Now, try to access a file that doesn't exist in the manifest.\n            with self.assertRaises(ValueError) as e:\n                storage.staticfiles_storage.url('nonexistent_file.css')\n            self.assertIn(\"Missing staticfiles manifest entry for 'nonexistent_file.css'\", str(e.exception))\n\n            # Clean up\n            shutil.rmtree(settings.STATIC_ROOT)\n\n"], "sample_170": ["    def test_callable_setting_wrapper_str(self):\n        \"\"\"Test that CallableSettingWrapper correctly calls __str__ on wrapped callable.\"\"\"\n            return 'hello'\n\n        wrapped_function = CallableSettingWrapper(my_function)\n        self.assertEqual(str(wrapped_function), str(my_function()))\n", "    def test_callable_setting_wrapper_str(self):\n        class DummySetting:\n                return 'dummy_setting'\n\n        wrapped_setting = CallableSettingWrapper(DummySetting())\n        self.assertEqual(str(wrapped_setting), 'dummy_setting')\n\n", "    def test_callable_setting_wrapper(self):\n        class CallableSetting:\n                return 'callable_value'\n\n        wrapped_setting = CallableSettingWrapper(CallableSetting())\n        self.assertEqual(str(wrapped_setting), \"callable_value\")\n        self.assertEqual(repr(wrapped_setting), \"<CallableSettingWrapper object>\")\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'test')\n        self.assertEqual(repr(wrapped_callable), '<function <lambda> at 0x...> ')\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'test')\n        self.assertIsInstance(wrapped_callable, CallableSettingWrapper)\n        self.assertEqual(repr(wrapped_callable), '<CallableSettingWrapper object at 0x...> ')\n"], "sample_1093": ["    def test_piecewise_with_default(self):\n        expr = Piecewise((x, x < 1), (x**2, True))\n        self.assertEqual(pycode(expr, standard='python3'),\n                         'Piecewise((x, x < 1), (x**2, True))')\n", "    def test_MatrixSolve(self):\n        n = symbols('n')\n        A = MatrixSymbol('A', n, n)\n        b = MatrixSymbol('b', n, 1)\n        code = SciPyPrinter().doprint(MatrixSolve(A, b))\n        assert code == \"scipy.linalg.solve(A, b)\"\n", "def test_MatrixSolve():\n    n = symbols('n', integer=True)\n    A = MatrixSymbol('A', n, n)\n    b = MatrixSymbol('b', n, 1)\n    x = MatrixSymbol('x', n, 1)\n    expr = MatrixSolve(A, b)\n    assert pycode(expr, language='python') == 'sympy.MatrixSolve(A, b)'\n", "def test_print_code_array_tensor_product():\n    from sympy.codegen.array_utils import CodegenArrayTensorProduct\n    a = MatrixSymbol('a', 2, 2)\n    b = MatrixSymbol('b', 3, 2)\n    expr = CodegenArrayTensorProduct(a, b)\n    assert pycode(expr, language='numpy') == 'numpy.einsum(\"ij,kl->ikjl\", a, b)'\n", "    def test_print_Assignment(self):\n        x = symbols('x')\n        y = symbols('y')\n        code = PythonCodePrinter().doprint(Assignment(x, y))\n        assert code == \"x = y\"\n"], "sample_723": ["compilation error", "    def test_sparse_input_with_zeros(self):\n        X = sparse.csr_matrix([[1, 0, 3],\n                               [0, 4, 0],\n                               [5, 6, 7]])\n        X_true = X.copy()\n        missing_values = 0\n        strategy = 'mean'\n\n        statistics = np.array([3., 4., 4.66666667])\n        _check_statistics(X, X_true, strategy, statistics, missing_values)\n        \n", "    def test_zero_missing_values(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        X_true = X.copy()\n\n        _check_statistics(X, X_true, strategy='mean',\n                           statistics=[1.5, 3.5, 4.5, 5.5],\n                           missing_values=0)\n\n", "    def test_sparse_fit_transform_zero():\n        \"\"\"Check that the imputer correctly handles sparse matrices with\n        missing values encoded as 0.\"\"\"\n        # Missing values encoded as 0\n        X = sparse.csc_matrix([[1, 0, 2],\n                               [0, 3, 0],\n                               [4, 0, 6]])\n        X_true = sparse.csc_matrix([[1, 2.5, 2],\n                                   [2.5, 3, 2.5],\n                                   [4, 2.5, 6]])\n\n        _check_statistics(X, X_true, \"mean\",\n                          np.array([2.5, 2.5]), 0)\n", "    def test_sparse_transform_error():\n        X = sparse.csc_matrix([[1, np.nan, 3], [4, 5, np.nan]])\n        imputer = SimpleImputer(missing_values=np.nan, strategy='mean',\n                               axis=1)\n        imputer.fit(X)\n        with assert_raises(ValueError):\n            imputer.transform(X)\n"], "sample_252": ["    def test_key_transform_on_null_field(self):\n        mj = JSONModel.objects.create(data=None)\n        with self.assertRaises(OperationalError):\n            mj.data['key']\n\n", "    def test_key_transform_text_lookups_case_insensitive(self):\n        model = JSONModel.objects.create(data={'name': 'Django', 'value': 'django'})\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT JSON_EXTRACT(data, \\'$.name\\') FROM myapp_jsonmodel WHERE LOWER(JSON_EXTRACT(data, \\'$.name\\')) = LOWER(%s)', ['django'])\n            self.assertEqual(len(cursor.fetchall()), 1)\n", "    def test_key_transform_isnull_false(self):\n        obj = JSONModel.objects.create(\n            data={'key': 'value'}\n        )\n        self.assertTrue(JSONModel.objects.filter(data__key__isnull=False).exists())\n        self.assertFalse(JSONModel.objects.filter(data__key__isnull=True).exists())\n", "    def test_key_transform_with_expressionwrapper(self):\n        json_data = {'a': 1, 'b': 2}\n        obj = JSONModel.objects.create(data=json_data)\n        qs = JSONModel.objects.annotate(\n            double_a=ExpressionWrapper(\n                KeyTransform('a', obj), output_field=IntegerField()\n            ) * 2,\n        )\n        self.assertEqual(list(qs.values_list('double_a', flat=True))[0], 2)\n", "    def test_key_transform_exact_null(self):\n        obj = JSONModel.objects.create(data={'name': 'John', 'age': 30})\n\n        # Test that KeyTransformExact works with a NULL value for a key.\n        with self.assertRaisesMessage(\n            JSONModel.DoesNotExist, \"JSONFieldLookupError: Key 'age' not found.\"\n        ):\n            JSONModel.objects.get(data__age__exact=None)\n\n        # Test that KeyTransformExact with a Null value works with HasKey lookup. \n        with self.assertRaisesMessage(\n            JSONModel.DoesNotExist, \"No matching records found.\"\n        ):\n            JSONModel.objects.get(data__age__exact=None)\n\n\n\n        with CaptureQueriesContext(connection) as ctx:\n            qs = JSONModel.objects.filter(data__age__isnull=True)\n            self.assertQuerySetEqual(qs, [obj],)\n\n\n\n"], "sample_990": ["    def test_acsch_rewrite_as_log():\n        x = Symbol('x')\n        assert acsch(x)._eval_rewrite_as_log() == log(1/x + sqrt(1/x**2 + 1))\n", "def test_asech():\n    x = Symbol('x')\n    assert asech(1).evalf() == 0\n    assert asech(2).evalf() == pi*I/3\n    assert asech(-sqrt(2)).evalf() == 3*pi*I/4\n    assert asech((sqrt(6) - sqrt(2))).evalf() == pi*I/12\n    assert asech(S.Infinity) == pi*S.ImaginaryUnit/2\n    assert asech(S.NegativeInfinity) == pi*S.ImaginaryUnit/2\n    assert asech(S.Zero) == oo\n    assert asech(-S.One) == pi*S.ImaginaryUnit\n\n\n", "def test_acsch_eval():\n    x = Symbol('x')\n    assert acsch(oo) == S.Zero\n    assert acsch(-oo) == S.Zero\n    assert acsch(0) == zoo\n    assert acsch(1) == log(1 + sqrt(2))\n    assert acsch(-1) == -log(1 + sqrt(2))\n    assert acsch(S.ImaginaryUnit) == -I*pi/2\n    assert acsch(-S.ImaginaryUnit) == I*pi/2\n\n", "def test_acsch_rewrite():\n    x = Symbol('x')\n    assert asech(x)._eval_rewrite_as_log() == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n", "    def test_asech_rewrite():\n        assert asech(1).rewrite(log) == S.Zero\n        assert asech(S.Half).rewrite(log) == log(2) - log(S.Half + sqrt(3)/2)\n        assert asech(sqrt(2)).rewrite(log) == log(sqrt(2) + 1)\n        assert asech(S(2)).rewrite(log) == log(2 + sqrt(3))\n"], "sample_556": ["    def test_tight_layout_rect(self):\n        fig, ax = plt.subplots()\n        ax.set_xlabel('X Label')\n        ax.set_ylabel('Y Label')\n        ax.set_title('Title')\n        fig.tight_layout(rect=[0.1, 0.1, 0.9, 0.9])\n        fig.canvas.draw()\n\n", "    def test_constrained_layout_label_overlap(self):\n        fig, ax = plt.subplots(constrained_layout=True)\n        ax.set_xlabel('X Label', labelpad=20)\n        ax.set_ylabel('Y Label', labelpad=20)\n        ax.set_title('Title', pad=20)\n        fig.canvas.draw()\n        # Check if labels overlap with the axes or each other\n        assert not ax.xaxis.label.overlaps_with_axes\n        assert not ax.yaxis.label.overlaps_with_axes\n        assert not ax.title.overlaps_with_axes\n", "def test_stale_figure():\n    fig = Figure()\n    assert not fig.stale\n    fig.canvas = None\n    assert fig.stale\n    fig.canvas = FigureCanvasBase(fig)\n    assert not fig.stale\n\n", "    def test_figure_add_axes(self):\n        fig = Figure()\n        ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n        assert isinstance(ax1, Axes)\n        ax2 = fig.add_axes(ax1)\n        assert ax2 == ax1\n", "    def test_pickle_figure(self):\n        fig = Figure()\n        ax = fig.add_subplot()\n        ax.plot([1, 2], [3, 4])\n        # Save the figure to a buffer\n        buffer = io.BytesIO()\n        pickle.dump(fig, buffer)\n        # Load the figure from the buffer\n        buffer.seek(0)\n        loaded_fig = pickle.load(buffer)\n        # Compare the original and loaded figures\n        assert loaded_fig.axes[0].lines[0].get_ydata().tolist() == [3, 4]\n"], "sample_700": ["    def test_evaluate_skip(self, testdir):\n        testdir.create_testfile(\"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"skip this\")\n                pass\n        \"\"\")\n        item = testdir.getitems()[0]\n        with pytest.raises(pytest.skip.SkipTest) as excinfo:\n            runtestprotocol(item, doctest=False)\n        assert excinfo.value.args[0] == \"skip this\"\n", "    def test_skip_if(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == 'win32', reason=\"don't run on windows\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "    def test_skip_reason(self, testdir):\n        testdir.create_testfile(\"\"\"\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == \"win32\", reason=\"windows is bad\")\n                pass\n        \"\"\")\n        result = testdir.runpytest()\n        assert result.ret == 0\n        assert \"windows is bad\" in result.stdout.str()\n", "    def test_function_definition_evaluate_skip_marks(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n\n                if func.__name__ == 'test_func':\n                    return pytest.mark.skip(reason='some reason')\n                return func\n\n            test_func = my_marker(test_func)\n        \"\"\"\n        )\n\n        result = testdir.runpytest()\n        assert result.ret == 0\n        assert \"SKIPPED\" in result.stdout.str()\n        assert 'some reason' in result.stdout.str()\n", "    def test_skip_with_reason(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skip(reason=\"skipping for now\")\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n"], "sample_646": ["    def test_pytest_pycollect_makeitem_handles_missing_unittest(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n                yield None\n            \"\"\"\n        )\n        testdir.create_file(\"test_something.py\", \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                    pass\n        \"\"\")\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "    def test_inject_setup_teardown_fixtures(self, pytester: Pytester):\n        p = pytester.makepyfile(\n            \"\"\"\n            import unittest\n            class TestClass(unittest.TestCase):\n                    self.setup_called = True\n                    self.teardown_called = True\n                    assert self.setup_called\n                    assert not hasattr(self, 'teardown_called')\n        \"\"\"\n        )\n        result = pytester.runpytest(p, \"-v\")\n        assert result.ret == ExitCode.OK\n        result.stdout.fnmatch_lines([\n            \"*setUp*\",\n            \"test_method PASSED*\",\n            \"*tearDown*\",\n        ])\n", "    def test_runtest_xfail(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n            import pytest\n\n            class TestExample(unittest.TestCase):\n                @pytest.mark.xfail\n                      assert 1 == 2\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=0, skipped=0, xfailed=1, failed=0)\n", "    def test_collect_unittest_test_with_skip_reason(self, testdir) -> None:\n        testdir.create_file(\n            \"test_sample.py\",\n            \"\"\"\n        import unittest\n        class TestClass(unittest.TestCase):\n            @unittest.skip(\"skip reason\")\n                pass\n        \"\"\",\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(skipped=1)\n        assert result.stdout.re(\"skip reason\")\n\n", "    def test_unittest_skip(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestExample(unittest.TestCase):\n                @unittest.skip(\"reason\")\n                    assert False\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(skipped=1)\n        assert \"reason\" in result.stdout.str()\n        assert result.ret == ExitCode.OK\n"], "sample_1196": ["    def test_Contains_eval_non_Set():\n        x = Symbol('x')\n        raises(TypeError, lambda: Contains(x, 1))\n\n", "    def test_contains_eval_non_set():\n        x = Symbol('x')\n        raises(TypeError, lambda: Contains(x, 1))\n", "    def test_Contains_eval_Set():\n        \"\"\"Test that Contains.eval() works with different Set types.\"\"\"\n        x = Symbol('x')\n        assert Contains(x, FiniteSet(1, 2, 3)).eval() == Contains(x, FiniteSet(1, 2, 3))\n        assert Contains(2, FiniteSet(1, 2, 3)).eval() == S.true\n        assert Contains(4, FiniteSet(1, 2, 3)).eval() == S.false\n        assert Contains(x, Interval(0, 1)).eval() == Contains(x, Interval(0, 1))\n        assert Contains(0.5, Interval(0, 1)).eval() == S.true\n        assert Contains(2, Interval(0, 1)).eval() == S.false\n", "    def test_Contains_eval():\n        assert Contains(S(2), S.Integers) is S.true\n        assert Contains(S(-2), S.Naturals) is S.false\n        i = Symbol('i', integer=True)\n        assert Contains(i, S.Naturals) == Contains(i, S.Naturals)\n        assert Contains(oo, S.Reals) is S.true\n        assert Contains(oo, S.Naturals) is S.false\n        assert Contains(S.Pi, S.Reals) is S.true\n        assert Contains(S.Pi, FiniteSet(1, 2, 3)) is S.false\n        assert Contains(S.Pi, Interval(0, 1)) is S.false\n", "    def test_Contains_eval_non_Set():\n        raises(TypeError, lambda: Contains(S.One, 1))\n"], "sample_590": ["    def test_concat_empty_datasets():\n        ds1 = Dataset({'a': [], 'b': []}, coords={'c': []})\n        ds2 = Dataset({'a': [], 'b': []}, coords={'c': []})\n        assert_identical(concat([ds1, ds2], dim='d'), Dataset({'a': [], 'b': []}, coords={'c': [], 'd': [0, 1]}))\n\n", "    def test_concat_identical_coords_with_different_names():\n        ds1 = Dataset({\"foo\": ((\"x\"), [1, 2, 3]), \"bar\": 0})\n        ds1.coords[\"x\"] = (\"x\", np.arange(3))\n        ds2 = Dataset({\"foo\": ((\"x\"), [4, 5, 6]), \"baz\": 1})\n        ds2.coords[\"x\"] = (\"x\", np.arange(3, 6))\n        with pytest.raises(ValueError, match=\"identical\"):\n            concat([ds1, ds2], dim=\"x\", compat=\"identical\")\n\n", "    def test_concat_identical_coords_diff_dims():\n        # Test concatenating datasets with identical coordinates but different dimensions\n        ds1 = Dataset({'a': (['x'], [1, 2, 3]), 'b': (['y'], [4, 5])},\n                      coords={'x': [0, 1, 2], 'y': [3, 4]})\n        ds2 = Dataset({'a': (['x'], [4, 5, 6]), 'b': (['y'], [7, 8])},\n                      coords={'x': [0, 1, 2], 'y': [3, 4]})\n\n        expected = Dataset({'a': (['x', 'concat_dim'], [[1, 2, 3], [4, 5, 6]]),\n                            'b': (['y', 'concat_dim'], [[4, 5], [7, 8]])},\n                           coords={'x': [0, 1, 2], 'y': [3, 4], 'concat_dim': [0, 1]})\n        actual = concat([ds1, ds2], dim='concat_dim')\n        assert_identical(actual, expected)\n\n", "    def test_concat_override_compat(self):\n        a = DataArray(np.arange(4), dims=[\"x\"], coords={\"x\": [0, 1, 2, 3]})\n        b = DataArray(np.arange(4, 8), dims=[\"x\"], coords={\"x\": [0, 1, 2, 3]})\n        with pytest.warns(UserWarning):\n            c = concat([a, b], dim=\"x\", compat=\"override\")\n        assert_identical(c, a)\n", "    def test_concat_different_coords_override():\n        data = create_test_data()\n        ds1 = Dataset({'foo': ('x', [1, 2])})\n        ds2 = Dataset({'foo': ('x', [3, 4])}, coords={'x': [5, 6]})\n        expected = Dataset({'foo': ('x', [1, 2, 3, 4]), 'x': [5, 6]})\n\n        actual = concat([ds1, ds2], dim='x', compat='override')\n        assert_identical(actual, expected)\n\n"], "sample_843": ["def test_kernel_operator_exceptions():\n    # Test exception handling for KernelOperator\n    kernel = RBF()\n    op = KernelOperator(kernel)\n    with pytest.raises(ValueError):\n        op.transform(X)  # Requires a kernel object\n    with pytest.raises(ValueError):\n        KernelOperator(ConstantKernel(1.0) + kernel)\n\n", "def test_kernel_gradient_eval(kernel):\n    if isinstance(kernel.theta, (float, int)):\n        x = np.array([[1, 2], [3, 4]])\n        k, grad = kernel(x, eval_gradient=True)\n        assert grad.shape == (x.shape[0], x.shape[0], 1)\n    elif isinstance(kernel.theta, np.ndarray):\n        x = np.array([[1, 2], [3, 4]])\n        k, grad = kernel(x, eval_gradient=True)\n        assert grad.shape == (x.shape[0], x.shape[0], kernel.theta.shape[0])\n    else:\n        pytest.skip(\"Test not applicable for kernel: {}\".format(kernel))\n", "    def test_gradient_kernel(self, kernel):\n        if isinstance(kernel, (PairwiseKernel, DotProduct)):\n            return\n        if kernel.is_stationary():\n            sig = signature(kernel.gradient)\n            if 'eval_gradient' not in sig.parameters:\n                return\n            \n            grad = kernel(X, Y, eval_gradient=True)\n\n            # approximate gradient numerically\n                kernel.theta = theta\n                return kernel(X, Y)\n\n            grad_num = _approx_fprime(kernel.theta, f, 1e-10)\n            assert_array_almost_equal(grad[1], grad_num, decimal=4)\n", "def test_kernel_clone(kernel):\n    kernel_clone = clone(kernel)\n    assert_array_almost_equal(kernel(X), kernel_clone(X))\n    assert_array_almost_equal(kernel(X, Y), kernel_clone(X, Y))\n\n", "def test_kernel_operator():\n    kernel = RBF()\n    operator = KernelOperator(kernel, X)\n    K = kernel(X)\n    K_op = operator(X)\n    assert_array_almost_equal(K, K_op) \n    assert_raises(ValueError, KernelOperator, DotProduct(), X)\n"], "sample_199": ["    def test_row_range_window_function(self):\n        result = list(Book.objects.annotate(\n            prev_price=Window(\n                F('price'),\n                partition_by=F('contact'),\n                order_by=F('pubdate').asc(),\n                frame=RowRange(start=-1),\n            )\n        ).values_list('name', 'price', 'prev_price'))\n        self.assertListEqual(\n            result,\n            [\n                ('The Definitive Guide to Django: Web Development Done Right', Decimal('30.00'), None),\n                ('Sams Teach Yourself Django in 24 Hours', Decimal('23.09'), Decimal('30.00')),\n                ('Practical Django Projects', Decimal('29.69'), Decimal('23.09')),\n                ('Python Web Development with Django', Decimal('29.69'), Decimal('29.69')),\n                ('Artificial Intelligence: A Modern Approach', Decimal('82.80'), None),\n                ('Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', Decimal('75.00'), Decimal('82.80')),\n            ],\n        )\n\n", "    def test_window_function_partition_by(self):\n        # Test PARTITION BY with RANGE UNBOUNDED PRECEDING\n        query = (\n            Book.objects.annotate(\n                total_pages_by_contact=Window(\n                    Sum('pages'),\n                    partition_by=F('contact'),\n                    order_by=F('pubdate').asc(),\n                    frame=RowRange(start=Value(None), end=Value(None)).asc()\n                )\n            ).order_by('contact')\n        )\n        self.assertQuerysetEqual(\n            query,\n            [\n                ('Adrian Holovaty', 447),\n                ('Adrian Holovaty', 447),\n                ('Brad Dayley', 528),\n                ('Jacob Kaplan-Moss', 447),\n                ('Jacob Kaplan-Moss', 447),\n                ('Jeffrey Forcier', 350),\n                ('Jeffrey Forcier', 350),\n                ('Jeffrey Forcier', 350),\n                ('Peter Norvig', 1132),\n                ('Peter Norvig', 1132),\n                ('Stuart Russell', 300),\n            ],\n            lambda x: (x.contact__name, x.total_pages_by_contact),\n        )\n\n", "    def test_complex_annotation_with_transform(self):\n        qs = Book.objects.annotate(\n            authors_len=Count('authors'),\n            avg_rating=Avg('rating'),\n            pubdate_year=ExtractYear('pubdate'),\n        ).filter(authors_len__gt=1).annotate(\n            pubdates_diff=Max('pubdate') - Min('pubdate')\n        )\n        self.assertQuerysetEqual(\n            qs.values_list('id', 'authors_len', 'avg_rating', 'pubdate_year', 'pubdates_diff'),\n            [\n                (1, 2, Decimal('4.25'), 2007, timedelta(days=0)),\n                (4, 3, Decimal('4.00'), 2008, timedelta(days=0)),\n            ],\n        )\n\n", "    def test_annotate_with_case_when_then_null_else(self):\n        # Test annotating with CASE WHEN ... THEN ... ELSE ... END,\n        # including NULL values in the ELSE clause.\n        qs = Author.objects.annotate(\n            most_recent_book_pubdate=Case(\n                When(F('book__pubdate') > datetime.date(2007, 12, 31), then=F('book__pubdate')),\n                default=Value(None, output_field=DateField()),\n            )\n        )\n\n        for author in qs:\n            if author.name in ('Adrian Holovaty', 'Jacob Kaplan-Moss', 'Brad Dayley'):\n                self.assertEqual(author.most_recent_book_pubdate, datetime.date(2008, 6, 23))\n            elif author.name in ('James Bennett', 'Jeffrey Forcier', 'Paul Bissex'):\n                self.assertIsNone(author.most_recent_book_pubdate)\n", "    def test_annotate_count_related_distinct(self):\n        queryset = Author.objects.annotate(num_books_distinct=Count('book__pk', distinct=True))\n        self.assertQuerysetEqual(\n            queryset.order_by('num_books_distinct'),\n            [\n                (\n                    'Adrian Holovaty',\n                    2,\n                ),\n                (\n                    'Brad Dayley',\n                    1,\n                ),\n                (\n                    'James Bennett',\n                    1,\n                ),\n                (\n                    'Jacob Kaplan-Moss',\n                    1,\n                ),\n                (\n                    'Jeffrey Forcier',\n                    1,\n                ),\n                (\n                    'Paul Bissex',\n                    1,\n                ),\n                (\n                    'Peter Norvig',\n                    1,\n                ),\n                (\n                    'Stuart Russell',\n                    1,\n                ),\n                (\n                    'Wesley J. Chun',\n                    1,\n                ),\n            ],\n            lambda a, b: (a[0], b[1])\n        )\n"], "sample_204": ["    def test_replacements_with_missing_dependencies(self):\n        # Test a scenario where a replacing migration has missing dependencies,\n        # but the replacements are not applied. This should raise an error as Django\n        # cannot determine the correct replacement to use.\n\n        # Create a test database.\n        with connections['other'].cursor() as cursor:\n            cursor.execute('CREATE TABLE migrations_tests_model (id INTEGER PRIMARY KEY)')\n\n        # Create a migration that introduces a field and replaces another migration.\n        # This migration has a dependency on a non-existent migration.\n        with connections['other'].schema_editor() as schema_editor:\n            schema_editor.execute(\n                'ALTER TABLE migrations_tests_model ADD COLUMN field_a TEXT'\n            )\n        MigrationRecorder(connections['other']).record_applied('migrations', '0001_initial')\n\n        with connections['other'].cursor() as cursor:\n            cursor.execute('DROP TABLE migrations_tests_model')\n\n        # Attempt to load the migrations and validate the consistency.\n        loader = MigrationLoader(connections['other'], ignore_no_migrations=True)\n        with self.assertRaises(NodeNotFoundError):\n            loader.build_graph()\n", "    def test_load_disk_missing_file(self):\n        # Simulate a missing migrations file (e.g., 0001_initial.py)\n        os.remove(os.path.join(os.path.dirname(__file__), 'myapp', 'migrations', '0001_initial.py'))\n        compileall.compiledir(os.path.dirname(__file__))\n        loader = MigrationLoader(connection=connections['default'])\n\n        # Don't check for AmbiguityError here as the test_migrations app is the only\n        # one loaded, so an AmbiguityError wouldn't be raised.\n        with self.assertRaises(BadMigrationError):\n            loader.load_disk()\n", "    def test_replace_migration_partially_applied(self):\n        # Create a scenario where a replacing migration is partially applied.\n        with override_settings(MIGRATION_MODULES={'testapp': 'tests.migrations_tests.testapp'}), \\\n             modify_settings(INSTALLED_APPS=['testapp']):\n            # Apply the initial migration.\n            migration_loader = MigrationLoader(connections['default'])\n            migration_loader.build_graph()\n            plan = migration_loader.graph.make_plan(\n                None,\n                (\n                    ('testapp', '0001_initial'),\n                )\n            )\n\n            with connections['default'].cursor() as cursor:\n            # Apply migration\n                for migration, backwards in plan:\n                    migration.apply(migration_loader.project_state(), connections['default'].schema_editor())\n                # Check the applied migrations in recorder.\n                recorder = MigrationRecorder(connections['default'])\n                applied_migrations = recorder.applied_migrations()\n                self.assertIn(('testapp', '0001_initial'), applied_migrations)\n            # Simulate a scenario where the replacing migration is partially applied.\n            recorder.record_migration(\n                'testapp', '0002_replace_initial', connections['default']\n            )\n            migration_loader = MigrationLoader(connections['default'], ignore_no_migrations=True)\n            migration_loader.build_graph()\n            plan = migration_loader.graph.make_plan(\n                None,\n                (\n                    ('testapp', '0002_replace_initial'),\n                )\n            )\n\n            with connections['default'].cursor() as cursor:\n                with self.assertRaises(InconsistentMigrationHistory):\n                    for migration, backwards in plan:\n                        migration.apply(migration_loader.project_state(), connections['default'].schema_editor())\n", "    def test_load_disk_app_label_with_underscore(self):\n        \"\"\"\n        Loading migrations from app labeled with underscore should not raise an error\n        \"\"\"\n        with self.assertRaises(AmbiguityError):\n            loader = MigrationLoader(connection=connections['default'], ignore_no_migrations=True)\n            loader.build_graph()\n\n        # Clean up migration tables and files\n        MigrationRecorder(connections['default'].connection).reset(MigrationRecorder.Migration)\n        self.delete_migration_files(\n            migrations_dir=os.path.join(\n                os.path.dirname(__file__), 'migrations',\n            ),\n\n        )\n\n", "    def test_recorder_consistent_history(self):\n        # This test checks that the MigrationRecorder correctly detects an\n        # InconsistentMigrationHistory when an applied migration has unapplied dependencies.\n        with self.connection.cursor() as cursor:\n            # Create a test migration with a dependency on another migration.\n            test_migration_name = 'test_migration'\n            dependency_migration_name = 'dependency_migration'\n            self.apply_test_migration(\n                cursor, test_migration_name, dependency_migration_name\n            )\n            # Mark the dependency migration as unapplied.\n            recorder = MigrationRecorder(self.connection)\n            recorder.applied_migrations = {\n                (self.app_label, dependency_migration_name): None,\n            }\n            # Attempt to mark the test migration as applied.\n            with self.assertRaises(InconsistentMigrationHistory):\n                recorder.record_applied(self.app_label, test_migration_name)\n"], "sample_576": ["    def test_pair_spec_structure(self):\n        p = Plot()\n        p._pair_spec = {\"structure\": {\"x\": [\"a\", \"b\"], \"y\": [\"c\", \"d\"]}}\n        p._setup_scales(Default(), None, [], [])\n        assert p._scales[\"a\"].orientation == \"x\"\n        assert p._scales[\"b\"].orientation == \"x\"\n        assert p._scales[\"c\"].orientation == \"y\"\n        assert p._scales[\"d\"].orientation == \"y\"\n", "    def test_empty_data(self):\n\n        p = Plot()\n        p.add(MockMark(), \"x\", \"y\")\n        p.plot()\n        assert p._data.frame.empty\n", "    def test_facetgrid_stat_transform(self):\n        p = Plot(data=pd.DataFrame({\n            'x': list('AABBCCDDEE'),\n            'y': np.arange(10),\n            'g': list('ababaabaab'),\n        }))\n        p.add(\n            Mark(MockMark),\n            \"x\", \"y\",\n            col='g',\n            stat=Agg(\"mean\", 'y'),\n        )\n        \n        p.plot()\n        assert p._subplots[0][\"ax\"].get_title() == \"a\"\n", "    def test_subplot_interactions(self):\n\n        p = Plot()\n\n        x, y = np.array([1, 2, 3]), np.array([4, 5, 6])\n        df = pd.DataFrame({\"x\": x, \"y\": y})\n\n        sizes = np.array([10, 20, 30])\n        p.add(MockMark(), {\"x\": \"x\", \"y\": \"y\", \"size\": \"size\"}, data=df)\n        p.add(MockMark(), {\"x\": \"x\", \"y\": \"y\"}, data=df)\n\n        p._assign_subplots(rows=1, cols=2)\n        p._setup_scales(df)\n\n        # TODO test interaction between scales, e.g., if one layer defines\n        # a color scale that gets reused by another\n\n        p._plot_layers(None)\n\n        assert len(p._layers[0]._marks) == 2\n\n        # TODO check that subplot generation / axis sharing works as expected\n\n\n\n", "    def test_plot_layer_with_move(self):\n\n        p = Plot(data=pd.DataFrame({\"x\": [1, 2, 3, 1, 2, 3],\n                                  \"y\": [1, 2, 3, 4, 5, 6],\n                                  \"c\": [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]}),\n                 x=\"x\", y=\"y\", kind=\"scatter\")\n\n        move = Shift(by=\"c\")\n        layer = {\"mark\": MockMark(), \"data\": p._data,\n                 \"move\": move, \"orient\": None}\n\n        plot = Plotter(p)\n        plot._setup_scales(p, layer)\n        plot._plot_layer(p, layer)\n\n        assert layer[\"mark\"].n_splits == 2\n        assert layer[\"mark\"].passed_orient is None\n\n        for i in range(2):\n            assert layer[\"mark\"].passed_keys[i][\"c\"] in (\"a\", \"b\")\n\n            assert_array_equal(\n                layer[\"mark\"].passed_data[i][\"x\"],\n                np.array([1, 2, 3]) + (i * .5 if i == 1 else 0)\n            )\n\n"], "sample_807": ["    def test_calibration_curve_empty_bins_quantile():\n        y_true = np.array([0, 0, 1, 1])\n        y_prob = np.array([0.1, 0.2, 0.7, 0.8])\n\n        prob_true, prob_pred = calibration_curve(y_true, y_prob,\n                                                  n_bins=3,\n                                                  strategy='quantile')\n\n        assert_equal(len(prob_true), 2)\n        assert_equal(len(prob_pred), 2)\n\n", "    def test_calibration_curve_multiclass():\n        X, y = make_blobs(n_samples=200, centers=3, random_state=0)\n        clf = MultinomialNB(alpha=1)\n        clf.fit(X, y)\n\n        prob = clf.predict_proba(X)\n        true, pred = calibration_curve(y, prob[:, 1], n_bins=10)\n\n        assert_greater(len(true), 0)\n        assert_greater(len(pred), 0)\n        assert_equal(len(true), len(pred))\n        assert_array_equal(np.shape(true), (len(true),))\n        assert_array_equal(np.shape(pred), (len(pred),))\n", "    def test_calibration_curve_sparse_input():\n        # Check if calibration_curve works with sparse input\n        X, y = make_classification(random_state=42)\n        X = sparse.csr_matrix(X)\n        y_prob = LinearSVC().fit(X, y).decision_function(X)\n        prob_true, prob_pred = calibration_curve(y, y_prob,\n                                                  normalize=True)\n        assert_equal(len(prob_true), len(prob_pred))\n        assert_greater_equal(prob_true.min(), 0)\n        assert_greater_equal(prob_true.max(), 0)\n        assert_greater_equal(prob_pred.min(), 0)\n        assert_greater_equal(prob_pred.max(), 0)\n\n", "    def test_log_loss_and_brier_score_loss(self):\n        X, y = make_classification(n_samples=300, n_features=20, random_state=0)\n        # Test with a model that can make good probability predictions\n        clf = CalibratedClassifierCV(LinearSVC(), cv='prefit', method='sigmoid')\n        clf.fit(X, y)\n        y_prob = clf.predict_proba(X)[:, 1]\n\n        assert_greater(log_loss(y, y_prob), 0)\n        assert_greater_equal(brier_score_loss(y, y_prob), 0)\n\n        # Test with a model that is poorly calibrated\n        clf = CalibratedClassifierCV(RandomForestClassifier(), cv='prefit', method='sigmoid')\n        clf.fit(X, y)\n        y_prob = clf.predict_proba(X)[:, 1]\n\n        assert_greater(log_loss(y, y_prob), 0)\n        assert_greater_equal(brier_score_loss(y, y_prob), 0)\n", "    def test_calibration_curve():\n        \"\"\"Test calibration curve computation.\"\"\"\n        # Generate some data\n        n_samples = 1000\n        X, y = make_classification(n_samples=n_samples, n_features=10,\n                                   random_state=0)\n\n        # Fit a simple classifier\n        clf = LinearSVC(random_state=0)\n        clf.fit(X, y)\n\n        # Compute the predicted probabilities\n        y_prob = clf.decision_function(X)\n\n        # Compute the calibration curve\n        prob_true, prob_pred = calibration_curve(y, y_prob, n_bins=10)\n\n        # Check if the probabilities are within reasonable bounds\n        assert_greater_equal(prob_pred.min(), 0)\n        assert_less_equal(prob_pred.max(), 1)\n        assert_greater_equal(prob_true.min(), 0)\n        assert_less_equal(prob_true.max(), 1)\n\n        # Check that the length of the output arrays is not larger than the\n        # number of bins\n        assert_less_equal(len(prob_true), 10)\n        assert_less_equal(len(prob_pred), 10)\n"], "sample_67": ["    def test_form_with_media(self):\n        form = ModelFormWithMedia()\n        self.assertEqual(\n            form.media._js,\n            ['/some/form/javascript'],\n        )\n        self.assertEqual(\n            form.media._css['all'],\n            ['/some/form/css'],\n        )\n", "    def test_modelform_initial_data(self):\n        data = {'title': 'New title', 'content': 'New content', 'status': 'draft',\n                'author': 1}\n        form = ArticleForm(data=data)\n        self.assertTrue(form.is_valid())\n        article = form.save()\n        self.assertEqual(article.title, 'New title')\n        self.assertEqual(article.content, 'New content')\n\n", "    def test_custom_error_message(self):\n        form = CustomErrorMessageForm({'name1': 'invalid'})\n        self.assertEqual(form.is_valid(), False)\n        self.assertEqual(form.errors['name1'], ['Form custom error message.'])\n", "    def test_explicit_pk_form(self):\n        form = ExplicitPKForm(data={'key': '1', 'desc': 'Some description'})\n        self.assertTrue(form.is_valid())\n        instance = form.save()\n        self.assertEqual(instance.key, '1')\n        self.assertEqual(instance.desc, 'Some description')\n", "    def test_modelform_with_custom_media(self):\n        form = ModelFormWithMedia()\n        self.assertEqual(form.Media.__class__.__name__, 'Media')\n        self.assertEqual(form.Media.js, ('/some/form/javascript',))\n        self.assertEqual(form.Media.css.get('all'), ('/some/form/css',))\n"], "sample_1080": ["def test_refine_re_im_complex_argument():\n    # Test refining re and im with complex arguments\n    z = x + y*I\n    assert refine(re(z), Q.real(x) & Q.real(y)) == x\n    assert refine(im(z), Q.real(x) & Q.real(y)) == y\n", "def test_refine_Pow_issue_19662():\n    x = Symbol('x', real=True)\n    assert refine(Pow(-1, x/2), Q.even(x)) == Pow(-1, x/2)\n\n", "def test_refine_Pow_complex():\n    assert refine(Pow(S.ImaginaryUnit, 2), True) == S.NegativeOne\n    assert refine(Pow(S.ImaginaryUnit, 3), True) == -S.ImaginaryUnit\n    assert refine(Pow(S.ImaginaryUnit, 4), True) == S.One\n", "def test_refine_Pow_rational_power():\n    assert refine(x**(Rational(1, 2)), Q.positive(x)) == sqrt(x)\n    assert refine(x**(Rational(2, 3)), Q.positive(x)) == x**(Rational(2, 3))\n    assert refine(x**(Rational(3, 2)), Q.positive(x)) == x**(Rational(3, 2))\n    assert refine(x**(Rational(-1, 2)), Q.positive(x)) == 1/sqrt(x)\n    assert refine(x**(Rational(-2, 3)), Q.positive(x)) == 1/x**(Rational(2, 3))\n", "    def test_refine_atan2_with_complex():\n        x = Symbol('x', real=True)\n        y = Symbol('y', real=True)\n        expr = atan2(y, x)\n        assert refine(expr, Q.positive(x) & Q.positive(y)) == atan(y/x)\n        assert refine(expr, Q.positive(x) & Q.negative(y)) == atan(y/x) + pi\n        assert refine(expr, Q.negative(x) & Q.real(y)) == atan(y/x) + pi\n        assert refine(expr, Q.negative(x) & Q.positive(y)) == atan(y/x) + pi\n        assert refine(expr, Q.zero(x) & Q.positive(y)) == pi/2\n        assert refine(expr, Q.zero(x) & Q.negative(y)) == -pi/2\n"], "sample_936": ["def test_stringify_generic_alias():\n    test_cases = [\n        (List[int], 'list[int]'),\n        (List[str], 'list[str]'),\n        (List[MyClass1], 'list[sphinx.tests.test_util_typing.MyClass1]'),\n        (MyList[int], 'MyList[int]'),\n        (MyList[T], 'MyList[T]'),\n        (Dict[str, int], 'dict[str, int]'),\n        (Dict[str, MyClass1], 'dict[str, sphinx.tests.test_util_typing.MyClass1]'),\n        (Generator[int, None, None], 'generator[int, None, None]'),\n        (Tuple[int, str], 'tuple[int, str]'),\n        (Optional[int], 'Optional[int]'),\n        (Union[int, str], 'Union[int, str]'),\n        (Union[int, NoneType], 'Optional[int]'),\n        (Union[int, MyClass1],\n         'Union[int, sphinx.tests.test_util_typing.MyClass1]'),\n    ]\n\n    for input_type, expected_str in test_cases:\n        assert stringify(input_type) == expected_str\n\n", "    def test_stringify_py36_generic_meta_optional(self):\n        assert stringify(Optional[int]) == 'Optional[int]'\n        assert stringify(Optional[MyClass1]) == 'Optional[MyClass1]'\n        assert stringify(List[Optional[int]]) == 'List[Optional[int]]'\n        assert stringify(Optional['MyClass1']) == 'Optional[MyClass1]'\n\n", "    def test_stringify_typing_generic_alias_with_forwardref():\n        T = TypeVar('T')\n        U = TypeVar('U')\n        ForwardRefT = ForwardRef('T')\n\n        assert stringify(MyList[ForwardRefT]) == 'MyList[T]'\n", "    def test_stringify_forwardref():\n        forwardref = ForwardRef('MyClass1')\n        assert stringify(forwardref) == 'MyClass1'\n\n", "    def test_stringify_broken_type():\n        assert stringify(BrokenType()) == 'BrokenType'\n"], "sample_1057": ["    def test_render_as_module_fully_qualified_modules():\n        expr = Print(1)\n        code = render_as_module(expr, standard='python3', fully_qualified_modules=True)\n        assert 'import sympy' in code\n        assert 'from sympy.codegen.ast import Print' in code\n        assert 'print(1)' in code\n\n", "    def test_render_as_module_fully_qualified(self):\n        code = Print(1 + 2)\n        expected = \"import sympy\\n\\nprint(sympy.sympify(1) + sympy.sympify(2))\"\n        assert render_as_module(code, standard='python3', fully_qualified_modules=True) == expected\n", "    def test_render_as_module_fully_qualified():\n        expr = Print(1 + 2)\n        expected = ('import sympy\\n'\n                    '\\n'\n                    'print(sympy.Integer(3))')\n        assert render_as_module(expr, standard='python3', fully_qualified_modules=True) == expected \n", "    def test_render_as_module_basic():\n        code = Print(1)\n        expected = \"import sympy\\n\\nprint(1)\"\n        assert render_as_module(code) == expected\n", "    def test_render_as_module_with_imports():\n        code = Print(\"x + 1\")\n        expected_output = (\n            \"from sympy.codegen.ast import Print\\n\"\n            \"\\n\"\n            \"Print(x + 1)\"\n        )\n        assert render_as_module(code) == expected_output\n"], "sample_732": ["    def test_subset_sa(self):\n        data = fetch_kddcup99(subset='SA', percent10=True, return_X_y=True)\n        X, y = data\n        assert_equal(y.shape[0], 97615)\n", "    def test_subset_SF(self):\n        kddcup99 = fetch_kddcup99(subset='SF', download_if_missing=False)\n        if not exists(os.path.join(self.data_home, 'kddcup99_10')):\n            raise SkipTest(\"Skipping test as 'kddcup99' data not found.\")\n        assert_equal(kddcup99.data.shape[1], 4)\n        assert_equal(kddcup99.target.size, 699691)\n        check_return_X_y(fetch_kddcup99, subset='SF', download_if_missing=False)\n", "    def test_subset_return_X_y(self):\n        X, y = fetch_kddcup99(subset='SA', data_home='.', return_X_y=True)\n        assert_equal(X.shape, (976158, 41))\n        assert_equal(y.shape, (976158,))\n\n", "    def test_subset_selection(self):\n        # Test if the correct subset is returned\n        fetch_kddcup99(subset='SA', data_home='kddcup99_test')\n        fetch_kddcup99(subset='SF', data_home='kddcup99_test')\n        fetch_kddcup99(subset='http', data_home='kddcup99_test')\n        fetch_kddcup99(subset='smtp', data_home='kddcup99_test')\n", "    def test_fetch_kddcup99_subset(self):\n\n        try:\n            fetch_kddcup99(subset='SF', percent10=True)\n            fetch_kddcup99(subset='http', percent10=True)\n            fetch_kddcup99(subset='smtp', percent10=True)\n        except IOError:\n\n            raise SkipTest(\"KDDCUP99 data not found. You can download it using \"\n                           \"`fetch_kddcup99(download_if_missing=True)`\")\n\n        data_SF = fetch_kddcup99(subset='SF', percent10=True)\n        data_http = fetch_kddcup99(subset='http', percent10=True)\n        data_smtp = fetch_kddcup99(subset='smtp', percent10=True)\n\n        assert_equal(data_SF.data.shape, (69969, 4))\n        assert_equal(data_http.data.shape, (61905, 3))\n        assert_equal(data_smtp.data.shape, (9537, 3))\n\n"], "sample_63": ["    def test_render_to_string_with_context(self):\n        template = self.engine.from_string(\n            'Hello {{ name }}!'\n        )\n        context = Context({'name': 'World'})\n        rendered = template.render(context)\n        self.assertEqual(rendered, 'Hello World!')\n", "    def test_render_to_string_empty_context(self):\n        self.assertEqual(self.engine.render_to_string('test.html'), 'Hello, world!')\n", "    def test_render_to_string_with_context(self):\n        context = Context({'name': 'World'})\n        template = self.engine.from_string('Hello {{ name }}!')\n        rendered = template.render(context)\n        self.assertEqual(rendered, 'Hello World!')\n", "    def test_select_template_multiple(self):\n        t = self.engine.select_template(['template1.html', 'template2.html'])\n        self.assertEqual(t.template_name, 'template1.html')\n\n        with self.assertRaises(TemplateDoesNotExist):\n            self.engine.select_template(['nonexistent.html', 'also_nonexistent.html'])\n", "    def test_render_to_string_with_context(self):\n        template = self.engine.get_template('template.html')\n        context = {'name': 'John Doe'}\n        rendered = template.render(Context(context))\n        self.assertEqual(rendered.strip(), 'Hello, John Doe!')\n"], "sample_918": ["    def test_parse_annotation(self):\n        self.assertEqual(_parse_annotation(\"int\"), \"int\")\n        self.assertEqual(_parse_annotation(\"List[str]\"), \"List[str]\")\n        self.assertEqual(_parse_annotation(\"Dict[str, int]\"), \"Dict[str, int]\")\n        self.assertEqual(_parse_annotation(\"Optional[int]\"), \"Optional[int]\")\n        self.assertEqual(_parse_annotation(\"Union[int, str]\"), \"Union[int, str]\")\n\n", "def test_parse_annotation_basic():\n    assert _parse_annotation('foo') == ('foo', None)\n    assert _parse_annotation('foo: bar') == ('foo', 'bar')\n    assert _parse_annotation(': bar') == (None, 'bar')\n    assert _parse_annotation('') == (None, None)\n\n", "def test_parse_annotation_nested():\n    res = _parse_annotation('Optional[List[str]]')\n    assert res == ('Optional', 'List[str]')\n\n    res = _parse_annotation('List[Tuple[int, str]]')\n    assert res == ('List', 'Tuple[int, str]')\n\n    res = _parse_annotation('Dict[str, Tuple[float, bool]]')\n    assert res == ('Dict', 'str, Tuple[float, bool]')\n", "def test_parse_annotation_basic():\n    assert _parse_annotation(\"str\") == (\"str\", None)\n    assert _parse_annotation(\"int\") == (\"int\", None)\n\n\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == 'int'\n    assert _parse_annotation('list[str]') == 'list[str]'\n    assert _parse_annotation('dict[str, int]') == 'dict[str, int]'\n    assert _parse_annotation('Tuple[int, ...]') == 'Tuple[int, ...]'\n    assert _parse_annotation('Callable[[str], int]') == 'Callable[[str], int]'\n    assert _parse_annotation('Union[int, str]') == 'Union[int, str]'\n    assert _parse_annotation('Optional[str]') == 'Optional[str]'\n    assert _parse_annotation('Any') == 'Any'\n"], "sample_493": ["    def test_window_function(self):\n        with CaptureQueriesContext(connection) as queries:\n            queryset = (\n                Book.objects.annotate(\n                    rank=Window(\n                        expression=Rank(),\n                        partition_by=[F(\"contact__name\")],\n                        order_by=[F(\"rating\").desc()],\n                    )\n                )\n                .filter(rank=1)\n                .values(\"contact__name\", \"name\", \"rating\")\n            )\n            self.assertQuerySetEqual(\n                queryset,\n                [\n                    {\"contact__name\": \"Adrian Holovaty\", \"name\": \"The Definitive Guide to Django: Web Development Done Right\", \"rating\": Decimal(\"4.5\")},\n                    {\"contact__name\": \"Brad Dayley\", \"name\": \"Sams Teach Yourself Django in 24 Hours\", \"rating\": Decimal(\"3.0\")},\n                    {\"contact__name\": \"James Bennett\", \"name\": \"Practical Django Projects\", \"rating\": Decimal(\"4.0\")},\n                    {\"contact__name\": \"Jeffrey Forcier\", \"name\": \"Python Web Development with Django\", \"rating\": Decimal(\"4.0\")},\n                    {\"contact__name\": \"Peter Norvig\", \"name\": \"Artificial Intelligence: A Modern Approach\", \"rating\": Decimal(\"4.0\")},\n                ],\n            )\n            self.assertEqual(len(queries.captured_queries), 1)\n\n\n\n\n", "    def test_annotate_aggregate_subquery(self):\n        # Test annotating with an aggregate over a subquery.\n        subquery = Book.objects.filter(contact__name=\"Adrian Holovaty\").values(\"publisher__name\")\n        annotated = Author.objects.annotate(\n            publisher_count=Count(\"book__publisher__name\", input_field=\"book__publisher__name\", distinct=True)\n        ).filter(publisher_count__gt=1)\n        self.assertEqual(\n            annotated.values(\"name\", \"publisher_count\").order_by(\"name\"),\n            [{\"name\": \"Adrian Holovaty\", \"publisher_count\": 2}]\n        )\n\n", "    def test_annotate_subquery_join(self):\n        subquery = Book.objects.filter(contact__age__gt=40).values(\"publisher\")\n        annotated_books = (\n            Book.objects.annotate(published_by_older_author=Subquery(subquery))\n        )\n        self.assertEqual(\n            annotated_books.filter(published_by_older_author__isnull=False).count(), 4\n        )\n        self.assertEqual(\n            annotated_books.filter(published_by_older_author__isnull=True).count(), 2\n        )\n\n", "    def test_aggregate_trunc_date(self):\n        with Approximate(Decimal('2.5'), places=1) as approx:\n            self.assertQuerysetEqual(\n                Book.objects.values(\"pubdate\").annotate(\n                    avg_age=Avg(F(\"contact__age\")),\n                    truncated_date=TruncDate(F(\"pubdate\"))\n                ).order_by(\"truncated_date\"),\n                [\n                    {\n                        \"pubdate\": datetime.date(2007, 12, 6),\n                        \"avg_age\": approx,\n                        \"truncated_date\": datetime.date(2007, 12, 6),\n                    },\n                    {\n                        \"pubdate\": datetime.date(2008, 3, 3),\n                        \"avg_age\": approx,\n                        \"truncated_date\": datetime.date(2008, 3, 3),\n                    },\n                    {\n                        \"pubdate\": datetime.date(2008, 6, 23),\n                        \"avg_age\": approx,\n                        \"truncated_date\": datetime.date(2008, 6, 23),\n                    },\n                    {\n                        \"pubdate\": datetime.date(2008, 11, 3),\n                        \"avg_age\": approx,\n                        \"truncated_date\": datetime.date(2008, 11, 3),\n                    },\n                ],\n                transform=lambda x: x,\n            )\n", "    def test_aggregate_with_subqueries(self):\n        # Test using aggregates with subqueries.\n\n        # Find the average rating of books for publishers whose name contains\n        # \"Sams\"\n        avg_rating = (\n            Book.objects.filter(publisher__name__icontains=\"Sams\")\n            .values(\"publisher\")\n            .annotate(avg_rating=Avg(\"rating\"))\n            .values(\"avg_rating\")\n            .first()[\"avg_rating\"]\n        )\n\n        self.assertEqual(avg_rating, 3.0)\n\n"], "sample_194": ["    def test_unique_constraint_condition(self):\n        with atomic():\n            UniqueConstraintConditionProduct.objects.create(name='Product A', price=10)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='Product A', price=20,\n                    is_active=True,\n                )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='Product B', price=10,\n                    is_active=True,\n                )\n            UniqueConstraintConditionProduct.objects.create(\n                name='Product B', price=20,\n                is_active=True,\n            )\n", "    def test_unique_constraint_with_include(self):\n        with atomic():\n            UniqueConstraintInclude.objects.create(name='foo', value='bar', extra='baz')\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintInclude.objects.create(name='foo', value='bar', extra='qux')\n            UniqueConstraintInclude.objects.create(name='bar', value='bar', extra='baz')\n", "    def test_unique_constraint_opclasses(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(id=1, data='abc', data2='def')\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(id=2, data='abc', data2='ghi')\n\n            # Change opclasses to allow duplicates\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"ALTER TABLE %s ALTER COLUMN data2 TYPE TEXT USING data2::TEXT\" %\n                    UniqueConstraintProduct._meta.db_table\n                )\n                cursor.execute(\n                    \"ALTER TABLE %s ADD CONSTRAINT unique_constraint_product_data_data2 UNIQUE (data, data2) NOT VALID;\" %\n                    UniqueConstraintProduct._meta.db_table\n                )\n\n            UniqueConstraintProduct.objects.create(id=2, data='abc', data2='ghi')\n\n            constraints = get_constraints(UniqueConstraintProduct._meta.db_table)\n            self.assertIn('unique_constraint_product_data_data2', [c['name'] for c in constraints])\n", "    def test_unique_constraint_with_condition(self):\n        with atomic():\n            model = UniqueConstraintConditionProduct.objects.create(\n                name='Product 1', price=10, stock=10\n            )\n            model.save()\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintConditionProduct.objects.create(\n                    name='Product 2', price=10, stock=10\n                )\n            model.delete()\n            model2 = UniqueConstraintConditionProduct.objects.create(\n                name='Product 3', price=20, stock=10\n            )\n            model2.save()\n", "    def test_unique_constraint_create_sql(self):\n        with mock.patch('django.db.backends.utils.execute_sql') as mock_execute:\n            with atomic():\n                Product.objects.create(name='test', description='test')\n                sql = mock_execute.mock_calls[0][2][0]\n                expected = (\n                    \"ALTER TABLE 'product' ADD CONSTRAINT 'product_name_unique' UNIQUE ('name')\"\n                )\n                self.assertIn(expected, sql)\n"], "sample_1085": ["    def test_sympify_mpz(self):\n        assert sympify(gmpy.mpz(1)) == Integer(1)\n        assert sympify(gmpy.mpz(-2)) == Integer(-2)\n", "    def test_isqrt_issue_11417(self):\n        self.assertEqual(isqrt(100000000000000000000000000000000000000), 1000000000000000000000)\n", "def test_catalan():\n    assert Catalan.is_real is True\n    assert Catalan.is_positive is True\n    assert Catalan.is_negative is False\n    assert Catalan.is_irrational is None\n    assert Catalan.is_number is True\n    assert Catalan.is_algebraic is True\n    assert Catalan.is_transcendental is False\n    assert Catalan > 0\n    assert Catalan < 2\n\n", "    def test_sympify_Integer():\n        assert sympify(1) == S.One\n        assert sympify(-5) == S.NegativeFive\n        assert type(sympify(1)) is Integer\n        assert type(sympify(-5)) is Integer\n", "def test_int_cmp():\n    assert (1 > 0) is True\n    assert (0 > 1) is False\n    assert (1 < 0) is False\n    assert (0 < 1) is True\n\n    assert (1 >= 0) is True\n    assert (0 >= 1) is False\n    assert (1 <= 0) is False\n    assert (0 <= 1) is True\n    \n    assert (1 >= 1) is True\n    assert (0 >= 0) is True\n\n    assert (1 <= 1) is True\n    assert (0 <= 0) is True\n"], "sample_916": ["def test_nested_class_template():\n    check(\"class\", \"template <typename T> class Foo{ int a; };\", {\n        1: \"Foo\",\n        2: \"cpp:class@Foo\",\n        3: \"cpp:class@Foo\"\n    })\n", "def test_function_template():\n    check('function',\n          'template <typename T> void foo(T t);',\n          {1:'cpp:function:TestDoc:foo', 2:'cpp:function:TestDoc:foo_t'},\n          'void foo(T t);')\n", "def test_enum_with_values():\n    check(\"enum\", \"enum MyEnum { A, B, C }\",\n          {1: \"MyEnum\"},\n          \"enum MyEnum { A, B, C }\")\n\n    check(\"enum\", \"enum MyEnum : int { A = 1, B = 2, C = 3 }\",\n          {1: \"MyEnum\"},\n          \"enum MyEnum : int { A = 1, B = 2, C = 3 }\")\n", "    def test_enum_scoped():\n        # Test the handling of scoped enums and their members\n        check(\"enum\", \"enum MyEnum {\\\n                   MyEnumMember1,\\\n                   MyEnumMember2\\\n               };\", {1: \"MyEnum\",\n                     2: \"MyEnum:MyEnumMember1\",\n                     3: \"MyEnum:MyEnumMember2\"},\n              )\n", "def test_member_object():\n    check(\n        \"member\",\n        \"int foo;\",\n        idDict={1: \"module-foo\"},\n        output=\"int foo\",\n    )\n"], "sample_157": ["    def test_test_db_signature_returns_unique_signature(self):\n        connection = get_connection_copy()\n        connection.settings_dict['NAME'] = 'test_db'\n        creation = BaseDatabaseCreation(connection)\n        signature1 = creation.test_db_signature()\n\n        connection.settings_dict['NAME'] = 'another_test_db'\n        creation = BaseDatabaseCreation(connection)\n        signature2 = creation.test_db_signature()\n\n        self.assertNotEqual(signature1, signature2)\n\n", "    def test_test_db_signature_returns_identifier_tuple(self):\n        connection = get_connection_copy()\n        creation = BaseDatabaseCreation(connection)\n\n        # Modify some settings to ensure they are included in the signature\n        connection.settings_dict['HOST'] = 'test-host'\n        connection.settings_dict['PORT'] = 1234\n        connection.settings_dict['ENGINE'] = 'test-engine'\n        expected_signature = (\n            'test-host',\n            1234,\n            'test-engine',\n            TEST_DATABASE_PREFIX + connection.settings_dict['NAME'],\n        )\n\n        self.assertEqual(creation.test_db_signature(), expected_signature)\n\n", "    def test_test_db_signature(self):\n        # Create a mock connection settings dict\n        connection_settings = {\n            'HOST': 'localhost',\n            'PORT': '5432',\n            'ENGINE': 'django.db.backends.postgresql',\n            'NAME': 'test_db',\n        }\n        with mock.patch('django.db.utils.ConnectionHandler.settings_dict', connection_settings):\n            test_db_creation = BaseDatabaseCreation(get_connection_copy())\n            signature = test_db_creation.test_db_signature()\n            self.assertEqual(signature, ('localhost', '5432', 'django.db.backends.postgresql', 'test_test_db'))\n\n", "    def test_test_db_signature_includes_relevant_settings(self):\n        connection = get_connection_copy()\n        connection.settings_dict['NAME'] = 'test_db'\n        connection.settings_dict['HOST'] = 'localhost'\n        connection.settings_dict['PORT'] = 5432\n        connection.settings_dict['ENGINE'] = 'django.db.backends.postgresql'\n\n        creation = BaseDatabaseCreation(connection)\n        signature = creation.test_db_signature()\n\n        self.assertEqual(signature, ('localhost', 5432, 'django.db.backends.postgresql', 'test_db'))\n", "    def test_test_db_signature(self):\n        test_db_creation = BaseDatabaseCreation(get_connection_copy())\n        original_signature = test_db_creation.test_db_signature()\n\n        # Changing the database name shouldn't change the signature\n        test_db_creation.connection.settings_dict['NAME'] = 'new_test_db'\n        self.assertEqual(\n            original_signature,\n            test_db_creation.test_db_signature(),\n            \"Changing the database name shouldn't change the signature\",\n        )\n\n        # Changing a different setting should change the signature\n        test_db_creation.connection.settings_dict['HOST'] = 'new_host'\n        self.assertNotEqual(\n            original_signature,\n            test_db_creation.test_db_signature(),\n            \"Changing a different setting should change the signature\",\n        )\n"], "sample_514": ["def test_colorbar_orientation():\n    fig, ax = plt.subplots()\n    data = np.arange(100).reshape(10, 10)\n    im = ax.imshow(data, cmap='viridis')\n    \n    # Test vertical colorbar\n    cbar = fig.colorbar(im, ax=ax, orientation='vertical')\n    assert cbar.orientation == 'vertical'\n    \n    # Test horizontal colorbar\n    cbar = fig.colorbar(im, ax=ax, orientation='horizontal')\n    assert cbar.orientation == 'horizontal'\n\n    with pytest.raises(ValueError):\n        fig.colorbar(im, ax=ax, orientation='invalid')\n\n", "    def test_colorbar_extension_ticks(self):\n        # Create a figure and axes.\n        fig, ax = plt.subplots()\n        # Create a colormap and norm.\n        cmap = cm.get_cmap(\"RdBu\", lut=5)\n        norm = BoundaryNorm([-5, -2.5, -.5, .5, 1.5, 3.5], 5)\n        # Create a colorbar with extensions.\n        cb = Colorbar(ax, cmap=cmap, norm=norm, extend='both',\n                      orientation='vertical')\n        # Get the colorbar ticks.\n        ticks = cb.get_ticks()\n        # Assert that the ticks include the extension values.\n        assert np.allclose(ticks, [-5, -2.5, -.5, .5, 1.5, 3.5])\n        # Add a second colorbar to check for axis sharing.\n        cb2 = Colorbar(ax, cmap=cmap, norm=norm, extend='both',\n                       orientation='vertical')\n        # Get the ticks for the second colorbar.\n        ticks2 = cb2.get_ticks()\n        # Assert that the ticks for both colorbars are the same.\n        assert np.allclose(ticks, ticks2)\n", "    def test_colorbar_extensions_shape(self):\n        for spacing in ('uniform', 'proportional'):\n            fig = _colorbar_extension_shape(spacing)\n            with io.BytesIO() as buf:\n                fig.savefig(buf, format='png')\n                buf.seek(0)\n                im = Image.open(buf)\n                self.assertEqual(im.mode, 'RGB')\n                self.assertLess(\n                    np.abs(np.array(im.getdata()) - 255).max(), 10)\n\n", "def test_colorbar_extension_ticks():\n    \"\"\"\n    Test tick placement for colorbars with extensions.\n\n    This tests that the ticks are\n    correctly placed when there are extensions.\n    \"\"\"\n    cmap = cm.get_cmap(\"viridis\")\n    norm = Normalize(vmin=0, vmax=10)\n    fig, ax = plt.subplots()\n\n    # Create a colorbar with both min and max extensions\n    cb = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax,\n                      extend='both', orientation='vertical')\n\n    # Ensure ticks are located correctly\n    ticks = cb.ax.get_yticks()\n    assert len(ticks) == 12  # including extensions\n    assert ticks[0] == 0\n    assert ticks[-1] == 10\n    assert ticks[1] > 0\n    assert ticks[-2] < 10\n\n", "def test_colorbar_extension_tick_locations():\n    \"\"\"Test tick locations for colorbars with extensions.\"\"\"\n    cmap, norms = _get_cmap_norms()\n    fig, axs = plt.subplots(2, 2)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        norm = norms[extension_type]\n        boundaries = norm.boundaries\n\n        ax = axs.flat[i]\n        cb = Colorbar(ax, cmap=cmap, norm=norm,\n                      boundaries=boundaries,\n                      extend=extension_type,\n                      orientation='vertical')\n        # Make sure ticks are placed correctly on the colorbar\n        # This is a simple check, more comprehensive checks could be added\n        tick_locs = cb.get_ticks()\n\n        if extension_type == 'neither':\n            assert np.allclose(tick_locs, boundaries[:-1])\n        else:\n            assert np.allclose(tick_locs[:len(boundaries)-1], boundaries[:-1])\n"], "sample_994": ["    def test_Add_identity():\n        assert Add.identity == 0\n        assert 0 + Add.identity == 0\n\n\n", "def test_NumberSymbols():\n    assert S.Zero.is_zero\n    assert S.One.is_one\n    assert S.Pi.is_irrational\n    assert S.E.is_transcendental\n    assert S.I.is_imaginary\n\n", "def test_Integer_is_prime():\n    assert Integer(2).is_prime is True\n    assert Integer(3).is_prime is True\n    assert Integer(4).is_prime is False\n    assert Integer(5).is_prime is True\n    assert Integer(6).is_prime is False\n    assert Integer(7).is_prime is True\n    assert Integer(8).is_prime is False\n    assert Integer(9).is_prime is False\n    assert Integer(10).is_prime is False\n    assert Integer(11).is_prime is True\n", "def test_sympify_Float():\n    assert sympify(Float(1.23)) == Float(1.23)\n    assert sympify(Float(1.23e10)) == Float(1.23e10)\n    assert sympify(Float(1.23e-10)) == Float(1.23e-10)\n\n", "def test_issue_11950():\n    x = Symbol('x')\n    assert simplify(x*0) == 0\n"], "sample_390": ["    def test_serve_if_modified_since(self):\n        self.client.get(f\"/{self.prefix}/test.txt\")\n        response = self.client.get(\n            f\"/{self.prefix}/test.txt\", HTTP_IF_MODIFIED_SINCE=http_date()\n        )\n        self.assertEqual(response.status_code, 304)\n", "    def test_serve_if_modified_since(self):\n        # Test that If-Modified-Since header works correctly.\n        self.client.get(f\"/{self.prefix}/test.txt\")\n        response = self.client.get(\n            f\"/{self.prefix}/test.txt\",\n            HTTP_IF_MODIFIED_SINCE=http_date(self.file_mtime),\n        )\n        self.assertEqual(response.status_code, 304)\n\n        response = self.client.get(\n            f\"/{self.prefix}/test.txt\",\n            HTTP_IF_MODIFIED_SINCE=http_date(self.file_mtime - 1),\n        )\n        self.assertEqual(response.status_code, 200)\n\n", "    def test_serve_modified_since_header(self):\n        # Test that the If-Modified-Since header is handled correctly.\n        filepath = path.join(media_dir, \"test.txt\")\n        self.client.get(f\"/{self.prefix}/test.txt\")\n\n        response = self.client.get(f\"/{self.prefix}/test.txt\", HTTP_IF_MODIFIED_SINCE=http_date(\n            path.getmtime(filepath)\n        ))\n\n        self.assertEqual(response.status_code, 304)\n\n        # Test that the header is ignored if the file has been modified.\n        response = self.client.get(f\"/{self.prefix}/test.txt\", HTTP_IF_MODIFIED_SINCE=http_date(\n            path.getmtime(filepath) - 1\n        ))\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.__class__, FileResponse)\n", "    def test_serve_file_not_modified(self):\n        response = self.client.get(\n            f\"/{self.prefix}/css/style.css\",\n            HTTP_IF_MODIFIED_SINCE=http_date(self.css_file.stat().st_mtime),\n        )\n        self.assertEqual(response.status_code, 304)\n        self.assertEqual(response.__class__, HttpResponseNotModified)\n", "    def test_directory_index(self):\n        response = self.client.get(f\"/{self.prefix}/directory_index/\")\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, \"Index of directory_index/\")\n        self.assertTemplateUsed(response, 'static/directory_index.html')\n\n"], "sample_134": ["    def test_serialize_lazy_object(self):\n        class LazyString(SimpleLazyObject):\n                self._wrapped = value\n\n                return self._wrapped\n\n        lazy_str = LazyString('test')\n\n        serializer = migrations.serializer.serializer_factory(lazy_str)\n        serialized, imports = serializer.serialize()\n\n        self.assertEqual(serialized, repr('test'))\n        self.assertEqual(imports, set())\n", "    def test_serialize_deconstructible_with_module_name(self):\n        class MyDeconstructibleClass(deconstructible):\n                return ('my_module.MyDeconstructibleClass', [], {})\n\n        instance = MyDeconstructibleClass()\n        serialized = serializer_factory(instance)\n        self.assertEqual(serialized.serialize()[0], 'my_module.MyDeconstructibleClass()')\n", "    def test_serialize_lazy_string(self):\n        lazy_string = SimpleLazyObject(lambda: 'test string')\n        serialized, imports = serializer_factory(lazy_string).serialize()\n        self.assertEqual(serialized, \"'test string'\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_lazy_object(self):\n        lazy_value = SimpleLazyObject(lambda: 'foo')\n        serialized, imports = serializer_factory(lazy_value).serialize()\n        self.assertEqual(serialized, \"'foo'\")\n        self.assertEqual(imports, set())\n", "    def test_serialize_lazy_object(self):\n        lazy_obj = SimpleLazyObject(lambda: \"Hello World!\")\n        serializer = serializer_factory(lazy_obj)\n        self.assertEqual(serializer.serialize(), (\"'Hello World!'\", {'from django.utils.functional import SimpleLazyObject'}))\n"], "sample_104": ["    def test_manifest_strict_mode(self):\n        # Create a file that's not in the manifest.\n        file_path = os.path.join(TEST_ROOT, 'static', 'test_manifest_strict.css')\n        with open(file_path, 'w') as f:\n            f.write('body { background-color: red; }')\n\n        # Generate the manifest.\n        call_command('collectstatic', interactive=False, verbose=0)\n\n        # Reference the file in a template.\n        self.render_template(\n            f'''\n            <link rel=\"stylesheet\" href=\"{settings.STATIC_URL}test_manifest_strict.css\">\n            ''',\n        )\n\n        # Assert that the file is hashed correctly.\n        hashed_path = self.hashed_file_path('test_manifest_strict.css')\n        self.assertTrue(os.path.exists(os.path.join(settings.STATIC_ROOT, hashed_path)))\n\n        # Now, clear the manifest and try to access the same file.\n        with mock.patch('django.contrib.staticfiles.storage.ManifestFilesMixin.read_manifest', return_value=None):\n            # Expect ValueError because the manifest entry is missing.\n            with self.assertRaises(ValueError):\n                self.render_template(\n                    f'''\n                    <link rel=\"stylesheet\" href=\"{settings.STATIC_URL}test_manifest_strict.css\">\n                    ''',\n                )\n", "    def test_manifest_strict(self):\n        \"\"\"\n        Test that the ManifestStaticFilesStorage raises a ValueError if a file\n        is referenced in a template but missing from the manifest.\n        \"\"\"\n        self.create_static('strict_manifest.css', 'body { color: blue; }')\n        self.create_static('some_other_file.js', 'console.log(\"hello world\");')\n\n        # Create a template that references the CSS file and a non-existing file.\n        self.create_template(\n            'strict_manifest.html',\n            f'<link rel=\"stylesheet\" href=\"{self.static_url(\"strict_manifest.css\")}\">'\n            f'<script src=\"{self.static_url(\"some_other_file.js\")}\"></script>'\n        )\n\n        with self.assertRaises(ValueError) as cm:\n            call_command('collectstatic', interactive=False, verbosity=0)\n        self.assertEqual(\n            str(cm.exception),\n            \"Missing staticfiles manifest entry for 'some_other_file.js'\"\n        )\n", "    def test_hashed_css_file(self):\n        # Ensure a CSS file is hashed correctly, and its content is updated\n        # accordingly in the template.\n        css_file = 'styles.css'\n        css_content = 'body { color: red; }'\n        self.write_file(css_file, css_content)\n        self.assertTemplateOutput(\n            css_file,\n            '<link rel=\"stylesheet\" href=\"%s\">' % self.hashed_file_path(css_file)\n        )\n\n        # Update the content of the CSS file and re-run the test.\n        new_css_content = 'body { color: blue; }'\n        self.write_file(css_file, new_css_content)\n        self.assertTemplateOutput(\n            css_file,\n            '<link rel=\"stylesheet\" href=\"%s\">' % self.hashed_file_path(css_file)\n        )\n", "    def test_manifest_staticfiles_storage_collectstatic(self):\n        \"\"\"\n        Tests that ManifestStaticFilesStorage correctly collects static files\n        and generates a manifest.json file.\n        \"\"\"\n        self.maxDiff = None\n        finders.finders = [\n            mock.MagicMock(spec=finders.Finder),\n        ]\n        finders.finders[0].list.return_value = [\n            'path/to/file.css',\n            'path/to/another/file.js',\n        ]\n\n        # Mock file contents for hashing\n        finders.finders[0].find.side_effect = [\n            ContentFile(b'content of file.css'),\n            ContentFile(b'content of another file.js'),\n        ]\n\n        # Set up a temporary directory for static files\n        with tempfile.TemporaryDirectory() as tmpdir:\n            settings.STATIC_ROOT = tmpdir\n            settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n\n            call_command('collectstatic', verbosity=0, interactive=False)\n\n            # Check if manifest.json is created\n            manifest_path = os.path.join(tmpdir, 'staticfiles.json')\n            self.assertTrue(os.path.exists(manifest_path))\n\n            with open(manifest_path, 'r') as f:\n                manifest = json.load(f)\n\n            expected_manifest = {\n                'paths': {\n                    'path/to/file.css': self.hashed_file_path('path/to/file.css'),\n                    'path/to/another/file.js': self.hashed_file_path('path/to/another/file.js'),\n                },\n                'version': '1.0',\n            }\n            self.assertEqual(manifest, expected_manifest)\n", "    def test_collected_static_files_are_hashed_and_updated(self):\n        \"\"\"\n        Verify that when collect static is called twice,\n        the second time it updates the hashed files, rather than creating new ones.\n\n        See: https://github.com/django/django/issues/32096\n        \"\"\"\n        # Create a file\n        filename = 'test.css'\n        filepath = os.path.join(self.static_dir, filename)\n        with open(filepath, 'w') as f:\n            f.write('body { color: red; }')\n\n        # First collectstatic run\n        call_command('collectstatic', verbosity=0, interactive=False)\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir, self.hashed_file_path(filename))))\n\n        # Modify the file\n        with open(filepath, 'a') as f:\n            f.write('\\n h1 { color: blue; }')\n\n        # Second collectstatic run\n        call_command('collectstatic', verbosity=0, interactive=False)\n\n        # Assert the hashed filename changed\n        original_hash = self.hashed_file_path(filename)\n        new_hash = self.hashed_file_path(filename)\n        self.assertNotEqual(original_hash, new_hash, \"Hashed filenames should change after content update.\")\n"], "sample_399": ["    def test_subquery_in_aggregate(self):\n        with CaptureQueriesContext(connection) as queries:\n            qs = Author.objects.annotate(\n                num_books=Count(\"book\"),\n                avg_price=Avg(\n                    Subquery(Book.objects.filter(contact=OuterRef(\"pk\")).values(\"price\"))\n                ),\n            )\n\n            self.assertEqual(qs.get(pk=self.a1.pk).num_books, 2)\n            self.assertAlmostEqual(qs.get(pk=self.a1.pk).avg_price, Decimal(\"26.345\"))\n        self.assertEqual(len(queries), 3)\n", "    def test_aggregate_functions_with_subqueries(self):\n        with CaptureQueriesContext(connection) as queries:\n            qs = Book.objects.annotate(\n                num_authors=Count(\"authors\"),\n                avg_author_age=Subquery(\n                    Author.objects.filter(books=OuterRef(\"pk\")).annotate(\n                        avg_age=Avg(\"age\")\n                    ).values(\"avg_age\")\n                )\n            ).filter(num_authors__gt=1)\n\n            self.assertQuerySetEqual(\n                qs,\n                [\n                    \"Django Web Development Done Right\",\n                    \"Practical Django Projects\",\n                    \"Artificial Intelligence: A Modern Approach\",\n                    \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\",\n                ],\n                lambda b: b.name,\n            )\n\n            # Check the subquery was used correctly\n            self.assertEqual(len(queries), 2)\n\n", "    def test_aggregate_functions_with_subqueries(self):\n        subquery = Book.objects.filter(rating__gt=4).values(\"publisher__name\").annotate(\n            avg_price=Avg(\"price\")\n        )\n        with CaptureQueriesContext(connection) as queries:\n            results = (\n                Publisher.objects.annotate(\n                    avg_price_of_top_rated=Subquery(subquery.values(\"avg_price\")[:1]),\n                )\n                .order_by(\"avg_price_of_top_rated\")\n                .values(\"name\", \"avg_price_of_top_rated\")\n            )\n            self.assertEqual(len(queries), 2)\n", "    def test_subquery_exists_with_join(self):\n        qs = Book.objects.annotate(\n            has_author_with_friends=Exists(\n                Author.objects.filter(books=OuterRef(\"pk\"), friends__isnull=False)\n            )\n        )\n        self.assertQuerysetEqual(\n            qs.filter(has_author_with_friends=True),\n            [\n                \"The Definitive Guide to Django: Web Development Done Right\",\n                \"Sams Teach Yourself Django in 24 Hours\",\n                \"Practical Django Projects\",\n                \"Python Web Development with Django\",\n            ],\n            lambda b: b.name,\n        )\n\n", "    def test_aggregate_double_distinct_subquery_join(self):\n        with CaptureQueriesContext(connection) as queries:\n            # This query combines a distinct subquery with a join and a non-distinct aggregate.\n            q = (\n                Author.objects.annotate(\n                    num_books_distinct=Count(\n                        \"book__id\", distinct=True, filter=Q(book__rating__gt=4.0)\n                    ),\n                )\n                .filter(num_books_distinct__gt=1)\n                .values(\"name\")\n                .annotate(avg_age=Avg(\"friends__age\"))\n            )\n            self.assertQuerysetEqual(\n                q,\n                [{\"name\": \"Adrian Holovaty\", \"avg_age\": 35.0}, {\"name\": \"Jacob Kaplan-Moss\", \"avg_age\": 35.0}],\n                ordered=False,\n            )\n            self.assertEqual(len(queries), 3)\n"], "sample_685": ["def test_logging_setup_teardown(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"setup\")\n            yield\n            logger.info(\"teardown\")\n\n            logger.info(\"setup 2\")\n            yield\n            logger.info(\"teardown 2\")\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=2)\n    for when in (\"setup\", \"call\", \"teardown\"):\n        for caplog_records in result.getfixturevalue(caplog_records_key).values():\n            assert any(\n                rec.message.startswith(f\"{when} \") for rec in caplog_records\n            )\n\n\n\n", "def test_log_records_at_log_level(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n        import pytest\n\n        logger = logging.getLogger(__name__)\n        logger.setLevel(logging.DEBUG)\n        \n            logger.debug('debug message')\n            logger.info('info message')\n            logger.warning('warning message')\n        \n        \n            with caplog.at_level(logging.INFO):\n                test_function()\n            assert len(caplog.records) == 2\n            assert caplog.records[0].levelname == 'INFO'\n            assert caplog.records[1].levelname == 'WARNING'\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n\n\n", "    def test_log_records_with_different_loggers(testdir: Testdir) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import logging\n\n            logger = logging.getLogger(__name__)\n            sublogger = logging.getLogger(__name__ + \".baz\")\n\n                logger.info(\"message from logger\")\n                sublogger.info(\"message from sublogger\")\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        assert len(result.stdout.lines) == 3\n        for record in result._session._tests[0].records:\n            if record.name == __name__:\n                assert record.msg == \"message from logger\"\n            elif record.name == f\"{__name__}.baz\":\n                assert record.msg == \"message from sublogger\"\n\n", "def test_set_level(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"this is an info message\")\n        \"\"\"\n    )\n    result = testdir.runpytest(\"--log-level\", \"WARNING\")\n    result.assert_outcomes(passed=1)\n\n    # Check that the log message is captured with the correct level\n\n    assert len(result.getfixturevalue(\"caplog\").records) == 1\n    assert result.getfixturevalue(\"caplog\").records[0].levelno == logging.INFO\n", "def test_log_capture_during_test_setup(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        logger = logging.getLogger(__name__)\n\n            logger.info(\"message from test_function\")\n            assert True\n        \n        @pytest.fixture\n            logger.info(\"message from fixture_function\")\n            yield\n            logger.info(\"teardown message from fixture_function\")\n\n        test_function\n\n        \"\"\"\n    )\n\n    result = testdir.runpytest()\n    # check that the log messages from both the test function and fixture are captured\n    assert result.stdout.splitlines()[-3:] == [\n        'INFO:__main__:message from fixture_function',\n        'INFO:__main__:message from test_function',\n        'INFO:__main__:teardown message from fixture_function',\n    ]\n\n\n\n"], "sample_1194": ["def test_julia_code_piecewise():\n    p = Piecewise((x, x < 1), (x**2, x > 1), (1, True))\n    assert julia_code(p) == '((x < 1) ? (x) : ((x > 1) ? (x .^ 2) : (1)))'\n", "def test_julia_code_piecewise_inline():\n    pw = Piecewise((x + 1, x > 0), (x, True))\n    assert julia_code(pw, inline=False, assign_to='tau') == \"\"\"", "def test_julia_code_hadamard_product():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    assert julia_code(A * B) == 'A * B'\n    assert julia_code(A.hadamard_product(B)) == 'A .* B'\n", "def test_julia_code_indexed_loop1():\n    i = symbols('i', integer=True)\n    n = symbols('n', integer=True, positive=True)\n    A = MatrixSymbol('A', n, n)\n    expr = Sum(A[i, i], (i, 0, n - 1))\n    expected = \"\"\"\n    function tr_A()\n        sum = 0\n        for i in 0:n - 1\n            sum += A[i + 1, i + 1]\n        end\n        return sum\n    end\n    tr_A()\n    \"\"\"\n\n    code = julia_code(expr,  contract=True)\n    assert code == expected\n\n", "def test_julia_code_indexedbase():\n    i = Idx('i', 3)\n    A = IndexedBase('A', shape=(3,))\n    e = Eq(A[i], i*x)\n    code = julia_code(e, contract=False)\n    assert code == \"A[i] = i * x\"\n\n"], "sample_520": ["def test_poly3dcollection_with_zsort():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Create a Poly3DCollection with three polygons\n    verts = [\n        [[0, 0, 0], [1, 0, 0], [0, 1, 0]],\n        [[1, 0, 0], [1, 1, 0], [0, 1, 0]],\n        [[0, 0, 1], [1, 0, 1], [0, 1, 1]],\n    ]\n    pc = art3d.Poly3DCollection(verts, zsort='min')\n    ax.add_collection(pc)\n\n    # Set the viewing angle\n    ax.view_init(elev=30, azim=45)\n\n    return fig\n\n", "def test_poly3dcollection_empty():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    verts = []\n    pc = art3d.Poly3DCollection(verts)\n    ax.add_collection(pc)\n    fig.canvas.draw()\n\n", "    def test_axes3d_set_box_aspect(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n\n        ax.set_box_aspect((1, 1, 1))\n        ax.plot([0, 1], [0, 1], [0, 1], 'k--')\n\n        ax.set_box_aspect((2, 1, 3))\n        ax.plot([0, 1], [0, 1], [0, 1], 'r-')\n\n        ax.set_box_aspect([1, 2, 1])\n        ax.plot([0, 1], [0, 1], [0, 1], 'b:')\n\n", "    def test_Poly3DCollection_zsort_average_with_empty_segments(self):\n        verts = [\n            np.array([[0, 0, 0], [1, 0, 1], [1, 1, 1]]),\n            np.array([]),  # Empty segment\n            np.array([[2, 2, 2], [3, 2, 3], [3, 3, 3]]),\n        ]\n        pc = art3d.Poly3DCollection(verts, zsort='average')\n        fig, ax = plt.subplots(subplot_kw=dict(projection='3d'))\n        ax.add_collection(pc)\n        ax.view_init(elev=30, azim=45)\n        fig.canvas.draw()\n\n        # Assert that the rendering doesn't crash due to empty segments\n        assert pc._facecolors2d.size > 0\n", "def test_axes3d_set_aspect():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.set_aspect('equal')\n    ax.plot([1, 2], [1, 2], [0, 0])\n    ax.plot([1, 1], [1, 2], [0, 0])\n"], "sample_16": ["    def test_shape(self):\n        self.check(np.shape, self.q)\n", "    def test_unique(self):\n        self.check(np.unique, return_index=True, return_inverse=True)\n", "    def test_zeros_like(self):\n        self.check(np.zeros_like, self.q)\n", "    def test_shape(self):\n        self.check(np.shape, self.q)\n", "    def test_shape(self):\n        self.check(np.shape, self.q)\n"], "sample_1084": ["def test_intersection_complexregion_complexregion():\n    # Test intersecting two ComplexRegions with polar representations\n    cr1 = ComplexRegion(Interval(0, 2*S.Pi), Interval(1, 2))\n    cr2 = ComplexRegion(Interval(S.Pi/2, 3*S.Pi/2), Interval(1, sqrt(2)))\n    intersection = intersection_sets(cr1, cr2)\n    assert intersection == ComplexRegion(Interval(S.Pi/2, 3*S.Pi/2, True, True), Interval(1, 2))\n", "def test_intersection_complexregion_image_set():\n    x = Symbol('x')\n    cr = ComplexRegion(Interval(0, 2)*Interval(0, 2*pi))\n    f = Lambda(x, x**2)\n    is_ = imageset(f, S.Reals)\n    assert intersection_sets(cr, is_) == cr\n\n", "def test_intersection_sets_complexregion_and_finite_set():\n    a = ComplexRegion(Interval(0, 1)*Interval(0, 1))\n    b = FiniteSet(sqrt(2)/2 + I*sqrt(2)/2)\n    assert intersection_sets(a, b) == b\n\n", "def test_intersection_complexregion_interval():\n    # Test intersection of ComplexRegion with Interval\n    cr = ComplexRegion(Interval(0, 2), polar=False)\n    i = Interval(1, 3)\n    expected = ComplexRegion(Interval(1, 2), polar=False)\n    assert intersection_sets(cr, i) == expected\n", "    def test_intersection_complexregion_interval():\n        a = ComplexRegion(Interval(0, 2)*Interval(0, 2*S.Pi), polar=True)\n        b = Interval(1, 2)\n        assert intersection_sets(a, b) == ComplexRegion(Interval(1, 2) * Interval(0, 2*S.Pi), polar=True)\n\n"], "sample_200": ["    def test_emailmessage_attach_text_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filepath = os.path.join(tmpdir, \"test.txt\")\n            with open(filepath, 'w') as f:\n                f.write(\"This is a test file.\\n\")\n            msg = EmailMessage(\n                'Subject', 'Body', 'sender@example.com', ['recipient@example.com']\n            )\n            msg.attach_file(filepath)\n            msg.send()\n            self.assertEqual(\n                len(mail.outbox[0].attachments), 1\n            )\n            filename, content, mimetype = self.get_decoded_attachments(msg)[0]\n            self.assertEqual(filename, 'test.txt')\n            self.assertEqual(content.decode(), \"This is a test file.\\n\")\n            self.assertEqual(mimetype, 'text/plain')\n", "    def test_emailmessage_attach_file_with_utf8_content(self):\n        with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:\n            f.write('\u4f60\u597d\u4e16\u754c'.encode('utf-8'))\n            f.seek(0)\n            filename = f.name\n        try:\n            msg = EmailMessage(\n                subject='Test', body='Test message', from_email='from@example.com',\n                to=['to@example.com']\n            )\n            msg.attach_file(filename)\n\n            attachments = self.get_decoded_attachments(msg)\n            self.assertEqual(len(attachments), 1)\n            self.assertEqual(attachments[0][0], os.path.basename(filename))\n            self.assertEqual(attachments[0][1].decode('utf-8'), '\u4f60\u597d\u4e16\u754c')\n            self.assertEqual(attachments[0][2], 'text/plain; charset=\"utf-8\"')\n\n        finally:\n            os.unlink(filename)\n", "    def test_emailmessage_with_unicode_filename_in_attachment(self):\n        msg = EmailMessage(\n            'Subject', 'Body', from_email='from@example.com',\n            to=['to@example.com'],\n        )\n        filename = 'r\u00e9sum\u00e9.txt' \n        content = 'This is my r\u00e9sum\u00e9'.encode('utf-8')\n        msg.attach(filename, content, mimetype='text/plain')\n        msg.send()\n        with mail.outbox as outbox:\n            self.assertEqual(len(outbox), 1)\n            email = outbox[0]\n            self.assertEqual(email.subject, 'Subject')\n            self.assertEqual(email.body, 'Body')\n            attachments = self.get_decoded_attachments(email)\n            self.assertEqual(len(attachments), 1)\n            self.assertEqual(attachments[0][0], filename)\n            self.assertEqual(attachments[0][1], content)\n            self.assertEqual(attachments[0][2], 'text/plain')\n", "    def test_emailmessage_attach_file_with_specified_mimetype(self):\n        with tempfile.NamedTemporaryFile(suffix=\".txt\") as f:\n            f.write(b\"This is a test file.\\n\")\n            f.seek(0)\n            filename = f.name\n            email_message = EmailMessage(\n                subject=\"Test Email\",\n                body=\"Test message\",\n                from_email=\"from@example.com\",\n                to=[\"to@example.com\"],\n            )\n            mimetype = 'text/plain'\n            email_message.attach_file(filename, mimetype=mimetype)\n            attachments = self.get_decoded_attachments(email_message)\n            self.assertEqual(len(attachments), 1)\n            self.assertEqual(attachments[0][0], os.path.basename(filename))\n            self.assertEqual(attachments[0][1], b\"This is a test file.\\n\")\n            self.assertEqual(attachments[0][2], mimetype)\n", "    def test_emailmessage_attach_file_with_non_ascii_filename(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            filename = os.path.join(tempdir, ' r\u00e9sum\u00e9.txt')\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write('This is a r\u00e9sum\u00e9.')\n\n            email = EmailMessage(\n                subject='Test email',\n                body='Test body',\n                from_email='test@example.com',\n                to=['recipient@example.com'],\n            )\n            email.attach_file(filename)\n            \n            attachments = self.get_decoded_attachments(email)\n\n            self.assertEqual(len(attachments), 1)\n            filename, content, mimetype = attachments[0]\n            self.assertEqual(filename, ' r\u00e9sum\u00e9.txt')\n            self.assertEqual(content.decode('utf-8'), 'This is a r\u00e9sum\u00e9.')\n            self.assertEqual(mimetype, 'text/plain')\n\n"], "sample_1022": ["    def test_repeated_decimals():\n        assert parse_expr(\"0.2[1]\") == sympy.Rational(19, 90)\n        assert parse_expr(\"0.2[12]\") == sympy.Rational(211, 990)\n        assert parse_expr(\"1.2[34]\") == sympy.Rational(1234 - 12, 990)\n        assert parse_expr(\".2[1]\") == sympy.Rational(19, 90)\n\n        raises(ValueError, lambda: parse_expr(\"0.[123]\"))\n        raises(ValueError, lambda: parse_expr(\"0.2[123] \"))\n", "def test_convert_xor():\n    assert parse_expr('1^2', transformations=(convert_xor,)) == 1**2\n    assert parse_expr('x^y', transformations=(convert_xor,)) == x**y\n    assert parse_expr('a^b^c', transformations=(convert_xor,)) == a**(b**c)\n\n\n\n", "    def test_auto_symbol():\n        assert parse_expr('x + 2*y', transformations=(standard_transformations)) == sympy.Add(sympy.Symbol('x'), sympy.Mul(2, sympy.Symbol('y')))\n        assert parse_expr('sin(x)', transformations=(standard_transformations)) == sympy.sin(sympy.Symbol('x'))\n        assert parse_expr('x + sin(y)', transformations=(standard_transformations)) == sympy.Add(sympy.Symbol('x'), sympy.sin(sympy.Symbol('y')))\n", "    def test_function_exponentiation():\n        assert parse_expr(\"sin**2(x)\", transformations=(standard_transformations + (function_exponentiation,))) == sympy.sin(x)**2\n        assert parse_expr(\"sin(x)**2\", transformations=(standard_transformations + (function_exponentiation,))) == sympy.sin(x)**2\n        assert parse_expr(\"sin**2(x+y)\", transformations=(standard_transformations + (function_exponentiation,))) == sympy.sin(x + y)**2\n", "    def test_factorial_notation():\n        assert parse_expr('5!!', transformations=standard_transformations) == sympy.factorial(5) / sympy.factorial(3)\n        assert parse_expr('5!!!', transformations=standard_transformations) == sympy.factorial2(5)\n        assert parse_expr('5!', transformations=standard_transformations) == sympy.factorial(5)\n"], "sample_726": ["    def test_labelbinarizer_multi_output():\n        lb = LabelBinarizer()\n        X = np.array([[1, 2], [3, 4]])\n        assert_raises(ValueError, lb.fit_transform, X)\n", "    def test_multilabelbinarizer_transform_sparse(self):\n        mlb = MultiLabelBinarizer(sparse_output=True)\n        mlb.fit([[1, 2], [3], []])\n        X = [[1, 2], [3], [1]]\n        Xt = mlb.transform(X)\n        assert_true(issparse(Xt))\n        assert_equal(Xt.shape, (3, 3))\n        assert_array_equal(Xt.toarray(), [[1, 1, 0], [0, 0, 1], [1, 0, 0]])\n\n", "    def test_label_binarize_empty_array(self):\n        # Test for empty array input\n        y = np.array([])\n        lb = LabelBinarizer()\n        y_bin = lb.fit_transform(y)\n        assert_array_equal(y_bin, np.array([]).reshape(0, 0))\n        assert_equal(lb.classes_.shape[0], 0) \n", "    def test_label_binarize_multilabel_indicator(self):\n        y = np.array([[1, 0, 1], [0, 1, 0]])\n        classes = [0, 1, 2]\n        result = label_binarize(y, classes=classes, sparse_output=True)\n        assert_true(isinstance(result, csc_matrix))\n        expected = coo_matrix([[1, 0, 1], [0, 1, 0]])\n        assert_equal(result.toarray(), expected.toarray())\n", "    def test_label_binarize_multiclass_sparse_pos_label(self):\n        y = np.array([0, 1, 2, 0]).reshape(-1, 1)\n        classes = [0, 1, 2]\n        expected = coo_matrix([[1, 0, 0],\n                               [0, 1, 0],\n                               [0, 0, 1],\n                               [1, 0, 0]])\n\n        assert_equal(toarray(label_binarize(y, classes,\n                                          pos_label=1, sparse_output=True)),\n                     expected.toarray())\n\n\n\n"], "sample_1162": ["    def test_expand_mul_noncommutative():\n        # test expand_mul with noncommutative symbols\n        x, y = noncomm_x, Symbol('y')\n        assert expand_mul(x*y*x*y) == x*x*y*y\n        assert expand_mul(x*(y*x)) == x*y*x\n        assert expand_mul((x*y)*x) == x*y*x\n", "def test_count_ops_matrix():\n    x = Symbol('x')\n    A = MatrixSymbol('A', 2, 2)\n    assert count_ops(A, visual=False) == 0\n    assert count_ops(A*A, visual=False) == 4\n    assert count_ops(A + A, visual=False) == 4\n    assert count_ops(2*A, visual=False) == 4\n    assert count_ops(A**2, visual=False) == 4\n    assert count_ops(Matrix([[1, x], [x, 1]]), visual=False) == 2\n", "    def test_expand_power_base_noncommutative():\n        x, y = noncomm_x, Symbol('y')\n        assert expand_power_base((x*y)**2) == x**2*y**2\n        assert expand_power_base((x*y)**(x + 1)) == (x*y)**(x + 1)\n", "def test_expand_commutative_matrix():\n    A = Matrix([\n        [1, 2],\n        [3, 4]\n    ])\n    B = Matrix([\n        [5, 6],\n        [7, 8]\n    ])\n\n    assert expand((A + B)**2) == expand((A**2 + 2*A*B + B**2))\n\n", "    def test_expand_mul_noncommutative():\n        i = Symbol('i', commutative=False)\n        x = Symbol('x')\n        assert expand_mul(x*i*x) == x*x*i\n        assert expand_mul(x*i*x*i) == x*x*i*i\n"], "sample_694": ["def test_deprecation_warning_hook_legacy_marking(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        from _pytest.mark import Mark\n            config.addinivalue_line(\"markers\", \"mymark: description\")\n\n        class MyPlugin:\n                # This is the old style.\n                if \"mymark\" in item.keywords:\n                    print(\"setup\")\n\n        \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_outcomes(skipped=1)\n    result.stdout.fnmatch_lines(\n        [\n            \"*pytest.ini*\",\n            \"*MyPlugin*\",\n            \"*Please use the pytest.hook*\",\n            \"* instead*\",\n        ]\n    )\n", "def test_nose_support(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert 1 == 1\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.assert_contains(deprecated.NOSE_SUPPORT.format(nodeid=\"test_nose\", method=\"test_nose\", stage=\"setup\"))\n", "def test_deprecated_plugin_warning(pytester: Pytester, plugin: str) -> None:\n    pytester.mkdir(\"plugins\")\n    pytester.chdir(\"plugins\")\n    pytester.makepyfile(\n        f\"{plugin}.py\", \"def pytest_addoption(parser): pass\"\n    )\n\n    with pytest.raises(pytest.fail.Exception) as excinfo:\n        pytester.parseconfig([])\n    assert re.search(f\"Plugin '{plugin}' is deprecated\", str(excinfo.value))\n\n", "    def test_deprecated_plugin(self, plugin, pytester: Pytester) -> None:\n        pytester.mkdir(\"plugins\")\n        pytester.chdir(\"plugins\")\n\n        pytester.makepyfile(\n            f\"conftest.py\",\n            f\"import pytest\\npytest.mark.skipif(not pytest.config.pluginmanager.hasplugin('{plugin}'), reason='plugin is not installed')\",\n        )\n\n        result = pytester.runpytest()\n        result.assert_outcomes(passed=1)\n", "def test_deprecate_nose_support(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n\n            pass\n    \"\"\"\n    )\n    result = pytester.runpytest()\n    result.stdout.fnmatch_lines(\n        [\n            \"*nose*deprecated*\",\n            \"*test_nose*nose method*\",\n        ]\n    )\n"], "sample_1049": ["def test_plane_parameter_value():\n    p = Plane((2, 0, 0), (0, 0, 1), (0, 1, 0))\n    t = symbols('t')\n    pt = p.arbitrary_point(t)\n    assert p.parameter_value(pt.subs(t, 0), t) == {t: 0}\n\n    u, v = symbols('u v')\n    pt = p.arbitrary_point(u, v)\n    assert p.parameter_value(pt.subs({u: 1, v: 2}), u, v) == {u: 1, v: 2}\n\n    raises(ValueError, lambda: p.parameter_value(Point3D(1, 1, 1), t))\n\n", "def test_plane_parameter_value():\n    p = Plane((0, 0, 0), (1, 1, 1))\n    t = Dummy('t')\n    pt = p.arbitrary_point(t)\n    assert p.parameter_value(pt, t) == {t: t}\n    assert p.parameter_value(pt, Dummy('u')) == {Dummy('u'): t}\n    pt2 = p.arbitrary_point(t, t)\n    assert p.parameter_value(pt2, Dummy('u'), Dummy('v')) == {Dummy('u'): t, Dummy('v'):", "    def test_plane_projection_line():\n        a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n        b = Line(Point3D(1, 1), Point3D(2, 2))\n        assert a.projection_line(b) == Line3D(Point3D(4/3, 4/3, 1/3), Point3D(5/3, 5/3, -1/3))\n        c = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n        assert a.projection_line(c) == Point3D(1, 1, 1)\n        d = Ray(Point3D(1, 1), Point3D(2, 2))\n        assert a.projection_line(d) == Ray3D(Point3D(4/3, 4/3, 1/3), Point3D(5/3, 5/3, -1/3))\n        e = Segment(Point3D(1, 1), Point3D(2, 2))\n        assert a.projection_line(e) == Segment3D(Point3D(4/3, 4/3, 1/3), Point3D(5/3, 5/3, -1/3))\n\n", "def test_plane_projection():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    b = Line(Point3D(1, 1), Point3D(2, 2))\n    assert a.projection_line(b) == Line3D(Point3D(4/3, 4/3, 1/3), Point3D(5/3, 5/3, -1/3))\n    c = Line3D(Point3D(1, 1, 1), Point3D(2, 2, 2))\n    assert a.projection_line(c) == Point3D(1, 1, 1)\n", "def test_plane_projection():\n    a = Plane(Point3D(1, 1, 1), normal_vector=(1, 1, 1))\n    b = Line3D(Point3D(1, 1), Point3D(2, 2, 2))\n    assert a.projection_line(b) == Point3D(1, 1, 1)\n"], "sample_790": ["    def test_kernel_pca_sparse_input(self):\n        X, y = make_circles(noise=0.1, factor=0.5, random_state=0)\n        X_sparse = sp.csr_matrix(X)\n        kpca = KernelPCA(kernel='linear', fit_inverse_transform=True,\n                         n_components=2)\n        kpca.fit(X_sparse)\n        X_transformed = kpca.transform(X_sparse)\n        assert_array_almost_equal(kpca.inverse_transform(X_transformed), X)\n", "    def test_precomputed_kernel(self):\n        X = np.random.rand(10, 5)\n        kernel = rbf_kernel(X)\n        kpca = KernelPCA(kernel=\"precomputed\", n_components=2).fit(kernel)\n        assert_equal(kpca.n_components, 2)\n        assert_allclose(kpca.lambdas_, [x for x in np.linalg.eigvals(kernel) if x > 0])\n\n", "    def test_kernelpca_precomputed_kernel(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        K = rbf_kernel(X)\n        kpca = KernelPCA(kernel=\"precomputed\", n_components=1)\n        X_transformed = kpca.fit_transform(K)\n        assert_equal(X_transformed.shape, (3, 1))\n", "    def test_kernelpca_rbf_sparse(self):\n        X, _ = make_circles(n_samples=100, factor=0.5, random_state=0)\n        X = sp.csr_matrix(X)\n        kpca = KernelPCA(kernel=\"rbf\", gamma=10, fit_inverse_transform=True)\n        kpca.fit(X)\n        X_transformed = kpca.transform(X)\n        X_back = kpca.inverse_transform(X_transformed)\n        assert_allclose(X.toarray(), X_back, atol=1e-5)\n", "    def test_kernelPCA_precomputed_kernel(self):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        # We use a non-linear kernel here to make sure it works\n        K = rbf_kernel(X, gamma=1)\n        kPCA = KernelPCA(kernel='precomputed', n_components=2)\n        X_transformed = kPCA.fit_transform(K)\n\n        assert_equal(X_transformed.shape, (4, 2))\n        assert_allclose(kPCA.lambdas_, [1.9944, 0.0056])\n        assert_allclose(kPCA.alphas_,\n                        np.array([[-0.3855, -0.5197],\n                                  [-0.5197, 0.3855],\n                                  [-0.2931, 0.1568],\n                                  [-0.2931, 0.1568]]))\n\n"], "sample_1061": ["def test_Pow_is_constant():\n    assert Pow(2, 3).is_constant()\n    assert Pow(Symbol('x'), 2).is_constant() is False\n    assert Pow(2, Symbol('x')).is_constant() is False\n    assert Pow(Symbol('x'), Symbol('y')).is_constant() is None\n    assert Pow(2, 3).is_constant(simplify=False)\n    assert Pow(2, 3).is_constant(expr=Pow(2, 3))\n\n", "def test_Pow_evalf_imperfect_square():\n    assert Pow(2, Rational(1, 2)).evalf(2) == 1.41\n    assert Pow(3, Rational(1, 2)).evalf(2) == 1.73\n\n\n", "    def test_eval_as_leading_term(self):\n        x = Symbol('x')\n        assert self.func(x + 1, 2).as_leading_term(x) == x**2\n        assert self.func(1 + x, 2).as_leading_term(x) == x**2\n        assert self.func(x, 2).as_leading_term(x) == x**2\n        assert self.func(x + 1, -2).as_leading_term(x) == 1/x**2\n        assert self.func(1 + x, -2).as_leading_term(x) == 1/x**2\n        assert self.func(x, -2).as_leading_term(x) == 1/x**2\n        assert self.func(1 + x, 1/2).as_leading_term(x) == sqrt(1+x)\n        assert self.func(x + 1, 1/2).as_leading_term(x) == sqrt(1+x)\n        assert self.func(2*x + 1, 2).as_leading_term(x) == 4*x**2\n\n", "def test_is_constant():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert (x**2).is_constant() is False\n    assert (x**0).is_constant() is True\n    assert (2**x).is_constant() is False\n    assert (2**0).is_constant() is True\n    assert (x**y).is_constant() is False\n    assert (2**y).is_constant(y) is None\n\n", "def test_Pow_Mul():\n    x = Symbol('x')\n    y = Symbol('y')\n    assert Pow(Mul(x, y), 2) == Mul(x, y)**2\n    assert Pow(Mul(x, 2), 2) == (x*2)**2\n\n"], "sample_88": ["    def test_emailmessage_bad_header_values(self):\n        msg = EmailMessage(\n            'Subject', 'Body', 'sender@example.com', ['recipient@example.com'],\n        )\n        with self.assertRaises(BadHeaderError):\n            msg['Invalid-Header'] = 'This is\\n a bad header'\n\n        with self.assertRaises(BadHeaderError):\n            msg['Another-Invalid-Header'] = 'This is\\r a bad header'\n", "    def test_emailmessage_attach_file_non_ascii_filename(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            filename = os.path.join(tempdir, 'R\u00e9sum\u00e9.txt')\n            with open(filename, 'w', encoding='utf-8') as f:\n                f.write('Some text')\n            msg = EmailMessage('Subject', 'Body', from_email='from@example.com',\n                               to=['to@example.com'])\n            msg.attach_file(filename)\n            attachments = self.get_decoded_attachments(msg)\n            self.assertEqual(len(attachments), 1)\n            filename, content, mimetype = attachments[0]\n            self.assertEqual(filename, 'R\u00e9sum\u00e9.txt')\n            self.assertEqual(content.decode('utf-8'), 'Some text')\n            self.assertEqual(mimetype, 'text/plain')\n", "    def test_emailmessage_attachments_from_file(self):\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filename = os.path.join(tmpdir, 'test_attachment.txt')\n            with open(filename, 'w') as f:\n                f.write('This is a test attachment.')\n\n            message = EmailMessage(\n                subject='Test',\n                body='Test body',\n                from_email='from@example.com',\n                to=['to@example.com'],\n            )\n            message.attach_file(filename)\n\n            expected_headers = set([\n                ('Subject', 'Test'),\n                ('From', 'from@example.com'),\n                ('To', 'to@example.com'),\n                ('Message-ID', email.utils.make_msgid()),\n            ])\n\n            # Check that the headers are correct\n            self.assertMessageHasHeaders(message.message().as_bytes(), expected_headers)\n\n            # Check that the attachment is decoded correctly\n\n            attachments = self.get_decoded_attachments(message)\n            self.assertEqual(len(attachments), 1)\n            self.assertEqual(attachments[0][0], 'test_attachment.txt')\n            self.assertEqual(attachments[0][1], b'This is a test attachment.')\n            self.assertEqual(attachments[0][2], 'text/plain')\n", "    def test_email_message_sanitize_address_trailing_whitespace(self):\n        addr = 'user@example.com '\n        self.assertEqual(sanitize_address(addr), 'user@example.com')\n\n", "    def test_bad_header(self):\n        msg = EmailMessage(\n            'Subject', 'Body', 'from@example.com', ['to@example.com']\n        )\n        with self.assertRaises(BadHeaderError):\n            msg['Subject'] = 'Bad\\nHeader'\n"], "sample_1073": ["def test_sqrtdenest_rational_combination():\n    expr = sqrt(3 + 2*r2) + sqrt(3 - 2*r2)\n    assert sqrtdenest(expr) == 2*sqrt(3)\n\n", "def test_sqrtdenest_issue_20693():\n    x = Symbol('x')\n    expr = sqrt(root(x, 4) + 1)\n    assert sqrtdenest(expr) == expr\n", "def test_sqrtdenest_issue_19814():\n    expr = sqrt(r2 + sqrt(r5) + sqrt(r30))\n    assert sqrtdenest(expr) == sqrt(r2 + sqrt(r5) + sqrt(r30))\n", "def test_sqrtdenest_issue_18177():\n    x = Symbol('x', positive=True)\n    expr = sqrt(sqrt(sqrt(3) + 1) + 1) + 1 + sqrt(2)\n    assert sqrtdenest(sqrt((expr**2).expand())) == expr\n\n", "    def test_sqrt_match():\n        assert _sqrt_match(1 + sqrt(2) + sqrt(2)*sqrt(3) +  2*sqrt(1+sqrt(5))) == (1 + sqrt(2) + sqrt(6), 2, 1 + sqrt(5))\n        assert _sqrt_match(sqrt(2)/2 + sqrt(3)/2) == ( S.Half, S.Half, 2)\n        assert _sqrt_match(sqrt(r2 + r6)) == (sqrt(r2 + r6), 1, 1)\n\n\n\n"], "sample_169": ["    def test_deserialize_json_field_with_custom_decoder(self):\n        data = '{\"key1\": 1, \"key2\": \"value2\", \"key3\": true}'\n        serialized_data = serializers.serialize('json', [JSONModel.objects.create(json_field=data)])\n        deserialized_objects = list(serializers.deserialize('json', serialized_data))\n        loaded_object = deserialized_objects[0].object\n        self.assertEqual(loaded_object.json_field, {'key1': 1, 'key2': 'value2', 'key3': True})\n", "    def test_deserialize_with_natural_keys(self):\n        model_data = [\n            {\n                'pk': 1,\n                'name': 'Test1',\n                'tags': ['tag1', 'tag2'],\n            },\n            {\n                'pk': 2,\n                'name': 'Test2',\n                'tags': ['tag3', 'tag4'],\n            },\n        ]", "    def test_deserialization_with_natural_keys(self):\n        # Test deserialization with natural key objects\n\n        obj = JSONModel.objects.create(data={'name': 'test', 'age': 30})\n        serialized_data = serializers.serialize('xml', [obj])\n\n        # Modify the serialized data to use natural keys\n        serialized_data = serialized_data.replace(\n            f'<field name=\"data\" type=\"JSONField\">{serializers.serialize(\"json\", obj.data)}</field>',\n            f'<field name=\"data\" type=\"JSONField\"><natural>test</natural><natural>30</natural></field>',\n        )\n\n        deserialized_objects = list(serializers.deserialize('xml', serialized_data))\n        self.assertEqual(len(deserialized_objects), 1)\n        deserialized_obj = deserialized_objects[0].object\n\n        self.assertEqual(deserialized_obj.data['name'], 'test')\n        self.assertEqual(deserialized_obj.data['age'], 30)\n\n", "    def test_deserialize_jsonfield_with_custom_decoder(self):\n        # Test deserialization with a custom JSON decoder.\n        data = serializers.serialize(\"xml\", [JSONModel.objects.create(data='{\"key\": \"value\", \"nested\": {\"subkey\": \"subvalue\"}}')],indent=2)\n        deserialized = list(serializers.deserialize(\"xml\", data, using=self.db, decoder=CustomJSONDecoder))\n        self.assertEqual(deserialized[0].object.data[\"key\"], \"value\")\n        self.assertEqual(deserialized[0].object.data[\"nested\"][\"subkey\"], \"subvalue\")\n\n", "    def test_deserialization_with_natural_keys(self):\n        data = {'model': JSONModel._meta.model_name, 'pk': str(uuid.uuid4()), 'field': '{\"key1\": \"value1\"}'}\n        serialized_data = serializers.serialize('xml', [JSONModel.objects.create(field={'key1': 'value1'})])\n        deserialized_data = serializers.deserialize('xml', serialized_data)\n        self.assertEqual(list(deserialized_data), [JSONModel.objects.get(pk=JSONModel.objects.latest('pk').pk)])\n"], "sample_851": ["    def test_mean_tweedie_deviance_power_2():\n        # Gamma deviance\n        y_true = np.array([2, 1, 4, 3])\n        y_pred = np.array([2.5, 1.5, 3.5, 2.5])\n        assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=2),\n                             1.015495)\n", "    def test_mean_tweedie_deviance_power_range():\n        # Check Tweedie deviance for various power values\n        y_true = [2, 0, 1, 4]\n        y_pred = [0.5, 0.5, 2., 2.]\n        for power in [-1.5, 0, 1, 2, 3, 4]:\n            # Skip power values that are not supported\n            if power < -1 or (power > 1 and power < 2):\n                with pytest.raises(ValueError):\n                    mean_tweedie_deviance(y_true, y_pred, power=power)\n            else:\n                loss = mean_tweedie_deviance(y_true, y_pred, power=power)\n                assert isinstance(loss, float)\n\n", "    def test_mean_tweedie_deviance_negative_power(self):\n        y_true = [2, 0, 1, 4]\n        y_pred = [0.5, 0.5, 2., 2.]\n        with pytest.raises(ValueError):\n            mean_tweedie_deviance(y_true, y_pred, power=-1)\n\n", "    def test_mean_tweedie_deviance_power_1():\n        y_true = np.array([2, 0, 1, 4])\n        y_pred = np.array([0.5, 0.5, 2., 2.])\n        expected = 1.426038\n        assert_almost_equal(mean_tweedie_deviance(y_true, y_pred, power=1),\n                            expected)\n", "    def test_mean_tweedie_deviance_all_powers():\n        y_true = np.array([2, 0, 1, 4, 0, 3])\n        y_pred = np.array([0.5, 0.5, 2., 2., 1., 2.])\n        powers = [-1.5, 0, 1, 1.5, 2, 2.5]\n\n        for power in powers:\n            with pytest.warns(UserWarning) if power < 0 else pytest.raises(ValueError) \\\n                    if power < 0 else pytest.raises(ValueError) if power < 1 else pytest.raises(ValueError) if power > 2 else None:\n                mean_tweedie_deviance(\n                    y_true, y_pred, power=power\n                )\n"], "sample_446": ["    def test_floatformat01(self):\n        with translation.override(\"en-US\"):\n            a = Decimal(\"1234.56789\")\n            b = Decimal(\"-9876.54321\")\n            self.assertEqual(\n                self.engine.render_to_string(\"floatformat01\", {\"a\": a, \"b\": b}),\n                \"1234.56789 -9876.54321\"\n            )\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):  # Ensure English locale\n            output = self.render({\"a\": 12.3456789, \"b\": Decimal(\"12.3456789\")})\n            self.assertEqual(output, \"12.3456789 12.3456789\")\n", "    def test_floatformat01(self):\n        with translation.override(\"en-us\"):\n            with localcontext() as ctx:\n                ctx.prec = 3\n                output = self.engine.render_to_string(\n                    \"floatformat01\", {\"a\": Decimal(\"12.345\"), \"b\": Decimal(\"67.89\")}\n                )\n\n                self.assertEqual(output, \"12.345 67.890\")\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):\n            template = self.engine.from_string(self.templates[\"floatformat01\"])\n            self.assertEqual(\n                template.render(self.context.update({\"a\": Decimal(\"12.345\"), \"b\": 12.345})),\n                \"12.345 12.345\",\n            )\n\n", "    def test_floatformat01(self):\n        with translation.override(\"en\"):\n            output = self.engine.render_to_string(\"floatformat01\", {\"a\": 123.45678901234567, \"b\": 0.0001})\n        self.assertEqual(output, \"123.456789012 0.0001\")\n"], "sample_842": ["def test_kernel_approx_fprime():\n    # Test _approx_fprime function\n    n_samples = 10\n    X = np.random.randn(n_samples, 3)\n    kernel = RBF(length_scale=1.0)\n    grad = _approx_fprime(kernel.K(X), X, 1e-4)\n    assert grad.shape == (n_samples, 3)\n", "def test_kernel_operator(kernel):\n    # Check if kernel can be combined with a KernelOperator\n    try:\n        KernelOperator(kernel)\n    except TypeError:\n        # Some kernels, like PairwiseKernel, are not compatible\n        pass\n    else:\n        k = KernelOperator(kernel)\n        assert isinstance(k, Kernel)\n\n\n", "def test_kernel_gradient(kernel):\n    # Check gradient of kernel wrt its hyperparameters\n    if isinstance(kernel, KernelOperator) or isinstance(kernel, PairwiseKernel):\n        # Skip KernelOperator and PairwiseKernel as they are wrappers\n        return\n\n    # Check if kernel has hyperparameters\n    if len(signature(kernel.get_params).parameters) <= 1:\n        return\n\n    try:\n        grad = _approx_fprime(kernel.get_params,\n                              lambda theta: kernel(X, Y, eval_gradient=False), 1e-4)\n    except NotImplementedError:\n        return\n    # Compare numerical and analytical gradients\n    analytical_grad = kernel.gradient(X, Y)\n\n    assert_array_almost_equal(grad, analytical_grad, decimal=5)\n\n", "    def test_kernel_diag(self, kernel):\n        K = kernel(X)\n        assert_array_almost_equal(np.diag(K), kernel.diag(X))\n", "    def test_kernel_diag(self, kernel):\n        # Check if diag outputs the correct diagonal\n        K = kernel(X)\n        K_diag = kernel.diag(X)\n        assert_array_almost_equal(np.diag(K), K_diag)\n"], "sample_1068": ["def test_octave_Piecewise_with_inequality():\n    p = Piecewise((x + 1, x > 0), (x, True))\n    code = octave_code(p)\n    assert code == '((x > 0).*(x + 1) + (~(x > 0)).*(x))'\n\n", "def test_piecewise_no_default():\n    pw = Piecewise((x + 1, x > 0))\n    raises(ValueError, lambda: octave_code(pw))\n", "    def test_octave_code_indexed_base_contraction():\n        from sympy import Eq, IndexedBase, Idx, ccode\n        len_y = 5\n        y = IndexedBase('y', shape=(len_y,))\n        t = IndexedBase('t', shape=(len_y,))\n        Dy = IndexedBase('Dy', shape=(len_y-1,))\n        i = Idx('i', len_y-1)\n        e = Eq(Dy[i], (y[i+1]-y[i])/(t[i+1]-t[i]))\n        assert octave_code(e.rhs, assign_to=e.lhs, contract=True\n                           ) == octave_code(e.rhs, assign_to=e.lhs, contract=False)\n", "    def test_octave_Piecewise_no_default():\n        pw = Piecewise((x + 1, x > 0), (x, x >= 0))  # Note the x>=0\n        raises(ValueError, lambda: octave_code(pw))\n\n\n\n", "def test_octave_print_expint():\n    assert octave_code(expint(1, x)) == \"expint(x)\"\n"], "sample_1166": ["def test_Monomial_as_expr():\n    m = Monomial((1, 2, 3), gens=[x, y, z])\n    assert m.as_expr() == x*y**2*z**3\n\n    m = Monomial((0, 1, 0), gens=[a, b, c])\n    assert m.as_expr() == b\n\n    m = Monomial((2, 0, 1))\n    assert m.as_expr(x, y, z) == x**2*z\n    raises(ValueError, lambda: m.as_expr())\n\n", "def test_monomial_divides():\n    assert monomial_divides((1, 2), (3, 4))\n    assert not monomial_divides((1, 2), (0, 2))\n    assert monomial_divides((1, 2, 3), (1, 2, 3))\n    assert monomial_divides((0, 0, 0), (1, 2, 3))\n    assert not monomial_divides((3, 2, 1), (1, 2, 0))\n", "def test_monomial_pow():\n    assert monomial_pow((1, 2, 3), 2) == (2, 4, 6)\n    assert Monomial((1, 2, 3))**2 == Monomial((2, 4, 6))\n    assert Monomial((1, 2, 3), [a, b, c])**2 == Monomial((2, 4, 6), [a, b, c])\n\n\n", "def test_monomial_divides():\n    assert monomial_divides((1, 2), (3, 4))\n    assert not monomial_divides((1, 2), (0, 2))\n\n    assert monomial_divides((0, 0), (1, 2))\n    assert monomial_divides((0, 0), (0, 0))\n\n    assert not monomial_divides((1, 2, 3), (1, 2))\n    assert monomial_divides((1, 2, 1), (1, 2, 1))\n", "    def test_monomial_pow_zero(self):\n        m = Monomial((1, 2, 3))\n        assert m**0 == Monomial((0, 0, 0))\n"], "sample_499": ["    def test_legend_handler_tuple():\n        # Create a figure and axes\n        fig, ax = plt.subplots()\n\n        # Create a tuple of Line2D artists\n        lines = (mlines.Line2D([0, 1], [0, 1]),\n                 mlines.Line2D([0, 1], [1, 0]))\n\n        # Create a legend using the tuple of lines\n        ax.legend(lines, ('Line 1', 'Line 2'))\n\n        # Assert that the legend handler is a HandlerTuple\n        handler = ax.get_legend().get_legend_handler(lines[0])\n        assert isinstance(handler, HandlerTuple)\n", "    def test_legend_with_empty_label(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2], [1, 2], label=\"\")\n        with pytest.warns(UserWarning):\n            ax.legend()\n        # We don't care about the exact content of the warning, only\n        # that one is raised.\n", "    def test_legend_handler(self, handler_map, expected_labels):\n\n        fig, ax = plt.subplots()\n\n        line = mlines.Line2D([0, 1], [0, 1], color='b')\n        collection = mcollections.PathCollection(\n                    [plt.Circle((i, i), 0.1) for i in range(3)],\n                    facecolors=['r', 'g', 'b'])\n        rectangle = mpatches.Rectangle((0.1, 0.1), 0.5, 0.5)\n        circle = mpatches.Circle((0.5, 0.5), 0.2)\n        text = ax.text(0.1, 0.1, 'Text')\n\n        ax.add_artist(line)\n        ax.add_collection(collection)\n        ax.add_patch(rectangle)\n        ax.add_patch(circle)\n        ax.add_artist(text)\n\n        handles, labels = _get_legend_handles_labels([ax],\n                                                    handler_map=handler_map)\n\n        assert [h.__class__.__name__ for h in handles] == expected_labels\n\n", "    def test_legend_empty_labels():\n        # Test that legend handles empty strings or None labels correctly\n\n        fig, ax = plt.subplots()\n        line1 = ax.plot([1, 2, 3], label=\"\")\n        line2 = ax.plot([1, 2, 3], label=None)\n        line3 = ax.plot([1, 2, 3], label=\"_nolegend_\")\n\n        with pytest.raises(ValueError):\n            ax.legend()\n\n        # Ensure that the legend handles empty strings and None labels\n\n        legend = mpl.pyplot.legend([line1[0], line2[0]], [None, \"\"])\n        assert legend is not None\n        assert len(legend.get_texts()) == 2\n        assert legend.get_texts()[0].get_text() == \"\"\n        assert legend.get_texts()[1].get_text() == \"\"\n\n", "    def test_legend_handler_tuple():\n        fig, ax = plt.subplots()\n\n        x = np.arange(0, 10, 0.1)\n        line1, = ax.plot(x, np.sin(x), label='sin')\n        line2, = ax.plot(x, np.cos(x), label='cos')\n        tuple_artist = (line1, line2)\n        ax.legend(handles=[tuple_artist], handler_map={tuple: HandlerTuple()})\n\n        # Check that the legend contains both labels\n        labels = [text.get_text() for text in ax.get_legend().get_texts()]\n        assert labels == ['sin', 'cos']\n\n"], "sample_156": ["    def test_hidden_fields(self):\n        form = Person(\n            data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'},\n        )\n        self.assertEqual(len(form.visible_fields()), 3)\n        self.assertEqual(len(form.hidden_fields()), 0)\n\n        form = Person(\n            data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'},\n            prefix='pre'\n        )\n        self.assertEqual(len(form.visible_fields()), 3)\n\n", "    def test_has_changed(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-01-01'}\n        form = Person(data)\n        self.assertFalse(form.has_changed())\n\n        form = Person({'first_name': 'Jane', 'last_name': 'Doe', 'birthday': '2023-01-01'})\n        self.assertTrue(form.has_changed())\n\n        form = Person({'first_name': 'John', 'last_name': 'Smith', 'birthday': '2023-01-01'})\n        self.assertTrue(form.has_changed())\n\n        form = Person({'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-02-01'})\n        self.assertTrue(form.has_changed())\n\n", "    def test_form_field_order(self):\n        class CustomForm(Form):\n            field1 = CharField()\n            field2 = CharField()\n            field3 = CharField()\n\n        form = CustomForm(field_order=['field3', 'field1'])\n        self.assertEqual(list(form.fields.keys()), ['field3', 'field1', 'field2'])\n\n        form.order_fields(['field2', 'field3'])\n        self.assertEqual(list(form.fields.keys()), ['field2', 'field3', 'field1'])\n\n        form.order_fields(None)\n        self.assertEqual(list(form.fields.keys()), ['field1', 'field2', 'field3'])\n\n", "    def test_hidden_fields(self):\n        form = PersonNew(data={'first_name': 'John', 'last_name': 'Doe'})\n        self.assertEqual(len(form.hidden_fields()), 0)\n        form = PersonNew({'birthday': '2023-10-26'}, initial={'first_name': 'John'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 1)\n        self.assertTrue(isinstance(hidden_fields[0].widget, HiddenInput))\n        self.assertEqual(hidden_fields[0].name, 'first_name')\n", "    def test_hidden_fields_rendered(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        self.assertEqual(form.as_p(), '<p><label for=\"id_first_name\">First name: </label><input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\"><br><span class=\"helptext\"></span></p><p><label for=\"id_last_name\">Last name: </label><input type=\"text\" name=\"last_name\" value=\"Doe\" id=\"id_last_name\"><br><span class=\"helptext\"></span></p><p><label for=\"id_birthday\">Birthday: </label><input type=\"text\" name=\"birthday\" value=\"\" id=\"id_birthday\"><br><span class=\"helptext\"></span></p>')\n\n"], "sample_410": ["    def test_get_email_field_name_returns_email_field_if_no_email_field_defined(self):\n        class NoEmailFieldUser(AbstractBaseUser):\n            pass\n\n        self.assertEqual(NoEmailFieldUser.get_email_field_name(), \"email\")\n", "    def test_get_email_field_name(self):\n        self.assertEqual(AbstractBaseUser.get_email_field_name(), \"email\")\n        class CustomUser(AbstractBaseUser):\n            EMAIL_FIELD = \"custom_email\"\n        self.assertEqual(CustomUser.get_email_field_name(), \"custom_email\")\n", "    def test_get_email_field_name(self):\n        self.assertEqual(AbstractBaseUser.get_email_field_name(), \"email\")\n\n        class CustomUser(AbstractBaseUser):\n            EMAIL_FIELD = \"custom_email\"\n\n        self.assertEqual(CustomUser.get_email_field_name(), \"custom_email\")\n", "    def test_normalize_username(self):\n        user_model = get_user_model()\n        self.assertEqual(user_model.normalize_username(\"testuser\"), \"testuser\")\n        self.assertEqual(user_model.normalize_username(\"TestUser\"), \"TestUser\")\n        self.assertEqual(user_model.normalize_username(\"  Test User  \"), \"Test User\")\n        self.assertEqual(user_model.normalize_username(\"\\u00C4\\u00E4\\u00f6\"), \"\\u00C4\\u00E4\\u00f6\")\n", "    def test_get_email_field_name(self):\n        self.assertEqual(AbstractBaseUser.get_email_field_name(), 'email')\n\n        class MyUser(AbstractBaseUser):\n            EMAIL_FIELD = 'custom_email'\n\n        self.assertEqual(MyUser.get_email_field_name(), 'custom_email')\n\n"], "sample_840": ["    def test_pls_regression_coef(self):\n        # Test that the coefficients from PLSRegression match those obtained\n        # from a linear regression model fitted on the latent components.\n\n        rng = check_random_state(42)\n        n_samples = 100\n        n_features = 5\n        n_targets = 2\n        X = rng.randn(n_samples, n_features)\n        Y = rng.randn(n_samples, n_targets)\n\n        pls = pls_.PLSRegression(n_components=2)\n        pls.fit(X, Y)\n        x_scores = pls.x_scores_\n        y_scores = pls.y_scores_\n\n        # Fit a linear regression model on the latent components\n        from sklearn.linear_model import LinearRegression\n        lr = LinearRegression(fit_intercept=False)\n        lr.fit(x_scores, y_scores)\n        coef_lr = lr.coef_\n\n        assert_array_almost_equal(coef_lr, pls.coef_)\n\n", "def test_plsvd_n_components():\n    # Test for correct handling of n_components\n    # Check if PLSSVD raises an error when n_components is greater than the\n    # number of features in X or targets in Y\n\n    rng = check_random_state(42)\n    n_samples = 50\n    n_features = 20\n    n_targets = 10\n\n    X = rng.randn(n_samples, n_features)\n    Y = rng.randn(n_samples, n_targets)\n\n    # n_components should be smaller than number of features and targets\n    plsvd = PLSSVD(n_components=n_features + 1)\n    with pytest.raises(ValueError):\n        plsvd.fit(X, Y)\n\n    plsvd = PLSSVD(n_components=n_targets + 1)\n    with pytest.raises(ValueError):\n        plsvd.fit(X, Y)\n\n", "    def test_plsca_fit_transform(self):\n        X, y = load_linnerud(return_X_y=True)\n        plsca = PLSCanonical(n_components=2)\n        X_c, y_c = plsca.fit_transform(X, y)\n        assert X_c.shape == (y_c.shape[0], 2)\n\n\n\n", "    def test_pls_regression_predict_consistency(self, random_state):\n        # Check consistency between predict and transform for PLSRegression\n        rng = check_random_state(random_state)\n        n_samples = 20\n        n_features = 10\n        n_targets = 5\n        X = rng.rand(n_samples, n_features)\n        Y = rng.rand(n_samples, n_targets)\n        pls = pls_.PLSRegression(n_components=2, scale=True)\n        pls = pls.fit(X, Y)\n        Y_pred_predict = pls.predict(X)\n        X_scores, Y_scores = pls.transform(X, Y)\n        # Multiply scores with loadings to get X_pred\n        Y_pred_transform = np.dot(X_scores, pls.y_loadings_.T)\n        assert_array_almost_equal(Y_pred_predict, Y_pred_transform, decimal=6)\n", "    def test_PLSSVD_with_empty_Y():\n        # Test PLSSVD with empty Y array. Should not raise error.\n        X = np.random.rand(5, 3)\n        y = np.array([])\n        pls = PLSSVD(n_components=2)\n        pls.fit(X, y)\n\n"], "sample_521": ["    def test_poly3dcollection_set_verts(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        verts = np.array([[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]],\n                          [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]]])\n        pc = art3d.Poly3DCollection(verts, alpha=0.5)\n        ax.add_collection(pc)\n\n        pc.set_verts([[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]]])\n\n        ax.view_init(elev=30, azim=45)\n        fig.canvas.draw()\n", "    def test_poly3dcollection_zsort_average(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(projection='3d')\n        verts = []\n        for i in range(5):\n            xs = np.linspace(0, i, 5)\n            ys = np.linspace(0, i, 5)\n            zs = np.linspace(0, i, 5)\n            verts.append(np.column_stack((xs, ys, zs)))\n        p = art3d.Poly3DCollection(verts, zsort='average')\n        ax.add_collection3d(p)\n        ax.view_init(elev=30, azim=45)\n\n        # Save the plot\n        fig.savefig(\"test_poly3dcollection_zsort_average.png\")\n\n", "def test_poly3dcollection_2d_projection():\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    verts = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0)],\n        [(0, 0, 1), (1, 0, 1), (1, 1, 1)],\n    ]\n    pc = art3d.Poly3DCollection(verts)\n    ax.add_collection(pc)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_zlim([0, 1])\n    ax.view_init(elev=30, azim=45)\n    return fig\n", "def test_zalpha():\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    x = np.array([1, 2, 3, 4, 5])\n    y = np.array([2, 3, 4, 5, 6])\n    z = np.array([1, 2, 3, 4, 5])\n    colors = ['r', 'g', 'b', 'y', 'm']\n\n    col = ax.scatter(x, y, z, c=colors)\n    assert np.allclose(col._facecolors[:, 3],\n                       np.array([1, 0.3, 0.0, 0.7, 0.4]))\n\n", "def test_poly3dcollection_zsort():\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n\n    verts = [\n        [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n        [(0, 0, 1), (1, 0, 1), (1, 1, 1), (0, 1, 1)],\n    ]\n    col = art3d.Poly3DCollection(verts, facecolor=\"blue\", edgecolor=\"red\")\n    ax.add_collection3d(col)\n\n    # Set zsort to 'max'\n    col.set_zsort('max')\n    ax.view_init(elev=30, azim=45)\n    fig.canvas.draw()\n\n    # Check if the polygons are drawn in the intended order\n    # based on the maximum z-value of each polygon.\n\n    # ... Add assertions here to verify the drawing order ...\n"], "sample_370": ["    def test_prefetch_related_with_reverse_fk(self):\n        with CaptureQueriesContext(connection) as captured_queries:\n            # Prefetch the 'authors' related managers on the Book objects.\n            prefetch_authors = Book.objects.prefetch_related('authors').all()\n            self.assertEqual(len(captured_queries), 2)\n\n            # Accessing the 'authors' attribute on a Book object\n            # shouldn't trigger extra queries.\n            for book in prefetch_authors:\n                authors = list(book.authors.all())\n                self.assertEqual(len(captured_queries), 2)\n\n\n\n", "    def test_prefetch_related_with_select_related_and_ordering(self):\n        author_qs = Author.objects.prefetch_related('books').order_by('name')\n        with CaptureQueriesContext(connection) as captured_queries:\n            list(author_qs)\n        self.assertEqual(len(captured_queries), 2)\n", "    def test_prefetch_related_with_reversed_fk(self):\n        with CaptureQueriesContext(connection) as queries:\n            prefetch_related_objects(\n                [self.book1], 'authors',\n            )\n\n        # Only one query should be executed for the authors\n        self.assertEqual(len(queries), 2)\n", "    def test_prefetch_related_on_reverse_m2m(self):\n        with CaptureQueriesContext(connection) as queries:\n            prefetch_related_objects(\n                [self.author1],\n                Prefetch('book_set', queryset=Book.objects.filter(title__startswith='Jane')),\n            )\n        self.assertEqual(len(queries.captured_queries), 2)\n        self.assertWhereContains(\n            queries.captured_queries[1]['sql'], \"'Jane'\"\n        )\n", "compilation error"], "sample_1019": ["def test_gcd_terms_radical():\n    x = Symbol('x', real=True)\n    assert gcd_terms(sqrt(x)*y**2 + sqrt(x)*z**2 + x*sqrt(x)*y) == sqrt(x)*(y + z + x*y)\n", "def test_gcd_terms_issue_19554():\n    f = Function('f')\n    x = Symbol('x', real=True)\n    assert gcd_terms(x*f(x) + f(x)).is_constant() is False\n", "def test_factor_nc_issue_20140():\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    C = Symbol('C', commutative=False)\n    expr = A*B*C + A*B*C + A*C*B + A*C*B\n    factored = factor_nc(expr)\n    assert factored == 2*A*B*C + 2*A*C*B\n\n\n", "def test_factor_nc_issue_16484():\n    A = Symbol('A', commutative=False)\n    B = Symbol('B', commutative=False)\n    C = Symbol('C', commutative=False)\n    assert factor_nc(A*B + A*C) == A*(B + C)\n\n", "    def test_factor_terms_sign(self):\n        assert factor_terms(x - y, sign=False) == x - y\n        assert factor_terms(-x + y, sign=False) == -x + y\n        assert factor_terms(-x - y, sign=False) == -x - y\n"], "sample_308": ["    def test_format_timezone_aware_datetime(self):\n        d = datetime(2023, 10, 26, 10, 30, 0, tzinfo=utc)\n        formatted = format(d, 'Y-m-d H:i:s O')\n        self.assertEqual(formatted, '2023-10-26 10:30:00 +0000')\n\n        d_aware = make_aware(\n            d,\n            timezone=get_fixed_timezone(get_default_timezone())\n        )\n        formatted = format(d_aware, 'Y-m-d H:i:s O')\n        self.assertEqual(formatted, '2023-10-26 10:30:00 +0100')\n", "    def test_strftime_u(self):\n        d = datetime(2023, 10, 26, 10, 30, 5, 123456)\n        self.assertEqual(format(d, 'u'), '123456')\n\n", "    def test_dateformat_c_format(self):\n        d = date(2008, 1, 2)\n        self.assertEqual(dateformat.format(d, 'c'), '2008-01-02')\n", "    def test_format_timezone(self):\n        dt = datetime(2023, 10, 26, 12, 0, 0)\n        aware_dt = make_aware(dt, get_fixed_timezone(utc))\n\n        self.assertEqual(format(aware_dt, 'e'), '+0000')\n        self.assertEqual(format(aware_dt, 'O'), '+0000')\n        self.assertEqual(format(aware_dt, 'P'), '12:00 p.m.')\n\n        get_default_timezone().localize(dt)\n        self.assertEqual(format(dt, 'e'), 'CET')\n        self.assertEqual(format(dt, 'O'), '+0100')\n        self.assertEqual(format(dt, 'P'), '1:00 p.m.')\n", "    def test_datetime_timezone_offsets(self):\n        dt = datetime(2023, 10, 26, 12, 0, 0, tzinfo=get_fixed_timezone(10))\n        self.assertEqual(format(dt, 'O'), '+1000')\n\n"], "sample_883": ["    def test_ard_regression_constant_features():\n        rng = check_random_state(0)\n        n_samples, n_features = 50, 10\n        X = rng.randn(n_samples, n_features)\n        X[:, 2] = 1.\n        y = rng.randn(n_samples)\n\n        ard = ARDRegression()\n        ard.fit(X, y)\n\n        assert ard.lambda_[2] == 0 or ard.lambda_[2] > ard.threshold_lambda\n\n        ridge = Ridge()\n        ridge.fit(X, y)\n\n        assert_array_almost_equal(ard.coef_[:2], ridge.coef_[:2], decimal=2)\n        assert_array_almost_equal(ard.coef_[3:], ridge.coef_[3:], decimal=2) \n", "    def test_ard_regression_intercept(self, random_state=42):\n        # Test that ARD regression can handle intercept properly.\n        rng = check_random_state(random_state)\n        n_samples = 100\n        n_features = 10\n        X = rng.randn(n_samples, n_features)\n        y = X[:, 0] + rng.randn(n_samples)\n        n_alphas = 5\n        alphas = np.logspace(-3, 3, n_alphas)\n\n        for fit_intercept in [True, False]:\n            for alpha in alphas:\n                clf = ARDRegression(alpha_1=alpha, alpha_2=alpha, fit_intercept=fit_intercept)\n                clf.fit(X, y)\n                assert clf.coef_.shape[0] == n_features\n                if fit_intercept:\n                    assert clf.intercept_ != 0.0\n                else:\n                    assert clf.intercept_ == 0.0\n\n", "    def test_ard_regression_compute_score(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        clf = ARDRegression(compute_score=True)\n        clf.fit(X, y)\n        assert len(clf.scores_) > 0\n", "    def test_ard_regression_compute_score(self):\n        # Test if the log marginal likelihood is computed correctly.\n        rng = check_random_state(0)\n        n_samples, n_features = 10, 5\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        model = ARDRegression(compute_score=True)\n        model.fit(X, y)\n        score = model.scores_[-1]\n\n        # Compute the log marginal likelihood manually\n        alpha = model.alpha_\n        lambda_ = model.lambda_\n        sigma = model.sigma_\n        coef = model.coef_\n        rmse = np.sum((y - np.dot(X, coef)) ** 2)\n        s = (model.lambda_1 * np.log(lambda_) - model.lambda_2 * lambda_).sum()\n        s += model.alpha_1 * log(alpha) - model.alpha_2 * alpha\n        s += 0.5 * (\n            fast_logdet(sigma)\n            + n_samples * log(alpha)\n            + np.sum(np.log(lambda_))\n        )\n        s -= 0.5 * (alpha * rmse + (lambda_ * coef**2).sum())\n        assert_almost_equal(score, s, 6)\n", "    def test_ard_regression_n_features_gt_n_samples(self):\n        rng = check_random_state(42)\n        n_samples = 5\n        n_features = 10\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        ard = ARDRegression()\n        ard.fit(X, y)\n\n        # check that the predictions are not\n        # completely off after fitting\n        assert_array_less(np.abs(ard.predict(X) - y), 10)\n"], "sample_332": ["    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice, max_num=5, absolute_max=7, extra=2\n        )\n\n        formset = formset_class(\n            {'choices-TOTAL_FORMS': '6', 'choices-INITIAL_FORMS': '0', 'choices-MAX_NUM_FORMS': '5', 'choices-MIN_NUM_FORMS': '0'},\n        )\n        self.assertEqual(formset.total_form_count(), 6)\n        self.assertEqual(len(formset.forms), 6)\n\n        formset = formset_class(\n            {'choices-TOTAL_FORMS': '8', 'choices-INITIAL_FORMS': '0', 'choices-MAX_NUM_FORMS': '5', 'choices-MIN_NUM_FORMS': '0'},\n        )\n        self.assertEqual(formset.total_form_count(), 7)\n        self.assertEqual(len(formset.forms), 7)\n\n        with self.assertRaises(ValidationError):\n            formset_class(\n                {'choices-TOTAL_FORMS': '8', 'choices-INITIAL_FORMS': '0', 'choices-MAX_NUM_FORMS': '5', 'choices-MIN_NUM_FORMS': '0'},\n            )\n", "    def test_formset_factory_with_can_delete_extra_false(self):\n        formset = formset_factory(FavoriteDrinkForm, extra=2, can_delete=True, can_delete_extra=False)\n        # Initial formset has 2 forms.\n        formset_instance = formset(data={\n            'form-0-name': 'Milk',\n            'form-1-name': 'Water',\n            'form-0-DELETE': 'y',\n        })\n        self.assertFalse(formset_instance.is_valid())\n        self.assertEqual(len(formset_instance.deleted_forms), 0)\n\n", "    def test_formset_factory_absolute_max(self):\n        form = Form({'field': CharField()})\n        FormSet = formset_factory(\n            form,\n            absolute_max=5\n        )\n        formset = FormSet(data={'form-0-field': 'a', 'form-1-field': 'b'})\n        self.assertEqual(formset.absolute_max, 5)\n", "    def test_formset_factory_can_delete_extra(self):\n        initial_data = [('Choice 1', 10), ('Choice 2', 5)]\n        formset = self.make_choiceformset(\n            formset_data=initial_data,\n            extra=2,\n            can_delete_extra=True,\n        )\n        self.assertEqual(formset.total_form_count(), 4)\n        formset.forms[2].data['DELETE'] = True\n        formset.full_clean()\n        self.assertEqual(len(formset.deleted_forms), 1)\n        self.assertEqual(formset.total_form_count(), 3)\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(FavoriteDrinkForm, max_num=10, absolute_max=5)\n        formset = formset_class({'form-TOTAL_FORMS': '6', 'form-INITIAL_FORMS': '0'})\n        self.assertEqual(formset.errors, [\n            {'votes': ['Please submit at most 5 forms.']},\n\n        ])\n"], "sample_888": ["    def test_sparse_input(self):\n        # Check that IsolationForest works with sparse input\n\n        X_diabetes = csc_matrix(diabetes.data)\n        X_train, X_test = train_test_split(X_diabetes, test_size=0.2, random_state=42)\n\n        clf = IsolationForest(random_state=42).fit(X_train)\n        y_pred = clf.predict(X_test)\n\n        assert len(y_pred) == X_test.shape[0]\n\n        # Check that sparse input works with score_samples\n        scores = clf.score_samples(X_test)\n        assert scores.shape[0] == X_test.shape[0]\n", "    def test_sparse_input(self):\n        X_sparse = csc_matrix(iris.data)\n        clf = IsolationForest(random_state=0).fit(X_sparse)\n        clf.predict(X_sparse)\n        clf.decision_function(X_sparse)\n        clf.score_samples(X_sparse)\n", "    def test_average_path_length_corner_case():\n        for n_samples in [0, 1, 2]:\n            assert_allclose(\n                _average_path_length(np.array([n_samples])),\n                _average_path_length(np.array([n_samples])),\n            )\n", "def test_average_path_length():\n    n_samples_leaf = np.array([1, 2, 3, 4, 5, 10, 100])\n    average_path_length_expected = np.array(\n        [0.0, 1.0,\n        2 * (np.log(2) + np.euler_gamma) - 2 * 2 / 3,\n        2 * (np.log(3) + np.euler_gamma) - 2 * 3 / 4,\n        2 * (np.log(4) + np.euler_gamma) - 2 * 4 / 5,\n        2 * (np.log(9) + np.euler_gamma) - 2 * 9 / 10,\n        2 * (np.log(99) + np.euler_gamma) - 2 * 99 / 100,\n        ]\n    )\n\n    assert_allclose(_average_path_length(n_samples_leaf), average_path_length_expected)\n\n", "    def test_sparse_input(self):\n        # Check that IsolationForest works with sparse input (csc & csr)\n        rng = check_random_state(42)\n        X = rng.randn(100, 10)\n        X_sparse_csc = csc_matrix(X)\n        X_sparse_csr = csr_matrix(X)\n\n        # Fit on sparse & dense, predict on sparse & dense\n        clf = IsolationForest(random_state=0).fit(X_sparse_csc)\n        pred_csc = clf.predict(X_sparse_csc)\n        pred_csr = clf.predict(X_sparse_csr)\n        pred_dense = clf.predict(X)\n        assert_array_equal(pred_csc, pred_csr)\n        assert_array_equal(pred_csc, pred_dense)\n\n"], "sample_1075": ["    def test_beta_eval_conjugate(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        assert conjugate(beta(x,y)) == beta(conjugate(x), conjugate(y))\n", "    def test_beta_eval_rewrite_as_gamma():\n\n        x = Symbol('x')\n        y = Symbol('y')\n        assert beta(x, y)._eval_rewrite_as_gamma(x, y) == gamma(x)*gamma(y)/gamma(x+y)\n", "    def test_beta_diff():\n        x = Symbol('x')\n        y = Symbol('y')\n        assert diff(beta(x, y), x) == beta(x, y)*(digamma(x) - digamma(x + y))\n        assert diff(beta(x, y), y) == beta(x, y)*(digamma(y) - digamma(x + y))\n", "    def test_beta_eval_rewrite_as_gamma(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        assert beta(x,y)._eval_rewrite_as_gamma(x, y) == gamma(x)*gamma(y)/gamma(x + y)\n", "    def test_beta_eval_conjugate():\n        x = Symbol('x', real=True)\n        y = Symbol('y', real=True)\n        assert conjugate(beta(x,y)) == beta(conjugate(x), conjugate(y))\n"], "sample_37": ["    def test_fits_header_creation(self):\n        with catch_warnings(AstropyUserWarning) as w:\n            wcs = wcs.WCS(\n                naxis=2,\n                cdelt=[1., 1.],\n                crpix=[1., 1.],\n                crval=[0., 0.],\n                ctype=['RA---TAN', 'DEC--TAN'],\n            )\n        assert len(w) == 1\n        assert \"CTYPEi keywords are deprecated\" in str(w[0].message)\n        header = wcs.to_header()\n        assert header['CTYPE1'] == 'RA---TAN'\n        assert header['CTYPE2'] == 'DEC--TAN'\n\n", "    def test_maps_readwrite(self):\n        for filename in self._file_list:\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', UserWarning)\n\n                with open(filename, 'rb') as hdub:\n                    hdul = fits.HDUList.fromstring(hdub.read())\n                    wcs = wcs.WCS(hdul[0].header)\n                    hdul[0].header['COMMENT'] = 'Testing'\n                    hdul.writeto(filename + '.tmp', overwrite=True)\n\n                    with open(filename + '.tmp', 'rb') as hdub2:\n                        hdul2 = fits.HDUList.fromstring(hdub2.read())\n                    wcs2 = wcs.WCS(hdul2[0].header)\n\n                    assert wcs.to_header() == wcs2.to_header()\n                    os.remove(filename + '.tmp')\n", "    def test_find_wcs_in_fits(self):\n        \"\"\"Test that we can find WCS in a FITS file.\"\"\"\n\n        with fits.open(self._file_list[0]) as hdul:\n            wcses = wcs.find_all_wcs(hdul[0].header)\n            assert len(wcses) == 1\n\n", "    def test_sip_to_polynomial(self):\n        # Test Sip.to_polynomial() method using a sample SIP header\n\n        header_file = get_pkg_data_filename(\"maps/sip_test.hdr\")\n        with open(header_file, \"r\") as f:\n            header = fits.Header.fromstring(f.read())\n\n        wcs = wcs.WCS(header)\n\n        # Extract SIP coefficients from the header\n        sip = wcs.sip\n\n        # Convert SIP coefficients to polynomial representation\n        polynomial = sip.to_polynomial()\n\n        # Assert that the polynomial coefficients are not None\n        assert polynomial is not None\n", "    def test_maps_read(self):\n        for filename in self._file_list:\n\n            with open(filename, 'rb') as hd:\n                header = fits.Header.fromtextfile(hd)\n            wcs_obj = wcs.WCS(header)\n            assert isinstance(wcs_obj, wcs.WCS)\n\n"], "sample_836": ["def test_type_of_target_non_array_like():\n    for X in NON_ARRAY_LIKE_EXAMPLES:\n        with pytest.raises(ValueError, match=\"Expected array-like\"):\n            type_of_target(X)\n", "    def test_type_of_target_multilabel_sequences(self):\n        for y in MULTILABEL_SEQUENCES:\n            assert_equal(type_of_target(y), 'multilabel-indicator')\n", "    def test_unique_labels_empty_multilabel(self):\n        for y in MULTILABEL_SEQUENCES:\n            with pytest.raises(ValueError):\n                unique_labels(y)\n", "    def test_class_distribution_sparse(self):\n        # Test class_distribution on sparse input\n        X = sp.csc_matrix([[0, 1, 0], [1, 0, 1], [0, 0, 1]])\n        sample_weight = np.array([1, 2, 1])\n        classes, n_classes, class_prior = class_distribution(X, sample_weight)\n        assert_array_equal(classes, [np.array([0, 1]), np.array([0, 1, 2]),\n                                      np.array([0, 1])])\n        assert_array_equal(n_classes, [2, 3, 2])\n        assert_allclose(class_prior[0], np.array([0.5, 0.5]))\n        assert_allclose(class_prior[1], np.array([0.25, 0.5, 0.25]))\n        assert_allclose(class_prior[2], np.array([0.5, 0.5]))\n", "    def test_type_of_target_multilabel_indicator_sparse(self):\n        X = csc_matrix([[0, 1], [1, 0]])\n        assert_equal(type_of_target(X), 'multilabel-indicator')\n"], "sample_782": ["    def test_column_transformer_sparse_output(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]]).astype(float)\n        y = np.array([0, 1, 2])\n        ct = ColumnTransformer(\n            transformers=[\n                ('num', StandardScaler(), [0]),\n                ('text', OneHotEncoder(sparse=True), [1])\n            ],\n            remainder='passthrough',\n            sparse_threshold=0.5\n        )\n        X_transformed = ct.fit_transform(X)\n        assert isinstance(X_transformed, sparse.csr_matrix)\n        X_transformed = ct.transform(X)\n        assert isinstance(X_transformed, sparse.csr_matrix)\n\n        ct = ColumnTransformer(\n            transformers=[\n                ('num', StandardScaler(), [0]),\n                ('text', OneHotEncoder(sparse=True), [1])\n            ],\n            remainder='passthrough',\n            sparse_threshold=0\n        )\n        X_transformed = ct.fit_transform(X)\n        assert isinstance(X_transformed, np.ndarray)\n        X_transformed = ct.transform(X)\n        assert isinstance(X_transformed, np.ndarray)\n", "    def test_column_transformer_sparse_output():\n        X = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n        ct = ColumnTransformer(transformers=[\n            ('num', StandardScaler(), [0, 1]),\n            ('cat', OneHotEncoder(), [2, 3]),\n        ], sparse_threshold=0.5)\n\n        X_t = ct.fit_transform(X)\n        assert isinstance(X_t, sparse.csr_matrix)\n\n        ct = ColumnTransformer(transformers=[\n            ('num', StandardScaler(), [0, 1]),\n            ('cat', OneHotEncoder(), [2, 3]),\n        ], sparse_threshold=0.1)\n\n        X_t = ct.fit_transform(X)\n        assert isinstance(X_t, np.ndarray)\n", "    def test_error_in_fit(self):\n        X = np.array([[1, 2], [3, 4]])\n        transformer = ColumnTransformer([('t', TransRaise(), [0])],\n                                        remainder='passthrough')\n        with pytest.raises(ValueError, match=\"specific message\"):\n            transformer.fit(X)\n", "    def test_sparse_threshold_mixed_output(self):\n        # Verify that mixed sparse and dense output is handled correctly\n        # based on the sparse_threshold\n\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        sparse_data = sparse.csc_matrix(X[:, 1:].reshape(3, 2))\n        transformer = ColumnTransformer(\n            transformers=[\n                ('dense', 'passthrough', [0]),\n                ('sparse', StandardScaler(), [1, 2]),\n            ],\n            sparse_threshold=0.5\n        )\n\n        transformed = transformer.fit_transform(X)\n        assert isinstance(transformed, sparse.csr_matrix), \"Output should be sparse\"\n\n\n", "def test_sparse_output():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n\n    # All transformers are sparse\n    ct = ColumnTransformer(\n        transformers=[\n            ('scaler', StandardScaler(), [0]),\n            ('normalize', Normalizer(), [1])\n        ],\n        remainder='passthrough',\n        sparse_threshold=0.5\n    )\n    Xt = ct.fit_transform(X)\n    assert isinstance(Xt, sparse.csr_matrix)\n\n    # One transformer is sparse\n    ct = ColumnTransformer(\n        transformers=[\n            ('scaler', StandardScaler(), [0]),\n            ('normalize', Normalizer(), [1])\n        ],\n        remainder='passthrough',\n        sparse_threshold=0.9\n    )\n    Xt = ct.fit_transform(X)\n    assert isinstance(Xt, np.ndarray)\n    \n    # Test with all sparse input data\n    X_sparse = sparse.csr_matrix(X)\n    ct = ColumnTransformer(\n        transformers=[\n            ('scaler', StandardScaler(), [0]),\n            ('normalize', Normalizer(), [1])\n        ],\n        remainder='passthrough',\n        sparse_threshold=0.5\n    )\n    Xt = ct.fit_transform(X_sparse)\n    assert isinstance(Xt, sparse.csr_matrix)\n\n\n"], "sample_762": ["    def test_clone_dtype(self):\n        # Test cloning estimators with specific dtypes\n        X = np.array([[1, 2], [3, 4]], dtype=np.float32)\n        clf = DecisionTreeClassifier()\n        clf.fit(X, [0, 1])\n        cloned_clf = clone(clf)\n        assert_equal(cloned_clf.tree_.value.dtype, np.float64)\n", "    def test_clone_gridsearchcv_pipeline(self):\n        # check if cloning works with GridSearchCV and Pipelines\n        X, y = datasets.load_iris(return_X_y=True)\n        pipe = Pipeline([('svc', SVC())])\n        params = {'svc__C': [1, 10]}\n        grid = GridSearchCV(pipe, params)\n\n        grid.fit(X, y)\n        cloned_grid = clone(grid)\n        assert_equal(grid.best_params_, cloned_grid.best_params_)\n        assert_array_equal(grid.best_estimator_.named_steps['svc'].coef_,\n                           cloned_grid.best_estimator_.named_steps['svc'].coef_)\n\n", "    def test_clone_sparse_matrix(self):\n        # Test cloning with sparse matrices\n        X = sp.csr_matrix([[1, 0], [0, 1]])\n        est = K(c=X)\n        cloned = clone(est)\n        assert_true(isinstance(cloned.c, sp.csr_matrix))\n        assert_array_equal(cloned.c.toarray(), est.c.toarray())\n", "    def test_clone_no_estimator(self):\n        est = NoEstimator()\n        clone_est = clone(est)\n        assert_true(isinstance(clone_est, NoEstimator))\n\n", "    def test_clone_pipeline(self):\n        # Test that cloning a pipeline works\n        iris = datasets.load_iris()\n        pipe = Pipeline([('svc', SVC())])\n        pipe.fit(iris.data, iris.target)\n        cloned_pipe = clone(pipe)\n        assert_true(pipe.named_steps['svc'] is not cloned_pipe.named_steps['svc'])\n        assert_array_equal(pipe.predict(iris.data), cloned_pipe.predict(iris.data))\n"], "sample_686": ["    def test_pytest_collect_module_deprecation(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            pytest.collect.Module()\n            \"\"\"\n        )\n        with pytest.raises(PytestDeprecationWarning) as excinfo:\n            testdir.runpytest()\n        assert (\n            \"pytest.collect.{name} was moved to pytest.{name}\"\n            in str(excinfo.value)\n        )\n", "def test_DEPRECATED_EXTERNAL_PLUGINS(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n            assert 1 == 1\n    \"\"\"\n    )\n\n    config = testdir.parseconfig()\n    config.addinivalue_line(\"addopts\", \"-p pytest_catchlog\")\n    with pytest.raises(pytest.UsageError) as excinfo:\n        config.perform_collect()\n    assert \"plugin 'pytest_catchlog' is deprecated\" in str(excinfo.value)\n\n", "    def test_fillfuncargs_deprecation(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                pass\n\n                return {'arg': 1}\n\n            pytest._fillfuncargs(request)\n        \"\"\"\n        )\n        with pytest.warns(deprecated.FILLFUNCARGS):\n            testdir.runpytest()\n", "    def test_deprecated_fscollector_gethookproxy_isinitpath(self, testdir):\n        testdir.makepyfile(\"\"\"\n                pass\n        \"\"\")\n        config = Config.fromdict(\n            {'args': ['test_something.py'],}, pluginmanager=testdir.getpluginmanager()\n        )\n        collector = nodes.Collector.from_config(config)\n        with pytest.raises(deprecated.PytestDeprecationWarning) as w:\n            collector.isinitpath()\n        assert (\n            str(w.value)\n            == \"The gethookproxy() and isinitpath() methods of FSCollector and Package are deprecated; \"\n            \"use self.session.gethookproxy() and self.session.isinitpath() instead. \"\n        )\n", "    def test_deprecated_pytester_collect_module(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n        \"\"\"\n        )\n        with warnings.catch_warnings(record=True) as w:\n            testdir.runpytest(\"--collect-only\")\n        assert len(w) == 1\n        assert isinstance(w[0].message, deprecated.PYTEST_COLLECT_MODULE)\n\n"], "sample_718": ["    def test_no_sample_weight_pandas_series_type(self):\n        # Check if estimator correctly handles sample_weight with pandas Series\n        estimator = NoSampleWeightPandasSeriesType()\n        # Generate dummy data\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        # Import pandas DataFrame and Series\n        import pandas as pd\n        # Create a pandas Series for sample_weight\n        sample_weight = pd.Series([1, 2, 3])\n        assert_raises_regex(ValueError, \"Estimator does not accept 'sample_weight'\",\n                            estimator.fit, X, y, sample_weight=sample_weight)\n", "    def test_no_sample_weight_pandas_series(self):\n        # Test that estimator raises ValueError when sample_weight\n        # is pandas Series.\n        estimator = NoSampleWeightPandasSeriesType()\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([1, 0])\n        import pandas as pd\n        sample_weight = pd.Series([1, 2])\n        assert_raises_regex(ValueError,\n                               \"Estimator does not accept 'sample_weight'\"\n                               \" of type pandas.Series\",\n                               estimator.fit, X, y, sample_weight=sample_weight)\n", "    def test_not_invariant_predict(self):\n        estimator = NotInvariantPredict()\n        X = np.array([[1], [2], [3]])\n        y = estimator.predict(X)\n        assert_array_equal(y, np.ones(3))\n        X = np.array([1])\n        y = estimator.predict(X)\n        assert_array_equal(y, np.zeros(1))\n\n\n", "    def test_check_estimator_not_invariant_predict(self):\n        # Test if estimator's predict method is invariant to input shape changes\n        # while maintaining the same number of samples.\n\n        estimator = NotInvariantPredict()\n        X_test = np.arange(4).reshape((2, 2))  \n\n        check_estimator(estimator)\n        # Fit the estimator\n        estimator.fit(X_test, np.zeros(2))\n        # Predict on the original input shape\n        pred1 = estimator.predict(X_test)\n\n        # Reshape the input while keeping the number of samples the same\n        X_test_reshaped = X_test.reshape((4, 1)) \n\n        # Predict on the reshaped input\n        pred2 = estimator.predict(X_test_reshaped)\n\n        # Assert that predictions are the same regardless of input shape\n        assert_array_equal(pred1, pred2)\n", "    def test_not_invariant_predict(self):\n        msg = (\"Predict method is not invariant to the \"\n               \"number of samples.\")\n        estimator = NotInvariantPredict()\n        X = np.array([1, 2, 3])\n        y = np.array([0, 1, 0])\n        estimator.fit(X.reshape(-1, 1), y)\n        pred1 = estimator.predict(X.reshape(1, -1))\n        pred2 = estimator.predict(X.reshape(-1, 1))\n        assert_not_equal(pred1[0], pred2[0], msg)\n\n"], "sample_1137": ["    def test_convert_to_zero_dimension():\n        assert convert_to(Quantity(1, dimension=Dimension()),\n                         kilogram) == Quantity(1)\n\n", "def test_convert_to_quantity_simplify():\n    x = Quantity(\"x\", 10*meter)\n    y = Quantity(\"y\", 100*centimeter)\n    z = Quantity(\"z\", 1*kilometer)\n    expr = x + y + z\n    result = convert_to(expr, meter)\n    assert result.free_symbols == {Symbol('x'), Symbol('y'), Symbol('z')}\n    assert quantity_simplify(result) == 1110*meter\n", "    def test_unit_conversion_with_symbols():\n        x = Symbol('x')\n        expr = x*meter\n        res = convert_to(expr, kilometer)\n        assert not isinstance(res, Quantity)\n        assert res.subs(x, 1000) == 1*kilometer\n", "def test_convert_to_multiple_units_with_powers():\n    from sympy.physics.units import meter, second\n    assert convert_to(meter**2/second**3, [meter, second]) == meter**2/second**3\n", "def test_convert_to_dimensionless_constant():\n    x = Symbol('x')\n    assert convert_to(x * 2, dimensionless) == 2 * x * dimensionless  \n"], "sample_648": ["    def test_mark_parametrize_mixed_types_in_ids(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\n                \"input, expected\",\n                [\n                    (1, 1),\n                    (\"a\", \"a\"),\n                    (None, None),\n                ],\n                ids=[\"int\", \"str\", \"None\"],\n            )\n                assert input == expected\n        \"\"\"\n        )\n        result = pytester.runpytest(p)\n        assert result.ret == 0\n        assert result.stdout.str().count(\"int\") == 1\n        assert result.stdout.str().count(\"str\") == 1\n        assert result.stdout.str().count(\"None\") == 1\n", "    def test_mark_parametrize_empty_parameterset_skip(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"arg\", [])\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == ExitCode.SKIPPED\n        assert result.stdout.lines == [\n            \"collected 1 item\",\n            \"test_empty_parameterset.py F\",\n            \"============================= test session starts ========================\",\n            f\"test_empty_parameterset.py::test_empty_parameterset[arg] SKIPPED {os.getcwd()}/{pytester.path}\",\n            \"got empty parameter set ['arg'], function test_empty_parameterset at {}:3\".format(\n                pytester.path\n            ),\n            \"=========================== 1 skipped in 0.01s ====================\",\n        ]\n", "                    def test_mark_parametrize_empty_set_xfail(\n                        self, pytester: Pytester, monkeypatch: Optional[mock.Mock]", "    def test_mark_parametrize_for_single_value(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [1])\n                assert x == 1\n        \"\"\"\n        )\n        result = pytester.runpytest(p)\n        result.assert_outcomes(passed=1)\n", "    def test_mark_parametrize_empty_set_raises_error(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"x\", [])\n                pass\n            \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == ExitCode.INTERNAL_ERROR\n        assert result.stdout.lines[-1].startswith(\"Empty parameter set\")\n"], "sample_42": ["    def test_brightness_temperature_beam_area():\n        freq = 5 * u.GHz\n        beam_area = 2 * np.pi * (10 * u.arcsec)**2\n        equiv = u.brightness_temperature(freq, beam_area=beam_area)\n\n        Jy_beam = 1 * u.Jy / beam_area\n        K = Jy_beam.to(u.K, equivalencies=equiv)\n\n        assert_quantity_allclose(K, 3.526295144567176 * u.K)\n", "    def test_brightness_temperature_beam():\n        freq = 5 * u.GHz\n        beam_sigma = 50 * u.arcsec\n        beam_area = 2 * np.pi * (beam_sigma)**2\n        equiv = u.brightness_temperature(freq, beam_area)\n\n        jy_per_beam = 1 * u.Jy / beam_area\n\n        k = jy_per_beam.to(u.K, equivalencies=equiv)\n        assert_quantity_allclose(k, 3.526295144567176 * u.K)\n", "    def test_brightness_temperature_beam_area():\n        freq = 5 * u.GHz\n        beam = 50 * u.arcsec\n\n        equiv = u.brightness_temperature(freq, beam_area=2*np.pi*(beam)**2)\n        assert_allclose((1 * u.Jy/u.sr).to(u.K, equivalencies=equiv),\n                        3.526295144567176 * u.K)\n", "    def test_brightness_temperature_beam_area_roundtrip():\n        freq = 1.4 * u.GHz\n        beam_sigma = 10 * u.arcsec\n        beam_area = 2 * np.pi * (beam_sigma)**2\n        equiv = u.brightness_temperature(freq, beam_area)\n        intensity = 1 * u.Jy / beam_area\n        temperature = intensity.to(u.K, equivalencies=equiv)\n        assert_quantity_allclose(temperature, (intensity / beam_area).to(u.K,\n                                                                         equivalencies=equiv))\n", "    def test_brightness_temperature_beam():\n        freq = 5 * u.GHz\n        beam_area = 2 * np.pi * (15 * u.arcsec)**2\n        equiv = u.brightness_temperature(freq, beam_area)\n        assert_quantity_allclose((1 * u.Jy / beam_area).to(u.K, equivalencies=equiv),\n                                217.2658703625732 * u.K,\n                                rtol=1e-10)\n\n"], "sample_871": ["    def test_silhouette_samples_sparse_input(self):\n        X, y = datasets.make_blobs(n_samples=50, centers=3, random_state=0)\n        X_sparse = csr_matrix(X)\n        assert_allclose(silhouette_samples(X_sparse, y), silhouette_samples(X, y))\n\n", "def test_silhouette_samples_sparse_input():\n    # test silhouette_samples with sparse matrix input\n    X, y = datasets.make_blobs(n_samples=50, centers=3, random_state=0)\n    X_sparse = csr_matrix(X)\n    sil = silhouette_samples(X_sparse, y)\n    sil_dense = silhouette_samples(X, y)\n    assert_allclose(sil, sil_dense)\n\n", "    def test_silhouette_samples_empty_cluster(\n        self,", "    def test_silhouette_samples_sparse_input(self):\n        # Test silhouette_samples with sparse input\n        X, y = datasets.make_blobs(n_samples=20, centers=3, random_state=0)\n        X_sparse = csr_matrix(X)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=DeprecationWarning)\n            silhouette = silhouette_samples(\n                X_sparse, y, metric=\"euclidean\"\n            )\n\n        assert_allclose(silhouette, silhouette_samples(X, y, metric=\"euclidean\"))\n", "    def test_silhouette_samples_sparse_input():\n        # Test silhouette_samples with a sparse input and CSR format.\n        X, y = datasets.make_blobs(n_samples=100, random_state=42)\n        X_sparse = csr_matrix(X)\n        labels = y.copy()\n\n        # Ensure that the sparse version produces the same results as the dense version.\n        silhouette_dense = silhouette_samples(X, labels, metric='euclidean')\n        silhouette_sparse = silhouette_samples(X_sparse, labels, metric='euclidean')\n        assert_allclose(silhouette_dense, silhouette_sparse)\n\n"], "sample_395": ["    def test_watch_for_template_changes(self):\n        autoreload.autoreload_started()\n        expected_directories = [EXTRA_TEMPLATES_DIR]\n        with mock.patch(\"django.template.autoreload.get_template_directories\") as mocked_get_template_directories:\n            mocked_get_template_directories.return_value = expected_directories\n            watch_for_template_changes(autoreload, **{})\n            assert autoreload.watched_dirs == expected_directories\n", "    def test_watch_for_template_changes(self):\n        sender = mock.Mock()\n        watch_for_template_changes(sender)\n        sender.watch_dir.assert_called_once_with(EXTRA_TEMPLATES_DIR, \"**/*\")\n", "    def test_watch_for_template_changes_calls_watch_dir(self, mock_get_template_directories, mock_reset_loaders):\n        mock_get_template_directories.return_value = [EXTRA_TEMPLATES_DIR]\n        autoreload.autoreload_started()\n        mock_get_template_directories.assert_called_once()\n        mock_reset_loaders.assert_not_called()\n", "    def test_watch_for_template_changes(self):\n        mock_watch_dir = mock.MagicMock()\n        with mock.patch(\"django.template.autoreload.autoreload_started\") as mock_signal, \\\n                mock.patch(\"django.template.autoreload.watch_for_template_changes.sender\", return_value=mock_watch_dir):\n            mock_signal.dispatch(sender=mock_watch_dir)\n            mock_watch_dir.assert_called_with(EXTRA_TEMPLATES_DIR, \"**/*\")\n", "    def test_watch_for_template_changes_calls_watch_dir(self, mock_get_template_directories, mock_reset_loaders):\n        mock_get_template_directories.return_value = {EXTRA_TEMPLATES_DIR}\n        sender = mock.Mock()\n        autoreload.watch_for_template_changes(sender)\n        sender.watch_dir.assert_called_once_with(EXTRA_TEMPLATES_DIR, \"**/*\")\n"], "sample_326": ["    def test_urlize_trim_url_limit(self):\n        text = \"Visit https://www.djangoproject.com/ for more information. \" \\\n               \"And also http://example.example.example.example.\"\n        self.check_output(\n            urlize, text, trim_url_limit=25,\n            output=\"Visit <a href=\\\"https://www.djangoproject.com/\\\">https://www.djangoproject.com/</a> for more information. And also <a href=\\\"http://example.example.example.example.\\\">http://example.example.example...</a>.\"\n        )\n", "    def test_urlize_trim_url_limit(self):\n        self.check_output(urlize, 'https://www.example.com/very/long/url',\n                          '<a href=\"https://www.example.com/very/long/url\">https://www.example.com/very/long/url</a>')\n        self.check_output(urlize, 'https://www.example.com/very/long/url',\n                          '<a href=\"https://www.example.com/very/long/url\">https://www.example.com/very/long/url</a>',\n                          trim_url_limit=None)\n        self.check_output(urlize, 'https://www.example.com/very/long/url',\n                          '<a href=\"https://www.example.com/very/long/url\">https://www.example.com/very/lo\u2026</a>',\n                          trim_url_limit=20)\n\n\n\n", "    def test_urlize_nofollow(self):\n        # Test if nofollow attribute added correctly.\n        text = 'link: http://example.com'\n        output = '<a href=\"http://example.com\" rel=\"nofollow\">link: http://example.com</a>'\n        self.assertEqual(urlize(text, nofollow=True), output)\n", "    def test_urlize_trim_url_limit(self):\n        text = 'This is a http://verylongwebsiteaddress.com with a long URL.'\n        expected = 'This is a <a href=\"http://verylongwebsiteaddress.com\">http://verylongwebsiteaddress.com</a> with a long URL.'\n        self.check_output(urlize, text, expected)\n\n        text = 'This is a http://verylongwebsiteaddress.com with a long URL.'\n        expected = 'This is a <a href=\"http://verylongwebsiteaddress.com\">http://verylongwebsiteaddress.com\u2026</a> with a long URL.'\n        self.check_output(lambda t: urlize(t, trim_url_limit=30), text, expected)\n", "    def test_js_escape(self):\n        self.assertEqual(escapejs(\"'\"), '\\\\u0027')\n        self.assertEqual(escapejs('\"'), '\\\\u0022')\n        self.assertEqual(escapejs('>\\'\\\"'), '\\\\u003E\\\\u0027\\\\u0022')\n        self.assertEqual(escapejs('<&\\`'), '\\\\u003C\\\\u0026\\\\u0060')\n"], "sample_407": ["compilation error", "    def test_foreign_key_relation_one_to_many_reverse(self):\n        \"\"\"\n        Check if the reverse relationship works correctly with ForeignKey.\n        \"\"\"\n        articles = Article.objects.filter(reporter=self.r)\n        self.assertEqual(len(articles), 1)\n        self.assertEqual(articles[0], self.a)\n\n", "    def test_related_object_deletion(self):\n        count = Article.objects.count()\n        self.a.delete()\n        self.assertEqual(Article.objects.count(), count - 1)\n        with self.assertRaises(Article.DoesNotExist):\n            Article.objects.get(pk=self.a.pk)\n\n", "    def test_related_object_access(self):\n        self.assertEqual(self.a.reporter.first_name, \"John\")\n        self.assertEqual(\n            self.r.article_set.all()[0].headline, \"This is a test\"\n        )\n", "def test_get_or_create_with_defaults(self):\n    with transaction.atomic():\n        obj, created = Article.objects.get_or_create(\n            headline=\"This is a test\", pub_date=datetime.date(2005, 7, 27), defaults={\"reporter\": self.r2}\n        )\n        self.assertEqual(obj.headline, \"This is a test\")\n\n        obj, created = Article.objects.get_or_create(\n            headline=\"This is another test\", pub_date=datetime.date(2005, 7, 28), defaults={\"reporter\": self.r2}\n        )\n        self.assertEqual(obj.headline, \"This is another test\")\n\n"], "sample_524": ["def test_colorbar_extension_length():\n    _colorbar_extension_length(spacing='uniform')\n    _colorbar_extension_length(spacing='proportional')\n\n", "def test_colorbar_extension_label(spacing):\n    cmap, norms = _get_cmap_norms()\n    fig, axs = plt.subplots(2, 2)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        ax = axs[i // 2, i % 2]\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n        Colorbar(ax, cmap=cmap, norm=norm, boundaries=boundaries,\n                 values=values, extend=extension_type,\n                 extendrect=True, spacing=spacing,\n                 label=\"Test Label\")\n        ax.set_title(extension_type)\n    return fig\n", "def test_colorbar_extension_ticks():\n    \"\"\"\n    Test tick placement on colorbars with extensions.\n    \"\"\"\n    cmap, norms = _get_cmap_norms()\n    fig, axes = plt.subplots(2, 2)\n    for i, extension_type in enumerate(('neither', 'min', 'max', 'both')):\n        norm = norms[extension_type]\n        boundaries = values = norm.boundaries\n        values = values[:-1]\n\n        cax = axes.flat[i]\n        cb = Colorbar(cax, cmap=cmap, norm=norm,\n                      boundaries=boundaries, values=values,\n                      extend=extension_type, extendrect=True,\n                      orientation='vertical')\n        cax.set_ylabel(extension_type)\n    plt.tight_layout()\n    \n\n", "def test_colorbar_extension_length():\n    return _colorbar_extension_length('uniform')\n", "    def test_colorbar_extension_length_auto(self):\n        # Create colorbars with extendfrac='auto' and check that the\n        # extension length is automatically determined.\n        fig = _colorbar_extension_length('proportional')\n        for i, ax in enumerate(fig.axes):\n            if i % 3 == 1:  # Check 'auto' extendfrac\n                # Get the colorbar object.\n                colorbar = ax.get_children()[0]\n                # Check if the extension length is within a reasonable range.\n                assert 0.05 < colorbar.extendfrac < 0.25\n\n"], "sample_443": ["    def test_cull(self):\n        cache = caches[\"default\"]\n        cache.clear()\n        for i in range(cache._cull_frequency * 2):\n            cache.set(str(i), i)\n        self.assertEqual(len(cache._list_cache_files()), cache._cull_frequency * 2)\n        cache._cull()\n        self.assertLess(len(cache._list_cache_files()), cache._cull_frequency * 2)\n\n", "    def test_file_based_cache_cull(self):\n        cache = caches['default']\n        cache._max_entries = 10\n        cache._cull_frequency = 2\n        # Add enough entries to trigger culling\n        for i in range(20):\n            cache.set(f'key_{i}', i)\n        self.assertEqual(len(cache._list_cache_files()), 10)\n", "    def test_file_based_cache_cull(self):\n        cache_dir = tempfile.mkdtemp()\n        try:\n            with override_settings(CACHES={\n                \"default\": {\n                    \"BACKEND\": \"django.core.cache.backends.filebased.FileBasedCache\",\n                    \"LOCATION\": cache_dir,\n                    \"MAX_ENTRIES\": 10,\n                    \"CULL_FREQUENCY\": 2\n                }\n            }):\n                cache.set(\"key_1\", \"value_1\")\n                cache.set(\"key_2\", \"value_2\")\n                cache.set(\"key_3\", \"value_3\")\n                cache.set(\"key_4\", \"value_4\")\n                cache.set(\"key_5\", \"value_5\")\n                cache.set(\"key_6\", \"value_6\")\n                cache.set(\"key_7\", \"value_7\")\n                cache.set(\"key_8\", \"value_8\")\n                cache.set(\"key_9\", \"value_9\")\n                cache.set(\"key_10\", \"value_10\")\n                cache.set(\"key_11\", \"value_11\")\n\n                self.assertEqual(len(os.listdir(cache_dir)), 10)\n                time.sleep(1)\n                cache.get(\"key_1\")\n                cache.get(\"key_2\")\n                self.assertEqual(len(os.listdir(cache_dir)), 10)\n\n                # The cache should be culled\n                cache.set(\"key_12\", \"value_12\")\n                self.assertEqual(len(os.listdir(cache_dir)), 10)\n        finally:\n            shutil.rmtree(cache_dir)\n\n\n\n", "    def test_file_based_cache_cull(self):\n        self.cache = caches['default']\n        self.cache._max_entries = 10\n        self.cache._cull_frequency = 2\n        for i in range(20):\n            self.cache.set(f\"key_{i}\", i)\n        self.assertEqual(len(self.cache._list_cache_files()), 10)\n", "    def test_file_based_cache_cull(self):\n        cache = caches[\"default\"]\n        self.assertEqual(cache.__class__.__name__, \"FileBasedCache\")\n        cache._max_entries = 5  # Set a low max_entries\n        cache._cull_frequency = 2  # Cull every other entry\n\n        # Add some entries\n        for i in range(10):\n            cache.set(f\"key_{i}\", f\"value_{i}\")\n\n        # Verify entries are present\n        for i in range(10):\n            self.assertEqual(cache.get(f\"key_{i}\"), f\"value_{i}\")\n\n        # Cull the cache\n        cache._cull()\n\n        # Only about half the entries should remain\n        self.assertTrue(len(cache._list_cache_files()) < 5)\n\n        # Verify some entries are still present\n        for i in range(5):\n            self.assertEqual(cache.get(f\"key_{i}\"), f\"value_{i}\")\n"], "sample_265": ["    def test_get_templatetag_libraries_with_custom_libraries(self):\n        custom_libraries = {'my_custom_library': 'my_app.templatetags.my_custom_library'}\n        engine = self.engine_class({'OPTIONS': {'libraries': custom_libraries}})\n        libraries = engine.get_templatetag_libraries(custom_libraries)\n        self.assertIn('my_custom_library', libraries)\n", "    def test_get_installed_libraries(self):\n        # Ensure built-in and dummy app libraries are found\n        libraries = get_installed_libraries()\n        self.assertIn('i18n', libraries)\n\n        # Verify a library from a dummy app\n        self.assertIn('test_dummy', libraries)\n", "    def test_get_templatetag_libraries(self):\n        app_config = apps.get_app_config('template_tests')\n        self.assertIn(\n            'template_tests.templatetags.test_tags',\n            self.engine.get_templatetag_libraries({}),\n\n        )\n\n\n\n", "    def test_get_installed_libraries(self):\n        # Test that get_installed_libraries correctly identifies\n        # templatetag libraries from installed apps.\n        self.assertIn('test_dummy', DjangoTemplates.get_installed_libraries())\n", "    def test_get_installed_libraries(self):\n        with override_settings(INSTALLED_APPS=['template_tests.test_app']):\n            engines = DjangoTemplates({'OPTIONS': {}})\n            libraries = engines.get_templatetag_libraries({})\n            self.assertIn('test_app', libraries)\n\n"], "sample_802": ["    def test_pipeline_deprecation_warning(self):\n        # Test that a warning is raised when calling deprecated methods.\n        with pytest.warns(FutureWarning) as record:\n            Pipeline()\n        assert \"The 'steps' parameter is deprecated\" in str(record[0].message)\n\n\n", "    def test_pipeline_memory_fit_transform(self):\n        # Test that fit_transform cache works for pipelines\n        X = np.random.randn(5, 3)\n        y = np.random.randn(5)\n\n        clf = Pipeline([\n            ('scaler', StandardScaler()),\n            ('clf', DummyEstimatorParams())\n        ])\n\n        # Use memory to cache the fit_transform result\n        memory = Memory(location=mkdtemp(), verbose=0)\n        clf_memory = clone(clf)\n        clf_memory.set_params(memory=memory)\n\n        # First fit_transform\n        clf_memory.fit_transform(X, y)\n        # Check that the cache exists\n        assert len(list(memory.cache.keys())) == 1\n\n        # Second fit_transform should be cached\n        clf_memory.fit_transform(X, y)\n\n        # Check that the fit_transform result is the same\n        assert_array_almost_equal(\n            clf_memory.fit_transform(X, y), clf_memory.fit_transform(X, y))\n\n        shutil.rmtree(memory.location)\n\n", "    def test_fit_predict_with_joblib_caching(self):\n        # Check that fit_predict works with joblib caching\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([0, 1, 0])\n        memory = Memory(location=mkdtemp())  \n    \n        pipeline = Pipeline([\n            ('clf', FitParamT())\n        ], memory=memory)\n        pipeline.fit_predict(X, y, should_succeed=True)\n        # Second call should read from cache\n        first_timestamp = pipeline.named_steps['clf'].timestamp_\n        pipeline.fit_predict(X, y, should_succeed=True)\n        second_timestamp = pipeline.named_steps['clf'].timestamp_\n        assert first_timestamp == second_timestamp\n        shutil.rmtree(memory.cachedir)\n", "    def test_pipeline_memory_cache(self):\n        # Test that the cached pipeline is reused\n        X = np.random.randn(10, 5)\n        y = np.random.randn(10)\n\n        # Create a pipeline that uses a transformer with memory\n        pipeline = Pipeline(\n            [\n                (\"transformer\", DummyTransf()),\n                (\"estimator\", DummyEstimatorParams()),\n            ],\n            memory=Memory(location=mkdtemp(), verbose=0),\n        )\n\n        # Fit the pipeline twice\n        pipeline.fit(X, y)\n        timestamp1 = pipeline.named_steps[\"transformer\"].timestamp_\n\n        pipeline.fit(X, y)\n        timestamp2 = pipeline.named_steps[\"transformer\"].timestamp_\n\n        assert timestamp1 == timestamp2\n\n        # Delete the cache\n        shutil.rmtree(pipeline.memory.location)\n\n        # Fit the pipeline again, a new fit should occur\n        pipeline.fit(X, y)\n        timestamp3 = pipeline.named_steps[\"transformer\"].timestamp_\n\n        assert timestamp1 != timestamp3\n\n", "    def test_pipeline_memory_cache_file(self, tmpdir):\n        # check if pipeline can cache transformers with a file backed memory\n        memory = Memory(location=tmpdir)\n        clf = Pipeline([\n            ('scaler', StandardScaler()),\n            ('clf', SVC())\n        ], memory=memory)\n        X = np.random.randn(10, 3)\n        y = np.random.randint(2, size=10)\n\n        # first fit should trigger training\n        clf.fit(X, y)\n        assert (tmpdir / 'cachedir' /\n                'scaler.StandardScaler.pkl').exists()\n\n        # second fit should load from cache\n        clf.fit(X, y)\n        assert (tmpdir / 'cachedir' /\n                'scaler.StandardScaler.pkl').exists()\n"], "sample_744": ["    def test_categorical_encoder_transform_unknown_dense(self):\n        # Test transform with unknown categories in 'ignore' mode\n        enc = CategoricalEncoder(handle_unknown='ignore',\n                                encoding='onehot-dense')\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc.fit(X)\n\n        X_test = [['Female', 1], ['Male', 4]]\n        X_trans = enc.transform(X_test)\n        expected = np.array([[1., 0., 1., 0., 0.],\n                             [0., 1., 0., 0., 0.]])\n        assert_array_equal(X_trans, expected)\n\n", "def test_quantile_transformer_nquantiles():\n    # Test different n_quantiles values\n    for n_quantiles in [2, 10, 100, 1000]:\n        # Generate some test data\n        X = np.array([[-1, 0, 1], [0, 1, 2], [1, 2, 3]]).T\n\n        # Create and fit the transformer\n        transformer = QuantileTransformer(n_quantiles=n_quantiles,\n                                           random_state=0)\n        transformer.fit(X)\n\n        # Transform the data\n        X_trans = transformer.transform(X)\n\n        # Check that the transformed data has the correct shape\n        assert X_trans.shape == X.shape\n\n        # Check that the transformed data has the correct number of quantiles\n        assert_equal(np.unique(X_trans).size, n_quantiles)\n", "    def test_quantile_transform_ignore_zeros(self):\n        X = np.array([[0, 1], [1, 0], [1, 1]])\n        transformer = QuantileTransformer(output_distribution='normal',\n                                          ignore_zeros=True)\n        X_transformed = transformer.fit_transform(X)\n        assert_array_almost_equal(X_transformed[0],\n                                    transformer.transform([0, 0]))\n\n", "    def test_quantile_transform_sparse(self):\n        X = sparse.csr_matrix(\n            [[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6]]\n        )\n        transformer = QuantileTransformer(n_quantiles=3, random_state=0,\n                                          output_distribution='uniform')\n        X_trans = transformer.fit_transform(X)\n        assert_almost_equal(X_trans.toarray(),\n                            np.array([[0.0, 0.5, 0.5],\n                                      [0.5, 0.5, 1.0],\n                                      [1.0, 1.0, 1.0],\n                                      [1.0, 1.0, 1.0]]))\n", "    def test_quantile_transform_sparse(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4], [5, 6]])\n        qt = QuantileTransformer()\n        Xt = qt.fit_transform(X)\n        assert_sparse(Xt)\n        for col in range(Xt.shape[1]):\n            assert_almost_equal(np.unique(Xt[:, col].A1),\n                                np.linspace(0, 1, num=9))\n"], "sample_359": ["    def test_add_field_foreign_key_constraint(self):\n        # Test adding a field with a foreign key constraint\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        with atomic():\n            Author.objects.create(name='Jane Austen')\n            Book.objects.create(title='Pride and Prejudice')\n\n        # Add a foreign key constraint to the Book model\n        operation = AddField(\n            model_name='book',\n            name='author',\n            field=models.ForeignKey(Author, on_delete=models.CASCADE),\n        )\n\n        self.assertOperation(operation, [\n            (\"ALTER TABLE `book` ADD COLUMN `author_id` INT NULL\", [], [])\n\n        ])\n\n        # Apply the migration\n        self._apply_operations_to_schema([operation])\n        \n\n        # Add a database constraint after adding the field\n        operation = AddConstraint(\n            model_name='book',\n            constraint=models.ForeignKeyConstraint(fields=['author'], name='author_id_refs_id_5a455731'),\n        )\n        self.assertOperation(operation, [\n            ('ALTER TABLE `book` ADD CONSTRAINT `author_id_refs_id_5a455731` FOREIGN KEY (`author_id`) REFERENCES `autho` (`id`)', [], [])\n\n        ])\n        # Apply the migration\n        self._apply_operations_to_schema([operation])\n\n", "    def test_alter_field_null(self):\n        # Make a model with a CharField\n        class MyModel(models.Model):\n            name = models.CharField(max_length=100)\n            nullable_field = models.CharField(max_length=100, null=False)\n\n        with atomic(using=self.db_name):\n            # Make a migration that makes the field nullable\n            state = ProjectState.from_apps(self.apps)\n            model_state = state.models[self.app_label, 'mymodel']\n            field = model_state.fields['nullable_field']\n            field._kwargs['null'] = True\n            operation = AlterField(\n                model_name='mymodel',\n                name='nullable_field',\n                field=field,\n            )\n            migration = Migration(\n                'test_alter_field_null',\n                [operation],\n            )\n            # Test its state\n            new_state = migration.apply(state)\n            self.assertEqual(new_state.models[self.app_label, 'mymodel'].fields['nullable_field'].null, True)\n            # Test the database\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT NOT NULL FROM django_migrations_mymodel WHERE column_name = 'nullable_field'\")\n                not_null_value, = cursor.fetchone()\n                self.assertFalse(not_null_value)\n\n            # Test backwards\n            previous_state = ProjectState.from_apps(self.apps)\n            migration = Migration(\n                'test_alter_field_null_backwards',\n                [operation.reverse()],\n            )\n            # Test its state\n            new_state = migration.apply(previous_state)\n            self.assertEqual(new_state.models[self.app_label, 'mymodel'].fields['nullable_field'].null, False)\n            # Test the database\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT NOT NULL FROM django_migrations_mymodel WHERE column_name = 'nullable_field'\")\n                not_null_value, = cursor.fetchone()\n                self.assertTrue(not_", "    def test_add_constraint(self):\n        with connection.schema_editor() as editor:\n            editor.create_model(\n                \"test_app\",\n                \"ModelWithConstraint\",\n                [\n                    ('id', models.AutoField(primary_key=True)),\n                    ('name', models.CharField(max_length=10)),\n                ],\n\n            )\n        with atomic(using='test_app'):\n           # Add a check constraint\n            model_state = ProjectState.from_apps(self.apps).models['test_app', 'ModelWithConstraint']\n            constraint = migrations.CheckConstraint(\n                check=models.Q(name__startswith='a'), name='startswith_a'\n            )\n            model_state.add_constraint(constraint)\n            migration_name = '0001_initial'\n            operation = AddConstraint('ModelWithConstraint', constraint)\n            migration = Migration(\n                'test_app', migration_name,\n                operations=[operation],\n                # ...\n            )\n            # ...\n            \n            with CaptureQueriesContext(connection) as captured_queries:\n\n\n                migration.apply(\n                    ProjectState.from_apps(self.apps),\n                    database='test_app'\n                )\n\n\n\n            self.assertEqual(\n                len(captured_queries), 2 ,\n\n                f'Expected 2 query, got {len(captured_queries)}',\n            )\n            # Check constraint in database\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"\"\"\n                  SELECT CONSTRAINT_NAME FROM information_schema.TABLE_CONSTRAINTS\n                  WHERE TABLE_NAME = 'test_app_modelwithconstraint'\n                  AND CONSTRAINT_TYPE = 'CHECK';\n                \"\"\"\n                )\n                self.assertEqual([row[0] for row in cursor.fetchall()], ['startswith_a'])\n\n\n\n", "    def test_alter_order_with_respect_to(self):\n        with self.atomic_migration_test_case() as testname:\n            # Create the model\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"CREATE TABLE test_alter_order_with_respect_to (\"\n                    \" id INTEGER PRIMARY KEY, name VARCHAR(100) NOT NULL, \"\n                    \"order_ INTEGER)\"\n                )\n            with transaction.atomic():\n                class Article(models.Model):\n                    name = models.CharField(max_length=100)\n                    order_ = models.PositiveIntegerField(default=0)\n\n            # Apply AlterOrderWithRespectTo operation\n            state_before = self.get_project_state()\n            AlterOrderWithRespectTo('Article', 'name').state_forwards(\n                'app', state_before\n            )\n            # Check state change\n            self.assertListEqual(\n                [f.name for f in state_before.models['app', 'article'].fields],\n                ['name', 'order_'],\n            )\n            operation = AlterOrderWithRespectTo('Article', 'name')\n            migrations.Migration(\"alter_order_with_respect_to\", [operation]).apply(\n                'app',\n                state_before,\n            )\n            # Check database change\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"SELECT name, order_ FROM test_alter_order_with_respect_to\"\n                )\n                result = cursor.fetchall()\n                self.assertEqual(len(result), 0)\n\n            # Apply AlterOrderWithRespectTo operation to remove\n            state_after = self.get_project_state()\n            AlterOrderWithRespectTo('Article', None).state_forwards(\n                'app', state_after\n            )\n            # Check state change\n            self.assertListEqual(\n                [f.name for f in state_after.models['app', 'article'].fields],\n                ['name', 'order_'],\n            )\n            operation = AlterOrderWithRespectTo('Article', None)\n            migrations.Migration(\"alter_order_with_respect_to\", [operation]).apply(\n                'app',\n                ", "    def test_alter_unique_together(self):\n        with atomic(database=self.connection.alias):\n            before = ProjectState.from_apps(self.apps)\n            after = before.clone()\n            after.models['django_migrations.model'] = ModelState.from_model(\n                fields=[\n                    models.CharField('name', max_length=10, primary_key=True),\n                    models.CharField('category', max_length=10),\n                    models.CharField('description', max_length=255),\n                ],\n                unique_together=[('name', 'category')],\n            )\n            migration = Migration(\n                'test_alter_unique_together',\n                (\n                    AlterUniqueTogether(name='django_migrations.model', unique_together=[('name', 'category')]),\n                )\n            )\n            with self.assertRaises(IntegrityError):\n                with connection.cursor() as cursor:\n                    connection.run_migration(cursor, migration, before)\n            # Ensure the unique constraint is properly created.\n            cursor.execute(\n                \"SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name='django_migrations_model'\"\n            )\n            self.assertEqual(cursor.fetchone()[0], 1)\n\n"], "sample_263": ["    def test_dumpdata_with_natural_keys(self):\n        Site.objects.create(pk=1, domain='example.com', name='Example Site')\n        with NamedTemporaryFile(suffix='.json') as f:\n            management.call_command(\n                'dumpdata', 'sites', format='json', output=f.name,\n                use_natural_foreign_keys=True, use_natural_primary_keys=True,\n            )\n            f.seek(0)\n            loaded_data = json.load(f)\n        self.assertEqual(loaded_data[0]['fields'], {'domain': 'example.com', 'name': 'Example Site'})\n", "    def test_dumpdata_proxy_model_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            management.call_command(\n                'dumpdata', 'testapp', format='json', stdout=StringIO()\n            )\n            self.assertEqual(len(w), 1)\n            self.assertIsInstance(w[0].message, ProxyModelWarning)\n            self.assertEqual(str(w[0].message), 'ProxySpy is a proxy model and won\\'t be serialized.')\n", "    def test_dumpdata_proxy_model_not_serialized(self):\n        with warnings.catch_warnings(record=True) as warning_list:\n            call_command('dumpdata', 'testapp.proxymodel', format='json', stdout=StringIO())\n        self.assertEqual(len(warning_list), 1)\n        self.assertEqual(warning_list[0].category, ProxyModelWarning)", "    def test_dumpdata_with_proxy_model_warning(self):\n        # Test that a warning is issued when a proxy model is encountered.\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\", ProxyModelWarning)\n            management.call_command(\"dumpdata\", \"tests.proxy_spy\", format=\"json\")\n        self.assertEqual(len(w), 1)\n        self.assertIsInstance(w[0].message, ProxyModelWarning)\n", "    def test_dumpdata_natural_keys(self):\n        \"\"\"\n        Test that dumpdata handles models with natural keys correctly.\n        \"\"\"\n        # Create a model instance with a natural key\n        thing1 = NaturalKeyThing.objects.create(name='Thing 1')\n\n        # Dump the data\n        out = StringIO()\n        management.call_command(\n            'dumpdata', 'tests.naturalkeything', format='json', stdout=out,\n            use_natural_primary_keys=True,\n        )\n\n        # Load the dumped data\n        loaded_data = json.loads(out.getvalue())\n        self.assertEqual(len(loaded_data), 1)\n        self.assertEqual(loaded_data[0]['fields']['name'], 'Thing 1')\n"], "sample_177": ["    def test_get_related_models_recursive_with_fk_and_m2m(self):\n        class Product(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Order(models.Model):\n            product = models.ForeignKey(Product, on_delete=models.CASCADE)\n            items = models.ManyToManyField(Product)\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(Product))\n        state.add_model(ModelState.from_model(Order))\n        related_models = get_related_models_recursive(Order)\n        self.assertEqual(\n            related_models - {(Order._meta.app_label, Order._meta.model_name)},\n            {(Product._meta.app_label, Product._meta.model_name)},\n        )\n", "    def test_get_related_models_recursive_with_generic_relation(self):\n        class MyGenericModel(models.Model):\n            name = models.CharField(max_length=100)\n\n        class RelatedModel(models.Model):\n            content_type = models.ForeignKey(models.ContentType, on_delete=models.CASCADE)\n            object_id = models.PositiveIntegerField()\n            content_object = GenericForeignKey('content_type', 'object_id')\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(MyGenericModel))\n        state.add_model(ModelState.from_model(RelatedModel))\n        related_models = get_related_models_recursive(MyGenericModel)\n        self.assertIn((MyGenericModel._meta.app_label, MyGenericModel._meta.model_name), related_models)\n        self.assertIn((RelatedModel._meta.app_label, RelatedModel._meta.model_name), related_models)\n\n", "    def test_clone_with_dependencies(self):\n        with isolate_apps():\n            from migration_tests.models import Author, Book\n\n            project_state = ProjectState.from_apps(Apps())\n\n            # We need a state with dependencies to test the cloning process\n            author_state = ModelState.from_model(Author)\n            book_state = ModelState.from_model(Book)\n\n            project_state.add_model(author_state)\n            project_state.add_model(book_state)\n\n            cloned_state = project_state.clone()\n\n            self.assertEqual(cloned_state, project_state)\n            self.assertNotSame(cloned_state.models, project_state.models)\n            self.assertIsNotNone(cloned_state.models.get((Author._meta.app_label, 'author')))\n", "    def test_related_models_recursive_with_proxy(self):\n        # Regression test for #24513: Ensure proxy models get included\n        class Base(models.Model):\n            pass\n\n        class Concrete(Base):\n            pass\n\n        class Proxy(Concrete):\n            class Meta:\n                proxy = True\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(Base))\n        related_models = state._find_reload_model(\n            Base._meta.app_label, Base._meta.model_name, delay=False\n        )\n        self.assertIn((Base._meta.app_label, Base._meta.model_name.lower()), related_models)\n        self.assertIn((Concrete._meta.app_label, Concrete._meta.model_name.lower()), related_models)\n        self.assertIn((Proxy._meta.app_label, Proxy._meta.model_name.lower()), related_models)\n", "    def test_get_related_models_recursive(self):\n        class A(models.Model):\n            pass\n        class B(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n        class C(models.Model):\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n\n        state = ProjectState()\n        state.add_model(ModelState.from_model(A))\n        state.add_model(ModelState.from_model(B))\n        state.add_model(ModelState.from_model(C))\n\n        related_models = get_related_models_recursive(C)\n\n        self.assertEqual(set(related_models), {('app_label_c', 'c'), ('app_label_b', 'b'), ('app_label_a', 'a')})\n"], "sample_1181": ["    def test_ArrayDiagonal(self):\n        A = MatrixSymbol('A', 3, 3)\n        expr = ArrayDiagonal(A, (0, 1))\n        code = NumPyPrinter().doprint(expr)\n        expected = \"numpy.diagonal(A, 0, axis1=0, axis2=1)\"\n        assert code == expected\n\n", "    def test_print_ArrayDiagonal(self):\n        A = MatrixSymbol('A', 3, 3)\n        expr1 = ArrayDiagonal(A)\n        expr2 = ArrayDiagonal(A, (0, 1))\n        expr3 = ArrayDiagonal(A, (1, 2))\n\n        self.assertEqual(NumPyPrinter().doprint(expr1), \"numpy.diagonal(A, axis1=0, axis2=1)\")\n        self.assertEqual(NumPyPrinter().doprint(expr2), \"numpy.diagonal(A, axis1=0, axis2=1)\")\n        raises(NotImplementedError, lambda: NumPyPrinter().doprint(expr3))\n\n", "    def test_array_diagonal(self):\n        A = MatrixSymbol('A', 3, 3)\n        expr = ArrayDiagonal(A, (0, 1))\n        printer = NumPyPrinter()\n        code = printer.doprint(expr)\n        assert code == 'numpy.diagonal(A, 0, axis1=0, axis2=1)'\n", "def test_print_ArrayContraction():\n    A = Array([[1, 2], [3, 4]])\n    B = Array([[5, 6], [7, 8]])\n    expr = ArrayContraction(\n        ArrayTensorProduct(A, B),\n        contraction_indices=[\n            (1, 3),\n        ]\n    )\n    numpy_printer = NumPyPrinter()\n    assert numpy_printer.doprint(expr) == \"numpy.einsum('ij,jk->ik', array([[1, 2], [3, 4]]), array([[5, 6], [7, 8]]))\"\n", "def test_print_ArrayContraction():\n    A = Array([x, y], shape=(2,))\n    B = Array([a, b, c, d], shape=(2, 2))\n    expr = ArrayContraction(A, B, contraction_indices=[(0,)])\n    assert NumPyPrinter().doprint(expr) == \"numpy.einsum('ij,j->i', [A, B])\"\n"], "sample_435": ["    def test_create_user_with_username_containing_non_ascii_characters(self):\n        with translation.override(\"fr\"):\n            data = {\"username\": \"\u00fcser\", \"password1\": \"password\", \"password2\": \"password\"}\n            form = UserCreationForm(data=data)\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertEqual(user.username, \"\u00fcser\")\n", "    def test_password_hash_field(self):\n        form = UserCreationForm()\n        self.assertIsInstance(form.fields[\"password1\"], forms.CharField)\n        self.assertIsInstance(\n            form.fields[\"password2\"], forms.CharField\n        )\n        self.assertTrue(\n            isinstance(\n                form.fields[\"password1\"].widget, forms.PasswordInput\n            )\n        )\n", "    def test_creation_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL=\"tests.models.custom_user.CustomUser\"):\n            data = {\"username\": \"newuser\", \"password1\": \"password\", \"password2\": \"password\"}\n            form = UserCreationForm(data=data)\n            self.assertTrue(form.is_valid())\n\n            user = form.save()\n            self.assertEqual(user.username, \"newuser\")\n            self.assertTrue(user.check_password(\"password\"))\n            self.assertIsInstance(user, CustomUser)\n\n", "    def test_creation_with_custom_model(self):\n        \"\"\"\n        Test UserCreationForm with a custom user model.\n        \"\"\"\n        custom_user_class = CustomUser\n        custom_user_class.objects.create_user(\n            username=\"customuser\", password=\"password\", email=\"customuser@example.com\"\n        )\n        with self.settings(AUTH_USER_MODEL=custom_user_class):\n            form = UserCreationForm(data={\"username\": \"newuser\", \"password1\": \"password\", \"password2\": \"password\"})\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertIsInstance(user, custom_user_class)\n", "    def test_password_validation(self):\n        form = UserCreationForm(data={\"username\": \"newuser\", \"password\": \"password\"})\n        self.assertFalse(form.is_valid())\n        self.assertIn(\"password2\", form.errors)\n\n\n"], "sample_137": ["    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(body, \"some_metadata: some data\")\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n\n        self.assertEqual(title, 'Display an individual :model:`myapp.MyModel`.')\n        self.assertEqual(body, '\\n\\nDisplay an individual :model:`myapp.MyModel`.\\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n\\n:template:`myapp/my_template.html`')\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, 'Display an individual :model:`myapp.MyModel`.')\n        self.assertEqual(body, 'some_metadata: some data')\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(body, \"some_metadata: some data\")\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, 'Display an individual :model:`myapp.MyModel`.')\n        self.assertEqual(\n            body,\n            '**Context**\\n``RequestContext``\\n\\n``mymodel``\\nAn instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n:template:`myapp/my_template.html`'\n        )\n        self.assertEqual(metadata, {'some_metadata': 'some data'})\n"], "sample_957": ["    def test_restify_typing_special_form():\n        assert restify(typing.Any) == ':obj:`~typing.Any`'\n        assert restify(typing.NoReturn) == ':obj:`~typing.NoReturn`'\n        assert restify(typing.Union) == ':obj:`~typing.Union`'\n        assert restify(typing.Optional) == ':obj:`~typing.Optional`'\n\n\n", "def test_stringify_union_with_none():\n    assert stringify(Union[int, None]) == 'Optional[int]'\n    assert stringify(Union[None, int]) == 'Optional[int]'\n    assert stringify(Union[int, None, str]) == 'Optional[Union[int, str]]'\n    assert stringify(Union[str, int, None]) == 'Optional[Union[str, int]]'\n\n", "    def test_restify_py37_union_with_none():\n        assert restify(Union[int, None]) == 'Optional[int]'\n        assert restify(Union[None, int]) == 'Optional[int]'\n        assert restify(Union[str, None, int]) == 'Optional[Union[str, int]]'\n        assert restify(Union[int, Integral]) == 'Union[int, Integral]'\n", "def test_restify_union_none():\n    assert restify(Union[MyClass1, NoneType]) == 'Optional[MyClass1]'\n", "def test_restify_invalid_builtins():\n    assert restify(Struct) == ':class:`struct.Struct`'\n    assert restify(TracebackType) == ':class:`types.TracebackType`\n\n\n\n"], "sample_414": ["    def test_formfield_for_dbfield_choices(self):\n        # Test that choices are correctly applied to a CharField\n        ff = self.assertFormfield(\n            Company,\n            \"status\",\n            forms.widgets.Select,\n            fields=[\"status\"],\n        )\n        self.assertEqual(\n            ff.choices,\n            [\n                (\"draft\", \"Draft\"),\n                (\"published\", \"Published\"),\n                (\"archived\", \"Archived\"),\n            ],\n        )\n        ff = self.assertFormfield(Company, \"status\", forms.widgets.Select, editable=False)\n        self.assertEqual(ff.choices, [])\n\n", "    def test_formfield_for_dbfield_custom_widget(self):\n        \"\"\"\n        Test that custom widgets set in formfield_for_dbfield are used.\n        \"\"\"\n        class MyModelAdmin(admin.ModelAdmin):\n            formfield_overrides = {\n                CharField: {\"widget\": forms.Textarea},\n            }\n        ff = self.assertFormfield(\n            Company, \"name\", forms.Textarea, admin_overrides={\"formfield_overrides\": MyModelAdmin.formfield_overrides}\n        )\n", "    def test_formfield_for_dbfield_file_field_with_upload_to(self):\n        formfield = self.assertFormfield(\n            MyFileField, \"file\", widgets.FileInput,\n        )\n        self.assertEqual(formfield.widget.attrs[\"accept\"], \".txt\")\n", "    def test_formfield_for_dbfield_custom_widget(self):\n        class MyModelForm(forms.ModelForm):\n            class Meta:\n                model = Car\n                fields = \"__all__\"\n                widgets = {\n                    \"model\": forms.TextInput(attrs={\"placeholder\": \"Model name\"}),\n                }\n\n        class MyModelAdmin(admin.ModelAdmin):\n            form = MyModelForm\n\n        ma = MyModelAdmin(Car, admin.site)\n        ff = ma.formfield_for_dbfield(Car._meta.get_field(\"model\"), request=None)\n        self.assertIsInstance(ff.widget, forms.TextInput)\n        self.assertEqual(ff.widget.attrs[\"placeholder\"], \"Model name\")\n\n", "    def test_formfield_for_dbfield_UUIDField(self):\n        ff = self.assertFormfield(\n            MyFileField, \"file\", widgets.FileInput, formfield_callback=None\n        )\n        self.assertEqual(ff.required, False)\n        self.assertEqual(ff.label, \"File\")\n        self.assertEqual(ff.help_text, \"\")\n\n"], "sample_720": ["    def test_quantile_transform_axis_1(self):\n        # Test quantile_transform with axis=1\n        X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        Xt = quantile_transform(X, axis=1)\n        expected = np.array([[0, 0.5, 1], [0, 0.5, 1], [0, 0.5, 1]])\n        assert_array_almost_equal(Xt, expected)\n", "def test_polynomial_features_interaction_only():\n    X = np.array([[1, 2], [3, 4]])\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    X_poly = poly.fit_transform(X)\n    assert_array_equal(X_poly, np.array([[2, 4], [6, 8]]))\n", "    def test_categorical_encoder_inverse_transform_unknown(self):\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        enc = CategoricalEncoder(handle_unknown='ignore')\n        enc.fit(X)\n        X_t = [['Female', 4], ['Male', 1]]\n\n        X_transformed = enc.transform(X_t)\n        X_inverse_transformed = enc.inverse_transform(X_transformed)\n        assert_equal(X_inverse_transformed[0][1], None)  \n\n", "compilation error", "    def test_categorical_encoder_handle_unknown_error(self):\n        le = CategoricalEncoder(handle_unknown='error')\n        X = [['Male', 1], ['Female', 3], ['Female', 2]]\n        le.fit(X)\n        assert_raises(ValueError, le.transform, [['Female', 4]])\n"], "sample_828": ["    def test_pairwise_distances_chunked_empty_array():\n        X = np.array([])\n        for chunk in pairwise_distances_chunked(X):\n            assert_array_equal(chunk, np.array([]))\n\n", "    def test_pairwise_distances_chunked_sparse_float64():\n        n_samples = 10\n\n        indices = np.array([[0, 1], [1, 2], [2, 3], [3, 4], [4, 5],\n                            [5, 6], [6, 7], [7, 8], [8, 9]])\n        data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n        X = csr_matrix((data, (indices[:, 0], indices[:, 1])),\n                       shape=(n_samples, n_samples))\n\n        # Convert to float64 sparse matrix to ensure compatibility with\n        # pairwise_distances_chunked\n        X = X.astype('float64')\n\n        for chunk_size in [2, 3, 5]:\n            D_chunks = list(pairwise_distances_chunked(X, chunk_size=chunk_size,\n                                                      metric='euclidean'))\n\n            D = pairwise_distances(X, metric='euclidean')\n            D_combined = np.concatenate(D_chunks, axis=0)\n            assert_allclose(D, D_combined)\n", "    def test_pairwise_distances_chunked_sparse_input():\n        X = csr_matrix([[1, 2], [3, 4], [5, 6]])\n        # Reduce chunk size to force multiple chunks\n        for chunk_size in [1, 2]:\n            for metric in [\"euclidean\", \"manhattan\"]:\n                results = [chunk for chunk in pairwise_distances_chunked(\n                    X, metric=metric, chunk_size=chunk_size)]\n\n                expected = pairwise_distances(X, metric=metric)\n                for i in range(len(results)):\n                    assert_array_almost_equal(results[i], expected[i * chunk_size:(i + 1) * chunk_size, :])\n\n\n", "    def test_pairwise_distances_chunked_reduce_func_output():\n        X = np.random.rand(5, 3)\n        \n            return D_chunk.sum(axis=1)\n\n        results = []\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n        for chunk in gen:\n            results.extend(chunk)\n\n        expected = pairwise_distances(X).sum(axis=1)\n        assert_array_almost_equal(results, expected)\n", "    def test_pairwise_distances_precomputed(self, metric, params, expected_func):\n        # Test precomputed case \n        X = np.array([[1, 2], [3, 4]])\n        Y = np.array([[5, 6], [7, 8]])\n\n        dist_matrix = pairwise_distances(X, metric=\"euclidean\")\n        result = pairwise_distances(\n            dist_matrix, Y, metric=metric, **params\n        )\n\n        # Expected result is pairwise distances between X and Y\n        expected_result = pairwise_distances(\n            X, Y, metric=expected_func, **params\n        )\n\n        assert_array_almost_equal(result, expected_result)\n"], "sample_586": ["def test_concat_different_coords():\n    # Test concatenating DataArrays with different coordinates\n    x = DataArray(np.arange(4), dims=['time'], coords={'time': np.arange(4)})\n    y = DataArray(np.arange(4), dims=['time'], coords={'time': np.arange(4, 8)})\n\n    expected = DataArray(np.arange(8), dims=['time'], coords={'time': np.arange(8)})\n    \n    actual = concat([x, y], dim='time')\n    assert_identical(actual, expected)\n\n", "def test_concat_dataarray_different_compat():\n    arr1 = DataArray(np.arange(4).reshape((2, 2)), dims=[\"x\", \"y\"])\n    arr2 = DataArray(np.arange(4).reshape((2, 2)) + 1, dims=[\"x\", \"y\"])\n    arr3 = DataArray(np.arange(4).reshape((2, 2)) + 2, dims=[\"x\", \"y\"])\n\n    ds = concat([arr1, arr2, arr3], dim=\"z\", compat=\"different\")\n    assert_identical(ds,\n                     concat([arr1, arr2, arr3], dim=\"z\", data_vars='all', compat=\"different\"))\n\n\n", "    def test_concat_datasets_different_coords():\n        ds1 = Dataset({'a': (('x', 'y'), np.arange(6).reshape(2, 3))},\n                      coords={'x': [0, 1], 'y': [0, 1, 2]})\n        ds2 = Dataset({'a': (('x', 'y'), np.arange(6).reshape(2, 3))},\n                      coords={'x': [2, 3], 'y': [0, 1, 2]})\n        actual = concat([ds1, ds2], dim='x')\n        expected = Dataset({'a': (('x', 'y'), np.arange(12).reshape(4, 3))},\n                           coords={'x': [0, 1, 2, 3], 'y': [0, 1, 2]})\n        assert_identical(actual, expected)\n", "    def test_concat_dataarray_with_coords():\n        # Test concatenation of DataArray with coordinates\n\n        # Create two DataArrays with overlapping coordinates\n        data1 = DataArray(np.arange(4).reshape(2, 2), dims=[\"x\", \"y\"],\n                          coords={\"x\": [0, 1], \"y\": [0, 1], \"a\": (\"x\", [1, 2])})\n        data2 = DataArray(np.arange(4, 8).reshape(2, 2), dims=[\"x\", \"y\"],\n                          coords={\"x\": [1, 2], \"y\": [0, 1], \"a\": (\"x\", [3, 4])})\n\n        # Concatenate DataArrays along the 'x' dimension\n        concatenated = concat([data1, data2], dim=\"x\")\n\n        # Assert that the concatenated DataArray has the expected shape and coordinates\n        assert_equal(concatenated.shape, (4, 2))\n        assert_identical(concatenated.coords, {\"x\": [0, 1, 1, 2],\n                                             \"y\": [0, 1, 0, 1],\n                                             \"a\": [1, 2, 3, 4]})\n\n", "    def test_concat_coords_override():\n        data = create_test_data()\n        ds1 = data['ds1']\n        ds2 = data['ds2']\n        ds3 = data['ds3']\n\n        ds2 = ds2.rename({'coord1': 'coord2'})\n        ds3 = ds3.rename({'coord1': 'coord3'})\n\n        with pytest.warns(UserWarning):\n            ds = concat([ds1, ds2, ds3], dim='dim1', compat='override', coords='minimal')\n\n        assert \"coord1\" in ds.coords\n        assert \"coord2\" not in ds.coords\n        assert \"coord3\" not in ds.coords\n\n\n"], "sample_290": ["    def test_rename_model_with_m2m_through_same_name(self):\n        changes = self.get_changes(\n            [\n                self.author_with_m2m_through,\n            ],\n            [\n                self.author_with_renamed_m2m_through,\n            ],\n        )\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\", old_name=\"Author\")\n\n", "    def test_m2m_through_unchanged_fields(self):\n        \"\"\"\n        Test that changes to fields (other than the auto-created through table)\n        on models involved in M2M relationships are detected correctly.\n        \"\"\"\n        before_states = [\n            self.author_with_m2m_through,\n            self.contract,\n            self.publisher,\n        ]\n        after_states = [\n            self.author_with_m2m_through.clone(options={\n                'fields': [\n                    ModelState.from_dict('name', models.CharField(max_length=200)),\n                ]\n            }),\n            self.contract,\n            self.publisher,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(\n            changes, 'testapp', 0, ['AlterField']\n        )\n\n", "    def test_create_model_with_unique_together_with_m2m_field(self):\n        changes = self.get_changes(\n            [self.author],\n            [self.author_with_m2m, self.author_with_m2m_blank],\n        )\n\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddUniqueConstraint'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author_with_m2m_unique',\n                                        fields=['publishers'],\n                                        condition=None)\n", "    def test_circular_foreign_keys(self):\n        changes = self.get_changes(\n            [self.knight],\n            [self.knight, self.rabbit],\n        )\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertOperationTypes(changes, 'eggs', 0, ['CreateModel', 'AddField'])\n        self.assertOperationAttributes(\n            changes, 'eggs', 0, 1, name='parent', field=self.rabbit.fields['parent']\n        )\n        self.assertOperationAttributes(\n            changes, 'eggs', 0, 1, field=self.rabbit.fields['parent'],\n            on_delete=models.CASCADE,\n        )\n\n", "    def test_field_rename_with_related_object(self):\n        before = [\n            self.author_with_book,\n            self.book,\n        ]\n        after = [\n            self.author_with_book_renamed,\n            self.book_with_author_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", old_name=\"name\")\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, field=\"author\")\n"], "sample_832": ["    def test_ard_regression_sparse_input(self):\n        rng = check_random_state(42)\n        n_samples, n_features = 50, 20\n        X = rng.randn(n_samples, n_features)\n        X = X[:, 5]  # use only one feature\n        y = rng.randn(n_samples)\n        X = np.sparse.csr_matrix(X)\n\n        clf = ARDRegression()\n        clf.fit(X, y)\n        y_pred = clf.predict(X)\n        assert_array_almost_equal(y_pred, clf._decision_function(X))\n", "    def test_ard_regression_with_alpha(self):\n        # Test ARD regression with manually set alpha value\n        rng = check_random_state(42)\n        n_samples = 50\n        n_features = 20\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n        alpha = 0.5\n        \n        ard = ARDRegression(alpha_init=alpha)\n        ard.fit(X, y)\n        assert_almost_equal(ard.alpha_, alpha, decimal=5)\n        \n", "    def test_bayesian_ridge_alpha(self):\n        # Test alpha update in BayesianRidge\n        rng = check_random_state(0)\n        n_samples, n_features = 50, 10\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        # Test with a fixed alpha\n        clf = BayesianRidge(alpha_init=1.0)\n        clf.fit(X, y)\n        assert_almost_equal(clf.alpha_, 1.0)\n\n        # Test with alpha estimation\n        clf = BayesianRidge()\n        clf.fit(X, y)\n        assert clf.alpha_ > 0\n", "    def test_ard_regression_threshold_lambda(self):\n        # Check if ARDRegression can prune features correctly\n        n_samples = 100\n        n_features = 20\n        rng = check_random_state(42)\n        X = rng.randn(n_samples, n_features)\n        coef = rng.randn(n_features)\n        coef[10:] = 0\n        y = np.dot(X, coef) + rng.randn(n_samples)\n\n        # Ensure high precision for a subset of features\n        lambda_ = np.ones(n_features)\n        lambda_[10:] = 1e6\n        \n        model = ARDRegression(lambda_1=1e-6, lambda_2=1e-6,\n                              threshold_lambda=1e5,\n                              compute_score=True)\n        model.fit(X, y)\n\n        # Check if the pruned features have coef and lambda close to zero\n        assert_array_less(np.abs(model.coef_[10:]), 1e-6)\n        assert_array_less(np.abs(model.lambda_[10:]), 1e-6)\n\n        # Check if the kept features have non-zero coef and lambda\n        assert_array_less(np.abs(model.coef_[:10]), 1e-3)\n        assert_array_less(np.abs(model.lambda_[:10]-10), 1e-3)\n", "    def test_ard_regression_alpha(self):\n        rng = check_random_state(42)\n        n_samples, n_features = 50, 10\n        X = rng.randn(n_samples, n_features)\n        y = rng.randn(n_samples)\n\n        # Test that alpha is correctly estimated\n        clf = ARDRegression(alpha_1=1., alpha_2=1., fit_intercept=False)\n        clf.fit(X, y)\n        assert_almost_equal(clf.alpha_, np.var(y - np.dot(X, clf.coef_)), 2)\n"], "sample_1141": ["def test_ZeroMatrix_scalar_multiplication():\n    assert ZeroMatrix(2, 3) * 2 == ZeroMatrix(2, 3)\n    assert 2 * ZeroMatrix(2, 3) == ZeroMatrix(2, 3)\n", "def test_KroneckerDelta_matrix_eval():\n    A = Matrix([ [1, 2], [3, 4] ])\n    B = Matrix([ [5, 6], [7, 8] ])\n    assert KroneckerDelta(n, m) * A == A * KroneckerDelta(n, m)\n    assert KroneckerDelta(n, m) * KroneckerDelta(m, k) * A == KroneckerDelta(n, k) * A\n    assert KroneckerDelta(n, m) * KroneckerDelta(n, k) * A == KroneckerDelta(m, k) * A\n    assert KroneckerDelta(n, m) * A * B == A * B * KroneckerDelta(n, m)\n", "def test_ZeroMatrix():\n    z = ZeroMatrix(n,m)\n    assert z.shape == (n, m)\n    assert z[0,0] == 0\n    assert z + z == z\n    assert z*A == z\n    assert A*z == z\n", "def test_MatrixElement_is_real():\n    i, j = symbols('i j', integer=True)\n    X = MatrixSymbol('X', 3, 3)\n    me = MatrixElement(X, i, j)\n    assert me.is_real is None\n    X = Matrix([[S.One, S.Two], [S.Three, S.Four]])\n    me = MatrixElement(X, 0, 1)\n    assert me.is_real is True\n    me = MatrixElement(X, 1, 0)\n    assert me.is_real is True\n", "def test_MatrixElement_add():\n    a = MatrixSymbol('A', 2, 2)\n    b = MatrixElement(a, 0, 0)\n    c = MatrixElement(a, 1, 1)\n    assert b + c == MatrixElement(a, 0, 0) + MatrixElement(a, 1, 1)\n"], "sample_1165": ["def test_conjugate():\n    q = Quaternion(1, 2, 3, 4)\n    assert conjugate(q) == Quaternion(1, -2, -3, -4)\n    assert conjugate(Quaternion(0, 1, 0, 0)) == Quaternion(0, -1, 0, 0)\n\n", "def test_quaternion_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    result = q.pow_cos_sin(4)\n    assert isinstance(result, Quaternion)\n    assert result.a.is_Number\n    assert result.b.is_Number\n    assert result.c.is_Number\n    assert result.d.is_Number\n", "def test_integrate():\n    q = Quaternion(x, y, x*y, z)\n    assert q.integrate(x) == Quaternion(x**2/2, y*x, x**2*y/2, x*z)\n    assert q.integrate((x, 1, 2)) == Quaternion(3/2, 2*y, 2*y, 3*z) \n", "def test_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    q2 = q.pow_cos_sin(2)\n    assert q2.a == 900*cos(2*acos(sqrt(30)/30))\n    assert q2.b == 1800*sqrt(29)*sin(2*acos(sqrt(30)/30))/29\n    assert q2.c == 2700*sqrt(29)*sin(2*acos(sqrt(30)/30))/29\n    assert q2.d == 3600*sqrt(29)*sin(2*acos(sqrt(30)/30))/29\n\n", "def test_quaternion_pow_cos_sin():\n    q = Quaternion(1, 2, 3, 4)\n    assert q.pow_cos_sin(4) == Quaternion.from_axis_angle((sqrt(29)/30, sqrt(29)/30, sqrt(29)/30), 4*acos(sqrt(30)/30)) * 900\n\n"], "sample_211": ["    def test_customizable_view(self):\n        view = CustomizableView()\n        view.parameter = {'key': 'value'}\n        request = self.rf.get('/')\n        response = view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(view.parameter, {'key': 'value'})\n\n", "    def test_view_initkwargs(self):\n        view = SimpleView.as_view(parameter='value')\n        instance = view(self.rf.get('/'))\n        self.assertEqual(instance.parameter, 'value')\n", "    def test_view_as_view(self):\n        view = views.SimpleView.as_view()\n        request = self.rf.get('/')\n        response = view(request)\n        self._assert_simple(response)\n\n", "    def test_view_initkwargs(self):\n        view = SimpleView.as_view(parameter='hello')\n        self.assertEqual(view.parameter, 'hello')\n", "    def test_view_initkwargs(self):\n        view = SimpleView.as_view(parameter='value')\n        self.assertEqual(view.parameter, 'value')\n\n"], "sample_15": ["compilation error", "    def test_ufunc_helpers_no_unit_out_but_output_is_quantity(self):\n        # This test checks that when a ufunc helper is applied to an array with\n        # units, but the ufunc itself doesn't produce a unitful output, the result\n        # is still a Quantity with units\n\n            return x.shape\n        \n        q_in =  np.array([1,2,3]) * u.m\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", UserWarning)\n            result = qh.helper_ufunc(my_op, q_in, output_unit=None)\n        assert isinstance(result, u.Quantity)\n        assert result.unit == u.dimensionless_unscaled\n", "    def test_ufunc_atleast_2d_warning(self):\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            np.atleast_2d(self.q1)\n        assert len(w) == 1\n\n", "compilation error", "compilation error"], "sample_1178": ["def test_FunctionCall():\n    fcall = FunctionCall('foo', [x, y])\n    assert str(fcall) == \"foo(x, y)\"\n\n    fcall = FunctionCall('bar')\n    assert str(fcall) == \"bar()\"\n\n    fcall = FunctionCall('baz', [1, 2, 3])\n    assert str(fcall) == \"baz(1, 2, 3)\"\n", "def test_FunctionCall():\n    fcall = FunctionCall('foo', [x, y])\n    assert fcall.name == 'foo'\n    assert fcall.function_args == (x, y)\n\n", "def test_FunctionDefinition():\n    f = FunctionDefinition(return_type=real, name='foo', parameters=[x], body=[Return(sin(x))])\n    assert str(f) == 'double foo(double x){return sin(x);}'\n\n    f = FunctionDefinition(return_type=real, name='foo', parameters=[x], body=[Return(sin(x))])\n    assert f.type == real, f.type\n\n    f = FunctionDefinition(return_type=Type.from_expr(x), name='foo', parameters=[x], body=[Return(sin(x))])\n    assert str(f) == 'double foo(double x){return sin(x);}'\n\n    f = FunctionDefinition(return_type=Type.from_expr(mat), name='foo', parameters=[x], body=[Return(mat)])\n    assert str(f) == 'Matrix foo(double x){return Matrix([1, 2, 3]);}'\n\n\n\n", "def test_FunctionPrototype_ccode():\n    fp = FunctionPrototype(real, 'foo', [Variable('x', real), Variable('y', real)])\n    assert ccode(fp) == 'double foo(double x, double y)'\n\n", "    def test_augmented_assignments(self):\n        assert str(AddAugmentedAssignment(x, y)) == 'x += y'\n        assert str(SubAugmentedAssignment(x, y)) == 'x -= y'\n        assert str(MulAugmentedAssignment(x, y)) == 'x *= y'\n        assert str(DivAugmentedAssignment(x, y)) == 'x /= y'\n        assert str(ModAugmentedAssignment(x, y)) == 'x %= y'\n"], "sample_33": ["    def test_set_locale(capsys):\n        with misc.set_locale('C'):\n            # Make sure locale is actually changed\n            assert locale.getlocale()[0] == 'C'\n            # And it's restored when exiting the context\n        assert locale.getlocale()[0] != 'C'\n\n        # Check that errors are handled\n        with pytest.raises(ValueError):\n            with misc.set_locale('nonexistent'):\n                pass\n\n", "    def test_shaped_like_ndarray_flatten(self):\n        times = data.Time([1, 2, 3], format='jd')\n        flat_times = times.flatten()\n        assert flat_times.shape == (3,)\n        assert flat_times.ndim == 1\n\n", "    def test_check_broadcast_empty(self):\n        assert misc.check_broadcast() == ()\n", "def test_did_you_mean():\n    assert misc.did_you_mean('sprctrum', ['spectrum', 'source']) == 'Did you mean spectrum?'\n    assert misc.did_you_mean('sprctrum', ['spectrum', 'source'], n=1) == 'Did you mean spectrum?'\n    assert misc.did_you_mean(\n        'sprctrum', ['spectrum', 'source'], n=2) == 'Did you mean spectrum?'\n    assert misc.did_you_mean('sprctrum', ['source']) is ''\n    assert misc.did_you_mean('sprctrum', [], n=2) is ''\n    assert misc.did_you_mean('sprctrum', ['Spectrum'], fix=str.lower) == 'Did you mean spectrum?'\n\n\n", "    def test_set_locale(tmpdir):\n        with tmpdir.as_cwd():\n            with misc.set_locale('C'):\n                # Test that setting the locale to 'C' actually sets it\n                assert locale.getlocale()[0] == 'C'\n\n                # Test that the locale is correctly restored after exiting\n                # the context manager.\n                with misc.set_locale('fr_FR'):\n                    assert locale.getlocale()[0] == 'fr_FR'\n                assert locale.getlocale()[0] == 'C'\n\n"], "sample_464": ["    def test_file_response_with_no_content_length(self):\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(b\"Hello, world!\")\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertEqual(response['Content-Length'], '13')\n", "    def test_file_response_with_no_content_disposition(self):\n        # Make sure no Content-Disposition is set if no filename is provided\n        with tempfile.NamedTemporaryFile(mode=\"wb\") as f:\n            f.write(b\"Hello World!\")\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertFalse('Content-Disposition' in response)\n", "    def test_file_response_with_seekable_file(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b\"Hello, world!\")\n            f.seek(0)\n            response = FileResponse(f)\n            self.assertEqual(response[\"Content-Length\"], 13)\n", "    def test_file_response_with_seekable_filelike(self):\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(b\"content\")\n            f.flush()\n            response = FileResponse(f, content_type=\"text/plain\")\n            self.assertEqual(response[\"Content-Length\"], 7)\n\n", "    def test_file_response_with_unseekable_file(self):\n        with tempfile.TemporaryFile(\"w+b\") as f:\n            f.write(b\"some content\")\n            f.seek(0)\n            response = FileResponse(UnseekableBytesIO(f.read()))\n            self.assertEqual(response[\"Content-Length\"], len(f.read()))\n"], "sample_153": ["    def test_check_database_backends_with_mock(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check', return_value=[]):\n            errors = check_database_backends(connection)\n            self.assertEqual(errors, [])\n\n\n", "    def test_check_database_backends(self):\n        with mock.patch('django.db.utils.DatabaseIntrospection') as mock_introspection:\n            mock_introspection.return_value.get_table_names = mock.MagicMock(return_value=['test_table'])\n            \n            with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n                check_database_backends(self.databases)\n                mock_check.assert_called_once_with(\n                    connection.creation.TEST_DATABASE_WRAPPERS\n                )\n\n", "    def test_check_database_backends_router(self):\n        with mock.patch('django.db.connections.router.router') as mock_router:\n            mock_router.allow_migrate_model.return_value = True\n            check_database_backends(\n                connection.databases['default'],\n                connection.databases\n            )\n            mock_router.allow_migrate_model.assert_called()\n", "    def test_check_database_backends(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation') as mock_backend:\n            mock_backend.return_value.check.side_effect = lambda *args, **kwargs: []\n            errors = check_database_backends(self.databases)\n            self.assertEqual(errors, [])\n", "    def test_check_database_backends_invalid(self):\n        with mock.patch('django.db.backends.base.validation.BaseDatabaseValidation.check') as mock_check:\n            mock_check.side_effect = KeyError('Fake Error')\n            with self.assertRaises(KeyError):\n                check_database_backends(connection.databases)\n"], "sample_856": ["    def test_shuffle_split_error_message(self):\n        # Test if the error message when trying to use n_splits > number of samples\n        X = np.array([[1], [2], [3]])\n        with pytest.raises(ValueError) as excinfo:\n            ShuffleSplit(n_splits=5, test_size=0.25).split(X)\n        assert \"The total number of splits cannot exceed\" in str(excinfo.value)\n        assert \"the number of samples\" in str(excinfo.value)\n", "def test_predefined_split_indices():\n    # Test that PredefinedSplit works correctly with different index sets\n    X = np.arange(10)\n    y = np.arange(10)\n\n    # Test case 1: all samples in different folds\n    test_fold = np.arange(10)\n    ps = PredefinedSplit(test_fold)\n    for train_index, test_index in ps.split():\n        assert set(train_index) | set(test_index) == set(range(10))\n\n    # Test case 2: some samples excluded\n    test_fold = [-1, 0, 1, -1, 3, 4, -1, -1, 7, -1]\n    ps = PredefinedSplit(test_fold)\n    expected_splits = [([1, 3, 4, 7], [0, 2]), ([0, 2], [1, 3, 4, 7])]\n    for i, (train_index, test_index) in enumerate(ps.split()):\n        assert_array_equal(train_index, expected_splits[i][0])\n        assert_array_equal(test_index, expected_splits[i][1])\n\n    # Test case 3: empty folds\n    test_fold = [-1] * 10\n    ps = PredefinedSplit(test_fold)\n    for train_index, test_index in ps.split():\n        assert len(train_index) == 0\n        assert len(test_index) == 0\n\n    # Test case 4: invalid fold value\n    test_fold = [0, 1, 2, -2]\n    with pytest.raises(ValueError):\n        PredefinedSplit(test_fold)\n\n", "    def test_cross_val_score_sparse_data():\n        # Test cross_val_score with sparse data\n        X = coo_matrix(np.array([[0, 1], [1, 0]]))\n        y = np.array([0, 1])\n        clf = MockClassifier()\n\n        for cv in [KFold(), StratifiedKFold()]:\n            scores = cross_val_score(clf, X, y, cv=cv)\n            assert len(scores) == cv.get_n_splits(X, y)\n            assert np.allclose(scores, 1. / (1 + np.abs(clf.a)))\n\n\n", "def test_validate_shuffle_split():\n    assert_raises_regexp(\n        ValueError, \"test_size={0} should be either positive and smaller\"\n        \" than the number of samples {1} or a float in the (0, 1)\"\n        \" range\".format(0, 10),\n        _validate_shuffle_split, 10, 0, default_test_size=None)\n\n    assert_raises_regexp(\n        ValueError, \"test_size={0} should be either positive and smaller\"\n        \" than the number of samples {1} or a float in the (0, 1)\"\n        \" range\".format(11, 10),\n        _validate_shuffle_split, 10, 11, default_test_size=None)\n", "    def test_train_test_split_sparse_with_stratify(self):\n        X = coo_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        y = np.array([0, 0, 1])\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, stratify=y, test_size=0.33, random_state=0)\n\n        assert_isinstance(X_train, coo_matrix)\n        assert_isinstance(X_test, coo_matrix)\n        assert_array_equal(y_train, np.array([0, 0]))\n        assert_array_equal(y_test, np.array([1]))\n\n"], "sample_51": ["    def test_parse_datetime_with_timezone(self):\n        dt_str = '2023-10-26T12:34:56+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n", "    def test_parse_datetime_with_timezone(self):\n        dt_str = '2023-10-27T14:30:00+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n", "    def test_parse_datetime_with_timezone(self):\n        dt = parse_datetime('2023-10-27T10:00:00+02:00')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 10)\n        self.assertEqual(dt.minute, 0)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n", "    def test_parse_datetime_with_timezone(self):\n        dt_str = '2023-10-26T14:30:00+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo, get_fixed_timezone(120))\n", "    def test_parse_datetime_tzinfo(self):\n        dt = parse_datetime('2023-10-26T12:00:00-05:00')\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), -18000)\n        dt = parse_datetime('2023-10-26T12:00:00+02:30')\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 9000)\n"], "sample_1209": ["def test_prefix_unit():\n    from sympy.physics.units import meter\n    prefixed_units = prefix_unit(meter, PREFIXES)\n    assert prefixed_units == [\n        Quantity('millimeter', abbrev='mm', is_prefixed=True),\n        Quantity('centimeter', abbrev='cm', is_prefixed=True),\n        Quantity('decimeter', abbrev='dm', is_prefixed=True),\n        Quantity('meter', abbrev='m', is_prefixed=True),\n        Quantity('decameter', abbrev='dam', is_prefixed=True),\n        Quantity('hectometer', abbrev='hm', is_prefixed=True),\n        Quantity('kilometer', abbrev='km', is_prefixed=True),\n        Quantity('megameter', abbrev='Mm', is_prefixed=True),\n        Quantity('gigameter', abbrev='Gm', is_prefixed=True),\n        Quantity('terameter', abbrev='Tm', is_prefixed=True),\n        Quantity('petameter', abbrev='Pm', is_prefixed=True),\n        Quantity('exameter', abbrev='Em', is_prefixed=True),\n        Quantity('zettameter', abbrev='Zm', is_prefixed=True),\n        Quantity('yottameter', abbrev='Ym', is_prefixed=True),\n\n\n    ]\n", "    def test_prefix_multiplication():\n        assert kilo * meter == kilometer\n        assert kilo * 10 == 10000\n", "def test_prefix_mul():\n    assert kilo*meter == Quantity('kilometer', abbrev=\"km\")\n    assert kibi*meter == Quantity('kibimeter', abbrev=\"Kib\")\n    assert kilo*W == Quantity('kilowatt', abbrev='kW')\n    assert kilo*kilo*meter == Quantity('megameter', abbrev='Mm')\n", "    def test_prefix_unit():\n        from sympy.physics.units.quantities import Quantity\n        m = Quantity('meter', abbrev='m', length=1)\n        prefixed_units = prefix_unit(m, PREFIXES)\n        assert prefixed_units[0].name == 'millimeter'\n        assert prefixed_units[0].abbrev == 'mm'\n        assert prefixed_units[0].scale_factor == 1e-3\n\n        assert prefixed_units[1].name == 'centimeter'\n        assert prefixed_units[1].abbrev == 'cm'\n        assert prefixed_units[1].scale_factor == 1e-2\n\n        assert prefixed_units[2].name == 'decimeter'\n        assert prefixed_units[2].abbrev == 'dm'\n        assert prefixed_units[2].scale_factor == 1e-1\n", "def test__mul__():\n    assert kilo * meter == Quantity('kilometer', abbrev='km')\n    assert meter * kilo == Quantity('kilometer', abbrev='km')\n    assert kilo * kilo * meter == Quantity('megameter', abbrev='Mm')\n    assert 2 * kilo * meter == Quantity('2 kilometer', abbrev='2 km')\n    assert kilo * (2 * meter) == Quantity('2 kilometer', abbrev='2 km')\n    assert kilo * (x * meter) == Quantity('x kilometer', abbrev=x * ' km')\n    assert (x * meter) * kilo == Quantity('x kilometer', abbrev=x * ' km')\n    assert kibi * meter == Mul(kibi.scale_factor * meter, 1)\n"], "sample_1039": ["def test_print_MatrixSymbol():\n    a = MatrixSymbol('A', 2, 2)\n    assert mathml(a) == '<ci>A</ci>'\n    assert mathml(a, printer='presentation') == '<mi>A</mi>'\n", "def test_print_MatrixSymbol():\n    m = MatrixSymbol('A', 2, 3)\n    assert mathml(m) == '<ci>A</ci>'\n    assert mathml(m, printer='presentation') == '<mi>A</mi>'\n\n", "def test_print_MatrixSymbol_presentation():\n    x = Symbol('x')\n    y = MatrixSymbol('M', 2, 2)\n    assert mpp._print(y) == '<mi>M</mi>'\n", "def test_print_MatrixSymbol():\n    assert mp._print(MatrixSymbol('A', 2, 3)) == '<operatorname mathsize=\"small\" style=\"italic\">A</operatorname>'\n    assert mpp._print(MatrixSymbol('A', 2, 3)) == '<mi>A</mi>'\n\n", "def test_print_matrix_symbol():\n    m = MatrixSymbol('M', 2, 2)\n    assert mathml(m) == '<ci>M</ci>'\n    assert mathml(m, printer='presentation') == '<mi>M</mi>'\n\n"], "sample_411": ["    def test_no_translations(self):\n        with translation.override(\"fr\"):\n            with captured_stderr() as stderr:\n                management.call_command(\"dance\", stdout=StringIO(), stderr=StringIO())\n            self.assertEqual(stderr.getvalue(), \"\")\n", "    def test_command_with_stealth_options(self):\n        old_stderr = sys.stderr\n        try:\n            sys.stderr = StringIO()\n            with mock.patch.object(BaseCommand, 'stealth_options', new_callable=mock.PropertyMock) as mock_stealth_options:\n                mock_stealth_options.return_value = ('foo', 'bar')\n                management.call_command('dance', 'input', stderr=sys.stderr)\n                self.assertIn('foo', str(sys.stderr.getvalue()))\n                self.assertIn('bar', str(sys.stderr.getvalue()))\n        finally:\n            sys.stderr = old_stderr \n\n", "    def test_no_translations(self):\n        with translation.override(\"fr\"):\n            output = StringIO()\n            with mock.patch(\"django.utils.translation.get_language\", return_value=None):\n                with mock.patch(\"sys.stdout\", output):\n                    management.call_command(\"dance\", stdout=output)\n                self.assertEqual(output.getvalue(), \"Dancing in Django\\n\")\n", "    def test_basecommand_call_command(self):\n        out = StringIO()\n        err = StringIO()\n        with mock.patch(\"sys.stdout\", out):\n            with mock.patch(\"sys.stderr\", err):\n                management.call_command(\"dance\", stdout=out, stderr=err)\n        self.assertEqual(out.getvalue().strip(), \"Dancing!\")\n        self.assertEqual(err.getvalue(), \"\")\n\n", "    def test_no_translations(self):\n        # Test that the no_translations decorator correctly deactivates translations.\n\n        class MyCommand(BaseCommand):\n                parser.add_argument('--name', type=str)\n\n            @no_translations\n                return options['name']\n\n        out = StringIO()\n        err = StringIO()\n        with mock.patch('django.utils.translation.get_language') as get_language, \\\n                mock.patch('django.utils.translation.activate') as activate:\n\n            get_language.return_value = 'fr'\n            cmd = MyCommand()\n            cmd.stdout = out\n            cmd.stderr = err\n            cmd.handle_no_translations = True\n            cmd.run_from_argv(['manage.py', 'mycommand', '--name', 'John'])\n            self.assertEqual(out.getvalue(), 'John')\n            get_language.assert_called_once()\n            activate.assert_not_called()\n\n"], "sample_1108": ["def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3), (-1, 2, 3), (1, -2, 3), (1, 2, -3),\n                                             (-1, -2, 3), (-1, 2, -3), (1, -2, -3), (-1, -2, -3)]\n\n", "    def test_permute_signs_with_zeroes():\n        a = (0, 1, 2, 0)\n        signs = list(permute_signs(a))\n        assert len(signs) == 4\n        assert (0, 1, 2, 0) in signs\n        assert (0, -1, 2, 0) in signs\n        assert (0, 1, -2, 0) in signs\n        assert (0, -1, -2, 0) in signs\n\n", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n\n", "def test_is_palindromic():\n    assert is_palindromic('racecar')\n    assert is_palindromic([1, 2, 3, 2, 1])\n    assert is_palindromic((1, 2, 3, 2, 1))\n    assert not is_palindromic('hello')\n    assert not is_palindromic([1, 2, 3, 4, 5])\n\n\n\n", "def test_generate_oriented_forest():\n    for i in range(5):\n        l = list(generate_oriented_forest(i))\n        for p in l:\n            assert len(p) == i\n            for j in range(i - 1):\n                assert p[j] < p[j + 1]\n\n"], "sample_889": ["    def test_calibrated_classifier_cv_ensemble(self, data, method):\n        X, y = data\n        # Create a pipeline with a classifier and a scaler\n        pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n        clf = CalibratedClassifierCV(\n            base_estimator=pipeline, method=method, cv=3\n        )\n        clf.fit(X, y)\n        y_prob = clf.predict_proba(X)\n\n        # Check that the probabilities are calibrated\n        assert_almost_equal(\n            brier_score_loss(y, y_prob[:, 1], pos_label=1),\n            0.0,\n            decimal=2,\n        )\n\n", "def test_calibration_curve_with_sparse_matrix(data):\n    X, y = data\n    X_sparse = sparse.csr_matrix(X)\n    # Test that calibration_curve works with sparse matrices\n    prob_true, prob_pred = calibration_curve(y, estimator.predict_proba(X_sparse)[:, 1], n_bins=10)\n    assert len(prob_true) == len(prob_pred)\n", "def test_calibration_curve_with_sparse_matrix(data):\n    X, y = data\n    X_sparse = sparse.csr_matrix(X)\n\n    y_prob = LogisticRegression().fit(X, y).predict_proba(X)[:, 1]\n    prob_true, prob_pred = calibration_curve(y, y_prob, n_bins=5)\n    y_prob_sparse = LogisticRegression().fit(X_sparse, y).predict_proba(X_sparse)[:,\n                                                                      1]\n    prob_true_sparse, prob_pred_sparse = calibration_curve(y, y_prob_sparse, n_bins=5)\n    assert_allclose(prob_true, prob_true_sparse)\n    assert_allclose(prob_pred, prob_pred_sparse)\n\n", "    def test_calibrated_classifier_proba(self, data, method, ensemble):\n        X, y = data\n        if ensemble:\n            clf = RandomForestClassifier(random_state=42)\n        else:\n            clf = LogisticRegression(random_state=42)\n        clf.fit(X, y)\n        calibrated_clf = CalibratedClassifierCV(clf, method=method, cv=\"prefit\")\n        calibrated_clf.fit(X, y)\n\n        y_prob = calibrated_clf.predict_proba(X)\n        # Check that probabilities sum to 1\n        assert_allclose(y_prob.sum(axis=1), np.ones(N_SAMPLES))\n\n", "    def test_calibrated_classifier_pipeline(self, method, ensemble):\n\n        # Test if pipeline calibration works correctly\n        X, y = self.data\n\n        if ensemble:\n            clf = VotingClassifier(\n                estimators=[\n                    ('lr', LogisticRegression()),\n                    ('dt', DecisionTreeClassifier())\n                ]\n            )\n        else:\n            clf = LogisticRegression()\n\n        pipeline = Pipeline([('scaler', StandardScaler()), ('clf', clf)])\n\n        calibrated_clf = CalibratedClassifierCV(\n            base_estimator=pipeline, method=method, cv=3\n        )\n        calibrated_clf.fit(X, y)\n\n        # check if predict_proba outputs probabilities\n        probas = calibrated_clf.predict_proba(X)\n        assert_allclose(probas[:, 1], calibrated_clf.predict_proba(X)[:, 1])\n        assert_allclose(np.sum(probas, axis=1), np.ones(N_SAMPLES))\n\n"], "sample_66": ["    def test_load_post_and_files_with_multipart_parser_error(self):\n        request = HttpRequest()\n        request._read_started = True\n\n        request._body = b'--boundary\\r\\nContent-Disposition: form-data; name=\"field1\"\\r\\n\\r\\nvalue1\\r\\n--boundary\\r\\nContent-Disposition: form-data; name=\"field2\"\\r\\n\\r\\nvalue2\\r\\n--boundary--\\r\\n'\n        request._stream = BytesIO(request._body)\n\n        with self.assertRaises(MultiPartParserError):\n            request._load_post_and_files()\n", "    def test_get_full_path_info_with_trailing_slash(self):\n        request = HttpRequest()\n        request.path_info = '/some/path/'\n        self.assertEqual(request.get_full_path_info(), '/some/path/')\n        self.assertEqual(request.get_full_path_info(force_append_slash=True), '/some/path/')\n", "    def test_get_full_path_info(self):\n        request = HttpRequest()\n        request.path_info = '/some/path'\n\n        self.assertEqual(request.get_full_path_info(), '/some/path')\n        self.assertEqual(request.get_full_path_info(force_append_slash=True), '/some/path/')\n\n", "    def test_get_host_with_invalid_host(self):\n        with override_settings(ALLOWED_HOSTS=[]):\n            request = HttpRequest()\n            request.META = {'HTTP_HOST': 'invalid.host'}\n            msg = 'Invalid HTTP_HOST header: \"invalid.host\".'\n            with self.assertRaisesMessage(DisallowedHost, msg):\n                request.get_host()\n", "    def test_http_host(self):\n        request = HttpRequest()\n        request.META = {\n            'HTTP_HOST': 'testserver',\n            'SERVER_NAME': 'localhost',\n            'SERVER_PORT': '80',\n        }\n\n        self.assertEqual(request.get_host(), 'testserver')\n\n"], "sample_650": ["    def test_log_cli_level_disabled(self, pytester: Pytester, tmp_path: tmp_path) -> None:\n        pytester.makeconftest(\n            \"\"\"\n            import pytest\n\n            pytest.ini_options[\"log_cli\"] = \"WARNING\"\n            pytest.ini_options[\"log_cli_level\"] = 39\n            \"\"\"\n        )\n        result = pytester.runpytest(\n            \"--log-level\",\n            \"INFO\",\n            tmp_path / \"test_log.py\",\n        )\n        assert result.ret == ExitCode.OK\n        result.stdout.fnmatch_lines(\n            [\n                \"*test_log.py::test_function PASSED*\",\n            ]\n        )\n", "    def test_log_cli_level_overrides_log_level(\n        self, pytester: Pytester, tmpdir: tmpdir", "def test_log_level_set_to_integer(pytester: Pytester) -> None:\n    pytester.makepyfile(\n        \"\"\"\n        import logging\n        import pytest\n\n        @pytest.mark.parametrize(\"log_level\", [\n            logging.CRITICAL,\n            logging.ERROR,\n            logging.WARNING,\n            logging.INFO,\n            logging.DEBUG,\n            logging.NOTSET,\n        ])\n            with pytest.raises(ValueError):\n                pytest.main([\"-s\", \"--log-level=\" + str(log_level), \"test_dummy.py\"])\n        \"\"\"\n    )\n    result = pytester.runpytest(\"--log-level=10\")\n    assert result.ret == ExitCode.OK\n\n\n", "    def test_log_level_set_correctly_for_tests(self, pytester: Pytester) -> None:\n        \"\"\"Ensure that the log level set on the handler is correctly applied.\"\"\"\n        pytester.makepyfile(\n            \"\"\"\n            import logging\n            import pytest\n\n                logging.info(\"This is an info log message\")\n                logging.debug(\"This is a debug log message\")\n\n        \"\"\"\n        )\n        result = pytester.runpytest(\"--log-level=INFO\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*INFO*This is an info log message*\",\n                \"*test_logging*PASSED*\",\n            ]\n        )\n        assert result.ret == ExitCode.OK\n", "def test_log_level_disabled_logging(\n    pytester: Pytester,"], "sample_369": ["    def test_rename_field_with_default(self):\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_default_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"old_name\", new_name=\"new_name\")\n\n", "    def test_rename_field_with_default(self):\n        \"\"\"\n        Tests renaming a field that has a default value.\n\n        Migration should:\n\n        1. Rename the field\n        2. Set the new default value on the field if the default has changed\n        3. Not touch the field if the default value remains the same\n\n        \"\"\"\n        before = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Unknown\")),\n        ])\n        after = ModelState(\"testapp\", \"Author\", [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"pen_name\", models.CharField(max_length=200, default=\"Anonymous\")),\n        ])\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name_old='name', name_new='pen_name',\n                                      default='Anonymous')\n\n\n", "    def test_rename_with_existing_model(self):\n        \"\"\"\n        Tests that renaming a model to a name that already exists in a different app\n        results in the correct migration.\n\n        \"\"\"\n        old_testapp_author = self.author\n        old_otherapp_author = self.otherapp_author\n\n        # Create initial state\n        initial_state = self.make_project_state([\n            old_testapp_author,\n            old_otherapp_author,\n        ])\n\n        # Rename the testapp.Author model to \"Writer\"\n        new_testapp_author = self.author_renamed\n        new_testapp_author.rename('testapp', 'Writer')\n\n        # Create final state\n        final_state = self.make_project_state([\n            new_testapp_author,\n            old_otherapp_author,\n        ])\n\n        changes = self.get_changes(initial_state, final_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='Writer')\n\n", "    def test_rename_field_retains_default(self):\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_default_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"name\", field_name=\"name\")\n\n\n", "    def test_circular_fk_migration(self):\n        changes = self.get_changes(\n            [self.knight, self.rabbit],\n            [self.knight, self.rabbit],\n        )\n\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertOperationTypes(changes, 'eggs', 0, ['AlterModelTable'])\n        self.assertOperationAttributes(\n            changes, 'eggs', 0, 0, table='eggs_rabbit', name='eggs_rabbit',\n        )\n\n"], "sample_547": ["    def test_draggable_annotation():\n        fig, ax = plt.subplots()\n\n        offsetbox = AnchoredOffsetbox(\n            loc='upper left',\n            child=TextArea(\"Hello world\"),\n            pad=0.2,\n            borderpad=0.5,\n            frameon=True,\n            bbox_to_anchor=(0.5, 0.5),\n        )\n\n        ann = AnnotationBbox(\n            offsetbox, (0.5, 0.5),\n            xycoords='data',\n            boxcoords='axes fraction',\n            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'),\n        )\n\n        ax.add_artist(ann)\n        draggable = DraggableAnnotation(ann, use_blit=False)\n\n        # Simulate mouse events\n        fig.canvas.draw()\n        event = namedtuple(\"MouseEvent\", [\"x\", \"y\", \"button\", \"inaxes\"])\n        draggable.on_pick(event(x=100, y=100, button=MouseButton.LEFT, inaxes=ax))\n\n        draggable.on_motion(event(x=150, y=150, button=MouseButton.LEFT, inaxes=ax))\n\n        draggable.on_release(event(x=150, y=150, button=MouseButton.LEFT, inaxes=ax))\n\n        # Ensure the position after dragging is different from the initial\n        assert ann.xyann != (0.5, 0.5)\n        # assert_allclose(ann.xyann, (0.6, 0.6))\n\n", "    def test_draggable_offsetbox(self):\n        fig, ax = plt.subplots()\n\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        line, = ax.plot(x, y)\n\n        bbox = AnchoredOffsetbox(\n            loc='upper left',\n            child=TextArea(\"Hello\"),\n            pad=0.2,\n        )\n        ax.add_artist(bbox)\n\n        draggable_offsetbox = DraggableOffsetBox(bbox, bbox)\n\n        # We need a backend that supports blitting for this to work\n        canvas = fig.canvas\n        if canvas.supports_blit:\n            plt.draw()\n            event = MouseEvent(\n                'button_press_event',\n                figure=fig,\n                canvas=canvas,\n                x=bbox.get_window_extent().x0,\n                y=bbox.get_window_extent().y0,\n                button=MouseButton.LEFT\n            )\n            draggable_offsetbox.on_pick(event)\n", "    def test_draggableannotation(self):\n        fig, ax = plt.subplots()\n        ann = AnnotationBbox(\n            TextArea(\"Test\"), (0.5, 0.5), xycoords=\"axes fraction\")\n        ax.add_artist(ann)\n        draggabble_ann = DraggableAnnotation(ann)\n        # Simulate a drag event\n        event = MouseEvent(\n            'motion_notify_event', fig.canvas,\n            x=100, y=100, button=MouseButton.LEFT\n        )\n        draggabble_ann.on_motion(event)\n        # Check that the annotation has moved\n        assert ann.xyann != (0.5, 0.5)\n", "def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    txt = AnnotationBbox(TextArea(\"Hello\"), (0.5, 0.5),\n                         xybox=(-50, 50),\n                         boxcoords=\"offset points\",\n                         arrowprops=dict(arrowstyle=\"-\"))\n\n    ax.add_artist(txt)\n    dragged = DraggableAnnotation(txt)\n\n    canvas = fig.canvas\n    initial_xy = txt.xy\n    # Simulate click, drag and release\n    event = MouseEvent('button_press_event', \n                       canvas, 0, x=canvas.renderer.points_to_pixels(initial_xy[0]\n                       ), y=canvas.renderer.points_to_pixels(initial_xy[1]), \n                       button=MouseButton.LEFT)\n    txt._clicked_artist = txt\n    txt.on_pick(event)\n\n    initial_xybox = txt.xybox\n    event = MouseEvent('motion_notify_event', canvas, 0, \n                       x=canvas.renderer.points_to_pixels(initial_xy[0]+10),\n                       y=canvas.renderer.points_to_pixels(initial_xy[1]+10))\n    txt.on_motion(event)\n    event = MouseEvent('button_release_event', canvas, 0,\n                       x=canvas.renderer.points_to_pixels(initial_xy[0]+10),\n                       y=canvas.renderer.points_to_pixels(initial_xy[1]+10),\n                       button=MouseButton.LEFT)\n    txt.on_release(event)\n    final_xybox = txt.xybox\n\n    assert txt.xybox != initial_xybox\n    assert_allclose(final_xybox, initial_xybox + (10, 10))\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        x, y = 0.5, 0.5\n        ann = AnnotationBbox(\n            TextArea(\"Test\"), (x, y), xybox=(0.7, 0.7), boxcoords=\"axes fraction\")\n        ax.add_artist(ann)\n        dragggable = DraggableAnnotation(ann)\n\n        # Simulate a pick event\n        fake_event = MouseEvent(\n            'pick', fig.canvas, x=100, y=100,\n            button=MouseButton.LEFT,\n            axes=ax,\n        )\n        dragggable.on_pick(fake_event)\n\n        # Simulate a move event\n        fake_event = MouseEvent(\n            'motion_notify', fig.canvas, x=120, y=120,\n            button=MouseButton.LEFT,\n            axes=ax,\n        )\n        dragggable.on_motion(fake_event)\n\n        # Assert that the annotation position has been updated\n        assert_allclose(ann.xyann, (0.72, 0.72))\n"], "sample_112": ["    def test_submit_row(self):\n        request = self.request_factory.get(reverse('admin:app_article_change', args=(1,)))\n        request.user = User.objects.create_user('test', 'test@example.com', 'password')\n        context = submit_row(self.get_context(request, ArticleAdmin(Article, site), Article.objects.get(pk=1)))\n        self.assertTrue(context['can_change'])\n        self.assertFalse(context['show_delete_link'])\n        self.assertFalse(context['show_save_as_new'])\n        self.assertFalse(context['show_save_and_add_another'])\n        self.assertFalse(context['show_save_and_continue'])\n        self.assertTrue(context['show_save'])\n        self.assertFalse(context['show_close'])\n", "    def test_submit_row_tag(self):\n        request = self.request_factory.get('/')\n        request.user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        admin_site = site\n        view = self.get_view(admin_site, ArticleAdmin, 'change', Article.objects.create())\n        context = view(request, object_id=1)\n\n        rendered_html = self.render_template_tag(submit_row, context)\n        self.assertIn('can_change', rendered_html)\n        self.assertIn('show_delete_link', rendered_html)\n        self.assertIn('show_save_as_new', rendered_html)\n        self.assertIn('show_save_and_add_another', rendered_html)\n        self.assertIn('show_save_and_continue', rendered_html)\n        self.assertIn('show_save', rendered_html)\n        self.assertIn('show_close', rendered_html)\n\n", "    def test_prepopulated_fields_js_tag(self):\n        request = self.request_factory.get(reverse('admin:articles_article_add'))\n        request.user = self.user\n        context = ArticleAdmin(Article, site).get_form(request, None).get_context(request)\n\n        template = 'admin/prepopulated_fields_js.html'\n        rendered_template = render_to_string(template, context)\n        self.assertContains(rendered_template, '<script type=\"text/javascript\">')\n", "    def test_submit_row_tag_render(self):\n        request = self.request_factory.get(reverse('admin:articles_article_change', args=[1]))\n        request.user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        context = {\n            'add': False,\n            'change': True,\n            'is_popup': False,\n            'save_as': False,\n            'show_save': True,\n            'show_save_and_add_another': True,\n            'show_save_and_continue': True,\n            'has_add_permission': True,\n            'has_change_permission': True,\n            'has_view_permission': True,\n            'has_editable_inline_admin_formsets': False,\n        }\n        template = self.get_template('admin/submit_line.html')\n        rendered = self.render_template_with_context(template, context)\n        self.assertContains(rendered, '<button type=\"submit\" name=\"_save\" class=\"default button\">Save</button>')\n", "    def test_prepopulated_fields_js(self):\n        request = self.request_factory.get(reverse('admin:articles_article_add'))\n        request.user = User.objects.create_user('testuser', password='password')\n        response = self.client.post(reverse('admin:articles_article_add'), data={'title': 'Test Title'})\n        context = response.context\n        self.assertIn('prepopulated_fields_json', context)\n"], "sample_133": ["    def test_set_language_redirect_with_next(self):\n        factory = RequestFactory()\n        request = factory.post(reverse('set_language'), {'next': '/some/path/', 'language': 'fr'})\n        response = set_language(request)\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'http://testserver/some/path/')\n        self.assertEqual(request.session.get(LANGUAGE_SESSION_KEY), 'fr')\n", "    def test_set_language_redirect_invalid(self):\n        inactive_language = self._get_inactive_language_code()\n        response = self.client.post(reverse('set_language'), {\n            'next': '/some/path/',\n            LANGUAGE_QUERY_PARAMETER: inactive_language\n        })\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/some/path/')\n\n", "    def test_set_language_post(self):\n        \"\"\"Test POST request to set_language view.\"\"\"\n        request = self.client.post(reverse('set_language'), {'language': 'de', 'next': '/some/path'})\n        self.assertEqual(request.session[LANGUAGE_SESSION_KEY], 'de')\n        self.assertEqual(get_language(), 'de')\n        self.assertRedirects(request, '/some/path')\n", "    def test_set_language_with_invalid_language(self):\n        with override(LANGUAGE_SESSION_KEY, 'en'):\n            get_language()\n            inactive_language = self._get_inactive_language_code()\n            response = self.client.post(reverse('set_language'), {'language': inactive_language})\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(self.client.session.get(LANGUAGE_SESSION_KEY), 'en')\n\n\n", "    def test_set_language_with_invalid_language_code(self):\n        \"\"\"Test that the set_language view returns a 404 if an invalid language code is provided.\"\"\"\n        response = self.client.post(reverse('set_language'), {'language': 'xx'})\n        self.assertEqual(response.status_code, 404)\n"], "sample_208": ["    def test_rename_field_with_default(self):\n        # Rename a field while also changing its default value.\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_default_renamed_with_default,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name')\n        self.assertOperationAttributes(changes, 'testapp', 0, 0,\n                                        default=models.IntegerField())\n", "    def test_field_rename_with_default(self):\n        \"\"\"\n        Test renaming a field that has a default value.\n        \"\"\"\n        before = [\n            self.author_name_default,\n        ]\n        after = [\n            self.author_name_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name', new_name='full_name')\n\n", "    def test_empty_migration_name(self):\n        before_states = [\n            self.author,\n        ]\n        after_states = [\n            self.author_empty_migration,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name') \n\n\n", "    def test_deconstructible_custom_pk_rename(self):\n        \"\"\"\n        Tests the behavior when renaming a model with a custom primary key,\n        which is deconstructible.\n        \"\"\"\n        before = [\n            self.author_unmanaged_custom_pk\n        ]\n        after = [\n            self.author_renamed_unmanaged_custom_pk\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, old_name='Author', new_name='RenamedAuthor')\n\n", "    def test_rename_field_with_default_value(self):\n        initial_state = [\n            self.author_name,\n            self.book,\n        ]\n        final_state = [\n            self.author_name_with_renamed_field,\n            self.book_with_renamed_author_field,\n        ]\n\n        changes = self.get_changes(initial_state, final_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name')\n"], "sample_912": ["def test_parse_annotation():\n    assert _parse_annotation(\"foo\") == (\"foo\", None)\n    assert _parse_annotation(\"foo: int\") == (\"foo\", \"int\")\n    assert _parse_annotation(\"foo: typing.List[int]\") == (\"foo\", \"typing.List[int]\")\n\n    with pytest.raises(ValueError):\n        _parse_annotation(\": int\")\n", "    def test_parse_annotation(self):\n        assert _parse_annotation(\"None\") == (\"NoneType\", [])\n        assert _parse_annotation(\"str\") == (\"str\", [])\n        assert _parse_annotation(\"typing.List[str]\") == (\"List\", [\"str\"])\n        assert _parse_annotation(\"typing.Dict[str, int]\") == (\"Dict\", [\"str\",\n                                                                       \"int\"])\n        assert _parse_annotation(\"Optional[str]\") == (\"Optional\", [\"str\"])\n        assert _parse_annotation(\"Union[str, int]\") == (\"Union\", [\"str\", \"int\"])\n", "def test_parse_arglist_with_typehint():\n    sig = 'def func(a: int, b: str) -> float:'\n    signode = addnodes.desc_signature(sig, '')\n    _pseudo_parse_arglist(signode, sig.split('(')[1].split(')')[0])\n    assert len(signode) == 3\n\n    assert isinstance(signode[0], desc_parameter)\n    assert signode[0].astext() == 'a: int'\n\n    assert isinstance(signode[1], desc_parameter)\n    assert signode[1].astext() == 'b: str'\n    \n    assert isinstance(signode[2], desc_returns)\n    assert signode[2].astext() == '-> float'\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", None)\n    assert _parse_annotation(\"str\") == (\"str\", None)\n    assert _parse_annotation(\"list[int]\") == (\"list\", \"int\")\n    assert _parse_annotation(\"dict[str, float]\") == (\"dict\", \"str, float\")\n    assert _parse_annotation(\"Optional[str]\") == (\"Optional\", \"str\")\n    assert _parse_annotation(\"Union[int, str]\") == (\"Union\", \"int, str\")\n    assert _parse_annotation(\"Callable[[int], str]\") == (\"Callable\", \"[[int], str]\")\n    assert _parse_annotation(\"Any\") == (\"Any\", None)\n", "def test_parse_annotation():\n    assert _parse_annotation('None') == ('None', None)\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('List[str]') == ('List', 'str')\n    assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n    assert _parse_annotation('Callable[[int, str], bool]') == ('Callable', ((\n        'int', 'str'), 'bool'))\n    assert _parse_annotation('Tuple[Tuple[int, int], str]') == ('Tuple', (('Tuple', ('int',\n        'int')), 'str'))\n"], "sample_518": ["    def test_connectionpatch_coordinates(self):\n        fig, ax = plt.subplots()\n        con = ConnectionPatch(\n            xyA=(0.2, 0.2), xyB=(0.8, 0.8),\n            coordsA=\"axes fraction\", coordsB=\"axes fraction\",\n            axesA=ax, axesB=ax, arrowstyle='-',\n            zorder=10,\n        )\n        ax.add_patch(con)\n        fig.canvas.draw()\n        points = con.get_path().vertices\n        assert_almost_equal(points[0], [(0.2), (0.2)])\n        assert_almost_equal(points[-1], [(0.8), (0.8)])\n\n", "    def test_fancyarrowpatch(self):\n        fig, ax = plt.subplots()\n        arrow = FancyArrowPatch((0.1, 0.1), (0.9, 0.9),\n                              arrowstyle='->',\n                              connectionstyle='arc3',\n                              color='red',\n                              linewidth=2,\n                              mutation_scale=20)\n        ax.add_patch(arrow)\n        ax.set_xlim(0, 1)\n        ax.set_ylim(0, 1)\n\n", "    def test_FancyArrowPatch_set_connectionstyle(self):\n        fig, ax = plt.subplots()\n        # Test that setting connectionstyle works for FancyArrowPatch\n        arrow = FancyArrowPatch((0, 0), (1, 1), connectionstyle=\"arc3\",\n                                arrowstyle=\"->\")\n        ax.add_patch(arrow)\n        ax.set_xlim([-0.5, 1.5])\n        ax.set_ylim([-0.5, 1.5])\n        # Assert that the connectionstyle is correctly set\n\n        assert arrow.get_connectionstyle().__class__.__name__ == \"Arc3\"\n\n\n\n", "def test_PathPatch_transforms():\n    verts = [(0, 0), (1, 0), (1, 1), (0, 1), (0, 0)]\n    codes = [mpath.Path.MOVETO, mpath.Path.LINETO, mpath.Path.LINETO,\n             mpath.Path.LINETO, mpath.Path.CLOSEPOLY]\n    path = mpath.Path(verts, codes)\n    patch = mpatches.PathPatch(path, transform=mtransforms.Affine2D().rotate_deg(45))\n\n    fig, ax = plt.subplots()\n    ax.add_patch(patch)\n    ax.set_xlim(-1, 2)\n    ax.set_ylim(-1, 2)\n", "    def test_patch_bbox(self):\n        rect = Rectangle((0, 0), 1, 1)\n        assert_almost_equal(rect.get_bbox().bounds, (0, 0, 1, 1))\n\n        annulus = Annulus((0, 0), 1, 0.5)\n        assert_almost_equal(annulus.get_bbox().bounds, (-1, -1, 2, 2))\n"], "sample_421": ["    def test_case_expression_o2o_relationship(self):\n        qs = CaseTestModel.objects.annotate(\n            o2o_integer=Case(\n                When(o2o=OuterRef('o2o'), then=F(\"o2o__integer\")),\n                default_value=0,\n            )\n        )\n        self.assertCountEqual(\n            qs.values_list(\"o2o_integer\", flat=True), [1, 2, 3, 2, 3, 3, 1]\n        )\n\n", "    def test_case_when_complex_filtering(self):\n        qs = CaseTestModel.objects.annotate(\n            case_when=Case(\n                When(integer=1, then=Value(\"One\")),\n                When(integer_gt=F(\"integer2\"), then=Value(\"More\")),\n                default=Value(\"Other\"),\n            )\n        )\n        self.assertCountEqual(\n            qs.values(\"case_when\").distinct(),\n            [{\"case_when\": \"One\"}, {\"case_when\": \"More\"}, {\"case_when\": \"Other\"}],\n        )\n\n", "    def test_case_when_multiple(self):\n        results = list(\n            CaseTestModel.objects.annotate(\n                int_case=Case(\n                    When(integer=1, then=Value(\"one\")),\n                    When(integer=2, then=Value(\"two\")),\n                    When(integer=3, then=Value(\"three\")),\n                    default=Value(\"other\"),\n                ),\n                str_case=Case(\n                    When(string=\"1\", then=Value(\"one\")),\n                    When(string=\"2\", then=Value(\"two\")),\n                    When(string=\"3\", then=Value(\"three\")),\n                    default=Value(\"other\"),\n                ),\n            )\n            .values(\"integer\", \"string\", \"int_case\", \"str_case\")\n            .order_by(\"integer\")\n        )\n        self.assertEqual(\n            results,\n            [\n                {\"integer\": 1, \"string\": \"1\", \"int_case\": \"one\", \"str_case\": \"one\"},\n                {\"integer\": 2, \"string\": \"2\", \"int_case\": \"two\", \"str_case\": \"two\"},\n                {\"integer\": 2, \"string\": \"2\", \"int_case\": \"two\", \"str_case\": \"two\"},\n                {\"integer\": 3, \"string\": \"3\", \"int_case\": \"three\", \"str_case\": \"three\"},\n                {\"integer\": 3, \"string\": \"3\", \"int_case\": \"three\", \"str_case\": \"three\"},\n                {\"integer\": 3, \"string\": \"3\", \"int_case\": \"three\", \"str_case\": \"three\"},\n                {\"integer\": 4, \"string\": \"4\", \"int_case\": \"other\", \"str_case\": \"other\"},\n            ],\n        )\n\n", "    def test_when_condition_with_f(self):\n        result = CaseTestModel.objects.annotate(\n            when_case=Case(\n                When(F(\"integer\") + F(\"integer2\") > 6, then=Value(\"greater\")),\n                When(F(\"integer\") + F(\"integer2\") == 6, then=Value(\"equal\")),\n                default=Value(\"less\"),\n            )\n        )\n\n        self.assertEqual(\n            result.filter(when_case=\"greater\").count(),\n            1,\n        )\n        self.assertEqual(\n            result.filter(when_case=\"equal\").count(),\n            2,\n        )\n        self.assertEqual(\n            result.filter(when_case=\"less\").count(),\n            4,\n        )\n\n", "    def test_case_when_equal_to_value(self):\n        self.assertEqual(\n            CaseTestModel.objects.annotate(\n                case_when=Case(\n                    When(integer=1, then=Value(\"one\")),\n                    When(integer=2, then=Value(\"two\")),\n                    default=Value(\"other\"),\n                )\n            ).order_by(\"id\").values_list(\"case_when\", flat=True),\n            [\"one\", \"two\", \"two\", \"other\", \"other\", \"other\", \"other\"],\n        )\n"], "sample_419": ["    def test_formset_factory_absolute_max(self):\n        class MyForm(Form):\n            field = CharField()\n\n        MyFormSet = formset_factory(\n            MyForm, absolute_max=5, max_num=10\n        )\n\n        formset = MyFormSet(data={'form-0-field': 'a'})\n        self.assertEqual(formset.total_form_count(), 1)\n        # absolute_max limits the maximum number of forms even if max_num is higher\n\n        formset = MyFormSet(data={f'form-{i}-field': 'a' for i in range(6)})\n        self.assertEqual(formset.total_form_count(), 5)\n\n", "    def test_management_form_data_missing(self):\n        formset = self.make_choiceformset(\n            formset_data=[], total_forms=1, initial_forms=1\n        ).management_form\n\n        formset.data = {\n            \"TOTAL_FORMS\": \"1\",\n            \"INITIAL_FORMS\": \"0\",\n            \"MIN_NUM_FORMS\": \"0\",\n            \"MAX_NUM_FORMS\": \"0\",\n        }\n\n        with self.assertRaises(ValidationError) as context:\n            formset.full_clean()\n\n        self.assertEqual(\n            context.exception.message,\n            \"ManagementForm data is missing or has been tampered with. Missing fields: choice-0, votes-0.\",\n        )\n\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(\n            Choice,\n            max_num=5,\n            absolute_max=3,\n        )\n        formset = formset_class(data={'choices-TOTAL_FORMS': '4'})\n        self.assertEqual(formset.errors, [\n            [{'TOTAL_FORMS': [ngettext_lazy('Please submit at most %(num)d form.', 'Please submit at most %(num)d forms.', 'num', 3) % {'num': 3}]}],\n        ])\n", "    def test_formset_factory_absolute_max(self):\n        # Test that absolute_max limits the number of forms that can be\n        # instantiated, even if max_num is higher.\n        formset_class = formset_factory(\n            Choice, max_num=10, absolute_max=5\n        )\n        formset = formset_class(data={\"TOTAL_FORMS\": \"10\"})\n        self.assertEqual(formset.total_form_count(), 5)\n        self.assertEqual(len(formset.forms), 5)\n\n", "    def test_formset_with_initial_data(self):\n        data = {\n            \"choices-0-choice\": \"Coke\",\n            \"choices-0-votes\": \"10\",\n            \"choices-1-choice\": \"Pepsi\",\n            \"choices-1-votes\": \"5\",\n        }\n        formset = self.make_choiceformset(formset_data=[(\"Coke\", 10), (\"Pepsi\", 5)], initial_forms=2)\n        self.assertEqual(formset.data, data)\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(len(formset.cleaned_data), 2)\n"], "sample_371": ["    def test_callable_setting_wrapper(self):\n            return 'hello'\n\n        wrapped = CallableSettingWrapper(my_callable)\n        self.assertEqual(repr(wrapped), repr(my_callable))\n        self.assertEqual(str(wrapped), str(my_callable))\n", "    def test_callable_setting_wrapper(self):\n        wrapped = CallableSettingWrapper(lambda: 'callable')\n        self.assertEqual(repr(wrapped), '<callable>')\n", "    def test_callable_setting_wrapper_repr(self):\n        wrapped = CallableSettingWrapper(lambda x: x)\n        self.assertEqual(repr(wrapped), repr(lambda x: x))\n", "    def test_callable_setting_wrapper_does_not_call_callable(self):\n        class FakeCallable:\n                self.called = False\n\n                self.called = True\n\n        wrapper = CallableSettingWrapper(FakeCallable())\n        self.assertFalse(wrapper.called)\n", "    def test_callable_setting_wrapper_does_not_call_on_repr(self):\n        class SomeCallable:\n                raise AssertionError('Should not be called')\n\n                return 'SomeCallable'\n\n        wrapper = CallableSettingWrapper(SomeCallable())\n        self.assertEqual(repr(wrapper), 'SomeCallable')\n"], "sample_1113": ["    def test_block_collapse_transpose_trace():\n        M = BlockMatrix([[A, B], [C, D]])\n        assert block_collapse(M.transpose()).trace() == block_collapse(M).transpose().trace()\n", "def test_block_collapse_transpose_twice():\n    A = BlockMatrix([[X1, X2], [X3, X4]])\n    B = A.transpose()\n    C = block_collapse(B.transpose())\n    assert C == A\n\n\n", "def test_block_collapse_issue_16123():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    C = BlockMatrix([[A, ZeroMatrix(2, 2)],\n                     [ZeroMatrix(2, 2), B]])\n    D = BlockMatrix([[Identity(2), ZeroMatrix(2, 2)],\n                     [ZeroMatrix(2, 2), Identity(2)]])\n\n    assert block_collapse(D*C) == BlockMatrix([[A, ZeroMatrix(2, 2)],\n                                             [ZeroMatrix(2, 2), B]])\n", "def test_block_collapse_trace():\n    A = BlockMatrix([[X1, X2], [X3, X4]])\n    assert trace(block_collapse(A)) == Trace(X1) + Trace(X4)\n\n    B = BlockMatrix([[A, ZeroMatrix(m, m)], [ZeroMatrix(m, m), A]])\n    assert trace(block_collapse(B)) == 2*Trace(X1) + 2*Trace(X4)\n\n", "def test_blockdiagmatrix_inverse():\n    B = BlockDiagMatrix(X1, X2)\n    assert isinstance(B.inverse(), BlockDiagMatrix)\n    assert B.inverse().args[0] == X1.inverse()\n    assert B.inverse().args[1] == X2.inverse()\n\n"], "sample_147": ["    def test_prefetch_related_empty_results_to_empty_results(self):\n        qs = Number.objects.filter(num=100)\n        qs = qs.prefetch_related('related_numbers')\n        self.assertSequenceEqual(list(qs), [])\n", "    def test_union_with_subquery(self):\n        # Test union with a subquery referencing another table\n        subquery = ReservedName.objects.filter(name__endswith='1').values('id')\n        queryset = Number.objects.filter(id__in=subquery).union(Number.objects.filter(num__lt=5))\n        self.assertNumbersEqual(queryset, [0, 1, 2, 3, 4, 1, 2, 3, 4, 5])\n", "    def test_union_with_empty_result(self):\n        # Tests union with one empty result\n        self.assertNumbersEqual(\n            Number.objects.filter(num=5).union(Number.objects.filter(num=15)),\n            [Number(num=5)],\n        )\n\n", "    def test_union_different_model(self):\n        with self.assertRaises(ValueError):\n            Number.objects.union(ReservedName.objects.all())\n", "    def test_union_with_select_related(self):\n        numbers1 = Number.objects.filter(num__lt=5).select_related('related')\n        numbers2 = Number.objects.filter(num__gt=5).select_related('related')\n        all_numbers = numbers1.union(numbers2)\n        self.assertNumbersEqual(all_numbers,\n                                list(Number.objects.all().values('num')),\n                                ordered=False)\n"], "sample_919": ["    def test_function_pointer_param_no_paren():\n        check('function', 'void foo(void (*func)(int));',\n              {1: 'foo', 2: 'foo'},\n              '''\n              void foo(void (*func)(int));\n              ''', key='foo ')\n", "    def test_enum_nested_in_class():\n        check('enum',\n              \"\"\"\n              enum class Outer {\n\n              };\n              \"\"\",\n              {\n                  1: 'outer'\n              },\n              output=\"\"\"\n              enum class Outer {\n\n              };\n              \"\"\",\n              key='Outer')\n", "    def test_functionPointer(self):\n        check('function', 'int (*funcPtr)(int, float)',\n              idDict={1: 'funcPtr'},\n              output='int (*funcPtr)(int, float)',\n              asTextOutput='int (*funcPtr)(int, float)')\n", "def test_cpp_class():\n    check(\n        \"class\",\n        \"class MyClass {{ public: int i; }};\",\n        {1: \"MyClass\"},\n        output=\"class MyClass {{ public: int i; }};\",\n        asTextOutput=\"MyClass\",\n    )\n", "def test_cpp_function_ptr():\n    check('function',\n          \n          \"void (*p)(int)\", \n          {1: \"p\"},\n          \"void (*p)(int)\",\n          asTextOutput=\"void (*p)(int)\")\n"], "sample_778": ["    def test_n_components_error(self):\n        X = np.random.rand(10, 10)\n        model = NMF(n_components=12, init='random')\n        assert_raise_message(ValueError,\n                             \"Number of components must be a \"\n                             \"positive integer; got (n_components=12)\",\n                             model.fit, X)\n\n", "    def test_n_components_error():\n        X = np.random.rand(10, 5)\n        model = NMF(n_components=6)\n        assert_raise_message(ValueError,\n                            \"Number of components must be a positive integer;\"\n                            \" got (n_components=6)\",\n                            model.fit, X)\n", "    def test_fit_transform_sparse_input_cd(self):\n        X = sp.csr_matrix([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n        model = NMF(n_components=2, init='nndsvd', solver='cd', random_state=0)\n        W = model.fit_transform(X)\n\n        assert isinstance(W, np.ndarray)\n        assert W.shape == (6, 2)\n\n", "    def test_n_components_output():\n        X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n        model = NMF(n_components=2, init='random', random_state=0)\n\n        # Test that the shape of components_ is correct\n        W = model.fit_transform(X)\n        assert model.components_.shape[0] == 2, \\\n            \"Number of components should match n_components\"\n\n", "    def test_n_components_auto_cd(self):\n        X = np.random.rand(100, 10)\n        model = NMF(n_components='auto', solver='cd', init='random',\n                    random_state=42, max_iter=1)\n        model.fit(X)\n        assert model.n_components == X.shape[1]\n\n"], "sample_355": ["    def test_get_user_permissions_with_backend(self):\n        # Test if the SimpleBackend is used to retrieve user permissions.\n        user_permissions = self.user.get_user_permissions()\n        self.assertIn('user_perm', user_permissions)\n", "    def test_get_group_permissions_anonymous_user(self):\n        anon = AnonymousUser()\n        self.assertEqual(\n            list(SimpleBackend().get_group_permissions(anon)),\n            []\n        )\n", "    def test_get_user_permissions(self):\n        # Test if SimpleBackend is used and returns expected permissions.\n        user_perms = self.user.get_user_permissions()\n        self.assertIn('user_perm', user_perms)\n", "    def test_get_user_permissions(self):\n        with self.settings(AUTH_USER_MODEL='auth_tests.test_auth_backends.CustomUser'):\n            CustomUser.objects.create_user('test', 'test@example.com', 'test')\n            user = authenticate(username='test', password='test')\n            self.assertEqual(user.get_user_permissions(), ['user_perm'])\n", "    def test_get_user_permissions(self):\n        with self.assertNumQueries(1):\n            self.assertEqual(self.user.get_user_permissions(), ['user_perm'])\n"], "sample_1115": ["def test_TensorExpr_equals():\n    i, j, k = symbols('i j k')\n    A = TensorHead('A', [TensorIndexType('Lorentz')]*2)\n    B = TensorHead('B', [TensorIndexType('Lorentz')]*2)\n    t1 = A(i, j)\n    t2 = A(j, i)\n    t3 = B(i, j)\n    assert _is_equal(t1, t1)\n    assert not _is_equal(t1, t2)\n    assert not _is_equal(t1, t3)\n\n", "    def test_riemann_cyclic_replace(self):\n        Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n        i, j, k, l = tensor_indices('i,j,k,l', Lorentz)\n        R = TensorHead('R', [Lorentz]*4, TensorSymmetry.riemann())\n        t = R(i,j,k,l)*(R(-i,-j,-k,-l) - 2*R(-i,-k,-j,-l))\n        t=riemann_cyclic_replace(t)\n        t = t.canon_bp()\n        assert t == S.Zero\n", "    def test_component_substitution(self):\n        m, n = symbols('m n', cls=Integer)\n        L = TensorIndexType('L')\n\n        i, j, k, l = tensor_indices('i j k l', L)\n\n        A = TensorHead('A', [L], TensorSymmetry.fully_symmetric(1))\n        B = TensorHead('B', [L, L], TensorSymmetry.fully_symmetric(2))\n\n        expr = A(i) * B(i, j)\n\n        expected_expr = A(m) * B(m, n)\n\n        substitutions = {i: m, j: n}\n        actual_expr = expr.substitute_indices(*substitutions.items())\n\n        assert _is_equal(actual_expr, expected_expr)\n", "def test_tensor_data_substitution():\n    Lorentz = TensorIndexType('Lorentz', dummy_name='L')\n    i, j, k = tensor_indices('i j k', Lorentz)\n    A = TensorHead('A', [Lorentz], TensorSymmetry.traceless())\n    B = TensorHead('B', [Lorentz]*2)\n    C = TensorHead('C', [Lorentz]*3)\n    t = A(i)*B(i,j)*C(j, k,k)\n\n    # Test substitution with a dictionary\n    subs_dict = {A(i): i**2, B(i,j): i*j, C(j,k,k): j**2 * k**2}\n    t_sub = t.subs(subs_dict)\n    assert t_sub == i**2 * i * j * j**2 * k**2\n\n    # Test substitution with a lambda function\n    t_sub = t.subs(lambda x: x[0]**2 if isinstance(x, TensorIndex) else x)\n    assert t_sub == A(i)**2 * B(i, j) * C(j, k, k)\n", "def test_canon_bp_empty():\n    i = Symbol('i')\n    j = Symbol('j')\n    A = TensorHead('A', [TensorIndexType('Lorentz', dummy_name='L')]*2)\n    expr = A(i, j)\n    expr = expr.canon_bp()\n    assert expr.equals(A(i, j))\n\n"], "sample_636": ["def test_similarity_with_ignore_docstrings(linter, tempdir):\n    tempdir.create_file(\"file1.py\", \"\"\"\n        '''This is a docstring'''\n        print('Hello')\n    \n        '''This is another docstring'''\n        print('World')\n\n    \"\"\")\n    tempdir.create_file(\"file2.py\", \"\"\"\n        '''This is a docstring'''\n        print('Hello')\n    \n        '''This is a docstring'''\n        print('World')\n\n    \"\"\")\n    expected = \"\"\"\n    RP0801: Duplication\n    now\n    previous\n    difference\n\n    | Duplicated Lines | 2 | 0 | 2 |\n    |------------------|---|---|---|\n\n    \"\"\"  \n    with _patch_streams(StringIO()) as out:\n        Run([\"--duplicates=2\", \"--ignore-docstrings\", str(tempdir.path), \"file1.py\", \"file2.py\"])\n        assert expected == out.getvalue()\n", "    def test_issue_763(self, out: TextIO) -> None:\n        with _patch_streams(out):\n            Run([\"--duplicates=3\", join(DATA, \"issue_763.py\")])\n        output = out.getvalue()\n        assert output.count(\"==issue_763.py:[2:5]\") == 1\n", "    def test_issue_1794(self, linter) -> None:\n        with _patch_streams(StringIO()) as out:  \n            linter.check(join(DATA, \"issue1794.py\"))\n            reports = out.getvalue()\n            # ensure no false positive for similar code blocks\n            assert \"R0801\" not in reports\n\n", "    def test_similar_code_ignore_imports(self) -> None:\n        \"\"\"Test that ignore-imports works.\"\"\"\n        with _patch_streams(StringIO()) as out:\n            Run([\"--score=n\", \"--duplicates=3\", \"--ignore-imports\", join(DATA, \"import_similarities.py\")])\n        output = out.getvalue()\n        assert \"R0801: Similar lines in 2 files\" not in output\n\n\n\n", "    def test_duplicate_code_with_comments(self, out, capsys):\n       with _patch_streams(out):\n           Run(['--output-format=colorized', '-d', '3', join(DATA, 'file1.py') , join(DATA, 'file2.py')])\n       out = capsys.readouterr().out\n       assert \"R0801\" in out\n       assert \"file1.py:[2:4]\" in out\n       assert \"file2.py:[2:4]\" in out\n\n"], "sample_628": ["    def test_wrong_spelling_in_docstring_with_suggestion(self):\n        self.checker.spelling_dict = enchant.Dict(spell_dict)\n        source = \"\"\"\n            \"\"\"This is a docstring with a mispelled word.\"\"\"\n            pass\n        \"\"\"\n        expected = [\n            Message(\n                \"wrong-spelling-in-docstring\",\n                line=2,\n                column=4,\n                text=\"Wrong spelling of a word 'my_functin' in a docstring:\\nThis is a docstring with a mispelled word.\\n    ^~~\\nDid you mean: 'function' or 'functin'?\",\n            ),\n        ]\n        self.assert_messages(source, expected)\n", "    def test_spelling_suggestion_limit(self):\n        set_config(spelling_dict=spell_dict, max_spelling_suggestions=2)\n        checker = self.checker\n\n        wrong_word = \"misspelled\"\n        suggestions = checker.spelling_dict.suggest(wrong_word)\n        \n        with self.assertAddsMessages(\n            Message(\n                msg_id=\"wrong-spelling-in-comment\", \n                line=1,\n                args=(wrong_word, \"This is a comment with misspelled\", \" ^^^^^^^^^^^^^^^\", self._get_msg_suggestions(wrong_word, 2)),\n            )\n        ):\n            self.checker.process_tokens(\n                _tokenize_str(\"This is a comment with %s\" % wrong_word)\n            )\n\n", "    def test_spelling_private_dict_file(self):\n        \"\"\"Check if private dictionary works.\"\"\"\n        self.checker.open()\n        self.checker.spelling_dict.add(\"w0rd\")\n        self.checker.close()\n        # Reopen to simulate running pylint again\n\n        # Reset the private dict file to empty and add a word\n\n        self.checker.open()\n\n        lines = [\n            \"# This is a comment with a misspelling w0rd\",\n        ]\n        for line in lines:\n            self.checker._check_spelling(\n                \"wrong-spelling-in-comment\", line, 1\n            )\n\n        self.assertEqual(\n            self.checker.messages,\n            [],\n        )\n\n", "    def test_C0402_wrong_spelling_in_docstring_suggestion_limit(self):\n        code = \"\"\"", "    def test_spelling_checker_camel_case(self):\n        self.checker.spelling_dict = enchant.Dict(spell_dict)\n        self.checker.tokenizer = spelling.get_tokenizer(spell_dict)\n        stmt = astroid.parse(\"def foo():\\n # This is a MyVariableName\\n pass\")\n        self.checker._check_spelling(\n            \"wrong-spelling-in-comment\", \"# This is a MyVariableName\", 2\n        )\n        self.assert_no_messages()\n\n"], "sample_983": ["    def test_row_swap(self):\n        a = SparseMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        a.row_swap(0, 2)\n        assert a == SparseMatrix([[7, 8, 9], [4, 5, 6], [1, 2, 3]])\n        a.row_swap(1, 1)\n        assert a == SparseMatrix([[7, 8, 9], [4, 5, 6], [1, 2, 3]])\n        raises(IndexError, lambda: a.row_swap(3, 1))\n        raises(IndexError, lambda: a.row_swap(1, 3))\n", "    def test_row_swap(self):\n        M = SparseMatrix([[1, 2], [3, 4]])\n        M.row_swap(0, 1)\n        assert M == SparseMatrix([[3, 4], [1, 2]])\n\n        M = SparseMatrix([[1, 2, 3], [4, 5, 6]])\n        M.row_swap(0, 1)\n        assert M == SparseMatrix([[4, 5, 6], [1, 2, 3]])\n", "def test_row_swap():\n    M = SparseMatrix([[1, 2], [3, 4]])\n    M.row_swap(0, 1)\n    assert M == SparseMatrix([[3, 4], [1, 2]])\n\n", "    def test_zip_row_op_sparse(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        A.zip_row_op(0, 1, lambda v, u: v + u)\n        assert A == Matrix([[4, 6], [3, 4]])\n", "def test_row_op():\n    M = SparseMatrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    M.row_op(1, lambda v, j: v*2)\n    assert M == SparseMatrix([[1, 2, 3], [8, 10, 12], [7, 8, 9]])\n    M.row_op(2, lambda v, j: v + M[1, j])\n    assert M == SparseMatrix([[1, 2, 3], [8, 10, 12], [15, 18, 21]])\n\n"], "sample_743": ["    def test_radius_neighbors_graph_sparse_float(\n        self,", "def test_kneighbors_graph_weights():\n    # test weighted kneighbors_graph\n\n    for algorithm in ALGORITHMS:\n        knn = neighbors.KNeighborsClassifier(n_neighbors=3,\n                                           algorithm=algorithm,\n                                           weights='distance',\n                                           metric='euclidean')\n        knn.fit(iris.data)\n\n        A = knn.kneighbors_graph(iris.data)\n\n        # check that the weights are proportional to the inverse square of the\n        # distances\n        distances = pairwise_distances(iris.data)\n        for i in range(distances.shape[0]):\n            for j in range(distances.shape[1]):\n                if i != j:\n                    expected_weight = 1.0 / distances[i, j] ** 2\n                    assert_almost_equal(A[i, j], expected_weight)\n\n                else:\n                    assert_equal(A[i, j], 0)\n\n\n        knn = neighbors.KNeighborsClassifier(n_neighbors=3,\n                                           algorithm=algorithm,\n                                           weights=_weight_func,\n                                           metric='euclidean')\n        knn.fit(iris.data)\n\n        A = knn.kneighbors_graph(iris.data)\n\n        # check that the weights are proportional to the inverse square of the\n        # distances\n        distances = pairwise_distances(iris.data)\n        for i in range(distances.shape[0]):\n            for j in range(distances.shape[1]):\n                if i != j:\n                    expected_weight = _weight_func(distances[i, j])\n                    assert_almost_equal(A[i, j], expected_weight)\n\n                else:\n                    assert_equal(A[i, j], 0)\n", "    def test_radius_neighbors_graph_sparse(self):\n        # Test radius_neighbors_graph with sparse matrices\n\n        n_samples = 10\n        n_features = 5\n        X = csr_matrix(rng.rand(n_samples, n_features))\n\n        # Create a NearestNeighbors instance\n        neigh = neighbors.NearestNeighbors(radius=1.5, metric='euclidean')\n        neigh.fit(X)\n\n        # Calculate the radius neighbors graph\n        graph = neigh.radius_neighbors_graph(X)\n\n        # Check that the graph is sparse\n        assert_true(issparse(graph))\n\n        # Check that the graph has the correct shape\n        assert_equal(graph.shape, (n_samples, n_samples))\n\n        # Check that the graph is symmetric\n        assert_array_equal(graph, graph.T)\n\n        # Check that the diagonal elements are zero\n        assert_array_equal(graph.diagonal(), np.zeros(n_samples))\n\n", "def test_radius_neighbors_graph_sparse_data():\n    # Test radius_neighbors_graph for sparse input data.\n    X = csr_matrix(rng.rand(10, 3))\n    neigh = neighbors.RadiusNeighbors(radius=0.5)\n    neigh.fit(X)\n    A = neigh.radius_neighbors_graph(X)\n    assert_true(issparse(A))\n\n", "def test_radius_neighbors_graph_sparse_inputs():\n    # Check that radius_neighbors_graph works with sparse input matrices\n    for sparse_type in SPARSE_TYPES:\n        X = sparse_type(iris.data)\n        neigh = neighbors.NearestNeighbors(radius=1.0)\n        neigh.fit(X)\n        A = neigh.radius_neighbors_graph(X)\n        assert_true(issparse(A))\n"], "sample_289": ["    def test_orderedset_comparison(self):\n        self.assertEqual(OrderedSet([1, 2, 3]), OrderedSet([1, 2, 3]))\n        self.assertEqual(OrderedSet([1, 2, 3]), OrderedSet([3, 2, 1]))\n        self.assertNotEqual(OrderedSet([1, 2, 3]), OrderedSet([1, 2, 4]))\n\n", "    def test_orderedset_discard(self):\n        s = OrderedSet([1, 2, 3, 4])\n        s.discard(2)\n        self.assertEqual(list(s), [1, 3, 4])\n        s.discard(5)\n        self.assertEqual(list(s), [1, 3, 4])\n", "    def test_orderedset_union(self):\n        s1 = OrderedSet([1, 2, 3])\n        s2 = OrderedSet([3, 4, 5])\n        s3 = s1 | s2\n        self.assertEqual(list(s3), [1, 2, 3, 4, 5])\n", "    def test_orderedset_discard(self):\n        s = OrderedSet([1, 2, 3, 4, 5])\n        s.discard(3)\n        self.assertNotIn(3, s)\n        s.discard(6)\n        self.assertEqual(list(s), [1, 2, 4, 5])\n", "    def test_ordered_set_copy(self):\n        s = OrderedSet([1, 2, 3])\n        s_copy = copy.copy(s)\n        self.assertEqual(s_copy, s)\n        self.assertNotSame(s_copy, s)\n"], "sample_384": ["    def test_bulk_update_with_related_model_using_only_select_related(self):\n        related_objects = [\n            RelatedObject.objects.create(note=note) for note in self.notes\n        ]\n        notes = Note.objects.select_related('related_objects')\n        # Only use the note field for the update\n        updated_notes = notes.filter(id__in=[n.id for n in self.notes[:5]]).update(\n            note='Updated'\n        )\n        self.assertEqual(updated_notes, 5)\n", "    def test_bulk_update_related_objects(self):\n        notes = Note.objects.all()\n        tags = self.tags[:5]\n        notes.update(tags=tags)\n        for note in notes:\n            self.assertEqual(note.tags.all(), tags)\n\n", "    def test_bulk_update_related_objects(self):\n        for note in self.notes:\n            note.tags.add(self.tags[0])\n\n        # Update the 'note' field of all notes related to a specific tag.\n        Note.objects.filter(tags__name=self.tags[0].name).update(note='Updated')\n\n        # Reload the notes\n        notes = Note.objects.all()\n        self.assertEqual(notes.count(), 10)\n        self.assertEqual(notes[0].note, 'Updated')\n", "    def test_bulk_update_related_objects(self):\n        for note in self.notes:\n            note.tags.add(self.tags[note.id % len(self.tags)])\n        Note.objects.filter().update(\n            note=F(\"note\") + \" updated\", misc=F(\"misc\") + \" updated\"\n        )\n        for note in Note.objects.all():\n            self.assertIn(\"updated\", note.note)\n            self.assertIn(\"updated\", note.misc)\n", "    def test_bulk_update_with_related_fields_filtering(self):\n        # Create some related objects\n        for i, note in enumerate(self.notes):\n            for j in range(i % 3):\n                note.tags.add(self.tags[j])\n        \n        # Filter notes by tag and update\n        updated_notes = Note.objects.select_related('tags').filter(tags__name='0').update(\n            note='Updated note'\n        )\n        self.assertEqual(updated_notes, 3)\n        for i, note in enumerate(self.notes):\n            if i % 3 == 0:\n                self.assertEqual(note.note, 'Updated note')\n            else:\n                self.assertNotEqual(note.note, 'Updated note')\n"], "sample_705": ["def test_spawn_pytest_timeout(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    with pytest.raises(subprocess.TimeoutExpired) as e:\n        testdir.spawn_pytest(\n            \"-k test_myfunc\", expect_timeout=0.1\n        ).wait()\n    assert 'TimeoutExpired' in str(e.value)\n", "    def test_runpytest_subprocess_timeout(self, pytester: Pytester) -> None:\n        \"\"\"Test that runpytest_subprocess() times out correctly.\"\"\"\n        pytester.makepyfile(\n            f\"\"\"\n            import time\n                time.sleep(10)\n        \"\"\"\n        )\n        with pytest.raises(pytester.Pytester.TimeoutExpired):\n            pytester.runpytest_subprocess(timeout=2)\n", "    def test_spawn_subprocess_timeout(self) -> None:\n        \"\"\"Test that spawn_pytest times out correctly.\"\"\"\n        subprocess.run([\"sleep\", \"10\"], check=True)  # Ensure sleep is available\n        with self.pushd(self.tmpdir):\n            script = \"\"\"\n            import time\n\n            time.sleep(5)\n            print(\"This should not be printed\")\n            \"\"\"\n            self.makepyfile(script)\n            with pytest.raises(\n                pytester_mod.Pytester.TimeoutExpired\n            ) as excinfo:\n                self.spawn_pytest(\n                    \"-s\",\n                    \"test_script.py\",\n                    expect_timeout=2,\n                )\n            assert \"timeout expired\" in str(excinfo.value)\n\n", "    def test_run_with_log_file(\n        self, testdir: Testdir", "def test_spawn_pytest_timeout(testdir: Testdir) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        import time\n            time.sleep(10)\n    \"\"\"\n    )\n    result = testdir.spawn_pytest(\n        \"-s\", \"--timeout=2\", testdir.tmpdir\n    )\n    result.expect_exact(\"KeyboardInterrupt\")\n    # expect non-zero exit code\n    assert result.status != 0\n    assert result.is_timeout\n"], "sample_93": ["    def test_json_field_lookup(self):\n        from django.db.models import JSONField\n        Author.add_to_class('data', JSONField())\n        self.a1.data = {'key': 'value'}\n        self.a1.save()\n        self.assertEqual(Author.objects.filter(data__key='value').count(), 1)\n", "    def test_case_expression(self):\n        qs = Book.objects.annotate(\n            average_age=Case(When(authors__age__gt=30, then=F('authors__age')), default=0),\n        ).filter(average_age__gt=30).order_by('average_age')\n        self.assertQuerysetEqual(qs, [\n            'Python Web Development with Django',\n            'Practical Django Projects',\n            'The Definitive Guide to Django: Web Development Done Right',\n            'Sams Teach Yourself Django in 24 Hours',\n        ], transform=lambda b: b.name)\n\n", "    def test_case_when_field_lookup(self):\n        youngsters = Author.objects.annotate(\n            is_young=Case(\n                When(age__lt=30, then=Value(True)),\n                default=Value(False),\n                output_field=BooleanField(),\n            )\n        )\n        self.assertEqual(youngsters.filter(is_young=True).count(), 3)\n", "    def test_window_function(self):\n        q = Book.objects.annotate(\n            rank=Window(\n                Func(F('price'), function='RANK'),\n                partition_by=F('publisher'),\n                order_by=F('price').desc(),\n            )\n        ).filter(rank=1)\n        self.assertQuerysetEqual(\n            q.order_by('publisher', 'price'),\n            [\n                ('Apress', 'The Definitive Guide to Django: Web Development Done Right'),\n                ('Morgan Kaufmann', 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp'),\n                ('Prentice Hall', 'Artificial Intelligence: A Modern Approach'),\n                ('Sams', 'Sams Teach Yourself Django in 24 Hours'),\n            ],\n            transform=lambda b: (b.publisher.name, b.name),\n        )\n\n", "    def test_window_functions(self):\n        with CaptureQueriesContext(connection) as captured:\n            qs = Book.objects.annotate(\n                rank=Window(\n                    Func(F('price'), function='RANK'),\n                    partition_by=F('publisher'),\n                    order_by=F('price').desc(),\n                )\n            ).order_by('publisher', 'rank')\n            self.assertQuerysetEqual(qs, [\n                ('Apress', 'The Definitive Guide to Django: Web Development Done Right', 1),\n                ('Apress', 'Practical Django Projects', 2),\n                ('Sams', 'Sams Teach Yourself Django in 24 Hours', 1),\n                ('Prentice Hall', 'Python Web Development with Django', 1),\n                ('Prentice Hall', 'Artificial Intelligence: A Modern Approach', 2),\n                ('Morgan Kaufmann', 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 1),\n            ], lambda x: (x.publisher.name, x.name, x.rank))\n\n            self.assertEqual(len(captured.captured_queries), 1)\n\n"], "sample_798": ["    def test_ridge_solver(self, solver, filter_func, X, y):\n        n_samples = X.shape[0]\n        X = filter_func(X)\n        y = y.astype(np.float64)\n        # Create a Ridge with a small alpha to avoid issues with singularity\n        ridge = Ridge(solver=solver, alpha=1e-5)\n\n        # Fit the model\n        ridge.fit(X, y)\n\n        # Check if the coefficient are the same\n        if solver == 'cholesky':\n            # Special handling for cholesky as it only works for dense matrices\n            if isinstance(X, sp.csr_matrix):\n                assert_raises(ValueError, ridge.predict, X)\n            else:\n                coefs_true = ridge_regression(X, y, alpha=1e-5)[1]\n                assert_allclose(ridge.coef_, coefs_true)\n        else:\n            coefs_true = ridge_regression(X, y, alpha=1e-5)[1]\n            assert_allclose(ridge.coef_, coefs_true)\n\n        # Test predict\n        predict = ridge.predict(X)\n        assert_equal(predict.shape, (n_samples,))\n", "    def test_ridge_classifiercv_scoring(self):\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([0, 0, 1, 1])\n\n        clf = RidgeClassifierCV(scoring='accuracy')\n        clf.fit(X, y)\n        assert clf.score(X, y) == 1.0\n\n        # Check that custom scorer works\n            y_pred = estimator.predict(X)\n            return np.mean(y_pred == y_true)\n\n        clf = RidgeClassifierCV(scoring=my_scorer)\n        clf.fit(X, y)\n        assert clf.score(X, y) == 1.0\n", "def test_ridge_solver(solver, X_func, random_state=0):\n    n_samples = 50\n    n_features = 30\n    rng = np.random.RandomState(random_state)\n    X = X_func(rng.randn(n_samples, n_features))\n    y = rng.randn(n_samples)\n\n    ridge = Ridge(alpha=1.0, solver=solver, random_state=random_state)\n    ridge.fit(X, y)\n", "    def test_ridge_cv_sparse_cholesky(self):\n        # check that sparse_cg solver raises error when no alpha is given\n        clf = RidgeCV(alphas=None, solver='cholesky')\n        with pytest.raises(ValueError):\n            clf.fit(X_iris, y_iris)\n\n", "    def test_ridge_sparse_solver_irls(self):\n        # Test Ridge with sparse input and IRLS solver\n        X = SPARSE_FILTER(make_regression(n_samples=50, n_features=10,\n                                          random_state=0)[0])\n        y = make_regression(n_samples=50, n_features=10,\n                            random_state=0)[1]\n\n        ridge = Ridge(alpha=1.0, solver='irls')\n        ridge.fit(X, y)\n\n        # Check if the coefficients are not NaN\n        assert not np.isnan(ridge.coef_).any()\n"], "sample_905": ["    def test_signature_from_ast_varargs():\n        code = \"\"\"\n            pass\n        \"\"\"\n        module = ast.parse(code)\n        function = module.body[0]  # type: ignore\n        sig = inspect.signature_from_ast(function, code)\n        assert sig.parameters[0].kind == Parameter.VAR_POSITIONAL\n        assert sig.parameters[0].name == 'args'\n        assert sig.parameters[1].kind == Parameter.VAR_KEYWORD\n        assert sig.parameters[1].name == 'kwargs'\n\n\n", "    def test_stringify_signature_unqualified_typehints(self):\n        sig = inspect.signature(\n            lambda x: None, show_annotation=True, unqualified_typehints=True\n        )\n        assert stringify_signature(sig) == '(x)'\n\n\n\n", "    def test_stringify_signature_unqualified_typehints(self):\n        @ast.parse('def func(x: io.StringIO) -> io.StringIO: pass').body[0]\n            pass\n\n        sig = inspect.signature(func)\n        result = inspect.stringify_signature(sig, unqualified_typehints=True)\n        assert result == '(x: StringIO) -> StringIO'\n\n\n", "def test_unwrap_wrapped():\n        pass\n    \n    @functools.wraps(inner)\n        pass\n\n    assert inspect.unwrap(outer) is inner\n    assert inspect.unwrap_all(outer) is inner\n", "    def test_stringify_signature_unqualified(self):\n        sig = inspect.signature(self.func)\n        assert stringify_signature(sig, unqualified_typehints=True) == (\n            '(a: int, b: Optional[datetime.datetime], '\n            'c: MyEnum, *args: str, **kwargs: typing.Any) -> int'\n        )\n"], "sample_352": ["    def test_where_node_split_having_with_aggregate_and_negation(self):\n        # Test WHERE clause splitting for nodes containing aggregates and negation.\n        node = WhereNode([\n            F('created') + 1,\n            ~Q(count_items=Count('item__id')) > 2\n        ], AND)\n        where_node, having_node = node.split_having()\n        self.assertIsNotNone(where_node)\n        self.assertIsNotNone(having_node)\n        self.assertEqual(len(where_node.children), 1)\n        self.assertEqual(where_node.children[0].lhs.name, 'created')\n        self.assertEqual(len(having_node.children), 1)\n        self.assertEqual(having_node.children[0].lhs.name, 'count_items')\n\n", "    def test_where_node_split_having_with_aggregate(self):\n        node = WhereNode([\n            F('item__created') < datetime.datetime.now(),\n            Count('item__tags') > 2,\n        ], AND, False)\n        where_node, having_node = node.split_having()\n        self.assertEqual(len(where_node.children), 1)\n        self.assertEqual(len(having_node.children), 1)\n        self.assertEqual(where_node.children[0].lhs.output_field.name, 'item__created')\n        self.assertEqual(having_node.children[0].lhs.output_field.name, 'item__tags')\n", "    def test_where_node_split_having_empty(self):\n        node = WhereNode(\n            [\n                F('id'),\n            ],\n            connector=AND,\n        )\n        where_node, having_node = node.split_having()\n        self.assertEqual(where_node, node)\n        self.assertIsNone(having_node)\n\n", "    def test_where_node_split_having_with_aggregate(self):\n        node = WhereNode(\n            [\n                F('item__created'),\n                F('item__modified'),\n            ],\n            connector='AND',\n            negated=False,\n        )\n        where_node, having_node = node.split_having()\n\n        self.assertIsNone(having_node)\n        self.assertEqual(where_node.children, [F('item__created'), F('item__modified')])\n\n        node.children.append(Count('item'))\n        where_node, having_node = node.split_having()\n\n        self.assertIsNone(where_node)\n        self.assertEqual(having_node.children, [Count('item')])\n\n", "    def test_where_split_having_with_aggregate(self):\n        qs = Item.objects.annotate(count=Count('tags')).filter(\n            Q(count__gt=1) | Q(name='one')\n        )\n        with CaptureQueriesContext(connection) as queries:\n            list(qs)\n        self.assertEqual(len(queries), 2)\n"], "sample_1051": ["    def test_dotprint_basic():\n        x = Symbol('x')\n        assert dotprint(x) == \\", "def test_dotprint_repeat_false():\n    x = Symbol('x')\n    expr = x + x*x\n    # This should only have 2 'x' nodes because repeat is False\n    dot_str = dotprint(expr, repeat=False)\n    nodes = [l.split('\"')[1] for l in dot_str.splitlines() if '->' not in l]\n    assert len(nodes) == 3\n    assert 'x_(' in dot_str\n\n", "    def test_dotprint_repeat_false():\n        x = Symbol('x')\n        expr = x + x*x\n        s = dotprint(expr, repeat=False)\n        # Make sure there is only one node for x\n        assert s.count('\"Symbol(\\'x\\')') == 1\n", "    def test_dotprint_repeat_false(self):\n        x = Symbol('x')\n        expr = x + x*x\n        dot = dotprint(expr, repeat=False)\n        self.assertEqual(dot.count('\"x_'), 1)\n", "    def test_dotprint_repeat_false():\n        x = Symbol('x')\n        expr = x + x*x\n        dot = dotprint(expr, repeat=False)\n        assert '\"Symbol(\\'x\\')' in dot\n        assert dot.count('\"Symbol(\\'x\\')') == 1\n"], "sample_158": ["    def test_m2m_through_fields_invalid(self):\n        with mock.patch('django.db.models.fields.LazyOperation') as LazyOperationMock:\n            class Artist(models.Model):\n                name = models.CharField(max_length=100)\n\n            class Album(models.Model):\n                title = models.CharField(max_length=100)\n                artists = models.ManyToManyField(Artist, through='Collaboration', through_fields=('artist', 'album'))\n\n            class Collaboration(models.Model):\n                artist = models.ForeignKey(Artist, on_delete=models.CASCADE)\n                album = models.ForeignKey(Album, on_delete=models.CASCADE)\n\n            msg = (\n                \"Field specifies 'through_fields' but does not provide \"\n                \"the names of the two link fields that should be used \"\n                \"for the relation through model 'Collaboration'.\"\n            )\n            with self.assertRaisesMessage(ValueError, msg):\n                Album._meta.get_field('artists')\n\n            LazyOperationMock.assert_called_once_with(\n                Collaborations,\n                'artist',\n                Album,\n                'artists',\n            )\n", "    def test_many_to_many_field_checks(self):\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n            books = models.ManyToManyField(Book)\n\n        errors = Book._meta.get_field(\"books\").check()\n        self.assertEqual(len(errors), 0)\n\n        class AuthorWithSymmetricalManyToMany(models.Model):\n            name = models.CharField(max_length=100)\n            friends = models.ManyToManyField(\"self\", symmetrical=True)\n\n        errors = AuthorWithSymmetricalManyToMany._meta.get_field(\"friends\").check()\n        self.assertEqual(len(errors), 0)\n\n        class AuthorWithThroughModel(models.Model):\n            name = models.CharField(max_length=100)\n            books = models.ManyToManyField(Book, through='AuthoredBook')\n\n        class AuthoredBook(models.Model):\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n\n        errors = AuthorWithThroughModel._meta.get_field(\"books\").check()\n        self.assertEqual(len(errors), 0)\n\n        # Test with a through model that isn't installed\n        with mock.patch('django.apps.apps.get_models', side_effect=lambda include_auto_created=True: []):\n            with self.assertRaises(checks.Error) as context:\n                class InvalidAuthorWithThroughModel(models.Model):\n                    name = models.CharField(max_length=100)\n                    books = models.ManyToManyField(Book, through='NonExistentThroughModel')\n\n            self.assertIn(\n                \"Field specifies a many-to-many relation through model 'NonExistentThroughModel', which has not been installed.\",\n                str(context.exception)\n            )\n", "    def test_through_fields_with_invalid_fields(self):\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            authors = models.ManyToManyField(Author, through='BookAuthor', symmetrycal=False)\n\n        class BookAuthor(models.Model):\n            book = models.ForeignKey(Book, on_delete=models.CASCADE)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n            genre = models.CharField(max_length=50)\n\n        with self.assertRaises(checks.Error) as cm:\n\n            Book.check()\n        self.assertEqual(cm.exception.msg, \"The intermediary model 'BookAuthor' has no field 'author_to_book'.\"\n                         )\n", "    def test_through_fields_duplicate_names(self):\n        class A(models.Model):\n            pass\n\n        class B(models.Model):\n            pass\n\n        class C(models.Model):\n            a = models.ManyToManyField(A, through='Link', related_name='a')\n            b = models.ManyToManyField(B, through='Link', related_name='b')\n\n        class Link(models.Model):\n            a = models.ForeignKey(A, on_delete=models.CASCADE)\n            b = models.ForeignKey(B, on_delete=models.CASCADE)\n            c = models.ForeignKey(C, on_delete=models.CASCADE)\n\n            class Meta:\n                unique_together = ('a', 'b', 'c')\n\n        with self.assertRaisesMessage(\n            ValueError,\n            'The intermediary model \"Link\" has more than one foreign key '\n            'to \"Link\", which is ambiguous. You must specify '\n            'which foreign key Django should use via the through_fields '\n            'keyword argument.'\n        ):\n            C.objects.create()\n", "    def test_m2m_through_fields_error(self):\n        class A(models.Model):\n            b = models.ManyToManyField('B', through='Link')\n\n        class B(models.Model):\n            pass\n\n        class Link(models.Model):\n            pass\n\n        with self.assertRaisesMessage(\n                ValueError,\n                \"ManyToManyField specifies 'through_fields' but does not provide \"\n                \"the names of the two link fields that should be used for the \"\n                \"relation through model 'Link'.\"\n        ):\n            A.b.through_fields = ('a',)\n\n"], "sample_374": ["    def test_prefetch_related_with_select_related(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            # Ensure select_related doesn't unnecessarily join when prefetching.\n            queryset = Book.objects.select_related('authors').prefetch_related('authors__address')\n            self.assertQuerysetEqual(\n                queryset.all(),\n                [\n                    self.book1,\n                    self.book2,\n                    self.book3,\n                    self.book4,\n                ],\n                transform=lambda o: o.id\n            )\n\n", "    def test_prefetch_related_with_custom_manager(self):\n        # Ensure that prefetch_related works correctly with custom managers.\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            #  We need to ignore the warning raised in Django 4.2\n            qs = Reader.objects.filter(books_read__title=\"Jane Eyre\").prefetch_related('books_read')\n\n        self.assertEqual(\n            qs.get().books_read.all()[0].title,\n            'Jane Eyre'\n        )\n", "    def test_prefetch_related_with_null_values(self):\n        \"\"\"\n        Regression test for: #24473\n\n        When using select_related with a foreign key that has null values,\n        ensure that the related object is correctly set to None.\n\n        \"\"\"\n        # Create an Author with no bio\n        author_no_bio = Author.objects.create(name='No Bio Author')\n\n        # Create an Author with a bio\n        author_with_bio = Author.objects.create(name='Author with Bio')\n        Bio.objects.create(author=author_with_bio, bio='Some bio text')\n\n        authors = Author.objects.prefetch_related('bio').all()\n\n        self.assertEqual(authors[0].name, 'No Bio Author')\n        self.assertIsNone(authors[0].bio)\n        self.assertEqual(authors[1].name, 'Author with Bio')\n        self.assertIsNotNone(authors[1].bio)\n", "    def test_prefetch_related_across_m2m_to_related_model(self):\n        with CaptureQueriesContext(connection) as queries:\n            prefetch_related_objects(\n                [self.reader1],\n                'books_read__authors',\n            )\n\n        self.assertGreater(len(queries.captured_queries), 1)\n        self.assertWhereContains(queries.captured_queries[1], 'authors.name')\n\n", "    def test_prefetch_related_through_related_name(self):\n        with CaptureQueriesContext(connection) as queries:\n            authors = Author.objects.prefetch_related('book__readers').all()\n        self.assertEqual(len(queries), 2)  # 1 for authors, 1 for related\n        self.assertEqual(len(authors), 4)\n        for author in authors:\n            self.assertTrue( hasattr(author, 'book_set'))\n            for book in author.book_set.all():\n                self.assertTrue( hasattr(book, 'readers'))  # prefetched correctly\n\n\n"], "sample_510": ["    def test_scatter_c():\n        x = np.arange(10)\n        y = np.arange(10)\n        c = np.random.rand(10)\n        fig, ax = plt.subplots()\n        ax.scatter(x, y, c=c)\n        assert len(ax.collections) == 1\n        assert ax.collections[0].get_array().size == 10\n\n", "    def test_plot_date_with_tz():\n        # Test plotting dates with timezone information\n        dates = np.array(['2023-10-26T12:00:00-04:00',\n                          '2023-10-27T13:00:00-04:00',\n                          '2023-10-28T14:00:00-04:00'], dtype='datetime64[ns]')\n        values = np.array([10, 20, 30])\n\n        fig, ax = plt.subplots()\n        ax.plot_date(dates, values, tz='US/Eastern')\n        ax.set_xlabel('Date (Eastern Time)')\n\n        # Save the plot to a temporary file\n        temp_file = Path(f'test_plot_date_tz_{np.random.randint(1000)}.png')\n        fig.savefig(temp_file)\n\n        # Check if the plot file exists\n        assert temp_file.exists()\n\n        # Clean up the temporary file\n        temp_file.unlink()\n\n", "    def test_gca_deprecation():\n        with pytest.warns(MatplotlibDeprecationWarning):\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3], [4, 5, 6])\n            plt.gca()\n", "    def test_gca():\n        fig, ax = plt.subplots()\n        assert ax is plt.gca()\n        plt.close(fig)\n", "    def test_quiver_arrowprops():\n        fig, ax = plt.subplots()\n        X, Y = np.meshgrid(np.arange(0, 10, 1), np.arange(0, 10, 1))\n        U = Y\n        V = -X\n\n        # Test basic quiver plot\n\n        ax.quiver(X, Y, U, V, angles='xy', scale_units='xy',\n                  scale=1, pivot='mid', color='r')\n        # Test quiver with arrowprops\n        ax.quiver(X[::2, ::2], Y[::2, ::2], U[::2, ::2], V[::2, ::2],\n                  angles='xy', scale_units='xy',\n                  scale=1, pivot='mid', color='b',\n                  arrowprops=dict(arrowstyle='->', connectionstyle=\"arc3,rad=0.2\"))\n"], "sample_951": ["    def test_stringify_signature_with_forward_ref(self):\n        @functools.partial(inspect.signature)\n            pass\n        sig = foo()\n        result = inspect.stringify_signature(sig)\n        assert result == '(a: mod.Class)'\n", "    def test_isabstractmethod(self):\n        class AbstractBaseClass(abc.ABC):\n            @abc.abstractmethod\n                pass\n\n        class ConcreteClass(AbstractBaseClass):\n                return \"Hello, world!\"\n\n        assert inspect.isabstractmethod(AbstractBaseClass.abstract_method)\n        assert not inspect.isabstractmethod(ConcreteClass.abstract_method)\n\n\n", "def test_stringify_signature_default_values():\n        pass\n\n    sig = inspect.signature(func)\n    assert stringify_signature(sig) == '(a, b=1, *args, c=3, **kwargs)'\n", "    def test_signature_from_str_with_return_annotation(self):\n        sig = inspect.signature_from_str('(x: int, y: str) -> datetime.datetime')\n        assert sig.parameters['x'].annotation == int\n        assert sig.parameters['y'].annotation == str\n        assert sig.return_annotation == datetime.datetime\n\n", "    def test_signature_evaluate_forwardref(self):\n            pass\n\n        sig = inspect.signature(func)\n        localns = {'List': list, 'Optional': typing.Optional}\n        sig = inspect.evaluate_signature(sig, localns=localns)\n        assert str(sig.parameters['a'].annotation) == \" typing.List[int]\"\n        assert str(sig.parameters['b'].annotation) == \" typing.Optional[str]\"\n\n"], "sample_652": ["    def test_callspec_fixture_names(self, testdir, request):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return 42\n\n                assert myfixture == 42\n        \"\"\"\n        )\n        testdir.runpytest('-v')\n        result = testdir.stdout.str()\n        assert \"test_func\" in result\n        assert \"myfixture\" in result\n", "    def test_fixture_not_found(self, testdir):\n        testdir.create_testfile(\"\"\"\n            import pytest\n\n                pass\n\n        \"\"\")\n        result = testdir.runpytest()\n        result.assert_contains(\"fixtures.FixtureLookupError\")\n        assert result.ret == 1\n", "    def test_parametrize_id_with_idfn(testdir):\n        testdir.makepytestini(textwrap.dedent(\"\"\"\n            [pytest]\n            disable_test_id_escaping_and_forfeit_all_rights_to_community_support = True\n        \"\"\"))\n        testdir.create_test_file(\"\"\"\n            import pytest\n\n            @pytest.mark.parametrize(\"test_input,expected\", [\n                (1, 'one'),\n                (2, 'two'),\n                (3, 'three'),\n            ], ids=['a', 'b', 'c'])\n                assert test_input == expected\n        \"\"\")\n        result = testdir.runpytest(\"-v\")\n        result.stdout.fnmatch_lines([\n            \"*test_simple[a]*\",\n            \"*test_simple[b]*\",\n            \"*test_simple[c]*\",\n        ])\n\n", "    def test_fixture_lookup_error(self):\n            pytest.fixture(scope=\"function\")\n            return 1\n\n        with pytest.raises(FixtureLookupError):\n            func()\n\n", "    def test_parametrize_fixture_indirect_and_non_indirect(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return request.param\n\n            @pytest.fixture\n                return request.param\n\n            @pytest.mark.parametrize(\"a, b\", [(1, 2), (3, 4)])\n                assert a == fixme\n                assert b == indirect_fixme\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=2)\n"], "sample_954": ["    def test_nested_inline_transform(self, app, status, warning):\n        app.build()\n        self.assertEqual(len(app.env.warnings), 0, warning)\n\n        with open(app.outdir / 'man/simple.1', 'r', encoding='utf-8') as f:\n            text = f.read()\n\n        assert '<strong>foo=</strong><emphasis>var</emphasis>' in text\n        assert '<strong>&bar=</strong><emphasis>2</emphasis>' in text\n", "    def test_nested_inline_transform(self, app, status, warning):\n        app.builder.env.docname = 'index'\n        content = \"\"\"\n        <strong>foo=<emphasis>1</emphasis> &bar=<emphasis>2</emphasis></strong>\n        \"\"\"\n        node = nodes.document(content)\n        node[\"settings\"] = Config({}, None).settings\n\n        transform = NestedInlineTransform(node)\n        transform.apply()\n\n        expected_content = \"\"\"\n        <strong>foo=</strong><emphasis>1</emphasis>\n        <strong>&bar=</strong><emphasis>2</emphasis>\n        \"\"\"\n        assert node.astext() == expected_content\n", "def test_nested_inline_transform(app, status, warning):\n    app.builder.config.man_pages = default_man_pages\n\n    # create a simple document with nested inline nodes\n    doc = '<strong>foo=<emphasis>1</emphasis>&bar=<emphasis>2</emphasis></strong>'\n    node = docutils.nodes.paragraph()\n    node.extend(docutils.nodes.literal_block(doc, doc))\n    transform = NestedInlineTransform(node)\n    transform.apply()\n    # verify the transformation\n    expected = '<strong>foo=</strong><emphasis>var</emphasis><strong>&bar=</strong><emphasis>2</emphasis>'\n    assert node.astext() == expected\n", "    def test_nested_inline_transform(self, app, status, warning):\n        app.builder.build_man_pages(default_man_pages)\n        assert '<strong>foo=</strong><emphasis>1</emphasis>' in open(\n            status.getvalue(), 'r').read()\n", "        def test_manpage_title_upper(self, app: Sphinx, option_defaults: Dict[str, Any]) -> None:\n            app.config.title = 'Test Title'\n            app.build()\n            manpage_text = next(app.outdir.glob('test_title.1')).read_text()\n            assert '.TH \"TEST TITLE\"' in manpage_text\n"], "sample_171": ["    def test_migrate_creates_foreign_keys(self):\n        \"\"\"\n        Test that migrate creates foreign keys.\n        \"\"\"\n        # Create a migration that adds a ForeignKey\n        with open(os.path.join(self.migration_dir, '0001_initial.py'), 'w') as f:\n            f.write(\n                'from django.db import migrations, models\\n\\n'\n                'class Migration(migrations.Migration):\\n'\n                '    dependencies = [\\n'\n                '    ]\\n\\n'\n                '    operations = [\\n'\n                '        migrations.CreateModel(\\n'\n                '            name=\\'Foo\\', \\n'\n                '            fields=[ \\n'\n                '                (\\'id\\', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\\'ID\\')),\\n'\n                '                (\\'name\\', models.CharField(max_length=100)),\\n'\n                '            ],\\n'\n                '        ),\\n'\n                '        migrations.CreateModel(\\n'\n                '            name=\\'Bar\\', \\n'\n                '            fields=[ \\n'\n                '                (\\'id\\', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name=\\'ID\\')),\\n'\n                '                (\\'name\\', models.CharField(max_length=100)),\\n'\n                '                (\\'foo\\', models.ForeignKey(\\n'\n                '                    related_name=\\'bars\\', \\n'\n                '                    on_delete=models.CASCADE, \\n'\n                '                    to=\\'Foo\\'\\n'\n\n                '                )),\\n'\n                '            ],\\n'\n                '        ),\\n'\n                '    ]\\n'\n            )\n        # Run the migration\n        call_command('migrate', '--fake', app='migrations')\n\n        # Check if the foreign key constraint exists\n        with connection.cursor() as cursor:\n            cursor.execute('SELECT CONSTRAINT_NAME FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_", "    def test_migrate_fake_initial(self):\n        \"\"\"\n        Tests the --fake-initial option to mark initial migrations as applied.\n\n        \"\"\"\n        with mock.patch(\n            'django.db.migrations.executor.MigrationExecutor.migrate'\n        ) as mock_migrate:\n            call_command('migrate', '--fake-initial', app='migrations')\n\n            mock_migrate.assert_called_once_with(\n                targets=[('migrations', None)],\n                plan=mock.ANY,\n                state=mock.ANY,\n                fake=False,\n                fake_initial=True,\n            )\n\n", "    def test_migrate_fake_initial(self):\n        \"\"\"Tests the migration command when using the --fake-initial flag.\"\"\"\n        # Create a table for a model\n        with connection.cursor() as cursor:\n            cursor.execute(\"\"\"\n                CREATE TABLE migrations_unicodemodel (\n                    id INTEGER PRIMARY KEY,\n                    name VARCHAR(100)\n                );\n            \"\"\")\n\n        # Create a migration file for the model\n        self.migrate('test_migrations', '0001_initial')\n\n        # Migrate with the --fake-initial flag\n\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.apply_migration',\n                       return_value=True), \\\n             mock.patch('django.db.migrations.executor.MigrationExecutor._fake_initial_migration',\n                       return_value=True):\n\n            call_command('migrate', database='default', verbosity=0, fake_initial=True)\n\n        # Check that the migration has been marked as applied\n        recorder = MigrationRecorder(connections['default'])\n\n        applied_migrations = recorder.applied_migrations()\n        self.assertIn(\n            ('test_migrations', '0001_initial'),\n            applied_migrations,\n        )\n", "    def test_fake_initial(self):\n        \"\"\"\n        Tests that fake_initial creates tables if they don't exist.\n\n        The initial migration should be applied without actually running it.\n        \"\"\"\n        # Create a fresh database\n        self.setUpTestDatabase('default')\n\n        with self.assertRaises(DatabaseError):\n            connection.cursor().execute(\"SELECT * FROM migrations_unicodemodel\")\n\n        # Run migrate with --fake-initial\n        call_command('migrate', '--fake-initial', verbosity=0, database='default')\n\n        # Now the table should exist\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM migrations_unicodemodel\")\n            self.assertEqual(cursor.fetchall(), [])\n\n", "    def test_migrate_no_apps(self):\n        with mock.patch('django.db.migrations.executor.MigrationExecutor.loader.graph.leaf_nodes',\n                       return_value=[]):\n            call_command('migrate', app_label='', database=\"default\", stdout=io.StringIO())\n\n"], "sample_288": ["    def test_key_transform_in_with_null(self):\n        with self.assertRaisesMessage(\n            IntegrityError,\n            'null value in column \"data\" violates not null constraint',\n        ):\n\n            NullableJSONModel.objects.create(data={'key': None})\n\n        obj = NullableJSONModel.objects.create(\n            data={'key': 'value'},\n        )\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                UPDATE \"tests_nullablejsonmodel\" SET data = %s\n                WHERE id = %s\n                \"\"\",\n                [json.dumps({'key': None}), obj.id],\n            )\n        obj.refresh_from_db()\n        self.assertIsNone(obj.data.get('key'))\n        self.assertQuerysetEqual(\n            NullableJSONModel.objects.filter(data__key__in=['value', None]),\n            ['<NullableJSONModel: NullableJSONModel object (1)>'],\n        )\n\n", "    def test_key_transform_isnull_false(self):\n        model = JSONModel.objects.create(data={'name': 'test'})\n        self.assertTrue(model.data.name)\n        self.assertEqual(\n            JSONModel.objects.filter(data__name__isnull=False).count(), 1\n        )\n\n", "    def test_keytransform_isnull_with_has_key(self):\n        model = JSONModel.objects.create(data={'a': 1})\n        self.assertTrue(JSONModel.objects.filter(data__a__isnull=False).exists())\n        self.assertFalse(JSONModel.objects.filter(data__b__isnull=False).exists())\n\n\n\n", "    def test_key_transform_isnull_false(self):\n        model = JSONModel.objects.create(data={'key': 'value'})\n        self.assertTrue(JSONModel.objects.filter(data__key__isnull=False).exists())\n        self.assertFalse(JSONModel.objects.filter(data__key__isnull=True).exists())\n", "    def test_key_transform_isnull_with_null_key(self):\n        model = JSONModel.objects.create(data={'a': 1})\n        with self.assertNumQueries(1):\n            self.assertFalse(JSONModel.objects.filter(data__b__isnull=True).exists())\n        with self.assertNumQueries(1):\n            self.assertTrue(JSONModel.objects.filter(data__b__isnull=False).exists())\n"], "sample_764": ["    def test_error_single_column_selection_callable(self):\n        X = np.array([[1, 2], [3, 4]])\n        transformers = [('trans1', StandardScaler(), lambda x: x[:, 0])]\n        ct = ColumnTransformer(transformers)\n\n        assert_raises(ValueError, ct.fit_transform, X)\n", "    def test_column_transformer_sparse_output_mixed_density(self):\n\n        X = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]).astype(float)\n        X_sparse = sparse.csr_matrix(X)\n        transformer_list = [\n            ('scaler', StandardScaler(), [0]),\n            ('double', DoubleTrans(), [1]),\n            ('passthrough', 'passthrough', [2])\n        ]\n        ct = ColumnTransformer(transformer_list, sparse_threshold=0.5)\n        X_t = ct.fit_transform(X)\n        assert_false(sparse.issparse(X_t))\n\n        ct = ColumnTransformer(transformer_list, sparse_threshold=0.1)\n        X_t = ct.fit_transform(X)\n        assert_true(sparse.issparse(X_t))\n\n\n\n        ct = ColumnTransformer(transformer_list, sparse_threshold=0.1)\n        X_t = ct.fit_transform(X_sparse)\n        assert_true(sparse.issparse(X_t))\n\n", "    def test_fit_transform_errors_sparse_output(self):\n        X = np.array([[1, 2], [3, 4]])\n        trans = ColumnTransformer([('trans1', Trans(), [0]),\n                             ('trans2', Trans(), [1])],\n                             sparse_threshold=0.5)\n\n        with pytest.warns(DataConversionWarning):\n            trans.fit_transform(X)\n\n", "    def test_column_transformer_sparse_output_sparse_input(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        transformer1 = Trans()\n        transformer2 = DoubleTrans()\n        ct = ColumnTransformer([\n            ('trans1', transformer1, [0]),\n            ('trans2', transformer2, [1])\n        ], sparse_threshold=0)\n        Xt = ct.fit_transform(X)\n        assert_true(sparse.issparse(Xt))\n\n", "    def test_column_transformer_errors_sparse(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n\n        ct = ColumnTransformer([(\n            'num', StandardScaler(), [0]\n        )])\n\n        # check if error is raised when transformer doesn't accept sparse input\n        msg = (\n            \"Transformer 'num' does not support sparse input.\"\n            \" Please input a dense array or use a transformer\"\n            \" that supports sparse matrices.\"\n        )\n        with pytest.raises(TypeError, match=msg):\n            ct.fit_transform(X)\n"], "sample_1116": ["    def test_refine_Inverse():\n        X = MatrixSymbol('X', 2, 2)\n        with assuming(Q.orthogonal(X)):\n            assert refine(X.I) == X.T\n        with assuming(Q.unitary(X)):\n            assert refine(X.I) == X.conjugate()\n        with assuming(Q.singular(X)):\n            raises(ValueError, lambda: refine(X.I)) \n", "def test_Inverse_determinant():\n    assert Inverse(C).determinant() == 1/C.det()\n", "    def test_Inverse_doit_MatPow():\n        X = MatrixSymbol('X', 2, 2)\n        invX = Inverse(X)\n        assert isinstance(invX.doit(inv_expand=False), Inverse)\n        assert isinstance(invX.doit(), MatPow)\n\n\n", "    def test_inverse_eval_determinant(self):\n        raises(NonSquareMatrixError, lambda: Inverse(A))\n        raises(NonInvertibleMatrixError, lambda: Inverse(ZeroMatrix(2, 2)))\n        assert Inverse(C).doit() == C.inverse()\n        assert Inverse(C).doit(inv_expand=False) == C**(-1)\n", "    def test_refine_inverse():\n        X = MatrixSymbol('X', 2, 2)\n        with assuming(Q.orthogonal(X)):\n            assert refine(Inverse(X)) == X.T\n\n"], "sample_516": ["    def test_pdf_unicode_text(self):\n        # Test that Unicode text is rendered correctly in a PDF using PdfPages.\n        text = \"This is a test string with Unicode characters: \u00e9\u00e0\u00e7\u00fc\u00f6\"\n        with PdfPages(self.temp_path) as pdf:\n            fig, ax = plt.subplots()\n            ax.text(0.5, 0.5, text, ha='center', va='center')\n            pdf.savefig(fig)\n        # Open the PDF and check the text content.\n        pdf_file = open(self.temp_path, 'rb')\n        reader = PdfReader(pdf_file)\n        page = reader.pages[0]\n        text = page.extract_text()\n        assert text == \"This is a test string with Unicode characters: \u00e9\u00e0\u00e7\u00fc\u00f6\"\n\n", "    def test_pdf_metadata(self):\n        with NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmpfile:\n            filename = tmpfile.name\n\n        metadata = {'Creator': 'pytest', 'Producer': 'pytest'}\n        with PdfPages(filename, metadata=metadata) as pdf:\n            fig, ax = plt.subplots()\n            ax.plot([1, 2, 3, 4])\n            pdf.savefig(fig)\n\n        with open(filename, 'rb') as fh:\n            pdf_data = fh.read()\n        os.remove(filename)\n\n        # Check for metadata entries\n        assert b'%PDF-' in pdf_data\n        for key, value in metadata.items():\n            assert f'{key}: {value}'.encode() in pdf_data\n", "    def test_pdf_glyphs(self):\n        # Test that glyphs are correctly embedded in the PDF file.\n        # Create a figure with text containing non-standard characters.\n        fig, ax = plt.subplots()\n        ax.set_title('This is a test with some special characters: \u00e6\u00f8\u00e5')\n        ax.text(0.5, 0.5, '\u00df', ha='center', va='center')\n\n        # Save the figure to a temporary PDF file.\n        with NamedTemporaryFile(delete=False) as tmp_file:\n            filename = tmp_file.name\n            fig.savefig(filename)\n\n        # Open the PDF file and extract the glyphs.\n        with open(filename, 'rb') as f:\n            pdf_content = f.read()\n        glyphs = get_glyphs_subset(pdf_content)\n\n        # Assert that the glyphs for the special characters are present.\n        assert '\u00e6' in glyphs\n        assert '\u00f8' in glyphs\n        assert '\u00e5' in glyphs\n        assert '\u00df' in glyphs\n\n        # Clean up the temporary file.\n        os.remove(filename)\n", "    def test_pdfpages_keep_empty(self):\n        with PdfPages('empty.pdf', keep_empty=False) as pdf:\n            pass\n\n        assert not os.path.exists('empty.pdf')\n", "    def test_pickle_pdfpages(self):\n        with NamedTemporaryFile() as tmpfile:\n            pdf = PdfPages(tmpfile.name)\n            pdf.savefig(plt.figure())\n            pdf.close()\n\n            unpickled_pdf = PdfPages(tmpfile.name)\n            assert unpickled_pdf.get_pagecount() == 1\n\n\n"], "sample_559": ["def test_zoomed_inset_axes():\n    fig, ax = plt.subplots()\n    axins = zoomed_inset_axes(ax, zoom=6, loc='upper left')\n    axins.set_xlim(1.5, 2.5)\n    axins.set_ylim(0, 1)\n    plt.xticks(visible=False)\n    plt.yticks(visible=False)\n    mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n    return fig\n", "    def test_rgb_axes(self):\n        fig, ax = plt.subplots()\n        ax_r, ax_g, ax_b = RGBAxes(ax, extent=[0, 1, 0, 1], pad=0.0)\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n\n        ax_r.imshow(np.zeros([3, 3]), cmap='Reds')\n        ax_g.imshow(np.zeros([3, 3]), cmap='Greens')\n        ax_b.imshow(np.zeros([3, 3]), cmap='Blues')\n", "    def test_inset_locator(self):\n        fig, ax = plt.subplots()\n        axins = zoomed_inset_axes(ax, 6, loc=1)  # zoom-factor: 6\n\n        axins.plot(np.arange(10), np.sin(np.arange(10)))\n\n        mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n        plt.xticks(visible=False)\n        plt.yticks(visible=False)\n", "    def test_axesgrid_inset_locator(self):\n        fig, ax = plt.subplots()\n        axins = zoomed_inset_axes(ax, 6, loc=1)\n        axins.set_xlim(0, 1)\n        axins.set_ylim(0, 1)\n\n        p = plt.Rectangle((0.2, 0.2), 0.1, 0.1)\n        ax.add_patch(p)\n        mark_inset(ax, axins, loc1=2, loc2=4, fc=\"none\", ec=\"0.5\")\n\n        # Test BboxConnectorPatch\n        bbox = Bbox.from_bounds(0.2, 0.2, 0.1, 0.1)\n        bbox2 = Bbox.from_bounds(0.4, 0.4, 0.2, 0.2)\n        con = BboxConnectorPatch(bbox, bbox2, loc1=3, loc2=1,\n                                 fc='blue', alpha=0.5)\n        ax.add_patch(con)\n\n", "    def test_colorbar_no_label(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow(np.random.rand(10, 10), cmap='viridis')\n        ax.cax = fig.add_axes([0.85, 0.1, 0.05, 0.8])\n        cbar = fig.colorbar(im, cax=ax.cax)\n        cbar.set_label('')\n"], "sample_821": ["    def test_affinity_propagation_preference():\n        # Test with various preference settings\n        for preference in [None, -1, 0, 1, np.median(euclidean_distances(X))]:\n            ap = AffinityPropagation(preference=preference).fit(X)\n            assert_warns(ConvergenceWarning, ap.fit, X)\n\n\n", "    def test_affinity_propagation_precomputed():\n        # Test affinity propagation with precomputed affinity matrix\n        similarity_matrix = euclidean_distances(X)\n        similarities = -similarity_matrix ** 2\n        af = AffinityPropagation(affinity='precomputed')\n        af.fit(similarities)\n        assert_equal(len(af.cluster_centers_indices_), n_clusters)\n", "    def test_affinity_propagation_all_equal():\n        # Test when all training samples have equal similarities and equal\n        # preferences\n        S = np.ones((10, 10))\n        preference = np.ones((10,))\n        labels = affinity_propagation(S, preference=preference)[1]\n\n        # Check if all samples are clustered together if preference is smaller\n        # than similarities\n        assert_equal(np.unique(labels).size, 1)\n        # Check if each sample is its own cluster if preference is larger\n        # than similarities\n\n        preference = np.full((10,), 2)\n        labels = affinity_propagation(S, preference=preference)[1]\n        assert_equal(np.unique(labels).size, 10)\n\n\n\n", "    def test_affinity_propagation_preference_equal_similarities(self):\n        # Test case where preferences are equal to similarities\n        S = -euclidean_distances(X, squared=True)\n        preference = np.median(S)\n        S = np.full(S.shape, preference)\n        labels, cluster_centers_indices, n_iter = \\\n            affinity_propagation(S, preference=preference,\n                                  convergence_iter=1, max_iter=1,\n                                  return_n_iter=True)\n\n        assert_equal(len(cluster_centers_indices), 1)\n        assert_equal(n_iter, 1)\n", "    def test_same_similarities_and_preferences():\n        # Test cases for _equal_similarities_and_preferences\n        S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n        preference = np.array([1, 1, 1])\n        assert _equal_similarities_and_preferences(S, preference)\n\n        S = np.array([[1, 2, 3], [2, 1, 3], [3, 3, 1]])\n        preference = np.array([1, 1, 1])\n        assert not _equal_similarities_and_preferences(S, preference)\n\n        S = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n        preference = np.array([2, 2, 2])\n        assert _equal_similarities_and_preferences(S, preference)\n\n        S = np.array([[1, 2, 3], [2, 1, 3], [3, 3, 1]])\n        preference = np.array([2, 2, 2])\n        assert not _equal_similarities_and_preferences(S, preference)\n"], "sample_522": ["    def test_colorbar_extension_shape(self):\n        with rc_context({'axes.facecolor': 'w'}):\n            fig = _colorbar_extension_shape('uniform')\n            self.compare(fig)\n            fig = _colorbar_extension_shape('proportional')\n            self.compare(fig)\n\n", "    def test_colorbar_ticks_spacing(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow(np.arange(100).reshape(10, 10), cmap='viridis')\n        cb = fig.colorbar(im, ax=ax, orientation='vertical')\n\n        for spacing in ['uniform', 'proportional']:\n            cb.set_ticks(np.arange(10))\n            cb.set_spacing(spacing)\n            cb.update_ticks()\n            assert np.allclose(\n                cb.ax.get_yticks(), np.arange(10),\n                err_msg=f\"Ticks incorrect for spacing: {spacing}\"\n            )\n", "def test_colorbar_extension_length():\n    return _colorbar_extension_length('uniform')\n", "    def test_colorbar_extension_ticks(self):\n        \"\"\"Test that ticks are correctly placed when using extensions.\"\"\"\n        cmap, norms = _get_cmap_norms()\n        fig, ax = plt.subplots()\n        im = ax.imshow([[1, 2], [3, 4]], cmap=cmap, norm=norms['both'])\n        cbar = fig.colorbar(im, extend='both')\n        \n        tick_locs = cbar.ax.get_yticks()\n        # Check if ticks extend beyond the colorbar's main range\n        assert tick_locs[-1] > norms['both'].vmax\n        assert tick_locs[0] < norms['both'].vmin\n", "    def test_colorbar_extension_shape(self):\n        # Test that extensions are shaped correctly in both uniform and\n        # proportional spacing configurations.\n        fig_uniform = _colorbar_extension_shape('uniform')\n        fig_proportional = _colorbar_extension_shape('proportional')\n        yield check_figures_equal(fig_uniform, fig_proportional)\n"], "sample_334": ["    def test_form_hidden_fields(self):\n        form = PersonFormWithHiddenFields()\n        self.assertEqual(form.hidden_fields(), [form['hidden_field']])\n\n        form = PersonFormWithHiddenFields(data={'first_name': 'John'})\n        self.assertEqual(form.hidden_fields(), [form['hidden_field']])\n\n", "    def test_form_hidden_fields(self):\n        form = Person(\n            {'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-03-15'},\n        )\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = Person(\n            {'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-03-15'},\n            hidden_fields=['birthday']\n\n        )\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 1)\n        self.assertIn('birthday', [f.name for f in hidden_fields])\n", "    def test_empty_permitted(self):\n        data = {'first_name': '', 'last_name': '', 'birthday': ''}\n        form = Person(data, empty_permitted=True)\n        self.assertTrue(form.is_valid())\n        self.assertEqual(form.cleaned_data, {'first_name': '', 'last_name': '', 'birthday': None})\n\n", "    def test_hidden_fields(self):\n        form = Person(\n            data={'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-01-01'}\n        )\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = Person(data={}, initial={'first_name': 'John'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = Person(\n            data={}, initial={'first_name': 'John'},\n        )\n        form.fields['first_name'].widget = HiddenInput()\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 1)\n        self.assertEqual(hidden_fields[0].name, 'first_name')\n\n\n", "    def test_form_hidden_fields(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n        form = Person({'first_name': 'John', 'last_name': 'Doe'})\n        hidden_fields = form.hidden_fields()\n        self.assertEqual(len(hidden_fields), 0)\n\n"], "sample_239": ["    def test_formset_factory_custom_kwargs(self):\n        formset_class = formset_factory(CustomKwargForm, extra=2)\n        formset = formset_class(\n            {'form-0-custom_kwarg': 'test1', 'form-1-custom_kwarg': 'test2'},\n            custom_kwarg='test',\n        )\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test1')\n        self.assertEqual(formset.forms[1].custom_kwarg, 'test2')\n", "    def test_formset_factory_with_ordering(self):\n        class MyForm(Form):\n            field = CharField()\n\n        class MyFormSet(BaseFormSet):\n            can_order = True\n            can_delete = True\n\n                ordered_data = [form.cleaned_data['field'] for form in self.ordered_forms]\n                self.assertEqual(ordered_data, ['c', 'a', 'b'])\n\n        MyFormSet = formset_factory(MyForm, formset=MyFormSet)\n        formset = MyFormSet(\n            data={\n                'form-0-field': 'a',\n                'form-1-field': 'b',\n                'form-2-field': 'c',\n                'form-0-ORDERING_FIELD_NAME': '2',\n                'form-1-ORDERING_FIELD_NAME': '3',\n                'form-2-ORDERING_FIELD_NAME': '1',\n                'TOTAL_FORMS': '3',\n                'INITIAL_FORMS': '0',\n                'MAX_NUM_FORMS': '10',\n            },\n        )\n        self.assertTrue(formset.is_valid())\n", "    def test_formset_max_num_validation(self):\n        formset_data = [('Choice 1', 10), ('Choice 2', 5)]\n        formset = self.make_choiceformset(\n            formset_data, max_num_forms=1, total_forms=2,\n        )\n\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors.as_data(), {\n            'non_form_errors': [\n                ValidationError(\n                    'Please submit at most 1 form.',\n                    code='too_many_forms',\n                ),\n            ],\n        })\n\n", "    def test_formset_factory_with_custom_kwarg(self):\n        class CustomKwargFormSet(BaseFormSet):\n                super().__init__(*args, **kwargs)\n\n        formset = formset_factory(CustomKwargForm, formset=CustomKwargFormSet, extra=1, custom_kwarg='test')\n        self.assertEqual(formset().forms[0].custom_kwarg, 'test')\n", "    def test_formset_factory_with_custom_kwarg(self):\n            return CustomKwargForm(*args, custom_kwarg=custom_kwarg, **kwargs)\n\n        CustomFormSet = formset_factory(CustomKwargForm, form = make_custom_form)\n        formset = CustomFormSet(data={'form-0-field': 'data'}, custom_kwarg='test')\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test')\n"], "sample_392": ["    def test_count_annotated_with_json_field_expression(self):\n        with self.assertRaises(OperationalError):\n            JSONModel.objects.annotate(\n                Count(\"data.nested_field\")\n            ).filter(data__nested_field=1).count()\n", "    def test_key_transform_isnull_with_none(self):\n        obj = JSONModel.objects.create(data={'key': None})\n        qs = JSONModel.objects.filter(data__key__isnull=True)\n        self.assertQuerysetEqual(qs, [obj], transform=lambda o: o)\n", "    def test_key_transform_isnull_lookup_with_sqlite(self):\n        \"\"\"Test KeyTransformIsNull lookup with SQLite.\"\"\"\n        with self.subTest(\"has_key\"):\n            self.assertEqual(\n                JSONModel.objects.filter(data__key__isnull=False).count(), 2\n            )\n        with self.subTest(\"has_key=False\"):\n            self.assertEqual(\n                JSONModel.objects.filter(data__key__isnull=True).count(), 0\n            )\n\n", "    def test_key_transform_in_with_empty_list(self):\n        model = JSONModel.objects.create(data={\"tags\": [\"foo\"]})\n\n        with self.assertRaises(OperationalError):\n            list(JSONModel.objects.filter(data__tags__in=[]))\n", "    def test_key_transform_in_with_null(self):\n        # Tests JSONField__key__in lookup with null value in the list.\n        with self.assertNumQueries(1):\n            qs = JSONModel.objects.filter(data__key__in=['value', None])\n        self.assertJSONEqual(\n            qs.order_by('id'),\n            [\n                {'data': {'key': 'value', 'key2': 'value2'}},\n                {'data': {'key': None, 'key2': 'value2'}},\n            ]\n        )\n\n"], "sample_381": ["    def test_handle_model_renamed(self):\n        before = [\n            self.author,\n            self.book,\n        ]\n        after = [\n            self.author_renamed,\n            self.book_with_author_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0,\n                                       old_name=\"Author\", new_name=\"Writer\")\n", "    def test_circular_dependencies_with_foreign_key(self):\n        before = [\n            self.knight,\n            self.rabbit,\n        ]\n        after = [\n            self.knight,\n            self.rabbit\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"eggs\", 1)\n        self.assertOperationTypes(changes, \"eggs\", 0, ['AlterField'])\n        self.assertOperationAttributes(changes, \"eggs\", 0, 0, name='knight_id')\n\n\n\n", "    def test_reorder_fields_with_dependencies(self):\n        # This test checks that we handle cases where fields are reordered in a model,\n        # and those fields are referenced by other models via FK relationships.\n\n        before = [\n            self.author_name_default,\n        ]\n\n        after = [\n            self.author_name_default_reordered,\n        ]\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, [\n            'RenameField',\n            'AddField',\n        ])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='name')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='alternate_name')\n\n", "    def test_initial_migration_with_options(self):\n        changes = self.get_changes([\n            self.author_with_options,\n        ], [\n            self.author_with_options,\n            self.book,\n        ])\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertMigrationDependencies(changes, \"testapp\", 1, ['testapp:initial'])\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"CreateModel\"])\n        self.assertOperationTypes(changes, \"testapp\", 1, [\"CreateModel\"])\n", "    def test_remove_unique_together(self):\n        changes = self.get_changes(\n            [self.book_foo_together],\n            [self.book],\n        )\n        self.assertNumberMigrations(changes, \"otherapp\", 2)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveUniqueTogether\"])\n        self.assertOperationTypes(changes, \"otherapp\", 1, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='book_foo_together', fields=[(\"author\", \"title\")])\n        self.assertOperationAttributes(changes, \"otherapp\", 1, 0, old_name='title', new_name='name')\n"], "sample_552": ["    def test_tight_layout_constrained_layout(self):\n        fig, axs = plt.subplots(2, 2, constrained_layout=True)\n        axs[0, 0].set_title(\"Top Left\")\n        axs[0, 1].set_title(\"Top Right\")\n        axs[1, 0].set_title(\"Bottom Left\")\n        axs[1, 1].set_title(\"Bottom Right\")\n        fig.tight_layout()\n        # check the layout\n", "    def test_add_axes(self):\n        fig = Figure()\n\n        ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n        assert ax1.get_position() == [0.1, 0.1, 0.8, 0.8]\n\n        ax2 = fig.add_axes(\"foo\")\n        assert ax2.get_position() == [0.1, 0.1, 0.8, 0.8]\n\n        ax3 = fig.add_axes((0.2, 0.2, 0.5, 0.5))\n        assert ax3.get_position() == [0.2, 0.2, 0.5, 0.5]\n", "    def test_tight_layout(self):\n        fig, axs = plt.subplots(2, 2)\n        fig.tight_layout()\n\n        # Test that the subplots are not overlapping\n        for ax in axs.flat:\n            assert ax.get_position().width < 0.5\n            assert ax.get_position().height < 0.5\n", "def test_figure_savefig_date_format():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3], [4, 5, 6])\n    ax.set_xlabel('Date')\n    ax.set_xticks([1, 2, 3])\n    ax.set_xticklabels([datetime(2023, 1, 1), datetime(2023, 2, 1),\n                        datetime(2023, 3, 1)])\n    fig.savefig(io.BytesIO(), format='png', dpi=100,\n                bbox_inches='tight')\n    # Verify that the saved figure contains the date labels\n", "    def test_figure_dpi_set():\n        fig = Figure(dpi=50)\n        assert fig.dpi == 50\n\n        fig.dpi = 100\n        assert fig.dpi == 100\n"], "sample_618": ["    def test_unify_chunks_mismatch():\n        ds1 = xr.Dataset({\"a\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n                         coords={\"x\": [0, 1], \"y\": [0, 1, 2]})\n        ds1 = ds1.chunk({\"x\": 2})\n        ds2 = xr.Dataset({\"b\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n                         coords={\"x\": [0, 1], \"y\": [0, 1, 2]})\n        ds2 = ds2.chunk({\"y\": 2})\n\n        with pytest.raises(ValueError):\n            unify_chunks(ds1, ds2)\n", "    def test_broadcast_compat_data_dask(dim):\n        x = xr.DataArray(\n            np.arange(4).reshape((2, 2)), dims=[\"x\", \"y\"], name=\"x\"\n        ).chunk({\"x\": 1, \"y\": 2})\n        y = xr.DataArray(np.arange(2), dims=[dim], name=\"y\").chunk({dim: 1})\n        actual = broadcast_compat_data(x, y)\n\n        expected_x = x.chunk({\"x\": 1, \"y\": 2})\n        expected_y = y.broadcast_like(x).chunk({\"x\": 1, \"y\": 2})\n        assert_identical(actual[0], expected_x)\n        assert_identical(actual[1], expected_y)\n", "def test_unify_chunks(coords):\n    ds1 = xr.DataArray([1, 2, 3], dims=[\"x\"], coords=coords, chunks={'x': 2})\n    ds2 = xr.DataArray([4, 5, 6], dims=[\"y\"], coords=coords, chunks={'y': 2})\n\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n    assert unified_ds1.chunks == {'x': 2}\n    assert unified_ds2.chunks == {'y': 2}\n\n", "    def test_where_keep_attrs(self, keep_attrs):\n        x = xr.DataArray(\n            np.arange(10),\n            dims=[\"y\"],\n            coords={\"y\": np.arange(10)},\n            attrs={\"foo\": \"bar\"},\n        )\n        cond = xr.DataArray([True] * 5 + [False] * 5, dims=[\"y\"])\n        expected = x.where(cond, drop=True)\n        if keep_attrs is True:\n            expected.attrs = x.attrs\n        result = xr.where(cond, x, x, keep_attrs=keep_attrs)\n        assert_identical(result, expected)\n\n\n\n", "def test_unify_chunks_empty_arrays():\n    # Test handling of empty arrays in unify_chunks.\n    ds1 = xr.Dataset(\n        {\n            \"a\": xr.DataArray([], dims=[\"x\"]),\n            \"b\": xr.DataArray([], dims=[\"y\"]),\n        },\n    )\n    ds2 = xr.Dataset(\n        {\n            \"a\": xr.DataArray([], dims=[\"x\"]),\n            \"c\": xr.DataArray([], dims=[\"z\"]),\n        },\n    )\n    unified_ds1, unified_ds2 = unify_chunks(ds1, ds2)\n\n    assert set(unified_ds1.dims) == {\"x\", \"y\", \"z\"}\n    assert set(unified_ds2.dims) == {\"x\", \"y\", \"z\"}\n    assert unified_ds1[\"a\"].chunks is None\n    assert unified_ds1[\"b\"].chunks is None\n    assert unified_ds1[\"c\"].chunks is None\n    assert unified_ds2[\"a\"].chunks is None\n    assert unified_ds2[\"b\"].chunks is None\n    assert unified_ds2[\"c\"].chunks is None\n\n\n\n"], "sample_131": ["    def test_test_db_signature_with_different_test_db_name(self):\n        connection = self.get_connection_copy()\n        connection.settings_dict['TEST']['NAME'] = 'custom_test_db'\n        creation = BaseDatabaseCreation(connection)\n        signature_1 = creation.test_db_signature()\n        connection.settings_dict['TEST']['NAME'] = 'another_test_db'\n        signature_2 = creation.test_db_signature()\n        self.assertNotEqual(signature_1, signature_2)\n\n", "    def test_test_db_signature(self):\n        conn = self.get_connection_copy()\n        conn.settings_dict[\"NAME\"] = \"test_db\"\n        creation = BaseDatabaseCreation(conn)\n        signature = creation.test_db_signature()\n        # Assert that the signature includes expected elements\n\n        self.assertIn(conn.settings_dict[\"HOST\"], signature)\n        self.assertIn(conn.settings_dict[\"PORT\"], signature)\n        self.assertIn(conn.settings_dict[\"ENGINE\"], signature)\n        self.assertIn(creation._get_test_db_name(), signature)\n", "    def test_test_db_signature(self):\n        conn = self.get_connection_copy()\n        original_settings = conn.settings_dict.copy()\n\n        # Test the default signature\n        expected = (\n            original_settings['HOST'],\n            original_settings['PORT'],\n            original_settings['ENGINE'],\n            TEST_DATABASE_PREFIX + original_settings['NAME'],\n        )\n        self.assertEqual(\n            BaseDatabaseCreation(conn).test_db_signature(),\n            expected\n        )\n\n        # Test with a modified database name\n        conn.settings_dict['NAME'] = 'new_db_name'\n        expected = (\n            original_settings['HOST'],\n            original_settings['PORT'],\n            original_settings['ENGINE'],\n            TEST_DATABASE_PREFIX + 'new_db_name',\n        )\n        self.assertEqual(\n            BaseDatabaseCreation(conn).test_db_signature(),\n            expected\n        )\n", "    def test_test_db_signature_unique_names(self):\n        connection = self.get_connection_copy()\n        original_name = connection.settings_dict['NAME']\n        connection.settings_dict['NAME'] = 'test_db_1'\n        signature1 = BaseDatabaseCreation(connection).test_db_signature()\n        connection.settings_dict['NAME'] = 'test_db_2'\n        signature2 = BaseDatabaseCreation(connection).test_db_signature()\n        self.assertNotEqual(signature1, signature2)\n        connection.settings_dict['NAME'] = original_name\n", "    def test_test_db_signature_with_different_engine(self):\n        # Mock setting a different database engine.\n        test_connection = self.get_connection_copy()\n        test_connection.settings_dict['ENGINE'] = 'django.db.backends.dummy'\n\n        # Create an instance of BaseDatabaseCreation but don't initialize it\n        # with a real connection.\n        creation = BaseDatabaseCreation(test_connection)\n\n        original_signature = creation.test_db_signature()\n\n        # Change the engine back.\n        test_connection.settings_dict['ENGINE'] = connections[DEFAULT_DB_ALIAS].settings_dict['ENGINE']\n        new_signature = creation.test_db_signature()\n\n        self.assertNotEqual(original_signature, new_signature)\n\n"], "sample_1082": ["def test_acsch_rewrite():\n    assert acsch(1).rewrite(log) == log(1 + sqrt(2))\n", "def test_acsch_rewrite():\n    x = Symbol('x')\n    assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n", "def test_acsch_rewrite():\n    x = Symbol('x')\n    assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n", "def test_asech_rewrite():\n    x = Symbol('x')\n    assert asech(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x - 1) * sqrt(1/x + 1))\n", "    def test_acsch_rewrite_as_log():\n        x = symbols('x')\n        assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n\n"], "sample_513": ["    def test_legend_bbox_to_anchor_transform(self):\n        fig, ax = plt.subplots()\n        x, y = 0.5, 0.5\n        l = ax.plot([1, 2, 3], label='Line')[0]\n        fig.legend(loc='center', bbox_to_anchor=(x, y),\n                   bbox_transform=ax.transAxes)\n        ax.set_xlim([0, 4])\n        ax.set_ylim([0, 4])\n\n        # The bbox_to_anchor coordinates should correspond to\n        # (x, y) in data coordinates because ax.transAxes is used.\n        assert np.allclose(fig.legends[0].get_bbox_to_anchor().extents,\n                           ax.transData.transform(\n                               np.array([x, y]) * np.array([4, 4])))\n", "    def test_legend_bbox_to_anchor_transform(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6], label='line')\n        ax.legend()\n\n        # Create a custom transform that shifts the legend coordinates\n        class CustomTransform(mtransforms.Transform):\n                return points + [10, 20]\n\n        # Use the custom transform with bbox_to_anchor\n        handles, labels = ax.get_legend_handles_labels()\n        legend = plt.legend(handles, labels, bbox_to_anchor=(0, 0),\n                          transform=CustomTransform())\n\n        # Assert that the legend is positioned according to the transform\n        bbox = legend.get_bbox()\n        assert_allclose(bbox.x0, 10)\n        assert_allclose(bbox.y0, 20)\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label=\"Line\")\n        ax.legend(loc=loc)\n        fig.canvas.draw()\n        # Check if the legend is placed at the correct location\n        bbox = ax.legend().get_window_extent()\n        assert bbox.x0 > 0  # Check if x0 is within the figure bounds\n        assert bbox.y0 > 0  # Check if y0 is within the figure bounds\n\n\n", "    def test_legend_draggable(self):\n        fig, ax = plt.subplots()\n        lines = ax.plot([1, 2, 3], [4, 5, 6], label='line')\n        ax.legend(draggable=True)\n        return fig,\n\n\n\n", "    def test_legend_loc(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], label='Line 1')\n        ax.plot([2, 3, 4], label='Line 2')\n        legend = ax.legend(loc=loc)\n        # Check that the legend is placed correctly\n        fig.canvas.draw()\n        bbox = legend.get_window_extent()\n        assert bbox.x0 >= 0 and bbox.x1 <= fig.bbox.width\n        assert bbox.y0 >= 0 and bbox.y1 <= fig.bbox.height\n"], "sample_731": ["    def test_fetch_california_housing(self):\n        check_return_X_y(fetch,\n                         expected_data_shape=(20640, 8),\n                         expected_target_shape=(20640,),\n                         expected_feature_names=[\"MedInc\", \"HouseAge\", \"AveRooms\",\n                                                \"AveBedrms\", \"Population\",\n                                                \"AveOccup\", \"Latitude\",\n                                                \"Longitude\"])\n", "    def test_fetch_california_housing_data_home(tmpdir):\n        # Test that data is correctly loaded when data_home is specified\n        data_home = tmpdir.strpath\n        data = fetch_california_housing(data_home=data_home)\n        assert data.DESCR == __doc__\n        assert data.data.shape == (20640, 8)\n        assert data.target.shape == (20640,)\n\n", "compilation error", "compilation error", "compilation error"], "sample_600": ["    def test_unsigned_integer_coder_with_fill_value():\n        data = np.array([-1, 0, 1, 2, 3], dtype=np.int8)\n        attrs = {\"_FillValue\": -1, \"_Unsigned\": \"true\"}\n        var = xr.Variable('x', data, attrs=attrs)\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n        assert_equal(encoded.attrs, {\"_FillValue\": 255})\n        assert_allclose(decoded, data)\n", "    def test_unsigned_integer_coder():\n        data = np.array([10, 20, 30], dtype=\"uint8\")\n        var = xr.Variable((\"x\",), data)\n        var.attrs['_Unsigned'] = 'true'\n\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n\n        assert_identical(var, decoded)\n", "    def test_unsigned_integer_coder():\n        data = np.array([0, 1, 2, 3, 4, 2**31 - 1], dtype=np.uint32)\n        attrs = {\"_Unsigned\": \"true\"}\n        var = xr.Variable([\"x\"], data, attrs=attrs)\n\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        assert encoded.dtype == np.int32\n        assert encoded.attrs.get(\"_Unsigned\") is None\n        assert_allclose(encoded.values, data.astype(np.int32))\n\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n        assert decoded.dtype == np.uint32\n        assert decoded.attrs.get(\"_Unsigned\") == \"true\"\n        assert_allclose(decoded.values, data)\n", "    def test_unsigned_integer_coder(self):\n        data = np.array([0, 1, 2, 3, 4], dtype=np.uint8)\n        var = xr.Variable(\"x\", data)\n        var.attrs[\"_Unsigned\"] = \"true\"\n        var.attrs[\"units\"] = \"meters\"\n\n        encoded = encode_cf_variable(var)\n        assert encoded.dtype == np.int8\n        assert encoded.attrs[\"units\"] == \"meters\"\n\n        # Check that _Unsigned is not in the encoded variable\n        assert \"_Unsigned\" not in encoded.attrs\n\n        decoded = decode_cf_variable(encoded)\n        assert decoded.dtype == np.uint8\n        assert decoded.attrs[\"units\"] == \"meters\"\n\n        # Test with a negative value\n        data = np.array([0, 1, 2, -1, 4], dtype=np.int8)\n        var = xr.Variable(\"x\", data)\n\n        with pytest.raises(ValueError):\n            encode_cf_variable(var)\n", "    def test_cf_scale_offset_encode_decode(self, scale_factor, add_offset, data, expected_data):\n        var = xr.Variable([\"x\"], data)\n        var.attrs[\"scale_factor\"] = scale_factor\n        var.attrs[\"add_offset\"] = add_offset\n\n        encoded = encode_cf_variable(var)\n        decoded = decode_cf_variable(encoded)\n\n        assert_allclose(decoded.values, expected_data)\n\n"], "sample_377": ["    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: \"dummy\")\n        self.assertEqual(str(wrapped_callable), \"dummy\")\n        self.assertEqual(repr(wrapped_callable), \"<CallableSettingWrapper object>\")\n", "    def test_callable_setting_wrapper_repr(self):\n        class CallableSettingWrapper:\n                self._wrapped = callable_setting\n\n                return repr(self._wrapped)\n\n        wrapped_callable = CallableSettingWrapper(lambda x: x)\n        self.assertEqual(repr(wrapped_callable), \"<function <lambda> at 0x...>\n", "    def test_callable_setting_wrapper_call(self):\n        fake_setting = mock.Mock()\n        fake_setting.return_value = \"Fake Value\"\n        wrapped_setting = CallableSettingWrapper(fake_setting)\n        self.assertEqual(wrapped_setting(), \"Fake Value\")\n        fake_setting.assert_called_once()\n", "    def test_callablesettingwrapper_repr(self):\n        callable_object = lambda: None\n        wrapper = CallableSettingWrapper(callable_object)\n        self.assertEqual(repr(wrapper), repr(callable_object))\n", "    def test_callable_setting_wrapper(self):\n        wrapped = CallableSettingWrapper(lambda: 42)\n        with mock.patch('warnings.warn') as warn:\n            str(wrapped)\n        warn.assert_called_once()\n        self.assertEqual(repr(wrapped), 'CallableSettingWrapper(<lambda>)')\n"], "sample_781": ["def test_classification_toy_multioutput(name):\n    \"\"\"Check classification on a toy dataset with a multioutput target.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n    # create a multi-output dataset\n    y_multi = np.array([[i, i * 2] for i in y])\n    clf = ForestClassifier(n_estimators=10, random_state=1)\n    clf.fit(X, y_multi)\n    y_pred = clf.predict(T)\n    assert_equal(y_pred.shape, (len(T), 2))\n\n", "def check_classification_large(name):\n    \"\"\"Check classification on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_large, y_large)\n    score = clf.score(X_large, y_large)\n    assert score > 0.9, \"Failed with score = %f\" % score\n\n    # also test predict_proba\n    y_proba = clf.predict_proba(X_large)\n    assert y_proba.shape[0] == X_large.shape[0]\n    assert y_proba.shape[1] == len(np.unique(y_large))\n\n", "def check_feature_importances_large(name):\n    \"\"\"Check feature importances on a larger dataset.\"\"\"\n    ForestClassifier = FOREST_CLASSIFIERS[name]\n\n    clf = ForestClassifier(n_estimators=10, random_state=0)\n    clf.fit(X_large, y_large)\n    importances = clf.feature_importances_\n\n    assert importances.shape[0] == X_large.shape[1]\n    assert np.all(importances >= 0)\n    assert np.all(importances <= 1)\n\n\n", "    def test_classification_toy_oob_score(self):\n        ForestClassifier = FOREST_CLASSIFIERS[name]\n        clf = ForestClassifier(n_estimators=10, oob_score=True, random_state=1)\n        clf.fit(X_large, y_large)\n        assert hasattr(clf, 'oob_score_')\n        assert clf.oob_score_ >= 0\n\n", "def check_feature_importances(name):\n    \"\"\"Check feature importances for regressor and classifier.\"\"\"\n    Estimator = FOREST_CLASSIFIERS_REGRESSORS[name]\n    \n    # For classification, the iris dataset\n    if name in FOREST_CLASSIFIERS:\n        est = Estimator(n_estimators=10, random_state=42)\n        est.fit(iris.data, iris.target)\n        importances = est.feature_importances_\n        assert_equal(len(importances), iris.data.shape[1])\n        assert_greater(np.sum(importances), 0)\n    # For regression, the boston dataset\n    elif name in FOREST_REGRESSORS:\n        est = Estimator(n_estimators=10, random_state=42)\n        est.fit(boston.data, boston.target)\n        importances = est.feature_importances_\n        assert_equal(len(importances), boston.data.shape[1])\n        assert_greater(np.sum(importances), 0)\n\n\n\n"], "sample_1103": ["def test_Pow_nonnegative_denominator():\n    # issue sympy/sympy#19151\n    assert (1/(x**2)).evalf(subs={x: 0.5}) == 4\n", "    def test_Pow_as_content_primitive():\n        assert (2**x).as_content_primitive() == (1, 2**x)\n        assert (2**(x + 1)).as_content_primitive() == (2, 2**x)\n        assert ((2*x)**2).as_content_primitive() == (4, x**2)\n        assert ((2*x)**(x + 1)).as_content_primitive() == (1, (2*x)**(x + 1))\n        assert ((2*x)**(2*x + 1)).as_content_primitive() == (1, (2*x)**(2*x + 1))\n", "def test_pow_eval_nseries():\n    assert (x**2).nseries(x, 0, 2) == x**2\n    assert (x**2).nseries(x, 2, 0) == x**2\n    assert (sin(x)**2).nseries(x, 0, 2) == x**2 - (1/3)*x**4 + O(x**6)\n", "def test_Pow_as_content_primitive():\n    assert (x**2).as_content_primitive() == (1, x**2)\n    assert (2*x**2).as_content_primitive() == (2, x**2)\n    assert ((2*x + 2)**2).as_content_primitive() == (4, (x + 1)**2)\n    assert ((2*x)**2).as_content_primitive() == (4, x**2)\n    assert ((-2*x)**2).as_content_primitive() == (4, x**2)\n\n", "def test_Pow_expand_power_base():\n    # test case taken from #11975\n    expr = (2 + 2*x)**y\n    s = expand_power_base(expr)\n    assert s.is_Mul\n    assert s == 2**y*(x + 1)**y\n\n"], "sample_345": ["    def test_iter_modules_and_files_zipfile(self):\n        tempdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tempdir)\n        archive_path = Path(tempdir) / 'my_module.zip'\n        with zipfile.ZipFile(archive_path, 'w') as zf:\n            zf.writestr('my_module/__init__.py', '__version__ = \"1.0\"\\n')\n        sys.path.append(tempdir)\n        self.import_and_cleanup('my_module')\n        self.assertFileFound(archive_path)\n\n", "    def test_iter_modules_and_files_zipfile(self):\n\n        # Create a temporary zip file with a module.\n        with tempfile.TemporaryDirectory() as tmpdir:\n            zip_path = Path(tmpdir) / 'test.zip'\n            with zipfile.ZipFile(zip_path, 'w') as zipf:\n                zipf.writestr('module.py', 'print(\"Hello\")')\n            sys.path.append(str(tmpdir))\n            self.addCleanup(sys.path.pop)\n            self.import_and_cleanup('module')\n            self.assertFileFound(zip_path)\n\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        zip_file = self.temporary_file('test.zip')\n        with zipfile.ZipFile(zip_file, 'w') as zf:\n            zf.writestr('test_module/__init__.py', '')\n            zf.writestr('test_module/main.py', '')\n        self.import_and_cleanup('test_module')\n        self.assertFileFound(zip_file)\n", "    def test_iter_modules_and_files_zip(self):\n        with tempfile.NamedTemporaryFile(suffix=\".zip\") as f:\n            zip_filename = Path(f.name)\n            with zipfile.ZipFile(zip_filename, 'w') as z:\n                z.writestr('test_module/__init__.py', '')\n                z.writestr('test_module/test.py', '')\n\n            self.import_and_cleanup('test_module')\n\n            self.assertFileFound(zip_filename)\n", "    def test_iter_modules_and_files_with_zipfile(self):\n        with contextlib.ExitStack() as stack:\n            tempdir = stack.enter_context(tempfile.TemporaryDirectory())\n            zip_filename = Path(tempdir) / 'test.zip'\n\n            with zipfile.ZipFile(zip_filename, 'w') as z:\n                z.writestr('module.py', 'pass')\n\n            sys.path.insert(0, str(tempdir))\n            stack.callback(sys.path.pop, 0)\n\n            self.clear_autoreload_caches()\n            self.import_and_cleanup('module')\n            self.assertFileFound(zip_filename)\n\n            with open(zip_filename, 'w') as f:\n                f.write('updated')\n\n            self.assertFileFound(zip_filename)\n"], "sample_909": ["    def test_namedtuple_subclass(self):\n        docstring = NamedtupleSubclass.__doc__\n        doc = GoogleDocstring(docstring, config=Config())\n        self.assertEqual(doc.lines(), [\n            'Sample namedtuple subclass',\n            '',\n            'Attributes',\n            '----------',\n            'attr1 : Arbitrary type',\n            '    Quick description of attr1',\n            'attr2 : Another arbitrary type',\n            '    Quick description of attr2',\n            'attr3 : Type',\n            '',\n            '    Adds a newline after the type',\n        ])\n", "    def test_numpy_namedtuple_subclass(self):\n        docstring = cleandoc(NamedtupleSubclass.__doc__)\n        doc = NumpyDocstring(docstring)\n        parsed_doc = doc.__str__()\n        self.assertEqual(len(parsed_doc.splitlines()), 13)\n        self.assertIn('attr1 : Arbitrary type', parsed_doc)\n        self.assertIn('attr2 : Another arbitrary type', parsed_doc)\n        self.assertIn('attr3 : Type', parsed_doc)\n\n", "    def test_namedtuple_subclass(self):\n        docstring = NamedtupleSubclass.__doc__\n        expected = [\n            'Sample namedtuple subclass',\n            '',\n            'Attributes',\n            '----------',\n            'attr1 : Arbitrary type',\n            '    Quick description of attr1',\n            'attr2 : Another arbitrary type',\n            '    Quick description of attr2',\n            'attr3 : Type',\n            '',\n            '    Adds a newline after the type',\n        ]\n        self.assertEqual(GoogleDocstring(docstring).lines(), expected)\n", "    def test_namedtuple_subclass(self):\n        docstring = cleandoc(\n            NamedtupleSubclass.__doc__\n        )\n        expected = dedent('''\n        .. class:: NamedtupleSubclass\n\n        Sample namedtuple subclass\n\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n        ''')\n        config = Config()\n        docstring = GoogleDocstring(docstring, config)\n        self.assertEqual(docstring.__str__(), expected)\n", "    def test_numpydoc_namedtuple(self):\n        docstring = cleandoc('''\n        Attributes\n        ----------\n        attr1 : Arbitrary type\n            Quick description of attr1\n        attr2 : Another arbitrary type\n            Quick description of attr2\n        attr3 : Type\n\n            Adds a newline after the type\n\n        ''')\n        expected = dedent('''\n        :ivar attr1: Quick description of attr1\n        :ivar attr2: Quick description of attr2\n        :ivar attr3: Adds a newline after the type\n\n        ''')\n        self.assertEqual(NumpyDocstring(docstring).lines(), expected.splitlines())\n"], "sample_1195": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_432": ["    def test_changelist_empty_values(self):\n        empty_user = self._create_superuser(\"empty_user\")\n        request = self.factory.get(reverse(\"admin:adminsite_emptyvaluechild_changelist\"))\n        request.user = empty_user\n        response = EmptyValueChildAdmin.changelist_view(request)\n        self.assertContains(\n            response, \"<td></td>\"\n        )  # Ensure empty values are displayed correctly\n\n", "    def test_get_filtered_objects_for_related_field(self):\n        user = self._create_superuser(\"testuser\")\n        request = self._mocked_authenticated_request(reverse(\"admin:myapp_band_changelist\"), user)\n        request.user = user\n        \n        class TestAdmin(admin.ModelAdmin):\n                return Band.objects.filter(name=\"Test Band\")\n        \n        site = admin.AdminSite()\n        site.register(Band, TestAdmin)\n        cl = site._registry[Band].get_changelist_view(request)\n        \n        self.assertEqual(len(cl.result_list), 1)\n", "    def test_get_ordering_from_request(self):\n        request = self.factory.get(\"/admin/events/event/\")\n        request.GET = {'o': 'name'}\n        # Order by name.\n        self.assertEqual(EventAdmin.get_ordering(request), ['name'])\n\n        request.GET = {'o': '-name'}\n        # Order by name in descending order.\n        self.assertEqual(EventAdmin.get_ordering(request), ['-name'])\n\n        request.GET = {'o': 'date'}\n        # Order by date.\n        self.assertEqual(EventAdmin.get_ordering(request), ['date'])\n\n        request.GET = {'o': 'location__name'}\n        # Order by related Location's name.\n        self.assertEqual(EventAdmin.get_ordering(request), ['location__name'])\n", "    def test_changelist_search_exact_lookup(self):\n        band1 = Band.objects.create(name=\"Band 1\")\n        band2 = Band.objects.create(name=\"Band with spaces\")\n        self.client.force_login(self.superuser)\n        response = self.client.get(reverse(\"admin:music_band_changelist\"))\n        self.assertContains(response, band1.name)\n        self.assertContains(response, band2.name)\n        response = self.client.get(reverse(\"admin:music_band_changelist\") + \"?name__exact=Band 1\")\n        self.assertContains(response, band1.name)\n        self.assertNotContains(response, band2.name)\n", "    def test_changelist_with_empty_search_field(self):\n        user = self._create_superuser(\"testuser\")\n        request = self._mocked_authenticated_request(reverse(\"admin: Musician_changelist\"), user)\n        response = MusicianAdmin.changelist_view(request)\n        self.assertContains(response, \"Search:\")\n        self.assertContains(response, '<input type=\"text\" name=\"q\" value=\"\">')\n        self.assertContains(response, '<input type=\"hidden\" name=\"_popup\" value=\"\">')\n\n"], "sample_572": ["    def test_integrate_kde(self, x, rng):\n        kde = KDE()\n        density, support = kde(x)\n        computed_area = self.integrate(density, support)\n        assert np.isclose(computed_area, 1)\n\n", "    def test_kde_integrate(self, x):\n        kde = KDE()\n        density, support = kde(x)\n        obtained = self.integrate(density, support)\n        expected = 1\n        assert_almost_equal(obtained, expected)\n", "    def test_kde_bivariate_gridsize(self, x, y):\n        kde = KDE(gridsize=20)\n        grid1, grid2 = kde.define_support(x, y)\n        assert len(grid1) == 20\n        assert len(grid2) == 20\n", "    def test_kde_univariate_integrate(self, x, rng):\n        kde = KDE()\n        density, support = kde(x)\n        assert_array_almost_equal(kde.integrate(density, support), 1, 2)\n", "    def test_univariate_cumulative(self, x, rng):\n        kde = KDE(cumulative=True)\n        support = kde.define_support(x)\n        density, _ = kde(x)\n        assert_array_almost_equal(density[-1], 1)\n        assert_array_almost_equal(density, [self.integrate(density, support[:i]) for i in range(1, len(support) + 1)])\n"], "sample_1125": ["    def test_identity_operator_apply(self):\n        I = IdentityOperator()\n        k = Ket('k')\n        assert I*k == k\n", "    def test_IdentityOperator_represent():\n        from sympy.physics.quantum.operator import IdentityOperator\n        I = IdentityOperator(3)\n        m = I._represent_default_basis()\n        assert m == Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n", "    def test_Operator_str(self):\n        A = Operator('A')\n        assert str(A) == 'A'\n", "def test_innerproduct_dagger():\n    from sympy.physics.quantum.state import Ket, Bra\n\n    x, y = symbols('x y')\n    k = Ket('k')\n    b = Bra('b')\n    op = k*b\n    assert adjoint(op) == b*k\n\n", "def test_IdentityOperator_apply():\n    A = Operator('A')\n    ket = symbols('k', cls=Function)\n    ket = ket(0)\n    I = IdentityOperator()\n    assert I * ket == ket\n"], "sample_217": ["    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(attrs={'class': 'input1'}),\n                          TextInput(attrs={'class': 'input2'}))\n                super().__init__(widgets, attrs)\n\n        class MyForm(Form):\n            field = MyMultiWidget()\n\n        form = MyForm()\n        expected_media = Media(\n            {'css': {'all': ['http://media.example.com/static/admin/css/widgets.css']}}\n        )\n        self.assertEqual(form.media, expected_media)\n", "    def test_multiwidget_media(self):\n        class TestMultiWidget(MultiWidget):\n                widgets = (TextInput(attrs={'class': 'foo'}), TextInput())\n                super().__init__(widgets, attrs)\n\n        class TestForm(Form):\n            myfield = TestMultiWidget()\n\n        form = TestForm()\n\n        expected_media = Media(\n            css={'all': ['http://media.example.com/static/admin/css/widgets.css']},\n            js=['http://media.example.com/static/admin/js/vendor/jquery/jquery.js',\n                'http://media.example.com/static/admin/js/forms.js'],\n        )\n        self.assertEqual(form.media, expected_media)\n", "    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(), TextInput())\n                super().__init__(widgets, attrs)\n\n            @property\n                return Media(css={'all': ('my_css.css',)})\n\n        class MyForm(Form):\n            my_field = MyMultiWidget()\n\n        form = MyForm()\n        self.assertEqual(form.media.css.all, ['my_css.css'])\n", "    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(), TextInput())\n                super().__init__(widgets, attrs)\n\n                return Media(css={'all': ('test.css',)})\n\n        class MyForm(Form):\n            field = MyMultiWidget()\n\n        form = MyForm()\n        media = form.media\n        self.assertEqual(media.css.all, ['test.css'])\n", "    def test_multiwidget_media(self):\n        class MyMultiWidget(MultiWidget):\n                widgets = (TextInput(attrs={'class': 'foo'}), TextInput())\n                super().__init__(widgets, attrs)\n\n                return value\n\n        class MyForm(Form):\n            myfield = MyMultiWidget()\n\n        form = MyForm()\n        media = form.media\n        self.assertIn('http://media.example.com/static/', str(media))\n        self.assertIn('foo', str(media))\n\n"], "sample_944": ["    def test_restify_newtype():\n        assert restify(MyInt) == ':class:`MyInt`'\n\n\n", "    def test_stringify_GenericMeta():\n        assert stringify(MyList[str]) == 'MyList[str]'\n        assert stringify(MyList) == 'MyList'\n", "    def test_stringify_system_TypeVar(self):\n        assert stringify(T) == 'T'\n        assert stringify(MyInt) == 'MyInt'\n", "def test_restify_typing_forwardref():\n    from sphinx.util.typing import restify\n    assert restify(ForwardRef('MyClass2')) == ':class:`MyClass2`'\n", "    def test_restify_generic_alias_with_none():\n        assert restify(MyList[Optional[MyInt]]) == ':class:`MyList`\\\\ [\\\\ `Optional`\\\\ [:class:`MyInt`]]'\n"], "sample_984": ["def test_print_MatrixSymbol():\n    assert sstr(MatrixSymbol('A', 2, 3)) == \"MatrixSymbol('A', 2, 3)\"\n", "def test_sstrrepr_complex():\n    assert sstrrepr(1 + 2j) == \"complex(1, 2)\"\n", "def test_Poly():\n    p = Poly(x**2 + 2*x + 1, x)\n    assert sstr(p) == 'Poly(x**2 + 2*x + 1, x, domain=\"ZZ\")'\n\n\n", "def test_sstr_MatrixSymbol():\n    M = MatrixSymbol('X', 2, 3)\n    assert sstr(M) == 'X'\n", "def test_printmethod():\n    class MyExpr(Expr):\n            self.arg = arg\n\n            return f\"{printer.doprint(self.arg)}*\"\n\n    p = StrPrinter()\n    expr = MyExpr(x)\n    assert p.doprint(expr) == \"x*\"\n"], "sample_992": ["def test_print_Piecewise():\n    p = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(p, standard='numpy') == 'numpy.select([x, x**2], [x < 1, True], default=numpy.nan)'\n", "def test_numpy_Piecewise():\n    expr = Piecewise((x, x < 1), (x**2, x >= 1))\n    code = NumPyPrinter().doprint(expr)\n    expected = 'numpy.select([numpy.array(x < 1), numpy.array(x >= 1)], [numpy.array([x]), numpy.array([x**2])], default=numpy.nan)'\n    assert code == expected\n", "    def test_print_SparseMatrix_Assignment(self):\n        m = SparseMatrix([[1, 0, 0],\n                          [0, 0, 1],\n                          [0, 1, 0]])\n        code = pycode(Assignment(x, m), language='python')\n        assert code == 'x = scipy.sparse.coo_matrix([1, 1], ([0, 2], [0, 2]), shape=(3, 3))'\n", "    def test_scipy_sparse_matrix():\n        A = SparseMatrix([[1, 2], [3, 4]])\n        assert pycode(A, language='scipy') == \"scipy.sparse.coo_matrix([1, 2, 3, 4], ([0, 0, 1, 1], [0, 1, 0, 1]), shape=(2, 2))\"\n", "    def test_numpy_select(self):\n        from sympy import Piecewise\n        p = Piecewise((x, x < 0), (y, x >= 0), (z, True))\n        code = NumPyPrinter().doprint(p)\n        assert code == 'numpy.select([{x}, {y}, {z}], [{x < 0}, {x >= 0}, True])'\n"], "sample_192": ["    def test_formset_factory_with_custom_kwarg(self):\n        custom_kwarg = 'test_value'\n        CustomKwargFormSet = formset_factory(CustomKwargForm, extra=1)\n        formset = CustomKwargFormSet(\n            data={'form-0-custom_kwarg': custom_kwarg}, auto_id=False, prefix='form'\n        )\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.forms[0].custom_kwarg, custom_kwarg)\n", "    def test_formset_factory_absolute_max(self):\n        formset_cls = formset_factory(Choice, absolute_max=3)\n        formset = formset_cls(data={'choices-0-choice': 'A', 'choices-0-votes': 1, 'choices-1-choice': 'B', 'choices-1-votes': 2,\n                                  'choices-2-choice': 'C', 'choices-2-votes': 3, 'choices-TOTAL_FORMS': '4'})\n\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.total_form_count(), 3)\n", "    def test_formset_factory_with_custom_kwargs(self):\n        class CustomKwargFormSet(BaseFormSet):\n                super().__init__(*args, **kwargs)\n\n        formset_class = formset_factory(CustomKwargForm, formset=CustomKwargFormSet, extra=1)\n        formset = formset_class(custom_kwarg='test', data={'form-0-name': 'test'})\n        self.assertEqual(formset.forms[0].custom_kwarg, 'test')\n", "    def test_formset_factory_absolute_max(self):\n        formset_class = formset_factory(Choice, absolute_max=3)\n        formset = formset_class(data={'choices-TOTAL_FORMS': '4'})\n        self.assertEqual(formset.total_form_count(), 3)\n        self.assertEqual(len(formset.forms), 3)\n", "    def test_formset_factory_absolute_max(self):\n        formset = formset_factory(\n            FavoriteDrinkForm,\n            max_num=10,\n            absolute_max=5,\n        )\n        formset_instance = formset({'form-TOTAL_FORMS': '6', 'form-INITIAL_FORMS': '0'})\n        self.assertEqual(formset_instance.total_form_count(), 5)\n"], "sample_306": ["    def test_parse_datetime_with_timezone(self):\n        self.assertEqual(\n            parse_datetime('2001-02-03T04:05:06.789Z'),\n            datetime(2001, 2, 3, 4, 5, 6, 789000, tzinfo=utc),\n        )\n        self.assertEqual(\n            parse_datetime('2001-02-03T04:05:06.789+01:00'),\n            datetime(2001, 2, 3, 4, 5, 6, 789000, tzinfo=get_fixed_timezone(60)),\n        )\n", "    def test_parse_datetime_with_timezone(self):\n        aware_dt = parse_datetime(\"2023-10-27T10:00:00+02:00\")\n        self.assertEqual(aware_dt.tzinfo.utcoffset(aware_dt), timedelta(hours=2))\n        self.assertEqual(aware_dt, datetime(2023, 10, 27, 10, 0, 0, tzinfo=get_fixed_timezone(120)))\n\n        aware_dt = parse_datetime(\"2023-10-27T10:00:00-05:00\")\n        self.assertEqual(aware_dt.tzinfo.utcoffset(aware_dt), timedelta(hours=-5))\n        self.assertEqual(aware_dt, datetime(2023, 10, 27, 10, 0, 0, tzinfo=get_fixed_timezone(-300)))\n", "    def test_parse_duration_postgres_interval(self):\n        self.assertEqual(\n            parse_duration(\"3 days 04:05:06\"),\n            timedelta(days=3, seconds=14706),\n        )\n        self.assertEqual(\n            parse_duration(\"-1 days 10:20:30.123456\"),\n            timedelta(days=-1, seconds=37230, microseconds=123456),\n        )\n", "    def test_parse_datetime_with_time_zone(self):\n        aware_dt = parse_datetime('2023-10-26T12:00:00+02:00')\n        self.assertEqual(aware_dt.tzinfo.utcoffset(aware_dt), timedelta(hours=2))\n        self.assertEqual(aware_dt.year, 2023)\n        self.assertEqual(aware_dt.month, 10)\n        self.assertEqual(aware_dt.day, 26)\n        self.assertEqual(aware_dt.hour, 12)\n        self.assertEqual(aware_dt.minute, 0)\n        self.assertEqual(aware_dt.second, 0)", "    def test_parse_datetime_with_timezone(self):\n        dt_str = '2023-10-27T12:34:56+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 12)\n        self.assertEqual(dt.minute, 34)\n        self.assertEqual(dt.second, 56)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n"], "sample_1168": ["def test_generate_oriented_forest():\n    for n in range(1, 5):\n        for p in generate_oriented_forest(n):\n            assert len(p) == n\n            assert all(0 <= x < n for x in p)\n\n        assert len([i for i in generate_oriented_forest(n)]) == factorial(n-1)\n", "    def test_is_palindromic():\n        assert is_palindromic(\"racecar\") is True\n        assert is_palindromic(\"madam\") is True\n        assert is_palindromic(\"level\") is True\n        assert is_palindromic(\"rotor\") is True\n        assert is_palindromic(\"kayak\") is True\n        assert is_palindromic(\"a\") is True\n        assert is_palindromic(\"\") is True\n        assert is_palindromic(\"apple\") is False\n        assert is_palindromic(\"banana\") is False\n        assert is_palindromic([1, 2, 3, 2, 1]) is True\n        assert is_palindromic([1, 2, 3, 4, 5]) is False\n", "    def test_is_palindromic():\n        assert is_palindromic(\"\") is True\n        assert is_palindromic(\"a\") is True\n        assert is_palindromic(\"madam\") is True\n        assert is_palindromic(\"racecar\") is True\n        assert is_palindromic(\"level\") is True\n        assert is_palindromic(\"rotor\") is True\n        assert is_palindromic(\"12321\") is True\n        assert is_palindromic(\"12321\") is True\n        assert is_palindromic([1, 2, 3, 2, 1]) is True\n        assert is_palindromic(\"ab\") is False\n        assert is_palindromic(\"abcba\") is True\n        assert is_palindromic((1, 2, 3, 2, 1)) is True\n        assert is_palindromic([1, 2, 3, 4, 1]) is False\n\n", "def test_permute_signs():\n    assert list(permute_signs((1, 2, 3))) == [(1, 2, 3),\n                                              (1, -2, 3),\n                                              (1, 2, -3),\n                                              (1, -2, -3)]\n    assert list(permute_signs((0, 1))) == [(0, 1), (0, -1)]\n    assert list(permute_signs((0, 1, 1))) == [(0, 1, 1), (0, -1, 1),\n                                              (0, 1, -1), (0, -1, -1)]\n\n", "    def test_permute_signs():\n        assert list(permute_signs((0, 1, 2))) == [\n            (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)\n        ]\n"], "sample_400": ["    def test_autodetector_rename_model_with_fk(self):\n        before = [\n            self.author,\n            self.book,\n        ]\n        after = [\n            self.renamed_author,\n            self.book_with_author_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameModel\"])\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 0, old_name=\"author\", new_name=\"NewAuthor\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"AlterField\", field=\"author\"\n        )\n", "    def test_remove_ManyToManyField_with_related_name(self):\n        before_state = self.make_project_state(\n            [\n                self.author_with_book,\n                self.book,\n            ]\n        )\n        after_state = self.make_project_state(\n            [\n                self.author_with_book,\n                self.book_no_related_name,\n            ]\n        )\n\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"AlterField\"])\n\n", "    def test_field_rename_with_default(self):\n        \"\"\"\n        #23312 - Ensure field rename with default value is handled correctly\n        \"\"\"\n        before = [\n            self.author_with_name,\n        ]\n        after = [\n            self.author_with_name_renamed,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(\n            changes, \"testapp\", 0, [\"RenameField\", \"AlterField\"]\n        )\n        change_file_path = self.get_migration_path()\n        # ensure default value is included in the migration\n        with open(change_file_path, 'r') as f:\n            content = f.read()\n        assert 'default=1' in content\n\n", "    def test_rename_model_with_m2m_through_field_with_new_fieldname(self):\n        # Ensure renaming a model with an M2M through field doesn't break\n        # when the field name in the through model changes.\n\n        before_state = self.make_project_state(\n            [\n                self.author_with_m2m_through,\n                self.contract,\n            ]\n        )\n        after_state = self.make_project_state(\n            [\n                self.author_with_renamed_m2m_through,\n                self.deal,\n            ]\n        )\n\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, \"testapp\", 2)\n        self.assertOperationTypes(\n            changes,\n            \"testapp\",\n            0,\n            [\"RenameModel\", \"AlterField\", \"RenameModel\"],\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 0, 1, name=\"Contract\", old_name=\"Deal\"\n        )\n        self.assertOperationAttributes(\n            changes, \"testapp\", 1, 0, name=\"Author\", old_name=\"Author2\"\n        )\n", "    def test_rename_model_with_many_to_many_through_field(self):\n        before = [\n            self.author,\n            self.contract,\n        ]\n        after = [\n            self.author_renamed,\n            self.contract_renamed_through,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'testapp', 2)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'AlterField'])\n        self.assertOperationTypes(changes, 'testapp', 1, ['RenameModel'])\n"], "sample_138": ["    def test_manifest_strict_missing_entry(self):\n        # Create a simple file structure\n        os.makedirs(os.path.join(TEST_ROOT, \"test_app\", \"static\", \"css\"))\n        with open(os.path.join(TEST_ROOT, \"test_app\", \"static\", \"css\", \"style.css\"), 'w') as f:\n            f.write('body { color: black; }')\n\n        # Configure settings for ManifestStaticFilesStorage\n        with override_settings(STATICFILES_STORAGE='django.contrib.staticfiles.storage.ManifestStaticFilesStorage'):\n            # Collect static files, forcing a manifest creation\n            call_command('collectstatic', verbosity=0, interactive=False)\n\n            # Remove the manifest entry for style.css to simulate a missing entry\n            manifest_path = os.path.join(settings.STATIC_ROOT, 'staticfiles.json')\n            with open(manifest_path, 'r+') as f:\n                manifest_data = json.load(f)\n                manifest_data['paths'].pop('test_app/static/css/style.css')\n                f.seek(0)\n                f.truncate()\n                json.dump(manifest_data, f)\n\n            # Check if the hashed file exists after CollectstaticCommand\n\n            self.assertFalse(os.path.exists(os.path.join(settings.STATIC_ROOT,\n                                                       self.hashed_file_path(\"test_app/static/css/style.css\"))))\n\n            # Assert that trying to access the original file raises the expected error\n            with self.assertRaises(ValueError) as cm:\n                storage.staticfiles_storage.url('test_app/static/css/style.css')\n            self.assertIn(\"Missing staticfiles manifest entry for 'test_app/static/css/style.css'\",\n                          str(cm.exception))\n", "    def test_hashed_file_post_process(self):\n        \"\"\"\n        Test that hashed files are correctly generated and saved during\n        post-processing.\n        \"\"\"\n        self.create_static_file('style.css')\n\n        # Create a temporary directory to store the hashed files.\n        temp_dir = tempfile.mkdtemp()\n        settings.STATIC_ROOT = temp_dir\n\n        try:\n            # Run collectstatic to generate the hashed files.\n            call_command('collectstatic', interactive=False, verbosity=0)\n\n            # Assert that the hashed file exists and has the correct content.\n            original_content = self.read_static_file('style.css')\n            hashed_file_path = self.hashed_file_path('style.css')\n            hashed_content = self.read_static_file(hashed_file_path)\n            self.assertNotEqual(original_content, hashed_content)\n            self.assertTrue(hashed_content.endswith(f'\\n/* hashed */'))\n\n        finally:\n            # Clean up the temporary directory.\n            shutil.rmtree(temp_dir)\n", "    def test_hashed_files_from_manifest(self):\n        \"\"\"\n        Test that hashed files are loaded from the manifest on initialization.\n        \"\"\"\n        test_dir = Path(TEST_ROOT, self.test_dir)\n        test_file = test_dir / 'test.css'\n        test_file.write_text('body { color: red; }')\n\n        # Create a storage with a manifest.\n        manifest_storage = storage.ManifestStaticFilesStorage(location=str(tempfile.mkdtemp()))\n        manifest_storage.save('test.css')\n        manifest_storage.save_manifest()\n\n        # Create a new storage instance, which should load the manifest.\n        new_storage = storage.ManifestStaticFilesStorage(location=str(tempfile.mkdtemp()))\n        self.assertEqual(new_storage.hashed_files.get(test_file.name), 'test.css')\n", "    def test_hash_collision(self):\n        \"\"\"\n        Test that files with the same hash are not overwritten.\n\n        This tests that the storage backend handles hash collisions properly.\n        When two different files have the same hash, the storage should\n        create unique filenames to avoid overwriting.\n        \"\"\"\n        # Create two files with the same content but different names\n        file1_path = self.get_test_path('file1.css')\n        file2_path = self.get_test_path('file2.css')\n        with open(file1_path, 'w') as f:\n            f.write('body { color: red; }')\n        with open(file2_path, 'w') as f:\n            f.write('body { color: red; }')\n\n        # Add the files to the staticfiles finder\n        finders.find(all=True)\n\n        # Run the collectstatic command\n        out = StringIO()\n        err = StringIO()\n\n        call_command('collectstatic', stdout=out, stderr=err,\n                     verbosity=0)\n        \n        # Check if the output contains the expected messages\n        self.assertFalse('All' in out.getvalue())\n        self.assertFalse('All' in err.getvalue())\n\n        # Check if both files have been stored with unique filenames\n        hashed_file1_path = self.hashed_file_path(\n            file1_path\n        )\n        hashed_file2_path = self.hashed_file_path(\n            file2_path\n        )\n        self.assertTrue(\n            storage.staticfiles_storage.exists(hashed_file1_path)\n        )\n        self.assertTrue(\n            storage.staticfiles_storage.exists(hashed_file2_path)\n        )\n        self.assertNotEqual(\n            hashed_file1_path, hashed_file2_path\n        )\n\n        self.assertPostCondition()\n", "    def test_manifest_strict_mode(self):\n        \"\"\"\n        Test that manifest strict mode raises an error when a file is referenced\n        that doesn't exist in the manifest.\n        \"\"\"\n        settings.STATICFILES_STORAGE = 'django.contrib.staticfiles.storage.ManifestStaticFilesStorage'\n        with tempfile.TemporaryDirectory() as tmpdir:\n            settings.STATIC_ROOT = tmpdir\n            self._setup()\n\n            # Create a manifest file with one entry.\n            manifest_data = {\n                'paths': {'css/style.css': 'css/style-abcdef.css'},\n                'version': '1.0'\n            }\n            manifest_path = os.path.join(tmpdir, storage.ManifestStaticFilesStorage.manifest_name)\n            with open(manifest_path, 'w') as f:\n                json.dump(manifest_data, f)\n\n            # Access a file that's not in the manifest.\n            with self.assertRaises(ValueError) as cm:\n                storage.staticfiles_storage.url('css/missing.css')\n\n            self.assertIn(\"Missing staticfiles manifest entry for 'css/missing.css'\", str(cm.exception))\n\n"], "sample_958": ["def test_function_simple():\n    check(\n      \"function\",\n      \"void myfunc(int a, int b)\",\n      {1: \"myfunc\"},\n      \"void myfunc(int a, int b)\",\n      \"myfunc\",\n    )\n", "def test_cpp_enum_scoped():\n    check(\n        'enum',\n        'enum { Value1, Value2 };',\n        {},\n        'enum { Value1, Value2 }',\n        key='MyEnum ',\n        asTextOutput=\"MyEnum { Value1, Value2 }\",\n    )\n", "def test_function_template_return():\n\n    check('function',\n          'template<typename T> T foo(int x)',\n          {1: 'foo', 2: 'cpp_function_template_foo', 3: 'cpp_function_template_foo_T_int'},\n          'T foo(int x) ', key='template<typename T>',\n          asTextOutput='foo(int x)')\n", "    def test_enum_unnamed_simple():\n        check(\n            'enum',\n            'enum {{ a, b, c }};',\n            {1: None, 2: 'cpp:enum'},\n            'enum { a, b, c };',\n        )\n        \n", "def test_cpp_parser_enum_values():\n    check(\n        'enum',\n        'enum { a, b = 5, c };',\n        {1: 'enum'},\n        output='enum { a, b = 5, c }',\n        key='enum'\n    )\n"], "sample_337": ["    def test_csrf_failure_view_called_when_token_missing(self):\n        request = HttpRequest()\n        request.method = 'POST'\n        request.META['CSRF_COOKIE'] = MASKED_TEST_SECRET1\n        with self.assertRaisesMessage(RejectRequest, REASON_CSRF_TOKEN_MISSING):\n            CsrfViewMiddleware().process_view(request, lambda: None, (), {})\n", "    def test_get_token_generates_different_tokens(self):\n        with self.settings(CSRF_USE_SESSIONS=False):\n            token1 = get_token(HttpRequest())\n            token2 = get_token(HttpRequest())\n            self.assertNotEqual(token1, token2)\n", "    def test_csrf_trusted_origin_https(self):\n        request = HttpRequest()\n        request.method = 'POST'\n        request.META['HTTP_ORIGIN'] = 'https://example.com'\n        request.META[\"CSRF_COOKIE\"] = MASKED_TEST_SECRET1\n        middleware = CsrfViewMiddleware()\n        response = middleware.process_view(request, lambda r: HttpResponse(), (), {})\n        self.assertIsNone(response)\n", "    def test_requires_csrf_token_decorator_with_bad_token(self):\n        request = HttpRequest()\n        request.method = 'POST'\n        request.META['CSRF_COOKIE'] = MASKED_TEST_SECRET1\n        request.POST = {'csrfmiddlewaretoken': 'bad'}\n        response = requires_csrf_token(post_form_view)(request)\n        self.assertEqual(response.status_code, 403)\n        self.assertIn('CSRF token from POST incorrect.', response.content.decode())\n", "    def test_rotate_token(self):\n        request = HttpRequest()\n        request.META['CSRF_COOKIE'] = MASKED_TEST_SECRET1\n        rotate_token(request)\n        self.assertEqual(request.META['CSRF_COOKIE'], MASKED_TEST_SECRET2)\n        self.assertTrue(request.csrf_cookie_needs_reset)\n        # Ensure the cookie hasn't actually been set yet.\n        self.assertFalse hasattr(request, 'csrf_secure')\n\n"], "sample_609": ["    def test_unify_chunks_multi_datasets():\n        ds1 = xr.Dataset({'a': (('x', 'y'), np.ones((3, 4))), 'b': (('x', 'y'), np.ones((3, 4)))},\n                        chunks={'x': 2, 'y': 2})\n        ds2 = xr.Dataset({'c': (('x', 'y'), np.ones((3, 4))), 'd': (('x', 'y'), np.ones((3, 4)))},\n                        chunks={'x': 1, 'y': 4})\n        ds1_unified, ds2_unified = unify_chunks(ds1, ds2)\n\n        # Check chunk sizes\n        assert ds1_unified['a'].chunks == (2, 4)\n        assert ds2_unified['c'].chunks == (2, 4)\n\n        # Make sure data is not changed\n        assert_identical(ds1_unified['a'].data, ds1['a'].data)\n        assert_identical(ds2_unified['c'].data, ds2['c'].data)\n\n\n\n", "    def test_result_name(self, has_dask):\n        if has_dask and not has_dask:\n            pytest.skip(\"Test requires dask.\")\n\n        # Test with single variable\n        da = xr.DataArray(np.arange(10), dims=[\"x\"])\n        expected_name = \"result\"\n        actual_name = result_name(da, None)\n        assert actual_name == expected_name\n\n        # Test with multi-variable operation\n        da1 = xr.DataArray(np.arange(10), dims=[\"x\"])\n        da2 = xr.DataArray(np.arange(10), dims=[\"x\"])\n        expected_name = \"result\"\n        actual_name = result_name(da1, da2)\n        assert actual_name == expected_name\n\n        # Test with named output\n        expected_name = \"my_result\"\n        actual_name = result_name(da1, da2, output_name=\"my_result\")\n        assert actual_name == expected_name\n\n        # Test with dask array input\n        if has_dask:\n            da = xr.DataArray(np.arange(10), dims=[\"x\"]).chunk({\"x\": 2})\n            expected_name = \"result\"\n            actual_name = result_name(da, None)\n            assert actual_name == expected_name\n\n", "    def test_result_name():\n        assert result_name(\"foo\", \"bar\") == \"foo_bar\"\n        assert result_name(\"foo\", None) == \"foo\"\n        assert result_name(None, \"bar\") == \"bar\"\n        assert result_name(None, None) is None\n", "    def test_apply_ufunc_dask_chunks():\n        da = xr.DataArray(np.arange(100).reshape((10, 10)), chunks=(5, 5))\n        result = apply_ufunc(np.sqrt, da)\n        assert isinstance(result.data, dask.array.Array)\n        assert result.chunks == da.chunks\n\n\n", "    def test_unify_chunks_mixed_dims(self, dim):\n        da1 = xr.DataArray(\n            np.arange(10), dims=[\"x\"], chunks=(5,), name=\"a\"\n        ).to_dataset()\n        da2 = xr.DataArray(\n            np.arange(20).reshape(4, 5), dims=[\"x\", \"y\"], chunks=(2, 5), name=\"b\"\n        ).to_dataset()\n\n        if dim is None:\n            da1 = da1.drop_dims(\"x\")\n        unified = unify_chunks(da1, da2)\n        assert unified[0].chunks == (\n            (5,) if dim == \"x\" else (None,),\n            \n        )\n        assert unified[1].chunks == ((2, 2), (5,)) if dim == \"x\" else ((None, None), (5,))    \n"], "sample_388": ["    def test_remote_user_create_unknown_user_false(self):\n        # Create a new client to handle requests with the REMOTE_USER header.\n        client = Client()\n\n        # Set the REMOTE_USER header to a non-existent username.\n        client.environ = {self.header: \"unknownuser\"}\n        request = client.get(\"/\")\n\n        # Since create_unknown_user is False, the backend should not create a user.\n        self.assertFalse(User.objects.filter(username=\"unknownuser\").exists())\n\n        # The authentication should fail as the user doesn't exist.\n        self.assertIsNone(request.user)\n\n", "    def test_remote_user_authentication_create_unknown_user(self):\n        # With create_unknown_user=True, a new User object should be created if\n        # the REMOTE_USER value is not found in the database.\n\n        with self.client:\n            self.client.environ['REMOTE_USER'] = 'newuser'\n            response = self.client.get('/')\n            self.assertEqual(response.status_code, 200)\n            self.assertTrue(User.objects.filter(username='newuser').exists())\n", "    def test_configure_user_called(self):\n        remote_user = \"testuser\"\n\n        with self.settings(\n            AUTHENTICATION_BACKENDS=[\n                \"django.contrib.auth.backends.RemoteUserBackend\"\n            ],\n            MIDDLEWARE=[\n                \"django.contrib.auth.middleware.RemoteUserMiddleware\",\n            ],\n        ):\n            class MockRemoteUserBackend(RemoteUserBackend):\n                    self.user = user\n                    self.created = created\n                    return user\n\n            backend = MockRemoteUserBackend()\n            request = self.create_request(HTTP_REMOTE_USER=remote_user)\n            backend.authenticate(request)\n            self.assertEqual(backend.user.username, remote_user)\n            self.assertTrue(backend.created)\n\n", "    def test_remote_user_create_unknown_user(self):\n        with self.settings(REMOTE_USER=self.known_user):\n            response = self.client.get('/')\n            self.assertEqual(response.status_code, 200)\n            self.client.logout()\n            user = authenticate(remote_user=self.known_user)\n            self.assertIsNotNone(user)\n            self.assertEqual(user.username, self.known_user)\n", "    def test_remote_user_create_unknown_user(self):\n        with self.settings(REMOTE_USER=self.known_user):\n            response = self.client.get('/')\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue(User.objects.filter(username=self.known_user).exists())\n\n"], "sample_511": ["    def test_imshow_data_argument():\n        fig, ax = plt.subplots()\n        data = np.random.rand(10, 10)\n\n        ax.imshow(data, data=data)\n        assert ax.images[0].get_array() is data\n\n        # Make sure it doesn't raise an error if we pass data twice\n        with pytest.raises(ValueError):\n            ax.imshow(data, data=data, cmap='viridis')\n", "    def test_xlabel():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n        ax.set_xlabel('My XLabel')\n        fig.canvas.draw()\n        assert ax.get_xlabel() == 'My XLabel'\n", "    def test_imshow_data():\n        fig, ax = plt.subplots()\n        data = np.random.rand(10, 10)\n        img = ax.imshow(data, data=data)\n        assert img.get_array() is data\n", "def test_set_xscale():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2, 3, 4], [1, 4, 9, 16])\n\n    ax.set_xscale('log')\n    assert ax.get_xscale() == 'log'\n", "    def test_imshow_data():\n        fig, ax = plt.subplots()\n        data = np.random.rand(10, 10)\n        img = ax.imshow(data, data=data)\n        assert img.get_array() is data\n\n"], "sample_785": ["    def test_cv_with_sparse_matrix_and_group_kfold(self):\n        X = coo_matrix([[1, 2], [3, 4]])\n        groups = np.array([0, 1])\n        cv = GroupKFold(n_splits=2)\n        for train, test in cv.split(X, groups=groups):\n            assert isinstance(train, np.ndarray)\n            assert isinstance(test, np.ndarray)\n", "    def test_gridsearchcv_with_predefined_split(self):\n        # Test that GridSearchCV works with a PredefinedSplit cross-validator.\n        X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n        y = np.array([0, 0, 1, 1])\n        test_fold = [0, 1, -1, 1]\n        ps = PredefinedSplit(test_fold)\n\n        clf = SVC(kernel='linear')\n        param_grid = {'C': [1, 10]}\n        grid = GridSearchCV(clf, param_grid, cv=ps)\n        grid.fit(X, y)\n\n        assert_equal(grid.best_params_, {'C': 1})\n\n", "def test_nested_cv():\n    \"\"\"Test cross-validation inside cross-validation\"\"\"\n    X, y = make_classification(n_samples=100, random_state=42)\n    cv_outer = KFold(n_splits=3, shuffle=True, random_state=42)\n    cv_inner = KFold(n_splits=2, shuffle=True, random_state=42)\n    clf = MockClassifier()\n    scores = cross_val_score(clf, X, y, cv=cv_outer, scoring='accuracy',\n                            n_jobs=1)\n    # Assert that the number of scores is correct\n    assert_equal(len(scores), 3)\n\n", "def test_cross_val_score_classes_with_sparse_matrix():\n   # test that cross_val_score works with sparse matrix input\n   X = coo_matrix(np.eye(10))\n   y = np.arange(10) // 2\n   clf = MockClassifier()\n   scores = cross_val_score(clf, X, y, cv=3)\n   assert_equal(scores.shape[0], 3)\n", "    def test_train_test_split_stratify_pandas(self):\n        df = MockDataFrame({'A': np.arange(10), 'B': [0, 1] * 5})\n        train, test = train_test_split(df, test_size=0.25, stratify=df['B'])\n        assert len(train) == 7.5\n        assert len(test) == 2.5\n        assert np.all(train['B'].value_counts() / len(train) == test['B'].value_counts() / len(test))\n"], "sample_1007": ["    def test_binomial_evalf(self):\n        n = Symbol('n', integer=True)\n        assert binomial(n, 5).evalf(subs={n: 10}) == binomial(10, 5)\n        assert binomial(5, n).evalf(subs={n: 2}) == binomial(5, 2)\n", "def test_ff_poly():\n    x = Poly(x**2 + 2*x + 1, x)\n    assert ff(x, 2) == Poly(x**4 + 4*x**3 + 6*x**2 + 4*x + 1, x)\n", "def test_binomial_expand_func_2():\n    n = Symbol('n', integer=True, positive=True)\n    assert expand_func(binomial(n, 3)) == n*(n - 2)*(n - 1)/6\n", "    def test_binomial_expand_func(self):\n        n = Symbol('n', integer=True, positive=True)\n        k = Symbol('k', integer=True)\n        assert binomial(n, k).expand(func=True) == Piecewise((n**k/factorial(k), Eq(k, 0)),\n        (n*(n - 1)*(n - 2) / 6, Eq(k, 3)), (binomial(n, k), True))\n", "def test_binomial_expand_negative_k():\n    n = Symbol('n', integer=True, positive=True)\n   \n\n    assert expand_func(binomial(n, -1)) == 0\n\n"], "sample_1006": ["def test_binomial_expand_func():\n    n = Symbol('n', integer=True)\n    k = Symbol('k', integer=True)\n    assert binomial(n, 3).expand_func() ==  n*(n - 2)*(n - 1)/6\n", "    def test_binomial_expand_func_with_symbol_in_k():\n        n = symbols('n', integer=True, positive=True)\n        k = n+2\n        assert binomial(n, k)._eval_expand_func() == binomial(n, n + 2)\n\n", "def test_binomial_expand_func():\n    n = Symbol('n')\n    k = Symbol('k')\n    assert binomial(n, 3).expand_func() == n*(n - 2)*(n - 1)/6\n", "    def test_binomial_expand_func(self):\n        n = Symbol('n', integer=True)\n        k = Symbol('k', integer=True)\n        assert binomial(n, k).expand_func() == binomial(n, k)\n        assert binomial(n, 2).expand_func() == n*(n - 1)/2\n        assert binomial(n, 3).expand_func() == n*(n - 1)*(n - 2)/6 \n\n", "    def test_binomial_expand_func(self):\n        n = Symbol('n', integer=True, positive=True)\n        k = Symbol('k', integer=True)\n        assert binomial(n, k).expand(func=True) == binomial(n, k)\n        assert binomial(n + 1, k + 1).expand(func=True) == (n + 1) * binomial(n, k) / (k + 1)\n        assert binomial(n, 2).expand(func=True) == n * (n - 1) / 2\n"], "sample_333": ["    def test_get_initial_for_field(self):\n        form = Person(initial={'first_name': lambda: 'Justin'})\n        self.assertEqual(form.get_initial_for_field(None, 'first_name'), 'Justin')\n", "    def test_form_invalid_data(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': 'invalid_date'}\n        form = Person(data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('birthday', form.errors)\n", "    def test_non_field_errors(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': 'invalid'}\n        form = Person(data)\n        form.full_clean()\n\n        self.assertIsNotNone(form.errors.get(NON_FIELD_ERRORS))\n        self.assertEqual(len(form.errors.get(NON_FIELD_ERRORS)), 1)\n", "    def test_form_media(self):\n        form = PersonNew()\n        media = form.media\n        self.assertEqual(media.css, {'all': ['<link rel=\"stylesheet\" type=\"text/css\" href=\"/static/admin/css/base.css\">']})\n        self.assertEqual(\n            media.js,\n            [\n                '<script type=\"text/javascript\" src=\"/static/admin/js/jquery.js\"></script>',\n                '<script type=\"text/javascript\" src=\"/static/admin/js/django.js\"></script>',\n                '<script type=\"text/javascript\" src=\"/static/admin/js/forms.js\"></script>',\n            ],\n        )\n\n", "    def test_as_ul(self):\n        p = PersonNew({'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'})\n        self.assertEqual(p.as_ul(), '<li><label for=\"first_name_id\">first_name:</label> <input type=\"text\" name=\"first_name\" value=\"John\" id=\"first_name_id\"><span class=\"helptext\"></span></li><li><label for=\"id_last_name\">last_name:</label> <input type=\"text\" name=\"last_name\" value=\"Doe\" id=\"id_last_name\"><span class=\"helptext\"></span></li><li><label for=\"id_birthday\">birthday:</label> <input type=\"text\" name=\"birthday\" value=\"2000-01-01\" id=\"id_birthday\"><span class=\"helptext\"></span></li>')\n"], "sample_457": ["    def test_unique_constraint_opclasses(self):\n        with atomic():\n            # Ensure the opclasses are correctly used in the constraint\n\n            Product.objects.create(name='Product 1', slug='product-1', price=10.00)\n            with self.assertRaises(IntegrityError):\n                Product.objects.create(\n                    name='Product 2', slug='product-2', price=10.00,\n                )\n            # Check that the opclasses are present in the constraint definition\n            constraints = get_constraints(Product._meta.db_table)\n            constraint = next(\n                c for c in constraints if c['name'] == 'unique_name_slug_price'\n            )\n            self.assertEqual(constraint['columns'], ['name', 'slug', 'price'])\n            self.assertEqual(constraint['opclasses'], ['text_pattern_ops', 'text_pattern_ops', 'numeric_ops'])\n", "    def test_unique_constraint_with_expressions_and_conditions(self):\n        with atomic(), ignore_warnings(RemovedInDjango60Warning):\n            Product.objects.create(name=\"Foo\", price=10, description=\"bar\")\n            constraint = product_constraints.get(name=\"unique_name_price_description_lower\")[0]\n            self.assertEqual(\n                len(constraint.expressions), 2\n            )\n            self.assertEqual(\n                constraint.expressions[0].name, 'lower'\n            )\n            self.assertEqual(\n                constraint.expressions[1].name, 'price'\n            )\n            self.assertIsNotNone(constraint.condition)\n\n", "    def test_unique_constraint_with_expressions_and_deferrable(self):\n        with atomic(), ignore_warnings(RemovedInDjango60Warning):\n            # Create a deferred unique constraint with expressions.\n            constraint = UniqueConstraint(\n                expressions=[\n                    F(\"name\") + \"abc\",\n                    Lower(F(\"slug\")),\n                ],\n                name=\"deferrable_unique_constraint\",\n                deferrable=UniqueConstraint.Deferrable.DEFERRED,\n            )\n            Product.add_to_class(\"unique_together\", constraint)\n\n            # No error when inserting the first product.\n            Product.objects.create(name=\"Product 1\", slug=\"product-1\")\n\n            # Deferred constraint violation won't be triggered immediately.\n            Product.objects.create(name=\"Product 2abc\", slug=\"PRODUCT-1\")\n        \n            # Triggering the constraint check should raise a ValidationError.\n            with self.assertRaises(ValidationError):\n                Product.objects.create(name=\"Product 3abc\", slug=\"PRODUCT-1\")\n\n", "    def test_unique_constraint_with_expressions_deferrable(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(\n                name=\"Product 1\",\n                description=\"Description 1\",\n                rating=5,\n            )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 1\",\n                    description=\"Description 2\",\n                    rating=5,\n                )\n        with atomic():\n            UniqueConstraintDeferrable.objects.create(\n                name=\"Product 1\",\n                description=\"Description 1\",\n                rating=5,\n            )\n            UniqueConstraintDeferrable.objects.create(\n                 name=\"Product 1\",\n                 description=\"Description 2\",\n                 rating=5,\n            )\n", "    def test_unique_constraint_expressions_with_deferrable(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE deferrable_products (id INT PRIMARY KEY, name VARCHAR(255) UNIQUE DEFERRABLE INITIALLY IMMEDIATE)\")\n        with atomic():\n            UniqueConstraintProduct.objects.create(name=\"Test\")\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(name=\"Test\")\n        with atomic():\n            UniqueConstraintDeferrable.objects.create(name=\"Test\")\n            UniqueConstraintDeferrable.objects.create(name=\"Test\")  # Will not raise an error\n\n"], "sample_1203": ["def test_homomorphism_isomorphism_with_trivial_kernel():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = AlternatingGroup(4)\n    (check, T) = group_isomorphism(G, H)\n    assert check\n    assert T.kernel().order() == 1\n", "    def test_isomorphism_between_cyclic_group_and_dihedral():\n        D = DihedralGroup(8)\n        C = CyclicGroup(8)\n        assert is_isomorphic(D, C) is False\n", "def test_homomorphism_restrict_to():\n    G = CyclicGroup(5)\n    H = CyclicGroup(5).subgroup([2])\n    phi = homomorphism(G, G, [G.generators[0]], [G.generators[0]**2])\n    phi_restricted = phi.restrict_to(H)\n    assert phi_restricted.domain == H\n    assert phi_restricted(H.generators[0]) == G.generators[0]**2\n", "def test_homomorphism_identity():\n    F, a, b = free_group(\"a, b\")\n    G = FpGroup(F, [a**3, b**3, (a*b)**2])\n    H = CyclicGroup(6)\n\n    H_gens = list(H.generators)\n    i = homomorphism(G, H, G.generators, [H_gens[0]]*len(G.generators))\n    assert i.is_homomorphism()\n    assert i.is_surjective() == False\n    assert i.is_injective() == False\n\n", "def test_homomorphism_isomorphism_cyclic_groups():\n    C3 = CyclicGroup(3)\n    C3_perm = PermutationGroup([Permutation(0, 1, 2)])\n    assert is_isomorphic(C3, C3_perm)\n\n"], "sample_1105": ["def test_combine_permutations():\n    X = PermutationMatrix([[0, 1, 2], [1, 2, 0], [2, 0, 1]])\n    Y = PermutationMatrix([[0, 1], [1, 0]])\n    assert isinstance(MatMul(X, Y).doit(), PermutationMatrix)\n    assert isinstance(MatMul(Y, Y).doit(), Identity)\n", "def test_combine_permutations():\n    P1 = PermutationMatrix([\n        (0, 1, 2, 3),\n        (1, 2, 3, 0),\n    ])\n    P2 = PermutationMatrix([\n        (0, 1),\n        (2, 3),\n    ])\n    assert Mul(P1, P2) == (P1*P2).doit()\n", "    def test_issue_15684():\n        X = MatrixSymbol('X', 2, 2)\n        Y = MatrixSymbol('Y', 2, 2)\n        expr = MatMul(X, Y.T)\n        with assuming(Q.orthogonal(X)):\n            assert refine(expr) == MatMul(X, Y.T)\n", "    def test_MatMul_simplify_scalar_inverse():\n        X = MatrixSymbol('X', 2, 2)\n        expr = MatMul(2, Inverse(X), X)\n        assert expr.doit() == 2 * Identity(2)\n", "def test_matmul_scalar_matrix_inverse():\n    X = MatrixSymbol('X', 2, 2)\n    Y = MatrixSymbol('Y', 2, 2)\n    expr = 2*X * X.I * Y\n    assert expr.doit() == 2*Y\n    expr = 2*X * Y * X.I\n    assert expr.doit() == 2*Y\n"], "sample_26": ["    def test_section_read_write1(self):\n        hdu = fits.PrimaryHDU(np.arange(100).reshape((10, 10)))\n        hdul = fits.HDUList([hdu])\n        hdul.writeto('test.fits')\n        hdul.close()\n\n        with fits.open('test.fits') as hdul:\n            data = hdul[0].section[(1, 2):(5, 8)].copy()\n\n        assert_equal(data, np.arange(36).reshape((4, 6)))\n\n        with fits.open('test.fits', mode='update') as hdul:\n            hdul[0].section[(1, 2):(5, 8)] = data * 2\n", "    def test_image_hdu_section(self):\n        hdu = fits.ImageHDU(np.arange(100).reshape((10, 10)))\n        s = hdu.section[(2:5, 3:8)]\n        assert_equal(s[:], hdu.data[2:5, 3:8])\n        assert isinstance(s, fits.Section)\n\n        # Test scalar access\n        hdu = fits.ImageHDU(\n            np.arange(100).reshape((10, 10)),\n        )\n\n        s = hdu.section[5, 3]\n        assert_equal(s, hdu.data[5, 3])\n        assert isinstance(s, np.int64)\n\n", "    def test_image_hdus(self):\n        \"\"\"Test creation and manipulation of ImageHDU objects.\"\"\"\n        # Create an ImageHDU with some data\n        hdu = fits.ImageHDU(data=np.arange(100).reshape((10, 10)))\n\n        # Verify that the data is correctly stored\n        assert_equal(hdu.data, np.arange(100).reshape((10, 10)))\n\n        # Modify the data in the HDU\n        hdu.data[0, 0] = 999\n\n        # Verify that the data has been modified\n        assert_equal(hdu.data[0, 0], 999)\n\n        # Write the HDU to a file\n        hdu.writeto('test.fits')\n\n        # Read the HDU from the file\n        hdu_read = fits.open('test.fits')[0]\n\n        # Verify that the data is the same\n        assert_equal(hdu.data, hdu_read.data)\n\n        # Clean up the test file\n        os.remove('test.fits')\n", "    def test_image_hdu_section_getitem_slice_uint(self):\n        \"\"\"Test getting a section of a uint image.\"\"\"\n\n        hdu = fits.ImageHDU(np.array([[1, 2, 3], [4, 5, 6]], dtype=np.uint8))\n        section = hdu.section\n\n        # Test getting a single element\n        assert_equal(section[0, 0], 1)\n\n        # Test getting a row\n        assert_equal(section[0, :].tolist(), [1, 2, 3])\n\n        # Test getting a column\n        assert_equal(section[:, 0].tolist(), [1, 4])\n\n        # Test getting a sub-section\n        assert_equal(section[0:2, 1:3].tolist(), [[2, 3], [5, 6]])\n\n", "    def test_image_hdu_section_getitem(self):\n        hdu = fits.ImageHDU(np.arange(100).reshape((10, 10)))\n        section = hdu.section\n\n        assert_equal(section[0, :], np.arange(10))\n        assert_equal(section[1:3, :], np.arange(10, 30).reshape((2, 10)))\n        assert_equal(section[:, 2], np.arange(2, 100, 10))\n        assert_equal(section[2:5, 2:5], np.arange(22, 47).reshape((3, 3)))\n"], "sample_528": ["    def test_style_context_after_reset(self):\n        with style.context('classic', after_reset=True):\n            assert mpl.rcParams[PARAM] == 'viridis'\n        assert mpl.rcParams[PARAM] != VALUE\n", "def test_context_after_reset():\n    \"\"\"Test style context manager with after_reset=True.\"\"\"\n    fig, ax = plt.subplots()\n    with plt.rc_context():\n        # Set an initial param\n        plt.rcParams[PARAM] = 'viridis'\n        # Use context manager with after_reset=True\n        with style.context(DUMMY_SETTINGS, after_reset=True):\n            assert plt.rcParams[PARAM] == VALUE\n        # Check that the param is back to the initial value after the context\n        assert plt.rcParams[PARAM] == 'viridis'\n", "    def test_context_after_reset(self):\n        with temp_style('test_context') as tmpdir:\n            with style.context('test_context', after_reset=True):\n                assert mpl.rcParams[PARAM] == VALUE\n            assert mpl.rcParams[PARAM] != VALUE  # Assert reset to default\n\n\n", "def test_user_library_update():\n    with temp_style('test_style'):\n        # Verify that style is available after reload library.\n        assert 'test_style' in style.available\n        # Check that the style was loaded correcty\n        style.use('test_style')\n        assert mpl.rcParams[PARAM] == VALUE\n", "def test_style_context():\n    with temp_style('test'):\n        with style.context('test'):\n            assert mpl.rcParams[PARAM] == VALUE\n        assert mpl.rcParams[PARAM] != VALUE\n\n"], "sample_469": ["    def test_annotation_with_case(self):\n        # Test annotation with CASE expression\n        case_annotation = Case(\n            When(Q(book__rating__gt=4), then=Value('Great')),\n            When(Q(book__rating__gt=3), then=Value('Good')),\n            default=Value('Average'),\n            output_field=CharField(),\n        )\n        annotations = {'rating_category': case_annotation}\n        queryset = (\n            Book.objects.filter(rating__isnull=False)\n            .annotate(**annotations)\n            .values('rating_category')\n            .distinct()\n        )\n        self.assertQuerysetEqual(queryset, [\n            {'rating_category': 'Great'},\n            {'rating_category': 'Good'},\n            {'rating_category': 'Average'}\n        ], transform=lambda x: x)\n", "    def test_annotation_with_coalesce(self):\n        # Test coalesce annotation with a field that might be null\n        qs = Book.objects.annotate(\n            price_or_zero=Coalesce(F(\"price\"), Value(0))\n        ).order_by(\"price_or_zero\")\n        self.assertQuerysetEqual(\n            qs,\n            [\n                (\"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\", 75.00),\n                (\"Artificial Intelligence: A Modern Approach\", 82.80),\n                (\"Python Web Development with Django\", 29.69),\n                (\"Practical Django Projects\", 29.69),\n                (\"The Definitive Guide to Django: Web Development Done Right\", 30.00),\n                (\"Sams Teach Yourself Django in 24 Hours\", 23.09),\n            ],\n            lambda b: (b.name, b.price_or_zero),\n        )\n\n", "    def test_values_select_related_and_annotations(self):\n        qs = (\n            Book.objects.values(\"name\", \"publisher__name\")\n            .annotate(\n                average_rating=Avg(\"rating\"),\n                price_with_tax=F(\"price\") * Decimal(\"1.07\"),\n            )\n            .select_related(\"publisher\")\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                {\n                    \"name\": \"The Definitive Guide to Django: Web Development Done Right\",\n                    \"publisher__name\": \"Apress\",\n                    \"average_rating\": Decimal(\"4.50\"),\n                    \"price_with_tax\": Decimal(\"32.19\"),\n                },\n                {\n                    \"name\": \"Practical Django Projects\",\n                    \"publisher__name\": \"Apress\",\n                    \"average_rating\": Decimal(\"4.00\"),\n                    \"price_with_tax\": Decimal(\"31.75\"),\n                },\n                {\n                    \"name\": \"Sams Teach Yourself Django in 24 Hours\",\n                    \"publisher__name\": \"Sams\",\n                    \"average_rating\": Decimal(\"3.00\"),\n                    \"price_with_tax\": Decimal(\"24.63\"),\n                },\n                {\n                    \"name\": \"Python Web Development with Django\",\n                    \"publisher__name\": \"Prentice Hall\",\n                    \"average_rating\": Decimal(\"4.00\"),\n                    \"price_with_tax\": Decimal(\"31.75\"),\n                },\n                {\n                    \"name\": \"Artificial Intelligence: A Modern Approach\",\n                    \"publisher__name\": \"Prentice Hall\",\n                    \"average_rating\": Decimal(\"4.00\"),\n                    \"price_with_tax\": Decimal(\"88.07\"),\n                },\n                {\n                    \"name\": \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\",\n                    \"publisher__name\": \"Morgan Kaufmann\",\n                    \"average_rating\": Decimal(\"5.00\"),\n                    \"price_with_tax\": Decimal(\"80.25\"),\n                },\n            ],\n        )\n", "    def test_non_aggregate_annotation_ordering(self):\n        # Test that an annotation with a non-aggregate function can be used\n        # in ordering.\n        qs = Book.objects.annotate(upper_name=Upper(F('name'))).order_by(\n            'upper_name'\n        )\n        self.assertQuerysetEqual(\n            qs,\n            [\n                \"Artificial Intelligence: A Modern Approach\",\n                \"Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp\",\n                \"Practical Django Projects\",\n                \"Python Web Development with Django\",\n                \"Sams Teach Yourself Django in 24 Hours\",\n                \"The Definitive Guide to Django: Web Development Done Right\",\n            ],\n            lambda b: b.name,\n            ordered=True,\n        )\n\n", "    def test_complex_annotation(self):\n        q = Book.objects.annotate(\n            author_count=Count(\"authors\"),\n            average_rating=Avg(\"rating\"),\n            price_diff=F(\"price\") - Decimal(\"20\"),\n            pub_date_year=ExtractYear(\"pubdate\"),\n        ).filter(author_count__gt=1, average_rating__gt=4.0, price_diff__gt=0)\n\n        self.assertQuerysetEqual(\n            q,\n            [\n                \"Practical Django Projects\",\n                \"Python Web Development with Django\",\n            ],\n            transform=lambda b: b.name,\n        )\n\n"], "sample_812": ["    def test_estimator_repr_compact_dict_items(self):\n        # Test repr of Estimator with dict-like parameters and compact=True\n        estimator = LogisticRegressionCV(Cs=[0.1, 1, 10], cv=3)\n\n        pprint_output = _EstimatorPrettyPrinter(compact=True).pformat(estimator)\n\n        expected_output = (\n            \"LogisticRegressionCV(Cs=[0.1, 1, 10], cv=3)\"\n        )\n        assert pprint_output == expected_output\n\n", "    def test_estimator_deep_get_params(self):\n        set_config(print_changed_only=True)\n        lr = LogisticRegression(penalty='l1', C=0.5)\n        lr.fit(np.random.randn(10, 5), np.zeros(10))\n        expected = \"LogisticRegression(C=0.5, penalty='l1')\"\n        pp = _EstimatorPrettyPrinter()\n        result = pp.pformat(lr)\n        assert result == expected\n", "    def test_changed_only_with_pipeline(self):\n        set_config(print_changed_only=True)\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        pipe.fit(np.zeros((10, 10)), np.zeros(10))\n        pp = _EstimatorPrettyPrinter(compact=True)\n        repr(pipe, pp)\n\n", "    def test_pipeline_pprint(self):\n        # Issue #13613: Pipelines with estimators with many parameters\n        # shouldn't overflow the line width\n\n        pipe = make_pipeline(\n            StandardScaler(),\n            LogisticRegressionCV(\n                scoring='accuracy', fit_intercept=True, l1_ratios=[0.01, 0.1],\n                penalty='l1', solver='liblinear', random_state=42,\n                Cs=[0.1, 1, 10], cv=5, max_iter=1000\n            )\n        )\n        expected = (\n            \"Pipeline(steps=[('standardscaler', StandardScaler()), \"\n            \"('logisticregressioncv', LogisticRegressionCV(Cs=[0.1, 1, 10], \"\n            \"cv=5, fit_intercept=True, l1_ratios=[0.01, 0.1], max_iter=1000, \"\n            \"penalty='l1', random_state=42, scoring='accuracy', \"\n            \"solver='liblinear'))])\"\n        )\n\n        # Check compact=False\n        with set_config(print_changed_only=False):\n            self.assertEqual(pprint.pformat(pipe), expected)\n        \n\n", "def test_estimator_with_long_params():\n    # Test that long lists/tuples in estimators are handled correctly\n    # (truncated with ellipsis)\n    set_config(print_changed_only=True)\n    pipe = make_pipeline(\n        StandardScaler(),\n        LogisticRegression(C=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                           penalty=[\"l1\", \"l2\", \"elasticnet\"]),\n    )\n    pp = _EstimatorPrettyPrinter(compact=True, n_max_elements_to_show=3)\n    output = pp.pformat(pipe)\n    assert re.search(r\"penalty=['l1', 'l2', 'elasticnet', ...]\", output)\n    \n\n"], "sample_472": ["    def test_get_elided_page_range(self):\n        paginator = Paginator(range(100), 10)\n\n        self.assertEqual(list(paginator.get_elided_page_range(number=1)), list(range(1, 11)))\n        self.assertEqual(list(paginator.get_elided_page_range(number=10)), list(range(1, 11)))\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=5)), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        )\n\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=11)), [1, 2, 3, ..., 10, 11]\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=89)), [1, 2, ..., 88, 89, 90]\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=99)), [1, 2, ..., 98, 99, 100]\n        )\n", "    def test_get_elided_page_range(self):\n        paginator = Paginator(range(100), 10, orphans=0)\n        self.assertEqual(\n            list(paginator.get_elided_page_range(1)),\n            list(range(1, 11)),\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(5)),\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(10)),\n            list(range(1, 11)),\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(50)),\n            [1, 2, ..., 48, 49, 50],\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(51)),\n            [1, 2, ..., 50],\n        )\n", "    def test_get_elided_page_range(self):\n        paginator = Paginator([i for i in range(1, 51)], 10)\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=25)),\n            [1, 2, 3, ..., 23, 24, 25, 26, 27, ..., 48, 49, 50],\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=1)),\n            [1, 2, 3, 4, 5, ..., 48, 49, 50],\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(number=50)),\n            [1, 2, ..., 47, 48, 49, 50],\n        )\n\n", "    def test_get_elided_page_range(self):\n        \"\"\"\n        Test the get_elided_page_range method.\n        \"\"\"\n\n        paginator = Paginator(\n            range(1, 51), per_page=10, orphans=0, allow_empty_first_page=True\n        )\n\n        expected = [1, 2, ..., 40, 41, 42, 43, 44, 45, 46, ..., 49, 50]\n        self.assertEqual(list(paginator.get_elided_page_range(43)), expected)\n\n        expected = [1, 2, ..., 10, 11, 12, 13, ..., 19, 20]\n        self.assertEqual(list(paginator.get_elided_page_range(13)), expected)\n\n        expected = range(1, 6)\n        self.assertEqual(list(paginator.get_elided_page_range(5)), expected)\n\n        expected = [1, 2, 3, 4, 5, ..., 46, 47, 48, 49, 50]\n        self.assertEqual(list(paginator.get_elided_page_range(48)), expected)\n", "    def test_get_elided_page_range(self):\n        paginator = Paginator(range(100), 10)\n        self.assertEqual(\n            list(paginator.get_elided_page_range(1)),\n            list(range(1, 6)) + [paginator.ELLIPSIS] + list(range(96, 101)),\n        )\n        self.assertEqual(list(paginator.get_elided_page_range(5)), list(range(1, 6)) + [paginator.ELLIPSIS] + list(range(96, 101)))\n        self.assertEqual(\n            list(paginator.get_elided_page_range(10)),\n            list(range(1, 11)) + [paginator.ELLIPSIS] + list(range(96, 101)),\n        )\n        self.assertEqual(\n            list(paginator.get_elided_page_range(50)), list(range(1, 6)) + [paginator.ELLIPSIS] + list(range(46, 51)) + [paginator.ELLIPSIS] + list(range(96, 101)),\n        )\n"], "sample_142": ["    def test_multiple_inlines_same_model(self):\n        class MyInline(GenericStackedInline):\n            model = City\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = (MyInline, MyInline)\n\n        admin_site = AdminSite()\n        with self.assertRaises(checks.Error) as context:\n            admin_site.register(State, MyAdmin)\n        self.assertIn('You cannot have multiple inlines of the same model.', str(context.exception))\n", "    def test_modelformset_factory_with_invalid_fk_name(self):\n        with self.assertRaisesMessage(ValueError, \"'Album' has more than one ForeignKey to 'Author'. You must specify a 'fk_name' attribute.\"):\n            forms.modelformset_factory(Album, form=forms.ModelForm, fk_name='author')\n", "    def test_modelformset_raises_error_on_invalid_fk(self):\n        class InvalidFKFormset(BaseModelFormSet):\n            class Meta:\n                model = Song\n                fields = ['title']\n                fk_name = 'album'\n\n        with self.assertRaisesMessage(\n                ValueError,\n                'fk_name \\'album\\' is not a ForeignKey to \\'Album\\''\n        ):\n            InvalidFKFormset(data={'title': 'test'}, queryset=Song.objects.none())\n\n", "    def test_inlineformset_factory_with_fk_name(self):\n        class BookInlineForm(forms.ModelForm):\n            class Meta:\n                model = Book\n                fields = ['title']\n\n        class AuthorAdmin(admin.ModelAdmin):\n            inlines = [GenericStackedInline(model=Book, form=BookInlineForm, fk_name='city')]\n\n        model_admin = AuthorAdmin(model=Author, admin_site=AdminSite())\n\n        checks_output = model_admin.check(None)\n        self.assertEqual(checks_output, [])\n", "    def test_checks_for_invalid_inline_formset(self):\n        class BookInline(admin.TabularInline):\n            model = Book\n            fields = ['title']\n\n        class AuthorAdmin(admin.ModelAdmin):\n            inlines = [BookInline]\n\n        admin_site = AdminSite()\n        admin_site.register(Author, AuthorAdmin)\n        errors = admin_site.check()\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.W101')\n"], "sample_554": ["    def test_annotation_clipping(self):\n        fig, ax = plt.subplots()\n        ax.set_xlim(-1, 1)\n        ax.set_ylim(-1, 1)\n\n        ann1 = ax.annotate(\"Clip\", xy=(-0.5, -0.5), xycoords='data',\n                          annotation_clip=True)\n\n        ann2 = ax.annotate(\"No Clip\", xy=(-0.5, -0.5), xycoords='data',\n                          annotation_clip=False)\n\n        fig.canvas.draw()\n\n        # Check if the annotations are drawn as expected\n        assert ann1.get_visible() is False\n        assert ann2.get_visible() is True\n", "    def test_Annotation_arrowprops(self):\n        fig, ax = plt.subplots()\n        anno = Annotation(\"Test\", xy=(0.5, 0.5), xytext=(0.7, 0.8),\n                          arrowprops=dict(arrowstyle=\"->\"))\n        ax.add_artist(anno)\n        fig.canvas.draw()\n\n        # Check if the arrow is actually drawn\n        assert len(anno.arrow_patch.get_path().vertices) > 1\n", "    def test_annotation_offset_textcoords_default(self):\n        fig, ax = plt.subplots()\n        a = Annotation(\"test\", (0.5, 0.5), xycoords=\"figure fraction\",\n                       textcoords='offset points', ha='center', va='center')\n        ax.add_artist(a)\n        fig.canvas.draw()\n        # Check text position\n        assert_almost_equal(a.get_position(),\n                            (0.5, 0.5))\n\n", "def test_annotation_arrow(self):\n    fig, ax = plt.subplots()\n    xy = (0.5, 0.5)\n    xytext = (0.7, 0.8)\n    ann = ax.annotate(\"Annotation\", xy, xytext,\n                      arrowprops=dict(arrowstyle=\"->\",\n                                      connectionstyle=\"arc3,rad=0.2\"))\n    assert ann.arrow_patch is not None\n\n\n\n", "def test_annotation_arrowprops():\n    fig, ax = plt.subplots()\n    # Test simple arrowstyle\n    ann = ax.annotate(\"test1\", xy=(0.5, 0.5), xytext=(0.7, 0.7),\n                      arrowprops=dict(width=2, headwidth=6, headlength=8))\n    assert isinstance(ann.arrow_patch, mpatches.FancyArrowPatch)\n\n    # Test fancy arrowstyle\n    ann = ax.annotate(\"test2\", xy=(0.3, 0.3), xytext=(0.8, 0.8),\n                      arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0.2\"))\n    assert isinstance(ann.arrow_patch, mpatches.FancyArrowPatch)\n\n"], "sample_107": ["    def test_callable_setting_wrapper_repr(self):\n        class MySetting:\n                return 'MySetting()'\n        wrapped_setting = CallableSettingWrapper(MySetting())\n        self.assertEqual(repr(wrapped_setting), 'MySetting()')\n", "    def test_callable_setting_wrapper_repr(self):\n        class MockCallable:\n                return 'MockCallable'\n\n        wrapper = CallableSettingWrapper(MockCallable())\n        self.assertEqual(repr(wrapper), 'MockCallable')\n", "    def test_callable_setting_wrapper(self):\n        wrapped_callable = CallableSettingWrapper(lambda: 'value')\n        self.assertEqual(repr(wrapped_callable), \"<bound method <lambda> of <object object at 0x...>>\")\n", "    def test_callable_setting_wrapper(self):\n            return 'called'\n\n        wrapped_callable = CallableSettingWrapper(callme)\n\n        # Ensure callable is not called when wrapped.\n        self.assertEqual(repr(wrapped_callable), repr(callme))\n", "    def test_callable_setting_wrapper(self):\n        classCallableSettingsWrapper = CallableSettingWrapper(lambda: \"test value\")\n        self.assertEqual(str(classCallableSettingsWrapper), \"<function <lambda> at 0x...>\"),\n\n"], "sample_998": ["def test_latex_ComplexRegion():\n    R = ComplexRegion( x**2 + y**2 <= 1 )\n    assert latex(R) == r\"\\left\\{ (x, y) \\in \\mathbb{C}^{2} \\mid x^{2} + y^{2} \\leq 1 \\right\\}\"\n\n", "def test_latex_printing_of_ComplexRegion():\n    cr = ComplexRegion(\n        (abs(x) < 1) & (abs(y) < 2),\n        variables=(x, y)\n    )\n    assert latex(cr) == r'\\left\\{ \\left(x, y\\right) \\in \\mathbb{C}^{2} \\mid \\left| x \\right| < 1 \\wedge \\left| y \\right| < 2 \\right\\}'\n", "def test_latex_KroneckerDelta():\n    assert latex(KroneckerDelta(m, n)) == '\\\\delta_{m,n}'\n    assert latex(KroneckerDelta(x, y)) == '\\\\delta_{x,y}'\n", "def test_latex_SingularityFunction():\n    assert latex(SingularityFunction(x, x - 1)) == r\"\\operatorname{Sing}(x - 1)\"\n\n", "    def test_latex_ComplexRegion(self):\n        r = Symbol('r')\n        theta = Symbol('theta')\n        reg = ComplexRegion({x: (r*cos(theta), r*sin(theta)),\n                             r: (0, 1), theta: (0, 2*pi)})\n        self.assertEqual(latex(reg), r\"\\left\\{ x \\in \\mathbb{C} \\mid  x = r e^{i \\theta} \\wedge 0 \\le r \\le 1 \\wedge 0 \\le \\theta \\le 2 \\pi \\right\\}\")\n"], "sample_735": ["    def test_gaussian_mixture_precisions_init(self):\n        # Test that user-provided precisions are correctly initialized\n\n        for covariance_type in COVARIANCE_TYPE:\n            gm = GaussianMixture(n_components=self.n_components,\n                                covariance_type=covariance_type,\n                                n_init=1, random_state=0)\n            if covariance_type == 'full':\n                gm.precisions_init = np.array([\n                    make_spd_matrix(self.n_features, random_state=0)\n                    for _ in range(self.n_components)])\n            elif covariance_type == 'tied':\n                gm.precisions_init = make_spd_matrix(self.n_features,\n                                                    random_state=0)\n            elif covariance_type == 'diag':\n                gm.precisions_init = np.eye(self.n_features) + np.random.rand(\n                    self.n_components, self.n_features)\n            elif covariance_type == 'spherical':\n                gm.precisions_init = np.array([\n                    np.random.rand() for _ in range(self.n_components)])\n\n            gm.fit(self.X[covariance_type])\n\n            assert_array_almost_equal(gm.precisions_cholesky_,\n                                      _compute_precision_cholesky(\n                                          gm.covariances_,\n                                          covariance_type))\n", "    def test_precisions_cholesky(self):\n        n_samples = 500\n        n_features = 2\n        n_components = 2\n\n        rng = np.random.RandomState(0)\n        random_data = RandomData(rng, n_samples, n_components, n_features)\n\n        for covariance_type in COVARIANCE_TYPE:\n            gm = GaussianMixture(n_components=n_components,\n                                covariance_type=covariance_type,\n                                random_state=rng).fit(random_data.X[covariance_type])\n            # check if the precisions_cholesky_ are properly computed\n            if covariance_type == 'full':\n                assert_allclose(\n                    np.dot(gm.precisions_cholesky_, gm.precisions_cholesky_.transpose(0, 2, 1)),\n                    gm.precisions_)\n            elif covariance_type == 'tied':\n                assert_allclose(\n                    np.dot(gm.precisions_cholesky_, gm.precisions_cholesky_.T),\n                    gm.precisions_)\n            elif covariance_type in ['diag', 'spherical']:\n                assert_allclose(gm.precisions_cholesky_ ** 2, gm.precisions_)\n", "    def test_precisions_update_tied(self):\n        # Check if tied precisions are updated correctly\n        n_samples = 100\n        n_features = 2\n        n_components = 2\n        rng = np.random.RandomState(0)\n\n        X = self.X['tied']\n        gm = GaussianMixture(n_components=n_components,\n                            covariance_type='tied', random_state=rng)\n\n        gm.fit(X)\n        # Re-estimate precisions from the fitted model\n        gm._m_step(X, gm.predict_log_proba(X))\n        # Check if the updated precisions are close to the expected precision\n        assert_allclose(gm.precisions_cholesky_,\n                        linalg.cholesky(self.covariances['tied'],\n                                        lower=True),\n                        rtol=1e-2, atol=1e-2)\n", "    def test_gaussian_mixture_score_samples(self):\n        # Test the score_samples method \n        for covariance_type in COVARIANCE_TYPE:\n            X = self.X[covariance_type]\n            gmm = GaussianMixture(n_components=self.n_components,\n                                  covariance_type=covariance_type,\n                                  random_state=0).fit(X)\n            log_prob = gmm.score_samples(X)\n            assert_equal(log_prob.shape[0], X.shape[0])\n            assert_greater_equal(np.sum(np.exp(log_prob)), 0.99)\n", "    def test_gaussian_mixture_covariance_type_full_init(self):\n        n_samples = 100\n        n_features = 2\n        n_components = 2\n        rng = np.random.RandomState(42)\n        X = generate_data(n_samples, n_features, self.weights, self.means,\n                          self.covariances, 'full')\n\n        gm = GaussianMixture(n_components=n_components, covariance_type='full',\n                            n_init=1, init_params='kmeans',\n                            random_state=rng)\n\n        gm.fit(X)\n        assert_true(gm.converged_)\n        assert_array_almost_equal(gm.weights_, self.weights, decimal=2)\n        assert_array_almost_equal(gm.means_, self.means, decimal=2)\n        assert_array_almost_equal(gm.covariances_,\n                                  self.covariances['full'], decimal=2)\n\n"], "sample_928": ["    def test_textwidth(self):\n        self.assertEqual(textwidth('Hello'), 5)\n        self.assertEqual(textwidth('Hello', 'WFA'), 5)\n        self.assertEqual(textwidth('\u4f60\u597d'), 4)\n        self.assertEqual(textwidth('\u4f60\u597d', 'WF'), 6)\n\n", "    def test_textwidth(self):\n        assert textwidth('hello') == 5\n        assert textwidth('hello\\u3000world') == 9\n        assert textwidth('hello\u4e16\u754c', 'WFA') == 7\n", "    def test_textwidth(self):\n        text = 'Hello, world!'\n        self.assertEqual(textwidth(text), len(text))\n\n        text = 'Hello, \u4e16\u754c!'\n        self.assertEqual(textwidth(text, 'WF'), len(text) * 2)\n\n        text = 'Hello, \u4e16\u754c!'\n        self.assertEqual(textwidth(text, 'WFA'), len(text) * 2)\n", "    def test_textwidth(self):\n        assert textwidth('') == 0\n        assert textwidth('a') == 1\n        assert textwidth('AA') == 2\n        assert textwidth('\uff01') == 2\n        assert textwidth('ja', 'WFA') == 2\n        assert textwidth('ja', 'WF') == 2\n", "    def test_heading(self):\n        env = Environment('ja')\n        assert heading(env, '\u30d8\u30c3\u30c9') == '\u30d8\u30c3\u30c9\\n========='\n"], "sample_385": ["    def test_autocomplete_select_multiple_required(self):\n        form = RequiredBandForm(data={'band': ''})\n        self.assertFalse(form.is_valid())\n        self.assertIn('band', form.errors)\n        self.assertEqual(\n            form.errors['band'],\n            [\n                \"This field is required.\"\n            ],\n        )\n", "    def test_autocomplete_select_with_empty_choices(self):\n        form = NotRequiredBandForm()\n        self.assertHTMLEqual(\n            str(form[\"band\"]),\n            '<select name=\"band\" id=\"id_band\" class=\"my-class admin-autocomplete\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/autocomplete/\" data-allow-clear=\"true\" data-app-label=\"admin_widgets\" data-field-name=\"band\" data-model-name=\"band\" data-placeholder=\"\" data-theme=\"admin-autocomplete\" lang=\"en\"><option value=\"\"></option></select>',\n        )\n", "    def test_autocomplete_select_required(self):\n        form = RequiredBandForm()\n        self.assertHTMLEqual(\n            str(form[\"band\"]),\n            '<select data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/autocomplete/\" data-app-label=\"admin_widgets\" data-model-name=\"band\" data-field-name=\"band\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" lang=\"en\" class=\"my-class admin-autocomplete\"><option value=\"\"></option></select>',\n        )\n", "    def test_select2_autocomplete_with_required_form(self):\n        form = RequiredBandForm()\n        self.assertHTMLEqual(\n            form.as_p(),\n            '<p><label for=\"id_band\">Band:</label><select id=\"id_band\" name=\"band\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/admin_widgets/autocomplete/\" data-app-label=\"admin_widgets\" data-model-name=\"album\" data-field-name=\"band\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"\" lang=\"en\" class=\"admin-autocomplete my-class\"><option value=\"\"></option></select></p>',\n        )\n", "    def test_autocomplete_form_with_empty_values(self):\n        form = NotRequiredBandForm(data={'band': ''})\n        self.assertTrue(form.is_valid())\n        self.assertIsNone(form.cleaned_data.get('band'))\n\n"], "sample_658": ["    def test_doctestmodule_collects_doctests_in_conftest_py(self, testdir):\n        testdir.makefile(\".txt\", test_doctest=\"\"\"\n            >>> 1\n            1\n        \"\"\")\n        testdir.makefile(\"conftest.py\", \"\"\"\n                pass\n        \"\"\")\n        result = testdir.runpytest(\"--doctest-modules\")\n        assert result.ret == 0\n        assert len(result.items) == 1\n", "    def test_doctestmodule_finds_tests_in_conftest_py(self, tmpdir):\n        conftest_path = tmpdir.join(\"conftest.py\")\n        conftest_path.write(\n            textwrap.dedent(\n                \"\"\"\n                pass\n            \"\"\"\n            )\n        )\n        modcol = DoctestModule(conftest_path, None)\n        for item in modcol.collect():\n            assert isinstance(item, DoctestItem)\n            assert item.name == \"test_something\"\n", "    def test_doctest_module_mocked(self, tmpdir):\n        p = tmpdir.join(\"mymodule.py\")\n        p.write(\n            textwrap.dedent(\n                \"\"\"\n                return 42\n        \"\"\"\n            )\n        )\n\n        m_mock = Mock()\n        m_mock.myfunc.return_value = 24\n        with _patch_unwrap_mock_aware():\n            from unittest.mock import MagicMock\n            import _pytest.doctest\n\n            _pytest.doctest._is_mocked = MagicMock(return_value=True)\n\n            d = DoctestModule(p, None)\n            for item in d.collect():\n                assert isinstance(item, DoctestItem)\n                assert _is_mocked(inspect.unwrap(item.obj))\n\n\n\n", "    def test_doctest_module_collect_mocked_object(self, tmpdir):\n        tmpdir.join(\"example.py\").write(\n            textwrap.dedent(\n                \"\"\"\n                    pass\n                \"\"\"\n            )\n        )\n        # Create a mock object\n        class Mock:\n            pass\n\n        with _patch_unwrap_mock_aware():\n            mod = tmpdir.join(\"example.py\").pyimport()\n            mod.my_mocked_func = Mock()\n            mod.__doctest_mock_attribute__ = True\n            doctest_item = DoctestModule(tmpdir.join(\"example.py\"), pytest.config)\n            for item in doctest_item.collect():\n                assert isinstance(item, DoctestItem)\n                assert not _is_mocked(item.obj)\n", "    def test_doctest_item_repr_failure_multiple_failures(self, tmpdir):\n        # create a doctest file with two failures\n        tmpdir.join(\"test_doctest.txt\").write_text(\n            textwrap.dedent(\n                \"\"\"\n            >>> 1 + 1\n            3\n            >>> 2 + 2\n            5\n            \"\"\"\n            )\n        )\n\n        config = pytest.Config(\n            [])\n        doctest_item = DoctestItem.from_parent(\n            parent=None, name=\"test_doctest.txt\", config=config\n        )\n        doctest_item.dtest = doctest_item._create_doctest(\n            filename=str(tmpdir.join(\"test_doctest.txt\"))\n        )\n        doctest_item.runner = _get_runner()\n\n        # mock the doctest failures\n        failures = [\n            doctest.DocTestFailure(\n                test=doctest_item.dtest,\n                example=doctest_item.dtest.examples[0],\n                got=\"3\",\n            ),\n            doctest.DocTestFailure(\n                test=doctest_item.dtest,\n                example=doctest_item.dtest.examples[1],\n                got=\"5\",\n            ),\n        ]\n        doctest_item._runner._report_failure = lambda *args: None\n\n        with pytest.raises(MultipleDoctestFailures) as excinfo:\n            doctest_item.runtest()\n\n        repr_failure = doctest_item.repr_failure(excinfo)\n\n        assert isinstance(repr_failure, ReprFailDoctest)\n        assert len(repr_failure.reprlocation_lines) == 2\n"], "sample_641": ["compilation error", "compilation error", "compilation error", "compilation error", "def test_get_pdata_path(path: str, recur: int, pylint_home: Path, expected: Path):\n    assert _get_pdata_path(Path(path), recur, pylint_home) == expected\n"], "sample_899": ["    def test_sparse_transformer_check_is_fitted(self):\n        sparse_transformer = SparseTransformer()\n        msg = \"This SparseTransformer instance is not fitted yet.\"\n        assert_raises_regex(NotFittedError, msg, sparse_transformer.transform,\n                            np.array([[1, 2], [3, 4]]))\n        sparse_transformer.fit(np.array([[1, 2], [3, 4]]))\n        sparse_transformer.transform(np.array([[1, 2], [3, 4]]))\n", "    def test_check_sparse_input(self):\n        # Test that sparse input is handled correctly by estimators\n\n        sparse_matrix = sp.csr_matrix([[1, 2], [3, 4]])\n        dense_matrix = np.array([[1, 2], [3, 4]])\n        estimators = [\n            LinearRegression(),\n            SVC(),\n            KNeighborsRegressor(),\n            GaussianMixture(),\n            MiniBatchKMeans(),\n            NMF(),\n            MultiTaskElasticNet(),\n            AdaBoostClassifier(),\n            RandomForestClassifier()\n        ]\n\n        for estimator in estimators:\n            # Test that estimators can handle sparse input in fit\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                estimator.fit(sparse_matrix, [1, 2])  \n\n            # Test that estimators can handle dense input in fit\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                estimator.fit(dense_matrix, [1, 2])\n", "    def test_sparse_transformer_with_large_sparse_support(self):\n        # Check sparse transformer with large sparse support\n        X = sp.csr_matrix(np.random.rand(100000, 10))\n        transformer = SparseTransformer()\n        transformer.fit(X)\n        transformed_data = transformer.transform(X)\n        assert isinstance(transformed_data, sp.csr_matrix)\n        assert transformed_data.shape == X.shape\n\n", "    def test_check_estimator_sparse_input_large_sparse_not_supported(self):\n        set_checking_parameters( LARGE_SPARSE_SUPPORTED=False )\n        estimator = LargeSparseNotSupportedClassifier()\n        msg = (\"Estimator doesn't support 64-bit indices\")\n\n        X_large_csr = sp.csr_matrix(np.random.randn(1000, 1000), dtype=np.float64)\n        X_large_csr.indices = X_large_csr.indices.astype(np.int64)\n        X_large_csc = sp.csc_matrix(np.random.randn(1000, 1000), dtype=np.float64)\n        X_large_csc.indices = X_large_csc.indices.astype(np.int64)\n\n        y = np.random.randn(1000)\n\n        # test CSR format with 64 bit indices\n        assert_raises_regex(ValueError, msg, estimator.fit, X_large_csr, y)\n\n        # test CSC format with 64 bit indices\n        assert_raises_regex(ValueError, msg, estimator.fit, X_large_csc, y)\n\n\n\n", "    def test_check_estimator_sparse_input_error(self):\n        # Test check_estimator with estimators that raise ValueError\n        # when fitted with sparse input\n        for Estimator in [NoSparseClassifier]:\n            estimator = Estimator()\n            with self.assertRaises(ValueError):\n                check_estimator(estimator)\n"], "sample_451": ["    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\\n\")\n        self.assertEqual(\n            body,\n            \"Display an individual :model:`myapp.MyModel`.\\n\\n\"\n            \"**Context**\\n\\n\"\n            \"``RequestContext``\\n\\n\"\n            \"``mymodel``\\n\"\n            \"        An instance of :model:`myapp.MyModel`.\\n\\n\"\n            \"**Template:**\\n\\n\"\n            \":template:`myapp/my_template.html`\\n\",\n        )\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(body, \"some_metadata: some data\")\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"Display an individual :model:`myapp.MyModel`.\\n\\n**Context**\\n\\n``RequestContext``\\n\\n``mymodel``\\n        An instance of :model:`myapp.MyModel`.\\n\\n**Template:**\\n\\n:template:`myapp/my_template.html`\"\n        )\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"\"\"Display an individual :model:`myapp.MyModel`.\n", "    def test_parse_docstring(self):\n        title, body, metadata = parse_docstring(self.docstring)\n        self.assertEqual(title, \"Display an individual :model:`myapp.MyModel`.\")\n        self.assertEqual(\n            body,\n            \"Display an individual :model:`myapp.MyModel`.\\n\\n\"\n            \"**Context**\\n\\n\"\n            \"``RequestContext``\\n\\n\"\n            \"``mymodel``\\n\"\n            \"        An instance of :model:`myapp.MyModel`.\\n\\n\"\n            \"**Template:**\\n\\n\"\n            \":template:`myapp/my_template.html` \",\n        )\n        self.assertEqual(metadata, {\"some_metadata\": \"some data\"})\n"], "sample_86": ["    def test_lazy_unpickling(self):\n        class MyClass:\n            pass\n        obj = lazy(MyClass)(1, 2)\n        pickled_obj = pickle.dumps(obj)\n        unpickled_obj = pickle.loads(pickled_obj)\n        self.assertIsInstance(unpickled_obj, MyClass)\n        self.assertEqual(unpickled_obj._wrapped, (1, 2))\n", "    def test_lazy_eval(self):\n        called = False\n            nonlocal called\n            called = True\n            return 'hello'\n        lazy_val = lazy(my_func, str)\n        self.assertFalse(called)\n        self.assertEqual(lazy_val, 'hello')\n        self.assertTrue(called)\n", "    def test_lazy_with_cached_property(self):\n        class Model:\n            @cached_property\n                return lazy(lambda: 42)()\n\n        self.assertEqual(Model().value, 42)\n", "    def test_lazy_str(self):\n        instance = mock.MagicMock()\n        instance.__class__.__name__ = 'Something'\n\n            return str(instance)\n\n        lazy_str = lazystr(get_str)\n        self.assertIsInstance(lazy_str(), str)\n        self.assertEqual(lazy_str(), str(instance))\n", "    def test_lazy_str(self):\n            return 'Hello, world!'\n        lazy_str = lazystr(get_text)\n        with mock.patch('django.utils.functional.get_text') as mock_get_text:\n            self.assertEqual(str(lazy_str), 'Hello, world!')\n            mock_get_text.assert_called_once()\n"], "sample_973": ["    def test_stringify_signature_unqualified_typehints(self):\n        class MyClass:\n            pass\n            pass\n\n        sig = inspect.signature(myfunc)\n        assert stringify_signature(sig, unqualified_typehints=True) == (\n            '(arg1: MyClass, arg2: list[int], *args: str, **kwargs: float) -> MyClass'\n        )\n", "    def test_stringify_signature_unqualified_typehints(self):\n                **kwargs: dict) -> 'module.AnotherClass':\n            pass\n\n        sig = inspect.signature(func)\n        unqualified_sig = inspect.stringify_signature(sig, unqualified_typehints=True)\n        # Check if unqualified type hints are used\n        assert 'module.Class' not in unqualified_sig\n        assert 'module.AnotherClass' not in unqualified_sig\n", "    def test_signature_from_ast_with_default_factory_function():\n        code = \"\"\"\n            pass\n        \"\"\"\n        node = ast.parse(code).body[0]\n\n        sig = inspect.signature_from_ast(node, code)\n        assert sig.parameters[\n            'b'\n        ].default == inspect.DefaultValue('<lambda()>')\n\n", "    def test_getdoc_inherited_decorated_method(self):\n        class Base:\n            @property\n                \"\"\"Base class property doc.\"\"\"\n                return 1\n\n        class Derived(Base):\n            @property\n                return super().prop * 2\n        d = Derived()\n        assert inspect.getdoc(d.prop) == \"Base class property doc.\"\n", "    def test_stringify_signature_unqualified_typehints(self):\n        sig = inspect.signature(self.func_with_annotations)\n        result = inspect.stringify_signature(sig, unqualified_typehints=True)\n        expected = '(arg1: int, arg2: StringIO, arg3: typing.List[str]) -> str'\n        assert result == expected\n\n"], "sample_13": ["def test_angle_wrap_at():\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n\n    a.wrap_at(360 * u.deg)\n    assert_allclose(a.degree, [340., 150., 350.])\n\n    a.wrap_at('180d', inplace=True)\n    assert_allclose(a.degree, [-20., 150., -10.])\n\n    # Test wrapping with a quantity with unit\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    a.wrap_at(Angle(180, unit=u.deg))\n    assert_allclose(a.degree, [-20., 150., -170.])\n\n    # Test wrapping with a single value\n    a = Angle([-20.0, 150.0, 350.0] * u.deg)\n    a.wrap_at(180 * u.deg)\n    assert_allclose(a.degree, [-20., 150., -170.])\n", "    def test_Longitude_wrap_angle_init():\n        lon = Longitude(180 * u.deg, wrap_angle=180 * u.deg)\n        assert lon.wrap_angle == 180 * u.deg\n        assert lon.value == 180\n        lon = Longitude(270 * u.deg, wrap_angle=180 * u.deg)\n        assert lon.wrap_angle == 180 * u.deg\n        assert_allclose(lon.value, -90)\n\n", "    def test_longitude_wrap():\n        lon = Longitude(180 * u.deg)\n        assert lon.wrap_angle == 360 * u.deg\n        lon = Longitude(180 * u.deg, wrap_angle=180 * u.deg)\n        assert lon.wrap_angle == 180 * u.deg\n        assert lon.value == 180.\n\n        lon = Longitude([-190, 270], u.deg)\n        assert_allclose(lon.value, [-170, 90])\n\n        lon.wrap_angle = 180 * u.deg\n        assert lon.wrap_angle == 180 * u.deg\n        assert_allclose(lon.value, [170, -90])\n\n        lon = Longitude([181, -179], u.deg, wrap_angle=180 * u.deg)\n        assert_allclose(lon.value, [-179, 1])\n", "    def test_longitude_wrap_angle_in_place(self):\n        lon = Longitude([190, -10], unit=u.deg)\n        lon.wrap_angle = Angle(180, unit=u.deg)\n        lon.wrap_at(lon.wrap_angle, inplace=True)\n        assert_array_equal(lon.degree, [-170, 170])\n", "def test_angle_from_hmsdms_scalar():\n    \"\"\"Test Angle from h,m,s and d,m,s for scalar inputs.\"\"\"\n    a = Angle(1, 2, 3, unit='hourangle')\n    assert a.value == 1.03416667\n\n    a = Angle(10, 11, 12, unit='hourangle')\n    assert a.value == 10.18666667\n\n    a = Angle(1, 2, 3, unit='degree')\n    assert_allclose(a.value, 1.03416667 * u.degree.to(u.radian))\n\n    a = Angle(10, 11, 12, unit='degree')\n    assert_allclose(a.value, 10.18666667 * u.degree.to(u.radian))\n\n"], "sample_551": ["def test_poly3dcollection_shade(self):\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    verts = [\n        [[-1, -1, 0], [1, -1, 0], [1, 1, 0], [-1, 1, 0]],\n        [[-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]],\n    ]\n\n    pc = art3d.Poly3DCollection(verts,\n                               facecolors=['b', 'r'],\n                               shade=True)\n\n    ax.add_collection(pc)\n    ax.set_xlim([-2, 2])\n    ax.set_ylim([-2, 2])\n    ax.set_zlim([-1, 2])\n", "    def test_shade_colors(self):\n        normals = np.array([[0, 1, 0], [0, 0, 1], [1, 0, 0]])\n        colors = ['red', 'green', 'blue']\n        shaded_colors = art3d._shade_colors(colors, normals)\n        assert len(shaded_colors) == 3\n        assert not same_color(shaded_colors[0], shaded_colors[1])\n        assert not same_color(shaded_colors[1], shaded_colors[2])\n\n", "    def test_shade_colors_with_array_colors(self):\n        normals = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1],\n                             [-1, 0, 0], [0, -1, 0], [0, 0, -1]])\n        colors = np.zeros((6, 4))\n        colors[:, :3] = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1],\n                                   [1, 1, 0], [1, 0, 1], [0, 1, 1]])\n\n        shaded_colors = art3d._shade_colors(colors, normals)\n\n        assert len(shaded_colors) == 6\n        assert shaded_colors.shape == (6, 4)\n\n\n", "    def test_poly3dcollection(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        verts = [\n            [(0, 0, 0), (1, 0, 0), (1, 1, 0), (0, 1, 0)],\n        ]\n        poly = art3d.Poly3DCollection(verts)\n        ax.add_collection(poly)\n        ax.set_xlim([0, 2])\n        ax.set_ylim([0, 2])\n        ax.set_zlim([0, 1])\n        # This checks if the plot is generated without error\n", "    def test_poly3dcollection_shade_with_lightsource(self):\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        verts = [[[0, 0, 0], [1, 0, 0], [1, 1, 0], [0, 1, 0]],\n                 [[0, 0, 1], [1, 0, 1], [1, 1, 1], [0, 1, 1]]]\n        pc = art3d.Poly3DCollection(verts, shade=True,\n                                   lightsource=mcolors.LightSource(azdeg=30))\n        ax.add_collection3d(pc)\n\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        ax.set_zlim([0, 1])\n        ax.view_init(elev=30, azim=45)\n\n        # Test that all polygons have different shades\n        shades = []\n        for face in pc.get_facecolors():\n            shades.append(face[:3].tolist())\n        self.assertTrue(len(set(tuple(shade) for shade in shades)) == 2)\n"], "sample_708": ["    def test_getstatement_with_empty_lines_in_between():\n        source = Source(\n            \"\"\"\n                pass\n\n            a = 1\n            \n            \n                pass\n            \"\"\"\n        )\n        statement = source.getstatement(2)\n        assert statement == Source(\n            '\\n            a = 1\\n            '\n        )\n", "    def test_getstatement_multiline():\n        source = Source(\n            \"\"\"\n                a = 1\n                b = 2\n                c = 3\n            \"\"\"\n        )\n        stmt = source.getstatement(1)\n        assert stmt.lines == [\"def func():\"]\n        stmt = source.getstatement(2)\n        assert stmt.lines == [\"    a = 1\"]\n\n\n\n", "    def test_getstatement_multi_statement_empty_line():\n        code = Source(\n            \"\"\"", "    def test_getstatementrange_multi_line_statement_indent():\n        code = \"\"\"\n        hello = (\n            1 +\n            2)\n        \"\"\"\n        source = Source(code)\n        astnode = ast.parse(code, \"source\", \"exec\")\n        start, end = getstatementrange_ast(0, source, astnode=astnode)\n        assert start == 0\n        assert end == 3\n", "    def test_getstatementrange_ast_multi_statement_line():\n        source = Source(\n            \"\"\"\n                x = 1\n                y = 2\n                if x:\n                    pass\n            \"\"\"\n        )\n        astnode = ast.parse(source_code, \"source\", \"exec\")\n        start, end = getstatementrange_ast(1, source, astnode=astnode)\n        assert start == 0\n        assert end == 0\n        start, end = getstatementrange_ast(2, source, astnode=astnode)\n        assert start == 1\n        assert end == 2\n        start, end = getstatementrange_ast(3, source, astnode=astnode)\n        assert start == 2\n        assert end == 3\n\n"], "sample_968": ["def test_parse_annotation_with_complex_types():\n    assert _parse_annotation(\"List[Tuple[str, int]]\") == (\n        'List',\n        '[Tuple[str, int]]',\n    )\n    assert _parse_annotation(\"Dict[str, List[int]]\") == (\n        'Dict',\n        '[str, List[int]]',\n    )\n", "    def test_parse_annotation(app: Sphinx, env: BuildEnvironment):\n        result = _parse_annotation(\"value: int\", env)\n        assert result == nodes.Text(\"int\")\n\n        result = _parse_annotation(\"value: List[str]\", env)\n        assert result == nodes.Text(\"List[str]\")\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == \"int\"\n    assert _parse_annotation(\"List[str]\") == \"List[str]\"\n    assert _parse_annotation(\"Dict[str, int]\") == \"Dict[str, int]\"\n    assert _parse_annotation(\"Callable[[int, str], bool]\") == \"Callable[[int, str], bool]\"\n    assert _parse_annotation(\"Union[int, str]\") == \"Union[int, str]\"\n    assert _parse_annotation(\"Optional[int]\") == \"Optional[int]\"\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", None)\n    assert _parse_annotation(\"List[int]\") == (\"List\", \"int\")\n    assert _parse_annotation(\"Dict[str, List[int]]\") == (\"Dict\", \"str, List[int]\")\n\n    # Test complex annotations with nested types:\n    assert _parse_annotation(\"Tuple[int, List[str], Dict[float, bool]]\") == (\n        \"Tuple\", \"int, List[str], Dict[float, bool]\"\n    )\n\n    # Test annotations with type aliases (using typing module):\n    assert _parse_annotation(\"Optional[str]\") == (\"Optional\", \"str\")\n\n    # Test annotations with generics and type arguments:\n    assert _parse_annotation(\"List[T]\") == (\"List\", \"T\")\n    assert _parse_annotation(\"Dict[K, V]\") == (\"Dict\", \"K, V\")\n\n    # Test annotations with ellipsis:\n    assert _parse_annotation(\"List[...]\") == (\"List\", \"...\")\n\n    # Test annotations with Any type:\n    assert _parse_annotation(\"Any\") == (\"Any\", None)\n", "def test_parse_signature():\n    sig = 'my_function(arg1, arg2=None, *args, **kwargs)'\n    result = parse(sig)\n    expected = 'my_function(arg1, arg2=None, *args, **kwargs)'\n    assert result == expected\n"], "sample_343": ["    def test_generic_relation_with_inherited_model(self):\n        class InheritedPost(Post):\n            pass\n\n        class InheritedAnswer(Answer):\n            pass\n\n        # Create an InheritedPost instance.\n        inherited_post = InheritedPost.objects.create(title='Inherited Post')\n\n        # Create an InheritedAnswer related to the InheritedPost.\n        InheritedAnswer.objects.create(content='Answer for Inherited Post', post=inherited_post)\n\n        # Verify that the GenericRelation on the InheritedPost works as expected.\n        self.assertEqual(inherited_post.answers.count(), 1)\n", "    def test_genericforeignkey_related_manager_clear(self):\n        post = Post.objects.create(title='My Post')\n        question1 = Question.objects.create(content='What is the meaning of life?', post=post)\n        Answer.objects.create(question=question1, content='42')\n        question2 = Question.objects.create(content='What is the air-speed velocity of an unladen swallow?', post=post)\n        Answer.objects.create(question=question2, content='African or European?')\n\n        self.assertEqual(Answer.objects.filter(question__post=post).count(), 2)\n\n        post.answers.clear()\n\n        self.assertEqual(Answer.objects.filter(question__post=post).count(), 0)\n", "    def test_generic_relation_get_queryset(self):\n        # Create a Question and a Post\n        q = Question.objects.create(question_text=\"What is Django?\")\n        p = Post.objects.create(title=\"My blog post\")\n\n        # Create two Answers related to the Question\n        Answer.objects.create(question=q, text=\"It's a web framework.\")\n        Answer.objects.create(question=q, text=\"It's awesome!\")\n\n        # Check that the Question's answers are correctly returned\n        self.assertQuerysetEqual(q.answers.all(), [Answer.objects.get(pk=1), Answer.objects.get(pk=2)])\n\n        # Check that the Post has no Answers\n        self.assertEqual(p.answers.count(), 0)\n", "    def test_generic_relation_get_queryset_filtering(self):\n        post = Post.objects.create(title=\"Test post\")\n        question1 = Question.objects.create(post=post, text=\"What is Django?\")\n        question2 = Question.objects.create(post=post, text=\"How to use ORM?\")\n\n        answer1 = Answer.objects.create(question=question1, text=\"Django is a web framework\")\n        answer2 = Answer.objects.create(question=question2, text=\"Use models, QuerySet, and Manager\")\n\n        self.assertQuerysetEqual(\n            post.questions.all(),\n            [\n                question1,\n                question2,\n            ],\n\n        )\n", "    def test_generic_relation_prefetching(self):\n        post = Post.objects.create()\n        question = Question.objects.create(related_to=post)\n        answer = Answer.objects.create(question=question)\n\n        with self.assertNumQueries(2):\n            # Prefetch the answers related to the post through the Question.\n            posts = Post.objects.prefetch_related('question__answer_set').all()\n\n        self.assertEqual(len(posts), 1)\n"], "sample_1170": ["    def test_printmethod_string(self):\n        class MyPrintable(Expr):\n                self.value = value\n\n                return str(self.value)\n\n        a = MyPrintable('Hello')\n        assert sstr(a) == 'Hello'\n        assert sstr(a, printmethod='_sympystr') == 'Hello'\n", "def test_print_NDimArray():\n    A = NDimArray([1, 2, 3], (3,))\n    assert sstr(A) == 'NDimArray([1, 2, 3], shape=(3,))'\n", "    def test_print_NDimArray(self):\n        a = NDimArray([1, 2, 3])\n        b = NDimArray([[1, 2], [3, 4]])\n        assert sstr(a) == \"NDimArray([1, 2, 3])\"\n        assert sstr(b) == \"NDimArray([[1, 2], [3, 4]])\"\n", "def test_tr():\n    # Test Tr printing\n    A = MatrixSymbol('A', 3, 3)\n    assert sstr(Tr(A)) == 'Tr(A)'\n", "def test_print_FiniteSet():\n    assert sstr(FiniteSet(x, y, z)) == '{x, y, z}'\n    assert sstr(FiniteSet(1, 2, 3)) == '{1, 2, 3}'\n    assert sstr(FiniteSet(x, FiniteSet(y, z))) == '{x, {y, z}}'\n"], "sample_635": ["    def test_param_no_docstring_with_args(self):\n        code = \"\"\"\n                pass\n        \"\"\"\n        tree = astroid.parse(code)\n        function = next(tree.walk(lambda node: isinstance(node, nodes.FunctionDef) if node.name == 'foo' else False))\n        with self.assertAddsMessages(\n            MessageTest(\n                msg_id=\"missing-param-doc\",\n                node=function.args.args[0],\n                args=(\"a\",),\n            ),\n            MessageTest(\n                msg_id=\"missing-param-doc\",\n                node=function.args.args[1],\n                args=(\"b\",),\n            ),\n        ):\n            self.checker.visit_functiondef(function)\n\n", "    def test_missing_param_doc(self,):\n        self.check_function(\n            \"\"\"\n                pass\n            \"\"\"\n        )\n", "    def test_missing_param_doc_multi_line(self):\n        code = \"\"\"\n            \"\"\"This function does something.\n\n            :param a:\n            :param b: This parameter is described\n\n            \"\"\"\n            pass\n        \"\"\"\n        tree = astroid.parse(code)\n        self.checker.check_docs(tree.body[0])\n        self.assertAddsMessage(\n            MessageTest(\n                msg_id=\"missing-param-doc\",\n                node=tree.body[0].args.args[0],\n                args=(\"a\",),\n            )\n        )\n", "    def test_param_doc_with_optional_parameter(self):\n        node = astroid.extract_node(\n            \"\"\"", "    def test_param_doc_no_docstring(self):\n        self.checker.analyze(nodes.FunctionDef(\"__init__\", [], [], []))\n        self.assertContainsMessage(\n            self.messages,\n            \"missing-param-doc\",\n            message=\"Missing docstring for parameter 'self'\",\n            lineno=1,\n            col=1,\n        )\n\n"], "sample_653": ["    def test_log_capture_handler_records_are_cleared_on_reset(caplog):\n        caplog.set_level(logging.DEBUG)  # ensure we capture log records\n        caplog.handler.setLevel(logging.DEBUG)\n        caplog.handler.emit({'message': 'First message'})\n        assert len(caplog.records) == 1\n\n        caplog.handler.reset()\n        assert len(caplog.records) == 0\n", "def test_log_capture(caplog, tmpdir):\n    log_file = tmpdir / \"test.log\"\n    caplog._plugin.set_log_path(str(log_file))\n\n    assert log_file.exists()\n\n    with caplog.at_level(logging.INFO, logger='my_logger'):\n        logging.getLogger('my_logger').info(\"This is a test message\")\n\n    assert caplog.text == ''\n    assert log_file.read().strip() == 'INFO:my_logger:This is a test message'\n", "def test_log_capture_with_setup_teardown(testdir, caplog):\n    testdir.makepyfile(\n        \"\"\"\n        import logging\n\n            log = logging.getLogger(__name__)\n            log.info(\"setup\")\n            yield\n            log.info(\"teardown\")\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"--capture=no\")\n    assert result.ret == 0\n    log_records = caplog.get_records(\"setup\")\n    assert log_records[0].message == \"setup\"\n    log_records = caplog.get_records(\"teardown\")\n    assert log_records[0].message == \"teardown\"\n\n", "def test_log_level_is_set_correctly():\n    result = _get_actual_log_level(\"ERROR\")\n    assert result == logging.ERROR\n", "    def test_capturing_logs_during_setup_and_teardown(self, caplog, testdir):\n        # Arrange\n        testdir.makepyfile(\n            \"\"\"\n        import logging\n        import time\n\n            logging.info('setup log')\n            time.sleep(0.1)\n            logging.info('call log')\n            time.sleep(0.1)\n            logging.info('teardown log')\n\n            yield test_method\n\n        \"\"\"\n        )\n\n        # Act & Assert\n        result = testdir.runpytest()\n        all_logs = caplog.records\n        setup_logs = caplog.get_records(\"setup\")\n        teardown_logs = caplog.get_records(\"teardown\")\n\n        assert result.ret == 0\n        assert len(all_logs) == 3\n\n        assert len(setup_logs) == 1\n        assert setup_logs[0].message == 'setup log'\n\n        assert len(teardown_logs) == 1\n        assert teardown_logs[0].message == 'teardown log'\n"], "sample_144": ["    def test_deferred_fields_with_inheritance(self):\n        \"\"\"\n        Test that deferred fields work correctly with model inheritance.\n        \"\"\"\n        person = Person.objects.create(name=\"Alice\")\n        self.assertEqual(person.name, \"Alice\")\n        with self.assertRaises(AttributeError):\n            person.age\n", "    def test_get_or_create_from_subclass(self):\n        \"\"\"\n        Test that get_or_create works correctly when called on a subclass\n        and the object exists in the database as an instance of the parent class.\n        \"\"\"\n        bachelor_party = BachelorParty.objects.create(\n            name=\"John's Bachelor Party\",\n            date=datetime.date(2024, 3, 15)\n        )\n\n        party, created = Party.objects.get_or_create(name=\"John's Bachelor Party\")\n        self.assertEqual(party, bachelor_party)\n        self.assertFalse(created)\n", "    def test_m2m_through_table_inheritance(self):\n        # Verify that fields defined on the 'through' model are correctly\n        # inherited and accessible on instances related through the\n\n        party = BirthdayParty.objects.create(name=\"Birthday Bash\")\n        guest1 = Person.objects.create(name=\"Alice\")\n        guest2 = Person.objects.create(name=\"Bob\")\n\n        party.guests.add(guest1, guest2)\n\n        # Access fields from the 'through' model\n        attended_party = party.guests.get(pk=guest1.id).attended_party\n        self.assertEqual(attended_party.name, \"Birthday Bash\")\n\n        # Verify that we can update the through model fields\n        attended_party.gift = \"Birthday Cake\"\n        attended_party.save()\n\n        self.assertEqual(party.guests.get(pk=guest1.id).attended_party.gift, \"Birthday Cake\")\n\n", "    def test_derived_m2m_relationship(self):\n        \"\"\"\n        Test a foreign key relationship to a model that inherits from an\n        abstract base class.\n        \"\"\"\n        # Create a Supplier and a M2MChild\n        supplier = Supplier.objects.create(name='Supplier A')\n        m2m_child = M2MChild.objects.create(name='Child A')\n\n        # Add the supplier to the child's related managers\n        m2m_child.suppliers.add(supplier)\n\n        # Retrieve the supplier through the m2m_child\n        retrieved_supplier = m2m_child.suppliers.first()\n\n        # Assert that the retrieved supplier is the same as the created one\n        self.assertEqual(retrieved_supplier, supplier)\n", "    def test_auto_created_name_truncation(self):\n        class TruncatedModel(models.Model):\n            name = models.CharField(max_length=10)\n\n        # Assuming a database with a maximum column name length of 10 characters\n        with self.assertRaises(django.core.exceptions.FieldDoesNotExist):\n            TruncatedModel.objects.create(name='very_long_name_that_will_be_truncated')\n\n"], "sample_358": ["    def test_table_rename(self):\n        self.reference.rename_table_references('table', 'new_table')\n        self.assertEqual(str(self.reference), 'NEW_TABLE')\n", "    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('TABLE'))\n        self.assertFalse(self.reference.references_table('another_table'))\n", "    def test_table_references_table(self):\n        self.assertTrue(self.reference.references_table('TABLE'))\n        self.assertFalse(self.reference.references_table('other_table'))\n", "    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('TABLE'))\n        self.assertFalse(self.reference.references_table('other_table'))\n\n", "    def test_references_table(self):\n        self.assertTrue(self.reference.references_table('table'))\n        self.assertFalse(self.reference.references_table('other_table'))\n"], "sample_625": ["    def _test_apply_ufunc_dim_sizes(self):\n        da = xr.DataArray(\n            np.arange(24).reshape((2, 3, 4)),\n            dims=[\"x\", \"y\", \"z\"],\n            coords={\"x\": [0, 1], \"y\": [2, 3, 4]},\n        )\n        # Test that dim sizes are unified correctly\n        result1 = apply_ufunc(\n            lambda x: x,\n            da,\n            input_core_dims=[[\"y\", \"z\"]],\n            output_core_dims=[[\"y\", \"z\"], [\"x\"]],\n            dask=\"forbidden\",\n        )\n        assert unified_dim_sizes(result1, \"y\") == [3, 4]\n        assert unified_dim_sizes(result1, \"x\") == [2]\n\n", "def test_unify_chunks(obj1, obj2, expected_chunks):\n    obj1, obj2 = unify_chunks(obj1, obj2)\n    assert obj1.chunks == expected_chunks\n\n", "    def test_polyval(self):\n        # Test simple polynomials\n\n        coords = xr.DataArray(np.arange(5), dims=\"x\")\n        coeffs = xr.DataArray([1, 2, 3], dims=\"degree\")\n        expected = 1 + 2 * coords + 3 * coords**2\n        actual = xr.polyval(coords, coeffs)\n        assert_identical(actual, expected)\n\n        # Test with a Dataset of coefficients\n        coeffs = xr.Dataset(\n            {\"a\": (\"degree\", [1, 2, 3]), \"b\": (\"degree\", [4, 5, 6])}\n        )\n        actual = xr.polyval(coords, coeffs)\n        expected = xr.Dataset(\n            {\"a\": 1 + 2 * coords + 3 * coords**2, \"b\": 4 + 5 * coords + 6 * coords**2}\n        )\n        assert_identical(actual, expected)\n", "    def test_where_keep_attrs(keep_attrs):\n        x = xr.DataArray(\n            [1, 2, 3],\n            dims=[\"a\"],\n            attrs={\"foo\": \"bar\"},\n        )\n        y = xr.DataArray([4, 5, 6], dims=[\"a\"])\n        cond = xr.DataArray([True, False, True], dims=[\"a\"])\n        expected = xr.DataArray([1, 5, 3], dims=[\"a\"], attrs={\"foo\": \"bar\"})\n        actual = xr.where(cond, x, y, keep_attrs=keep_attrs)\n        assert_identical(actual, expected)\n        assert actual.attrs == (expected.attrs if keep_attrs else {})\n", "def test_join_dict_keys(dim):\n    a = {dim: [1, 2, 3]}\n    b = {dim: [2, 3, 4]}\n    expected = {dim: [1, 2, 3, 4]}\n    assert join_dict_keys(a, b, dim) == expected\n"], "sample_914": ["compilation error", "compilation error", "compilation error", "compilation error", "compilation error"], "sample_2": ["def test_ccddata_fits_uncertainty_saving_and_loading_with_different_units():\n    \"\"\"Test saving and loading uncertainty with different units.\"\"\"\n    data = _random_array.copy()\n    ccd = CCDData(data, unit=u.adu, uncertainty=StdDevUncertainty(data * 0.1,\n                                                                    unit=u.electron))\n    with temp_file_factory(\"test.fits\") as tmp_file:\n        ccd.write(tmp_file)\n\n        ccd_loaded = CCDData.read(tmp_file)\n\n    assert ccd_loaded.uncertainty is not None\n    assert ccd_loaded.uncertainty.unit == u.electron\n    assert np.allclose(ccd_loaded.uncertainty.array, data * 0.1)\n\n", "    def test_ccd_data_fits_roundtrip_with_units_in_header():\n        \"\"\"\n        Test that CCDData can be written to and read from a FITS file,\n        and that the units are preserved correctly, even if they are\n        specified\n        in the header.\n        \"\"\"\n\n        ccd = create_ccd_data()\n        ccd.header['BUNIT'] = 'electron'\n\n        with TempFile() as tmp:\n            ccd.write(tmp)\n\n            with fits.open(tmp) as hdus:\n                ccd_read = CCDData.read(tmp)\n\n        assert ccd_read.unit == u.electron\n        assert np.allclose(ccd.data, ccd_read.data)\n        assert ccd.header == ccd_read.header\n", "    def test_ccddata_write_and_read_with_uncertainty(tmpdir):\n        ccd = create_ccd_data()\n        ccd.uncertainty = StdDevUncertainty(_random_array * 0.1)\n\n        filename = str(tmpdir.join(\"test.fits\"))\n        ccd.write(filename)\n\n        ccd_read = CCDData.read(filename)\n\n        assert np.allclose(ccd.data, ccd_read.data)\n        assert np.allclose(ccd.uncertainty.array,\n                           ccd_read.uncertainty.array)\n", "    def test_fits_reader_bad_unit():\n        with pytest.raises(ValueError) as err:\n            CCDData.read('data.fits', unit='m/s')\n\n        assert \"The Header value for the key BUNIT (ADU) cannot be interpreted as valid unit\" in str(err.value)\n\n", "    def test_read_fits_with_wcs_and_mask(self):\n        hdu = fits.PrimaryHDU()\n        hdu.data = _random_array.copy()\n        hdu.header['BUNIT'] = 'adu'\n        hdu.header['EXTNAME'] = 'DATA'\n\n        # Create a mask HDU\n        mask_hdu = fits.ImageHDU(np.zeros_like(_random_array, dtype=bool), name='MASK')\n        mask_hdu.header['EXTNAME'] = 'MASK'\n\n        # Create a WCS object\n        wcs = WCS(naxis=2)\n        wcs.wcs.crval = [100, 200]\n        wcs.wcs.cdelt = [0.1, 0.1]\n        wcs.wcs.naxis = [DEFAULT_DATA_SIZE, DEFAULT_DATA_SIZE]\n        hdu.header.update(wcs.to_header())\n\n        hdulist = fits.HDUList([hdu, mask_hdu])\n        with fits.TempFile() as temp_file:\n            hdulist.writeto(temp_file.name)\n            ccd = CCDData.read(temp_file.name, hdu='DATA', hdu_mask='MASK')\n\n        assert isinstance(ccd.wcs, WCS)\n        assert np.all(ccd.mask == np.zeros_like(_random_array, dtype=bool))\n\n"], "sample_126": ["    def test_many_to_many_through_renamed_table(self):\n        before = [\n            self.book_with_multiple_authors,\n        ]\n        after = [\n            self.book_with_multiple_authors_through_attribution,\n        ]\n        changes = self.get_changes(before, after)\n\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\n            'RenameModel',\n            'AlterField',\n        ])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name='attribution_book_authors')\n\n", "    def test_circular_fk_migration(self):\n        before = [\n            self.knight,\n            self.rabbit,\n        ]\n        after = [\n            self.knight,\n            self.rabbit,\n        ]\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, 'eggs', 0)\n", "    def test_remove_unique_together(self):\n        # Test removing a unique_together constraint\n        before = [\n            self.book_foo_together,\n        ]\n        after = [\n            self.book_author_pk,\n        ]\n        changes = self.get_changes(before, after)\n\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RemoveConstraint\"])\n\n\n\n", "    def test_rename_with_m2m_through(self):\n        before_state = self.make_project_state([\n            self.author_with_m2m_through,\n        ])\n        after_state = self.make_project_state([\n            self.author_with_renamed_m2m_through,\n        ])\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameModel', 'RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='Author', app_label='testapp')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='publishers', app_label='testapp')\n", "    def test_many_to_many_through_rename(self):\n        before_state = self.make_project_state([\n            self.book,\n            self.author_with_publisher,\n            self.publisher,\n        ])\n        after_state = self.make_project_state([\n            self.book_with_renamed_m2m_through,\n            self.author_with_publisher,\n            self.publisher,\n            self.deal,\n        ])\n        changes = self.get_changes(before_state.models, after_state.models)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField', 'AlterUniqueTogether'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='deal')\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='book_publishers', unique_together=['book', 'publisher'])\n"], "sample_565": ["    def test_mark_inset_with_zoom(self):\n        fig, ax1 = plt.subplots()\n        ax2 = inset_axes(ax1, \"30%\", \"40%\", loc=1)\n        mark_inset(ax1, ax2, loc1=1, loc2=3)\n        fig.set_size_inches(4, 4)\n        fig\n\n\n", "def test_mark_inset():\n    fig, ax = plt.subplots()\n\n    # Create inset axes\n    axins = inset_axes(ax, width='40%', height='30%', loc='lower right')\n\n    # Plot in the inset axes\n    axins.plot(np.arange(10), np.arange(10))\n\n    # Mark the inset axes\n    mark_inset(ax, axins, loc1=1, loc2=3,\n               facecolor=\"none\", edgecolor=\"red\", linestyle=\"--\", alpha=0.7)\n\n    ax.set_ylim(0, 10)\n    ax.set_xlim(0, 10)\n\n", "def test_InsetPosition():\n    fig, ax = plt.subplots()\n    parent_lbwh = [0.1, 0.1, 0.8, 0.8]\n    ip = InsetPosition(ax, parent_lbwh)\n    bbox = ip(ax, None)\n\n    assert_array_equal(bbox.get_points(),\n                       np.array([[0.1, 0.1], [0.9, 0.1],\n                                 [0.9, 0.9], [0.1, 0.9]]))\n\n    parent_lbwh = [0.2, 0.2, 0.6, 0.6]\n    ip = InsetPosition(ax, parent_lbwh)\n    bbox = ip(ax, None)\n    assert_array_equal(bbox.get_points(),\n                       np.array([[0.2, 0.2], [0.8, 0.2],\n                                 [0.8, 0.8], [0.2, 0.8]]))\n", "def test_inset_locator_connect():\n    fig, ax = plt.subplots()\n    axins = inset_axes(ax, width=\"30%\", height=\"40%\", loc='upper right')\n\n    ax.plot(np.arange(10), np.arange(10))\n\n    axins.plot(np.arange(10), np.arange(10) ** 2)\n\n    mark_inset(ax, axins, loc1=1, loc2=3,\n               linestyle='--', alpha=0.7, facecolor='none')\n    plt.draw()\n", "    def test_inset_axes_locator(self):\n        fig, ax1 = plt.subplots()\n        ax2 = inset_axes(ax1, 1.5, \"50%\", loc=1,\n                         bbox_to_anchor=(0.1, 0.1, 0.8, 0.8))\n        ax2.plot([1, 2, 3], [1, 2, 3])\n        ax1.plot([1, 2, 3], [1, 2, 3])\n"], "sample_1148": ["    def test_matrix_symbol_derivative():\n        x = Symbol('x')\n        A = MatrixSymbol('A', n, m)\n        assert diff(A, x) == ZeroMatrix(n, m)\n\n", "def test_matrixelement_doit():\n    assert MatrixElement(A, 1, 1).doit() == A[1, 1]\n    assert MatrixElement(A, 1, 1).doit(deep=False) == MatrixElement(A, 1, 1)\n\n", "    def test_MatMul_identity():\n\n        assert MatMul(A, Identity(m)) == A\n        assert MatMul(Identity(n), A) == A\n        assert MatMul(A, ZeroMatrix(m, l)) == ZeroMatrix(n, l)\n        assert MatMul(ZeroMatrix(m, n), A) == ZeroMatrix(m, l)\n\n", "    def test_matrix_symbol_derivatives():\n        x = Symbol('x')\n        A = MatrixSymbol('A', 2, 2)\n        assert diff(A * x, x) == A\n        assert diff(x * A, x) == A\n        assert diff(A.T * x, x) == A.T\n        assert diff(x * A.T, x) == A.T\n\n", "    def test_transpose_noncommutative():\n        A = MatrixSymbol(\"A\", 2, 2, commutative=False)\n        assert (A*A).transpose() == A.transpose()*A.transpose()\n        assert (A.transpose()*A).transpose() == A*A.transpose()\n\n"], "sample_1099": ["    def test_partial_derivative_expand():\n        expr = PartialDerivative(A(i)*B(j), A(k))\n        expanded_expr = expr.doit()\n        assert expanded_expr == PartialDerivative(A(i)*B(j), A(k))\n        assert expanded_expr.get_indices() == [i, j, -k]\n", "def test_partial_derivative_expand_multi_var():\n    A = TensorHead(\"A\", [L])\n    expr = PartialDerivative(A(i), A(i), A(j))\n    expr = expr.expand()\n    assert expr.doit() == PartialDerivative(A(i), A(i)) * A(j) + PartialDerivative(A(i), A(j)) * A(i)\n\n\n\n", "    def test_partial_derivative_multiple_variables():\n        expr = PartialDerivative(A(i), A(j), B(k))\n        assert expr.variables == [A(j), B(k)]\n        assert expr.get_indices() == [i, -j, -k]\n        assert expr.doit() == A(i)._replace_indices({j: -j, k: -k})._eval_partial_derivative(A(j))._eval_partial_derivative(B(k))\n", "    def test_contract_indices_for_derivative_with_repeated_indices():\n        expr = PartialDerivative(A(i, j), A(i, k))\n        assert expr.get_indices() == [i, -i, j, -k]\n", "    def test_contract_indices_for_derivative():\n        a = PartialDerivative(A(i), A(j))\n        a = a._contract_indices_for_derivative(a.expr, a.variables)\n        assert a[1] == [i, -j]\n"], "sample_85": ["    def test_m2m_deletion_cascade(self):\n        m1 = M.objects.create(name='m1')\n        m2 = M.objects.create(name='m2')\n        t1 = T.objects.create(name='t1')\n        t2 = T.objects.create(name='t2')\n\n        m1.t.add(t1)\n        m1.t.add(t2)\n        m2.t.add(t1)\n\n        m1.delete()\n\n        self.assertEqual(T.objects.count(), 1)\n        self.assertEqual(T.objects.get(name='t2').m_set.count(), 0)\n\n\n", "    def test_m2m_cascade_delete(self):\n        # Create some objects\n        a1 = create_a()\n        a2 = create_a()\n        r1 = R.objects.create(name='r1', m=self.DEFAULT)\n        r2 = R.objects.create(name='r2', m=self.DEFAULT)\n        r3 = R.objects.create(name='r3', m=self.DEFAULT)\n\n        # Link them up\n        a1.r.add(r1, r2)\n        a2.r.add(r3)\n\n        # Check initial counts\n        self.assertEqual(A.objects.count(), 2)\n        self.assertEqual(R.objects.count(), 3)\n\n        # Delete a1\n        a1.delete()\n\n        # Check counts\n        self.assertEqual(A.objects.count(), 1)\n        self.assertEqual(R.objects.count(), 2)\n        self.assertNotIn(r1, R.objects.all())\n        self.assertNotIn(r2, R.objects.all())\n", "    def test_m2m_cascade_delete(self):\n        a = create_a()\n        r = R.objects.create(name='r')\n        a.rs.add(r)\n        self.assertEqual(a.rs.count(), 1)\n        self.assertEqual(r.as_.count(), 1)\n\n        a.delete()\n        self.assertEqual(R.objects.count(), 0)\n\n", "    def test_delete_with_on_delete_set_null(self):\n        # Test setting on_delete to 'SET_NULL' on a ManyToManyField.\n        r = R.objects.create(name=\"r\")\n        c1 = Child.objects.create(name=\"c1\", parent=r)\n        c2 = Child.objects.create(name=\"c2\", parent=r)\n        r.children.add(c1, c2)\n\n        self.assertEqual(c1.parent, r)\n        self.assertEqual(c2.parent, r)\n\n        r.delete()\n        c1.refresh_from_db()\n        c2.refresh_from_db()\n\n        self.assertIsNone(c1.parent)\n        self.assertIsNone(c2.parent)\n\n", "    def test_m2m_cascade_delete(self):\n        a1 = create_a(name='a1')\n        a2 = create_a(name='a2')\n\n        m1 = M(name='m1')\n        m1.save()\n        m2 = M(name='m2')\n        m2.save()\n\n        m1.a.add(a1, a2)\n        m2.a.add(a1)\n\n        # Deleting m1 should cascade to the m2m relationship\n        m1.delete()\n        self.assertCountEqual(a1.m.all(), [m2])\n        self.assertCountEqual(a2.m.all(), [])\n        self.assertEqual(M.objects.count(), 1)\n\n"], "sample_818": ["    def test_spectral_clustering_small_input(self):\n        # Test spectral clustering on a small dataset with precomputed affinity\n        X = np.array([[1, 0], [0, 1], [1, 1], [0, 0]])\n        affinity_matrix = np.array([[0, 1, 1, 0],\n                                     [1, 0, 1, 0],\n                                     [1, 1, 0, 1],\n                                     [0, 0, 1, 0]])\n        labels_true = np.array([0, 1, 0, 1])\n\n        clustering = SpectralClustering(n_clusters=2,\n                                        affinity='precomputed',\n                                        eigen_solver='arpack',\n                                        assign_labels='kmeans')\n        clustering.fit(affinity_matrix)\n\n        assert_array_equal(clustering.labels_, labels_true)\n", "def test_spectral_clustering_sparse_input(random_state=42):\n    random_state = check_random_state(random_state)\n    # Generate sparse data\n    X, y = make_blobs(n_samples=50, centers=3, random_state=random_state)\n    X_sparse = sparse.csr_matrix(X)\n\n    # Fit Spectral Clustering with sparse input\n    sc = SpectralClustering(n_clusters=3,\n                            affinity='rbf',\n                            random_state=random_state)\n    sc.fit(X_sparse)\n\n    # Check Labels\n    assert_equal(len(np.unique(sc.labels_)), 3)\n\n", "def test_eigen_solver_amg(random_state=42):\n    #AMG requires pyamg\n    if not amg_loaded:\n      return\n    # Generate random data\n    X, y = make_blobs(n_samples=50, centers=3, random_state=random_state)\n    # set up spectral clustering with amg solver\n    sc = SpectralClustering(n_clusters=3, eigen_solver='amg',\n                            random_state=random_state)\n    sc.fit(X)\n    # check that the labels are assigned\n    assert sc.labels_.shape == (50,)\n\n", "def test_spectral_clustering_precomputed_kernel():\n    # Test that spectral_clustering works with a precomputed kernel matrix.\n    random_state = check_random_state(0)\n    n_samples = 30\n    n_features = 2\n    X = random_state.rand(n_samples, n_features)\n    kernel = rbf_kernel(X, gamma=1)\n\n    labels = spectral_clustering(\n        kernel, n_clusters=2, eigen_solver='arpack', random_state=random_state,\n        assign_labels='discretize',\n    )\n    assert len(np.unique(labels)) == 2\n\n\n", "    def test_sparse_input_spectral_clustering(self):\n        random_state = check_random_state(0)\n        n_samples = 50\n        n_features = 2\n        X = sparse.rand(n_samples, n_features, density=0.3,\n                       random_state=random_state)\n        # make sure there are some connected components\n        X[0, :] = 1\n        X[10, :] = 1\n\n        labels_true = np.zeros(n_samples)\n        labels_true[:20] = 1\n        spectral = SpectralClustering(n_clusters=2,\n                                      assign_labels='discretize',\n                                      random_state=random_state)\n        spectral.fit(X)\n        assert_equal(spectral.n_clusters, 2)\n        assert_equal(spectral.labels_.shape[0], n_samples)\n        # Since the input is sparse, the algorithm might not perfectly\n        # recover the ground truth. We check only the score.\n        score = adjusted_rand_score(labels_true, spectral.labels_)\n        assert score > 0.9\n"], "sample_864": ["    def test_bandwidth_estimation(self):\n        # Test if bandwidth estimation works as expected\n        bandwidth = estimate_bandwidth(X, quantile=0.3)\n        assert bandwidth > 0\n\n", "    def test_bandwidth_estimation_empty_data(self):\n        with pytest.raises(ValueError):\n            estimate_bandwidth(np.array([]), quantile=0.3) \n", "    def test_bandwidth_estimation(self):\n\n        # Test if estimate_bandwidth raises an error for empty input\n        X = np.array([])\n        with pytest.raises(ValueError):\n            estimate_bandwidth(X)\n\n        # Test if estimate_bandwidth raises a warning if quantile is out of bounds\n\n        with warnings.catch_warnings(record=True) as w:\n            # Cause all warnings to always be triggered.\n            warnings.simplefilter(\"always\")\n            estimate_bandwidth(X, quantile=-0.1)\n            assert len(w) == 1\n            assert \"quantile should be between [0, 1]\" in str(w[0].message)\n\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            estimate_bandwidth(X, quantile=1.1)\n            assert len(w) == 1\n            assert \"quantile should be between [0, 1]\" in str(w[0].message)\n\n        \n\n", "    def test_bandwidth_estimation():\n        # Test that bandwidth estimation works.\n        bandwidth = estimate_bandwidth(X, quantile=0.3, random_state=0)\n        assert bandwidth > 0\n", "    def test_bandwidth_estimation_sparse(self):\n        X_sparse = sparse.csc_matrix(X)\n        bandwidth = estimate_bandwidth(X_sparse, quantile=0.3,\n                                     random_state=11)\n        assert_almost_equal(bandwidth, estimate_bandwidth(X, quantile=0.3,\n                                                         random_state=11))\n\n"], "sample_1111": ["    def test_textplot_str_constant():\n        x = Symbol('x')\n        expr = S(2)\n        result = list(textplot_str(expr, 0, 10))\n        expected = [\n            \" 2 |_______________________________________________________\",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \"  |                                                       \",\n            \" -2 |_______________________________________________________\",\n            \"    0                                                    10\",\n        ]\n        assert result == expected\n", "def test_textplot_str_constant():\n    expr = S(1)\n    a = 0\n    b = 5\n    W = 20\n    H = 10\n    lines = list(textplot_str(expr, a, b, W, H))\n    expected_lines = [\n        \" 1 |_____________________________________\",\n        \"    0                      5\",\n    ]\n    assert lines == expected_lines\n", "    def test_log():\n        x = Symbol('x')\n        lines = list(textplot_str(log(x + 1), 0.1, 5, W=30, H=10))\n        assert lines[0] == \"   5.0 |.....................................\"\n        assert lines[-1] == \"   0.1 |___________________________________\"\n\n", "    def test_textplot_str_logarithmic():\n        x = Symbol('x')\n        expr = log(x)\n        lines = list(textplot_str(expr, 1, 10))\n        assert len(lines) == 22\n        assert lines[10] == \"  5 |----...........\"\n", "    def test_textplot_str_with_constant():\n        expr = S(2)\n        a = 0\n        b = 10\n        expected = [\n            \" 10 |_______________________________________________________\",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    |                                                        \",\n            \"    0 |_______________________________________________________\",\n            \"        0                                                   10\",\n        ]\n        result = list(textplot_str(expr, a, b))\n        assert result == expected\n"], "sample_403": ["    def test_rename_model(self):\n        app_label = \"migrations_tests\"\n        model_name = \"UnicodeModel\"\n        new_model_name = \"RenamedUnicodeModel\"\n\n        with self.assertRaises(FieldDoesNotExist):\n            UnicodeModel.objects.create(name='Pizza')\n        \n        # State changes\n        before_state = self.get_state()\n        state_op = RenameModel(\n            model_name, new_model_name\n        )\n\n        after_state = state_op.apply_to_state(before_state)\n        self.assertEqual(\n            after_state.models[app_label, new_model_name.lower()]._meta.model_name,\n            new_model_name,\n        )\n        self.assertNotIn(\n            app_label,\n            after_state.models,\n        )\n\n        # Database changes\n        with atomic(savepoint=False), connection.cursor() as cursor:\n            with CaptureQueriesContext(cursor) as queries:\n                state_op.database_forwards(\n                    app_label,\n                    connection.alias,\n                    before_state,\n                    after_state,\n                )\n\n            # Check that the CREATE TABLE statement is run\n            self.assertIn(\n                f\"CREATE TABLE \\\"{new_model_name.lower()}\\\"\",\n                str(queries),\n            )\n            \n\n        with atomic(savepoint=False), connection.cursor() as cursor:\n            with CaptureQueriesContext(cursor) as queries:\n                state_op.database_backwards(\n                    app_label,\n                    connection.alias,\n                    before_state,\n                    after_state,\n                )\n\n            self.assertIn(\n                f\"ALTER TABLE \\\"{new_model_name.lower()}\\\" RENAME TO \\\"{model_name.lower()}\\\"\",\n                str(queries),\n            )\n", "    def test_rename_index(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE app_test_food (id INT PRIMARY KEY, name VARCHAR(100), \"\n                \"price DECIMAL NOT NULL DEFAULT '0.00')\"\n            )\n            cursor.execute(\"CREATE INDEX app_test_food_price_idx ON app_test_food (price)\")\n            cursor.execute(\"INSERT INTO app_test_food (name, price) VALUES ('Pizza', '10.00')\")\n            cursor.execute(\"INSERT INTO app_test_food (name, price) VALUES ('Salad', '5.00')\")\n\n        old_index_name = \"app_test_food_price_idx\"\n        new_index_name = \"price_idx\"\n\n        operation = RenameIndex('Food', new_name=new_index_name, old_name=old_index_name)\n        self.assertEqual(operation.describe(), f\"Rename index {old_index_name} on Food to {new_index_name}\")\n\n        with atomic(using=\"test_db\"):\n            # State forwards\n            state = ProjectState()\n            state.add_model(\"app_test\", \"Food\", [\n                (\"name\", models.CharField(max_length=100)),\n                (\"price\", models.DecimalField(default='0.00', max_digits=10, decimal_places=2)),\n        ])\n            state.add_index(\"app_test\", \"Food\", models.Index(fields=['price'], name=old_index_name))\n            operation.state_forwards(\n                \"app_test\",\n                state\n            )\n            self.assertEqual(state.models[\"app_test\", \"Food\"].indexes, [\n                models.Index(fields=['price'], name=new_index_name),\n            ])\n\n            # Database forwards\n            with self.schema_editor() as schema_editor:\n                operation.database_forwards(\n                    \"app_test\",\n                    schema_editor,\n                    state,\n                    state\n                )\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT * FROM sqlite_master WHERE type='index'\")\n                rows = cursor", "    def test_alter_field_type_with_default(self):\n        with atomic(using=self.database):\n            self.create_table(\n                'test_alter_field_type',\n                [('id', models.AutoField(primary_key=True)), ('name', models.CharField(max_length=100))],\n            )\n\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO test_alter_field_type (name) VALUES ('test')\")\n\n        state = ProjectState.from_apps(self.apps)\n        migration = Migration(\"test\")\n        before = state.models['test_app', 'test_alter_field_type']\n        operation = AlterField(\n            model_name='test_alter_field_type',\n            name='name',\n            field=models.IntegerField(default=0),\n        )\n        migration.operations.append(operation)\n\n        # Forward migration\n        self.apply_migration(migration)\n\n        after = state.models['test_app', 'test_alter_field_type']\n        self.assertEqual(after.fields['name'].db_type(connection), 'INTEGER')\n        self.assertEqual(after.fields['name'].default, 0)\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM test_alter_field_type\")\n            row = cursor.fetchone()\n            self.assertEqual(row[0], 0)\n\n        # Backward migration\n        self.unapply_migration(migration)\n\n        after = state.models['test_app', 'test_alter_field_type']\n        self.assertEqual(after.fields['name'].db_type(connection), 'VARCHAR')\n        self.assertEqual(after.fields['name'].max_length, 100)\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM test_alter_field_type\")\n            row = cursor.fetchone()\n            self.assertEqual(row[0], 'test')\n\n", "    def test_rename_index_unnamed(self):\n        model = UnicodeModel\n        with connection.cursor() as cursor:\n            cursor.execute(\n                f\"CREATE INDEX {self.table_name}_name_idx ON {self.table_name} (name)\"\n            )\n        state = self.get_state(\n            models=(model, ),\n        )\n        migration = Migration(\n            \"test_rename_index_unnamed\",\n            (\n                RenameIndex(model.__name__, new_name=\"new_name\", old_fields=['name']),\n            ),\n            dependencies=[],\n        )\n        schema_editor = migrations.SchemaEditor(connection)\n        with atomic(using=connection.alias), schema_editor.atomic_migration():\n            migration.apply(state)\n        self.assertTableExists(model._meta.db_table)\n        self.assertHasIndex(model, fields=['name'], name=\"new_name\")\n", "    def test_rename_model_with_foreign_key(self):\n        with atomic(using=self.db_alias), transaction.atomic():\n            class Author(models.Model):\n                name = models.CharField(max_length=100)\n\n            class Book(models.Model):\n                title = models.CharField(max_length=100)\n                author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n            state = ProjectState.from_apps(self.apps)\n            author_state = state.models['myapp', 'author']\n            book_state = state.models['myapp', 'book']\n\n            # Rename Author to Writer\n            operation = RenameModel('Author', 'Writer')\n            operation.state_forwards(self.db_alias, state)\n            self.assertEqual(state.models['myapp', 'author'].name, 'writer')\n            self.assertEqual(state.models['myapp', 'book'].fields['author'].remote_field.model_name, 'writer')\n\n            # Forward migration:\n            with connection.cursor() as cursor:\n                old_table_name = book_state.db_table\n                old_fk_name = book_state.fields['author'].db_column\n                cursor.execute(\"SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = %s AND table_name = %s\",\n                               [self.database, old_table_name])\n                initial_count = cursor.fetchone()[0]\n                with self.assertRaises(IntegrityError):\n                    cursor.execute(\"ALTER TABLE %s RENAME TO %s\" % (old_table_name, state.models['myapp', 'book'].db_table))\n                cursor.execute(\"SELECT COUNT(*) FROM information_schema.columns WHERE table_schema = %s AND table_name = %s AND column_name = %s\",\n                               [self.database, old_table_name, old_fk_name])\n                initial_fk_count = cursor.fetchone()[0]\n\n            # Backward migration:\n\n            operation.state_backwards(self.db_alias, state)\n\n            # Check that the state has been reverted\n            self.assertEqual(state.models['myapp', 'author'].name, 'author')\n            "], "sample_484": ["    def test_right(self):\n        john = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(Value(john.name), 5).as_sql(connection), (\"RIGHT('John Smith', 5)\", ()))\n        self.assertEqual(Right(john.name, 5), \"Smith\")\n", "    def test_right(self):\n        author = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(author.name, 5).evaluate(), \"Smith\")\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT RIGHT(name, 5) FROM authors WHERE name = 'John Smith'\")\n            self.assertEqual(cursor.fetchone()[0], \"Smith\")\n\n", "    def test_right(self):\n        john = Author.objects.get(name=\"John Smith\")\n        self.assertEqual(Right(john.alias, 3)(), \"ith\")\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT RIGHT('John Smith', 3) FROM dual\"\n            )\n            self.assertEqual(cursor.fetchone()[0], \"ith\")\n\n", "    def test_right(self):\n        john = Author.objects.get(pk=1)\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT RIGHT(name, 5) FROM my_auth_author WHERE name = %s\", [john.name])\n            result = cursor.fetchone()[0]\n        self.assertEqual(result, \"Smith\")\n", "    def test_right(self):\n        john = Author.objects.get(name=\"John Smith\")\n        rhonda = Author.objects.get(name=\"Rhonda\")\n        self.assertEqual(Right(Value(john.name), 5).resolve(connection, None), \"Smith\")\n        self.assertEqual(Right(Value(rhonda.name), 3).resolve(connection, None), \"nda\")\n"], "sample_669": ["    def test_capture_manager(self, monkeypatch, tmpdir):\n        capman = capture.CaptureManager(\"fd\")\n        assert capman._method == \"fd\"\n        assert not capman.is_globally_capturing()\n\n        monkeypatch.setattr(sys, \"stdout\", io.StringIO())\n        monkeypatch.setattr(sys, \"stderr\", io.StringIO())\n        with capman.item_capture(\"call\", tmpdir.join(\"test\")) as fixture:\n            assert fixture.out is not None\n            assert fixture.err is not None\n            assert capman.is_globally_capturing()\n            print(\"hello\", file=sys.stdout)\n            print(\"world\", file=sys.stderr)\n        out, err = capman.read_global_capture()\n        assert out == \"hello\\n\"\n        assert err == \"world\\n\"\n        assert not capman.is_globally_capturing()\n", "    def test_start_stop_global_capture(self, monkeypatch):\n        capman = CaptureManager(\"fd\")\n        with monkeypatch.context() as m:\n            m.setattr(sys, \"stdout\", io.StringIO())\n            capman.start_global_capturing()\n            assert capman.is_globally_capturing()\n            capman.stop_global_capturing()\n            assert not capman.is_globally_capturing()\n\n", "    def test_activate_fixture_then_suspend(self, monkeypatch):\n        cm = CaptureManager(\"fd\")\n        with monkeypatch.context() as m:\n            m.setattr(cm, \"_capture_fixture\", capture.CaptureFixture(capture.FDCapture, None))\n            cm._capture_fixture._start = lambda: None\n            cm.resume_global_capture()\n            assert cm._capture_fixture._capture is not None\n            cm.suspend_global_capture()\n            assert cm._capture_fixture._capture is not None\n", "    def test_capture_fd_tee(self, tmpdir):\n        p = subprocess.Popen(\n            [sys.executable, \"-c\", \"import time; print('stdout'); time.sleep(.1); print('stderr', file=sys.stderr)\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            cwd=str(tmpdir),\n        )\n        cm = capture.CaptureManager(\"tee-sys\")\n        with cm._capturing_for_request(None) as fixture:\n            out, err = p.communicate()\n        assert fixture._capture._state == \"done\"\n        assert cm._global_capturing is None\n        assert fixture._capture.out.getvalue() == \"stdout\\n\"\n        assert fixture._capture.err.getvalue() == \"stderr\\n\"\n", "    def test_tee_capture(self, tmpdir):\n        with TeeStdCapture() as cap:\n            print(\"hello\")\n            sys.stderr.write(\"world\\n\")\n        assert cap.out.getvalue() == \"hello\\n\"\n        assert cap.err.getvalue() == \"world\\n\"\n"], "sample_298": ["    def test_token_expiration(self):\n        user = User.objects.create_user(username=\"testuser\", password=\"testpassword\")\n        generator = MockedPasswordResetTokenGenerator(now=datetime.now())\n        token = generator.make_token(user)\n        self.assertTrue(generator.check_token(user, token))\n        # Fast forward past the expiration time\n        generator._now_val += timedelta(seconds=301)\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_token_expiration(self):\n        user = User.objects.create_user(username='testuser', password='password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime.now())\n        token = generator.make_token(user)\n\n        # Advance time beyond the timeout\n        generator._now_val = generator._now_val + timedelta(seconds=settings.PASSWORD_RESET_TIMEOUT + 1)\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_token_expires(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        generator = PasswordResetTokenGenerator()\n        token = generator.make_token(user)\n        self.assertTrue(generator.check_token(user, token))\n        # Sleep for a bit longer than the timeout\n        time.sleep(301)\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_token_expires(self):\n        user = User.objects.create_user(username='testuser', email='test@example.com', password='password')\n        generator = MockedPasswordResetTokenGenerator(now=datetime.now())\n        token = generator.make_token(user)\n        # Sleep for 11 seconds to ensure token expires\n        time.sleep(11)\n        self.assertFalse(generator.check_token(user, token))\n", "    def test_token_expired(self):\n        user = User.objects.create_user('john')\n        generator = MockedPasswordResetTokenGenerator(now=datetime.now() - timedelta(seconds=20))\n        token = generator.make_token(user)\n        self.assertFalse(generator.check_token(user, token))\n"], "sample_844": ["    def test_extend_region(r_plot, end):\n        steep_downward = [False, False, False, False, True, True, True]\n        upward = [False, False, False, False, False, False, True]\n\n        assert _extend_region(steep_downward, upward, 0, 2) == end \n", "def test_extend_region(r_plot, end):\n    steep_upward = np.array([True, False, False, False, False, True, True])\n    upward = np.array([True, False, False, False, False, True, True])\n    start = 0\n    min_samples = 2\n    assert _extend_region(steep_upward, upward, start, min_samples) == end\n", "def test_extend_region(r_plot, end):\n    steep_upward = [False, False, False, False, True, True, True]\n    upward = [False, False, False, False, True, True, True]\n    start = 0\n    min_samples = 2\n    end = _extend_region(\n        steep_upward, upward, start, min_samples\n    )\n    assert end == end\n\n", "    def test_extend_region(self):\n        steep_points = np.array([True, True, False, False, False, False, True])\n        xward_points = np.array([True, True, True, False, False, False, True])\n        start = 2\n        min_samples = 2\n        end = _extend_region(steep_points, xward_points, start, min_samples)\n        assert end == 4\n", "def test_extend_region(r_plot, end):\n    steep_point = np.array([False, False, False, False, True, True, True])\n    xward_point = np.array([True, True, True, True, False, False, False])\n    start = 0\n    min_samples = 2\n    actual_end = _extend_region(steep_point, xward_point, start, min_samples)\n    assert actual_end == end\n"], "sample_995": ["def test_issue_10923():\n    assert (1/sqrt(2)**2).simplify() == 1/2\n", "def test_sympify_complex():\n    assert sympify(2 + 3j) == 2 + 3*S.ImaginaryUnit\n    assert sympify(complex(1.5,2.5)) == 1.5 + 2.5*S.ImaginaryUnit\n    assert sympify(complex(0, -2.5)) == -2.5*S.ImaginaryUnit\n\n", "def test_Pow_exponent_is_zero():\n    assert Pow(2, 0) == 1\n    assert Pow(Symbol('x'), 0) == 1\n", "    def test_sympify_float():\n        assert sympify(1.0) == Float(1.0)\n\n", "def test_real_im():\n    assert S.One.is_real\n    assert S.One.is_imaginary is False\n    assert S.I.is_real is False\n    assert S.I.is_imaginary\n    assert (S.One + S.I).is_real is False\n    assert (S.One + S.I).is_imaginary is False\n    assert (2 + 3*S.I).is_real is False\n    assert (2 + 3*S.I).is_imaginary is False\n\n"], "sample_529": ["    def test_legend_alignment(self, loc):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6], label='line1')\n        ax.plot([1, 2, 3], [7, 8, 9], label='line2')\n        legend = ax.legend(loc=loc)\n        # We only get the handleboxes for each legend item here.\n        # The legend title is not included.\n        handleboxes = legend._legend_box.get_children()[0].get_children()\n        # Check if all legend items are aligned properly\n        assert np.allclose(handleboxes[0].get_x(),\n                           handleboxes[1].get_x())\n\n\n\n", "    def test_legend_shadow(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [1, 2, 3], label='Line')\n        ax.legend(shadow=True)\n\n", "    def test_legend_with_handler_map():\n        fig, ax = plt.subplots()\n\n        # Mock a simple custom handler\n        class MyHandler(object):\n                              xdescent, ydescent):\n                x, y = 50, 50\n                width, height = 100, 100\n                rect = plt.Rectangle((x, y), width, height,\n                                     color=orig_handle.get_color())\n                rect.set_transform(ax.transData)\n                return rect\n\n        handler_map = {mpatches.Rectangle: MyHandler()}\n        rect = mpatches.Rectangle((0.1, 0.1), 0.2, 0.2, color='blue')\n        ax.add_patch(rect)\n\n        legend = ax.legend(handler_map=handler_map)\n\n        # Check if the legend has the custom shape\n        assert len(legend.get_patches()) == 1\n        assert isinstance(legend.get_patches()[0], mpatches.Rectangle)\n        assert legend.get_patches()[0].get_width() == 100\n\n", "    def test_legend_handles_with_empty_label(self):\n        fig, ax = plt.subplots()\n        ax.plot([], [], label='')\n        ax.legend()\n        # Ensure that the legend is empty\n        assert len(ax.get_legend().get_patches()) == 0\n", "    def test_legend_handler_tuple_with_labels():\n        fig, ax = plt.subplots()\n        line1, line2 = ax.plot([1, 2, 3], [1, 2, 3], label='line1'), \\\n            ax.plot([1, 2, 3], [3, 2, 1], label='line2')\n\n        # Create a dummy tuple handler\n        class MockHandler(HandlerTuple):\n                return [\n                    mlines.Line2D([] , [],\n                                  color='red',\n                                  marker='o',\n                                  linestyle='-',\n                                  linewidth=2,\n                                  label='Custom Marker'),\n                    mlines.Line2D([] , [],\n                                  color='blue',\n                                  marker='s',\n                                  linestyle='--',\n                                  linewidth=1,\n                                  label='Custom Line')\n                ]\n                \n        handler_map = {tuple: MockHandler()}\n        legend = ax.legend(handler_map=handler_map)\n        \n        assert len(legend.get_children()) == 6  # 2 original lines + 4 from handler\n\n"], "sample_225": ["    def test_each_context_available_apps(self):\n        self.assertIn('available_apps', self.ctx)\n        for app_dict in self.ctx['available_apps']:\n            self.assertIn('name', app_dict)\n            self.assertIn('app_label', app_dict)\n            self.assertIn('app_url', app_dict)\n            self.assertIn('has_module_perms', app_dict)\n            self.assertIn('models', app_dict)\n            for model_dict in app_dict['models']:\n                self.assertIn('name', model_dict)\n                self.assertIn('object_name', model_dict)\n                self.assertIn('perms', model_dict)\n                self.assertIn('admin_url', model_dict)\n                self.assertIn('add_url', model_dict)\n", "    def test_each_context_available_apps(self):\n        self.assertIn('available_apps', self.ctx)\n        for app in self.ctx['available_apps']:\n            self.assertIn('name', app)\n            self.assertIn('app_label', app)\n            self.assertIn('app_url', app)\n            self.assertIn('has_module_perms', app)\n            self.assertIn('models', app)\n            for model in app['models']:\n                self.assertIn('model', model)\n                self.assertIn('name', model)\n                self.assertIn('object_name', model)\n                self.assertIn('perms', model)\n                self.assertIn('admin_url', model)\n                self.assertIn('add_url', model)\n\n", "    def test_each_context_available_apps(self):\n        self.assertIn('available_apps', self.ctx)\n        self.assertIsInstance(self.ctx['available_apps'], list)\n        for app in self.ctx['available_apps']:\n            self.assertIn('name', app)\n            self.assertIn('app_label', app)\n            self.assertIn('app_url', app)\n            self.assertIn('has_module_perms', app)\n            self.assertIn('models', app)\n            for model in app['models']:\n                self.assertIn('name', model)\n                self.assertIn('object_name', model)\n                self.assertIn('perms', model)\n                self.assertIn('admin_url', model)\n                self.assertIn('add_url', model)\n\n", "    def test_each_context_available_apps(self):\n        self.assertIn('available_apps', self.ctx)\n        self.assertEqual(\n            self.ctx['available_apps'][0]['name'],\n            'Authentication and Authorization'\n        )\n        self.assertEqual(\n            self.ctx['available_apps'][0]['app_label'],\n            'auth'\n        )\n        self.assertTrue(self.ctx['available_apps'][0]['has_module_perms'])\n", "    def test_available_apps_context(self):\n        # Check the structure of available_apps context variable\n        self.assertIsInstance(self.ctx['available_apps'], list)\n        for app in self.ctx['available_apps']:\n            self.assertIsInstance(app, dict)\n            self.assertIn('name', app)\n            self.assertIn('app_label', app)\n            self.assertIn('has_module_perms', app)\n            self.assertIn('app_url', app)\n            self.assertIn('models', app)\n            for model in app['models']:\n                self.assertIsInstance(model, dict)\n                self.assertIn('name', model)\n                self.assertIn('object_name', model)\n                self.assertIn('perms', model)\n                self.assertIn('admin_url', model)\n                self.assertIn('add_url', model)\n"], "sample_1012": ["    def test_print_Assignment(self):\n        a = symbols('a')\n        eq = Eq(a, x + y)\n        assign = Assignment(a, eq)\n        printer = SciPyPrinter()\n        code = printer.doprint(assign)\n        assert code == 'a = Eq(a, x + y)'\n", "    def test_scipy_sparse_matrix(self):\n        s_mat = SparseMatrix([[1, 0, 0], [0, 2, 0], [0, 0, 3]])\n        code = SciPyPrinter().doprint(s_mat)\n        expected_code = \"scipy.sparse.coo_matrix([1, 2, 3], ([0, 1, 2], [0, 1, 2]), shape=(3, 3))\"\n        assert code == expected_code\n", "    def test_print_scipy_sparse_matrix(self):\n        m = SparseMatrix([[1, 2], [0, 3]])\n        s = SciPyPrinter().doprint(m)\n        assert s == (\"scipy.sparse.coo_matrix([1, 2, 3], ([0, 0, 1], [0, 1, 1]), \"\n                     \"shape=(2, 2))\")\n", "def test_scipy_sparse_matrix():\n    m = SparseMatrix([[1, 0, 0], [0, 0, 1]])\n    code = SciPyPrinter().doprint(m)\n    assert code == 'scipy.sparse.coo_matrix([1, 1], ([0, 1], [0, 2]), shape=(2, 3))'\n", "def test_printing_piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pycode(expr, standard='python3') == '1 if (x < 1) else (x**2)'\n"], "sample_322": ["    def test_migrate_all_backwards(self):\n        \"\"\"Test migrating backwards, covering the full backwards path.\"\"\"\n        # Make a migration that does some changes then unapply it\n        self.migrate_to('migrations2', 0002_auto_20220517_1953)\n        self.assertColumnExists('migrations2_thing', 'description')\n        # Now we roll back the migration.\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM migrations2_thing\")\n            initial_count = cursor.fetchone()[0]\n        self.executor.migrate(\n            targets=[('migrations2', None)],\n            fake=False,\n        )\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT COUNT(*) FROM migrations2_thing\")\n            final_count = cursor.fetchone()[0]\n\n        # Make sure the change is reversed\n        self.assertEqual(initial_count, final_count)\n        self.assertColumnNotExists('migrations2_thing', 'description')\n\n", "    def test_migrate_empty_database_with_forwards_plan(self):\n        \"\"\"\n        Test migrating an empty database with a forwards plan.\n        \"\"\"\n        # Create a new database for testing.\n        self._create_test_database()\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE django_migrations (id INT PRIMARY KEY);\")\n        # Set up the migration executor.\n        executor = MigrationExecutor(self.connection)\n        # Get the migration graph.\n        loader = MigrationExecutor.loader = MigrationLoader(self.connection)\n        graph = loader.graph\n        # Get the target migrations.\n        targets = [\n            (\"migrations2\", \"0002_second_migration\"),\n        ]\n        # Generate the migration plan.\n        plan = executor.migration_plan(targets)\n        # Run the migrations.\n        state = executor.migrate(targets, plan=plan)\n        # Check if the migrations were applied successfully.\n        self.assertEqual(len(state.apps), 2)\n        self.assertEqual(len(state.models), 2)\n", "    def test_unapply_replacements(self):\n        \"\"\"\n        Test that unapplying a replacement migration correctly unapplies\n        all replaced migrations.\n        \"\"\"\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE migrations2_replacedmodel (id INT PRIMARY KEY)\")\n        with self.assertRaises(DatabaseError):\n            with connection.schema_editor() as editor:\n                editor.delete_model(models.Model._meta.model)\n        # Apply the replacement migration\n        self.migrate()\n\n        # Unapply the replacement migration\n        self.migrate(\n            targets=[\n                (\"migrations2\", \"0002_replacedmodel\"),\n            ],\n            fake=True,\n        )\n\n        # The replaced migrations should be unapplied\n        recorder = MigrationRecorder(connection)\n        self.assertNotIn(('migrations2', '0001_initial'), recorder.applied_migrations())\n", "    def test_migrate_empty_database(self):\n        # Test that migrating an empty database works as expected.\n        # Create a new database for the test.\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE DATABASE migrate_test_db\")\n        self.connection.close()\n        # Connect to the new database.\n        self.connection = connection.connect(\"migrate_test_db\")\n        self.migration_executor = MigrationExecutor(self.connection)\n        # Run migrations.\n        self.migration_executor.migrate(\n            targets=[(\"migrations2\", \"0001_initial\"), (\"migrations\", \"0001_initial\")],\n        )\n        # Check that the tables were created.\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM django_migrations\")\n            rows = cursor.fetchall()\n            self.assertEqual(len(rows), 2)\n        self.connection.close()\n", "    def test_migrate_with_fake_initial_migration(self):\n        \"\"\"\n        Test that fake initial migrations work correctly.\n\n        This is more of an integration test, as it tests the interaction\n        between the executor, loader and recorder.\n        \"\"\"\n        with connection.cursor() as cursor:\n            self.connection.introspection.table_names(cursor)\n        self.migrate(\n            settings.MIGRATIONS_MODULES[\"migrations\"] + [\n                \"{}.migrations\".format(app) for app in self.available_apps[1:]\n            ],\n            fake_initial=True,\n        )\n        self.assertMigrationHistory()\n        with connection.cursor() as cursor:\n            self.connection.introspection.table_names(cursor)\n\n"], "sample_816": ["    def test_stop_words_parameter_with_vocabulary(self):\n        # Test that providing a vocabulary overrides stop words parameter\n\n        corpus = [\"This is a document.\", \"This document is the second document.\"]\n        stop_words = {'is', 'the'}\n\n        vectorizer = TfidfVectorizer(stop_words=stop_words)\n        X1 = vectorizer.fit_transform(corpus)\n\n        vocabulary = vectorizer.vocabulary_\n        vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n        X2 = vectorizer.fit_transform(corpus)\n\n        assert_array_equal(X1.toarray(), X2.toarray())\n", "    def test_tfidfvectorizer_stop_words_empty(self):\n        corpus = ['This is a test document']\n        vectorizer = TfidfVectorizer(stop_words=[])\n        vectorizer.fit(corpus)\n        assert_array_equal(vectorizer.stop_words_, set())\n", "    def test_tfidfvectorizer_vocabulary_pickle(self):\n        # test that vocabulary consistency is maintained upon pickling\n        # and unpickling a TfidfVectorizer\n        corpus = [\n            \"This is the first document.\",\n            \"This document is the second document.\",\n            \"And this is the third one.\",\n            \"Is this the first document?\"\n        ]\n        vectorizer = TfidfVectorizer()\n        vectorizer.fit(corpus)\n        \n        # pickle and unpickle the vectorizer\n        pickled_vectorizer = pickle.dumps(vectorizer)\n        unpickled_vectorizer = pickle.loads(pickled_vectorizer)\n        \n        # compare the vocabularies\n        assert_equal(vectorizer.vocabulary_, unpickled_vectorizer.vocabulary_)\n\n", "    def test_hashingvectorizer_stop_words(self):\n        # Test that HashingVectorizer respects stop_words\n\n        corpus = [\n            'this is a test document',\n            'this document is a test',\n            'one more test document',\n        ]\n\n        vectorizer_no_stop = HashingVectorizer(stop_words='english')\n        vectorizer_with_stop = HashingVectorizer(stop_words=None)\n\n        X_no_stop = vectorizer_no_stop.fit_transform(corpus)\n        X_with_stop = vectorizer_with_stop.fit_transform(corpus)\n\n        assert X_no_stop.shape[1] < X_with_stop.shape[1]\n", "    def test_tfidf_vectorizer_vocabulary_parameter(self):\n        corpus = ['This is the first document.',\n                  'This document is the second document.',\n                  'And this is the third one.',\n                  'Is this the first document?']\n        expected_vocabulary = {\n            'this': 0,\n            'is': 1,\n            'the': 2,\n            'first': 3,\n            'document': 4,\n            'second': 5,\n            'and': 6,\n            'third': 7,\n            'one': 8\n        }\n        expected_idf = np.array([\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098,\n            2.0986122886681098\n        ])\n\n        vectorizer = TfidfVectorizer(vocabulary=expected_vocabulary)\n\n        vectorizer.fit(corpus)\n\n        assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n        assert_array_almost_equal(vectorizer.idf_, expected_idf)\n"], "sample_1164": ["compilation error", "    def test_cg_simp_sum_varsh_872_4_0(self):\n        a, b, c = symbols('a b c', integer=True)\n        alpha, beta = symbols('alpha beta', integer=True)\n        cg1 = CG(a, alpha, b, beta, c, alpha)\n        cg2 = CG(a, alpha, b, beta, c, alpha + 1)\n        e = Sum(cg1*cg2, (alpha, -a, a), (beta, -b, b))\n        self.assertEqual(cg_simp(e), 0)\n\n", "def test_wigner6j_simp():\n    j1, j2, j3, j4, j5, j6 = symbols('j1 j2 j3 j4 j5 j6', integer=True)\n\n    assert Wigner6j(j1, j2, j3, j1, j2, j3).doit() == 1\n    assert Wigner6j(j1, j2, j3, j4, j5, j6).doit() == 0\n\n", "def test_cg_simp_sum():\n    a, alpha, b = symbols('a alpha b')\n    e = Sum(CG(a, alpha, b, 0, a, alpha), (alpha, -a, a))\n    assert cg_simp(e).doit() == KroneckerDelta(b, 0)*(2*a + 1)\n", "def test_cg_simp():\n    a, b, c, alpha, beta, gamma = symbols('a b c alpha beta gamma')\n    expr = CG(a, alpha, b, beta, c, gamma)\n    assert cg_simp(expr) == expr\n"], "sample_1204": ["    def test_coset(self):\n        S = SymmetricGroup(3)\n        a = Permutation(0, 1)\n        H = CyclicGroup(2).subgroup(S)\n        C = Coset(a, H)\n        assert C.is_right_coset\n        assert set(C.as_list()) == {a, Permutation(1, 2)}\n        C = Coset(a, H, dir=\"-\")\n        assert C.is_left_coset\n        assert set(C.as_list()) == {a, Permutation(1, 2)}\n", "def test_coset_left_right():\n    a = Permutation([0, 1, 2])\n    b = Permutation([1, 0, 2])\n    G = PermutationGroup([a, b])\n    cst_l = Coset(a, G, dir=\"-\")\n    cst_r = Coset(a, G, dir=\"+\")\n    assert cst_l.is_left_coset == True\n    assert cst_r.is_right_coset == True\n\n", "    def test_coset_representation():\n        a = Permutation(0, 1, 2)\n        b = Permutation(0, 2)\n        G = PermutationGroup([a, b])\n        H = G.subgroup([a])\n        c1 = Coset(a*b, H, G, dir='-')\n        c2 = Coset(a*b, H, G, dir='+')\n        assert c1.as_list() != c2.as_list()\n        assert set(c1.as_list()) == set(c2.as_list())\n\n", "    def test_coset_representation():\n        G = SymmetricGroup(4)\n        a = Permutation(1, 2)\n        H = G.subgroup([Permutation(0, 1), Permutation(2, 3)])\n        C = Coset(a, H, dir='+')\n        assert C.as_list() == [Permutation(1, 2), Permutation(2, 3), Permutation(0, 1, 2, 3)]\n", "def test_coset_equality():\n    a = Permutation(1, 2)\n    b = Permutation(0, 1)\n    G = PermutationGroup([a, b])\n    c1 = Coset(a, G, dir=\"+\")\n    c2 = Coset(a*b, G, dir=\"+\")\n    assert c1 == c2\n"], "sample_424": ["    def test_alter_model_options_managers(self):\n        \"\"\"Test altering model managers.\"\"\"\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE myapp_food (id INTEGER PRIMARY KEY)\")\n        \n        class MyFood(models.Model):\n            objects = FoodManager()\n\n            class Meta:\n                app_label = 'myapp'\n            \n        old_state = State.create_from_models(models.get_models())\n        manager_state = old_state.models['myapp', 'MyFood'].managers\n        self.assertEqual(len(manager_state), 1)\n\n        migration = Migration(\n            'test_alter_model_managers',\n            [\n                AlterModelManagers(\n                    'MyFood',\n                    managers=[\n                        ('objects', FoodQuerySet.as_manager()),\n                    ],\n                )\n            ]\n        )\n        \n        new_state = migration.apply(old_state)\n        manager_state = new_state.models['myapp', 'MyFood'].managers\n        self.assertEqual(len(manager_state), 1)\n\n", "    def test_rename_field_with_related_objects(self):\n        with connection.schema_editor() as editor:\n            with atomic(using=editor.connection.alias):\n                editor.create_model(\n                    Model(\n                        name=models.CharField(max_length=100),\n                        related_field=models.ForeignKey(\n                            \"self\", on_delete=models.CASCADE, related_name=\"related\"\n                        ),\n                    ),\n                )\n\n                # Create some objects\n                model = Model.objects.create(name=\"Test\")\n                related = Model.objects.create(name=\"Related\", related_field=model)\n\n        # Rename the field\n        operation = RenameField(\n            \"app\", \"Model\", \"related_field\", old_name=\"related_field\" , new_name=\"new_related_field\"\n        )\n        self.assertOperationEqual(\n            operation,\n            (\n                operation.__class__.__name__,\n                [],\n                {\"app\": \"app\", \"model\": \"Model\", \"old_name\": \"related_field\", \"new_name\": \"new_related_field\"},\n            ),\n        )\n\n        # Apply migration forward\n        self.apply_migration(\n            Migration(\n                \"test\",\n                [operation],\n            ),\n            \"test_app\",\n        )\n   \n        self.assertEqual(\n            Model.objects.get(name=\"Test\").new_related_field.name, \"Related\"\n        )\n\n        # Reverse the migration\n        operation = RenameField(\n            \"app\", \"Model\", \"new_related_field\", old_name=\"new_related_field\" , new_name=\"related_field\"\n        )\n\n        self.apply_migration(\n            Migration(\n                \"test\",\n                [operation],\n            ),\n            \"test_app\",\n            reverse=True,\n        )\n\n        self.assertEqual(\n            Model.objects.get(name=\"Test\").related_field.name, \"Related\"\n        )\n\n\n\n", "    def test_add_field_with_default(self):\n        \"\"\"\n        #22995 -- Adding a field with a default shouldn't break migrations.\n        \"\"\"\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE IF NOT EXISTS migrationstest_unicodemodel (id INTEGER PRIMARY KEY)\")\n\n        with migration_checks(\n            {\n                \"operations\": [\n                    migrations.AddField(\n                        model_name=\"unicodemodel\",\n                        name=\"extra_field\",\n                        field=models.CharField(default=\"hello\"),\n                    )\n                ],\n                \"apps\": [\n                    \"migrationstest\",\n                ],\n            }\n        ) as check_result:\n            self.assertEqual(check_result.errors, [])\n", "    def test_rename_index(self):\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE polls_poll (id INT PRIMARY KEY, question VARCHAR(255))\")\n            cursor.execute(\"CREATE INDEX polls_poll_question ON polls_poll (question)\")\n\n        with self.assertRaises(FieldDoesNotExist):\n            self.models.Poll._meta.get_field(\"new_question\")\n\n        with atomic(using=self.connection.alias):\n            self.migrate([\n                migrations.RenameIndex(\n                    model_name='poll',\n                    old_name='polls_poll_question',\n                    new_name='polls_poll_new_question',\n                )\n            ])\n\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM sqlite_master WHERE type = 'index' AND tbl_name = 'polls_poll'\")\n            row = cursor.fetchone()\n            self.assertEqual(row[2], 'polls_poll_new_question')\n    \n        with self.assertRaises(FieldDoesNotExist):\n            self.models.Poll._meta.get_field(\"question_old\")\n\n        with atomic(using=self.connection.alias):\n            self.migrate([\n                migrations.RenameIndex(\n                    model_name='poll',\n                    old_name='polls_poll_new_question',\n                    new_name='polls_poll_question_old',\n                )\n            ])\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"SELECT * FROM sqlite_master WHERE type = 'index' AND tbl_name = 'polls_poll'\")\n            row = cursor.fetchone()\n            self.assertEqual(row[2], 'polls_poll_question_old')\n\n", "    def test_alter_field_rename(self):\n        # Test renaming a field\n        with atomic():\n            with connection.cursor() as cursor:\n                cursor.execute(\"CREATE TABLE test_alter_field_rename (id SERIAL PRIMARY KEY, name TEXT)\")\n                cursor.execute(\"INSERT INTO test_alter_field_rename (name) VALUES ('Old Name')\")\n\n        class RenamedModel(models.Model):\n            new_name = models.CharField(max_length=100)\n\n        # Create Migration\n        operations = [\n            migrations.RenameField(\n                model_name=\"renamedmodel\",\n                old_name='name',\n                new_name='new_name',\n            ),\n        ]\n        migration = Migration(\n            \"test_rename_field\",\n            operations,\n        )\n\n        # Apply Migration\n        self.apply_migration(migration)\n\n        # Check state\n        self.assertTableExists(\"test_alter_field_rename\")\n\n        # Check database\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT new_name FROM test_alter_field_rename\")\n            row = cursor.fetchone()\n            self.assertEqual(row[0], 'Old Name')\n\n        # Reverse Migration\n        self.unapply_migration(migration)\n\n        # Check database after reversing\n        with connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM test_alter_field_rename\")\n            row = cursor.fetchone()\n            self.assertEqual(row[0], 'Old Name')\n"], "sample_959": ["def test_template_param_decl():\n    check(\n        'templateParam',\n        'template <typename T>',\n        {},\n        'template <typename T>',\n        key='templateParameter',\n    )\n\n", "def test_type_alias():\n    check('type', 'using MyType = int',\n          {}\n          , output='using MyType = int',\n          key='MyType',\n          asTextOutput=\"MyType\")\n", "    def test_enum_nested_in_namespace(self):\n        input = \"\"\"\n        namespace ns {\n        enum class MyEnum : int {{\n            VALUE1,\n            VALUE2\n        }};\n        }\n        \"\"\"\n        idDict = {\n            1: 'ns_1_MyEnum',\n            2: 'ns_2_MyEnum',\n            3: 'ns_3_MyEnum',\n        }\n        output = \"\"\"\n        enum class ns::MyEnum : int {{\n            VALUE1,\n            VALUE2\n        }};\n        \"\"\"\n        check('enum', input, idDict, output,\n              key='ns::MyEnum')\n", "def test_cpp_enum_with_nested_enum():\n    check(\n        \"enum\",\n        \"\"\"", "def test_enum_basic():\n    check(\"enum\", \"enum { a, b, c };\", {\n        1: \"enum_a\", 2: \"enum_enum_b\", 3: \"enum_enum_c\"\n    }, key=\"enum_\", asTextOutput=\"enum { a, b, c }\")\n"], "sample_56": ["    def test_admin_site_middleware(self):\n        site = AdminSite()\n        site.register(Song, ValidFields)\n        middleware = site.middleware\n\n        self.assertEqual(\n            [m.__class__.__name__ for m in middleware],\n            [\n                'AuthenticationMiddlewareSubclass',\n                'MessageMiddlewareSubclass',\n                'SessionMiddleware',\n                'CommonMiddleware',\n                'CSRFMiddleware',\n            ],\n        )\n\n", "    def test_check_middleware(self):\n        admin_site = AdminSite()\n        admin_site.register(Album, MyAdmin)\n        \n        checks_run = admin_site.check(include_all_models=True)\n        self.assertEqual(len([c for c in checks_run if 'admin.E101' in c.id]), 1)\n\n        admin_site.register(Album, ValidFields)\n        checks_run = admin_site.check(include_all_models=True)\n        self.assertEqual(len([c for c in checks_run if 'admin.E101' in c.id]), 0)\n\n", "    def test_check_exclude_of_parent_model(self):\n        class MyInline(admin.TabularInline):\n            model = Song\n            exclude = ['title',]\n            fk_name = 'album'\n\n        class MyAdmin(admin.ModelAdmin):\n            inlines = [MyInline]\n\n        site = AdminSite()\n        site.register(Album, MyAdmin)\n        errors = site._registry[Album].check(None)\n        self.assertEqual(\n            errors,\n            [\n                checks.Error(\n                    \"Cannot exclude the field 'title', because it is the foreign key \"\n                    \"to the parent model 'admin_checks.Album'.\",\n                    obj=MyInline,\n                    id='admin.E201',\n                ),\n            ],\n        )\n\n", "    def test_check_inlines_item_model_admin_subclass_with_error(self):\n        class MyInline(GenericStackedInline):\n            model = City\n            extra = 1\n                return ['error!']\n\n        class MyBookAdmin(admin.ModelAdmin):\n            inlines = [MyInline]\n\n        admin_site = AdminSite()\n        admin_site.register(Book, MyBookAdmin)\n        errors = admin_site.check()\n\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'admin.E104')\n        self.assertIn('MyInline', errors[0].msg)\n", "compilation error"], "sample_1092": ["def test_cse_piecewise():\n    p = Piecewise((x, x < 1), (x**2, x >= 1))\n    s = Symbol('s')\n    expr = p + s\n    reps, new_expr = cse([expr], symbols=numbered_symbols('tmp'))\n    assert new_expr == (tmp0 + s)\n    assert reps == [(tmp0, p)]\n", "    def test_cse_with_piecewise():\n        p = Piecewise((x, x > 0), (y, True))\n        q = Piecewise((x, x < 1), (z, True))\n        r = p + q\n        replacements, reduced_exprs = cse([r], order='none')\n        assert len(replacements) == 3\n        assert reduced_exprs[0].has(x0)\n        assert reduced_exprs[0].has(x1)\n        assert reduced_exprs[0].has(x2)\n", "def test_cse_issue_20966():\n    from sympy.functions import sin\n    n = Symbol('n', integer=True, positive=True)\n    expr = sin(n)*sin(x)**2 + sin(n)*sin(x)**2 + sin(n)**2*sin(x)\n\n    r, e = cse(expr, symbols=numbered_symbols('x'))\n    assert r == [(x0, sin(n)*sin(x)**2)]\n    assert e == [x0 + x0 + sin(n)**2*sin(x)]\n", "def test_cse_piecewise():\n    f = Function('f')\n    p = Piecewise((0, Eq(x, 0)), (f(x), True))\n    e = Add(p, p)\n    reps, e = cse([e], symbols=symbols('a'), optimizations=[])\n    assert reps == [(x0, f(x))]\n    assert e[0] == x0 + x0\n", "def test_cse_sparse_matrix_with_substitution():\n    m = SparseMatrix([[x + y, y + z], [x + z, x + y + z]])\n    exprs = [m, (x + y) ** 2]\n    replacements, reduced_exprs = cse(exprs)\n    assert isinstance(reduced_exprs[0], SparseMatrix)\n"], "sample_1047": ["def test_issue_8505():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x + y).is_real is True\n    assert (x + I).is_real is None\n", "    def test_issue_10424():\n        x = Symbol('x')\n        y = Symbol('y', real=True)\n        assert (x + 1).is_real is None\n        assert (y + 1).is_real is True\n\n", "def test_issue_7649():\n    assert I.is_imaginary is True\n    assert (2*I).is_imaginary is True\n    assert (2 + 2*I).is_imaginary is False\n", "def test_issue_10324():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert (x + y).is_real is True\n", "def test_is_imaginary():\n    assert I.is_imaginary is True\n    assert (2*I).is_imaginary is True\n    assert (1 + I).is_imaginary is False\n    assert sqrt(-1).is_imaginary is True\n    assert (1/I).is_imaginary is True\n    assert (2 + 2*I).is_imaginary is False\n    assert (2*I/3).is_imaginary is True\n    assert (I*exp(I*pi)).is_imaginary is True\n    assert (I*sin(pi/2)).is_imaginary is True\n\n"], "sample_121": ["    def test_check_unique_together_with_lazy_references(self):\n        with override_settings(DEBUG=True):\n            class LazyModel(models.Model):\n                name = models.CharField(max_length=100)\n                related_obj = models.ForeignKey('RelatedModel', on_delete=models.CASCADE)\n\n            class RelatedModel(models.Model):\n                value = models.IntegerField()\n\n            # Define unique_together constraint\n            LazyModel._meta.unique_together = ('name', 'related_obj')\n\n            # Signal post_init to trigger lazy loading\n            signal_received = False\n                nonlocal signal_received\n                signal_received = True\n\n            post_init.connect(handle_post_init)\n\n            # Check for issues during initialization\n            errors = LazyModel.check(apps=apps)\n\n            self.assertFalse(errors)\n            self.assertTrue(signal_received)\n\n", "    def test_invalid_index_together_long_name(self):\n        allowed_len, db_alias = get_max_column_name_length()\n        if allowed_len is None:\n            return\n\n        with override_settings(DATABASES={'default': {'ENGINE': 'django.db.backends.sqlite3'}}):\n            with self.assertRaisesMessage(ImproperlyConfigured,\n                                          f'Column name for field \"very_long_field_name\" is too long (max: {allowed_len}).'):\n\n                class MyModel(models.Model, ):\n                    very_long_field_name = models.CharField(max_length=255)\n\n                    class Meta:\n                        index_together = [\n                            (\"very_long_field_name\",)\n                        ]\n", "    def test_index_together_with_invalid_field(self):\n        with self.assertRaisesMessage(ImproperlyConfigured, \"The field 'does_not_exist' is not defined\"):\n            class ModelWithInvalidField(models.Model):\n                field1 = models.CharField(max_length=10)\n                index_together = [('field1', 'does_not_exist')]\n", "    def test_check_index_together_with_invalid_field(self):\n        class MyClass(models.Model):\n            field1 = models.CharField(max_length=10)\n            field2 = models.CharField(max_length=10)\n\n        with self.assertRaisesMessage(ImproperlyConfigured,\n                                     \"The field 'invalid_field' is not a \"\n                                     \"field on model 'MyClass'.\"):\n            MyClass.check(index_together=[('field1', 'invalid_field')])\n\n\n", "    def test_check_long_column_names(self):\n        (allowed_len, db_alias) = get_max_column_name_length()\n        if allowed_len is None:\n            return\n\n        # Create a model with a field name longer than the allowed length.\n        with self.assertRaisesMessage(\n            ImproperlyConfigured,\n            'Autogenerated column name too long for field \"%s\". Maximum length is \"%s\" for database \"%s\".' % (\n                'very_long_field_name' * (1 + allowed_len // len('very_long_field_name')),\n                allowed_len, db_alias\n            )\n        ):\n            class LongFieldNameModel(models.Model):\n                very_long_field_name = models.CharField(max_length=255)\n\n            LongFieldNameModel._meta.check()\n"], "sample_969": ["    def test_restify_Union_with_None():\n        res = restify(Union[str, None])\n        assert res == ':py:obj:`~typing.Optional`\\\\ [str]'\n", "    def test_stringify_newtype(self, app):\n        app.builder.env.temp_data['objects'] = {}\n        assert stringify(MyInt) == 'MyInt'\n        assert stringify(MyInt(1)) == 'MyInt'\n        assert stringify(MyInt, smartref=True) == 'MyInt'\n", "    def test_restify_optional_union():\n        assert restify(Optional[Union[MyClass1, str]]) == ':py:obj:`~typing.Optional`\\\\ [:py:obj:`~typing.Union`\\\\ [:py:class:`MyClass1`, :py:class:`str`]]'\n\n\n", "    def test_restify_typing_union_none():\n        assert restify(Union[str, NoneType]) == 'Optional[str]'\n        assert restify(Union[NoneType, str]) == 'Optional[str]'\n        assert restify(Union[MyClass1, NoneType]) == 'Optional[MyClass1]'\n        assert restify(Union[NoneType, MyClass1]) == 'Optional[MyClass1]'\n", "    def test_stringify_forwardref(self):\n        self.assertEqual(stringify(ForwardRef('MyClass')), 'MyClass')\n"], "sample_250": ["    def test_format_with_timezone_aware_datetime(self):\n        dt = make_aware(datetime(2023, 10, 26, 10, 0, 0), timezone=utc)\n        formatted = dateformat.format(dt, 'c')\n        self.assertEqual(formatted, '2023-10-26T10:00:00+00:00')\n\n", "    def test_formatting_aware_datetime(self):\n        d = make_aware(datetime(2023, 10, 26, 12, 30, 0), get_fixed_timezone(1))\n        # Check O format specifier output\n        self.assertEqual(format(d, 'O'), '+0100')\n        # Check T format specifier output\n        self.assertEqual(format(d, 'T'), 'CET')\n", "    def test_timezone_aware_datetime(self):\n        now = datetime.now().replace(tzinfo=utc).astimezone(get_default_timezone())\n        formatted = format(now, 'r')\n        self.assertEqual(formatted, now.strftime('%a, %d %b %Y %H:%M:%S %z'))\n\n", "    def test_format_z(self):\n        with self.settings(USE_TZ=True):\n            d = datetime(2023, 10, 26, 10, 30, 0, tzinfo=utc)\n            self.assertEqual(format(d, 'Z'), '0')\n\n            d = datetime(2023, 10, 26, 10, 30, 0, tzinfo=get_fixed_timezone(180))\n            self.assertEqual(format(d, 'Z'), '+18000')\n\n", "    def test_format_timezone_aware_datetime(self):\n        dt = make_aware(datetime(2008, 1, 2, 10, 30, 0), get_fixed_timezone(180))\n        self.assertEqual(format(dt, 'e'), '+0300')\n        self.assertEqual(format(dt, 'O'), '+0300')\n"], "sample_799": ["def test_cross_val_score_with_sparse_matrix():\n    \"\"\"Test cross_val_score with sparse matrix.\"\"\"\n    X = coo_matrix(np.eye(5))\n    y = np.array([0, 1, 2, 0, 1])\n\n    # Test with a classifier\n    clf = KNeighborsClassifier()\n    scores = cross_val_score(clf, X, y)\n    assert scores.shape == (5,)\n\n    # Test with a regressor\n    reg = Ridge()\n    scores = cross_val_score(reg, X, y, scoring='neg_mean_squared_error')\n    assert scores.shape == (5,)\n", "def test_cross_val_score_sparse_matrix():\n    # test cross_val_score with sparse matrices\n    clf = MockClassifier(allow_nd=True)\n    scores = cross_val_score(clf, X_sparse, y, cv=3)\n    assert_array_almost_equal(scores, [1.0 / (1 + np.abs(0))] * 3)\n", "    def test_validation_curve_with_scoring(self):\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   random_state=42)\n        clf = LogisticRegression()\n        param_range = np.linspace(0.01, 1, 10)\n\n        train_scores, test_scores = validation_curve(\n            clf, X, y, 'C', param_range, scoring='f1_macro', cv=5\n        )\n\n        assert train_scores.shape == (10, 5)\n        assert test_scores.shape == (10, 5)\n\n", "def test_cross_val_predict_with_sparse():\n    \"\"\"Test cross_val_predict with sparse input.\"\"\"\n    # Make sure sparse matrices are handled correctly\n    clf = KNeighborsClassifier()\n    y_pred = cross_val_predict(clf, X_sparse, y, cv=3)\n    assert_array_equal(y_pred.shape, y.shape)\n", "    def test_cross_val_score_sparse_matrices(self):\n        X = coo_matrix(np.random.randn(10, 5))\n        y = np.array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n\n        clf = MockClassifier()\n        scores = cross_val_score(clf, X, y, cv=3)\n        assert scores.shape == (3,)\n\n        clf = MockClassifier(allow_nd=True)\n        scores = cross_val_score(clf, X, y, cv=3)\n        assert scores.shape == (3,)\n"], "sample_459": ["    def test_integer_field_overflow(self):\n        # Test for overflow\n        if self.backend_range[1] == self.documented_range[1]:\n            with self.assertRaises(self.model.IntegerField.overflow_exception):\n                self.model.objects.create(value=self.documented_range[1] + 1)\n        else:\n            # Skip this test for backends with a different valid range.\n            raise SkipTest(\"Backend does not support testing for integer field overflow.\")\n\n", "    def test_integer_field_overflow(self):\n        with self.assertRaises(self.model.IntegerOverflowError):\n            self.model.objects.create(value=self.backend_range[1] + 1)\n        with self.assertRaises(self.model.IntegerUnderflowError):\n            self.model.objects.create(value=self.backend_range[0] - 1)\n\n", "    def test_integerfield_overflow(self):\n        # Test that IntegerField raises an exception on overflow/underflow\n        if self.backend_range != self.documented_range:\n            raise SkipTest(\n                \"Skipping test because backend integer range differs from \"\n                f\"documented range: {self.backend_range} != {self.documented_range}\"\n            )\n\n        with self.assertRaises(EmptyResultSet):\n            IntegerModel.objects.create(value=self.documented_range[0] - 1)\n\n        with self.assertRaises(EmptyResultSet):\n            IntegerModel.objects.create(value=self.documented_range[1] + 1)\n", "    def test_exact_overflow(self):\n        if self.backend_range is None:\n            raise SkipTest(\"Backend does not support integer field range checks.\")\n\n        if self.backend_range[0] is not None:\n            with self.assertRaises(EmptyResultSet):\n                self.model.objects.filter(value__exact=self.backend_range[0] - 1)\n\n        if self.backend_range[1] is not None:\n            with self.assertRaises(EmptyResultSet):\n                self.model.objects.filter(value__exact=self.backend_range[1] + 1)\n", "    def test_integer_field_overflow(self):\n        # Test that IntegerField overflow and underflow raise the correct exceptions.\n        with self.assertRaisesMessage(\n            EmptyResultSet, \"Integer field 'value' overflowed\"\n        ):\n            IntegerModel.objects.create(value=2147483648)\n        with self.assertRaisesMessage(\n            EmptyResultSet, \"Integer field 'value' underflowed\"\n        ):\n            IntegerModel.objects.create(value=-2147483649)\n"], "sample_1095": ["def test_af_parity():\n    assert _af_parity([]) == 1\n    assert _af_parity([0, 1, 2]) == 1\n    assert _af_parity([0, 2, 1]) == -1\n    assert _af_parity([1, 0, 2]) == -1\n    assert _af_parity([1, 2, 0]) == 1\n\n", "def test_inverse():\n    p = Permutation([0, 2, 1, 3])\n    assert p.inverse() == Permutation([0, 2, 1, 3])\n\n", "def test_perm_array_form():\n    p = Permutation([1, 0, 2, 3])\n    assert p.array_form == [1, 0, 2, 3]\n\n", "def test_rmuln():\n    p = Permutation([0, 1, 2, 3, 4])\n    assert _af_rmuln(p, 2) == p * p\n", "def test_apply_cycle():\n    p = Permutation([[0, 1, 2], [3, 4]])\n    x = Symbol('x', integer=True)\n    x = Permutation.apply(p, x)\n    assert x == AppliedPermutation(p, x)\n    assert x.perm == p\n    assert x.x == x\n"], "sample_822": ["    def test_pairwise_distances_sparse_input_check(self, metric):\n        X = csr_matrix([[0, 1], [1, 0]])\n        Y = csr_matrix([[0, 1], [1, 0]])\n\n        if metric in PAIRWISE_BOOLEAN_FUNCTIONS:\n            with pytest.raises(TypeError):\n                pairwise_distances(X, Y, metric=metric)\n\n        else:\n            pairwise_distances(X, Y, metric=metric)\n\n", "    def test_pairwise_distances_sparse_unravel_output(self):\n        # Test that the sparse output format of pairwise_distances\n\n        # Regression test for issue #19275\n        X = dok_matrix((5, 3))\n        X[0, 0] = 1\n        X[1, 1] = 1\n        X = X.tocsr()\n        D = pairwise_distances(X, metric='euclidean',\n                               n_jobs=-1)\n        assert issparse(D)\n\n        D = pairwise_distances(X, metric='euclidean',\n                               n_jobs=-1,\n                               force_all_finite=False)\n        assert issparse(D)\n\n", "    def test_pairwise_distances_chunked_row_by_row(self):\n        X = np.random.rand(10, 5)\n        n_jobs_list = [None, 1, 2]\n\n        for n_jobs in n_jobs_list:\n            for metric in ['euclidean', 'manhattan', 'cosine']:\n\n                with config_context(working_memory='0'): # force row-by-row\n\n                    chunks = list(pairwise_distances_chunked(X,\n                                                             metric=metric,\n                                                             n_jobs=n_jobs))\n                    full_dist = pairwise_distances(X, metric=metric)\n\n                assert_allclose(np.vstack(chunks), full_dist)\n\n", "    def test_pairwise_distances_chunked_empty_input(self):\n        # Test that pairwise_distances_chunked behaves correctly\n        # with empty input arrays.\n\n        X = np.array([])\n        Y = np.array([])\n\n        with pytest.warns(UserWarning):\n            result = next(pairwise_distances_chunked(X, Y))\n        assert result is None\n\n\n", "    def test_pairwise_distances_chunked_memory(self):\n        # Test that pairwise_distances_chunked doesn't exceed memory limit\n        rng = np.random.RandomState(0)\n        X = rng.rand(1000, 10)\n        Y = rng.rand(1000, 10)\n\n            return chunk.mean(axis=1)\n\n        # Test with working memory less than chunk size\n        for working_memory in [1000, 500]:\n            for chunk in pairwise_distances_chunked(\n                    X, Y, reduce_func=reduce_func,\n                    working_memory=working_memory\n            ):\n                assert chunk.shape[0] <= working_memory\n"], "sample_985": ["def test_Min_rewrite_as_Piecewise():\n    x, y = symbols('x y')\n    assert Min(x, y).rewrite(Piecewise) == Piecewise((x, x <= y), (y, True))\n", "def test_Max_Min_basic():\n    x, y = symbols(\"x y\")\n    assert Max(x, y).func is Max\n    assert Min(x, y).func is Min\n    assert Max(x, y) == Max(y, x)\n    assert Min(x, y) == Min(y, x)\n", "    def test_Min_Max_empty_args(self):\n        raises(ValueError, lambda: Min())\n        raises(ValueError, lambda: Max())\n\n", "def test_Min_basic():\n    x, y = symbols('x y')\n    assert Min(4, x).evalf(subs={x:2}) == 4\n    assert Min(x, 4).evalf(subs={x:2}) == 2\n    assert Min(x, y).evalf(subs={x:2, y:3}) == 2\n\n", "def test_Basic_args():\n    assert b1.args == ()\n    assert b2.args == (b1,)\n    assert b3.args == (b2,)\n    assert b21.args == (b2, b1)\n"], "sample_481": ["    def test_join_with_empty_list(self):\n        self.assertEqual(\n            self.engine.render_to_string(\"join01\", {\"a\": []}),\n            \"\",\n        )\n", "    def test_join(self):\n        template = self.engine.from_string('{{ a|join:\", \" }}')\n\n        with self.settings(DEBUG=True):\n            self.assertEqual(template.render({'a': ['one', 'two', 'three']}), 'one, two, three')\n", "    def test_join_empty_list(self):\n        template = Template('{{ empty|join:\", \" }}')\n        rendered = template.render(Context({'empty': []}))\n        self.assertEqual(rendered, '')\n", "    def test_join_empty_list(self):\n        self.assertEqual(self.engine.render({'a': []}), '')\n", "    def test_join_with_empty_list(self):\n        self.assertEqual(self.engine.render('join01', {'a': []}), '')\n"], "sample_502": ["    def test_spy():\n        x = np.random.rand(5, 5)\n        plt.spy(x)\n        plt.savefig('tmp/spy.png')\n", "    def test_axes_set_xlabel(self):\n        fig, ax = plt.subplots()\n        ax.set_xlabel('Test Label')\n        assert ax.get_xlabel() == 'Test Label'\n", "    def test_imshow_with_data():\n        data = np.random.rand(10, 10)\n        fig, ax = plt.subplots()\n        im = ax.imshow(data, data=data)\n        assert im.get_array() is data\n", "def test_plot_data():\n    plt.plot([1, 2, 3, 4])\n    plt.xlabel('X-axis')\n    plt.ylabel('Y-axis')\n    plt.title('Simple Plot')\n\n    # Capture the plot output as an image\n    image_file = Path(__file__).parent / 'test_plot.png'\n    plt.savefig(image_file)\n\n    # Compare the generated plot with a reference image (if available)\n    reference_file = Path(__file__).parent / 'test_plot_reference.png'\n    if reference_file.exists():\n        diff = subprocess.run(\n            ['compare', '-metric', 'RMSE', str(reference_file), str(image_file)],\n            capture_output=True,\n            text=True\n        )\n        assert diff.returncode == 0, f\"Plot mismatch: {diff.stdout}\"\n    else:\n        # Save the current plot as the new reference\n        plt.savefig(reference_file)\n    plt.close()\n\n", "    def test_quiverkey_deprecation():\n        fig, ax = plt.subplots()\n        with pytest.warns(MatplotlibDeprecationWarning):\n            ax.quiverkey(None, 0, 0, 0, \"\", coordinates='data')\n\n"], "sample_474": ["    def test_chr_function(self):\n        self.assertEqual(\n            Author.objects.annotate(first_char=Chr(F(\"name\"))).get(pk=self.john.pk).first_char,\n            'J',\n        )\n        self.assertEqual(\n            Author.objects.annotate(first_char=Chr(F(\"name\"))).get(pk=self.elena.pk).first_char,\n            '\u00c9',\n        )\n        self.assertEqual(\n            Author.objects.annotate(first_char=Chr(F(\"name\"))).get(pk=self.rhonda.pk).first_char,\n            'R',\n        )\n", "    def test_chr(self):\n        with register_lookup(Chr):\n            self.assertEqual(Author.objects.filter(name__chr__startwith='E').count(), 1)\n            self.assertEqual(Author.objects.filter(name__chr__startwith=Chr(69)).count(), 1)  \n", "    def test_chr(self):\n        self.assertEqual(\n            Author.objects.filter(name__chr__startswith=Chr(65)).count(), 1\n        )\n        self.assertEqual(\n            Author.objects.filter(name__chr__startswith=Chr(225)).count(), 1\n        )\n\n", "    def test_chr(self):\n        self.assertEqual(\n            Author.objects.annotate(first_letter=Chr(F(\"name\"))).get(pk=self.john.pk).first_letter,\n            \"J\",\n        )\n        self.assertEqual(\n            Author.objects.annotate(\n                first_letter=Chr(Left(F(\"alias\"), 1))\n            ).get(pk=self.elena.pk).first_letter,\n            \"e\",\n        )\n        self.assertEqual(\n            Author.objects.annotate(first_letter=Chr(F(\"name\"))).get(pk=self.rhonda.pk).first_letter,\n            \"R\",\n        )\n\n", "    def test_chr_sqlite(self):\n        with register_lookup(Chr.lookup_name, Chr):\n            self.assertEqual(\n                Author.objects.filter(name__chr=F(\"alias\")[:1]).count(), 3\n            )\n"], "sample_1101": ["    def test_schur_partition_large_n():\n        assert schur_partition(100) == [[1, 4, 13, 22, 31, 40, 49, 58, 67, 76, 85, 94],", "    def test_schur_partition_large_n():\n        n = 50\n        partition = schur_partition(n)\n        assert len(partition) == _schur_subsets_number(n)\n\n", "def test_schur_partition_random():\n    n = _randint(5, 100)\n    partition = schur_partition(n)\n    assert all(sum(subset) not in [sum(subset2) for subset2 in partition if subset != subset2] for subset in partition)\n", "    def test_schur_partition_large_n():\n        n = 100\n        subsets = schur_partition(n)\n        assert len(subsets) == _schur_subsets_number(n)\n        for subset in subsets:\n            assert all(a + b != c for a in subset for b in subset for c in subset if a != b and a != c and b != c)\n", "def test_schur_partition_lower_bound():\n    assert len(schur_partition(364)) == _schur_subsets_number(364)\n"], "sample_1154": ["def test_linsolve_complex():\n    eqs = [Eq(2 * x + 3 * I * y, 1), Eq(x - I * y, 2)]\n    sol = _linsolve(eqs, [x, y])\n    assert sol == {x: 5/7 + 6*I/7, y: 3/7 - 4*I/7}\n", "    def test_linsolve_complex():\n        eqs = [Eq(x + I*y, 1), Eq(x - I*y, 2)]\n        sol = _linsolve(eqs, [x, y])\n        assert sol == {x: 3/2, y: -I/2}\n", "    def test_linsolve_complex():\n        eqs = [Eq(x + I*y, 1), Eq(x - I*y, 2)]\n        sol = _linsolve(eqs, [x, y])\n        assert sol == {x: 3/2, y: -I/2}\n", "    def test_linsolve_complex():\n        eqs = [Eq(x + I*y, 1 + I), Eq(x - I*y, 1 - I)]\n        sol = _linsolve(eqs, [x, y])\n        assert sol == {x: 1, y: 0}\n", "    def test_linsolve_complex():\n        eqs = [Eq(x + I*y, 1 + 2*I), Eq(x - I*y, 1 - 2*I)]\n        sol = _linsolve(eqs, [x, y])\n        assert sol == {x: 1, y: 0}\n"], "sample_931": ["    def test_parse_annotation(app, env):\n        env.temp_data['objects'] = {}\n        # Test basic types\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('str') == ('str', None)\n        assert _parse_annotation('float') == ('float', None)\n        assert _parse_annotation('bool') == ('bool', None)\n        assert _parse_annotation('None') == ('None', None)\n\n        # Test complex types\n        assert _parse_annotation('List[int]') == ('List', ['int'])\n        assert _parse_annotation('Dict[str, float]') == ('Dict', ['str', 'float'])\n        assert _parse_annotation('Tuple[int, str]') == ('Tuple', ['int', 'str'])\n\n        # Test optional types\n        assert _parse_annotation('Optional[str]') == ('Optional', ['str'])\n\n        # Test Union types\n        assert _parse_annotation('Union[int, str]') == ('Union', ['int', 'str'])\n\n        # Test complex types with generics\n        assert _parse_annotation('List[Tuple[int, str]]') == ('List', [('Tuple', ['int', 'str'])])\n", "def test_parse_annotation_typing():\n    assert _parse_annotation(\"int\") == nodes.literal_block(\"int\", \"\")\n    assert _parse_annotation(\"typing.List[int]\") == nodes.literal_block(\"List[int]\", \"\")\n    assert _parse_annotation(\"typing.Optional[str]\") == nodes.literal_block(\"Optional[str]\", \"\")\n    assert _parse_annotation(\"typing.Union[float, int]\") == nodes.literal_block(\"Union[float, int]\", \"\")\n    assert _parse_annotation(\"dict\") == nodes.literal_block(\"dict\", \"\")\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('list[str]') == ('list', ('str',))\n    assert _parse_annotation('dict[str, int]') == ('dict', ('str', 'int'))\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', ('int', Ellipsis))\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', ((int,), 'str'))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n    assert _parse_annotation('Optional[int]') == ('Optional', ('int',))\n    assert _parse_annotation('Literal[\"a\", \"b\"]') == ('Literal', ('a', 'b'))\n\n\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('foo') == ('foo', '')\n        assert _parse_annotation('foo: bar') == ('foo', 'bar')\n        assert _parse_annotation('foo:bar') == ('foo:bar', '')\n        assert _parse_annotation(': bar') == ('', 'bar')\n", "def test_parse_annotation():\n    assert _parse_annotation(\"int\") == (\"int\", None)\n    assert _parse_annotation(\"str\") == (\"str\", None)\n    assert _parse_annotation(\"List[int]\") == (\"List\", (\"int\",))\n    assert _parse_annotation(\"Dict[str, int]\") == (\"Dict\", (\"str\", \"int\"))\n    assert _parse_annotation(\"A.B\") == (\"A.B\", None)\n    assert _parse_annotation(\"typing.Union[A, B]\") == (\"Union\", (\"A\", \"B\"))\n    assert _parse_annotation(\"Optional[int]\") == (\"Optional\", (\"int\",))\n    assert _parse_annotation(\"bool = True\") == (\"bool\", True)\n\n"], "sample_27": ["    def test_nonstandard_hdu_diff(self, tmpdir, data_type):\n        \"\"\"\n        Test diffing of non-standard extensions.\n        \"\"\"\n        data = np.arange(100, dtype=data_type)\n        hdu1 = DummyNonstandardExtHDU(data=data, name='EXT1')\n\n        hdu2 = DummyNonstandardExtHDU(data=data + 1, name='EXT1')\n\n        hdulist1 = HDUList([PrimaryHDU()])\n        hdulist1.append(hdu1)\n        hdulist2 = HDUList([PrimaryHDU()])\n        hdulist2.append(hdu2)\n\n        diff = FITSDiff(hdulist1, hdulist2)\n        assert diff.diff_extension_names == ('EXT1',)\n        assert diff.extension_diffs['EXT1'].diff_total == 100\n", "def test_diff_tables_ignore_fields(self):\n    hdula = Header()\n    hduab = Header()\n    hdula['NAME'] = 'TABLE1'\n    hduab['NAME'] = 'TABLE1'\n\n    tb1 = BinTableHDU.from_columns([Column('A', 'I', array=[1, 2, 3]), Column('B', 'I', array=[4, 5, 6])], header=hdula)\n    tb2 = BinTableHDU.from_columns([Column('A', 'I', array=[11, 12, 13]), Column('B', 'I', array=[14, 15, 16])], header=hduab)\n\n    # Test differences are found when ignore_fields contains no fields\n    diff = TableDataDiff(tb1, tb2)\n    assert diff.diff_values\n    assert len(diff.diff_values) == 6\n\n    # Test no differences are found when ignore_fields contains both fields\n    diff = TableDataDiff(tb1, tb2, ignore_fields=['a', 'b'])\n    assert not diff.diff_values\n\n    # Test no differences are found when ignore_fields contains one field\n    diff = TableDataDiff(tb1, tb2, ignore_fields=['a'])\n    assert len(diff.diff_values) == 3\n    \n", "    def test_fitsdiff_ignore_fields(self):\n        hdulist1 = fits.HDUList([\n            fits.PrimaryHDU(header={'TEST1': 1}),\n            fits.BinTableHDU.from_columns([\n                fits.Column(name='ID', format='I', array=[1, 2, 3]),\n                fits.Column(name='VALUE', format='E', array=[1.1, 2.2, 3.3]),\n            ])\n        ])\n        hdulist2 = fits.HDUList([\n            fits.PrimaryHDU(header={'TEST1': 1}),\n            fits.BinTableHDU.from_columns([\n                fits.Column(name='ID', format='I', array=[1, 2, 3]),\n                fits.Column(name='VALUE', format='E', array=[1.0, 2.0, 3.2]),\n            ])\n        ])\n\n        diff = FITSDiff(hdulist1, hdulist2, ignore_fields=['VALUE'])\n        diff.report()\n        assert diff.diff_values == []\n\n\n", "    def test_diff_empty_tables(self):\n        a = fits.HDUList([fits.PrimaryHDU()])\n        b = fits.HDUList([fits.PrimaryHDU()])\n\n        a.append(BinTableHDU.from_columns([Column('a', 'I', len=0)]))\n        b.append(BinTableHDU.from_columns([Column('a', 'I', len=0)]))\n\n        diff = FITSDiff(a, b, ignore_comments=True)\n\n        assert diff.table_diffs is None\n        assert diff.common_tables == []\n        assert diff.diff_tables == []\n\n", "    def test_diff_duplicate_keywords_ignore_comments(self):\n        a = PrimaryHDU()\n        a.header[\"FOO\"] = 1\n        a.header[\"FOO\"] = 2\n        a.header.comments[\"FOO\"] = [\n            \"Comment 1\",\n            \"Comment 2\",\n        ]\n        b = PrimaryHDU()\n        b.header[\"FOO\"] = 1\n        b.header[\"FOO\"] = 3\n        b.header.comments[\"FOO\"] = [\"Comment 1\"]\n\n        diff = FITSDiff(a, b, ignore_comments=True)\n\n        assert diff._diff is None\n        assert diff.diff_keywords == ({'FOO'}, set())\n\n"], "sample_885": ["    def test_interval_constraint(self, interval_type, left, right, closed):\n        constraint = Interval(interval_type, left, right, closed=closed)\n        assert constraint.is_satisfied_by(\n            left + 1\n            if left is not None\n            else right - 1\n        )\n        assert not constraint.is_satisfied_by(\n            left - 1\n            if left is not None\n            else right + 1\n        )\n", "    def test_interval_left_closed_right_open(self, interval_type):\n        interval = Interval(interval_type, 0, 5, closed=\"left\")\n        assert interval.is_satisfied_by(0)\n        assert interval.is_satisfied_by(4.9)\n        assert not interval.is_satisfied_by(5)\n        assert not interval.is_satisfied_by(-1)\n", "    def test_interval_constraint(self, interval_type, closed):\n        interval = Interval(interval_type, 0, 5, closed=closed)\n        for value in [0, 5]:\n            if closed in (\"left\", \"both\") and value == 0:\n                assert interval.is_satisfied_by(value)\n            elif closed in (\"right\", \"both\") and value == 5:\n                assert interval.is_satisfied_by(value)\n            else:\n                assert not interval.is_satisfied_by(value)\n\n", "    def test_interval_constraint_valid_values(self, interval_type, closed):\n        interval = Interval(interval_type, 1, 5, closed=closed)\n        for value in [1, 2, 3, 4, 5]:\n            assert interval.is_satisfied_by(value)\n\n        if interval_type is Integral:\n            assert not interval.is_satisfied_by(2.5)\n        else:\n            assert interval.is_satisfied_by(2.5)\n\n        if closed == \"left\":\n            assert not interval.is_satisfied_by(0)\n        if closed == \"right\":\n            assert not interval.is_satisfied_by(6)\n        if closed == \"neither\":\n            assert not interval.is_satisfied_by(1)\n            assert not interval.is_satisfied_by(5)\n", "    def test_interval(self, interval_type, closed):\n        interval = Interval(interval_type, -1, 1, closed=closed)\n        if interval_type is Integral:\n            valid_vals = [-1, 0, 1]\n        else:\n            valid_vals = [-1.0, -0.5, 0.0, 0.5, 1.0]\n        for val in valid_vals:\n            assert interval.is_satisfied_by(val)\n\n        invalid_vals = []\n        if interval_type is Integral:\n            invalid_vals.extend([-2, 2])\n        else:\n            invalid_vals.extend([-1.5, 1.5])\n        if closed in (\"left\", \"both\"):\n            invalid_vals.append(-1)\n        if closed in (\"right\", \"both\"):\n            invalid_vals.append(1)\n\n        for val in invalid_vals:\n            assert not interval.is_satisfied_by(val)\n"], "sample_317": ["    def test_atom_feed_with_enclosures(self):\n        site = Site.objects.get_current()\n        feed = Atom1Feed(\n            title=\"My Test Feed\",\n            link=site.domain,\n            description=\"A test feed\",\n            feed_url='{}/feed/atom/'.format(site.domain),\n        )\n        feed.add_item(\n            title=\"My entry with enclosure\",\n            link=\"http://example.com/entry/\",\n            description=\"This entry has an enclosure.\",\n            pubdate=timezone.now(),\n            enclosures=[Enclosure(\n                url=\"http://example.com/audio.mp3\",\n                length=\"12345\",\n                mime_type=\"audio/mpeg\",\n            )],\n        )\n\n        xml = feed.writeString('utf-8')\n        doc = minidom.parseString(xml)\n        entry = doc.getElementsByTagName('entry')[0]\n        self.assertChildNodes(entry, ['title', 'link', 'id',\n                                    'published', 'updated', 'author',\n                                    'content', 'link'])\n        self.assertChildNodeContent(entry, {'title': 'My entry with enclosure'})\n        enclosure = entry.getElementsByTagName('link')[1]\n        self.assertEqual(enclosure.attributes['rel'].value, 'enclosure')\n        self.assertEqual(enclosure.attributes['href'].value,\n                         'http://example.com/audio.mp3')\n        self.assertEqual(enclosure.attributes['length'].value, '12345')\n        self.assertEqual(enclosure.attributes['type'].value, 'audio/mpeg')\n\n", "    def test_atom_feed(self):\n        feed = Atom1Feed(\n            title=\"Test Feed\",\n            link=\"http://example.com/feed/\",\n            description=\"A sample feed\",\n            feed_url=\"http://example.com/feed/atom/\",\n        )\n        feed.add_item(\n            title=\"My first entry\",\n            link=\"http://example.com/entry/1/\",\n            description=\"This is the first entry.\",\n            pubdate=datetime.datetime(2023, 10, 26, 10, 0, 0, tzinfo=TZ),\n            author_name=\"John Doe\",\n            author_email=\"john@example.com\",\n        )\n        xml_str = feed.writeString('utf-8')\n        doc = minidom.parseString(xml_str)\n        feed_elem = doc.firstChild\n        self.assertEqual(feed_elem.nodeName, 'feed')\n        self.assertChildNodes(feed_elem, ['title', 'link', 'id', 'updated', 'author', 'entry'])\n        self.assertChildNodeContent(feed_elem, {\n            'title': 'Test Feed',\n            'link': {'href': 'http://example.com/feed/', 'rel': 'alternate'},\n            'id': None,\n            'updated': rfc3339_date(datetime.datetime(2023, 10, 26, 10, 0, 0, tzinfo=TZ)),\n            'author': [\n              'name': 'John Doe',\n              'email': 'john@example.com',\n            ]})\n\n        entry_elem = feed_elem.getElementsByTagName('entry')[0]\n        self.assertChildNodes(entry_elem, ['title', 'link', 'updated', 'author', 'id', 'summary'])\n        self.assertChildNodeContent(\n            entry_elem, {\n                'title': 'My first entry',\n                'link': {'href': 'http://example.com/entry/1/', 'rel': 'alternate'},\n                'updated': rfc3339_date(datetime.datetime(2023, 10, 26, 10,", "    def test_rss201rev2_feed(self):\n        feed = views.latest_entries_rss201rev2(Site.objects.get_current())\n        # Test if the feed has the correct number of entries.\n        self.assertEqual(len(feed.items), 5)\n        self.assertEqual(feed.feed['title'], 'Django Project')\n        self.assertEqual(feed.feed['description'], 'Sample Django project')\n        # Test the individual entry fields.\n        self.assertEqual(feed.items[0]['title'], 'My first entry')\n        self.assertEqual(feed.items[0]['description'], None)\n\n        # Test if the link and guid are correct.\n        self.assertEqual(feed.items[0]['link'], f'http://testserver/{self.e1.pk}/')\n        self.assertEqual(feed.items[0]['unique_id'], str(self.e1.pk))\n\n        # Test date formatting in the feed.\n        self.assertEqual(\n            rfc2822_date(self.e1.published),\n            feed.items[0]['pubDate']\n        )\n        self.assertEqual(\n            rfc2822_date(self.e5.updated), feed.items[4]['updateddate']\n        )\n\n\n\n", "    def test_rss201rev2_feed(self):\n\n        site = Site.objects.get(pk=1)\n        feed = views.latest_entries_feed(site)\n        self.assertIsInstance(feed, Rss201rev2Feed)\n\n        # Check feed metadata\n        self.assertEqual(feed.feed['title'], 'My Site Title')\n        self.assertEqual(feed.feed['link'], site.domain)\n        self.assertEqual(feed.feed['description'], 'My Site Description')\n        self.assertEqual(feed.feed['language'], 'en')\n        self.assertEqual(feed.feed['feed_url'], None)\n        self.assertEqual(feed.feed['ttl'], None)\n\n        # Check the last updated date\n        updated_date = timezone.make_aware(datetime.datetime(2013, 1, 20, 0, 0), TZ)\n        self.assertEqual(\n            rfc2822_date(feed.latest_post_date()),\n            rfc2822_date(updated_date)\n        )\n\n        # Check the items\n        items = feed.items\n        self.assertEqual(len(items), 5)\n\n        self.assertEqual(items[0]['title'], 'My last entry')\n        self.assertEqual(items[0]['link'], f'{site.domain}/articles/{self.e5.id}/')\n        self.assertEqual(items[0]['description'], None)\n        self.assertEqual(\n            rfc2822_date(items[0]['pubdate']),\n            rfc2822_date(self.e5.published)\n\n", "    def test_feed_with_custom_feed(self):\n        feed_url = '/custom_feed/'\n        response = self.client.get(feed_url)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response['Content-Type'], 'application/rss+xml; charset=utf-8')\n        feed = minidom.parseString(response.content)\n        self.assertEqual(feed.documentElement.nodeName, 'rss')\n        self.assertEqual(feed.documentElement.getAttribute('version'), '2.0')\n        self.assertChildNodes(feed.documentElement, ['channel'])\n\n\n"], "sample_592": ["    def test_summarize_coord_levels(self):\n        coord = xr.DataArray(\n            np.arange(6).reshape(2, 3), dims=(\"x\", \"level\"), coords={\"level\": [\"a\", \"b\", \"c\"]}\n        ).to_index_variable()\n        expected = dedent(\n            \"\"\"\n            MultiIndex\n            x  level\n            0  a       0\n               b       1\n               c       2\n            1  a       3\n               b       4\n               c       5\n        \"\"\"\n        )\n        assert formatting.coords_repr({coord.name: coord}, col_width=8) == expected\n", "    def test_summarize_variable_with_sparse_data(self):\n        data = np.arange(100).reshape((10, 10))\n        sparse_data = sparse.COO(data)\n        da = xr.DataArray(sparse_data)\n        expected = dedent(\n            \"\"\"\n              *var1 (10, 10): 100 values with dtype=int64\n                <sparse.COO: nnz=100, fill_value=0>\n            \"\"\"\n        )\n        actual = formatting.summarize_variable(\"var1\", da, col_width=15)\n        assert actual == expected\n", "    def test_summarize_variable_indexers(self):\n        ds = xr.Dataset(\n            {\n                \"foo\": (\n                    (\"x\", \"y\"),\n                    np.arange(6).reshape(2, 3),\n                )\n            },\n            coords={\"x\": [0, 1], \"y\": [0, 1, 2]},\n        )\n        var = ds[\"foo\"]\n\n        # Test with default behavior\n        summary = formatting.summarize_variable(\"foo\", var, 15)\n        assert summary == \"  foo  (x, y) float64 0 1 2 3 4 5\"\n\n        # Test with ellipsis when values are too long\n        summary = formatting.summarize_variable(\n            \"foo\", var, 15, max_width=10\n        )\n        assert summary == \"  foo  (x, y) float64 0 ... 5\"\n\n", "    def test_short_numpy_repr(self, array, expected):\n        assert formatting.short_numpy_repr(array) == expected\n", "    def test_summarize_coord_levels(self):\n        coord = xr.IndexVariable('level1', ['a', 'b', 'c'])\n        da = xr.DataArray(np.arange(6), dims=['level1'], coords={'level1': coord})\n        ds = xr.Dataset({'foo': da.rename('foo')})\n        expected = dedent(\"\"\"\n        Coordinates:\n          * level1 (level1) <U1 'a' 'b' 'c'\n        Data variables:\n          foo (level1) int64 0 1 2 3 4 5\n        \"\"\")\n        assert formatting.coords_repr(ds.coords) == expected\n"], "sample_714": ["    def test_zero_one_loss_multilabel():\n        y_true = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0]])\n        y_pred = np.array([[1, 1, 0], [0, 0, 1], [0, 1, 1]])\n\n        # test with binary labels\n        assert_equals(zero_one_loss(y_true, y_pred), 2. / 3)\n\n", "def test_classification_report_multilabel_average_weighted():\n    y_true = np.array([[0, 1, 1], [1, 0, 0], [1, 1, 0]])\n    y_pred = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 1]])\n    report = classification_report(y_true, y_pred, average='weighted')\n    expected_report = \"\"\"              precision    recall  f1-score   support\n\n           0       0.50      0.67      0.57         2\n           1       0.67      0.50      0.57         2\n           2       0.33      0.33      0.33         2\n\n    accuracy                           0.50         6\n   macro avg       0.50      0.50      0.50         6", "    def test_brier_score_loss_invalid_probabilities():\n        y_true = np.array([0, 1, 1, 0])\n        y_prob = np.array([1.2, 0.9, 0.8, -0.3])\n        with assert_raises(ValueError):\n            brier_score_loss(y_true, y_prob)\n\n", "    def test_hinge_loss_multiclass_labels():\n        # Test hinge_loss with multiclass problem and explicit labels\n\n        y_true = np.array([0, 1, 2, 3])\n        pred_decision = np.array([[0.1, -0.2, 0.3],\n                                  [-0.3, 0.5, -0.1],\n                                  [0.2, -0.1, 0.4],\n                                  [-0.1, 0.4, 0.3]])\n        labels = np.array([0, 1, 2, 3])\n        loss = hinge_loss(y_true, pred_decision, labels)\n        assert_almost_equal(loss, 0.8)\n\n", "    def test_brier_score_loss_multilabel():\n        # test brier score loss for multilabel classification\n        X, y = make_multilabel_classification(n_samples=20,\n                                              n_features=10, n_classes=3,\n                                              random_state=123)\n        clf = DummyClassifier(strategy=\"most_frequent\")\n        clf.fit(X, y)\n        y_pred = clf.predict_proba(X)\n\n        score = brier_score_loss(y, y_pred)\n        assert_almost_equal(score, 0.5)\n\n"], "sample_773": ["    def test_logistic_regression_cv_sample_weights():\n        # Test that LogisticRegressionCV handles sample weights correctly.\n        X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n        sample_weight = np.random.rand(len(y))\n\n        # Fit with and without sample weights\n        clf1 = LogisticRegressionCV(cv=5, random_state=42).fit(X, y)\n        clf2 = LogisticRegressionCV(cv=5, random_state=42).fit(X, y,\n                                                                  sample_weight=sample_weight)\n\n        # Check that coefficients are different for different weighting\n        assert_True(not np.allclose(clf1.coef_, clf2.coef_))\n\n", "    def test_logistic_regression_cv_multi_class_ovo(self):\n        # Test LogisticRegressionCV with multi_class='ovo'\n        X, y = make_classification(n_samples=100, n_features=20,\n                                   random_state=42, n_classes=3)\n        clf = LogisticRegressionCV(Cs=10, cv=5,\n                                   multi_class='ovo',\n                                   random_state=42).fit(X, y)\n        check_predictions(clf, X, y)\n        assert_equal(clf.classes_.shape[0], 3)\n", "    def test_logistic_regression_multiclass_fit_intercept_sparse(self):\n        # Test that sparse input works with multi_class='multinomial'\n        # and fit_intercept=False\n\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   n_informative=5, random_state=0)\n        X = sp.csr_matrix(X)\n        clf = LogisticRegression(multi_class='multinomial',\n                                 penalty='l2', solver='lbfgs',\n                                 fit_intercept=False)\n        check_predictions(clf, X, y)\n", "    def test_logistic_regression_cv_empty_cv_splitter():\n        # Test that LogisticRegressionCV raises an error if cv is empty\n        X, y = iris.data, iris.target\n        clf = LogisticRegressionCV()\n        assert_raises(ValueError, clf.fit, X, y, cv=[])\n", "def test_logistic_regression_cv_predict_proba():\n    # check if predict_proba works when using LogisticRegressionCV\n    X, y = make_classification(n_samples=100, n_features=10, random_state=42)\n    clf = LogisticRegressionCV(cv=5, random_state=42, multi_class='ovr')\n    clf.fit(X, y)\n    proba = clf.predict_proba(X)\n    assert proba.shape == (100, clf.classes_.size)\n    assert_almost_equal(proba.sum(axis=1), np.ones(100),\n                        err_msg=\"Probabilities do not sum to 1\")\n"], "sample_904": ["def test_resolve_numfig_xref_when_numfig_is_disabled(app, env, warn_log):\n    env.config.numfig = False\n\n    app.builder.build_all()\n    docname = 'dummy'\n    # Create a figure\n    env.get_doctree(docname).append(nodes.figure())\n    env.toc_fignumbers[docname] = {'figure': {}}\n\n    # Create a reference to the figure\n    source = ':numref:`my-figure`'\n    node = restructuredtext.parse(source, app.builder.env)\n    domain = StandardDomain(env)\n\n    assert_node(domain.resolve_xref(env, docname, app.builder, 'numref', 'my-figure',\n                                   node, node),\n                [addnodes.number_reference,\n                 {'refdocname': docname,\n                  'refexplicit': False,\n                  'target': 'my-figure'}]\n               )\n    assert not warn_log.getvalue()\n\n", "    def test_resolve_numref_xref_with_nested_section(\n            self, app: \"Sphinx\", env: \"BuildEnvironment\"):\n        app.build()\n        docname = 'index'\n        env.titles[docname] = 'Index'\n        env.doc2path(docname) = 'index.rst'\n\n        # Create a nested section structure\n        nested_section = nodes.section(ids=['nested-section'],\n                                       names=['nested-section'],\n                                       title='Nested Section')\n        nested_section += nodes.paragraph('', 'Content of nested section')\n        section = nodes.section(ids=['section'], names=['section'],\n                                title='Section',\n                                content=[nested_section])\n        env.all_docs[docname] = nodes.document(\n            ids=[docname],\n            title='Index',\n            content=[section],\n        )\n\n        # Create a pending xref node referencing the nested section\n        node = pending_xref('', 'nested-section', '', '', reftype='numref')\n        node.line = 1\n\n        domain = app.env.get_domain('std')  \n\n        result = domain.resolve_xref(env, docname, app.builder,\n                                     'numref', 'nested-section', node, node)\n        assert result is not None\n\n", "    def test_resolve_numref_xref_with_caption(self, app, status, warning):\n        app.build()\n        docname = 'index'\n        inv = app.env.domaindata['std']['labels']\n\n        # Check if numref is correctly resolved\n        node = addnodes.pending_xref('[NumRef]', refdomain='std', reftarget='my-figure',\n                                    reftype='numref', lineno=1)\n        result = StandardDomain.resolve_xref(app.env, docname, app.builder,\n                'numref', 'my-figure', node, node)\n        assert isinstance(result, nodes.reference)\n        assert result['refuri'] == '#my-figure'\n        assert result[0].astext() == 'Figure 1'\n", "    def test_resolve_numref_xref_with_caption(self, app, env):\n        app.builder.env = env\n        env.config.numfig = True\n        env.config.numfig_format = {'figure': '{figure}.'}\n        docname = 'test'\n        env.add_document(docname, '')\n        env.titles[docname] = 'Title'\n        fig = nodes.figure()\n        fig['ids'] = ['fig-1']\n        caption = nodes.caption('Figure 1 Caption', '')\n        fig.append(caption)\n        env.get_doctree(docname).append(fig)\n\n        node = nodes.target('', '', ids=['fig-1'])\n        refnode = StandardDomain.resolve_numref_xref(\n            env, docname, app.builder, 'numref', 'fig-1',\n            pending_xref('', 'fig-1', refdoc=docname, reftarget='fig-1'), node\n        )\n        assert refnode is not None\n        assert isinstance(refnode, addnodes.number_reference)\n        assert refnode.astext() == 'Figure 1.'\n\n", "def test_resolve_option_xref_progname():\n    env = mock.Mock()\n    env.progoptions = {('program-name', 'option'): ('docname', 'labelid')}\n    std_domain = StandardDomain(env)\n    node = pending_xref('', '', node_id='uniqueid',\n                        reftype='option', reftarget='option',\n                        std_program='program-name')\n    res = std_domain._resolve_option_xref(env, 'docname', None,\n                                      'option', 'option',\n                                      node, nodes.inline())\n\n    assert_node(res,\n                addnodes.ref(docname='docname',\n                             refid='labelid',\n                             refuri='internal://docname/#labelid',\n                             target='')\n                )\n"], "sample_779": ["    def test_check_estimators_unfitted_not_fitted_error(self):\n        # Test if check_estimators_unfitted raises an error when predict\n        # is called before fit on estimators that don't raise an error\n        # when not fitted\n\n        for estimator_class in [\n                        BaseBadClassifier,\n                        ChangesDict,\n                        NoCheckinPredict\n                        ]:\n            estimator = estimator_class()\n\n            with pytest.raises(ValueError):\n                check_estimators_unfitted(estimator_class)\n\n\n", "    def test_check_sparse_matrix_accept_large_sparse(self):\n        # Test that estimators can handle large sparse matrices\n        #  check_sparse_matrix should raise a ValueError if\n        #  accept_large_sparse is False and the matrix is too large\n        X = sp.csc_matrix((np.arange(10000), (np.arange(10000),\n                                            np.arange(10000))),\n                          shape=(10000, 10000))\n        estimator = LargeSparseNotSupportedClassifier()\n\n        # Test that the estimator raises a ValueError if\n        # accept_large_sparse is False\n        msg = \"Estimator doesn't support 64-bit indices\"\n        with pytest.raises(ValueError, match=msg):\n            estimator.fit(X, np.ones(10000))\n\n", "    def test_check_estimator_sparse_not_supported(self):\n        # Test that estimators can signal they don't support sparse input\n        # by raising a ValueError in fit.\n\n        estimator = LargeSparseNotSupportedClassifier()\n\n        X = sp.csr_matrix([[0, 1], [1, 0]])\n        y = np.array([0, 1])\n        assert_raises(ValueError, estimator.fit, X, y)\n\n", "    def __init__(self, parameter_with_error):\n        raise ValueError(\"This error should be caught.\")\n", "    def test_check_estimator_sparse_transform_raises(self):\n        # Test that check_estimator raises an error when transforming sparse\n        # data if the estimator doesn't support it\n        X = sp.csr_matrix([[0, 1], [1, 0]])\n        y = np.array([0, 1])\n        estimator = LargeSparseNotSupportedClassifier()\n\n        assert_raises(ValueError, check_estimator, estimator)\n\n\n\n"], "sample_232": ["    def test_jsonfield_key_transform_isnull_true(self):\n        model = JSONModel.objects.create(data={'key': 'value'})\n        results = JSONModel.objects.filter(data__key__isnull=True)\n        self.assertQuerysetEqual(results, [])\n        results = JSONModel.objects.filter(data__nonexistent_key__isnull=True)\n        self.assertEqual(results.count(), 1)\n", "    def test_key_transform_lookup_with_subquery(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO myapp_jsonmodel (data) VALUES (%s)\", [\n                json.dumps({'key': 'value', 'nested': {'inner_key': 'inner_value'}}),\n            ])\n        obj = JSONModel.objects.get(pk=1)\n        subquery = JSONModel.objects.filter(data__key='value').values('pk')\n        qs = JSONModel.objects.filter(pk__in=Subquery(subquery))\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.first(), obj)\n        qs = JSONModel.objects.filter(data__nested__inner_key='inner_value').filter(pk__in=Subquery(subquery))\n        self.assertEqual(qs.count(), 1)\n        self.assertEqual(qs.first(), obj)\n", "    def test_key_transform_in_subquery(self):\n        with self.assertNumQueries(1):\n           qs = JSONModel.objects.annotate(\n               num_items=Count(\n                   Subquery(\n                       JSONModel.objects.filter(data__contained_by=F('data')).values('pk')\n                   )\n               )\n           ).filter(num_items__gt=0)\n           self.assertQuerysetEqual(qs, JSONModel.objects.filter(data__contained_by=F('data')))\n\n", "    def test_key_transform_sqlite(self):\n        with connection.cursor() as cursor:\n            cursor.execute('PRAGMA foreign_keys=OFF')\n        json_data = {'name': 'Alice', 'age': 30, 'city': 'New York'}\n        JsonModel.objects.create(data=json_data)\n        obj = JsonModel.objects.get(data__name='Alice')\n\n        # Using KeyTransform with SQLite\n        self.assertEqual(obj.data['name'], 'Alice')\n        self.assertEqual(obj.data.get('age'), 30)\n        self.assertEqual(obj.data.get('city'), 'New York')\n        with self.assertRaises(KeyError):\n            obj.data['non_existent_key']\n\n        # Test KeyTransform with Subquery\n        subquery = JsonModel.objects.filter(data__name='Alice').values('data')\n        self.assertEqual(JsonModel.objects.annotate(age=Subquery(subquery.values('data__age')[0])).get(\n            data__name='Alice').age, 30\n        )\n        # Test KeyTransform with Q object\n        self.assertTrue(JsonModel.objects.filter(Q(data__name='Alice')).exists())\n        with self.assertRaises(OperationalError):\n            JsonModel.objects.filter(Q(data__age__gt=35))\n\n        with connection.cursor() as cursor:\n            cursor.execute('PRAGMA foreign_keys=ON')\n\n\n\n", "    def test_key_transform_regex_lookup_case_insensitive(self):\n        with self.assertNumQueries(1):\n            self.assertEqual(\n                JSONModel.objects.filter(data__description__iregex=r'test').count(),\n                1,\n            )\n\n"], "sample_235": ["    def test_on_commit_rollback(self):\n        with transaction.atomic():\n            self.do(1)\n            transaction.rollback()\n            self.assertNotified([])\n            self.assertEqual(Thing.objects.count(), 0)\n", "    def test_on_commit_autocommit_mode(self):\n        self.do(1)\n        self.do(2)\n        self.assertDone([1, 2])\n        self.assertEqual(connection.get_autocommit(), True)\n", "    def test_on_commit_failure(self):\n        \"\"\"\n        Test that on_commit callbacks are not executed if a rollback occurs.\n        \"\"\"\n        with self.assertRaises(ForcedError):\n            with transaction.atomic():\n                self.do(1)\n                self.do('error')\n                self.do(2)\n        self.assertNotified([])\n", "    def test_on_commit_rollbacks(self):\n        with transaction.atomic():\n            self.do(1)\n            self.do(2)\n            # rollback before commit\n            raise transaction.TransactionRollbackError\n\n        self.assertNotified([])\n        self.assertEqual([], list(Thing.objects.all()))\n\n", "    def test_on_commit_multiple_transactions(self):\n        with transaction.atomic():\n            self.do(1)\n        with transaction.atomic():\n            self.do(2)\n\n        self.assertDone([1, 2])\n"], "sample_837": ["    def test_show_versions(capsys):\n        show_versions()\n        captured = capsys.readouterr()\n        assert \"System:\" in captured.out\n        assert \"BLAS:\" in captured.out\n        assert \"Python deps:\" in captured.out\n\n", "    def test_get_blas_info():\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        # Check for presence of expected keys\n        assert 'macros' in blas_info\n        assert 'lib_dirs' in blas_info\n        assert 'cblas_libs' in blas_info\n", "    def test_get_blas_info(mocker):\n        mocker.patch('sklearn.utils._show_versions._build_utils',\n                     new_callable=mocker.MagicMock)\n        \n        from sklearn.utils._show_versions import _get_blas_info\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        assert all(key in blas_info for key in ('macros', 'lib_dirs', 'cblas_libs'))\n", "    def test_get_blas_info():\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        for key in ['macros', 'lib_dirs', 'cblas_libs']:\n            assert key in blas_info\n", "    def test_get_blas_info(self):\n        blas_info = _get_blas_info()\n        assert isinstance(blas_info, dict)\n        for key in ['macros', 'lib_dirs', 'cblas_libs']:\n            assert key in blas_info\n"], "sample_1129": ["    def test_MatrixSolve_scipy(self):\n        n = symbols('n', integer=True)\n        A = MatrixSymbol('A', n, n)\n        b = MatrixSymbol('b', n, 1)\n        code = SciPyPrinter().doprint(MatrixSolve(A, b))\n        assert code == 'scipy.linalg.solve(A, b)'\n", "    def test_print_ImmutableSparseMatrix(self):\n        a = SparseMatrix([[1, 2], [0, 3]])\n        x = MatrixSymbol('x', 2, 2)\n        code = SciPyPrinter().doprint(a)\n        assert code == 'scipy.sparse.coo_matrix((array([1, 2, 3]), (array([0, 0, 1]), array([0, 1, 1]))), shape=(2, 2))'\n        code = SciPyPrinter().doprint(x)\n        assert code == 'x'\n\n", "    def test_scipy_special_bessel(self):\n        from scipy.special import jv as besselj\n        self._test_function(besselj(2, x),\n                           SciPyPrinter, expected=\"scipy.special.jv(2, x)\")\n", "    def test_print_MatrixSolve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        x = MatrixSymbol('x', 2, 1)\n        eq = Eq(A * x, b)\n        sol = Solve(eq, x)\n        code = SciPyPrinter().doprint(sol[0].rhs)\n        assert code == \"scipy.linalg.solve(A, b)\"\n\n", "def test_print_piecewise_with_zero_branch():\n    expr = Piecewise((1, Gt(x, 0)), (0, True))\n    assert pycode(expr, function_name=\"my_piecewise\") == \"\"\"\\"], "sample_1156": ["    def test_acoth_rewrite_as_log():\n        x = Symbol('x')\n        assert acoth(x)._eval_rewrite_as_log() == (log(1 + 1/x) - log(1 - 1/x)) / 2\n", "def test_acoth_rewrite():\n    x = Symbol('x')\n    assert acoth(x).rewrite(log) == (log(1 + 1/x) - log(1 - 1/x))/2\n", "def test_acoth_eval():\n    x = Symbol('x')\n    assert acoth(oo) == oo\n    assert acoth(-oo) == -oo\n    assert acoth(nan).is_nan == True\n\n", "    def test_acsch_rewrite():\n        x = Symbol('x')\n        assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n        assert acsch(1).rewrite(log) == log(1 + sqrt(2))\n", "def test_acsch_rewrite():\n    x = Symbol('x')\n    assert acsch(x)._eval_rewrite_as_log(x) == log(1/x + sqrt(1/x**2 + 1))\n"], "sample_613": ["    def test_groupby_reduce_with_empty_groups(self):\n        ds = Dataset({\"a\": (\"x\", [1, 2, 3]), \"b\": ((\"x\", \"y\"), [[1, 2], [3, 4], [5, 6]])})\n        ds = ds.where(ds[\"a\"] < 3)\n        grouped = ds.groupby(\"x\")\n        result = grouped.sum()\n        expected = Dataset({\"a\": (\"x\", [1, 2]), \"b\": ((\"x\", \"y\"), [[1, 2], [3, 4]])})\n        assert_identical(result, expected)\n\n", "   def test_groupby_map_with_new_coord(array):\n        result = array.groupby(\"x\").map(\n            lambda x: x.assign_coords(new=(\"x\", np.arange(len(x.x))*10))\n        )\n\n        expected = create_test_data(\n            {\"x\": [\"a\", \"b\", \"c\"], \"y\": [1, 2, 3, 4], \"z\": [1, 2]},\n            {\"foo\": ((\"x\", \"y\", \"z\"), np.random.randn(3, 4, 2))},\n        )\n        expected[\"foo\"] = expected[\"foo\"].assign_coords(new=(\"x\", [0, 10, 20]))\n        assert_identical(result, expected[\"foo\"])\n", "def test_groupby_reduce_with_empty_groups(dataset):\n    ds = dataset.isel(x=[0, 2])  # Create empty group for 'b'\n    grouped = ds.groupby(\"x\")\n\n    # Test reduce with mean\n    actual = grouped.reduce(np.mean, dim=\"y\")\n\n    expected = ds.groupby(\"x\").mean(\"y\")\n    assert_identical(actual, expected)\n\n    # Test reduce with sum\n    actual = grouped.reduce(np.sum, dim=\"z\")\n    expected = ds.groupby(\"x\").sum(\"z\")\n    assert_identical(actual, expected)\n", "def test_groupby_apply_with_new_dims():\n    ds = xr.Dataset({\"a\": ((\"dim1\", \"dim2\"), np.arange(6).reshape(2, 3))})\n    ds[\"b\"] = ds[\"a\"] * 2\n    grouped = ds.groupby(\"dim1\")\n\n        ds[\"c\"] = ds[\"a\"] + ds[\"b\"]\n        return ds\n\n    expected = grouped.apply(func)\n    assert_identical(expected, ds)\n", "def test_groupby_reduce_multiple_dims(dataset):\n    ds = dataset.astype(\"float64\")\n    grouped = ds.groupby(\"z\")\n    actual = grouped.sum(dim=[\"x\", \"y\"])\n    expected = dataset.sum(dim=[\"x\", \"y\"]).groupby(\"z\")\n    assert_identical(actual, expected)\n"], "sample_711": ["def test_iterparentnodeids(pytester: Pytester) -> None:\n    config = pytester.parse_args([])\n    nodeid = \"a/b/c::D::eee\"\n    item = nodes.Item.from_parent(None, config=config, nodeid=nodeid)\n    expected_parent_nodeids = [\n        \"\",\n        \"a\",\n        \"a/b\",\n        \"a/b/c\",\n        \"a/b/c::D\",\n        \"a/b/c::D::eee\",\n    ]\n    assert list(nodes.iterparentnodeids(item.nodeid)) == expected_parent_nodeids\n", "    def test_iterparentnodeids(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            class TestClass:\n            \"\"\"\n        )\n        item = pytester.getitem(p, \"TestClass::test_c\")\n        res = list(item.listchain())\n        expected = [\n            \"test_nodes.py::TestClass::test_c\",\n            \"test_nodes.py::TestClass\",\n            \"test_nodes.py\",\n        ]\n        assert [x.nodeid for x in res] == expected\n\n", "    def test_nodeid_empty(self, pytester: Pytester) -> None:\n        item = pytester.getitem(\"test_module.test_func\")\n        assert item.nodeid == \"test_module.py::test_func\"\n", "    def test_nodeid_with_conftest(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n                config.addinivalue_line(\"markers\", \"mymarker: mymarker\")\n\n            @pytest.mark.mymarker\n                pass\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*test_func*::mymarker*\"])\n", "    def test_from_parent_with_kwargs(self, pytester: Pytester) -> None:\n        \"\"\"Test that parent constructors are called correctly, and kwargs are\n        passed correctly.\n        \"\"\"\n        class MyItem(nodes.Item):\n                super().__init__(name=\"myitem\", parent=p, **kw)\n                self.kwargs = kw\n\n        class MyCollector(nodes.FSCollector):\n                return [MyItem(self, foo=\"bar\")]\n\n        result = pytester.runpytest(\n            \"--collect-only\",\n            \"-vv\",\n            \"-\",\n            \"\"\"\n                pass\n        \"\"\",\n        )\n\n        result.stdout.fnmatch_lines(\n            [\n                \"*myitem*\",\n                \"*kwargs: {'foo': 'bar'}*\",\n            ]\n        )\n"], "sample_1114": ["def test_normalize_theta_set_empty():\n    assert normalize_theta_set(Interval(5*pi, 3*pi)) == Interval(\n\n", "def test_ComplexRegion_intersection():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = Interval(S.Pi/2, 3*S.Pi/2)\n    unit_disk = ComplexRegion(a*b, polar=True)\n    upper_half_unit_disk = ComplexRegion(a*c, polar=True)\n    intersection = unit_disk.intersect(upper_half_unit_disk)\n    assert intersection == ComplexRegion(a*c, polar=True)\n\n", "    def test_complex_region():\n        a = Interval(2, 4)\n        b = Interval(1, 3)\n        c = Interval(0, pi)\n        c1 = ComplexRegion(a*b)  # rectangular\n        c2 = ComplexRegion(a*c, polar=True)  # polar\n\n        assert c1.contains(3+2*I) is S.True\n        assert c1.contains(1+2*I) is S.False\n        assert c2.contains(3*exp(I*pi/4)) is S.True\n\n        # Check for empty set\n        c3 = ComplexRegion(Interval(1,2)*Interval(3,4))\n        assert c3.contains(1 + 5*I) is S.False\n\n        assert c1.measure == 6\n        assert c2.measure == pi\n\n", "def test_complexregion_intersection():\n    a = Interval(2, 3)\n    b = Interval(4, 6)\n    c = Interval(1, 8)\n    c1 = ComplexRegion(a*b)\n    c2 = ComplexRegion(b*c)\n    c3 = ComplexRegion(a*c)\n    assert c1.intersect(c2) == ComplexRegion(a*b)\n\n    unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, 2*S.Pi), polar=True)\n    upper_half_unit_disk = ComplexRegion(Interval(0, 1)*Interval(0, S.Pi), polar=True)\n    intersection = unit_disk.intersect(upper_half_unit_disk)\n    assert intersection == upper_half_unit_disk\n\n    intersection = upper_half_unit_disk.intersect(unit_disk)\n    assert intersection == upper_half_unit_disk\n", "def test_complex_region_intersection():\n    a = Interval(0, 1)\n    b = Interval(0, 2*S.Pi)\n    c = Interval(S.Pi/2, 3*S.Pi/2)\n    unit_disk = ComplexRegion(a*b, polar=True)\n    upper_half_disk = ComplexRegion(a*c, polar=True)\n    intersection = unit_disk.intersect(upper_half_disk)\n    assert intersection == ComplexRegion(ProductSet(a, Interval(S.Pi/2, S.Pi/2)), polar=True)\n\n"], "sample_437": ["    def test_close_if_health_check_failed(self, mock_is_usable):\n        mock_is_usable.return_value = False\n        connections['default'].health_check_enabled = True\n        connections['default'].close_if_health_check_failed()\n        self.assertTrue(connections['default'].closed)\n\n", "    def test_atomic_rollback_on_exception_in_savepoint_block(self):\n        with transaction.atomic():\n            with connection.cursor() as cursor:\n                cursor.execute(\"BEGIN\")\n                cursor.execute(\"INSERT INTO test_person (first_name, last_name) VALUES ('John', 'Doe')\")\n                cursor.execute(\"SAVEPOINT my_savepoint\")\n                raise Exception(\"Intentional exception\")\n                cursor.execute(\"COMMIT\")\n        # The transaction should be rolled back because of the exception\n        self.assertEqual(Person.objects.count(), 0)\n", "    def test_atomic_rollback_on_exception(self):\n        with transaction.atomic():\n            Person.objects.create(name='Alice')\n            raise Exception('Something went wrong')\n\n        self.assertEqual(Person.objects.count(), 0)\n\n", "    def test_temporary_connection_context_manager(self, mock_cursor):\n        with connection.temporary_connection() as cursor:\n            self.assertEqual(cursor, mock_cursor.return_value)\n            mock_cursor.assert_called_once()\n\n        mock_cursor.assert_called_once()\n", "    def test_close_if_unusable_or_obsolete_calls_is_usable(self, mock_is_usable):\n        mock_is_usable.return_value = False\n        connections['default'].close_if_unusable_or_obsolete()\n        mock_is_usable.assert_called()\n"], "sample_354": ["    def test_createsuperuser_interactive_password_validation_success(self):\n        call_command('createsuperuser', interactive=True, stdin=MockTTY())\n        self.assertEqual(User.objects.count(), 1)\n        user = User.objects.get(username='alice')\n        self.assertTrue(user.check_password('password'))\n\n", "    def test_createsuperuser_custom_model(self):\n        # Test creating a superuser with a custom model\n", "    def test_createsuperuser_interactive_with_existing_username(self):\n        # Regression test for #24778: Superuser creation should fail if\n        # username already exists.\n        apps.register('myapp', router=None)\n        CustomUser.objects.create_user(username='alice', password='oldpassword')\n        old_stderr = sys.stderr\n        sys.stderr = StringIO()\n        try:\n            call_command('createsuperuser', interactive=True, verbosity=0,\n                         stdin=MockTTY())\n        except CommandError as e:\n            self.assertIn(\"Error: That username is already taken.\", str(e))\n        finally:\n            sys.stderr = old_stderr\n", "    def test_createsuperuser_with_email_field(self):\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n        try:\n            call_command('createsuperuser', interactive=True,\n                         stdin=MockTTY(), verbosity=0)\n            output = sys.stdout.getvalue()\n\n            self.assertIn('Superuser created successfully.', output)\n            self.assertEqual(User.objects.count(), 1)\n            user = User.objects.get(username='alice')\n            self.assertEqual(user.email, 'alice@example.com')\n        finally:\n            sys.stdout = old_stdout\n", "    def test_createsuperuser_interactive_success(self):\n        out = StringIO()\n        err = StringIO()\n        with override_settings(AUTH_USER_MODEL='auth_app.CustomUser'):\n            call_command('createsuperuser', interactive=True, stdout=out, stderr=err,\n                         stdin=MockTTY())\n        # Check that the superuser was created\n        self.assertEqual(CustomUser.objects.count(), 1)\n        self.assertEqual(CustomUser.objects.get(username='alice').is_superuser, True)\n        self.assertEqual(CustomUser.objects.get(username='alice').email, 'alice@example.com')\n\n"], "sample_1014": ["    def test_reshape(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        b = a.reshape(3, 2)\n        assert b.shape == (3, 2)\n        assert b._array == [1, 2, 3, 4, 5, 6]\n\n        raises(ValueError, lambda: a.reshape(2, 3))\n", "    def test_reshape(self):\n        a = ImmutableDenseNDimArray([1,2,3,4,5,6], (2,3))\n        b = a.reshape(3,2)\n        assert b.shape == (3, 2)\n        assert b[0,0] == 1\n        assert b[1,1] == 4\n\n\n", "def test_reshape():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    b = a.reshape(3, 2)\n    assert b.shape == (3, 2)\n    assert b[0, 0] == 1\n    assert b[2, 1] == 6\n", "def test_reshape():\n    a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n    b = a.reshape(3, 2)\n    assert b.shape == (3, 2)\n    assert b[0, 0] == 1\n    assert b[1, 1] == 4\n    assert b[2, 1] == 6\n\n", "    def test_reshape(self):\n        a = ImmutableDenseNDimArray([1, 2, 3, 4, 5, 6], (2, 3))\n        b = a.reshape(3, 2)\n        assert b.shape == (3, 2)\n        assert b[0, 0] == 1\n        assert b[2, 1] == 6\n        raises(ValueError, lambda: a.reshape(2, 4))\n"], "sample_988": ["def test_Add_identity():\n    assert Add.identity == 0\n    assert Add(0,1) == 1\n    assert Add(1,0) == 1\n", "def test_Relational_evalf():\n    assert Relational(x, '<', 2).evalf(subs={x: 1}) == True\n    assert Relational(x + y, '>', 3).evalf(subs={x: 1, y: 2}) == True\n    assert Relational(x, '<=', 2).evalf(subs={x: 2}) == True\n    assert Relational(x, '>=', 2).evalf(subs={x: 2}) == True\n\n    raises(ValueError, lambda: Relational(x, '<', y).evalf())\n    raises(ValueError, lambda: Relational(x, '>=', 2).evalf())\n", "def test_evalf_interval():\n    assert Interval(0, 1).evalf(2) == Interval(Float(0.00), Float(1.00))\n\n    assert Interval(-oo, oo).evalf(2) == Interval(Float('-inf'), Float('inf'))\n\n    assert Interval(1, pi).evalf(2) == Interval(Float(1.00), Float(3.14))\n", "def test_imaginary_unit():\n    assert S.I**2 == -1\n    assert (S.I*2).is_imaginary\n    assert (S.I*2).is_Complex\n    assert 1j*S.I == S.I*1j == S.NegativeOne\n", "def test_golden_ratio_evalf():\n    assert S.GoldenRatio.evalf(10).is_Float\n    assert S.GoldenRatio.evalf(10) == Float('1.6180339887')\n\n"], "sample_367": ["    def test_cache_control(self):\n        view = self.my_view\n        response = view(HttpRequest())\n        self.assertTrue('Cache-Control' in response['Cache-Control'])\n        self.assertEqual(response['Cache-Control'].split(',')[0], 'private')\n\n\n", "    def test_cache_page_decorator_with_invalid_timeout(self):\n        with self.assertRaises(TypeError):\n            cache_page('invalid_timeout')(lambda x: None)\n", "    def test_cache_page(self):\n        # Test that cache_page interacts correctly with Vary headers from the response\n        response = HttpResponse()\n        response['Vary'] = 'Cookie'\n        response.content = b'test content'\n\n        @cache_page(10)\n            return response\n\n        request = HttpRequest()\n        # Simulate a cached response\n        with mock.patch('.caches.get') as mock_cache_get:\n            mock_cache_get.return_value = response\n            cached_response = view(request)\n            self.assertNotEqual(id(response), id(cached_response))\n            self.assertEqual(cached_response.content, response.content)\n            mock_cache_get.assert_called_once_with(\n                'cache_page/view/%s' % request.path,\n                headers={'Vary': response['Vary']},\n                timeout=10,\n            )\n\n        # Simulate cache miss\n        with mock.patch('.caches.get') as mock_cache_get:\n            mock_cache_get.return_value = None\n            response = view(request)\n            self.assertEqual(response.content, b'test content')\n            mock_cache_get.assert_called_once_with(\n                'cache_page/view/%s' % request.path,\n                headers={'Vary': response['Vary']},\n                timeout=10,\n            )\n\n", "    def test_cache_page_decorator(self):\n        request = HttpRequest()\n        with mock.patch('django.middleware.cache.CacheMiddleware.process_response') as mock_process_response:\n            decorated_view = cache_page(60)(lambda request: HttpResponse('Hello world'))\n            decorated_view(request)\n            mock_process_response.assert_called_once_with(request, HttpResponse('Hello world'), \n                                                        page_timeout=60, cache_alias=None, key_prefix=None)\n\n", "    def test_cache_page_with_vary_headers(self):\n        request = HttpRequest()\n        response = HttpResponse('Cached content')\n        response['Vary'] = 'Accept-Encoding'\n        with mock.patch('django.middleware.cache.CacheMiddleware.process_response') as mock_process_response:\n            cache_page(60)(lambda r: response)(request)\n            mock_process_response.assert_called_with(response, request)\n            self.assertEqual(response['Vary'], 'Accept-Encoding')\n\n"], "sample_462": ["    def test_empty_choices(self):\n        class TestForm(Form):\n            field = ChoiceField(choices=[])\n\n        form = TestForm()\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form.errors['field'], [ChoiceField.default_error_messages['invalid_choice']]\n        )\n\n        form.fields['field'].choices = [(\"\", \"\")]\n        self.assertTrue(form.is_valid())\n", "    def test_empty_choices(self):\n        field = ChoiceField(choices=[])\n        self.assertFieldOutput(field, 'blankoption', 'blankoption')\n        with self.assertRaises(ValidationError):\n            field.clean('test')\n", "    def test_empty_label(self):\n        class MyForm(Form):\n            my_choice = ChoiceField(choices=[('', '---------'), ('a', 'Option A')])\n\n        form = MyForm()\n        self.assertEqual(form.fields['my_choice'].label, 'My Choice')\n        with self.assertNoLogs():\n            self.assertEqual(form.fields['my_choice'].as_widget(), '<select name=\"my_choice\" id=\"id_my_choice\"><option value=\"\">---------</option><option value=\"a\">Option A</option></select>')\n\n", "    def test_choicefield_empty_value(self):\n        field = ChoiceField(choices=[('1', 'One'), ('2', 'Two')], required=False)\n\n        # Test with an empty value\n        self.assertFieldEqual(field, '', None)\n\n        # Test with a value that's not in the choices\n        with self.assertRaises(ValidationError):\n            field.clean('3')\n", "    def test_choice_field_required(self):\n        class MyForm(Form):\n            my_choice = ChoiceField(choices=[('a', 'A'), ('b', 'B')], required=True)\n\n        form = MyForm({})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['my_choice'], [ValidationError('This field is required.')])\n\n"], "sample_1008": ["    def test_orientnew(self):\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Axis', [dynamicsymbols('q'), N.x])\n        assert A.ang_vel_in(N) == dynamicsymbols('q')*N.x\n\n", "def test_orientnew_axis():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [2*pi/3, N.z])\n    assert A.dcm(N) == Matrix([[1/2, -sqrt(3)/2, 0],\n                               [sqrt(3)/2, 1/2, 0],\n                               [0, 0, 1]])\n", "    def test_orientnew_axis():\n        N = ReferenceFrame('N')\n        A = N.orientnew('A', 'Axis', [10*pi/180, N.x])\n        assert A.dcm(N) == Matrix([\n        [1, 0, 0],\n        [0, cos(10*pi/180), -sin(10*pi/180)],\n        [0, sin(10*pi/180), cos(10*pi/180)]])\n\n", "def test_orientnew():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [dynamicsymbols('q'), N.x + 2*N.y])\n    assert A.ang_vel_in(N) == dynamicsymbols('q')*(N.x + 2*N.y)\n\n    B = N.orientnew('B', 'Body', [dynamicsymbols('q1'), \n                                   dynamicsymbols('q2'), \n                                   dynamicsymbols('q3')], '123')\n    assert B.ang_vel_in(N) == (dynamicsymbols('q1')*N.x + \n                               dynamicsymbols('q2')*N.y +\n                               dynamicsymbols('q3')*N.z)\n\n", "def test_dcm_simple_rot():\n    N = ReferenceFrame('N')\n    A = N.orientnew('A', 'Axis', [pi/2, N.x])\n    assert A.dcm(N) == Matrix([[1, 0, 0],\n                              [0, 0, -1],\n                              [0, 1, 0]])\n\n\n\n"], "sample_721": ["def test_check_X_y_pandas_dataframe():\n    # Check if check_X_y handles pandas DataFrame correctly\n    df = MockDataFrame({\"feature1\": [1, 2, 3], \"feature2\": [4, 5, 6]},\n                       index=[1, 2, 3])\n    y = np.array([1, 0, 1])\n\n    # Check for warnings when y is a pandas Series\n    with warnings.catch_warnings(record=True) as w:\n        check_X_y(df, y)\n        assert_equal(len(w), 1)\n        assert_equal(w[0].category, DataConversionWarning)\n\n    # Check for success when y is a numpy array\n    X, y = check_X_y(df, y)\n    assert_equal(type(X), np.ndarray)\n    assert_equal(type(y), np.ndarray)\n\n", "    def test_check_array_dtype_obj_force_numeric(self):\n        X = np.array([['a', 1], [2, 'b']], dtype=object)\n        with pytest.warns(DataConversionWarning):\n            # dtype is enforced to numeric\n            X_converted = check_array(X, dtype=\"numeric\")\n        assert_equal(X_converted.dtype, np.float64)\n", "    def test_check_consistent_length_mixed_types():\n        X = np.array([[1, 2], [3, 4]])\n        y = [5, 6]\n        check_consistent_length(X, y)\n        \n        X = np.array([1, 2])\n        y = [[5], [6]]\n        check_consistent_length(X, y)\n\n", "    def test_check_X_y_multioutput_sparse():\n        X = sparse_random_matrix(50, 10)\n        y = np.random.randn(50, 3)\n        check_X_y(X, y, multi_output=True)\n\n        # Sparse y should also work:\n        y_sparse = sp.csr_matrix(y)\n        check_X_y(X, y_sparse, multi_output=True)\n\n        # Check error if y is not 2D for multioutput\n        y_err = np.random.randn(50)\n        with pytest.raises(ValueError):\n            check_X_y(X, y_err, multi_output=True)\n\n\n\n", "    def test_check_consistent_length_different_lengths():\n        X = np.array([[1, 2], [3, 4]])\n        y = np.array([1, 2, 3])\n        with pytest.raises(ValueError, match=\"Found input variables with inconsistent numbers of samples\"):\n            check_consistent_length(X, y)\n"], "sample_769": ["def test_brier_score_loss_empty():\n    y_true = np.array([])\n    y_prob = np.array([])\n\n    score = brier_score_loss(y_true, y_prob)\n    assert score == 0.0\n", "    def test_brier_score_loss_multilabel(self):\n        y_true = np.array([[1, 0], [0, 1], [1, 1]])\n        y_prob = np.array([[0.8, 0.2], [0.3, 0.7], [0.9, 0.1]])\n        score = brier_score_loss(y_true, y_prob)\n        assert_almost_equal(score, 0.083333)\n", "    def test_brier_score_loss_multiclass():\n        y_true = np.array([0, 1, 2, 0, 1, 2])\n        y_prob = np.array([[0.1, 0.8, 0.1],\n                           [0.2, 0.3, 0.5],\n                           [0.4, 0.2, 0.4],\n                           [0.1, 0.7, 0.2],\n                           [0.3, 0.4, 0.3],\n                           [0.2, 0.2, 0.6]])\n        assert_almost_equal(brier_score_loss(y_true, y_prob),\n                            0.5333333333333333)\n\n", "    def test_brier_score_loss_multiclass_warning():\n        y_true = np.array([0, 1, 2, 0])\n        y_prob = np.array([[0.1, 0.2, 0.7],\n                         [0.8, 0.1, 0.1],\n                         [0.3, 0.4, 0.3],\n                         [0.6, 0.2, 0.2]])\n\n        msg = (\"Brier score only works for binary and categorical \"\n               \"outcomes. Consider using multilabel_brier_score_loss for \"\n               \"multiclass problems.\")\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(\"always\")\n            brier_score_loss(y_true, y_prob)\n            assert_no_warnings(w, msg)\n\n", "    def test_classification_report_empty_labels():\n        y_true = np.array([])\n        y_pred = np.array([])\n        expected_output = {\n            'precision': {},\n            'recall': {},\n            'f1-score': {},\n            'support': {}\n        }\n        report = classification_report(y_true, y_pred)\n        assert_equal(report,\n                     \"                   precision    recall  f1-score   support\\n\\n\"\n                     \"        accuracy                           0\\n\"\n                     \"       macro avg                           0\\n\"\n                     \"    weighted avg                           0\\n\")\n"], "sample_703": ["    def test_not_expression():\n            return x == \"match\"\n\n        assert evaluate(\"not match\", matcher) is True\n        assert evaluate(\"not not match\", matcher) is True\n        assert evaluate(\"not no_match\", matcher) is False\n", "    def test_nested_expressions():\n            return s == \"foo\"\n        assert evaluate(\" ( not ( foo and bar ) or baz ) \", matcher) is False\n", "    def test_nested_or():\n        assert evaluate(\"not (a or b)\", lambda x: x == \"a\") is False\n        assert evaluate(\"not (a or b)\", lambda x: x == \"b\") is False\n        assert evaluate(\"not (a or b)\", lambda x: x == \"c\") is True\n", "    def test_not_parenthesized_or(self):\n            return name == \"test_a\" or name == \"test_c\"\n        assert evaluate(\"test_a or test_b or test_c\", matcher)\n", "    def test_not_expr_paren():\n        assert evaluate(\"(not a)\", lambda x: x == \"a\") is False\n        assert evaluate(\"(not (not a))\", lambda x: x == \"a\") is True\n\n"], "sample_338": ["    def test_default_pk_changes(self):\n        before_states = [\n            self.author_with_custom_pk,\n        ]\n        after_states = [\n            self.author_unmanaged_default_pk,\n        ]\n        changes = self.get_changes(before_states, after_states)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\", \"AddField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"pk_field\")\n        self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"id\")\n", "    def test_many_to_many_through_rename_table(self):\n        before = [\n            self.book_with_multiple_authors,\n            self.author_with_book,\n        ]\n        after = [\n            self.book_with_multiple_authors_through_attribution,\n            self.author_with_book,\n            self.attribution,\n        ]\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\n            \"CreateModel\",\n            \"AlterField\",\n        ])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='Attribution')\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name='book', field='book')\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, field='author', field='author')\n", "    def test_many_to_many_through_with_different_default(self):\n        before_state = ProjectState()\n        before_state.add_model(self.contract)\n        before_state.add_model(self.author)\n        before_state.add_model(self.publisher)\n\n        after_state = ProjectState()\n        after_state.add_model(self.contract)\n        after_state.add_model(self.author)\n        after_state.add_model(self.publisher)\n        after_state.add_model(self.author_with_m2m_through)\n\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel', 'AddField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 1, name='author_with_m2m_through')\n        self.assertOperationFieldAttributes(changes, 'testapp', 0, 1, name='publishers')\n", "    def test_circular_fk_dependency(self):\n        changes = self.get_changes([\n            self.knight,\n            self.rabbit,\n        ], [\n            self.knight,\n            self.rabbit,\n        ])\n        self.assertNumberMigrations(changes, 'eggs', 1)\n        self.assertOperationTypes(changes, 'eggs', 0, [\n            'CreateModel',\n            'AddField',\n        ])\n        self.assertMigrationDependencies(changes, 'eggs', 0, [])\n\n", "    def test_empty_initial_migration(self):\n        \"\"\"\n        Test that an empty initial migration is created correctly.\n        \"\"\"\n        before = ProjectState()\n        after = ProjectState()\n        changes = self.get_changes(before, after)\n\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertMigrationDependencies(changes, 'testapp', 0, [])\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n"], "sample_713": ["    def test_ridgecv_sparse_input(self):\n        # Test RidgeCV with sparse input\n        n_samples = 100\n        n_features = 50\n\n        X = sp.csr_matrix(np.random.rand(n_samples, n_features))\n        y = np.random.randn(n_samples)\n\n        ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1])\n        ridgecv.fit(X, y)\n\n        # Check if the coefficients are not NaN\n        assert_true(not np.isnan(ridgecv.coef_).any())\n\n        # Check if the shape of coefficients is correct\n        assert_equal(ridgecv.coef_.shape[0], n_features)\n\n", "    def test_ridge_cv_sparse_input(self):\n        # Check that sparse input works with RidgeCV\n        sparse_diabetes = sp.csr_matrix(X_diabetes)\n        ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(\n            sparse_diabetes, y_diabetes\n        )\n\n        dense_ridgecv = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(\n            X_diabetes, y_diabetes\n        )\n\n        assert_almost_equal(ridgecv.coef_, dense_ridgecv.coef_)\n        assert_almost_equal(ridgecv.intercept_, dense_ridgecv.intercept_)\n", "    def test_ridgecv_sparse_eigen_gcv(self):\n        X = sp.csr_matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n        y = np.array([1, 2, 3])\n        alphas = np.array([0.1, 1.0, 10.0])\n        ridgecv = RidgeCV(alphas=alphas, store_cv_values=True, gcv_mode='eigen',\n                          fit_intercept=False, cv=None)\n\n        ridgecv.fit(X, y)\n        # check that gcv_mode='eigen' works for sparse matrices\n        assert_true(ridgecv.alpha_ in alphas)\n        assert_equal(ridgecv.coef_.shape, (3,))\n        assert_true(isinstance(ridgecv.cv_values_, np.ndarray))\n\n", "    def test_ridgecv_cv_sparse(self):\n        # Test RidgeCV with sparse matrices and cross-validation\n        X = sp.csr_matrix(make_regression(n_samples=100, n_features=10,\n                                        random_state=0)[0])\n        y = make_regression(n_samples=100, n_features=10, random_state=0)[1]\n\n        model = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n        assert_true(model.coef_.shape == (10,))\n", "    def test_ridge_classifier_cv_sparse(self):\n        # Test RidgeClassifierCV with sparse input\n        rng = np.random.RandomState(0)\n        X = rng.randn(20, 5)\n        X_sparse = sp.csr_matrix(X)\n        y = rng.randint(0, 2, size=20)\n\n        for estimator in [RidgeClassifierCV(),\n                         RidgeClassifierCV(cv=5)]:\n            estimator.fit(X_sparse, y)\n            estimator.predict(X_sparse)\n"], "sample_349": ["    def test_autocomplete_select_required_field_render(self):\n        form = RequiredBandForm()\n        expected_html = (\n            '<select name=\"band\" id=\"id_band\" class=\"my-class admin-autocomplete '\n            'autocomplete-widget\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" '\n            'data-ajax--type=\"GET\" data-ajax--url=\"/admin/autocomplete/\" '\n            'data-app-label=\"music\" data-model-name=\"band\" data-field-name=\"band\" '\n            'data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" '\n            'data-placeholder=\"\" lang=\"en\">'\n            f'{self.empty_option}'\n            '</select>'\n        )\n        self.assertHTMLEqual(str(form['band']), expected_html)\n", "    def test_autocomplete_required_field_with_empty_option(self):\n        form = RequiredBandForm()\n        self.assertEqual(str(form.as_p()), '<p><label for=\"id_band\">Band:</label><select id=\"id_band\" class=\"admin-autocomplete select2-offscreen\" data-ajax--cache=\"true\" data-ajax--delay=\"250\" data-ajax--type=\"GET\" data-ajax--url=\"/admin/autocomplete/\" data-app-label=\"music\" data-model-name=\"band\" data-field-name=\"band\" data-theme=\"admin-autocomplete\" data-allow-clear=\"false\" data-placeholder=\"\" lang=\"en\">%s</select></p>' % self.empty_option)\n", "    def test_autocomplete_select_with_empty_value(self):\n        form = NotRequiredBandForm()\n        self.assertIn(self.empty_option, form.as_p())\n\n", "    def test_autocomplete_select_multiple_required(self):\n        form = RequiredBandForm(data={})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['band'], ['This field is required.'])\n\n        form = RequiredBandForm(data={'band': ''})\n        self.assertFalse(form.is_valid())\n        self.assertEqual(form.errors['band'], ['This field is required.'])\n\n", "    def test_required_field_empty_option(self):\n        form = RequiredBandForm()\n        self.assertNotIn(self.empty_option, form.as_p())\n\n"], "sample_190": ["    def test_isnull_with_rhs_none(self):\n        with self.assertRaisesMessage(ValueError, 'The QuerySet value for an isnull lookup must be True or False.'):\n            IsNullWithNoneAsRHS.objects.filter(some_field__isnull=None)", "    def test_in_lookup_with_empty_iterable(self):\n        with self.assertRaises(EmptyResultSet):\n            Article.objects.filter(tags__in=[])\n\n", "    def test_isnull_with_none_as_rhs(self):\n        with self.assertRaisesMessage(\n            ValueError,\n            'The QuerySet value for an isnull lookup must be True or False.',\n        ):\n            IsNullWithNoneAsRHS.objects.filter(f=None)\n", "    def test_isnull_with_none_as_rhs(self):\n        with self.assertRaises(ValueError):\n            IsNullWithNoneAsRHS.objects.filter(field__isnull=None)\n", "    def test_exact_microsecond(self):\n        article = Article.objects.create(\n            headline='Article with microseconds',\n            pub_date=datetime(2005, 7, 27, 10, 1, 30, 500000),\n            author=self.au1,\n        )\n        self.assertQuerysetEqual(\n            Article.objects.filter(pub_date__exact=article.pub_date),\n            [article],\n        )\n"], "sample_1001": ["def test_print_ComplexRegion():\n    r = Symbol('r', real=True)\n    theta = Symbol('theta', real=True)\n    cr = ComplexRegion(Interval(0, 1), Interval(0, 2*pi))\n    assert latex(cr) == r'\\left\\{ z \\in \\mathbb{C} \\mid 0 \\le \\operatorname{Re}{\\left(z \\right)} \\le 1 \\text{ and } 0 \\le \\operatorname{Im}{\\left(z \\right)} \\le 2 \\pi \\right\\}'\n\n", "def test_latex_SingularityFunction():\n    assert latex(SingularityFunction(x, 0)) == '\\operatorname{Sing}(x, 0)'\n    assert latex(SingularityFunction(x, 1)) == '\\operatorname{Sing}(x, 1)'\n", "    def test_print_tensor_product(self):\n        A = MatrixSymbol('A', 2, 2)\n        B = MatrixSymbol('B', 2, 2)\n        C = tensorproduct(A, B)\n        assert latex(C) == r'A \\otimes B'\n", "def test_latex_tensor_array():\n    A = ImmutableDenseNDimArray([ [1, 2], [3, 4] ])\n    assert latex(A) == r'\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}'\n    \n    B = ImmutableSparseNDimArray.zeros(2,2)\n    assert latex(B) == r'\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}'\n    \n    C = MutableDenseNDimArray([ [1, 2], [3, 4] ])\n    assert latex(C) == r'\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}'\n\n    D = MutableSparseNDimArray.zeros(2,2)\n    assert latex(D) == r'\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}'\n    \n    E = tensorproduct(A, B)\n    assert latex(E) == r'\\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\end{bmatrix}'\n\n", "def test_latex_permutation():\n    p = Permutation([0, 2, 1])\n    assert latex(p) == r\"\\left(0\\ 2\\ 1\\right)\"\n"], "sample_582": ["    def test_cli_with_no_app(runner):\n        result = runner.invoke(cli, [\"--help\"])\n        assert result.exit_code == 0\n        assert \"Error: Could not locate a Flask application\" in result.output\n\n\n", "    def test_run_command_with_ssl_context(runner, tmpdir):\n        app = Flask(__name__)\n        ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n\n        with open(tmpdir.join(\"cert.pem\"), \"w\") as f:\n            f.write(\"...\")\n\n        with open(tmpdir.join(\"key.pem\"), \"w\") as f:\n            f.write(\"...\")\n\n        ctx.load_cert_chain(str(tmpdir.join(\"cert.pem\")), str(tmpdir.join(\"key.pem\")))\n\n        with runner.isolated_filesystem():\n            result = runner.invoke(\n                run_command,\n                [\"--cert\", ctx, \"--host\", \"localhost\", \"--port\", \"5001\"],\n            )\n\n        assert result.exit_code == 0\n", "def test_find_best_app(runner, tmpdir):\n    with tmpdir.as_cwd():\n        (tmpdir / 'app.py').write_text(\n            'from flask import Flask\\n'\n            'app = Flask(__name__)\\n'\n        )\n        assert find_best_app() is not None\n", "def test_run_command_ssl_context(runner, monkeypatch):\n    @click.command(\"run\")\n    @click.option(\"--cert\", type=click.Path(exists=True))\n        assert cert is not None\n        with open(cert) as f:\n            cert_data = f.read()\n        return cert_data\n\n    app = Flask(__name__)\n    app.cli.add_command(run_command)\n\n    with monkeypatch.chdir(test_path):\n        result = runner.invoke(app.cli, [\"run\", \"--cert\", \"cert.pem\"])\n    assert result.exit_code == 0\n    assert result.output == \"Cert data\"\n\n", "def test_run_command_with_cert(runner, tmpdir):\n    config_path = tmpdir / \"config.py\"\n    config_path.write_text(\n        \"\"\""], "sample_617": ["    def test_groupby_then_agg(self, func, expected):\n      ds = xr.Dataset({\"foo\": ([\"dim\"], [1, 2, 3, 4])}, coords={\"dim\": [0, 0, 1, 1]})\n      actual = func(ds)\n      assert_identical(actual, expected)\n", "    def test_polyval(self, dask):\n        # Simple 1D case\n        x = xr.DataArray(\n            np.arange(5), dims=['x'], name='x'\n        )\n        if dask:\n            x = x.chunk()\n        coeffs = xr.DataArray([1, 2, 3], dims=['degree'], name='degree')\n        expected = 1 * x**2 + 2 * x + 3\n        actual = xr.polyval(x, coeffs)\n        assert_identical(actual, expected)\n\n        # 2D case\n        x = xr.DataArray(\n            np.arange(6).reshape(2, 3), dims=['y', 'x'], name='x'\n        )\n        if dask:\n            x = x.chunk({'y': 1})\n        coeffs = xr.DataArray(\n            [1, 2, 3, 4], dims=['degree'], name='degree'\n        )\n        expected = 1 * x**3 + 2 * x**2 + 3 * x + 4\n        actual = xr.polyval(x, coeffs)\n\n", "    def test_apply_ufunc_preserve_coords():\n        x = xr.DataArray(np.arange(4), dims=[\"a\"], coords={\"a\": [\"a\", \"b\", \"c\", \"d\"]})\n        y = xr.DataArray(np.arange(4), dims=[\"a\"], coords={\"a\": [\"a\", \"b\", \"c\", \"d\"]})\n        z = apply_ufunc(operator.add, x, y, output_core_dims=[None]).compute()\n        assert_identical(z.coords[\"a\"], x.coords[\"a\"])\n\n", "    def test_apply_ufunc_dask_compat(\n        self,", "    def test_broadcast_compat_data(data, expected):\n        actual = broadcast_compat_data(data)\n        assert_array_equal(actual, expected)\n"], "sample_1062": ["    def test_trig_split2():\n        x, y = symbols('x y', real=True)\n        assert trig_split(cos(x), -sqrt(3)*sin(x), two=True) == (2*sqrt(2), 1, -1, x, pi/6, False)\n        assert trig_split(cos(x), sin(y), two=True) is None\n        assert trig_split(cos(x)*cos(y), sin(x)*sin(y), two=False) == (1, 1, 1, x, y, False)\n        assert trig_split(cos(x)*cos(y), sin(x)*sin(y), two=True) is None\n        assert trig_split(sqrt(2)*cos(x), -sqrt(6)*sin(x), two=True) == (2*sqrt(2), 1, -1, x, pi/6, False)\n        assert trig_split(-sqrt(6)*cos(x), -sqrt(2)*sin(x), two=True) == (-2*sqrt(2), 1, 1, x, pi/3, False)\n        assert trig_split(cos(x)/sqrt(6), sin(x)/sqrt(2), two=True) == (sqrt(6)/3, 1, 1, x, pi/6, False)\n        assert trig_split(-sqrt(6)*cos(x)*sin(y), -sqrt(2)*sin(x)*sin(y), two=True) == (-2*sqrt(2)*sin(y), 1, 1, x, pi/3, False)\n", "    def test_trig_split_two_double_angle():\n        assert trig_split(2*cos(x)*cos(y), 2*sin(x)*sin(y), two=True) == (\n            2*sqrt(2), 1, 1, x, y, False)\n        assert trig_split(cos(x)*cos(y), sin(x)*sin(y), two=True) == (\n            1, 1, 1, x, y, False)\n        assert trig_split(-2*cos(x)*cos(y), -2*sin(x)*sin(y), two=True) == (\n            2*sqrt(2), 1, 1, x, y, False)\n        assert trig_split(cos(2*x)*cos(y), sin(2*x)*sin(y), two=True) == (\n            1, 1, 1, 2*x, y, False)\n", "    def test_fu_issue_20989():\n        x = Symbol('x')\n        e = fu(sqrt(2)*cos(x) - sqrt(2)*sin(x))\n        assert simplify(e - sqrt(2)*cos(x + pi/4)) == 0\n", "    def test_TR56():\n        assert T(sin(x)**2*cos(x), sin, cos, lambda x: x - 1) == -sin(x)**3/3 + sin(x)/3\n        assert verify_numerically(T(cos(x)**2*sin(x), cos, sin, lambda x: x - 1),\n                                -cos(x)**3/3 + cos(x)/3, x) is True\n\n        assert verify_numerically(T(sin(x)**3*cos(x), sin, cos, lambda x: x - 1),\n                                sin(x)**4/4, x) is True\n", "def test_TR12i():\n    x = Symbol('x')\n    assert TR12i(cos(x)**2 + sin(x)**2) == 1\n"], "sample_896": ["    def test_sparse_input_nmf(self):\n        # Check if NMF works with sparse input\n        rng = np.random.RandomState(42)\n        X = rng.rand(100, 10)\n        X[X < 0.5] = 0\n        X_sparse = csc_matrix(X)\n        model = NMF(n_components=5, random_state=42)\n        model.fit(X_sparse)\n        assert model.n_components_ == 5\n        assert_allclose(model.reconstruction_err_, squared_norm(X - model.transform(X_sparse) @ model.components_), rtol=1e-5)\n\n", "    def test_mini_batch_nmf_transform_partial_fit(self):\n        # Check that transform works after partial_fit\n        rng = np.random.RandomState(42)\n        X = rng.rand(50, 10)\n        n_components = 5\n\n        model = MiniBatchNMF(n_components=n_components, random_state=rng)\n        model.partial_fit(X)\n        transformed = model.transform(X)\n        assert transformed.shape == (50, 5)\n\n        model = MiniBatchNMF(n_components=n_components, random_state=rng)\n        model.fit(X)\n        transformed_full = model.transform(X)\n\n        assert_allclose(transformed, transformed_full)\n", "    def test_init_with_custom_components(Estimator, solver):\n        np.random.seed(42)\n        n_samples = 100\n        n_features = 5\n        n_components = 3\n        X = np.random.rand(n_samples, n_features)\n        W = np.random.rand(n_samples, n_components)\n        H = np.random.rand(n_components, n_features)\n        estimator = Estimator(\n            n_components=n_components, init='custom', random_state=0, **solver\n        ).fit_transform(X, W=W, H=H)\n\n        assert_array_almost_equal(estimator.components_, H)\n\n", "    def test_transform_sparse_input(self):\n        # Test that transform works with sparse input\n        rng = np.random.RandomState(42)\n        n_samples = 100\n        n_features = 50\n        n_components = 10\n\n        X = rng.rand(n_samples, n_features)\n        X_sparse = csc_matrix(X)\n\n        model = MiniBatchNMF(n_components=n_components, random_state=rng)\n        model.fit(X)\n        W1 = model.transform(X)\n\n        model = MiniBatchNMF(n_components=n_components, random_state=rng)\n        model.fit(X_sparse)\n        W2 = model.transform(X_sparse)\n\n        assert_allclose(W1, W2)\n", "    def test_n_components_warning(self, Estimator, solver, random_state):\n        # Test warning when n_components is None\n        n_samples = 10\n        n_features = 5\n        X = np.random.rand(n_samples, n_features)\n        with warnings.catch_warnings(record=True) as w:\n            estimator = Estimator(\n                n_components=None, solver=solver, random_state=random_state\n            )\n            estimator.fit(X)\n            assert len(w) == 1\n            assert (\n                \"n_components\" in str(w[0].message)\n                and \"automatically inferred\" in str(w[0].message)\n            )\n\n"], "sample_433": ["    def test_knight_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n        rabbit = Rabbit.objects.create(knight=knight)\n        with pytest.raises(DatabaseError):\n            Rabbit.objects.create(knight=knight, parent=rabbit)\n\n", "    def test_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight.rabbit_set.create(knight=knight))\n", "    def test_rabbit_circular_fk_unique_together(self):\n        knight = Knight.objects.create()\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight)\n\n", "    def test_unique_together_with_fk_circular_index(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit1)\n", "    def test_knight_and_rabbit_index(self):\n        knight = Knight.objects.create()\n        rabbit_1 = Rabbit.objects.create(knight=knight)\n        rabbit_2 = Rabbit.objects.create(knight=knight, parent=rabbit_1)\n        self.assertEqual(Rabbit.objects.filter(knight=knight).count(), 2)\n        self.assertEqual(Rabbit.objects.filter(parent=rabbit_1).count(), 1)\n"], "sample_550": ["    def test_axes_get_tightbbox_extraartists(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n        # Test with no extra artists\n        bbox = ax.get_tightbbox(renderer=fig.canvas.get_renderer())\n\n        # Test with a list of extra artists\n\n        extra_artists = [ax.get_children()[0]]  # Get the line artist\n        bbox_extra = ax.get_tightbbox(\n            renderer=fig.canvas.get_renderer(),\n            bbox_extra_artists=extra_artists,\n        )\n        assert bbox_extra == bbox\n\n        \n\n", "    def test_axes_zoom(self):\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n        # Zoom in using the zoom rectangle\n        event = MouseEvent('button_press_event', fig.canvas, x=100, y=100)\n        ax._on_button_press(event)\n        event = MouseEvent('motion_notify_event', fig.canvas, x=200, y=200)\n        ax._on_mouse_move(event)\n        event = MouseEvent('button_release_event', fig.canvas, x=200, y=200)\n        ax._on_button_release(event)\n\n        # Assert that the x and y limits have changed\n        assert ax.get_xlim() != (1, 3)\n        assert ax.get_ylim() != (4, 6)\n", "    def test_twin_axes(self, method):\n        fig, ax = plt.subplots()\n        twin_ax = getattr(ax, method)()\n        assert ax._twinned_axes.is_joined_with(twin_ax)\n        assert twin_ax._twinned_axes.is_joined_with(ax)\n\n        # Check that twinned axes share the same x/y limits\n        ax.set_xlim([0, 1])\n        ax.set_ylim([0, 1])\n        assert np.allclose(twin_ax.get_xlim(), [0, 1])\n        assert np.allclose(twin_ax.get_ylim(), [0, 1])\n\n        # Check that twinned axes have independent tick labels\n        ax.set_xticks([0.5])\n        ax.set_yticks([0.5])\n        twin_ax.set_xticks([0.25])\n        twin_ax.set_yticks([0.75])\n        assert np.allclose(ax.get_xticks(), [0.5])\n        assert np.allclose(ax.get_yticks(), [0.5])\n        assert np.allclose(twin_ax.get_xticks(), [0.25])\n        assert np.allclose(twin_ax.get_yticks(), [0.75])\n\n", "def test_axes_get_shared_axes(plt):\n    fig, axes = plt.subplots(2, 2)\n    axes[0, 0].get_shared_x_axes()\n\n\n", "    def test_axes_set_adjustable(self):\n        fig, ax = plt.subplots()\n        ax.set_adjustable(\"box\")\n        assert ax.get_adjustable() == \"box\"\n        ax.set_adjustable(\"datalim\")\n        assert ax.get_adjustable() == \"datalim\"\n        with pytest.raises(ValueError):\n            ax.set_adjustable(\"unknown\")\n"], "sample_895": ["    def test_nested_transformers(self):\n        transformer = ColumnTransformer(\n            transformers=[\n                (\"num\", StandardScaler(), [\"a\", \"b\"]),\n                (\"cat\", OneHotEncoder(), [\"c\"]),\n                (\n                    \"nested\",\n                    ColumnTransformer(\n                        transformers=[(\"double\", DoubleTrans(), [\"a\"])]\n                    ),\n                    [\"a\"],\n                ),\n            ]\n        )\n\n        X = np.array([\n            [1, 2, 'a'],\n            [3, 4, 'b'],\n            [5, 6, 'c']\n        ], dtype=object)\n\n        transformer.fit(X)\n        Xt = transformer.transform(X)\n        assert Xt.shape == (3, 6)\n\n", "    def test_sparse_output_with_remainder(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        sparse_transformer = SparseMatrixTrans()\n        ct = ColumnTransformer(\n            transformers=[('sparse', sparse_transformer, [0])],\n            remainder='passthrough',\n        )\n        Xt = ct.fit_transform(X)\n        assert isinstance(Xt, sparse.csr_matrix)\n        assert_allclose(\n            Xt.toarray(), np.array([[1, 2], [3, 4], [5, 6]]),\n        )\n\n", "    def test_sparse_output_with_sparse_transformer():\n        \"\"\"Test sparse output with a sparse transformer.\"\"\"\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        transformer = ColumnTransformer(\n            transformers=[\n                (\"scaler\", StandardScaler(), [0]),\n                (\"dense\", Trans(), [1]),\n            ],\n            sparse_threshold=0.3,\n        )\n         Xt = transformer.fit_transform(X)\n        assert isinstance(Xt, sparse.csr_matrix)\n        assert Xt.shape == (2, 2)\n\n", "def test_column_transformer_remainder_sparse_threshold():\n    # test that sparse output is handled correctly when remainder is a transformer\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    transformer = ColumnTransformer([\n        ('t1', StandardScaler(), [0]),\n        ('t2', 'passthrough', 1),\n        ('remainder', TransNo2D(), \n        [2])], sparse_threshold=0.5\n    )\n    transformer.fit(X)\n    Xt = transformer.transform(X)\n    assert isinstance(Xt, sparse.csr_matrix)\n\n", "    def test_column_transformer_remainder_sparse(self):\n        sparse_data = sparse.csr_matrix([[1, 2], [3, 4]])\n        ct = ColumnTransformer(\n            transformers=[\n                (\"num\", StandardScaler(), [0]),\n                (\"cat\", OneHotEncoder(), [1]),\n            ],\n            remainder=\"passthrough\",\n        )\n        transformed = ct.fit_transform(sparse_data)\n        assert_allclose_dense_sparse(\n            transformed, sparse.csr_matrix([[0.0, 0.0, 1.0, 2.0], [1.0, 0.0, 3.0, 4.0]])\n        )\n\n"], "sample_544": ["    def test_pcolormesh_alpha():\n        # Test that alpha values are properly handled in pcolormesh\n        x = np.arange(5)\n        y = np.arange(5)\n        Z = np.zeros((5, 5))\n        Z[::2, ::2] = 1\n        alpha = np.ones_like(Z) * 0.5\n        alpha[1, 1] = 0\n\n        fig, ax = plt.subplots()\n        img = ax.pcolormesh(x, y, Z, alpha=alpha)\n        img2 = ax.pcolormesh(x, y, Z, alpha=alpha, shading='gouraud')\n        fig.canvas.draw()\n        # Check that the rendered image has the correct alpha values\n        # This assumes that the backend has been tested independently to render\n        # alpha correctly\n\n        # this is not reliable, as the backend renders the image directly\n        # and it might not be in the FigureCanvasBase\n        # image_data = img.get_array().data\n        # assert_array_equal(image_data[:, :, 3], alpha * 255)\n\n        # We can't reliably test the rendered image directly, so we'll just check\n        # that the alpha values are being passed to the backend correctly.\n        assert hasattr(img, 'get_hatch')\n\n\n\n", "    def test_pil_to_array(self):\n        # Test conversion of PIL Image to numpy array\n\n        # Test RGB image\n        rgb_im = Image.new(\"RGB\", (10, 10), \"red\")\n        rgb_arr = mimage.pil_to_array(rgb_im)\n        assert rgb_arr.shape == (10, 10, 3)\n        assert np.all(rgb_arr == np.array([255, 0, 0]))\n\n        # Test RGBA image\n        rgba_im = Image.new(\"RGBA\", (10, 10), (255, 0, 0, 255))\n        rgba_arr = mimage.pil_to_array(rgba_im)\n        assert rgba_arr.shape == (10, 10, 4)\n        assert np.all(rgba_arr == np.array([255, 0, 0, 255]))\n\n        # Test grayscale image\n        gray_im = Image.new(\"L\", (10, 10), 128)\n        gray_arr = mimage.pil_to_array(gray_im)\n        assert gray_arr.shape == (10, 10)\n        assert np.all(gray_arr == 128)\n", "    def test_imshow_origin(self):\n        fig, ax = plt.subplots()\n        data = np.arange(9).reshape((3, 3))\n\n        ax.imshow(data, origin='upper', cmap='viridis')\n        ax.set_ylim(0, 3)\n        ax.set_xlim(0, 3)\n\n        # Check that the y-axis limits are correctly flipped to match the\n        # origin='upper' setting\n        assert ax.get_ylim() == (2.0, -1.0)\n\n        ax.clear()\n\n        ax.imshow(data, origin='lower', cmap='viridis')\n        ax.set_ylim(0, 3)\n        ax.set_xlim(0, 3)\n\n        # Check that the y-axis limits are aligned with the origin='lower'\n        # setting\n        assert ax.get_ylim() == (-1.0, 2.0)\n", "    def test_pil_to_array_mode_p(self):\n        # Test conversion of a PIL Image in mode 'P' to a NumPy array\n        # Create a PIL Image in mode 'P'\n        data = [(0, 0, 0), (255, 255, 255)]  # Palette\n        im = Image.new(\"P\", (10, 10))\n        im.putpalette(data)\n        # Load the image as a NumPy array\n        array = mimage.pil_to_array(im)\n        # Check the shape and data type of the array\n        assert array.shape == (10, 10)\n        assert array.dtype == np.uint8\n", "    def test_pcolorimage(self):\n        X, Y = np.mgrid[:10, :10]\n        Z = np.sin(X * Y)\n\n        fig, ax = plt.subplots()\n        im = ax.pcolormesh(X, Y, Z)\n\n        # Check that the extent is set correctly\n        assert im.get_extent() == (0, 10, 0, 10)\n\n        # Check that updating the data updates the image\n        Z[::2, ::2] = 0\n        im.set_array(Z)\n        assert_array_equal(im.get_data(),\n                           Z)\n"], "sample_256": ["    def test_username_field_widget_attributes(self):\n        form = UserCreationForm()\n        username_field = form.fields['username']\n        self.assertEqual(username_field.widget.attrs['autocapitalize'], 'none')\n        self.assertEqual(username_field.widget.attrs['autocomplete'], 'username')\n", "    def test_creation_form_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.custom_user.CustomUser'):\n            form = UserCreationForm()\n            self.assertFalse(form.fields['username'].required)\n\n", "    def test_create_user_with_integer_username(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.with_integer_username.IntegerUsernameUser'):\n            form_data = {'username': 123, 'password1': 'password', 'password2': 'password'}\n            form = UserCreationForm(data=form_data)\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertIsInstance(user, IntegerUsernameUser)\n            self.assertEqual(user.username, '123')\n            self.assertTrue(user.check_password('password'))\n", "    def test_creation_form_with_custom_user_model(self):\n        with override_settings(AUTH_USER_MODEL='tests.models.custom_user.CustomUser'):\n            form = UserCreationForm()\n            self.assertEqual(form.fields['username'].widget.attrs['autofocus'], True)\n            form.data = {'username': 'newuser', 'password1': 'password', 'password2': 'password'}\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertIsInstance(user, CustomUser)\n", "    def test_creation_with_custom_email_field(self):\n        email = 'testclient@example.com'\n        with self.settings(AUTH_USER_MODEL='tests.models.with_custom_email_field.CustomEmailFieldUser'):\n            form = UserCreationForm({'username': 'testclient', 'email': email,\n                                   'password1': 'password', 'password2': 'password'})\n            self.assertTrue(form.is_valid())\n            user = form.save()\n            self.assertEqual(user.email, email)\n"], "sample_861": ["def test_grid_iterator_error(klass, input, error_type, error_message):\n    with pytest.raises(error_type, match=error_message):\n        klass(input)\n\n", "    def test_parameter_sampler_sparse_matrix(self):\n        param_dist = {'X': [sp.csc_matrix(X), sp.csr_matrix(X)]}\n        sampler = ParameterSampler(param_dist, n_iter=2, random_state=0)\n        for param in sampler:\n            assert isinstance(param['X'], (sp.csc_matrix, sp.csr_matrix))\n", "    def test_parameter_sampler_error_messages(self, klass):\n        with pytest.raises(error_type) as excinfo:\n            klass(input)\n        assert re.search(error_message, str(excinfo.value))\n", "    def test_gridsearch_return_train_score(self, klass, input, error_type,\n                                           error_message, return_train_score):\n        # Test for issue #10678:\n        # GridSearchCV should accept 'return_train_score' argument\n        if error_type is not None:\n            with pytest.raises(error_type, match=error_message):\n                GridSearchCV(estimator=SVC(), param_grid=input,\n                             return_train_score=return_train_score)\n        else:\n            GridSearchCV(estimator=SVC(), param_grid=input,\n                         return_train_score=return_train_score)\n", "    def test_parameter_sampler_sparse_matrix(self):\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        param_dist = {'X': [X]}\n        sampler = ParameterSampler(param_dist, n_iter=1)\n        params = list(sampler)\n        assert isinstance(params[0]['X'], sp.csr_matrix)\n\n"], "sample_389": ["    def test_settings_modification(self):\n        self.assertEqual(settings.ITEMS, [\"b\", \"c\", \"d\"])\n        self.assertEqual(settings.ITEMS_OUTER, [1, 2, 3])\n        self.assertEqual(settings.TEST, \"override\")\n        self.assertEqual(settings.TEST_OUTER, \"outer\")\n", "    def test_override_settings(self):\n        self.assertEqual(settings.ITEMS, [\"b\", \"c\", \"d\"])\n        self.assertEqual(settings.ITEMS_OUTER, [1, 2, 3])\n        self.assertEqual(settings.TEST, \"override\")\n        self.assertEqual(settings.TEST_OUTER, \"outer\")\n", "    def test_modify_settings_and_override_settings(self):\n        self.assertEqual(settings.ITEMS, [\"b\", \"c\", \"d\"])\n        self.assertEqual(settings.ITEMS_OUTER, [1, 2, 3])\n        self.assertEqual(settings.TEST, \"override\")\n        self.assertEqual(settings.TEST_OUTER, \"outer\")\n", "    def test_modify_settings_and_override_settings(self):\n        self.assertEqual(settings.ITEMS, [\"b\", \"c\", \"d\"])\n        self.assertEqual(settings.ITEMS_OUTER, [1, 2, 3])\n        self.assertEqual(settings.TEST, \"override\")\n        self.assertEqual(settings.TEST_OUTER, \"outer\")\n", "    def test_settings_modification(self):\n        self.assertEqual(settings.ITEMS, ['b', 'c', 'd'])\n        self.assertEqual(settings.ITEMS_OUTER, [1, 2, 3])\n        self.assertEqual(settings.TEST, 'override')\n        self.assertEqual(settings.TEST_OUTER, 'outer')\n"], "sample_588": ["    def test_combine_nested_with_empty_datasets(self):\n        empty_ds = Dataset()\n        full_ds = create_test_data(name='test', data_vars={'a': (('x', 'y'), np.arange(6).reshape((2, 3)))})\n\n        # Test combining with empty list\n        combined = combine_nested([], concat_dim=['x', 'y'])\n        assert_identical(combined, Dataset())\n\n        # Test combining with list containing an empty dataset\n        combined = combine_nested([empty_ds, full_ds], concat_dim=['x', 'y'])\n        assert_identical(combined, full_ds)\n\n        # Test combining with list containing full dataset first, then empty\n        combined = combine_nested([full_ds, empty_ds], concat_dim=['x', 'y'])\n        assert_identical(combined, full_ds)\n", "    def test_infer_concat_order_from_coords(self):\n        ds1 = Dataset({'foo': (['x'], [1, 2], {'units': 'm'}),\n                       'bar': (['x'], [3, 4])})\n        ds2 = Dataset({'foo': (['x'], [5, 6], {'units': 'm'}),\n                       'bar': (['x'], [7, 8])})\n        ds3 = Dataset({'foo': (['x'], [9, 10], {'units': 'm'}),\n                       'bar': (['x'], [11, 12])})\n\n        combined_ids, concat_dims = _infer_concat_order_from_coords([ds1, ds2, ds3])\n", "    def test_infer_concat_order_from_positions_with_multiple_dimensions(\n        self,", "def test_combine_nested_merge_with_conflicts():\n    # Test combining nested Datasets where some variables have conflicts\n    # that can be resolved using merge\n\n    ds1 = Dataset({'temperature': (('x', 'y'), [[1, 2], [3, 4]]),\n                   'pressure': (('x', 'y'), [[5, 6], [7, 8]])})\n    ds2 = Dataset({'temperature': (('x', 'y'), [[9, 10], [11, 12]]),\n                   'humidity': (('x', 'y'), [[13, 14], [15, 16]])})\n\n    expected = Dataset(\n        {'temperature': (('x', 'y'), [[1, 2], [3, 4], [9, 10], [11, 12]]),\n         'pressure': (('x', 'y'), [[5, 6], [7, 8]]),\n         'humidity': (('x', 'y'), [[13, 14], [15, 16]])}\n    )\n\n    combined = combine_nested(\n        [[ds1, ds2]],\n        concat_dims=['x'],\n    )\n    assert_identical(combined, expected)\n", "    def test_combine_nested_with_dict_of_datasets(self):\n        ds1 = Dataset({'foo': (('x', 'y'), np.arange(6).reshape(2, 3)), 'bar': 0})\n        ds2 = Dataset(\n            {'foo': (('x', 'y'), np.arange(6, 12).reshape(2, 3)), 'bar': 1}\n        )\n        ds_grid = {'a': ds1, 'b': ds2}\n        expected = Dataset({'foo': (('a', 'x', 'y'), np.array([np.arange(6).reshape(2, 3), np.arange(6, 12).reshape(2,3)])),\n                           'bar': (('a',), np.array([0, 1]))})\n        combined = combine_nested(ds_grid, concat_dim=['a'])\n        assert_identical(combined, expected)\n"], "sample_937": ["compilation error", "compilation error", "    def test_parse_with_type_comments(self):\n        code = \"\"\"\n            return a\n        \"\"\"\n        tree = ast.parse(code, mode='exec', type_comments=True)\n        assert isinstance(tree, ast.Module)\n        assert isinstance(tree.body[0], ast.FunctionDef)\n        assert isinstance(tree.body[0].args.args[0].annotation, ast.Name)\n        assert tree.body[0].args.args[0].annotation.id == 'int'\n", "compilation error", "compilation error"], "sample_793": ["    def test_contamination_auto_behaviour_new():\n        # test if contamination parameter is correctly handled in the\n        # new behaviour when contamination == \"auto\".\n\n        X = iris.data\n        y = iris.target\n\n        # make sure contamination parameter is correctly handled in the\n        # new behavior when contamination == \"auto\".\n        iforest = IsolationForest(contamination=\"auto\", behaviour='new')\n        iforest.fit(X)\n        assert_equal(iforest.offset_, -0.5)\n        assert_allclose(np.percentile(iforest.score_samples(X),\n                                      100. * iforest._contamination),\n                        iforest.offset_)\n\n", "def test_iforest_sparse_input():\n    # Check that IsolationForest works with sparse matrices.\n    X = csc_matrix(boston.data)\n    y = boston.target\n    clf = IsolationForest(random_state=0)\n    clf.fit(X, y)\n    assert clf.predict(X).shape == (len(y),)\n\n", "    def test_iforest_sparse_csc(self):\n        # Test IsolationForest with sparse input in CSC format.\n        X_csc = csc_matrix(boston.data)\n        clf = IsolationForest(random_state=0, verbose=0).fit(X_csc)\n        pred = clf.predict(X_csc)\n        assert_equal(pred.shape[0], boston.data.shape[0])\n\n\n\n", "    def test_iforest_sparse_input(self):\n        X_train, X_test = train_test_split(csc_matrix(boston.data),\n                                         test_size=0.2, random_state=0)\n\n        clf = IsolationForest(random_state=0).fit(X_train)\n        clf.predict(X_test)\n        assert_equal(clf.score_samples(X_test).shape, (X_test.shape[0],))\n", "    def test_sparse_input(self):\n        # Test Isolation Forest with sparse input\n        X_sparse = csc_matrix(boston.data)\n        clf = IsolationForest(random_state=rng)\n        clf.fit(X_sparse)\n\n        # Check predictions\n        y_pred = clf.predict(X_sparse)\n        assert y_pred.shape[0] == boston.data.shape[0]\n        assert_equal(set(y_pred), {-1, 1})\n\n"], "sample_299": ["    def test_check_cache_location_not_exposed_no_exposure(self):\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/var/tmp/cache',\n                },\n            },\n            MEDIA_ROOT='/var/www/media',\n            STATIC_ROOT='/var/www/static',\n            STATICFILES_DIRS=[\n                '/var/www/staticfiles',\n            ],\n        ):\n            self.assertEqual(\n                check_cache_location_not_exposed({}, None),\n                []\n            )\n", "    def test_check_cache_location_not_exposed(self):\n        with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION, STATIC_ROOT='path/to/static'):\n            self.assertEqual(\n                check_cache_location_not_exposed(None, **{}),\n                [],\n            )\n\n        with override_settings(\n            CACHES=self.VALID_CACHES_CONFIGURATION,\n            STATIC_ROOT='path/to/static',\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': 'path/to/static/cache',\n                }\n            },\n        ):\n            self.assertEqual(\n                check_cache_location_not_exposed(None, **{}),\n                [Warning(\n                    \"Your 'default' cache configuration might expose your cache or lead to corruption of your data because its LOCATION matches STATIC_ROOT.\",\n                    id='caches.W002',\n                )],\n            )\n\n\n\n", "    def test_check_cache_location_not_exposed(self):\n        with override_settings(CACHES=self.VALID_CACHES_CONFIGURATION, MEDIA_ROOT='path/to/media', STATIC_ROOT='path/to/static'):\n            errors = check_cache_location_not_exposed([], **{})\n            self.assertEqual(len(errors), 0)\n\n        with override_settings(CACHES={'default': {'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache', 'LOCATION': 'path/to/media'}}, MEDIA_ROOT='path/to/media'):\n            errors = check_cache_location_not_exposed([], **{})\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Warning)\n\n", "    def test_check_cache_location_not_exposed(self):\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/tmp/mycache',\n                },\n                'other': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/var/www/myproject/staticfiles',\n                },\n            },\n            STATIC_ROOT='/var/www/myproject/staticfiles',\n        ):\n\n            errors = check_cache_location_not_exposed(None, **{})\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], Warning)\n            self.assertEqual(errors[0].id, 'caches.W002')\n", "    def test_check_cache_location_not_exposed(self):\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/tmp/cache',\n                },\n                'other': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/var/www/static',\n                },\n            },\n            STATIC_ROOT='/var/www/static',\n        ):\n            warnings = check_cache_location_not_exposed(None, **{})\n            self.assertEqual(len(warnings), 1)\n            self.assertIsInstance(warnings[0], Warning)\n\n        with override_settings(\n            CACHES={\n                'default': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/tmp/cache',\n                },\n                'other': {\n                    'BACKEND': 'django.core.cache.backends.filebased.FileBasedCache',\n                    'LOCATION': '/var/www/static/uploads',\n                },\n            },\n            STATIC_ROOT='/var/www/static',\n        ):\n            warnings = check_cache_location_not_exposed(None, **{})\n            self.assertEqual(len(warnings), 1)\n            self.assertIsInstance(warnings[0], Warning)\n"], "sample_956": ["    def test_resolve_reference_any_inventory_disabled_reftype_wildcard(\n            self, mock_read_from_url, mock_inventory_file):\n\n        app = self.app\n        set_config(app, {'proj': ('https://example.org/objects.inv',)})\n\n        mock_read_from_url.return_value = io.BytesIO(inventory_v2)\n        with http_server(self.app, 'https://example.org/'):\n            result = reference_check(app,\n                                        'py', 'func', 'my_func', refdomain='py')\n\n            assert result is not None\n\n            result = reference_check(app,\n                                        'std', 'term', 'my_term', refdomain='std')\n            assert result is None\n\n            app.config.intersphinx_disabled_reftypes = ['*']\n            result = reference_check(app,\n                                        'py', 'func', 'my_func', refdomain='py')\n            assert result is None\n", "    def test_resolve_reference_detect_inventory_with_named_inventory(self, mock_read,\n                                                              MockInventory):\n        self.app.builder.env.named_inventory = { 'myproj': MockInventory() }\n        set_config(self.app, {'myproj': ('https://example.org/inv', [INVENTORY_FILENAME])})\n        self.app.builder.env.config.intersphinx_mapping['myproj'] = ('myproj', ('https://example.org/inv',\n                                                                                 [INVENTORY_FILENAME]))\n        result = reference_check(self.app, 'std', 'func', 'myfunc', refdomain='myproj',\n                                 refname='myfunc')\n        assert isinstance(result, nodes.reference)\n        assert result['refuri'] == 'https://example.org/inv/myfunc.html'\n\n", "def test_missing_reference_disabled_reftype(app, read_from_url, InventoryFile):\n    set_config(app, {'std': 'http://example.com/docs'})\n    app.config.intersphinx_disabled_reftypes = ['std:*']\n\n    read_from_url.side_effect = lambda url: http_server.open_file(inventory_v2)\n    InventoryFile.load.return_value = inventory_v2\n\n    assert reference_check(app, 'std', 'func', 'foo') is None\n    assert reference_check(app, 'std', 'class', 'Foo') is None\n\n    app.config.intersphinx_disabled_reftypes = ['std:func']\n    assert reference_check(app, 'std', 'func', 'foo') is None\n    assert reference_check(app, 'std', 'class', 'Foo') is not None\n", "    def test_missing_reference_disabled_reftypes_any(self, mock_read_from_url, mock_inventory_file):\n        app = self.app\n        set_config(app, {'test': ('http://example.com', ['objects.inv'])})\n        mock_read_from_url.return_value = mock.Mock()\n        mock_read_from_url.return_value.read.return_value = b'{\"std:function\": {\"foo\": [\"test\", \"1.0\", \"http://example.com/foo\", \"-\"]}}'\n\n        # Test if a reftype in the disabled set is correctly ignored\n        app.config.intersphinx_disabled_reftypes = ['std:function']\n        node = reference_check(app, \"std\", \"function\", \"foo\")\n        assert node is None\n", "    def test_resolve_reference_detect_inventory_target_with_invalid_reference(self, mock_read_from_url, mock_inventory_file):\n        app = self.app\n        set_config(app, {'myproject': ('https://docproject.org/inv',)})\n\n        # Invalid reference\n        node, contnode = fake_node('myproject', 'func',\n                                  'invalid.reference', 'invalid.reference')\n        result = reference_check(app, node, contnode)\n        assert result is None\n"], "sample_1076": ["    def test_sinc(self):\n        expr = sympy.sinc(x)\n        code = SymPyPrinter().doprint(expr)\n        self.assertEqual(code, 'sympy.sin(sympy.pi*x)/(sympy.pi*x)')\n\n", "    def test_SparseMatrix(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        code = SciPyPrinter().doprint(A)\n        assert isinstance(code, str)\n        assert 'scipy.sparse.coo_matrix' in code\n", "    def test_python_code_blockmatrix(self):\n        A = SparseMatrix([[1, 2], [3, 4]])\n        B = SparseMatrix([[5, 6], [7, 8]])\n        block = BlockMatrix([[A, B]])\n        self.assertEqual(pycode(block, standard='python3'),\n                         'BlockMatrix([[SparseMatrix([[1, 2], [3, 4]]), SparseMatrix([[5, 6], [7, 8]])]])')\n", "    def test_scipy_special_functions(self):\n        from scipy.special import erf, erfc\n        assert pycode(erf(x), language='scipy') == \"scipy.special.erf(%s)\" % x\n\n        assert pycode(erfc(x), language='scipy') == \"scipy.special.erfc(%s)\" % x\n\n        assert pycode(sign(x), language='scipy') == \"numpy.sign(%s)\" % x\n\n\n", "    def test_print_MatrixSolve(self):\n        A = MatrixSymbol('A', 2, 2)\n        b = MatrixSymbol('b', 2, 1)\n        expr = MatrixSolve(A, b)\n        self.assertEqual(NumPyPrinter().doprint(expr), 'numpy.linalg.solve(A, b)')\n"], "sample_181": ["    def test_aggregate_annotation(self):\n        # Test that annotations can be aggregated.\n        with Approximate(self.b1.rating):\n            avg_rating = Book.objects.annotate(avg_rating=Avg('rating')).values('avg_rating').first()['avg_rating']\n        self.assertEqual(avg_rating, 4.0)\n\n", "    def test_annotate_aggregate_with_when(self):\n        # Test annotating with an aggregate function that uses a When clause\n        qs = Book.objects.annotate(\n            avg_rating_when_price_gt_25=Case(\n                When(price__gt=Decimal('25.00'), then=Avg('rating')),\n                default=0,\n                output_field=DecimalField(),\n            ),\n        )\n        self.assertEqual(qs.get(isbn='159059725').avg_rating_when_price_gt_25, 4.5)\n        self.assertEqual(qs.get(isbn='067232959').avg_rating_when_price_gt_25, 0)\n\n", "    def test_filtered_aggregate_with_subquery(self):\n        subquery = Book.objects.filter(contact__name='test').values('contact__id').annotate(Count('id'))\n        result = Author.objects.annotate(\n            book_count=Count('book'),\n            friends_with_books=Count('friends__book'),\n            friends_with_many_books=Count(\n                Case(\n                    When(friends__book__id__in=subquery, then=1),\n                    default=0, output_field=IntegerField(),\n                )\n            )\n        ).filter(book_count__gt=1)\n        self.assertEqual(result.count(), 1)\n        self.assertEqual(result[0].book_count, 2)\n        self.assertEqual(result[0].friends_with_books, 2)\n        self.assertEqual(result[0].friends_with_many_books, 2)\n\n", "    def test_filtered_aggregate_with_subquery(self):\n        # Test using a subquery for filtering a count.\n        subquery = Book.objects.filter(\n            Q(price__gt=Decimal('25')) | Q(rating__lte=3)\n        ).values('contact').annotate(count=Count('contact'))\n        result = Author.objects.annotate(\n            book_count=Count('book_set'),\n            filtered_count=Subquery(\n                subquery.filter(contact=OuterRef('pk')).values('count'),\n                output_field=models.IntegerField()\n            )\n\n        ).get(pk=self.a1.pk)\n        self.assertEqual(result.book_count, 2)\n        self.assertEqual(result.filtered_count, 1)\n", "    def test_aggregate_with_subquery(self):\n        subquery = Book.objects.filter(price__gt=F('rating')).values('contact')\n        qs = Author.objects.annotate(\n            num_books=Count('book'),\n            high_priced_books=Count(Case(When(pk__in=subquery, then=1), default=0)),\n        ).filter(num_books__gt=1)\n        self.assertQuerysetEqual(qs, [\n            ('test', 2, 1), ('test3', 2, 1)\n        ], lambda a: (a.name, a.num_books, a.high_priced_books))\n\n"], "sample_941": ["    def test_restify_forwardref():\n        assert restify(ForwardRef('MyClass')) == ':class:`MyClass`'\n", "    def test_restify_forwardref(self):\n        f = ForwardRef('MyClass')\n        assert restify(f) == ':class:`MyClass`'\n", "    def test_restify_newtype():\n        assert restify(MyInt) == ':obj:`MyInt`'\n", "    def test_restify_union_with_optional():\n        assert restify(Union[MyClass1, NoneType]) == ':obj:`Optional`\\\\ [:class:`MyClass1`]'\n        assert restify(Union[NoneType, MyClass1]) == ':obj:`Optional`\\\\ [:class:`MyClass1`]'\n        assert (restify(Union[MyClass1, MyClass2, NoneType]) ==\n                ':obj:`Optional`\\\\ [:class:`Union`\\\\ [:class:`MyClass1`, :class:`MyClass2`]]')\n", "    def test_restify_forwardref():\n        assert restify(ForwardRef('module.MyClass')) == ':class:`module.MyClass`'\n"], "sample_922": ["def test_parse_signature_with_defaults():\n    signode = parse('func(a, b=None, c=1)')\n    assert_node(signode[0], desc_name, 'func')\n    assert_node(signode[1], desc_parameterlist)\n    assert_node(signode[1][0], desc_parameter, 'a')\n    assert_node(signode[1][1], desc_parameter, 'b = None')\n    assert_node(signode[1][2], desc_parameter, 'c = 1')\n\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('str') == ('str', None)\n    assert _parse_annotation('list[int]') == ('list', ('int',))\n    assert _parse_annotation('dict[str, int]') == ('dict', ('str', 'int'))\n    assert _parse_annotation('Optional[str]') == ('Optional', ('str',))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', ('int', Ellipsis))\n    assert _parse_annotation('Callable[[int, str], bool]') == ('Callable', (['int', 'str'], 'bool'))\n    assert _parse_annotation('Type[T]') == ('Type', ('T',))\n    assert _parse_annotation('Any') == ('Any', None)\n", "    def test_parse_annotation(self):\n        assert _parse_annotation('int') == ('int', None)\n        assert _parse_annotation('List[str]') == ('List', 'str')\n        assert _parse_annotation('Dict[str, int]') == ('Dict', ('str', 'int'))\n        assert _parse_annotation('Callable[[int, str], float]') == ('Callable',\n                                                                  (['int', 'str'], 'float'))\n        assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n        assert _parse_annotation('Optional[int]') == ('Optional', 'int')\n        assert _parse_annotation('Tuple[int, ...] ') == ('Tuple', ('int', Ellipsis))\n        assert _parse_annotation('Tuple[int, str, ...]') == ('Tuple', ('int', 'str', Ellipsis))\n\n", "def test_parse_annotation():\n    assert _parse_annotation(\"foo: int\") == (\"foo\", \"int\")\n    assert _parse_annotation(\"foo: typing.List[str]\") == (\"foo\", \"typing.List[str]\")\n    assert _parse_annotation(\"foo: Union[int, str]\") == (\"foo\", \"Union[int, str]\")\n    assert _parse_annotation(\"foo\") == (\"foo\", None)\n    assert _parse_annotation(\"foo:\") == (\"foo\", None)\n", "def test_parse_annotation():\n    assert _parse_annotation('int') == ('int', None)\n    assert _parse_annotation('list[str]') == ('list', 'str')\n    assert _parse_annotation('dict[str, int]') == ('dict', ('str', 'int'))\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', (int, '...'))\n    assert _parse_annotation('Union[int, str]') == ('Union', ('int', 'str'))\n    assert _parse_annotation('Optional[int]') == ('Optional', ('int',))\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', '[[int], str]')\n    assert _parse_annotation('Any') == ('Any', None)\n"], "sample_221": ["    def test_prefetch_related(self):\n        g1 = Group.objects.create(name='Group 1')\n        g2 = Group.objects.create(name='Group 2')\n        e1 = Event.objects.create(name='Event 1', group=g1)\n        e2 = Event.objects.create(name='Event 2', group=g2)\n        e3 = Event.objects.create(name='Event 3', group=g1)\n\n        self.assert_pickles(Group.objects.prefetch_related('event_set'))\n        self.assert_pickles(Group.objects.prefetch_related('event_set').all())\n", "    def test_prefetch_related_ordering(self):\n        group = Group.objects.create(name=\"Test Group\")\n        event1 = Event.objects.create(name=\"Event 1\", group=group)\n        Event.objects.create(name=\"Event 2\", group=group)\n        container = Container.objects.create(name=\"Test Container\", groups=[group])\n\n        # Test ordering when prefetching related objects\n        ordered_events = Container.objects.prefetch_related('groups__event_set').get(\n            name=\"Test Container\"\n        ).groups.order_by('event_set__name')\n        self.assertEqual([e.name for e in ordered_events[0].event_set.all()],\n                         ['Event 1', 'Event 2'])\n\n", "    def test_pickleability_prefetching(self):\n        container = Container.objects.create()\n        container.events.create()\n        container.groups.create()\n        qs = Container.objects.prefetch_related('events', 'groups').all()\n        self.assert_pickles(qs)\n", "    def test_pickle_prefetch_related(self):\n        group = Group.objects.create()\n        event1 = Event.objects.create(group=group)\n        event2 = Event.objects.create(group=group)\n\n        MyEvent.objects.create(event=event1)\n        MyEvent.objects.create(event=event2)\n\n        qs = Event.objects.prefetch_related('myevent_set').all()\n\n        self.assert_pickles(qs)\n", "    def test_m2m_select_related(self):\n        g1 = Group.objects.create(name='Group 1')\n        g2 = Group.objects.create(name='Group 2')\n        m = M2MModel.objects.create(name='M2M Model')\n        m.groups.add(g1, g2)\n\n        qs = M2MModel.objects.select_related('groups').filter(pk=m.pk)\n        self.assert_pickles(qs)\n        self.assertEqual(len(qs), 1)\n        m2m_model = qs.get()\n        self.assertEqual(m2m_model.name, 'M2M Model')\n        self.assertEqual(len(m2m_model.groups.all()), 2)\n"], "sample_1042": ["def test_idx_bounds():\n    i = Idx('i', (0, 3))\n    assert i.lower == 0\n    assert i.upper == 3\n\n    j = Idx('j', 5)\n    assert j.lower == 0\n    assert j.upper == 4\n\n    k = Idx('k')\n\n    assert k.lower is None\n    assert k.upper is None\n", "    def test_indexed_free_symbols(self):\n        x, y = symbols('x y')\n        i = Idx('i')\n        j = Idx('j', (0, x-1))\n        A = IndexedBase('A')\n        B = IndexedBase('B', shape=(x, y))\n        assert A[i].free_symbols == {A, i}\n        assert B[i, j].free_symbols == {A, B, i, j, x}\n\n\n", "def test_indexedbase_shape_with_indices():\n    i = Idx('i', 3)\n    j = Idx('j', 4)\n    A = IndexedBase('A', shape=(2, 2))\n    assert A[i, j].shape == (2, 2)\n\n", "    def test_indexed_free_symbols():\n        i = Idx('i')\n        j = Idx('j', 5)\n        x = symbols('x')\n        a = IndexedBase('a')\n        b = Indexed('b', i)\n        c = Indexed('c', i, j)\n        assert a.free_symbols == {a}\n        assert b.free_symbols == {a, i}\n        assert c.free_symbols == {a, i, j}\n        assert (a[i]).free_symbols == {a, i}\n        assert Indexed(x, i).free_symbols == {x, i}\n\n", "    def test_Idx_comparison_with_symbols():\n        i = Idx('i', 5)\n        j = Idx('j', 3)\n        k = Symbol('k')\n\n        assert i > j\n        assert i >= j\n\n        assert i < k\n        assert i <= k\n\n        assert j < k\n        assert j <= k\n\n        \n"], "sample_740": ["    def test_check_X_y_multioutput_sparse(self):\n        X = sparse_random_matrix(10, 5, density=0.5)\n        y = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10],\n                      [11, 12], [13, 14], [15, 16], [17, 18], [19, 20]])\n        X, y = check_X_y(X, y, multi_output=True)\n        assert_true(sp.issparse(X))\n        assert_equal(y.shape, (10, 2))\n", "    def test_check_array_object_dtype(self):\n        # Test check_array with a mix of numeric and non-numeric data types\n        data = np.array([1, 2, 'a'], dtype=object)\n\n        # Check if warning is raised when dtype is 'numeric'\n        with pytest.warns(DataConversionWarning):\n            check_array(data, dtype='numeric')\n\n        # Check if array is converted to float64 by default\n        assert_array_equal(check_array(data), np.array([1., 2., np.nan]))\n", "    def test_check_X_y_pandas_dataframe():\n        # check if Pandas DataFrame raises error when y is also a DataFrame\n        df = MockDataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})\n\n        with pytest.raises(ValueError):\n            check_X_y(df, df)\n\n        # check if Pandas DataFrame raises error\n        # when dtype is object\n        df = MockDataFrame({'col1': [1, 2, 3], 'col2': ['a', 'b', 'c']})\n        with pytest.raises(ValueError):\n            check_X_y(df, df['col1'])\n\n", "    def test_check_array_dtype_object_numeric():\n        # Test that check_array correctly handles dtype='numeric'\n        # when the input array has dtype='object'\n\n        X = np.array([1, 2, 3], dtype=object)\n        with warnings.catch_warnings(record=True) as w:\n            X_converted = check_array(X, dtype='numeric')\n        assert_equal(X_converted.dtype, np.float64)\n        assert_equal(len(w), 1)\n        assert(issubclass(w[-1].category, FutureWarning))\n\n", "    def test_check_array_dtype_object():\n        # Test dtype conversion\n        X = np.array(['1', '2', '3'], dtype=object)\n        X_converted = check_array(X, dtype=np.float64)\n        assert_array_equal(X_converted, np.array([1, 2, 3], dtype=np.float64))\n\n        # Test dtype preservation\n        X = np.array([1, 2, 3], dtype=np.int32)\n        X_converted = check_array(X, dtype=np.int32)\n        assert_array_equal(X_converted, X)\n\n        # Test dtype preservation with list input\n        X = [1, 2, 3]\n        X_converted = check_array(X, dtype=np.int32)\n        assert_array_equal(X_converted, np.array([1, 2, 3], dtype=np.int32))\n\n        # Test dtype conversion with a list of dtypes\n        X = np.array(['1', '2', '3'], dtype=object)\n        X_converted = check_array(X, dtype=[np.float64, int])\n        assert_array_equal(X_converted, np.array([1, 2, 3], dtype=np.float64))\n"], "sample_241": ["    def test_isnull_expression(self):\n        self.assertQuerysetEqual(\n            Company.objects.filter(ceo__isnull=False),\n            ['Example Inc.', 'Foobar Ltd.', 'Test GmbH']\n        )\n        self.assertQuerysetEqual(\n            Company.objects.filter(ceo__isnull=True),\n            []\n        )\n", "    def test_expression_wrapper(self):\n        # Test that ExpressionWrapper works with a lambda function\n        query = self.company_query.annotate(\n            double_chairs=ExpressionWrapper(\n                lambda value: value * 2,\n                output_field=IntegerField(),\n            )\n        )\n        expected_results = [\n            {\n                \"name\": \"Example Inc.\",\n                \"num_employees\": 2300,\n                \"num_chairs\": 5,\n                \"double_chairs\": 10,\n            },\n            {\n                \"name\": \"Foobar Ltd.\",\n                \"num_employees\": 3,\n                \"num_chairs\": 4,\n                \"double_chairs\": 8,\n            },\n            {\n                \"name\": \"Test GmbH\",\n                \"num_employees\": 32,\n                \"num_chairs\": 1,\n                \"double_chairs\": 2,\n            },\n        ]\n        self.assertQuerysetEqual(query, expected_results, transform=lambda x: x)\n\n", "    def test_f_expression_in_subquery(self):\n        subquery = Result.objects.filter(\n            experiment__simulationrun__name='Simulation A',\n            value__gt=F('experiment__simulationrun__start_time')\n        ).values('value')\n        qs = Experiment.objects.filter(\n            simulationrun__in=Subquery(subquery)\n        )\n        self.assertQuerysetEqual(qs, Experiment.objects.filter(\n            simulationrun__name='Simulation A'),\n            transform=lambda o: o.id)\n", "    def test_expression_list_length(self):\n        self.assertEqual(len(ExpressionList()), 0)\n        expr_list = ExpressionList([\n            F(\"name\"), F(\"num_employees\"), F(\"num_chairs\")\n        ])\n        self.assertEqual(len(expr_list), 3)\n\n", "    def test_coalesce_expression(self):\n        with CaptureQueriesContext(connection) as queries:\n            # Test Coalesce with different input types\n            result = Company.objects.annotate(\n                num_chairs_or_employees=Coalesce(F('num_chairs'), F('num_employees'))\n            ).values_list('num_chairs_or_employees', flat=True)\n            self.assertEqual(\n                list(result),\n                [\n                    5,\n                    4,\n                    1,\n                ],\n            )\n        self.assertEqual(len(queries), 1)\n"], "sample_627": ["def test_concat_empty_datasets():\n    datasets = [\n        Dataset({'a': ('x', np.array([1, 2]))}),\n        Dataset({'b': ('y', np.array([3, 4, 5]))}),\n        Dataset(),\n        Dataset({'c': ('x', np.array([6, 7]))}),\n    ]\n    expected = Dataset({'a': ('x', np.array([1, 2])), 'b': ('y', np.array([3, 4, 5])), 'c': ('x', np.array([6, 7]))})\n    actual = concat(datasets, dim='concat_dim')\n    assert_identical(actual, expected)\n", "    def test_concat_different_dims():\n        datasets = create_concat_datasets(num_datasets=3, include_day=False)\n        datasets[0] = datasets[0].rename({\"day\": \"time\"})\n        datasets[1] = datasets[1].rename({\"day\": \"new_dim\"})\n\n        with pytest.raises(ValueError):\n            concat(datasets, dim=\"time\")\n\n\n", "    def test_concat_different_coords(self) -> None:\n        datasets = create_concat_datasets(num_datasets=3, seed=42, include_day=False)\n        datasets[1] = datasets[1].assign_coords(day= [\"day1\", \"day2\"])\n        datasets[2] = datasets[2].assign_coords(day= [\"day3\", \"day4\"])\n\n        result = concat(datasets, dim=\"day\", coords=\"minimal\")\n        assert_identical(result, datasets[0])\n\n\n", "    def test_concat_datasets_with_different_coords():\n        datasets = create_concat_datasets(num_datasets=2, seed=42, include_day=False)\n        datasets[0] = datasets[0].assign_coords(\n            {\"day\": [\"day1\"], \"extra_coord\": [\"coord1\"]}\n        )\n        datasets[1] = datasets[1].assign_coords({\"day\": [\"day2\"], \"extra_coord2\": [\"coord2\"]})\n\n        with pytest.raises(ValueError):\n            concat(datasets, dim=\"day\")\n", "    def test_concat_coords_compat(self, compat, expected_result):\n        datasets = create_concat_datasets(num_datasets=2)\n        datasets[0] = datasets[0].assign_coords({\"day\": [\"day1\", \"day2\"]})\n        datasets[1] = datasets[1].assign_coords({\"day\": [\"day1\", \"day3\"]})\n        combined = concat(datasets, dim=\"day\", compat=compat)\n        assert len(combined[\"day\"]) == expected_result\n\n\n"], "sample_664": ["    def test_result_log_deprecation(recwarn):\n        with pytest.raises(deprecated.RESULT_LOG):\n            pytest.main([\"--result-log\", \"somefile\"])\n\n", "    def test_deprecated_fixture_positional_arguments(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                return request.param\n\n                assert arg1 == 1\n            \"\"\",\n        )\n        result = testdir.runpytest(\"--fixtures\", \"-v\")\n        result.assert_produces(\n            \"WARNING: Passing arguments to pytest.fixture() as positional arguments\"\n        )\n", "    def test_deprecated_funcargnames(self):\n        with pytest.deprecated_call(match=str(deprecated.FUNCARGNAMES)):\n            getattr(self.config, 'funcargnames', None)\n", "    def test_deprecated_external_plugins(self):\n        for plugin_name in deprecated.DEPRECATED_EXTERNAL_PLUGINS:\n            with pytest.raises(pytest.fail.Exception):\n                pytest.importorskip(plugin_name)\n", "    def test_funcargnames_deprecation(self):\n        with pytest.warns(deprecated.FUNCARGNAMES):\n            self.config._ensure_unconfigured().get_plugin('fixtures').funcargnames\n"], "sample_396": ["    def test_complex_filter(self):\n        # This test checks if we correctly handle filters that involve multiple\n        # joins and subqueries.\n        qs = (\n            Item.objects.filter(\n                Q(creator__name=\"a2\") & Q(note__misc=\"foo\")\n            )\n            .filter(created__gt=datetime.datetime(2007, 12, 19, 22, 0, 0))\n            .select_related(\"creator\")\n            .defer(\"note\")\n        )\n        self.assertQuerysetEqual(\n            qs, [self.i3], lambda i: [i.id, i.creator.id, i.name]\n        )\n", "    def test_complex_lookup(self):\n        # Test a combination of lookups and joins.\n        report_comments = ReportComment.objects.filter(\n            report__creator__name=\"a1\", content__icontains=\"comment\"\n        )\n        self.assertEqual(report_comments.count(), 1)\n\n", "    def test_ticket_22962(self):\n        # Test that `in_` lookups don't incorrectly include null values\n        qs = Item.objects.filter(tags__in=[self.t1, self.t3])\n        self.assertCountEqual(qs, [self.i1, self.i2])\n", "    def test_ticket_21203(self):\n        \"\"\"\n        Regression test for ticket #21203: ForeignKey not handled correctly when\n        joined against itself.\n        \"\"\"\n        parent1 = Ticket21203Parent.objects.create(name=\"Parent 1\")\n        parent2 = Ticket21203Parent.objects.create(name=\"Parent 2\")\n        child1 = Ticket21203Child.objects.create(parent=parent1, name=\"Child 1\")\n        child2 = Ticket21203Child.objects.create(parent=parent2, name=\"Child 2\")\n\n        self.assertQuerysetEqual(\n            Ticket21203Child.objects.filter(parent__name=\"Parent 1\"),\n            [child1],\n        )\n\n", "    def test_ticket_21203(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            # Create a parent object.\n            parent = Ticket21203Parent.objects.create(name=\"Foo\")\n\n        # Create some child objects related to the parent.\n        Ticket21203Child.objects.create(parent=parent, name=\"Bar\")\n        Ticket21203Child.objects.create(parent=parent, name=\"Baz\")\n\n        # Filter by the parent's related children.\n\n        parents = Ticket21203Parent.objects.filter(\n            child__name__startswith=\"B\"\n        ).distinct()\n        self.assertQuerysetEqual(parents, [parent])\n"], "sample_967": ["def test_mathjax_config_options(app, status, warning):\n    app.builder.build_all()\n    out = (app.outdir / 'index.html').read_text()\n    assert '<script type=\"text/x-mathjax-config\">' in out\n    assert '<script src=\"%s\">' % MATHJAX_URL in out\n    assert 'MathJax.Hub.Config' in out\n", "def test_mathjax_config(app, status, warning):\n    app.builder.math_renderer_name = 'mathjax'\n    app.config.mathjax_path = MATHJAX_URL\n    app.config.mathjax3_config = {'tex': {'macros': {'\\\\R': r'\\mathbb{R}'}}}\n    warnings.simplefilter('ignore', DeprecationWarning)\n    with warnings.catch_warnings(record=True) as w:\n        app.build()\n    assert len(w) == 0\n\n    html = (app.outdir / 'index.html').read_text()\n    assert r'window.MathJax = {\"tex\":{\"macros\":{\"\\\\R\":\"\\\\mathbb{R}\"}}}' in html\n\n", "    def test_mathjax_install(app, status, warning):\n        assert 'mathjax' in app.html_domain\n        assert app.config.html_extra_js is not None\n\n        # Check if MathJax script tag is added to the output HTML\n        html = app.builder.templates.render_html(app.config.templates[0],\n                                                context={'app': app})\n        assert '<script' in html\n        assert MATHJAX_URL in html\n        assert 'defer' in html\n", "def test_mathjax_config(app):\n    app.env.domains['math'].has_equations = lambda _: True\n    app.builder.math_renderer_name = 'mathjax'\n    app.config.mathjax_path = MATHJAX_URL\n    app.config.mathjax3_config = {'a': 1}\n\n    install_mathjax(app, 'index', 'index', {}, None)\n\n    assert 'window.MathJax = {\"a\": 1}' in app.builder.scripts.getvalue()\n\n", "    def test_mathjax_config_options(app, status, warning):\n        app.builder.build_all()\n        output = (app.outdir / 'index.html').read_text()\n        assert '<script src=\"' + MATHJAX_URL + '\" defer>' in output\n\n        # Test mathjax_options\n        app.config.mathjax_options = {'a': 1, 'b': 2}\n        app.builder.build_all()\n        output = (app.outdir / 'index.html').read_text()\n        assert '<script src=\"' + MATHJAX_URL + '\" defer=\"defer\" a=\"1\" b=\"2\">' in output\n        \n        # Test mathjax3_config\n        app.config.mathjax3_config = {'a': 1, 'b': 2}\n        app.builder.build_all()\n        output = (app.outdir / 'index.html').read_text()\n        assert 'window.MathJax = {\"a\": 1, \"b\": 2}' in output\n\n"], "sample_1207": ["    def test_parse_expr_with_null():\n        local_dict = {'null': 'something'}\n        expr = parse_expr('null', local_dict=local_dict)\n        assert isinstance(expr, Symbol)\n        assert expr.name == 'something'\n        assert 'null' in local_dict and local_dict['null'] == 'something'\n", "    def test_parse_expr_issue_15337():\n        # https://github.com/sympy/sympy/issues/15337\n        expr = parse_expr('(x + y)**2')\n        assert expr == (x + y)**2\n        assert expr.func == Pow\n\n", "    def test_parse_expr_implicit_multiplication_application_issue_21414(self):\n        s = '2x(x+2)'\n        self.assertEqual(\n            parse_expr(s, transformations=(standard_transformations +\n                                           (implicit_multiplication_application,))),\n            2*x*(x+2))\n\n\n\n", "def test_repeated_decimals_zero():\n    assert parse_expr('.0[1]') == Rational(1, 9)\n", "    def test_repeated_decimals_with_exponent():\n        assert parse_expr(\"0.1[3]\") == Rational(1, 3) + Rational(3, 30)\n        assert parse_expr(\"0.1[3] ** 2\") == (Rational(1, 3) + Rational(3, 30))**2\n"], "sample_81": ["    def test_regex_pattern_without_dollar(self):\n        pattern = RegexPattern(r'^foo/bar/')\n        self.assertEqual(pattern.regex.pattern, r'^foo/bar/')\n        match = pattern.match('foo/bar/baz')\n        self.assertEqual(match, ('baz', (), {}))\n\n", "    def test_regex_pattern_with_translator(self):\n        pattern = RegexPattern(r'^(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/$',\n                              name='date_pattern')\n        self.assertEqual(str(pattern), r'^\\d{4}/\\d{2}/\\d{2}/$')\n        self.assertEqual(pattern.name, 'date_pattern')\n        # Test with translation\n        with override_settings(LANGUAGE_CODE='fr'):\n            self.assertEqual(str(pattern), r'^\\d{4}/\\d{2}/\\d{2}/$')\n", "    def test_regex_pattern_with_custom_converters(self):\n        pattern = RegexPattern(r'^/(?P<year>\\d{4})/(?P<month>\\d{2})/(?P<day>\\d{2})/$',\n                              name='date_archive')\n        pattern.converters = {\n            'year': lambda value: int(value) + 1000,\n            'month': lambda value: int(value) + 10,\n            'day': lambda value: int(value) + 1,\n        }\n\n        match = pattern.match('/2023/01/01/')\n        self.assertEqual(match[0], '')\n        self.assertEqual(match[1], ())\n        self.assertEqual(match[2], {'year': 3023, 'month': 21, 'day': 2})\n", "    def test_regex_pattern_with_converter(self):\n        pattern = RegexPattern(r'^foo/(?P<pk>\\d+)/$', converters={'pk': int})\n        match = pattern.match('foo/123/')\n        self.assertEqual(match, ('', (), {'pk': 123}))\n\n", "            def test_regex_pattern_with_language_prefix(self):"], "sample_139": ["    def test_get_queryset_empty_list_display(self):\n        # Test that get_queryset returns an empty result set when list_display is empty\n        admin_site = AdminSite()\n        admin_site.register(Band, EmptyValueChildAdmin)\n        request = self._mocked_authenticated_request('/admin/admin_changelist/', self.superuser)\n        admin = EmptyValueChildAdmin(Band, admin_site)\n        cl = admin.get_changelist_instance(request)\n\n        # Ensure the queryset is empty\n        self.assertEqual(len(cl.result_list), 0)\n\n", "    def test_changelist_pagination(self):\n        # Create a few objects to paginate.\n        for i in range(25):\n            Band.objects.create(name=f'Band {i}')\n        \n        user = self.superuser\n\n        # Get the changelist view.\n        request = self._mocked_authenticated_request('/admin/admin_changelist/band/', user)\n        response = BandAdmin.changelist_view(request)\n\n        # The changelist should have pagination links.\n        self.assertInHTML('<a href=\"?page=2\">', response.content.decode())\n\n        # Check the correct number of objects per page.\n        self.assertEqual(len(response.context_data['cl'].result_list), 20)\n", "    def test_custom_pagination_admin(self):\n        admin_site = CustomAdminSite()\n        admin_site.register(Band, BandAdmin)\n        request = self._mocked_authenticated_request(reverse('admin:custom_site_band_changelist'), self.superuser)\n\n        response = admin_site.index(request)\n        self.assertIn('paginator', response.context)\n        custom_paginator = response.context['paginator']\n        self.assertIsInstance(custom_paginator, CustomPaginator)\n\n", "    def test_changelist_list_filter_links(self):\n        admin_url = reverse('admin:auth_user_changelist')\n        request = self._mocked_authenticated_request(admin_url, self.superuser)\n        response = self.client.get(admin_url)\n        self.assertContains(response, '<a href=\"?username__startswith=super\">super</a>')\n", "    def test_changelist_ordering_field_display(self):\n        # Check that ordering by a display field works correctly.\n        band1 = Band.objects.create(name='Band A')\n        band2 = Band.objects.create(name='Band B')\n        band3 = Band.objects.create(name='Band C')\n\n        # Make sure the bands are not already in alphabetical order\n        self.assertNotEqual(band1.name, 'Band A')\n\n        request = self._mocked_authenticated_request(reverse('admin:admin_changelist', args=('admin_changelist', 'band')),\n                                                      self.superuser)\n        response = BandAdmin.changelist_view(request)\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.context_data['cl'].queryset[0].name, 'Band A')\n\n\n\n"], "sample_947": ["def test_function_nested():\n    check('function',\n          \"\"\"\n          void func(\n              int a, \n              int b\n          )\n          \"\"\",\n          {1: 'func'},\n          \"\"\"\n          void func(int a, int b)\n          \"\"\",\n          key=\"func\")\n", "def test_function_with_return_type():\n    check(\n        'function', \n        \"int foo(int a, int b)\", \n        {1: \"foo\"}, \n        output =\"int foo(int a, int b)\",\n        asTextOutput =\"foo(a, b)\",\n    )\n", "def test_function_default_args():\n    check(\n        'function',\n        'void func(int a, int b = 5)',\n        {1: 'func'} ,\n        'void func(int a, int b = 5)',\n        asTextOutput='void func(a, b=5)'\n    )\n", "    def test_function_with_empty_parameter_list():\n        check('function', 'int foo ()', {1: 'foo'},\n              output='int foo()')\n\n", "    def test_c_enum_member(self):\n        input = \"\"\"\n            {key}VALUE1, VALUE2 = 3, VALUE3\n        \"\"\"\n        idDict = {1: 'VALUE1', 2: 'VALUE2', 3: 'VALUE3'}\n        output = \"\"\"\n            {key}VALUE1, VALUE2 = 3, VALUE3\n        \"\"\"\n        check('enum', input, idDict, output, key='MyEnum')\n"], "sample_319": ["    def test_circular_foreign_key_creation(self):\n        knight = Knight.objects.create()\n        rabbit = Rabbit.objects.create(knight=knight)\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=rabbit)\n\n", "    def test_knight_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n        with self.assertRaises(IntegrityError) as cm:\n            Rabbit.objects.create(knight=knight, parent=rabbit2)\n        self.assertIn(\"UNIQUE constraint failed\", str(cm.exception))\n", "    def test_unique_together_with_m2m_through(self):\n        # This test checks if unique_together constraint works with a model\n        # that has a ManyToManyField through another model.\n        book = otherapp.Book.objects.create(\n            title=\"Test Book\", author=testapp.Author.objects.create(name=\"Test Author\")\n        )\n        attribution = otherapp.Attribution.objects.create(\n            book=book, author=book.author\n        )\n        with pytest.raises(IntegrityError):\n            otherapp.Attribution.objects.create(\n                book=book, author=book.author\n            )\n", "    def test_unique_together_with_circular_foreign_key(self):\n        knight = Knight.objects.create()\n        with self.assertRaises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight.rabbit_set.create(knight=knight))\n\n", "    def test_rabbit_circular_fk(self):\n        knight = Knight.objects.create()\n\n        with pytest.raises(IntegrityError):\n            Rabbit.objects.create(knight=knight, parent=knight)\n\n        rabbit1 = Rabbit.objects.create(knight=knight)\n        rabbit2 = Rabbit.objects.create(knight=knight, parent=rabbit1)\n\n"], "sample_862": ["    def test_tfidfvectorizer_stop_words_string_deprecation():\n        # Check if warning is raised when using stop_words='english'\n        with pytest.warns(FutureWarning,\n                          match=\"The 'stop_words' parameter is deprecated\"):\n            TfidfVectorizer(stop_words='english')\n\n", "    def test_tfidfvectorizer_transform_empty_vocabulary(self):\n        # Test that TfidfVectorizer doesn't crash on transform with an empty\n        # vocabulary\n        v = TfidfVectorizer()\n        v.fit([])\n        with pytest.raises(ValueError, match=r\"Vocabulary is empty\"):\n            v.transform([\"test\"])\n", "    def test_tfidfvectorizer_binary_with_stopwords(self):\n        # Check if binary=True overrides stop_words\n\n        vectorizer = TfidfVectorizer(binary=True, stop_words='english')\n        X = vectorizer.fit_transform(ALL_FOOD_DOCS)\n        assert_allclose_dense_sparse(X.toarray(),\n                                     [[1, 1, 1, 0]] * len(JUNK_FOOD_DOCS) +\n                                     [[0, 0, 0, 1]] * len(NOTJUNK_FOOD_DOCS))\n", "    def test_tfidfvectorizer_transform_vocabulary_unchanged(self):\n        # test that transform doesn't change the vocabulary\n        vectorizer = TfidfVectorizer(vocabulary={'a': 0, 'b': 1})\n        X1 = vectorizer.fit_transform(['a a b', 'b b'])\n\n        # transform with same data, vocabulary shouldn't change\n        X2 = vectorizer.transform(['a a b', 'b b'])\n\n        assert_array_equal(X1.toarray(), X2.toarray())\n        assert vectorizer.vocabulary_ == {'a': 0, 'b': 1}\n\n", "    def test_tfidf_vectorizer_vocabulary(self):\n        docs = [\"dogs and cats\", \"cats and dogs\", \"I like dogs\"]\n        vectorizer = TfidfVectorizer(vocabulary=[\"dogs\" , \"cats\"])\n        X = vectorizer.fit_transform(docs)\n\n        assert_array_equal(X.toarray(),\n                           np.array([[1., 1.], [1., 1.], [1., 0.]]))\n\n        # Test with vocabulary and analyzer\n        vectorizer = TfidfVectorizer(vocabulary=[\"dogs\", \"cats\"],\n                                     analyzer='word')\n\n        X = vectorizer.fit_transform(docs)\n        assert_array_equal(X.toarray(),\n                           np.array([[1., 1.], [1., 1.], [1., 0.]]))\n"], "sample_365": ["    def test_cached_property_name_deprecation(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            class MyClass:\n                @cached_property(name='foo')\n                    return 'bar'\n\n            obj = MyClass()\n            self.assertEqual(obj.my_property, 'bar')\n", "    def test_cached_property_name_deprecation(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            class MyClass:\n                @cached_property\n                    return 'value'\n            self.assertEqual(MyClass().my_prop, 'value')\n", "    def test_cached_property_deprecation_warning(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            class MyClass:\n                @cached_property(name='my_property')\n                    return 'This is my property'\n\n            instance = MyClass()\n            instance.my_property\n", "    def test_cached_property_name_deprecation(self):\n        with ignore_warnings(category=RemovedInDjango50Warning):\n            class Model:\n                @cached_property(name='test')\n                    return 1\n\n            model = Model()\n            self.assertEqual(model.test, 1)\n", "    def test_cached_property_name_argument(self):\n        class C:\n                self.value = value\n\n            @cached_property(name='my_property')\n                return self.value * 2\n\n        c = C(5)\n        self.assertEqual(c.my_property, 10)\n        self.assertEqual(c.my_property, 10)  # Accessing cached value\n\n"], "sample_775": ["def test_estimator_with_nested_estimator_with_custom_repr():\n    # Nested estimator with custom __repr__\n    class CustomEstimator(BaseEstimator):\n            self.param1 = param1\n            self.param2 = param2\n            return f\"CustomEstimator(param1={self.param1}, param2='{self.param2}')\"\n\n    pipeline = make_pipeline(CustomEstimator(param1=2, param2='world'),\n                              StandardScaler())\n    expected_repr = (\n        \"Pipeline(steps=[('customestimator', CustomEstimator(param1=2, \"\n        \"param2='world')), ('standardscaler', StandardScaler())])\")\n    repr_str = repr(pipeline)\n    assert repr_str == expected_repr\n", "    def test_estimator_with_nested_estimator_and_gridsearch(self):\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        grid = GridSearchCV(pipe, {\"logisticregression__C\": [1, 10]})\n        result = _safe_repr(grid, {}, 2, 0)\n        # Check that only the changed parameters are printed\n        self.assertTrue(result[0].startswith(\n            \"GridSearchCV(estimator=Pipeline(steps=[('standardscaler', \"\n        ))\n        self.assertTrue(result[0].endswith(\n            \"...]), param_grid={'logisticregression__C': [1, 10]})\"))\n        # Check that the repr is not too long\n        self.assertLessEqual(len(result[0]), 200)\n", "    def test_pipeline_with_changed_only():\n        set_config(print_changed_only=True)\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        pipe.fit([[1, 2]], [0])\n        printer = _EstimatorPrettyPrinter(compact=True)\n        output = printer.pformat(pipe)\n        expected_output = (\n            \"Pipeline(steps=[('standardscaler', StandardScaler()), \"\n            \"('logisticregression', LogisticRegression())])\"\n        )\n        assert output == expected_output\n", "    def test_pipeline_with_estimator_with_step_name(self):\n        pipe = make_pipeline(StandardScaler(with_mean=False),\n                             LogisticRegression(max_iter=100))\n        pipe.steps[0][1].with_mean = True  \n        expected = (\n            \"Pipeline(steps=[('standardscaler', StandardScaler(with_mean=True)), \"\n            \"('logisticregression', LogisticRegression(max_iter=100))])\"\n        )\n          \n        self.assertEqual(pprint.pformat(pipe, width=100), expected)\n\n", "    def test_pipeline_with_estimator_in_grid_search():\n        pipe = make_pipeline(StandardScaler(), LogisticRegression())\n        search = GridSearchCV(pipe, {'logisticregression__C': [0.1, 1]})\n        pp = _EstimatorPrettyPrinter()\n        expected = ('Pipeline(steps=[(\\'standardscaler\\', StandardScaler()), '\n                    '(\\'logisticregression\\', LogisticRegression(C=1))])')\n        assert pp.pformat(search).splitlines()[0] == expected\n"], "sample_461": ["    def test_assume_scheme_default(self):\n        field = URLField()\n        self.assertEqual(field.assume_scheme, 'http')\n        with self.assertWarns(RemovedInDjango60Warning):\n            self.assertEqual(field.to_python('example.com'), 'http://example.com')\n\n        with self.assertWarns(RemovedInDjango60Warning):\n            self.assertEqual(field.to_python('//example.com/path'), 'http://example.com/path')\n", "    def test_assume_scheme_default(self):\n        field = URLField()\n        self.assertEqual(field.assume_scheme, 'http')\n", "    def test_urlfield_assume_scheme_default(self):\n        field = URLField()\n        self.assertEqual(field.assume_scheme, 'http')\n        with self.assertRaisesMessage(\n            RemovedInDjango60Warning,\n            \"The default scheme will be changed from 'http' to 'https' in Django 6.0. \"\n            \"Pass the forms.URLField.assume_scheme argument to silence this warning.\",\n        ):\n            field.clean('example.com')\n\n", "    def test_assume_scheme_default(self):\n        field = URLField()\n        self.assertEqual(field.assume_scheme, 'http')\n\n", "    def test_urlfield_assume_scheme_deprecation(self):\n        field = URLField(assume_scheme='https')\n        with self.assertWarns(RemovedInDjango60Warning):\n            field.clean('example.com')\n\n"], "sample_11": ["    def test_sliced_wcs_invalid_slice():\n        with pytest.raises(IndexError):\n            SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(None), [1, 2, 3]))\n", "def test_slice_world_to_pixel_values():\n    wcs = WCS_SPECTRAL_CUBE\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None), 5, slice(None)))\n\n    # Test world to pixel conversion for a single point\n    world_coords = (11., 21. * u.Hz, 26.)\n    pixel_coords = sliced_wcs.world_to_pixel_values(*world_coords)\n    assert_allclose(pixel_coords, (0., 1., 1.))\n\n    # Test world to pixel conversion for multiple points\n    world_coords = np.array([[11., 21. * u.Hz, 26.],\n                            [10., 20. * u.Hz, 25.]])\n    pixel_coords = sliced_wcs.world_to_pixel_values(*world_coords.T)\n    assert_allclose(pixel_coords, [[0., 1., 1.],\n                                  [1., 0., 0.]])\n\n", "def test_sliced_wcs_with_negative_slices():\n    wcs = WCS_SPECTRAL_CUBE.copy()\n    sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None, -1), slice(3, 10), slice(None)))\n    \n    # Test pixel to world\n    pixel_coords = np.array([0, 5, 0])\n    world_coords = sliced_wcs.pixel_to_world_values(*pixel_coords)\n    assert_allclose(world_coords[0], WCS_SPECTRAL_CUBE.pixel_to_world_values(*pixel_coords)[0])\n    assert_allclose(world_coords[1], WCS_SPECTRAL_CUBE.pixel_to_world_values(*pixel_coords)[1])\n    assert_allclose(world_coords[2], WCS_SPECTRAL_CUBE.pixel_to_world_values(*pixel_coords)[2])\n\n\n\n", "    def test_slice_one_world_dim_out(self):\n        wcs = WCS_SPECTRAL_CUBE.copy()\n\n        # Slice the LATITUDE axis (first world axis)\n        sliced_wcs = SlicedLowLevelWCS(wcs, (slice(None), 5, slice(None)))\n\n        # Check that the sliced WCS has the correct dimensionality\n        assert sliced_wcs.world_n_dim == 2\n\n        # Check that the sliced WCS has the correct world axis names\n        assert sliced_wcs.world_axis_names == ['FREQ', 'GLON-CAR']\n\n        # Check pixel to world coordinates using the sliced WCS\n        pixel_coords = np.array([0, 0])\n        world_coords = sliced_wcs.pixel_to_world_values(*pixel_coords)\n        assert_allclose(world_coords, [20, 25])\n\n        # Check world to pixel coordinates using the sliced WCS\n        world_coords = np.array([20.5, 25.2])\n        pixel_coords = sliced_wcs.world_to_pixel_values(*world_coords)\n        assert_allclose(pixel_coords, [1, 2.2])\n", "    def test_sliced_wcs_world_to_pixel_slice_and_offset(self):\n\n        # Test a case with an offset and a slice in the world to pixel\n        # transformation. This requires calculating the offset correctly.\n        sliced_wcs = SlicedLowLevelWCS(WCS_SPECTRAL_CUBE, (slice(None), 1, slice(None)))\n        world = (11, 20.5 * u.Hz, 26)\n        expected = (0, 1, 1)\n        assert_allclose(sliced_wcs.world_to_pixel_values(*world), expected)\n"], "sample_274": ["    def test_modelmultiplechoicefield_invalid_pk(self):\n        class TestForm(Form):\n            choices = ModelMultipleChoiceField(ChoiceModel.objects.all())\n\n        form = TestForm()\n        self.assertFormErrors(\n            [\n                '\u201cinvalid\u201d is not a valid value.',\n            ],\n            form.save,\n        )\n", "    def test_modelmultiplechoicefield_invalid_pk(self):\n        class MyModelMultipleChoiceForm(Form):\n            choices = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n\n        data = {'choices': ['invalid1', 2]}\n        form = MyModelMultipleChoiceForm(data=data)\n        self.assertFormErrors([\n            ValidationError('\u201cinvalid1\u201d is not a valid value.', code='invalid_pk_value', params={'pk': 'invalid1'}),\n        ], form.is_valid)\n\n", "    def test_ModelMultipleChoiceField_required_empty(self):\n        class MyForm(Form):\n            choices = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all(), required=True)\n\n        form = MyForm({'choices': []})\n        self.assertFormErrors([\n            {'message': 'This field is required.'},\n        ], form.is_valid)\n\n", "    def test_model_multiple_choice_field_invalid_pk_values(self):\n        ChoiceModel.objects.create(name='test1', value='test1')\n        ChoiceModel.objects.create(name='test2', value='test2')\n        \n        class MyForm(Form):\n            choices = ModelMultipleChoiceField(queryset=ChoiceModel.objects.all())\n\n        form = MyForm({'choices': ['1', 'invalid', '3']})\n\n        expected = [\n            ValidationError(\n                '\u201cinvalid\u201d is not a valid value.',\n                code='invalid_pk_value',\n                params={'pk': 'invalid'},\n            ),\n            ValidationError(\n                '\u201c3\u201d is not a valid value.',\n                code='invalid_pk_value',\n                params={'pk': '3'},\n            ),\n        ]\n        self.assertFormErrors(expected, form.full_clean)\n\n", "    def test_modelchoicefield_empty_choices(self):\n        class MyForm(Form):\n            field = ModelChoiceField(ChoiceModel.objects.none())\n\n        form = MyForm()\n        self.assertEqual(form.errors, {})\n"], "sample_566": ["    def test_tight_layout_rect():\n        fig, ax = plt.subplots()\n        ax.plot([1, 2, 3], [4, 5, 6])\n\n        # Test rect argument with tight_layout\n        fig.tight_layout(rect=[0.1, 0.1, 0.9, 0.9])\n        fig.savefig(f'test_tight_layout_rect.png')\n\n", "def test_figure_tight_layout():\n    fig, ax = plt.subplots()\n    ax.plot([0, 1, 2], [0, 1, 2])\n    ax.set_title(\"Title\")\n    ax.set_xlabel(\"X Label\")\n    ax.set_ylabel(\"Y Label\")\n    fig.tight_layout()\n    fig.canvas.draw()\n    # Check that the title, xlabel, and ylabel do not overlap\n    assert ax.title.get_window_extent().overlaps(\n        ax.xaxis.get_label().get_window_extent()) is False\n    assert ax.yaxis.get_label().get_window_extent().overlaps(\n        ax.title.get_window_extent()) is False\n\n", "def test_figure_savefig_bbox_inches_tight():\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.plot([1, 2, 3], [4, 5, 6])\n\n    # Save with bbox_inches='tight'\n    with io.BytesIO() as buf:\n        fig.savefig(buf, bbox_inches='tight', format='png')\n        buf.seek(0)\n        img = Image.open(buf)\n    assert img.size[0] < fig.bbox.width * fig.dpi\n    assert img.size[1] < fig.bbox.height * fig.dpi\n\n    # Save without bbox_inches='tight'\n    with io.BytesIO() as buf:\n        fig.savefig(buf, format='png')\n        buf.seek(0)\n        img = Image.open(buf)\n    assert img.size[0] == fig.bbox.width * fig.dpi\n    assert img.size[1] == fig.bbox.height * fig.dpi\n\n", "    def test_figure_axes_add():\n        fig, ax = plt.subplots()\n        assert fig.axes == [ax]\n        ax2 = fig.add_subplot(111)\n        assert fig.axes == [ax, ax2]\n\n", "def test_figure_add_axes():\n    fig = Figure()\n\n    # Test adding axes with different rectangles\n    ax1 = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n    assert isinstance(ax1, Axes)\n\n    ax2 = fig.add_axes(fig.add_subplot(111).get_position())\n    assert isinstance(ax2, Axes)\n    \n    # Test adding axes with a shared x-axis\n    ax3 = fig.add_axes([0.1, 0.5, 0.8, 0.4], sharex=ax1)\n    assert isinstance(ax3, Axes) and ax3.get_shared_x_axes().get_siblings() == [ax1]\n\n    # Test axes with duplicate names\n    fig.add_axes([0.1, 0.1, 0.8, 0.8], label='test')\n    with pytest.raises(ValueError):\n        fig.add_axes([0.2, 0.2, 0.8, 0.8], label='test')\n"], "sample_191": ["    def test_iter_modules_and_files_handles_missing_modules(self):\n        with mock.patch('sys.modules', {}):\n            self.assertFileNotFound(self.temporary_file('missing_module.py'))\n            self.assertFileNotFound(self.temporary_file('missing_package/missing_module.py'))\n\n            # Test if a directory with no __init__.py is handled correctly.\n            self.assertFileNotFound(self.temporary_file('missing_package'))\n\n            # Test if a directory within a package with no __init__.py is handled\n            # correctly.\n            self.assertFileNotFound(self.temporary_file('missing_package/missing_sub_package'))\n\n", "    def test_watchman_unavailable(self):\n        with mock.patch('pywatchman.client', side_effect=pywatchman.WatchmanError) as mock_client:\n            reloader = autoreload.WatchmanReloader()\n            with self.assertRaises(WatchmanUnavailable):\n                reloader.check_availability()\n\n            self.assertTrue(mock_client.called)\n", "    def test_watch_dir_with_glob_pattern_on_nonexistent_directory(self):\n        dir_path = self.temporary_file('nonexistent_directory')\n        pattern = '*.txt'\n        reloader = autoreload.BaseReloader()\n        reloader.watch_dir(dir_path, pattern)\n        watched_files = list(reloader.watched_files(include_globs=True))\n        self.assertNotIn(dir_path / pattern, watched_files)\n", "    def test_module_with_zip_loader(self):\n        with extend_sys_path(self.temporary_file('my_module.zip').parent):\n            with zipfile.ZipFile(self.temporary_file('my_module.zip'), 'w') as zipf:\n                zipf.writestr('my_module.py', 'x = 1')\n            self.import_and_cleanup('my_module')\n            self.assertFileFound(self.temporary_file('my_module.zip'))\n", "    def test_zipfile_module_import(self):\n        with tempfile.TemporaryDirectory() as tempdir:\n            zip_filename = Path(tempdir) / 'test_zip.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('test_module.py', 'print(\"Hello\")')\n            sys.path.append(str(tempdir))\n\n            self.import_and_cleanup('test_module')\n            self.assertFileFound(zip_filename.resolve(strict=True))\n"], "sample_581": ["    def test_blueprint_record_once_multiple_registrations(self, app, bp):\n        app_called = []\n\n        @bp.record_once\n            app_called.append(state.app)\n\n        bp.record(lambda state: app_called.append(state.app))\n\n        app.register_blueprint(bp)\n        app.register_blueprint(bp)\n\n        assert len(app_called) == 1\n        assert app_called[0] is app\n", "    def test_blueprint_register_blueprint_twice_same_name(self):\n        app = flask.Flask(__name__)\n        bp1 = flask.Blueprint('my_blueprint', __name__)\n        bp2 = flask.Blueprint('my_blueprint', __name__)\n\n        with pytest.raises(ValueError) as excinfo:\n            app.register_blueprint(bp1)\n            app.register_blueprint(bp2)\n\n        assert \"The name 'my_blueprint' is already registered\" in str(excinfo.value)\n\n", "    def test_blueprint_register_twice(self):\n        app = flask.Flask(__name__)\n\n        class MyBlueprint(flask.Blueprint):\n                super().__init__(name, import_name)\n                self.registered = False\n\n                self.registered = True\n\n        bp = MyBlueprint(\"my_blueprint\", __name__)\n        bp.register(app)\n\n        with pytest.raises(AssertionError):\n            bp.register(app)\n        assert bp.registered is True \n\n", "    def test_blueprint_static_folder(self, app, bp):\n        bp.static_folder = 'static'\n        bp.register(app)\n\n        with bp.test_client() as c:\n            rv = c.get('/static/style.css')\n            assert rv.status_code == 404\n", "    def test_blueprint_register_twice(self):\n        app = flask.Flask(__name__)\n        bp = Blueprint('my_blueprint', __name__)\n\n        with pytest.raises(AssertionError):\n            bp.register(app, {})\n            bp.register(app, {})\n"], "sample_949": ["    def test_man_pages_section_directory(app, status, warning):\n        app.config.man_pages = [\n            ('index', 'mycommand', 'My Command', ['Me'], 1),\n            ('another', 'othercommand', 'Other Command', ['Someone Else'], 8),\n        ]\n        app.config.man_make_section_directory = True\n        app.build()\n        assert len(status.files) == 2\n        assert ('man1/mycommand.1' in status.files and\n                'man8/othercommand.8' in status.files)\n", "    def test_manpage_default(app_params, tmpdir):\n        config = Config(\n            {'man_pages': [('index', 'myprog', 'My Program', ['Me'], 1)]},\n        )\n        app_params['confoverrides'] = config.values\n        app = app_params['app']\n\n        app.build()\n\n        out = tmpdir.join('man1/myprog.1')\n        assert out.exists()\n", "    def test_manpage_builder_with_config(app, status, warning):\n        app.config.man_pages = [\n            ('index', 'mycommand', 'My Command', ['Me'], 1),\n        ]\n        app.build()\n        assert len(status.warnings) == 0\n        assert 'mycommand.1' in app.outdir\n\n", "    def test_man_pages(app, status, warning):\n        app.config.man_pages = [\n            ('index', 'my_command', 'My Command', ['Me'], 1),\n        ]\n        app.build()\n        assert len(status.files_to_copy) == 0\n        assert status.project_name == 'My Command'\n        assert len(status.docnames) == 1\n        assert 'my_command.1' in status.all_files\n\n", "    def test_manpage_builder(app, status, warning):\n        app.config.man_pages = default_man_pages(app.config)\n        app.build()\n        assert len(app.env.found_docs) == 1\n        assert app.statuscode == 0\n        assert len(warning) == 0\n"], "sample_246": ["    def test_no_obsolete_flag(self):\n        \"\"\"\n        Test the --no-obsolete flag.\n        \"\"\"\n        copytree(os.path.join(os.path.dirname(os.path.abspath(__file__)), 'test_project'), self.tmp_dir)\n\n        output, po_contents = self._run_makemessages(\n            locale=[LOCALE], no_obsolete=True,\n        )\n        self.assertNotIn(\"fuzzy\", po_contents)\n        self.assertNotIn(\"obsolete\", po_contents)\n", "    def test_makemessages_js_files_with_djangojs_domain(self):\n        \"\"\"\n        Tests that makemessages extracts translatable strings from JavaScript files\n        when the domain is 'djangojs'.\n        \"\"\"\n        src = os.path.join(self.work_subdir, 'locale', 'js_file.js')\n        with open(src, 'w', encoding='utf-8') as fp:\n            fp.write('gettext(\"Hello, world!\");')\n        out, po_contents = self._run_makemessages(domain='djangojs')\n\n        self.assertMsgId('Hello, world!', po_contents)\n        self.assertLocationCommentPresent(\n            self.PO_FILE, src, 1,\n        )\n", "    def test_ignore_patterns(self):\n        os.makedirs(os.path.join(self.work_subdir, 'ignoreme'))\n        with open(os.path.join(self.work_subdir, 'ignoreme', 'file.txt'), 'w') as f:\n            f.write('gettext(\"Hello, world!\")')\n\n        # Run makemessages with ignore_patterns\n        execute_from_command_line([\n            'makemessages',\n            '--locale', LOCALE,\n            '--ignore-patterns', 'ignoreme',\n            '--verbosity', '2',\n        ], stderr=StringIO())\n\n        # Ensure PO file is not created for ignored patterns\n        self.assertFalse(os.path.exists(self.PO_FILE))\n", "    def test_basic_extraction_no_i18n(self):\n        \"\"\"\n        #21995 - Test that makemessages works even when USE_I18N is False\n        \"\"\"\n        txt = \"\"\"\n        {% trans \"Hello, world!\" %}\n        \"\"\"\n        self.create_file('templates/index.html', txt)\n        output, po_contents = self._run_makemessages(domain='django')\n        self.assertMsgId('Hello, world!', po_contents)\n\n", "    def test_no_location_comment(self):\n        \"\"\"\n        Test the --no-location option.\n        \"\"\"\n        self.copytree('locale', 'locale_original')\n        output, po_contents = self._run_makemessages(nolocation=True)\n        self.assertRegex(po_contents, r'^msgid \"Hello World!\"$', re.MULTILINE)\n        self.assertNotRegex(po_contents, r'^#: .*\\.html:1$', re.MULTILINE)\n\n"], "sample_865": ["    def test_extra_tree_regressor_sparse_input(self):\n        # Test ExtraTreeRegressor on sparse input\n        for tree_cls in [ExtraTreeRegressor]:\n            for dataset in [\"sparse-pos\", \"sparse-neg\", \"sparse-mix\"]:\n                X, y = DATASETS[dataset][\"X\"], DATASETS[dataset][\"y\"]\n                est = tree_cls(random_state=0)\n                est.fit(X, y)\n                pred = est.predict(X)\n                assert len(pred) == len(y)\n                mse = mean_squared_error(y, pred)\n                assert mse < 0.5, f\"MSE too high: {mse} for {tree_cls} on {dataset}\"\n\n", "    def test_tree_sparse_input_sparse_output(self):\n        for tree_cls in SPARSE_TREES:\n            tree = ALL_TREES[tree_cls]()\n            sparse_matrix = csc_matrix(DATASETS[\"iris\"][\"X\"])\n            tree.fit(sparse_matrix, DATASETS[\"iris\"][\"y\"])\n            predictions = tree.predict(sparse_matrix)\n\n            # Sparse input, dense output (predict method)\n            assert isinstance(predictions, np.ndarray), (\n                f\"Expected ndarray output, got {type(predictions)} \"\n                f\"for {tree_cls} with sparse input\"\n            )\n\n            # Check predict_proba output\n            if hasattr(tree, \"predict_proba\"):\n                probas = tree.predict_proba(sparse_matrix)\n                assert isinstance(probas, np.ndarray)\n\n", "def test_tree_sparse_data():\n    for name in DATASETS:\n        X, y = DATASETS[name][\"X\"], DATASETS[name][\"y\"]\n        X_sparse = DATASETS[name][\"X_sparse\"]\n\n        for tree in SPARSE_TREES:\n            DenseTree = ALL_TREES[tree]\n            SparseTree = ALL_TREES[tree]\n\n            dense_tree = DenseTree()\n            sparse_tree = SparseTree()\n\n            # Test for both classification and regression\n            if tree.endswith(\"Classifier\"):\n                dense_tree.fit(X, y)\n                sparse_tree.fit(X_sparse, y)\n            else:\n                dense_tree.fit(X, y)\n                sparse_tree.fit(X_sparse, y)\n\n            assert_tree_equal(dense_tree.tree_, sparse_tree.tree_,\n                              f\"Sparse vs dense data for {tree} with dataset {name}\")\n", "    def test_tree_sparse_input(self):\n        for name, data in DATASETS.items():\n            if \"sparse-\" in name or name == \"zeros\":\n                Tree = DecisionTreeClassifier()\n                Tree.fit(data[\"X_sparse\"], data[\"y\"])\n                pred = Tree.predict(data[\"X_sparse\"])\n                acc = accuracy_score(data[\"y\"], pred)\n                assert acc > 0.8\n\n", "   def test_tree_sparse_input(self):\n        X_small_sparse = csc_matrix(X_small)\n        for name, Tree in CLF_TREES.items():\n            with assert_warns(FutureWarning):\n                clf = Tree().fit(X_small_sparse, y_small)\n"], "sample_1159": ["    def test_failing_assumptions_with_symbols():\n        x = Symbol('x', real=True, positive=True)\n        y = Symbol('y', real=True)\n\n        # Check for assumptions not satisfied\n        assert failing_assumptions(x*y, positive=True) == {'positive': None}\n\n        # Check for assumptions satisfied\n        assert failing_assumptions(x**2, positive=True) == {}\n", "def test_failing_assumptions_with_symbols():\n    x = Symbol('x', real=True, positive=True)\n    y = Symbol('y', real=True)\n    assert failing_assumptions(x**2 - 1, positive=True) == {'positive': None}\n    assert failing_assumptions(x*y, positive=True) == {'positive': None}\n", "def test_failing_assumptions():\n    x = Symbol('x', real=True, positive=True)\n    assert failing_assumptions(6*x + 1, positive=True) == {'positive': None}\n    assert failing_assumptions(x**2 - 1, positive=True) == {'positive': None}\n\n    assert failing_assumptions(x**2, positive=True) == {}\n    assert failing_assumptions(x, real=True, positive=True, odd=True) == {'odd': None}\n", "def test_failing_assumptions_symbol():\n    x = Symbol('x', real=True, positive=True)\n    assert failing_assumptions(x**2 - 1, positive=True) == {'positive': None}\n", "def test_failing_assumptions_issue():\n    x = Symbol('x', real=True)\n    assert failing_assumptions(x**2 + 1, positive=True) == {'positive': None}\n\n\n"], "sample_965": ["    def test_signature_from_ast_type_annotation_unparse(self):\n        code = 'def func(arg: MyEnum, arg2: datetime.datetime) -> MyEnum: pass'\n        module = ast.parse(code)\n        function = cast(ast.FunctionDef, module.body[0])  # type: ignore\n        sig = inspect.signature_from_ast(function, code)\n        assert sig.parameters[0].annotation == 'MyEnum'\n        assert sig.parameters[1].annotation == 'datetime.datetime'\n        assert sig.return_annotation == 'MyEnum'\n", "    def test_signature_from_str_with_forward_refs(self):\n        code = \"\"\"\n        from typing import List\n\n            pass\n\n        class Foo:\n            pass\n        \"\"\"\n        sig = inspect.signature_from_str('(a: List[\\'Foo\\'])')\n        assert sig.parameters['a'].annotation == 'List[\\'Foo\\']'\n", "    def test_stringify_signature_defaults(self):\n        sig = inspect.signature(lambda a=1, b=2, *args, c=3, **kwargs: None)\n        assert stringify_signature(sig) == '(a=1, b=2, *args, c=3, **kwargs)'\n", "    def test_stringify_signature_with_default_value():\n        sig = inspect.signature(lambda a=1, b='2', *args, c='3', **kwargs: None)\n        assert stringify_signature(sig) == '(a=1, b=\\'2\\', *args, c=\\'3\\', **kwargs)'\n\n", "    def test_stringify_signature_with_forwardref():\n        @functools.partial(inspect.signature, follow_wrapped=False)\n            pass\n\n        sig = inspect.signature(func)\n        sig_str = stringify_signature(sig, show_annotation=True)\n        assert sig_str == '(arg: MyType)'\n\n"], "sample_151": ["    def test_many_to_many_field_rename_through(self):\n        # See ticket #25196\n        before_state = [\n            self.book_with_multiple_authors,\n        ]\n        after_state = [\n            self.book_with_multiple_authors_through_attribution,\n        ]\n        changes = self.get_changes(before_state, after_state)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\n            \"RenameField\",\n            \"CreateModel\",\n        ])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 1, name='attribution')\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='authors')\n\n\n\n", "    def test_rename_field_with_default(self):\n        # Regression test for #23801\n\n        before = [\n            self.author_name,\n            self.book,\n        ]\n        after = [\n            ModelState(\"testapp\", \"Author\", [\n                (\"id\", models.AutoField(primary_key=True)),\n                (\"name\", models.CharField(max_length=200, default=\"Anonymous\")),\n            ]),\n            self.book,\n        ]\n\n        changes = self.get_changes(before, after)\n        self.assertNumberMigrations(changes, \"testapp\", 1)\n        self.assertOperationTypes(changes, \"testapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"Author\")\n        self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, default=\"Anonymous\")\n", "    def test_renamed_field_through_relation(self):\n        \"\"\"\n        #27645 - Renamed field should be reflected in a related model.\n        \"\"\"\n        old_state = [\n            self.book,\n        ]\n        new_state = [\n            self.book_with_renamed_author_field,\n        ]\n        changes = self.get_changes(old_state, new_state)\n        self.assertNumberMigrations(changes, \"otherapp\", 1)\n        self.assertOperationTypes(changes, \"otherapp\", 0, [\"RenameField\"])\n        self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name='author',\n                                       model_name='book',\n                                       old_name='author', new_name='writers')\n\n", "    def test_rename_field(self):\n        changes = self.get_changes(\n            [self.author],\n            [self.author_name_renamed],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['RenameField'])\n        self.assertOperationAttributes(changes, 'testapp', 0, 0, name='name', old_name='old_name')\n\n", "    def test_create_model_with_default(self):\n        changes = self.get_changes(\n            [self.author_no_migs],\n            [self.author_name_default],\n        )\n        self.assertNumberMigrations(changes, 'testapp', 1)\n        self.assertOperationTypes(changes, 'testapp', 0, ['CreateModel'])\n        self.assertOperationAttributes(\n            changes, 'testapp', 0, 0, name='Author',\n            fields=[\n                {'name': 'id', 'field': 'AutoField', 'args': [None]},\n                {'name': 'name', 'field': 'CharField', 'args': ['max_length=200', 'default=My Default Name']},\n            ],\n        )\n\n"], "sample_84": ["    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({'key1': ['value1', 'value2'], 'key2': 'value3'})\n        self.assertEqual(urlencode(data), 'key1=value1&key1=value2&key2=value3')\n", "    def test_urlencode_multivaluedict(self):\n        d = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': 'value3',\n        })\n        self.assertEqual(urlencode(d), 'key1=value1&key1=value2&key2=value3')\n\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode({'key': None})\n\n", "    def test_urlencode_multivaluedict(self):\n        self.assertEqual(\n            urlencode(MultiValueDict({\n                'a': ['1', '2'],\n                'b': ['3'],\n            })),\n            'a=1&a=2&b=3'\n        )\n", "    def test_urlencode_multivaluedict(self):\n        data = MultiValueDict({\n            'key1': ['value1', 'value2'],\n            'key2': 'value3',\n        })\n        self.assertEqual(urlencode(data), 'key1=value1&key1=value2&key2=value3')\n", "    def test_urlencode_multivaluedict(self):\n        d = MultiValueDict({'name': ['a', 'b']})\n        self.assertEqual(urlencode(d), 'name=a&name=b')\n\n        d = MultiValueDict({'name': ['a', 'b'], 'foo': None})\n        with self.assertRaisesMessage(TypeError, self.cannot_encode_none_msg):\n            urlencode(d)\n"], "sample_886": ["    def test_wrap_in_pandas_container_sparse():\n        with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n            _wrap_in_pandas_container(csr_matrix(np.eye(3)), columns=None)\n", "    def test_wrap_in_pandas_container_sparse_input():\n        with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n            _wrap_in_pandas_container(csr_matrix((2, 2)), columns=[1, 2], index=[1, 2])\n", "    def test_wrap_data_with_container_pandas_output_with_index(self):\n        class Estimator(_SetOutputMixin):\n                return np.array([[1, 2], [3, 4]])\n\n        estimator = Estimator()\n\n        with config_context(transform_output=\"pandas\"):\n            X = np.array([[1, 2], [3, 4]])\n            output = estimator.fit_transform(X)\n\n        assert isinstance(output, pd.DataFrame)\n        assert_array_equal(output.values, np.array([[1, 2], [3, 4]]))\n        assert output.index.tolist() == [0, 1]\n        assert output.columns.tolist() == [0, 1]\n\n", "    def test_wrap_in_pandas_container_sparse_error(self):\n        with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n            _wrap_in_pandas_container(csr_matrix((2, 2)), columns=[1, 2])\n", "    def test_wrap_in_pandas_container_with_sparse_input():\n        with pytest.raises(ValueError, match=\"Pandas output does not support sparse data.\"):\n            _wrap_in_pandas_container(csr_matrix((2, 2)),\n                                      columns=[\"a\", \"b\"],\n                                      index=[0, 1])\n\n"], "sample_1056": ["    def test_numexpr_basic():\n        expr = x**2 + sin(x)\n        nprinter = NumExprPrinter()\n        ncode = nprinter.doprint(expr)\n        assert \"evaluate\" in ncode\n        assert \"x**2\" in ncode\n        assert \"sin(x)\" in ncode\n\n", "    def test_NumExprPrinter_Piecewise():\n        p = NumExprPrinter()\n        expr = Piecewise((x, x < 1), (x**2, True))\n        raises(TypeError, lambda: p.doprint(expr))\n", "    def test_numexpr_ITE():\n        expr = Piecewise((x, x < 1), (x**2, True))\n        printer = NumExprPrinter()\n        result = printer.doprint(expr)\n        assert 'where' in result\n", "    def test_numexpr_function_with_complex_arg():\n        expr = sin(1 + 2j)\n        numpy_printer = NumExprPrinter()\n        numpy_code = numpy_printer.doprint(expr)\n        assert numpy_code == \"evaluate('sin(complex(1,2))', truediv=True)\"\n", "    def test_numexpr_Matrix():\n        raises(TypeError, lambda: NumExprPrinter().doprint(Matrix(x, y)))\n\n"], "sample_599": ["    def test_cf_scale_offset_coder_float32():\n        data = np.array([0, 1, 2, 3], dtype=np.float32)\n        variable = xr.Variable([\"x\"], data, attrs={\"scale_factor\": 2, \"add_offset\": 1})\n\n        encoded = encode_cf_variable(variable)\n        decoded = decode_cf_variable(encoded)\n\n        assert_identical(decoded, variable)\n", "    def test_unsigned_integer_coder():\n        data = np.array([0, 1, 2, 3], dtype=np.uint8)\n        var = xr.Variable([\"x\"], data, attrs={\"_Unsigned\": \"true\"})\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n        assert_identical(var, decoded)\n\n        with pytest.raises(ValueError):\n            data = np.array([-1, 0, 1, 2], dtype=np.int8)\n            var = xr.Variable([\"x\"], data, attrs={\"_Unsigned\": \"true\"})\n            variables.UnsignedIntegerCoder().encode(var)\n", "    def test_unsigned_integer_coder_round_trip():\n        # test for issue #3204\n        data = np.array([0, 1, 2**15 - 1, 2**15, 2**31 - 1, 2**31], dtype=np.uint32)\n        var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n        encoded = encode_cf_variable(var)\n        decoded = decode_cf_variable(encoded)\n        assert_identical(var, decoded)\n", "    def test_unsigned_integer_coder():\n        data = np.array([0, 1, 2, 3, 4], dtype=np.uint8)\n        var = xr.Variable([\"x\"], data, attrs={\"_Unsigned\": \"true\"})\n\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        assert encoded.dtype == np.int8\n        assert encoded.attrs == {}\n\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n        assert_identical(decoded, var)\n\n        # Test with FillValue\n        var = xr.Variable(\n            [\"x\"], data, attrs={\"_Unsigned\": \"true\", \"_FillValue\": 255}\n        )\n        encoded = variables.UnsignedIntegerCoder().encode(var)\n        assert encoded.attrs[\"_FillValue\"] == -1\n        assert encoded.dtype == np.int8\n\n        decoded = variables.UnsignedIntegerCoder().decode(encoded)\n        assert_identical(decoded, var)\n\n        # Test with non-integer data\n        var = xr.Variable([\"x\"], data.astype(float), attrs={\"_Unsigned\": \"true\"})\n        with pytest.warns(variables.SerializationWarning):\n            decoded = variables.UnsignedIntegerCoder().decode(var)\n            assert decoded.dtype == float\n\n", "    def test_unsigned_integer_coder(self, dtype):\n        data = np.array([0, 1, 2, 2**dtype.itemsize - 1], dtype=dtype)\n        var = xr.Variable('x', data, attrs={'_Unsigned': 'true'})\n\n        # Encode and decode\n        encoded =encode_cf_variable(var, variables.UnsignedIntegerCoder())\n        decoded = decode_cf_variable(encoded)\n\n        # Check data and attributes\n        assert_identical(decoded, var)\n\n        # Test with fill value\n        var.attrs['_FillValue'] = 2**dtype.itemsize - 1\n        encoded = encode_cf_variable(var, variables.UnsignedIntegerCoder())\n        decoded = decode_cf_variable(encoded)\n        assert_identical(decoded, var)\n"], "sample_683": ["    def test_capture_manager_resume(self, tmpdir: Path, monkeypatch: MonkeyPatch):\n        capman = CaptureManager(\"fd\")\n        monkeypatch.setattr(\n            os, \"isatty\", lambda fd: fd in (1, 2)\n        )\n        capman.start_global_capturing()\n        capman.suspend_global_capture(in_=True)\n        assert capman._global_capturing is not None\n        capman.resume_global_capture()\n        assert capman._global_capturing is not None\n", "    def test_empty_capture(self, method: str) -> None:\n        capman = CaptureManager(method)\n        assert capman.is_capturing() is False\n        capman.start_global_capturing()\n        out, err = capman.read_global_capture()\n        assert out == \"\"\n        assert err == \"\"\n        capman.stop_global_capturing()\n", "    def test_suspend_resume_resume_global(\n        self, tmpdir: tmpdir", "    def test_capturemanager_global_fixture_and_in_capture(self, tmpdir: tmpdir.TempdirFactory, monkeypatch):\n        config = pytest.Config(\n            option_defaults={\"capture\": \"sys\"},\n            plugins=[capture.CaptureManager(method=\"sys\")],\n        )\n        captureman = config.pluginmanager.getplugin(\"capturemanager\")\n        capfixture = capture.CaptureFixture(capture.SysCapture, None)\n        captureman.set_fixture(capfixture)\n\n        monkeypatch.setattr(\n            capture.MultiCapture, \"start_capturing\", lambda self: self._state.set(\"started\")\n        )\n        monkeypatch.setattr(\n            capture.MultiCapture, \"suspend_capturing\", lambda self: self._state.set(\"suspended\")\n        )\n        monkeypatch.setattr(\n            capture.MultiCapture, \"resume_capturing\", lambda self: self._state.set(\"resumed\")\n        )\n\n        capfixture._start()  # simulate fixture starting\n\n        captureman.suspend_global_capture(in_=True)\n\n        assert captureman._global_capturing.in_.state == \"suspended\"\n        assert captureman._capture_fixture is not None\n\n\n", "    def test_suspend_resume_global_capture(self, tmpdir):\n        capmanager = CaptureManager(\"sys\")\n        capmanager.start_global_capturing()\n        # Write to stdout and stderr\n        print(\"hello\")\n        print(\"world\", file=sys.stderr)\n        capmanager.suspend_global_capture()\n        print(\"should not be captured\")\n        print(\"should not be captured\", file=sys.stderr)\n        capmanager.resume_global_capture()\n        print(\"captured again\")\n        print(\"captured again\", file=sys.stderr)\n\n        out, err = capmanager.read_global_capture()\n        assert out == \"hello\\ncaptured again\\n\"\n        assert err == \"world\\ncaptured again\\n\"\n"], "sample_942": ["    def test_parse_annotation(self):\n        assert _parse_annotation(\"int\", self.env) == [\n            desc_annotation(\"int\", '', nodes.Text('int')),\n        ]\n        assert _parse_annotation(\"list[str]\", self.env) == [\n            desc_annotation(\"list\", '[', nodes.Text('str')),\n            desc_annotation(']', ']', nodes.Text(']')),\n        ]\n        assert _parse_annotation(\"Dict[str, int]\", self.env) == [\n            desc_annotation(\"Dict\", '[', nodes.Text('str')),\n            desc_annotation(\",\", \",\", nodes.Text(',')),\n            desc_annotation(\"int\", ']', nodes.Text('int')),\n            desc_annotation(']', ']', nodes.Text(']')),\n        ]\n", "def test_annotation_parsing():\n    assert_node(\n        _parse_annotation('int'),\n        desc_annotation('int', '', nodes.Text('int'))\n    )\n    assert_node(\n        _parse_annotation('list[str]'),\n        desc_annotation('list[str]', '',\n                        nodes.Text('list'),\n                        nodes.Text('['),\n                        desc_annotation('str', '', nodes.Text('str')),\n                        nodes.Text(']'))\n    )\n\n\n", "def test_parse_annotation():\n    assert_node(\n        _parse_annotation('x: int'),\n        desc_annotation('', '', nodes.Text('int')),\n    )\n    assert_node(\n        _parse_annotation('x: Optional[int]'),\n        desc_annotation('', '', nodes.Text('Optional'),\n                        desc_addname('', 'int')),\n    )\n\n", "    def test_parse_annotation(self):\n        # basic annotation\n        result = _parse_annotation('str')\n        assert result == [('text', 'str')]\n        # multiple annotations\n        result = _parse_annotation('List[str]')\n        assert result == [('name', 'List'), ('bracket', '['),\n                          ('text', 'str'), ('bracket', ']')]\n        # union annotation\n        result = _parse_annotation('Union[int, str]')\n        assert result == [('name', 'Union'), ('bracket', '['),\n                          ('text', 'int'), ('comma', ','),\n                          ('text', 'str'), ('bracket', ']')]\n        # complex annotation\n        result = _parse_annotation('Optional[Dict[str, List[int]]]')\n        assert result == [('name', 'Optional'), ('bracket', '['),\n                          ('name', 'Dict'), ('bracket', '['),\n                          ('text', 'str'), ('comma', ','),\n                          ('name', 'List'), ('bracket', '['),\n                          ('text', 'int'), ('bracket', ']'),\n                          ('bracket', ']'), ('bracket', ']')]\n\n", "def test_parse_annotation():\n    assert _parse_annotation('int', {}) == ('int', [])\n    assert _parse_annotation('List[str]', {}) == ('List', [('str', [])])\n    assert _parse_annotation('Dict[str, int]', {}) == ('Dict', [('str', []),\n                                                                  ('int', [])])\n    assert _parse_annotation('Optional[int]', {}) == ('Optional', [('int', [])])\n    assert _parse_annotation('Callable[[int, str], bool]', {}) == ('Callable',\n                                                                  [([], [('int', []),\n                                                                         ('str', [])],\n                                                                    'bool')])\n\n\n"], "sample_680": ["def test_evaluate_skip_marks_multi_conditions():\n    item = Item.from_parent(None, name=\"test_func\", nodeid=\"test_func\")\n    item._store[skipped_by_mark_key]: bool = False\n    item.config = pytest.Config([])\n    mark = pytest.mark.skipif(condition=(sys.platform == \"linux\", sys.platform == \"win32\"), reason=\"skip on linux or win\")\n    item.iter_markers = lambda name: (mark,) if name == \"skipif\" else ()\n\n    result = evaluate_skip_marks(item)\n\n    assert result is not None\n    assert result.reason == \"skip on linux or win\"\n\n", "    def test_evaluate_skipif_condition_string(self):\n        item = mock.Mock(\n            spec=Item,\n            config=mock.Mock(getini=lambda x: False),\n            obj=mock.Mock(__globals__={\"sys\": sys}),\n        )\n        mark = mock.Mock(name=\"skipif\", kwargs={\"condition\": \"sys.version_info < (3, 7)\"}, args=())\n\n        result, reason = evaluate_condition(item, mark, \"sys.version_info < (3, 7)\")\n        assert result is False\n        assert reason == \"condition: sys.version_info < (3, 7)\"\n", "    def test_evaluate_skipif_true(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.skipif(sys.platform == \"linux\")\n                pass\n            \"\"\",\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "    def test_evaluate_skip_marks_skipif_with_multiple_conditions(self) -> None:\n        item = runtestprotocol.Item.from_parent(None, fspath=\"test_skip\")\n        item._store[skipped_by_mark_key] = False\n        mark = pytest.mark.skipif(\n            sys.platform == \"win32\" or sys.platform == \"darwin\", reason=\"platform check\"\n        )\n        item.add_marker(mark)\n        skip_info = evaluate_skip_marks(item)\n        assert skip_info is not None\n        assert skip_info.reason == \"platform check\"\n", "    def test_evaluate_skip_marks_multiple_skipif_conditions(self):\n        item = runtestprotocol.Item(name=\"test_func\")\n\n        # Single condition\n        item._metafunc.addcall(\n            id=\"test_func\",\n            args=(),\n            kwargs={\"condition\": \"sys.platform == 'linux'\"},\n        )\n        mark = item.iter_markers(name=\"skipif\").__next__()\n        result, reason = evaluate_condition(item, mark, \"sys.platform == 'linux'\")\n        assert result is True\n        assert reason == \"condition: sys.platform == 'linux'\"\n\n        # Multiple conditions\n        item._metafunc.addcall(\n            id=\"test_func\",\n            args=(\"a\",),\n            kwargs={\"condition\": (\"sys.platform == 'linux'\", \"True\")},\n        )\n        mark = item.iter_markers(name=\"skipif\").__next__()\n        result, reason = evaluate_condition(item, mark, \"sys.platform == 'linux'\")\n        assert result is True\n        assert reason == \"condition: sys.platform == 'linux'\"\n\n        result, reason = evaluate_condition(item, mark, \"True\")\n        assert result is True\n        assert reason == \"condition: True\"\n\n"], "sample_853": ["    def test_inverse_func_error():\n        # check that error is raised when 'func' is provided without 'inverse_func'\n        y = np.array([1, 2, 3, 4])\n        regressor = DummyRegressor()\n\n        with pytest.raises(ValueError):\n            TransformedTargetRegressor(regressor=regressor, func=np.log)\n\n        with pytest.raises(ValueError):\n            TransformedTargetRegressor(regressor=regressor,\n                                        func=np.log, check_inverse=False)\n", "    def test_inverse_transform_consistency_with_transformer(self):\n        X = friedman[0]\n        y = friedman[1]\n        transformer = StandardScaler()\n        regressor = LinearRegression()\n        tt = TransformedTargetRegressor(regressor=regressor,\n                                        transformer=transformer)\n        tt.fit(X, y)\n        y_pred = tt.predict(X)\n        assert_allclose(y, y_pred, rtol=1e-2)\n", "    def test_transformer_inverse_check(self):\n        # check if the check_inverse option works\n        X, y = friedman\n        # Let's define a transformer that doesn't have a perfect inverse\n        class BadTransformer(TransformerMixin, BaseEstimator):\n                return self\n                return X ** 2\n                return np.sqrt(X)\n\n        with pytest.raises(ValueError\n                          ):\n\n          TransformedTargetRegressor(transformer=BadTransformer(),\n                                     check_inverse=True).fit(X, y)\n\n        # Now we test without inverse check\n        tt = TransformedTargetRegressor(transformer=BadTransformer(),\n                                       check_inverse=False).fit(X,y)\n\n        assert tt.transformer_.inverse_transform(tt.transformer_.transform(y))\n\n        # Now with FunctionTransformer which does have a perfect inverse\n        tt = TransformedTargetRegressor(transformer=FunctionTransformer(\n                                        func=np.log,\n                                        inverse_func=np.exp),\n                                        check_inverse=True).fit(X,y)\n\n        assert_allclose(tt.transformer_.inverse_transform(tt.transformer_.transform(y)), y)\n\n\n", "def test_inverse_transform_identity():\n    # check that if transformer is None and func/inverse_func are not set,\n    # the transformation is identity\n    X, y = friedman\n    reg = TransformedTargetRegressor(regressor=LinearRegression())\n    reg.fit(X, y)\n    assert_allclose(reg.predict(X), y)\n", "    def test_transformed_target_regressor_with_pipeline(self):\n        # test with pipeline inside\n\n        transformer = Pipeline([\n            ('scaler', StandardScaler()),\n            ('func', FunctionTransformer(func=np.log, inverse_func=np.exp))\n        ])\n        regressor = LinearRegression()\n        tt = TransformedTargetRegressor(regressor=regressor,\n                                        transformer=transformer)\n\n        tt.fit(friedman[0], friedman[1])\n        assert_allclose(tt.predict(friedman[0]),\n                        np.exp(regressor.predict(friedman[0])))\n\n\n"], "sample_1123": ["def test_conditionset_subs_with_emptyset():\n    c = ConditionSet(x, x > 0, S.EmptySet)\n    assert c.subs(x, y) == S.EmptySet\n", "def test_ConditionSet_subs_dont_adjust_condition_if_base_set_changes():\n    s = Symbol('s')\n    a = Symbol('a')\n    c = ConditionSet(s, s > 1, Interval(0, 10))\n    c2 = c.subs(s, a)\n    assert c2.base_set == Interval(0, 10).subs(s, a)\n    assert c2.sym == a\n    assert c2.condition == s > 1  # condition didn't change, was not evaluated\n", "    def test_empty_conditionset():\n        assert ConditionSet(x, S.false, S.Reals) == EmptySet\n        assert ConditionSet(x, S.false, FiniteSet(1,2,3)) == EmptySet\n        assert ConditionSet(x, S.false, ConditionSet(y, y > 0, S.Reals)) == EmptySet\n", "    def test_nonlin_conditionset_subs():\n        # nonlinsolve can return ConditionSet with Eq and unevaluated functions\n        from sympy.solvers.solveset import nonlinsolve\n        sol = nonlinsolve([Eq(x + y, 1), Eq(x - y, 2)], [x, y])\n        assert isinstance(sol, ConditionSet)\n        assert sol.subs(x, 0) == ConditionSet(y, Eq(y, 1), S.Reals)\n", "def test_conditionset_subs_lambda():\n    A = ConditionSet(x, Eq(x**2, 4), S.Reals)\n    B = ConditionSet(Lambda(x, x + 1), Eq(Lambda(x, x + 1)**2, 4), S.Reals)\n    assert A.subs(x, Lambda(x, x + 1)) == B\n\n\n"], "sample_1149": ["    def test_singleton_access_via_S():\n        assert S.Rational is Rational\n\n", "    def test_singleton_subclassing():\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n        a = MySingleton()\n        b = MySingleton()\n        assert a is b\n\n", "    def test_singleton_registration():\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n        assert MySingleton() is MySingleton()\n        assert S.MySingleton is MySingleton()\n", "    def test_singleton_registry_getattr():\n        class MySingleton(Basic, metaclass=Singleton):\n            pass\n        assert S.MySingleton is MySingleton()\n", "    def test_singleton_registration_and_access():\n        class MyClass(Basic, metaclass=Singleton):\n            pass\n        assert MyClass() is MyClass()\n        assert S.MyClass is MyClass()\n        assert isinstance(S.MyClass, Basic)"], "sample_148": ["    def test_nested_objects_cycle(self):\n        self._connect(1, 0)\n        self._connect(2, 1)\n        self._connect(3, 2)\n        self._collect(1, 2, 3)\n        target = [\n            [\n                {'num': 1},\n                ['num': 2],\n                ['num': 3],\n            ],\n            {'num': 0},\n        ]\n        self._check(target)\n", "    def test_nested_objects_m2m_through_model(self):\n        e1 = Event.objects.create(name='Event 1')\n        e2 = Event.objects.create(name='Event 2')\n        lg1 = EventGuide.objects.create(event=e1, location=Location.objects.create(name='Location 1'))\n        lg2 = EventGuide.objects.create(event=e2, location=Location.objects.create(name='Location 2'))\n\n        self.n.collect([e1, e2])\n        target = [\n            ['Event 1', [\n                ['Location 1'],\n            ]],\n            ['Event 2', [\n                ['Location 2'],\n            ]],\n        ]\n        self._check(target)\n\n", "    def test_nested_objects_circular_reference(self):\n        self._connect(1, 0)\n        self._connect(2, 1)\n        self._collect(0, 1, 2)\n        target = [\n            [self.objs[0].num],\n            [self.objs[1].num, [self.objs[2].num]],\n        ]\n        self._check(target)\n", "    def test_nested_objects_with_circular_dependency(self):\n        self._connect(1, 0)\n        self._connect(2, 1)\n        self._connect(3, 2)\n        self._collect(1, 2, 3)\n        self._check([[\n            self.objs[1].num,\n            [self.objs[2].num, [self.objs[3].num]]\n        ]])\n", "    def test_nested_objects_with_recursion(self):\n        self._connect(1, 0)\n        self._connect(2, 0)\n        self._collect(0, 1, 2)\n        target = [[self.objs[0], [self.objs[1], self.objs[2]]]]\n        self._check(target)\n"], "sample_1179": ["def test_print_NDimArray():\n    a = NDimArray([1, 2, 3], (3,))\n    assert sstr(a) == 'NDimArray([1, 2, 3], shape=(3,))'\n    a = NDimArray([[1,2],[3,4]],(2,2))\n    assert sstr(a) == 'NDimArray([[1, 2], [3, 4]], shape=(2, 2))'\n", "def test_print_NDimArray():\n    a = NDimArray([1, 2, 3], (3,))\n    assert sstr(a) == 'NDimArray([1, 2, 3], (3,))'\n\n", "def test_printmethod_none():\n    p = StrPrinter({'order': 'none'})\n    assert p.doprint(x + y) == 'x + y'\n    assert p.doprint(sin(x) + cos(x)) == 'sin(x) + cos(x)'\n", "def test_print_NDimArray():\n    a = NDimArray(range(8), (2, 2, 2))\n    assert sstr(a) == \"NDimArray([-1, 0, 2, 3, 1, 4, 5, 6], (2, 2, 2))\"\n", "def test_printmethod():\n    class Foo(Expr):\n            return 'str_Foo'\n            return 'sympystr_Foo'\n\n    f = Foo()\n    assert sstr(f) == 'str_Foo'\n    assert sstr(f, method='sympy') == 'sympystr_Foo'\n"], "sample_264": ["    def test_cookie_storage_legacy_hash_compatibility(self):\n        storage = self.storage_class(request=self.request)\n        messages = [Message(mark_safe('Message 1'), constants.INFO)]\n        set_cookie_data(storage, messages)\n\n        # Verify that the old hash format still works\n        signed_data = storage._encode(messages)\n        self.assertTrue(storage._decode(signed_data))\n\n        # Verify that old data without the sentinel is still interpreted\n        # correctly\n        old_data = get_random_string(32) + '$' + json.dumps(\n            messages, cls=MessageEncoder\n        ).encode('latin-1')\n        self.request.COOKIES = {CookieStorage.cookie_name: old_data}\n        self.assertEqual(storage.get()[0], messages)\n\n        # Verify that we can still decode old data with a sentinel\n        old_data = get_random_string(32) + '$' + json.dumps(\n            messages + [CookieStorage.not_finished], cls=MessageEncoder\n        ).encode('latin-1')\n        self.request.COOKIES = {CookieStorage.cookie_name: old_data}\n        self.assertEqual(storage.get()[0], messages)\n", "    def test_cookie_storage_samesite(self):\n        storage = self.storage\n        response = self._get_response()\n        storage._store([Message('Hello, world!', constants.INFO)], response)\n        self.assertEqual(response.cookies[CookieStorage.cookie_name]['samesite'], 'Lax')\n        \n        response = self._get_response(cookies={CookieStorage.cookie_name: response.cookies[CookieStorage.cookie_name]})\n        storage._get(response)\n        self.assertEqual(response.cookies[CookieStorage.cookie_name]['samesite'], 'Lax')\n", "    def test_cookie_storage_max_size_and_not_finished(self):\n        storage = self.storage()\n        storage.request.COOKIES = {storage.cookie_name: ''}\n        # Simulate a cookie that is very close to the maximum size limit.\n        messages = [Message(text=get_random_string(length=100), level=constants.WARNING) for _ in range(9)]\n        set_cookie_data(storage, messages)\n        response = self.get_response(request=self.request)\n\n        # Ensure that the cookie is stored even though it is close to the maximum size.\n        self.assertEqual(stored_cookie_messages_count(storage, response), len(messages))\n        self.assertTrue({storage.cookie_name} <= set(response.cookies))\n        messages.append(CookieStorage.not_finished)\n        self.assertEqual(storage._decode(response.cookies.get(storage.cookie_name).value), messages)\n\n", "    def test_cookie_storage_partial_retrieval(self):\n        storage = self.storage_class(self.request)\n        messages = [\n            Message(\n                'This is a message', constants.INFO\n            ) for _ in range(CookieStorage.max_cookie_size // 50 + 1)\n        ]\n        set_cookie_data(storage, messages)\n        response = self.client.get('/')\n        self.assertEqual(self.stored_messages_count(storage, response), CookieStorage.max_cookie_size // 50)\n        messages_from_cookie, all_retrieved = storage._get()\n        self.assertFalse(all_retrieved)\n        self.assertEqual(len(messages_from_cookie), CookieStorage.max_cookie_size // 50)\n", "    def test_cookie_storage_max_cookie_size(self):\n        storage = self.storage_class(self.request)\n        messages = [\n            Message(mark_safe('This is a <script>alert()</script> message'), constants.WARNING),\n            Message('Another message', constants.INFO),\n            Message('And another message', constants.DEBUG),\n        ]\n        for i in range(100):\n            messages.append(Message(f'Message {i}', constants.SUCCESS))\n        \n        response = self.request.build_not_found()\n        # Store the messages\n        storage._store(messages, response)\n        # Check the number of stored messages\n        self.assertTrue(stored_cookie_messages_count(storage, response) < len(messages))\n        # Check the cookie size and compare with the max_cookie_size\n        cookie = response.cookies.get(storage.cookie_name)\n        self.assertTrue(len(cookie.value.encode('latin-1')) <= storage.max_cookie_size)\n"], "sample_167": ["    def test_intword(self):\n        self.assertEqual(humanize.intword(12345), 12345)\n        self.assertEqual(humanize.intword(1234567), '1.2 million')\n        self.assertEqual(humanize.intword(1234567890), '1.2 billion')\n        self.assertEqual(humanize.intword(1234567890123), '1.2 trillion')\n\n", "    def test_intword_with_large_number(self):\n        self.assertEqual(humanize.intword(1000000), '1.0 million')\n        self.assertEqual(humanize.intword(1200000), '1.2 million')\n        self.assertEqual(humanize.intword(1200000000), '1.2 billion')\n", "    def test_intword(self):\n        with translation.override('en'):\n            self.assertEqual(humanize.intword(1000000), '1.0 million')\n            self.assertEqual(humanize.intword(1200000), '1.2 million')\n            self.assertEqual(humanize.intword(1200000000), '1.2 billion')\n            self.assertEqual(humanize.intword(1234567890123456789), '12.3 quintillion')\n", "    def test_naturaltime_past_spanish(self):\n        with translation.override('es'):\n            now = datetime.datetime(2012, 3, 9, 22, 30)\n            past_now = now - datetime.timedelta(seconds=10)\n            self.assertEqual(humanize.naturaltime(past_now), _(\"hace 10\u00a0segundos\"))\n\n\n\n", "    def test_naturalday_with_timezone(self):\n        with translation.override('en'):\n            with self.settings(USE_TZ=True, TIME_ZONE='America/Chicago'):\n                tz = get_fixed_timezone(timedelta(hours=-6))\n                context = Context({'value': now.replace(tzinfo=tz)})\n                template = Template('{{ value|naturalday }}')\n                self.assertEqual(template.render(context), 'today')\n"], "sample_530": ["    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        # create a simple annotation\n        ann = ax.annotate(\"test\", xy=(0.5, 0.5), xycoords=\"data\")\n        # make it draggable\n        draggable = DraggableAnnotation(ann)\n        # save the initial position\n        x0, y0 = ann.xy\n        # move the annotation using mouse drag\n        event1 = MouseEvent(\n            \"button_press_event\",\n            fig.canvas,\n            x=fig.transFigure.transform(ann.get_position())[0],\n            y=fig.transFigure.transform(ann.get_position())[1],\n            button=MouseButton.LEFT,\n        )\n        event2 = MouseEvent(\n            \"motion_notify_event\",\n            fig.canvas,\n            x=100,\n            y=100,\n        )\n        event3 = MouseEvent(\n            \"button_release_event\", fig.canvas, x=100, y=100, button=MouseButton.LEFT\n        )\n        draggable.on_pick(event1)\n        draggable.on_motion(event2)\n        draggable.on_release(event3)\n        # check that the annotation has moved\n        assert ann.xy != (x0, y0)\n\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        x = np.linspace(0, 10, 100)\n        y = np.sin(x)\n        ax.plot(x, y)\n\n        # Create an annotation\n        ann = ax.annotate(\n            \"This is an annotation\",\n            xy=(5, np.sin(5)), xycoords=\"data\",\n            xytext=(6, np.sin(5) + 0.5), textcoords=\"data\"\n        )\n\n        draggable_ann = DraggableAnnotation(ann)\n\n        # Simulate a mouse click and drag\n        mouse_event = MouseEvent(\n            \"button_press_event\", fig.canvas, x=ax.transData.transform((6, np.sin(5) + 0.5))[0],\n            y=ax.transData.transform((6, np.sin(5) + 0.5))[1], button=MouseButton.LEFT)\n        draggable_ann.on_pick(mouse_event)\n\n        # Simulate mouse movement\n        mouse_event = MouseEvent(\n            \"motion_notify_event\", fig.canvas, x=ax.transData.transform((7, np.sin(5) + 0.8))[0],\n            y=ax.transData.transform((7, np.sin(5) + 0.8))[1])\n        draggable_ann.on_motion(mouse_event)\n\n        # Simulate mouse release\n        mouse_event = MouseEvent(\n            \"button_release_event\", fig.canvas, x=ax.transData.transform((7, np.sin(5) + 0.8))[0],\n            y=ax.transData.transform((7, np.sin(5) + 0.8))[1], button=MouseButton.LEFT\n        )\n        draggable_ann.on_release(mouse_event)\n\n        plt.close(fig)\n\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n\n        # Create an AnnotationBbox\n        annotation = AnnotationBbox(\n            TextArea(\"Hello World\"), (0.5, 0.5), xycoords='axes fraction',\n            boxcoords='axes fraction',\n            box_alignment=(0.5, 0.5)\n        )\n        ax.add_artist(annotation)\n\n        # Make the annotation draggable\n        draggable = DraggableAnnotation(annotation)\n        plt.show()\n", "    def test_draggable_annotation(self):\n        fig, ax = plt.subplots()\n        xy = (0.5, 0.5)\n        ann = ax.annotate(\n            \"annotation\", xy, xytext=(10, 20), textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\"))\n        draggable = DraggableAnnotation(ann)\n        canvas = fig.canvas\n        event = MouseEvent(\n            'button_press_event', fig.canvas,\n            x=canvas.width * xy[0],\n            y=canvas.height * xy[1],\n            button=MouseButton.LEFT\n        )\n\n        ann.contains(event)  # Initialize the picker\n        draggable.on_pick(event)\n\n        event = MouseEvent(\n            'motion_notify_event', fig.canvas,\n            x=canvas.width * xy[0] + 10,\n            y=canvas.height * xy[1] + 10\n        )\n        draggable.on_motion(event)\n        # assert that the annotation is moved\n\n        event = MouseEvent(\n            'button_release_event', fig.canvas,\n            x=canvas.width * xy[0] + 10,\n            y=canvas.height * xy[1] + 10,\n            button=MouseButton.LEFT\n        )\n        draggable.on_release(event)\n\n", "def test_draggable_annotation():\n    fig, ax = plt.subplots()\n    box = mpatches.Rectangle((0.1, 0.1), 0.2, 0.2, facecolor=\"blue\",\n                               alpha=0.5)\n    ax.add_patch(box)\n\n    ann = AnnotationBbox(\n        TextArea(\"Annotation\"), (0.5, 0.5),\n        boxcoords=\"axes fraction\",\n        xycoords=\"data\",\n        arrowprops=dict(arrowstyle=\"->\"))\n    ax.add_artist(ann)\n\n    draggable = DraggableAnnotation(ann)\n\n    event = MouseEvent(\n        'button_press_event',\n        fig.canvas,\n        x=fig.canvas.renderer.points_to_pixels(100),\n        y=fig.canvas.renderer.points_to_pixels(100),\n        button=MouseButton.LEFT,\n    )\n\n    ann.on_pick(event)\n    ann.on_motion(event)\n    fig.canvas.draw()\n\n    # Assert that the annotation has moved\n\n    assert ann.xyann != (0.5, 0.5)\n\n\n"], "sample_39": ["    def test_maps_celestial(self):\n        \"\"\"Test world coordinate solutions for celestial maps.\"\"\"\n        for filename in self._file_list:\n            with open(filename, \"r\") as hdul:\n                header = fits.Header.fromtextfile(hdul)\n            wcsobj = wcs.WCS(header)\n            assert wcsobj.wcs.types == ['RA---TAN', 'DEC--TAN']\n\n", "    def test_tab_to_world(self):\n        # Test the WCSTab to world coordinate conversion\n        for filename in self._file_list:\n            with catch_warnings(AstropyUserWarning) as warning_lines:\n                hdu = fits.open(filename)\n                wcs = wcs.WCS(hdu[0].header)\n                # Test coordinate conversion with and without NaN values\n                x, y = np.arange(10), np.arange(10)\n                x_world, y_world = wcs.all_pix2world(x, y, 0)\n                assert_allclose(wcs.tab2world(x, y, 0), (x_world, y_world))\n\n                x[5] = np.nan\n                y[3] = np.nan\n                x_world, y_world = wcs.all_pix2world(x, y, 0)\n                assert_allclose(wcs.tab2world(x, y, 0),\n                                (x_world, y_world))\n", "    def test_header(self):\n        for filename in self._file_list:\n            with fits.open(filename) as hdul:\n                header = hdul[0].header\n                try:\n                    wcs_obj = wcs.WCS(header)\n                except Exception as e:\n                    raise Exception(f\"Failed to create WCS object for {filename}: {e}\")\n\n                # Check if the WCS object is valid\n                assert wcs_obj.is_celestial\n", "    def test_fits2pix(self):\n        for filename in self._file_list:\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', AstropyUserWarning)\n                hdul = fits.open(filename)\n                w = wcs.WCS(hdul[0].header)\n                # test if the 'fits2pix' method works\n                x, y = w.fits2pix(100, 200)\n\n", "    def test_maps_001(self):\n        with warnings.catch_warnings(record=True) as w:\n            with fits.open(self._file_list[0]) as hdul:\n                wcs = wcs.WCS(hdul[0].header)\n                assert isinstance(wcs, wcs.WCS)\n                # Test some basic properties\n                assert wcs.naxis == 2\n                assert_array_equal(wcs.pixel_bounds, [[1, 200], [1, 200]])\n\n"], "sample_1191": ["def test_hermite_normal_form_modulo_D_bad_D():\n    A = DomainMatrix([[ZZ(1), ZZ(2)], [ZZ(3), ZZ(4)]], (2, 2), ZZ)\n    raises(DMDomainError, lambda: hermite_normal_form(A, D=Symbol('x')))\n    raises(DMDomainError, lambda: hermite_normal_form(A, D=ZZ(-1)))\n", "def test_hermite_normal_form_modulo_D_raises():\n    A = DM([[1, 2, 3], [4, 5, 6], [7, 8, 9]], (3, 3), ZZ)\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(A, 10))\n    raises(DMShapeError, lambda: _hermite_normal_form_modulo_D(A.reshape((3, 2)), 10))\n", "def test_hermite_normal_form_modulo_D():\n    A = DomainMatrix([[ZZ(12), ZZ(6), ZZ(4)],\n                     [ZZ(3), ZZ(9), ZZ(6)],\n                     [ZZ(2), ZZ(16), ZZ(14)]], (3, 3), ZZ)\n    D = ZZ(2*3*5)\n    H = _hermite_normal_form_modulo_D(A, D)\n    assert H == DomainMatrix([[ZZ(10), ZZ(0), ZZ(2)],\n                              [ZZ(0), ZZ(15), ZZ(3)],\n                              [ZZ(0), ZZ(0), ZZ(2)]], (3, 3), ZZ)\n", "def test_hermite_normal_form_modulo_D_non_multiple():\n    A = DomainMatrix([[ZZ(1), ZZ(2)], [ZZ(3), ZZ(4)]], (2, 2), ZZ)\n    D = ZZ(5)\n    raises(DMDomainError, lambda: _hermite_normal_form_modulo_D(A, D))\n", "    def test__hermite_normal_form_modulo_D_non_invertible(self):\n        from sympy.polys.matrices import DomainMatrix\n        A = DomainMatrix([[ZZ(1), ZZ(2)], [ZZ(2), ZZ(4)]], (2, 2), ZZ)\n        with raises(DMDomainError):\n            _hermite_normal_form_modulo_D(A, ZZ(5))\n\n\n\n"], "sample_917": ["    def test_enum_member(self):\n        # Test parsing of enum members with and without initializer values.\n\n        self.check(\n            'enum', 'enum MyEnum { ONE, TWO = 2, THREE = 3 };',\n            {}\n        )\n\n", "def test_template_parameter_list():\n    check('class', 'template<typename T, typename U = int> class MyClass;',\n          {1: 'MyClass', 2: 'MyClass'},\n          'template<typename T, typename U = int> class MyClass;')\n\n", "def test_enum_constants():\n    check(\n        \"enum\",\n        \"enum MyEnum { CONST1, CONST2=10 };\",\n        {1: \"MyEnum\"},\n    )\n", "def test_function_pointer():\n    check(\n        'function',\n        'void (*funcPtr)(int a, int b);',\n        {\n            1: ' void (*funcPtr)(int a, int b)',\n            2: 'funcPtr',\n        }\n    )\n\n", "    def test_cpp_template_param_id():\n        check(\n            \"templateparam\",\n            \"template <typename T> struct Foo {};\",\n            {1: \"Foo\", 2: \"cpp:templateParam\"},\n        )\n"], "sample_943": ["def test_apidoc_exclude(apidoc):\n    assert (apidoc.outdir / 'pkg1.rst').exists()\n    assert not (apidoc.outdir / 'pkg2.rst').exists()\n", "def test_apidoc_exclude_package(apidoc):\n    assert not (apidoc.outdir / 'pkg.rst').exists()\n\n", "def test_apidoc_separate(apidoc):\n    \"\"\"Test the -e and -P options.\"\"\"\n    # ... test logic ...\n", "def test_apidoc_excludes(apidoc):\n    expected_files = [\n        'module1.rst',\n        'pkg1/package.rst',\n        'pkg1/module2.rst',\n    ]\n    assert (apidoc.outdir / 'modules.rst').exists()\n    for filename in expected_files:\n        assert (apidoc.outdir / filename).exists()\n\n", "def test_exclude_subdir(apidoc):\n    assert (apidoc.outdir / 'subdir').exists() is False\n"], "sample_428": ["    def test_grouping_with_sequence(self):\n        self.assertEqual(nformat(1234567890, decimal_sep='.', decimal_pos=0, grouping=(3, 2, 0), thousand_sep='.'), '1.234.567.890')\n        self.assertEqual(nformat(1234567890123, decimal_sep='.', decimal_pos=0, grouping=(3, 2, 0), thousand_sep='.'), '1.234.567.890.123')\n", "    def test_grouping_nonuniform(self):\n        self.assertEqual(nformat(1234567890.123, decimal_sep='.', decimal_pos=2, grouping=(3, 2, 0)), '1,234,567,890.12')\n\n", "    def test_format_scientific_notation(self):\n        self.assertEqual(\n            nformat(\n                Decimal(\"1e200\"),\n                decimal_sep=\".\",\n                decimal_pos=2,\n                grouping=3,\n                thousand_sep=\",\",\n                force_grouping=True,\n                use_l10n=False,\n            ),\n            \"1.00e+200\",\n        )\n", "    def test_format_int_grouping_large_number(self):\n        self.assertEqual(\n            nformat(1234567890, decimal_sep=\",\", thousand_sep=\".\", grouping=3),\n            \"1.234.567.890\",\n        )\n", "    def test_format_float_scientific_notation(self):\n        self.assertEqual(\n            nformat(\n                float(\"1.2345678901234567e+200\"),\n                decimal_sep=\",\",\n                decimal_pos=2,\n            ),\n            \"1.23e+200\",\n        )\n"], "sample_431": ["    def test_get_or_create_with_defaults(self):\n        with self.assertRaisesMessage(ValueError, \"get_or_create() must be called with exactly one positional argument\"):\n            Article.objects.get_or_create()\n        with self.assertRaisesMessage(ValueError, \"get_or_create() must be called with exactly one positional argument\"):\n            Article.objects.get_or_create(title='Title', content='Content', pub_date=datetime.now())\n        article, created = Article.objects.get_or_create(title='Title', defaults={'content': 'Content', 'pub_date': datetime.now()})\n        self.assertEqual(article.title, 'Title')\n        self.assertEqual(article.content, 'Content')\n        self.assertIsNotNone(article.pub_date)\n        self.assertTrue(created)\n", "    def test_model_save_with_defer(self):\n        # Test saving a model instance with deferred fields\n        article = Article.objects.create(headline='Test Article', pub_date=datetime.now())\n        article.content = 'This is the content of the test article.'\n        article.save(update_fields=['content'])\n        self.assertEqual(article.content, 'This is the content of the test article.')\n        # Check if other fields are not loaded\n        with self.assertRaises(AttributeError):\n            article.reporter\n\n", "    def test_get_or_create_with_defaults(self):\n        with mock.patch('django.db.models.signals.pre_save.send') as mock_pre_save:\n            Article.objects.get_or_create(headline=\"Django Rocks!\", defaults={\"slug\": \"django-rocks\"})\n            self.assertEqual(mock_pre_save.call_count, 1)\n\n", "    def test_get_or_create_with_defaults(self):\n        # Test the get_or_create() method with default values\n        article, created = Article.objects.get_or_create(\n            headline=\"Test Article\",\n            defaults={\"pub_date\": datetime.now() - timedelta(days=1)},\n        )\n        self.assertIsNotNone(article)\n        self.assertFalse(created)\n        self.assertEqual(article.headline, \"Test Article\")\n\n        # Ensure that the default value was used for pub_date\n        self.assertLess(article.pub_date, datetime.now())\n", "    def test_get_or_create_with_defaults(self):\n        with self.assertRaises(ValueError):\n            Article.objects.get_or_create(headline='Test article', defaults={'pub_date': datetime.now()})\n"], "sample_487": ["    def test_raw_id_fields_with_ordering(self):\n        class MyModelAdmin(ModelAdmin):\n            raw_id_fields = ['related_field']\n            ordering = ['related_field__some_field']\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            msg=r\"The value of 'ordering' refers to 'related_field__some_field', which is not a field of 'ValidationTestInlineModel'.\",\n            id=\"admin.E033\",\n        )\n", "    def test_raw_id_fields_with_filter(self):\n        class MyModelAdmin(ModelAdmin):\n            raw_id_fields = ['band']\n            list_filter = ['band']\n\n        model_admin = MyModelAdmin\n        model = Band\n\n        self.assertIsInvalid(\n            model_admin,\n            model,\n            r\"The field 'band' is listed in both 'raw_id_fields' and 'list_filter'.\"\n            \" You should either remove it from one or the other.\",\n            id=\"admin.E131\",\n        )\n", "    def test_raw_id_fields_in_inlines(self):\n        class MyInline(TabularInline):\n            model = ValidationTestInlineModel\n            raw_id_fields = ('test_raw_field',)\n\n        class MyModelAdmin(ModelAdmin):\n            inlines = [MyInline]\n\n        self.assertIsInvalid(\n            MyModelAdmin,\n            ValidationTestModel,\n            r\"The value of 'raw_id_fields' in 'MyInline' refers to 'test_raw_field', which is not a field of 'ValidationTestInlineModel'\\.\",\n            id=\"admin.E108\",\n        )\n\n", "    def test_raw_id_fields_and_related_fields(self):\n        class MyModelAdmin(ModelAdmin):\n            raw_id_fields = ('band',)\n\n        with isolate_apps() as apps:\n            apps.get_or_create_model(\n                'myapp', 'Song',\n                ('band', models.ForeignKey(Band, on_delete=CASCADE)),\n            )\n            self.assertIsInvalid(\n                MyModelAdmin, Song,\n                \"The value of 'raw_id_fields' contains 'band', which is not a related field of 'Song'\",\n                id=\"admin.E024\",\n            )\n\n", "    def test_inline_models_exclude_field_check(self):\n        class MyInline(TabularInline):\n            model = ValidationTestInlineModel\n            exclude = ('name',)\n\n        class MyAdmin(ModelAdmin):\n            inlines = [MyInline]\n\n        self.assertIsInvalid(\n            MyAdmin,\n            ValidationTestModel,\n            msg=r\"Cannot exclude the field 'band', because it is the foreign key to the parent model 'ValidationTestModel'.\",\n            id=\"admin.E201\"\n        )\n"], "sample_124": ["    def test_field_initial(self):\n        p = Person({'first_name': 'John', 'last_name': 'Doe', 'birthday': '2000-01-01'})\n        self.assertEqual(p.cleaned_data['first_name'], 'John')\n        self.assertEqual(p.cleaned_data['last_name'], 'Doe')\n        self.assertEqual(p.cleaned_data['birthday'], datetime.date(2000, 1, 1))\n\n", "    def test_hidden_widget(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe', 'birthday': '2023-01-01'})\n        self.assertEqual(form.as_p(),\n                         '<p><label for=\"first_name_id\">First name:</label> <input type=\"text\" name=\"first_name\" value=\"John\" required id=\"first_name_id\" /></p>\\n<p><label for=\"id_last_name\">Last name:</label> <input type=\"text\" name=\"last_name\" value=\"Doe\" required id=\"id_last_name\" /></p>\\n<p><label for=\"id_birthday\">Birthday:</label> <input type=\"text\" name=\"birthday\" value=\"2023-01-01\" required id=\"id_birthday\" /></p>')\n", "    def test_hidden_widget(self):\n        form = PersonNew({'first_name': 'John', 'last_name': 'Doe'})\n        self.assertEqual(\n            str(form['first_name'].field.widget),\n            '<input type=\"text\" name=\"first_name\" id=\"first_name_id\" required>'\n        )\n", "    def test_form_initial(self):\n        data = {'first_name': 'John', 'last_name': 'Doe', 'birthday': '1970-01-01'}\n        form = Person(data)\n        self.assertEqual(form.initial['first_name'], 'John')\n        self.assertEqual(form.initial['last_name'], 'Doe')\n        self.assertEqual(form.initial['birthday'], datetime.date(1970, 1, 1))\n", "    def test_multivaluedict(self):\n        # Regression test for ticket 12167\n        data = MultiValueDictLike({'foo': 'bar'})\n        form = Person(data)\n        self.assertEqual(form.data['foo'], 'bar')\n"], "sample_97": ["    def test_sys_path_directories(self):\n        with extend_sys_path(self.temporary_file('test_module.py').parent):\n            self.assertFileFound(self.temporary_file('test_module.py'))\n        self.assertFileNotFound(self.temporary_file('test_module.py'))\n", "    def test_iter_modules_and_files_zipfile(self):\n        with tempfile.TemporaryDirectory() as tempdirname:\n            zip_filename = Path(tempdirname) / 'my.zip'\n            with zipfile.ZipFile(zip_filename, 'w') as zf:\n                zf.writestr('mymodule.py', '')\n            sys.path.append(tempdirname)\n            self.addCleanup(lambda: sys.path.remove(tempdirname))\n            self.assertFileFound(zip_filename)\n", "    def test_watchman_unavailable(self):\n        with mock.patch('django.utils.autoreload.pywatchman') as mock_watchman:\n            mock_watchman.client.side_effect = WatchmanUnavailable('Service unavailable.')\n            reloader = autoreload.get_reloader()\n            self.assertIsInstance(reloader, autoreload.StatReloader)\n\n", "    def test_symlinked_modules(self):\n        # Test modules inside symlinked directories are correctly added to watchlist.\n        with contextlib.ExitStack() as stack:\n            original_dir = Path(tempfile.mkdtemp())\n            stack.enter_context(extend_sys_path(original_dir))\n            symlink_dir = Path(tempfile.mkdtemp())\n            os.symlink(original_dir, symlink_dir)\n            stack.enter_context(extend_sys_path(symlink_dir))\n\n            # Create a module inside the original directory.\n            module_file = original_dir / 'test_module.py'\n            module_file.write_text('print(\"Hello from test_module\")')\n\n            self.import_and_cleanup('test_module')\n\n            self.assertFileFound(module_file)\n", "    def test_watch_dir_with_glob(self):\n        with extend_sys_path(self.temporary_file('test_dir')):\n            self.temporary_file('test_dir').mkdir()\n            self.temporary_file('test_dir/test1.py').write_text('')\n            self.temporary_file('test_dir/test2.py').write_text('')\n\n            reloader = autoreload.StatReloader()\n            reloader.watch_dir(self.temporary_file('test_dir'), 'test*.py')\n            self.assertFileFound(self.temporary_file('test_dir/test1.py'))\n            self.assertFileFound(self.temporary_file('test_dir/test2.py'))\n"], "sample_453": ["    def test_submit_row_tag(self):\n        request = self.request_factory.get(reverse('admin:articles_article_add'))\n        request.user = self.user\n        ctx = {\n            \"add\": True,\n            \"change\": False,\n            \"is_popup\": False,\n            \"save_as\": False,\n            \"has_add_permission\": True,\n            \"has_change_permission\": True,\n            \"has_view_permission\": True,\n            \"has_editable_inline_admin_formsets\": False,\n            \"show_save\": True,\n            \"show_save_and_add_another\": True,\n            \"show_save_and_continue\": True,\n            \"show_delete\": True,\n        }\n        template = submit_row(Context(ctx))\n        self.assertEqual(template.render(request), \"\")\n", "    def test_submit_row(self):\n        request = self.request_factory.get(reverse(\"admin:articles_article_change\", args=(1,)))\n        request.user = self.user\n        context = submit_row(self.get_context(request, ArticleAdmin(Article, site),\n                                              add=False, change=True))\n        self.assertTrue(context[\"can_change\"])\n        self.assertTrue(context[\"show_delete_link\"])\n        self.assertTrue(context[\"show_save\"])\n        self.assertTrue(context[\"show_save_and_add_another\"])\n        self.assertTrue(context[\"show_save_and_continue\"])\n", "    def test_cell_count(self):\n        request = self.request_factory.get(reverse('admin:polls_article_add'))\n        request.user = self.user\n        response = ArticleAdmin(Article, site).response_add(request)\n        context = response.context_data\n\n        count = cell_count(context[\"adminform\"])\n        self.assertEqual(count, 6)\n", "    def test_submit_row(self):\n        user = User.objects.create_user('testuser', 'test@example.com', 'password')\n        request = self.request_factory.get(reverse('admin:articles_article_add'))\n        request.user = user\n\n        # Ensure the submit row context has the expected values\n        context = submit_row({'add': True, 'change': False, 'is_popup': False, 'save_as': False,\n                              'has_add_permission': get_perm(user, 'add_article'),\n                              'has_change_permission': get_perm(user, 'change_article'),\n                              'has_view_permission': get_perm(user, 'view_article'),\n                              'has_editable_inline_admin_formsets': False\n                              })\n\n        self.assertTrue(context['can_change'])\n        self.assertFalse(context['show_delete_link'])\n        self.assertFalse(context['show_save_as_new'])\n        self.assertTrue(context['show_save'])\n        self.assertFalse(context['show_close'])\n", "    def test_submit_row_tag(self):\n        request = self.request_factory.get(reverse('admin:polls_article_add'))\n        request.user = self.user\n        \n\n        context = submit_row(\n            {\n                \"add\": True,\n                \"change\": False,\n                \"is_popup\": False,\n                \"save_as\": False,\n                \"show_save\": True,\n                \"show_save_and_add_another\": True,\n                \"show_save_and_continue\": True,\n                \"has_add_permission\": True,\n                \"has_change_permission\": True,\n                \"has_view_permission\": True,\n                \"has_editable_inline_admin_formsets\": False,\n            }\n        )\n\n        self.assertTrue(context.get(\"show_save\"))\n        self.assertTrue(context.get(\"show_save_and_add_another\"))\n        self.assertTrue(context.get(\"show_save_and_continue\"))\n        \n"], "sample_101": ["    def test_wsgi_request_content_length(self):\n        request = self.request_factory.post('/', data={'key': 'value'}, content_type='application/x-www-form-urlencoded')\n        response = get_wsgi_application()(request.environ, start_response)\n        self.assertEqual(response[0], '200 OK')\n        self.assertEqual(len(response[1]), 2)\n        self.assertIn(('Content-Length', '0'), response[1])\n", "    def test_wsgi_handler_with_file_response(self):\n        response = FileResponse(BytesIO(b'content'))\n        with self.settings(DEBUG=True):\n            wsgi_app = get_wsgi_application()\n            request = self.request_factory.get('/')\n            response = wsgi_app(request.environ, lambda *args: None)[0]\n        self.assertEqual(response.content, b'content')\n", "    def test_wsgi_handler_with_file_response(self):\n        response = FileResponse(BytesIO(b'content'), content_type='application/octet-stream')\n        request = self.request_factory.get('/')\n        with self.assertRaises(AttributeError):\n            response.file_to_stream\n\n        # Simulate WSGI environment\n        environ = {}\n        start_response = lambda status, headers: None\n        wsgi_app = get_wsgi_application()\n\n        wsgi_response = wsgi_app(environ, start_response)\n        self.assertEqual(isinstance(wsgi_response, bytes), True)\n", "    def test_wsgihandler_with_file_response(self):\n        response = FileResponse(BytesIO(b\"Hello, world!\"), content_type='text/plain')\n        with self.settings(ALLOWED_HOSTS=['example.com']):\n            wsgi_app = get_wsgi_application()\n            request = self.request_factory.get('/test/')\n            response_obj = wsgi_app(request.environ, lambda *args, **kwargs: None)[0]\n\n        self.assertEqual(response_obj, response)\n\n", "    def test_wsgi_request_content_length(self):\n        request = self.request_factory.post(\"/\", data={'key': 'value'}, content_type=\"application/x-www-form-urlencoded\")\n        response = get_wsgi_application()(request.environ, self.start_response)\n        self.assertEqual(request._stream.remaining, len(request.body))\n"], "sample_606": ["    def test_ordered_set_union(array, expected):\n        assert list(ordered_set_union(array.dims)) == list(expected)\n\n", "    def test_join_dict_keys(join):\n        a = {\"x\": [1, 2], \"y\": [3, 4]}\n        b = {\"x\": [1, 2, 3], \"z\": [5, 6]}\n\n        result = join_dict_keys(a, b, join=join)\n\n        keys = {\"x\"}\n        if join == \"outer\":\n            keys.update({\"y\", \"z\"})\n        elif join in (\"left\", \"exact\"):\n            keys.update({\"y\"})\n        elif join == \"right\":\n            keys.update({\"z\"})\n\n        assert set(result.keys()) == keys\n        for k in keys:\n            if k in a:\n                assert_array_equal(result[k], a[k])\n            if k in b:\n                assert_array_equal(result[k], b[k])\n\n", "    def test_where_broadcast(self):\n        x = xr.DataArray(\n            np.arange(12).reshape(3, 4),\n            dims=[\"lat\", \"lon\"],\n        )\n        y = xr.DataArray(\n            np.arange(3),\n            dims=[\"lat\"],\n        )\n        z = xr.where(x > 5, y, x)\n\n        expected = xr.DataArray(\n            np.where(\n                np.arange(12).reshape(3, 4) > 5, np.arange(3).reshape(3, 1), np.arange(12).reshape(3, 4)\n            ),\n            dims=[\"lat\", \"lon\"],\n        )\n        assert_identical(z, expected)\n", "    def test_broadcast_compat_data_with_coords(dim):\n        da1 = xr.DataArray(\n            np.arange(6).reshape(2, 3),\n            dims=[\"x\", \"y\"],\n            coords={\"x\": np.arange(2), \"y\": np.arange(3)},\n        )\n        da2 = xr.DataArray(\n            np.arange(2),\n            dims=[dim],\n            coords={dim: [0, 1]},\n        )\n        expected = da1.broadcast_like(da2)\n        result = broadcast_compat_data(da1, da2)\n\n        assert_identical(result, expected)\n\n", "    def test_where_scalar_cond(self):\n        data = xr.DataArray(np.arange(12).reshape(3, 4), dims=[\"x\", \"y\"])\n        expected = xr.DataArray(\n            [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]], dims=[\"x\", \"y\"]\n        )\n        result = xr.where(True, data, 0)\n        assert_identical(result, expected)\n"], "sample_811": ["    def test_pairwise_distances_argmin_min_sparse(self):\n        # Check if argmin and min work correctly with sparse matrices\n        X = csr_matrix([[0, 1], [1, 0]])\n        Y = csr_matrix([[1, 0]])\n        dist_matrix = pairwise_distances(X, Y)\n        indices, distances = pairwise_distances_argmin_min(X, Y)\n        assert_array_equal(indices, np.array([1]))\n        assert_array_almost_equal(distances, np.array([1]))\n", "    def test_pairwise_distances_chunked_reduce_func(self):\n        X = np.random.RandomState(0).rand(5, 3)\n        r = .2\n\n            neigh = [np.flatnonzero(d < r) for d in D_chunk]\n            return neigh\n\n        gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n        neigh = next(gen)\n        assert_equal(neigh, [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]) \n\n", "    def test_pairwise_distances_chunked_different_metrics_sparse(self):\n        X = csr_matrix(np.random.rand(5, 3))\n        Y = csr_matrix(np.random.rand(5, 3))\n        metrics = ['euclidean', 'manhattan', 'cosine']\n\n        for metric in metrics:\n            D = pairwise_distances(X, Y, metric=metric)\n            D_chunked = list(pairwise_distances_chunked(X, Y, metric=metric))\n            D_chunked = np.concatenate(D_chunked)\n            assert_allclose(D, D_chunked)\n\n        # Test with precomputed distances\n        D = pairwise_distances(X, metric=\"precomputed\")\n        D_chunked = list(pairwise_distances_chunked(X, metric=\"precomputed\"))\n        D_chunked = np.concatenate(D_chunked)\n        assert_allclose(D, D_chunked)\n\n", "    def test_pairwise_distances_chunked_empty(self):\n        X = np.empty((0, 10))\n\n        for chunk in pairwise_distances_chunked(X):\n            assert chunk.shape[0] == 0\n\n        chunk = next(pairwise_distances_chunked(X,\n                                                 reduce_func=lambda x, i: (x, i)))\n        assert chunk[0].shape[0] == 0\n", "    def test_pairwise_distances_chunked_sparse_metric_precomputed_error(self):\n        X = dok_matrix((5, 3))\n        with assert_raises_regexp(TypeError, \"scipy distance metrics do not\"\n                                  \" support sparse matrices.\"):\n            list(pairwise_distances_chunked(X, metric='cosine'))\n\n"], "sample_52": ["    def test_modelchoicefield_empty_label(self):\n        f = forms.ModelChoiceField(queryset=Category.objects.all(), empty_label='--------')\n        self.assertEqual(f.choices[0][1], '--------')\n\n", "    def test_modelchoicefield_empty_label(self):\n        choices = Category.objects.all()\n        field = forms.ModelChoiceField(choices, empty_label='-----------')\n        self.assertEqual(field.choices[0][0], '')\n        self.assertEqual(field.choices[0][1], '-----------')\n", "    def test_modelchoicefield_empty_label(self):\n        class TestForm(forms.Form):\n            category = ModelChoiceField(Category.objects.all(), empty_label='Select a Category')\n        form = TestForm()\n        self.assertEqual(form.fields['category'].choices[0], ('', 'Select a Category'))\n", "    def test_modelchoicefield_empty_label(self):\n        class MyForm(forms.Form):\n            category = ModelChoiceField(Category.objects.all(), empty_label='Select a Category')\n\n        form = MyForm()\n        self.assertIn('<option value=\"\">Select a Category</option>', form.as_p())\n\n", "    def test_modelchoicefield_empty_label_with_choices(self):\n        class CategoryForm(forms.Form):\n            category = forms.ModelChoiceField(queryset=Category.objects.all(), empty_label='Select a category')\n\n        form = CategoryForm()\n        self.assertIn('', [choice[0] for choice in form.fields['category'].choices])\n        self.assertEqual(form.fields['category'].choices[0][1], 'Select a category')\n\n"], "sample_339": ["    def test_modelformset_factory_save_existing_objects(self):\n        \"\"\"\n        Test that modelformset_factory correctly saves existing objects when\n        using both commit=True and commit=False.\n        \"\"\"\n        initial_data = [\n            {'name': 'Book 1', 'author': 1},\n            {'name': 'Book 2', 'author': 2},\n        ]\n        formset = modelformset_factory(Book, fields=['name', 'author'])\n        bound_formset = formset(data=initial_data)\n        bound_formset.save(commit=True)\n\n        # Now modify one of the books and try saving again\n        bound_formset = formset(data={'0-name': 'Updated Book', '0-author': 1},\n                               queryset=Book.objects.all())\n        bound_formset.save(commit=True)\n        self.assertEqual(Book.objects.get(pk=1).name, 'Updated Book')\n\n        # Test saving with commit=False\n        bound_formset.save(commit=False)\n        self.assertEqual(Book.objects.get(pk=1).name, 'Updated Book')\n", "    def test_modelformset_factory_can_delete_extra(self):\n        formset = modelformset_factory(Book)\n        self.assertTrue(formset(data={}, can_delete=True).can_delete)\n        self.assertFalse(formset(data={}, can_delete=False).can_delete)\n", "    def test_modelformset_factory_with_can_delete_extra_false(self):\n        BookFormSet = modelformset_factory(Book, extra=2, can_delete_extra=False)\n        formset = BookFormSet()\n        self.assertEqual(len(formset.forms), 2)\n        self.assertTrue(all(form.instance.pk is None for form in formset.forms))\n        formset.forms[0].cleaned_data = {'title': 'New Book 1', 'author': Author.objects.first().id}\n        formset.forms[1].cleaned_data = {'title': 'New Book 2', 'author': Author.objects.first().id}\n        # Try to delete an extra form\n        with self.assertRaises(AttributeError):\n            formset.forms[2].delete()\n\n        formset.save()\n\n        self.assertEqual(Book.objects.count(), 2 + len(Book.objects.all()))\n", "    def test_delete_empty_formset(self):\n        Book.objects.create(title='The First Book')\n        Book.objects.create(title='The Second Book')\n        formset = modelformset_factory(Book, extra=2)\n        formset_data = {\n            'form-TOTAL_FORMS': '2',\n            'form-INITIAL_FORMS': '2',\n            'form-0-title': 'Updated First Book',\n            'form-1-title': 'Updated Second Book',\n            'form-2-title': '',\n            'form-2-DELETE': 'on',\n        }\n        formset = formset(data=formset_data)\n        self.assertTrue(formset.is_valid())\n        formset.save()\n        self.assertEqual(Book.objects.count(), 2)\n", "    def test_delete_form_with_non_auto_id(self):\n        class MyForm(forms.ModelForm):\n            class Meta:\n                model = BookWithCustomPK\n                fields = '__all__'\n\n        form = MyForm(data={'title': 'New book'})\n        form.instance.pk = 2  # Manually set the ID\n        formset = modelformset_factory(BookWithCustomPK, form=MyForm, extra=1)\n        fs = formset({'title': 'New book'})\n        fs.save()\n        self.assertEqual(BookWithCustomPK.objects.count(), 2)\n\n"], "sample_933": ["def test_gettext_build(app, status, warning):\n    app.builder.build_all()\n    po_file = os.path.join(app.outdir, 'messages.pot')\n    assert os.path.exists(po_file)\n\n    with open(po_file, 'r', encoding='utf-8') as f:\n        content = f.read()\n\n    # Check if the expected message is present in the .pot file\n    assert \"Translatable message\" in content\n", "    def test_build_gettext_with_templates(self, app, status, warning):\n        app.build()\n        po_file = os.path.join(app.outdir, 'sphinx.pot')\n        with open(po_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n        assert 'msgstr \"\"' in content\n        assert 'msgid \"Hello, world\"' in content\n        assert 'msgid \"This is a template\"' in content\n", "    def test_gettext_po_file(self, app, status, warning):\n        app.build()\n        po_file = os.path.join(app.outdir, 'sphinx.pot')\n        assert os.path.exists(po_file)\n\n        with open(po_file, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        assert 'msgid \"Hello World\"' in content\n        assert 'msgid \"Project documentation\"' in content\n        assert 'msgstr \"\"' in content\n\n        # Check for translator comments\n        assert 'Fuzzy' in content\n        assert 'LAST TRANSLATOR' in content\n        assert 'LANGUAGE TEAM' in content\n\n", "def test_gettext_empty_project(app_builder):\n    # remove all files under _build dir\n    shutil.rmtree(path.join(app_builder.srcdir, '_build'))\n    app_builder.build()\n    assert os.path.exists(path.join(app_builder.outdir, 'sphinx.pot'))\n    with open(path.join(app_builder.outdir, 'sphinx.pot'), 'r', encoding='utf-8') as f:\n        content = f.read()\n    assert 'msgid \"\"' in content\n", "    def test_gettext_uuid(self, app, status, warning):\n        app.config.gettext_uuid = True\n\n        # Run the build process\n        with cd(app.outdir):\n            subprocess.run(['sphinx-build', '-b', 'gettext', ' '..], stdout=PIPE, stderr=PIPE)\n\n        # Ensure a UUID is present in the generated POT file\n        with open('sphinx.pot', encoding='utf-8') as pofile:\n            content = pofile.read()\n            assert re.search(r'msgid \".*?uuid:.*?\"', content) is not None\n"], "sample_1074": ["def test_isomorphic_groups():\n    G = SymmetricGroup(4)\n    H = PermutationGroup([Permutation(0, 1)(2, 3), Permutation(0, 2),\n                         Permutation(1, 3)])\n\n    assert is_isomorphic(G, H)\n\n    G = CyclicGroup(5)\n    H = PermutationGroup([Permutation(0, 1, 2, 3, 4)])\n\n    assert is_isomorphic(G, H)\n\n    G = RubikGroup('2x2x2')\n    H = PermutationGroup(rubik_cube_generators('2x2x2'))\n    assert is_isomorphic(G, H)\n\n", "    def test_polycyclic_group_from_permgroup():\n        G = SymmetricGroup(3)\n        pcg = G.polycyclic_group()\n        assert pcg.structure == [(3, 1, 1)]\n\n", "    def test_stabilizer():\n        D4 = DihedralGroup(4)\n        stab = D4.stabilizer(0)\n        assert set([str(x) for x in stab.generators]) == set(['(0 2)(1 3)', '(0)(1)(2)(3)'])\n\n", "    def test_rubik_group():\n        G = RubikGroup()\n        assert G.order() == 43252003274489856000\n\n", "def test_orbit_transversal_slp():\n    G = SymmetricGroup(4)\n    alpha = 1\n    tr, slp = _orbit_transversal(G.degree, G.generators, alpha, pairs=True, slp=True)\n    for beta, g_beta in tr:\n        assert rmul(g_beta, G.generators[slp[beta][0]]).array_form == rmul(\n            rmul(g_beta, G.generators[slp[beta][0]]), G.generators[slp[beta][1]]\n        ).array_form\n\n"], "sample_589": ["def test_interp_fill_limit_multidim(da):\n    coords = [\n        (\"time\", da.time),\n        (\"x\", np.arange(0, 100, 10)),\n    ]\n    da = da.expand_dims(\"x\")\n\n    for method in [\"linear\", \"nearest\"]:\n\n        # test limit\n        expected = da.interp(\n            coords=coords, method=method, limit=2\n        ).fillna(0)\n\n        result = da.interp(coords=coords, method=method, limit=2)\n\n        assert_array_equal(expected, result)\n", "    def test_interp_na_dask_multidim(self):\n        if not has_dask:\n            pytest.skip(\"Requires dask\")\n\n        ds = self.ds.chunk({\"x\": 2, \"time\": 3})\n\n        expected = self.ds.interp(x=[0.5, 1.5, 2.5])\n        actual = ds.interp(x=[0.5, 1.5, 2.5]).compute()\n        assert_equal(expected, actual)\n\n", "def test_interp_na_limit(da):\n    # Test limit argument.\n    da_interp = da.interp_na(dim=\"time\", limit=1, method=\"linear\")\n    assert_array_equal(da_interp.values, [0, 0, 1, 2, np.nan, 3, 4, 5, np.nan, 6, 7])\n", "    def test_interp_1d(self, da, method):\n        da_interp = da.interp(time=[pd.to_datetime(\"2000-01-05\")])\n        assert_equal(da_interp.dims, (\"time\",))\n        assert_array_equal(da_interp.values, [np.nan])\n\n", "def test_interp_na_use_coordinate_false(da, dt_series):\n\n    da = da.assign_coords(time=dt_series)\n\n    actual = da.interp_na(\n        dim=\"time\",\n        method=\"linear\",\n        use_coordinate=False,\n        limit=1,\n    )\n\n    expected = da.copy()\n    expected.values[1] = 0.5 * (expected.values[0] + expected.values[2])\n\n    assert_array_equal(actual.values, expected.values)\n\n"], "sample_906": ["def test_cpp_function_return_type():\n    check('function', 'int foo(int x) { return x; }', {},\n          \"\"\"\n          int <emphasis>foo</emphasis>(int <literal classes=\"xref\" pygments-class=\"xref\" reftext=\"int\" id_prefix=\"c:type\">x</literal>);\n          \"\"\",\n          key=\"int\", asTextOutput=\"int foo(int x)\")\n", "def test_function_args_with_defaults():\n    check(\n        'function',\n        \"void foo(int a, int b = 5, float c = 3.14);\",\n        {},\n        \"void foo(int a, int b = 5, float c = 3.14)\",\n        key=\"foo\",\n        asTextOutput=\"foo(int a, int b, float c)\"\n    )\n", "compilation error", "def test_c_member_template_nested():\n    check(\n        'member',\n        'template <typename T> class MyClass::MyInnerClass { public: T {key}member; };',\n        {'1': 'member'},\n        output='T {key}member',\n        key='MyClass::MyInnerClass::'\n    )\n", "compilation error"], "sample_536": ["def test_lasso_callback(ax):\n    called = False\n\n        nonlocal called\n        called = True\n\n    lasso = widgets.Lasso(ax, (0, 0), callback)\n    click_and_drag(lasso, ax, (0, 0), (1, 1), button=1)\n    assert called\n", "    def test_polygon_selector_move_vertex(ax):\n        # Create a PolygonSelector with 4 vertices\n        verts = [(1, 1), (1, 2), (2, 2), (2, 1)]\n            pass\n        selector = widgets.PolygonSelector(ax, onselect,\n                                         useblit=False)\n        selector._xys = verts.copy()\n        selector._selection_completed = True\n        selector._draw_polygon()\n\n        # Simulate a click and drag event to move the second vertex.\n        mock_event = mock.MagicMock(\n            xdata=1.5, ydata=1.5\n        )\n        selector._active_handle_idx = 1\n        selector._onmove(mock_event)\n\n        # Assert that the second vertex has been moved to the new location.\n        assert_allclose(selector._xys[1], (1.5, 1.5))\n\n        # Simulate a mouse release event.\n        selector._active_handle_idx = -1\n\n", "    def test_lasso_callback(ax):\n        verts = [(0, 0), (1, 1), (2, 0), (0, 0)]\n\n            assert verts == [(0, 0), (1, 1), (2, 0), (0, 0)]\n\n        lasso = widgets.Lasso(ax, (0, 0), callback)\n        click_and_drag(ax, (0, 0), (1, 1), button=1)\n        click_and_drag(ax, (1, 1), (2, 0), button=1)\n        click_and_drag(ax, (2, 0), (0, 0), button=1)\n        lasso.release_event(mock_event(x=0, y=0))\n", "def test_lasso_callback(ax):\n    verts = [(1, 1), (2, 1), (2, 2), (1, 2), (1, 1)]\n\n    called = []\n\n        called.append(pathverts)\n\n    lasso = widgets.Lasso(ax, (1, 1), callback)\n    for x, y in verts[1:]:\n        mock_event(lasso, \"motion_notify_event\", x=x, y=y)\n    mock_event(lasso, \"button_release_event\", x=verts[-1][0], y=verts[-1][1])\n\n    assert called == [verts]\n", "def test_polygon_selector_box_scale(ax):\n    fig, ax = plt.subplots()\n    verts = [(0.1, 0.1), (0.9, 0.1), (0.9, 0.9), (0.1, 0.9), (0.1, 0.1)]\n\n        pass\n\n    selector = widgets.PolygonSelector(ax, onselect, draw_bounding_box=True)\n    selector._xys = verts\n    selector._draw_polygon()\n    selector._add_box()\n    selector._scale_polygon(mock_event(fig, x=0.5, y=0.5))\n    assert selector._xys != verts\n\n"], "sample_1018": ["def test_fcode_IndexedBase():\n    i = Idx('i', 3)\n    A = IndexedBase('A', shape=(3,))\n    e = Eq(A[i], i**2)\n    code = fcode(e, contract=False, assign_to=e.lhs)\n    assert code == '      A(i) = i**2'\n", "def test_fcode_indexed_base():\n    n = symbols('n')\n    i = symbols('i', integer=True)\n    A = IndexedBase('A', shape=(n,))\n    expr = A[i]*2\n    code = fcode(expr, contract=True)\n    expected = \"\"\"\n      do i = 0, n-1\n          A(i) = 2*A(i)\n      end do", "def test_fcode_IndexedBase():\n    i = Idx('i', 3)\n    A = IndexedBase('A', shape=(3,))\n    e = Eq(A[i], i*2)\n    code = fcode(e, assign_to='B', contract=True)\n    assert 'do i = 1, 3' in code\n    assert 'B(i) = i*2' in code\n\n\n\n", "def test_fcode_indexed_assign():\n    i = Idx('i', 5)\n    A = IndexedBase('A', shape=(5,))\n    expr = A[i]\n    code = fcode(expr, assign_to='B', contract=False)\n    assert code == '      B = A(i)' \n", "compilation error"], "sample_1096": ["def test_indexed_base_offset():\n    A = IndexedBase('A', strides=(2, 3), offset=5)\n    i, j = Idx('i'), Idx('j')\n    assert str(A[i, j]) == 'A[2*i + 3*j + 5]'\n\n    A = IndexedBase('A', shape=(2, 3), strides=(2, 3), offset=5)\n    assert str(A[i, j]) == 'A[2*i + 3*j + 5]'\n", "def test_indexed_base_subs():\n    i = Idx(\"i\", 2)\n    j = Idx(\"j\", 3)\n    A = IndexedBase('A')\n\n    # Simple substitution\n    assert Subs(A[i, j], i, 1).doit() == A[1, j]\n\n    # Substitution with a function\n    x = Symbol('x')\n    assert Subs(A[i, j], j, x * i).doit() == A[i, x*i]\n\n    # Substitution involving range\n    assert Subs(A[i], i, j).doit() == A[j]\n\n    # Substitution involving nested indices\n    k = Idx(\"k\", 4)\n    B = IndexedBase('B')\n    assert Subs(B[i, j, k], (i, j), (k, i)).doit() == B[k, i, k]\n\n", "def test_indexed_free_symbols():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A')\n    x = Indexed('A', i, j)\n    assert x.free_symbols == {A, i, j}\n    assert A[i].free_symbols == {A, i}\n    assert A.free_symbols == {A}\n", "def test_indexed_free_symbols():\n    i = Idx('i', 3)\n    j = Idx('j', 3)\n    A = IndexedBase('A')\n    x = symbols('x')\n    assert A[i, j].free_symbols == {A, i, j}\n    assert A[i, x].free_symbols == {A, i, x}\n\n\n\n", "def test_indexed_subs():\n    i = Idx('i', 2)\n    j = Idx('j', 3)\n    A = IndexedBase('A')\n    x = symbols('x')\n    A_ij = A[i, j]\n\n    assert Subs(A_ij, i, 1).args[1] == Tuple(1, j)\n    assert Subs(A_ij, i, x).args[1] == Tuple(x, j)\n\n    # Test for substitution of a symbol that is part of the index label\n    B = IndexedBase('B', shape=(x, x))\n    B_ii = B[i, i]\n\n    assert Subs(B_ii, i, 2).args[1] == Tuple(2, 2)\n"], "sample_363": ["    def test_autocomplete_select_multiple_widget(self):\n        \"\"\"\n        Test that AutocompleteSelectMultiple widget is correctly rendered.\n        \"\"\"\n        ff = self.assertFormfield(Member, 'bands', widgets.AutocompleteSelectMultiple)\n        self.assertEqual(ff.widget.url_name, '%s:autocomplete' % widget_admin_site.name)\n", "    def test_datefield_formfield(self):\n        ff = self.assertFormfield(Event, 'date', forms.DateField)\n        self.assertEqual(ff.widget.format, '%Y-%m-%d')\n", "    def test_formfield_for_dbfield_datefield_with_format(self):\n        ff = self.assertFormfield(Event, 'date', widgets.AdminDateWidget, {'format': '%Y-%m-%d'})\n        self.assertEqual(ff.widget.format, '%Y-%m-%d')\n", "    def test_formfield_for_dbfield_datetimefield_with_auto_now_add(self):\n        ff = self.assertFormfield(Event, 'created', widgets.AdminSplitDateTime)\n        self.assertTrue(ff.widget.attrs.get('class').endswith('vdatetime'))\n", "    def test_autocomplete_select_multiple(self):\n        ff = self.assertFormfield(Band, 'members', widgets.AutocompleteSelectMultiple, list_filter=['name',])\n        self.assertEqual(ff.widget.url_name, 'admin:widgets_band_autocomplete')\n\n"], "sample_413": ["    def test_check_setting_app_dirs_loaders(self):\n        errors = check_setting_app_dirs_loaders(None, **{})\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, E001.id)\n", "    def test_app_dirs_and_loaders(self):\n        self.assertEqual(\n            check_setting_app_dirs_loaders(None, **{}),\n            [E001],\n        )\n", "    def test_check_setting_app_dirs_loaders(self):\n        errors = check_setting_app_dirs_loaders(None, **{})\n        self.assertEqual(\n            errors,\n            [\n                Error(\n                    \"You have 'APP_DIRS': True in your TEMPLATES but also specify 'loaders' \"\n                    \"in OPTIONS. Either remove APP_DIRS or remove the 'loaders' option.\",\n                    id=\"templates.E001\",\n                )\n            ],\n        )\n", "    def test_check_setting_app_dirs_loaders(self):\n        errors = check_setting_app_dirs_loaders(None, **{})\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n        self.assertEqual(errors[0].id, E001.id)\n", "    def test_check_setting_app_dirs_loaders(self):\n        errors = check_setting_app_dirs_loaders(None, **{})\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'templates.E001')\n"], "sample_64": ["    def test_getlist(self):\n        qd = QueryDict('a=1&a=2&b=3')\n        self.assertEqual(qd.getlist('a'), ['1', '2'])\n        self.assertEqual(qd.getlist('b'), ['3'])\n", "    def test_cookie_parsing(self):\n        cookie_string = 'name=value; expires=Sat, 01 Jan 2022 00:00:00 GMT; path=/; domain=.example.com; secure'\n        cookie = parse_cookie(cookie_string)\n        self.assertEqual(cookie['name'].value, 'value')\n        self.assertEqual(cookie['expires'].value, 'Sat, 01 Jan 2022 00:00:00 GMT')\n        self.assertEqual(cookie['path'].value, '/')\n        self.assertEqual(cookie['domain'].value, '.example.com')\n        self.assertTrue(cookie['secure'])\n", "    def test_set_cookie_with_none_expires(self):\n        response = HttpResponse()\n        response.set_cookie('test', 'value', expires=None)\n        self.assertEqual(response.cookies['test']['expires'], 'Thu, 01 Jan 1970 00:00:00 GMT')\n", "    def test_getlist(self):\n        qd = QueryDict(\"a=1&a=2&b=3\")\n        self.assertEqual(qd.getlist('a'), ['1', '2'])\n        self.assertEqual(qd.getlist('b'), ['3'])\n        self.assertEqual(qd.getlist('c'), [])\n", "    def test_empty_querydict(self):\n        q = QueryDict('')\n        self.assertEqual(q, {})\n        self.assertEqual(len(q), 0)\n        self.assertEqual(list(q), [])\n"], "sample_203": ["    def test_file_extension_validator_with_empty_allowed_extensions(self):\n        validator = validators.FileExtensionValidator()\n        with self.assertRaisesMessage(ValidationError, validators.FileExtensionValidator.message % {'extension': 'txt', 'allowed_extensions': ''}):\n            validator(SimpleUploadedFile('test.txt', b'content'))\n", "    def test_validate_file_extension_with_allowed_extensions(self):\n        file_extension_validator = validators.FileExtensionValidator(allowed_extensions=['jpg', 'png', 'gif'])\n\n        # Test valid extensions\n        file_with_valid_extension = SimpleUploadedFile('test.jpg', b'test content')\n        file_extension_validator(file_with_valid_extension)\n\n        file_with_another_valid_extension = SimpleUploadedFile('image.png', b'test content')\n        file_extension_validator(file_with_another_valid_extension)\n\n        # Test invalid extension\n        file_with_invalid_extension = SimpleUploadedFile('document.pdf', b'test content')\n        with self.assertRaises(ValidationError):\n            file_extension_validator(file_with_invalid_extension)\n", "    def test_regex_validator_with_inverse_match(self):\n        validator = validators.RegexValidator(\n            regex=r'^[a-zA-Z]+$', inverse_match=True, message='Invalid characters.'\n        )\n\n        with self.assertRaisesMessage(ValidationError, 'Invalid characters.'):\n            validator('abc1')\n\n        self.assertIsNone(validator('123'))\n", "    def test_email_validator_allowlist(self):\n        validator = validators.EmailValidator(allowlist=['example.com', 'test.com'])\n        validator('user@example.com')\n        with self.assertRaises(ValidationError):\n            validator('user@invalid.com')\n        with self.assertRaises(ValidationError):\n            validator('user@test.co.uk')\n", "    def test_validate_email_with_domain_allowlist(self):\n        validator = validators.EmailValidator(domain_allowlist=['example.com', 'example.org'])\n        self.assertEqual(validator('test@example.com'), None)\n        self.assertEqual(validator('test@example.org'), None)\n        with self.assertRaises(ValidationError):\n            validator('test@other.com')\n"], "sample_1145": ["def test_refine_atan2_symbolic():\n    x = Symbol('x', real=True)\n    y = Symbol('y', real=True)\n    assert refine(atan2(y, x), Q.real(y) & Q.positive(x)) == atan(y/x)\n    assert refine(atan2(y, x), Q.negative(y) & Q.negative(x)) == atan(y/x) - pi\n    assert refine(atan2(y, x), Q.positive(y) & Q.negative(x)) == atan(y/x) + pi\n", "    def test_refine_sign_complex():\n        x = Symbol('x', real=True)\n        y = Symbol('y', imaginary=True)\n        assert refine(sign(x), Q.positive(x) & Q.nonzero(x)) == 1\n        assert refine(sign(x), Q.negative(x) & Q.nonzero(x)) == -1\n        assert refine(sign(y), Q.positive(im(y))) == I\n        assert refine(sign(y), Q.negative(im(y))) == -I\n", "    def test_refine_re_im_complex():\n        assert refine(re(x + I*y), Q.real(x) & Q.real(y)) == x\n        assert refine(im(x + I*y), Q.real(x) & Q.real(y)) == y\n        assert refine(re(x + I*y), Q.real(x) & Q.imaginary(y)) == x\n        assert refine(im(x + I*y), Q.real(x) & Q.imaginary(y)) == y\n", "    def test_refine_Pow_with_rational_exponent():\n        assert refine(x**Rational(1,2), Q.positive(x)) == sqrt(x)\n        assert refine(x**Rational(-1,2), Q.positive(x)) == 1/sqrt(x)\n", "    def test_refine_re_im_complex():\n        x = Symbol('x', complex=True)\n        assert refine(re(x), Q.real(x)) == x\n        assert refine(im(x), Q.real(x)) == 0\n        assert refine(re(x), Q.imaginary(x)) == 0\n        assert refine(im(x), Q.imaginary(x)) == -S.ImaginaryUnit * x\n\n"], "sample_1013": ["def test_evalf_tensorflow():\n    if not tensorflow:\n        skip('tensorflow not installed')\n    f = sqrt(x**2 + y**2)\n    f_lam = lambdify((x,y), f, 'tensorflow')\n    f_tf = f_lam(tensorflow.constant(3.0), tensorflow.constant(4.0))\n    assert (f_tf.numpy() == 5.0).numpy()\n\n", "    def test_lambdify_tensorflow_non_scalar_result():\n        from sympy import MatrixSymbol\n        m = MatrixSymbol('m', 2, 2)\n        f = lambdify(m, m*2, 'tensorflow')\n        a = numpy.array([[1, 2], [3, 4]])\n        assert numpy.array_equal(f(a), a * 2)\n", "def test_lambdify_matrix_function():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    C = Matrix([[9], [10]])\n    f = sympy.Function('f')\n    f_mat = lambdify(A, f(A))\n    assert f_mat(A) == f(A)\n    f_mat_AB = lambdify([A, B], f(A * B))\n    assert f_mat_AB(A, B) == f(A * B)\n    f_mat_ABC = lambdify([A, B, C], f(A * B + C))\n    assert f_mat_ABC(A, B, C) == f(A * B + C)\n", "def test_lambdify_piecewise_numpy():\n    from sympy import Piecewise\n    f = lambdify(x, Piecewise((x, x < 0), (x**2, x >= 0)), 'numpy')\n    assert f(-1) == -1\n    assert f(1) == 1\n", "def test_lambdify_with_tensorflow_matrix():\n    if not tensorflow:\n        skip(\"TensorFlow is not available.\")\n\n    x = tf.Variable(1.0)\n    f = lambdify(x, x**2, \"tensorflow\")\n    assert f(x) == tf.Tensor(1.0, shape=(), dtype=float32) \n    \n\n"], "sample_874": ["    def test_feature_names_out(self):\n        selector = StepSelector(step=2)\n        selector.fit(X, y)\n        assert_array_equal(selector.get_feature_names_out(), feature_names_t)\n", "def test_feature_names_out_with_sparse_input():\n    ss = StepSelector(step=2)\n    X_sparse = sp.csc_matrix(X)\n    ss.fit(X_sparse)\n    assert_array_equal(ss.get_feature_names_out(), feature_names_t)\n\n", "    def test_feature_names_out(self):\n        step_selector = StepSelector(step=2)\n\n        # Check feature names out with default input features\n        step_selector.fit(X)\n        assert_array_equal(step_selector.get_feature_names_out(), feature_names_t)\n\n        # Check feature names out with explicit input features\n        step_selector.fit(X, feature_names=feature_names)\n        assert_array_equal(step_selector.get_feature_names_out(feature_names), feature_names_t)\n\n        # Check feature names out with array-like input features\n        step_selector.fit(X, feature_names=np.array(feature_names))\n        assert_array_equal(step_selector.get_feature_names_out(np.array(feature_names)), feature_names_t)\n\n        # Check feature names out with empty input features\n        step_selector.fit(X, feature_names=[])\n        assert_array_equal(step_selector.get_feature_names_out(), feature_names_t)\n\n", "    def test_get_feature_names_out_with_feature_names_in(self):\n        selector = StepSelector(step=2)\n        selector.fit_transform(X, y)\n\n        assert_array_equal(\n            selector.get_feature_names_out(input_features=feature_names),\n            feature_names_t,\n        )\n", "    def test_transform_feature_names_out(self):\n        step_selector = StepSelector(step=2)\n        step_selector.fit(X, y)\n        Xt = step_selector.transform(X)\n        assert_array_equal(\n            step_selector.get_feature_names_out(feature_names), feature_names_t\n        )\n        assert_array_equal(\n            step_selector.get_feature_names_out(), feature_names_t\n        )\n"], "sample_1060": ["    def test_sympy_printing_Assignment(self):\n        a = symbols('a')\n        expr = Assignment(a, x + 1)\n        assert pycode(expr, language='sympy') == 'a = x + 1'\n", "def test_print_Mod():\n    assert pycode(Mod(x, 2)) == 'x % 2'\n    assert pycode(Mod(x, y)) == 'x % y'\n", "    def test_print_MatrixSymbol(self):\n        n = symbols('n', integer=True)\n        A = MatrixSymbol('A', n, n)\n        self.assertEqual(NumPyPrinter().doprint(A), 'A')\n", "    def test_print_Transpose(self):\n\n        a, b = symbols('a b')\n        matrix = MatrixSymbol('X', 2, 2)\n        t = matrix.T\n        assert pycode(t) == \"X.transpose()\"\n", "    def test_numpy_piecewise():\n        e = Piecewise((1, x > 0), (0, True))\n        assert NumPyPrinter().doprint(e) == \\\n            'numpy.select([x > 0], [1, 0], default=0)'\n\n\n"], "sample_978": ["def test_bspline_basis_set_repeated_knots():\n    d = 1\n    knots = [0, 0, 2, 3, 4]\n\n    splines = bspline_basis_set(d, knots, x)\n\n    assert len(splines) == len(knots) - d - 1\n\n    assert splines[0] == Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)), (0, True))\n    assert splines[1] == Piecewise((x/2 - 1, (x >= 2) & (x <= 3)), (0, True))\n", "    def test_bspline_basis_set_repeated_knots():\n        d = 1\n        knots = [0,0,2,3,4]\n        splines = bspline_basis_set(d, knots, x)\n        assert len(splines) == len(knots) - d - 1\n        assert splines[0] == Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)), (0, True))\n\n", "    def test_bsplines_basis_set():\n        knots = [0, 1, 2, 3, 4]\n        d = 2\n        splines = bspline_basis_set(d, knots, x)\n        assert len(splines) == len(knots) - d - 1\n        for i, spline in enumerate(splines):\n            assert isinstance(spline, Piecewise)\n", "    def test_bspline_basis_set_repeated_knots():\n        knots = [0, 0, 2, 3, 4]\n        d = 1\n        splines = bspline_basis_set(d, knots, x)\n        assert len(splines) == len(knots) - d - 1\n        assert splines[0] == Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)), (0, True)) \n", "def test_bspline_basis_set_repeated_knots():\n    knots = [0, 0, 2, 3, 4]\n    d = 1\n    splines = bspline_basis_set(d, knots, x)\n    assert len(splines) == len(knots) - d - 1\n    assert splines[0] == Piecewise((-x/2 + 1, (x >= 0) & (x <= 2)), (0, True))\n\n"], "sample_38": ["    def test_world2pix_simple(self):\n        \"\"\"\n        Test world2pix with a simple transformation\n        \"\"\"\n        with NumpyRNGContext(seed=12345):\n            filename = self._file_list[0]\n            hdr = fits.getheader(filename)\n            wcs_obj = wcs.WCS(hdr)\n            world = np.array([[1, 2]])\n            pix = wcs_obj.world2pix(world, 0)\n            assert_allclose(pix, [[10.0, 20.0]])\n", "    def test_wcssub(self):\n        \"\"\"Check WCS subsetting.\"\"\"\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', category=wcs.FITSFixedWarning)\n            w = wcs.WCS(get_pkg_data_filename('maps/fits_wcs.fits'))\n            subwcs = w.sub([1, 2])\n            assert subwcs.naxis == 2\n            assert set(subwcs.ctype) == {'RA---TAN', 'DEC--TAN'}\n", "    def test_reorient_celestial_first(self):\n        \"\"\"Test the reorient_celestial_first method.\"\"\"\n        with NumpyRNGContext(seed=0):\n            for filename in self._file_list:\n                hdul = fits.open(filename)\n                w = wcs.WCS(hdul[0].header)\n\n                # Only test if the wcs has celestial axes\n                if w.has_celestial:\n                    w_reoriented = w.reorient_celestial_first()\n                    assert w_reoriented.wcs.naxis == w.wcs.naxis\n                    for i in range(w_reoriented.wcs.naxis):\n                        if w.wcs.ctype[i].startswith('RA'):\n                            assert i == WCSSUB_CELESTIAL\n                        elif w.wcs.ctype[i].startswith('DEC'):\n                            assert i == WCSSUB_CELESTIAL + 1\n\n\n", "    def test_wcs_update(self):\n        for filename in self._file_list:\n            with open(filename, 'r') as f:\n                header = fits.Header.fromstring(f.read())\n            w = wcs.WCS(header)\n            header['CTYPE1'] = 'RA---TAN'\n            w.wcs.update(header, fix=True)\n            assert w.wcs.ctype[0] == 'RA---TAN'\n\n", "    def test_maps_read_and_evaluate(self):\n        for filename in self._file_list:\n            with open(filename, 'r') as f:\n                header = fits.Header.fromstring(f.read())\n\n            w = wcs.WCS(header)\n            x, y = np.mgrid[0:10, 0:10]\n            world = w.wcs_world2pix(np.array([x, y]), 0)\n            assert_array_equal(world, world[0])\n\n"], "sample_471": ["    def test_integerfield_invalid_input(self):\n        field = IntegerField()\n        with self.assertRaises(ValidationError):\n            field.clean('a')\n        with self.assertRaises(ValidationError):\n            field.clean('1.5')\n", "    def test_integerfield_to_python_accepts_strings(self):\n        field = IntegerField(required=False)\n        self.assertEqual(field.to_python('10'), 10)\n        self.assertIsNone(field.to_python(''))\n        self.assertRaises(ValidationError, field.to_python, 'abc')\n", "    def test_integer_field_with_min_value(self):\n        field = IntegerField(min_value=5)\n        self.assert ValidationError(field, '4')\n        self.assertNoValidationError(field, '5')\n        self.assertNoValidationError(field, '100')\n", "    def test_integerfield_max_value(self):\n        field = IntegerField(max_value=5)\n        self.assertFieldValidationPasses(field, 5)\n        with self.assertRaises(ValidationError):\n            field.clean(6)\n        self.assertFieldValidationPasses(field, 4)\n", "    def test_integer_field_empty_string_input(self):\n        field = IntegerField()\n        self.assert ValidationError(field, \"\", \"Enter a whole number.\")\n"], "sample_331": ["    def test_parse_datetime_with_time_zone(self):\n        dt = parse_datetime('2023-10-26T14:30:00+02:00')\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 26)\n        self.assertEqual(dt.hour, 14)\n        self.assertEqual(dt.minute, 30)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.tzinfo.utcoffset(dt).total_seconds(), 7200)\n", "    def test_parse_date_invalid_format(self):\n        with self.assertRaises(TypeError):\n            parse_date(12345)\n\n", "    def test_parse_duration_postgres_interval(self):\n        self.assertEqual(parse_duration('3 days 04:05:06'), timedelta(days=3, seconds=14706))\n        self.assertEqual(parse_duration('3 days'), timedelta(days=3))\n        self.assertEqual(parse_duration('-3 days 04:05:06'), timedelta(days=-3, seconds=-14706))\n        self.assertEqual(parse_duration('+4:05:06'), timedelta(seconds=14706))\n        self.assertEqual(parse_duration('04:05:06.123456'), timedelta(seconds=14706.123456))\n", "    def test_parse_datetime_with_time_zone(self):\n        dt = parse_datetime('2023-10-27T10:30:00+02:00')\n        self.assertEqual(dt, datetime(2023, 10, 27, 10, 30, 0, tzinfo=get_fixed_timezone(120)))\n", "    def test_parse_datetime_with_timezone(self):\n        dt_str = '2023-10-27T10:00:00+02:00'\n        dt = parse_datetime(dt_str)\n        self.assertEqual(dt.year, 2023)\n        self.assertEqual(dt.month, 10)\n        self.assertEqual(dt.day, 27)\n        self.assertEqual(dt.hour, 10)\n        self.assertEqual(dt.minute, 0)\n        self.assertEqual(dt.second, 0)\n        self.assertEqual(dt.microsecond, 0)\n        self.assertEqual(\n            dt.tzinfo,\n            get_fixed_timezone(120),  # +02:00\n        )\n"], "sample_729": ["    def test_multitask_lasso_cv_path(self):\n        X, y = load_boston(return_X_y=True)\n        X = X.reshape((-1, 13, 2))  \n        y = y.reshape((-1, 1))\n        n_tasks = 2\n        y = np.concatenate([y, y], axis=1)\n        # Test that the path is increasing\n        for alpha in self.model.alphas_:\n            lasso = MultiTaskLasso(alpha=alpha, fit_intercept=False)\n            lasso.fit(X, y)\n            coefs = lasso.coef_\n            assert_true(np.all(coefs[0] >= coefs[1]))\n\n", "    def test_enet_path_positive(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        y = np.array([1, 2, 3])\n        alphas = np.array([1, 0.5, 0.1])\n        l1_ratios = [0.5]\n         coef, _, _ = enet_path(X, y, alphas, l1_ratios, fit_intercept=False)\n        assert_array_equal(coef[0], np.array([0., 0.]))\n        assert_array_equal(coef[-1].round(decimals=5), np.array([0.11111, 0.22222]))\n\n", "    def test_multi_task_lasso_cv_sparse_input(self):\n        X, y = load_boston(return_X_y=True)\n        X = sparse.csr_matrix(X)\n        n_samples, n_features = X.shape\n        y = np.vstack((y, y)).T\n        clf = MultiTaskLassoCV().fit(X, y)\n        assert_equal(clf.coef_.shape, (2, n_features))\n\n", "    def test_enet_path_multitask(self):\n        X, y = make_regression(n_samples=10, n_features=5, n_targets=3,\n                              random_state=0)\n        # Check if enet path returns same results\n        # for a range of alphas\n        for path in [enet_path, lasso_path]:\n            for fit_intercept in [True, False]:\n                coefs = path(X, y, l1_ratio=0.5, eps=1e-3,\n                             n_alphas=5, fit_intercept=fit_intercept,\n                             tol=1e-6, positive=False)\n                assert_equal(coefs[0].shape, (3, 5))\n\n", "    def test_multi_task_lasso_cv_sparse_input(self):\n        X = sparse.csr_matrix([[0, 1], [1, 0], [2, 1]])\n        y = np.array([[1], [2], [3]])\n        clf = MultiTaskLassoCV(cv=3)\n        clf.fit(X, y)\n        assert_equal(clf.coef_.shape, (y.shape[1], X.shape[1]))\n"], "sample_858": ["    def test_voting_classifier_fit_empty_weights(self):\n        clf1 = LogisticRegression()\n        clf2 = RandomForestClassifier()\n        eclf = VotingClassifier([('lr', clf1), ('rf', clf2)],\n                               weights=[1, 0])\n        eclf.fit(X, y)\n        assert_array_equal(eclf.named_estimators_['lr'].classes_,\n                           eclf.classes_)\n", "    def test_voting_regressor_with_different_estimators_types():\n        # Test if VotingRegressor works with different types of regressors\n        estimators = [('lr', LinearRegression()),\n                      ('rf', RandomForestRegressor(n_estimators=10, random_state=42))]\n        reg = VotingRegressor(estimators=estimators)\n        reg.fit(X_r, y_r)\n        reg.predict(X_r[:1])\n", "    def test_voting_classifier_voting_hard_with_sample_weight(self):\n        X, y = make_multilabel_classification(\n            n_samples=50, n_features=2, random_state=0\n        )\n\n        clf1 = LogisticRegression()\n        clf2 = SVC(probability=True)\n        eclf = VotingClassifier(\n            estimators=[('lr', clf1), ('svc', clf2)], voting='hard',\n        )\n\n        # check if sample weights are passed correctly\n\n        sample_weight = np.ones(X.shape[0])\n\n        eclf.fit(X, y, sample_weight=sample_weight)\n\n       \n        # check if fitted estimators have seen sample weights\n        for est in eclf.estimators_:\n            if hasattr(est, 'fit') and hasattr(est.fit, 'sample_weight'):\n                assert hasattr(est, 'sample_weight_')\n                    \n", "    def test_voting_regressor_sample_weight(self):\n        # Check if sample weights are handled correctly\n        clf1 = LinearRegression()\n        clf2 = DummyRegressor()\n        X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n        y = np.array([2, 6, 12, 20, 30, 42])\n        sample_weight = np.array([1, 2, 1, 1, 2, 1])\n        er = VotingRegressor([('lr', clf1), ('dr', clf2)])\n        er.fit(X, y, sample_weight=sample_weight)\n        pred = er.predict(X)\n        # We can't directly compare predictions due to the randomness\n        # of DummyRegressor, but we can check if fit succeeded\n        assert er.named_estimators_.lr.coef_ is not None\n\n", "    def test_voting_regressor_gridsearchcv(self):\n        # check if VotingRegressor works with GridSearchCV\n        estimators = [('lr', LinearRegression()),\n                      ('rf', RandomForestRegressor(random_state=0))]\n        regressor = VotingRegressor(estimators)\n        grid = {\n            'rf__n_estimators': [10, 20],\n            'rf__max_depth': [3, 5]\n        }\n        gs = GridSearchCV(estimator=regressor, param_grid=grid, cv=3)\n        gs.fit(X_r[:100], y_r[:100])\n\n        assert isinstance(gs.best_estimator_, VotingRegressor)\n        assert hasattr(gs.best_estimator_, 'estimators_')\n\n"], "sample_607": ["    def test_remove_duplicates_keep_first(self, dummy_duplicated_entrypoints):\n        unique_entrypoints = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n        assert len(unique_entrypoints) == 2\n        assert unique_entrypoints[0].name == \"engine1\"\n        assert unique_entrypoints[1].name == \"engine2\"\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    unique_eps = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(unique_eps) == 2\n    assert [ep.name for ep in unique_eps] == [\"engine1\", \"engine2\"]\n", "    def test_remove_duplicates(dummy_duplicated_entrypoints):\n        unique_entrypoints = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n        assert len(unique_entrypoints) == 2\n        expected_names = {\"engine1\", \"engine2\"}\n        assert {ep.name for ep in unique_entrypoints} == expected_names\n\n", "    def test_backends_dict_from_pkg(self):\n        entrypoints = [\n            pkg_resources.EntryPoint.parse(\n                \"engine1 = xarray.tests.test_plugins:DummyBackendEntrypointArgs\"\n            ),\n            pkg_resources.EntryPoint.parse(\n                \"engine2 = xarray.tests.test_plugins:DummyBackendEntrypointKwargs\"\n            ),\n        ]\n        backends = plugins.backends_dict_from_pkg(entrypoints)\n        assert set(backends.keys()) == {\"engine1\", \"engine2\"}\n        assert isinstance(backends[\"engine1\"], DummyBackendEntrypointArgs)\n        assert isinstance(backends[\"engine2\"], DummyBackendEntrypointKwargs)\n", "def test_remove_duplicates(dummy_duplicated_entrypoints):\n    pkg_entrypoints = plugins.remove_duplicates(dummy_duplicated_entrypoints)\n    assert len(pkg_entrypoints) == 2\n    names = [ep.name for ep in pkg_entrypoints]\n    assert \"engine1\" in names\n    assert \"engine2\" in names\n"], "sample_570": ["    def test_kde_bivariate_cumulative(self, x, y, rng):\n        kde = KDE(cumulative=True)\n        density, bins = kde(x, y)\n\n        # Test that the density integrates to 1\n\n        assert_array_almost_equal(self.integrate(density.flatten(), bins[0]), 1)\n", "    def test_kde_cumulative(self, x, rng):\n        kde = KDE(cumulative=True)\n        x_eval = np.linspace(-3, 3, 100)\n        density, _ = kde(x, weights=None)\n        cdf_estimate = density\n\n        expected = smdist.norm.cdf(x_eval)\n        assert_array_almost_equal(cdf_estimate, expected, decimal=2)\n", "    def test_kde_univariate_cumulative(self, x, rng):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        assert np.allclose(density[-1], 1)\n        assert np.allclose(self.integrate(density, support), 1)\n", "    def test_kde_univariate_cumulative(self, x, rng):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        assert_array_almost_equal(density[-1], 1)\n        assert_array_almost_equal(\n            self.integrate(density, support), 1, decimal=2\n        )\n\n", "    def test_kde_univariate_cumulative(self, x):\n        kde = KDE(cumulative=True)\n        density, support = kde(x)\n        assert density[0] == 0\n        assert_array_almost_equal(density[-1], 1)\n        if not _no_scipy:\n            assert_array_almost_equal(density,\n                                      [self.integrate(kde._fit(x).pdf(support), support)])\n\n"], "sample_47": ["    def test_callable_setting_wrapper(self):\n        class TestCallable:\n                return 'TestCallable'\n\n        setting_wrapper = CallableSettingWrapper(TestCallable())\n        self.assertEqual(repr(setting_wrapper), repr(TestCallable()))\n", "    def test_callable_setting_wrapper_repr(self):\n        class MockSettings:\n                return lambda: key\n        settings = MockSettings()\n        wrapper = CallableSettingWrapper(settings['foo'])\n        self.assertEqual(repr(wrapper), '<bound method MockSettings.__getitem__ of <__main__.MockSettings object at ...> at ...>')\n", "    def test_callable_setting_wrapper(self):\n            return 'hello'\n        wrapper = CallableSettingWrapper(my_callable)\n        self.assertEqual(str(wrapper), str(my_callable))\n\n", "    def test_callable_setting_wrapper_repr(self):\n        class MySetting:\n                return 'MySetting'\n\n        wrapped = CallableSettingWrapper(MySetting())\n        self.assertEqual(repr(wrapped), repr(MySetting()))\n", "    def test_sensitive_variables_wrapper(self):\n        request = RequestFactory().get('/')\n        \n        request.user = User()\n        request.POST = {'secret': 'top_secret'}\n\n        with override_settings(DEFAULT_EXCEPTION_REPORTER_FILTER='tests.views.SafeExceptionReporterFilter'):\n            response = self.client.get(reverse('sensitive_variables_wrapper'), follow=True)\n\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, 'Sensitive variables are obfuscated')\n"], "sample_1053": ["    def test_numbers_is_integer():\n        assert numbers.Number.is_integer(1)\n        assert numbers.Number.is_integer(Integer(1))\n        assert not numbers.Number.is_integer(1.5)\n        assert not numbers.Number.is_integer(Float(1.5))\n        assert Integer(1).is_integer()\n        assert not Float(1.5).is_integer()\n\n", "    def test_Mod():\n        assert Mod(5, 3) == 2\n        assert Mod(5, 3.0) == 2.0\n        assert Mod(-5, 3) == 1\n        assert Mod(5, -3) == 2\n        assert Mod(-5, -3) == 1\n        assert Mod(5.5, 3) == 2.5\n        assert Mod(5.5, 3.0) == 2.5\n\n", "def test_gcd_lehmer():\n    assert igcd_lehmer(12345, 67890) == 3\n    assert igcd_lehmer(0, 12) == 12\n    assert igcd_lehmer(12, 0) == 12\n    assert igcd_lehmer(0, 0) == 0\n", "def test_issue_20391():\n    x = Symbol('x')\n    assert (1/x).as_base_exp() == (1, -1)\n\n", "def test_Integer_divmod():\n    assert Integer(10).divmod(Integer(3)) == (Integer(3), Integer(1))\n    assert Integer(10).divmod(Integer(-3)) == (Integer(-3), Integer(1))\n    assert Integer(-10).divmod(Integer(3)) == (Integer(-3), Integer(-1))\n    assert Integer(-10).divmod(Integer(-3)) == (Integer(3), Integer(-1))\n"], "sample_642": ["    def test_preprocess_options(self, pop_pylintrc: None, capsys: CaptureFixture[str]) -> None:\n        run = Run(config.Options())\n        args = [\"--init-hook\", \"print('hello')\", \"--load-plugins\", \"pylint.extensions.foo\"]\n        self._preprocess_options(run, args)\n\n        assert run._plugins == [\"pylint.extensions.foo\"]\n\n        out, err = capsys.readouterr()\n        assert out == \"hello\\n\"\n        assert err == \"\"\n\n", "    def test_preprocess_options(self, linters: Run) -> None:\n        with mock.patch(\"pylint.config.arg_helper._parse_rich_type_value\") as mock_parse_rich_type_value:\n            mock_parse_rich_type_value.return_value = \"mock_value\"\n            linters.args = [\"--init-hook=print('hello')\", \"--rcfile=a.rc\", \"--output=b\", \"--load-plugins=foo,bar\", \"--verbose\"]\n            linters._preprocess_options(linters.args)\n            assert linters._rcfile == \"a.rc\"\n            assert linters._output == \"b\"\n            assert linters._plugins == [\"foo\", \"bar\"]\n            assert linters.verbose is True\n\n", "def test_preprocess_options(capture_stdout: CaptureFixture) -> None:\n    \"\"\"Test that pre-processing options works as expected.\"\"\"\n\n    with tempdir() as tmpdir:\n        pylint_rc = Path(tmpdir) / \".pylintrc\"\n\n        with open(pylint_rc, \"w\") as f:\n            f.write(\n                \"[MESSAGES CONTROL]\\n\"\n                \"disable=missing-docstring\\n\"\n            )\n\n        run = Run([f\"--rcfile={pylint_rc}\", \"--verbose\", \"test.py\"], do_exit=False)\n\n        run._preprocess_options(run._args)\n        assert run.verbose  \n\n", "def test_preprocess_options_init_hook(run: Run, capsys: CaptureFixture[str]) -> None:\n    with tempdir() as tmpdirname:\n        # write a file to simulate an existing pylint.rc file\n        rc_file = Path(tmpdirname, \".pylintrc\")\n        rc_file.write_text(\"[MESSAGES CONTROL]\\ndisable=C0111\")\n\n        # use init-hook argument to execute code\n        args = [\"--init-hook\", \"print('Hello from init_hook')\", tmpdirname, \"--rcfile\", str(rc_file)]\n        config.parse_options(run, args)\n        assert run._rcfile == str(rc_file)\n        captured = capsys.readouterr()\n        assert captured.out == \"Hello from init_hook\\n\"\n", "def test_preprocess_options(\n    run: Run,\n    capfd: CaptureFixture,"], "sample_146": ["    def test_check_setting_language_code_invalid(self):\n        errors = check_setting_language_code(None, **{})\n        self.assertEqual(len(errors), 1)\n        self.assertIsInstance(errors[0], Error)\n", "    def test_check_setting_language_code(self):\n        for tag in self.valid_tags:\n            with override_settings(LANGUAGE_CODE=tag):\n                errors = check_setting_language_code(None, **{})\n                self.assertEqual(errors, [])\n\n        for tag in self.invalid_tags:\n            with override_settings(LANGUAGE_CODE=tag):\n                errors = check_setting_language_code(None, **{})\n                self.assertEqual(errors, [Error(\n                    'You have provided an invalid value for the LANGUAGE_CODE setting: {!r}.',\n                    id='translation.E001',\n                )])\n", "    def test_check_setting_languages(self):\n\n        with override_settings(LANGUAGES=[('en', 'English'),\n                                          ('fr', 'French'),\n                                          (123, 'French')]):\n            errors = check_setting_languages(None, **{})\n\n            self.assertEqual(len(errors), 1)\n            self.assertEqual(errors[0].id, 'translation.E002')\n            self.assertEqual(errors[0].msg, 'You have provided an invalid language code in the LANGUAGES setting: 123.')\n\n", "    def test_check_setting_languages(self):\n        for tag in self.valid_tags:\n            with override_settings(LANGUAGES=[(tag, 'Django')]):\n                self.assertEqual(check_setting_languages(None, **{}), [])\n        for tag in self.invalid_tags:\n            with override_settings(LANGUAGES=[(tag, 'Django')]):\n                errors = check_setting_languages(None, **{})\n                self.assertEqual(len(errors), 1)\n                self.assertIsInstance(errors[0], Error)\n", "    def test_check_setting_languages(self):\n        for tag in self.valid_tags:\n            with override_settings(LANGUAGES=[(tag, 'language')]):\n                self.assertEqual(check_setting_languages([], **{}), [])\n        for tag in self.invalid_tags:\n            with override_settings(LANGUAGES=[(tag, 'language')]):\n                msg = Error(\n                    'You have provided an invalid language code in the LANGUAGES setting: {!r}.'.format(tag),\n                    id='translation.E002',\n                )\n                self.assertEqual(check_setting_languages([], **{}), [msg])\n"], "sample_927": ["def test_cpp_class_template():\n    check(\n        'class',\n        \"template <typename T> class MyClass {};\",\n        idDict={1: \"myclass\"},\n        output=\"template <typename T> class MyClass {};\",\n        asTextOutput=\"MyClass\"\n    )\n\n", "def test_cpp_nested_namespace():\n    check(\n        \"class\",\n        \"\"\"\n        namespace A {{\n            namespace B {{\n                class C {{\n                    public:\n                        int foo();\n                }};\n            }}\n        }}\n        \"\"\",\n        {1: \"a-b-c\"},\n        \"\"\"\n        A::B::C\n        \"\"\",\n        key=\"A::B::C\",\n        asTextOutput=\"A::B::C\"\n    )\n\n", "    def test_cpp_function_template_specialization():\n        check(\"function\", \"void foo(int a, double b)\",\n              idDict={1: 'foo__int_double'},\n              output=\"void foo(int a, double b)\",\n              key=\"foo\")\n", "    def test_cpp_function_params():\n        check('function', 'void func(int a, float b, const std::string& c = \"hello\")',\n              idDict={})\n", "def test_nested_class():\n    check(\n        'class',\n        '''\n        class Outer {\n        public:\n            class Inner {\n            public:\n                void foo() const {}\n            };\n        };\n        ''',\n        {\n            1: 'outer',\n            2: 'outer_inner',\n            3: None\n        },\n        '''\n        class Outer {\n        public:\n            class Inner {\n            public:\n                void foo() const;\n            };\n        };\n        ''',\n        'Outer::Inner::',\n        asTextOutput='Outer::Inner::foo() const'\n    )\n"], "sample_1175": ["def test_pretty_trace():\n    M = Matrix([[1, 2], [3, 4]])\n    assert pretty(Tr(M)) == \"Tr([1, 2; 3, 4])\"\n", "def test_pretty_piecewise():\n    expr = Piecewise((x, x < 1), (x**2, True))\n    assert pretty(expr) == '\u23a7x if x < 1\\n\u23a8\\n\u23a9x\u00b2 otherwise'\n", "def test_pretty_abs_function():\n    assert pretty(Abs(x + 1)) == \" | x + 1 | \"\n    assert pretty(Abs(x**2 - 1)) == \" | x**2 - 1 | \"\n    assert pretty(Abs(x + y)) == \" | x + y | \"\n", "    def test_pretty_Heaviside():\n        assert pretty(Heaviside(x)) == 'H(x)'\n        assert pretty(Heaviside(x - 1)) == 'H(x - 1)'\n        assert pretty(Heaviside(x + y)) == 'H(x + y)'\n\n\n\n", "def test_pretty_Heaviside():\n    assert pretty(Heaviside(x)) == 'H(x)'\n    assert pretty(Heaviside(x - 1)) == 'H(x - 1)'\n    assert pretty(Heaviside(2*x + 1)) == 'H(2 x + 1)'\n\n"], "sample_901": ["    def test_minibatch_kmeans_sparse_input(representation, algo, dtype):\n        # Check MiniBatchKMeans works with sparse input\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n                                 init='k-means++',\n                                 algorithm=algo,\n                                 random_state=42,\n                                 tol=1e-4).fit(X_csr)\n        assert kmeans.cluster_centers_.shape == (n_clusters, n_features)\n        assert kmeans.labels_.shape[0] == n_samples\n\n", "    def test_sparse_input_mini_batch(representation, algo, dtype):\n        # Check MiniBatchKMeans works with sparse input\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters,\n                                 init='k-means++',\n                                 n_init=1,\n                                 max_iter=10,\n                                 random_state=42,\n                                 algorithm=algo).fit(X_csr)\n\n        # Check labels are consistent with dense input\n        kmeans_dense = KMeans(n_clusters=n_clusters,\n                              init='k-means++',\n                              n_init=1,\n                              max_iter=10,\n                              random_state=42,\n                              algorithm=algo).fit(X)\n\n        assert_array_equal(kmeans.labels_, kmeans_dense.labels_)\n", "    def test_kmeans_sparse_empty_cluster_warning(self):\n        # Test that a warning is raised when an empty cluster occurs\n        X = np.array([[1, 2], [1, 2], [2, 2]])\n        kmeans = KMeans(n_clusters=3, random_state=0)\n        with pytest.warns(ConvergenceWarning) as record:\n            kmeans.fit(X)\n        assert len(record) == 1\n        assert \"empty cluster\" in str(record[0].message)\n", "def test_mini_batch_kmeans_sparse_input_empty_cluster(\n        representation, algo, dtype):\n    # Check if MiniBatchKMeans handles empty clusters correctly\n\n    # create dataset where some clusters are empty\n    X2 = np.array([[1.0, 2.0], [2.0, 1.0], [1.5, 1.5], [10.0, 10.0]])\n    X2_sparse = sp.csr_matrix(X2)\n\n    if representation == 'sparse':\n        X = X2_sparse\n    else:\n        X = X2\n\n    km = MiniBatchKMeans(n_clusters=4, init=\"k-means++\",\n                          random_state=42, dtype=dtype,\n                          algorithm=algo)\n    # make sure an empty cluster is created\n    try:\n        km.fit(X)\n    except ValueError:\n        return\n\n    # test if the number of clusters is still 4, even though\n    # some clusters are empty\n    assert_array_equal(km.cluster_centers_.shape[0], 4)\n\n\n\n", "    def test_sparse_input(representation, algo, dtype):\n        # Test that KMeans works with sparse input\n        if representation == 'dense':\n            X_test = X.astype(dtype)\n        else:\n            X_test = X_csr.astype(dtype)\n\n        kmeans = KMeans(n_clusters=n_clusters, init='random',\n                        algorithm=algo, random_state=42).fit(X_test)\n        assert_almost_equal(kmeans.cluster_centers_,\n                            centers.astype(dtype), decimal=2)\n"], "sample_303": ["    def test_runshell_calls_subprocess_run(self, mock_subprocess_run):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.settings_to_cmd_args_env') as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (['psql', '-c', 'SELECT 1'], {})\n            self.client.runshell([])\n            mock_subprocess_run.assert_called_once_with(['psql', '-c', 'SELECT 1'], env={}, check=True)\n\n", "    def test_settings_to_cmd_args_env_not_implemented(self):\n        with self.assertRaises(NotImplementedError):\n            BaseDatabaseClient.settings_to_cmd_args_env({}, [])\n", "    def test_runshell_calls_subprocess_run(self, mock_subprocess_run):\n        with mock.patch('django.db.backends.base.client.BaseDatabaseClient.settings_to_cmd_args_env') as mock_settings_to_cmd_args_env:\n            mock_settings_to_cmd_args_env.return_value = (['psql', '-c', 'SELECT 1'], {})\n            self.client.runshell(['SELECT 1'])\n            mock_subprocess_run.assert_called_once_with(['psql', '-c', 'SELECT 1'], env={}, check=True)\n", "    def test_settings_to_cmd_args_env_raises_notimplementederror(self):\n        with self.assertRaises(NotImplementedError):\n            self.client.settings_to_cmd_args_env({}, {})\n", "    def test_runshell_raises_not_implemented_error(self):\n        with self.assertRaises(NotImplementedError):\n            self.client.runshell([])\n"], "sample_83": ["    def test_filter_function_with_flags(self):\n        @functools.wraps(lambda x: x)\n            return value\n\n        self.library.filter('test_filter', filter_func, expects_localtime=True)\n        self.assertTrue(getattr(filter_func, 'expects_localtime', False))\n        self.assertTrue(getattr(filter_func._decorated_function, 'expects_localtime', False))\n", "    def test_filter_function_with_flags(self):\n        @self.library.filter(is_safe=True, expects_localtime=True)\n            return value\n\n        self.assertTrue(my_filter.is_safe)\n        self.assertTrue(my_filter.expects_localtime)\n", "    def test_filter_takes_context(self):\n        @functools.wraps(lambda context: 'Hello')\n            return 'Hello'\n\n        self.library.filter('my_filter', my_filter, takes_context=True)\n        self.assertEqual(self.library.filters['my_filter'](\n            {'some_key': 'some_value'}), 'Hello')\n", "    def test_filter_with_flags(self):\n        @self.library.filter(is_safe=True)\n            return value\n\n        self.assertTrue(safe_filter._is_safe)\n        self.assertEqual(safe_filter._decorated_function._is_safe, True)\n\n", "    def test_filter_function_with_flags(self):\n        @self.library.filter(is_safe=True, expects_localtime=True)\n            return value\n\n        self.assertTrue(my_filter.is_safe)\n        self.assertTrue(my_filter.expects_localtime)\n"], "sample_149": ["    def test_user_model_username_field_not_in_required_fields(self):\n        with override_settings(AUTH_USER_MODEL='auth_tests.CustomUserUsernameFieldNotInRequiredFields'):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 0)\n", "    def test_is_authenticated_is_method(self):\n        errors = check_user_model(app_configs=self.apps)\n        self.assertEqual(len(errors), 1)\n        self.assertEqual(errors[0].id, 'auth.C010')\n", "    def test_user_model_username_field_not_unique_with_custom_backend(self):\n        with override_settings(\n            AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername',\n            AUTHENTICATION_BACKENDS=[\n                'django.contrib.auth.backends.ModelBackend',\n                'django.contrib.auth.backends.RemoteUserBackend',\n            ],\n        ):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], checks.Warning)\n", "    def test_user_model_required_fields_list(self):\n        with isolate_apps('auth_tests', attr_name='apps'):\n            with override_settings(AUTH_USER_MODEL='auth_tests.CustomUser'):\n                errors = check_user_model(app_configs=self.apps)\n        self.assertEqual(len(errors), 0)\n", "    def test_user_model_username_field_unique(self):\n        with override_settings(\n            AUTH_USER_MODEL='auth_tests.CustomUserNonUniqueUsername',\n            AUTHENTICATION_BACKENDS=[\n                'django.contrib.auth.backends.ModelBackend',\n            ],\n        ):\n            errors = check_user_model(app_configs=self.apps)\n            self.assertEqual(len(errors), 1)\n            self.assertIsInstance(errors[0], checks.Error)\n            self.assertEqual(errors[0].id, 'auth.E003')\n"], "sample_974": ["    def test_ccode_indexed_base_with_for(self):\n        n = symbols('n')\n        i = Idx('i', n)\n        A = IndexedBase('A', shape=(n,))\n        expr = For(i, Range(n), Assignment(A[i], i**2))\n        code = ccode(expr)\n        \n        expected_code = \"\"\"", "    def test_piecewise_no_default():\n        e = Piecewise((x + 1, x > 0), (x**2, x < 0))\n        raises(ValueError, lambda: ccode(e))\n", "def test_ccode_indexed_base_assign():\n    i, j = symbols('i j', integer=True)\n    A = IndexedBase('A', shape=(3,3))\n    expr = Eq(A[i, j], i + 2*j)\n    assert ccode(expr, contract=False) == 'A[i, j] = i + 2*j;' \n", "    def test_ccode_for_loop(self):\n        i = Idx('i', 3)\n        A = IndexedBase('A', shape=(3,))\n        expr = For(i, Range(3), Assignment(A[i], i*A[i]))\n        code = ccode(expr)\n        expected = (\n            'for (int i = 0; i < 3; i++){\\n'\n            '  A[i] = i*A[i];\\n'\n            '}\\n'\n        )\n        self.assertEqual(code, expected)\n", "def test_ccode_indexed_base():\n    i = Idx('i', 3)\n    A = IndexedBase('A', shape=(3,))\n    expr =  A[i]\n    code = ccode(expr)\n    assert code == 'A[i]' \n"], "sample_468": ["    def test_context_new(self):\n        context = Context({'foo': 'bar'})\n        new_context = context.new({'baz': 'quux'})\n        self.assertEqual(new_context['baz'], 'quux')\n        with self.assertRaises(KeyError):\n            new_context['foo']\n", "    def test_context_pop(self):\n        context = Context({'foo': 'bar'})\n        context.push({'baz': 'quux'})\n        self.assertEqual(context['baz'], 'quux')\n        context.pop()\n        with self.assertRaises(ContextPopException):\n            context.pop()\n        self.assertEqual(context['foo'], 'bar')\n", "    def test_context_pop(self):\n        context = Context({'a': 1})\n        context.push({'b': 2})\n        self.assertEqual(context['b'], 2)\n\n        context.pop()\n        with self.assertRaises(ContextPopException):\n            context.pop()\n\n        self.assertEqual(context['a'], 1)\n", "    def test_context_pop(self):\n        c = Context({'foo': 'bar'})\n        c.push({'baz': 'quux'})\n        self.assertEqual(c['baz'], 'quux')\n        c.pop()\n        with self.assertRaises(ContextPopException):\n            c.pop()\n        self.assertEqual(c['foo'], 'bar')\n", "    def test_context_pop_exception(self):\n        context = Context()\n        context.push()\n        with self.assertRaises(ContextPopException):\n            context.pop()\n            context.pop()\n\n"], "sample_1107": ["def test_rotations():\n    assert list(rotations([1, 2, 3])) == [[1, 2, 3], [2, 3, 1], [3, 1, 2]]\n    assert list(rotations([1, 2, 3], -1)) == [[1, 2, 3], [3, 1, 2], [2, 3, 1]]\n\n\n\n", "def test_strong_connected_components():\n    g = {0: [1, 2], 1: [2], 2: [0, 3], 3: [3]}\n    assert strongly_connected_components(g) == [[0, 1, 2], [3]]\n\n", "    def test_permute_signs():\n        assert list(permute_signs((0, 1, 2))) == [\n            (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)\n\n        ]\n", "def test_signed_permutations():\n    assert list(signed_permutations((0, 1))) == [(0, 1), (0, -1)]\n    assert set(signed_permutations((0, 1, 2))) == set([\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2),\n        (0, 2, 1), (0, -2, 1), (0, 2, -1), (0, -2, -1),\n        (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2),\n        (1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0),\n        (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0), (2, -1, 0), (-2, -1, 0)])\n\n", "   def test_multiset_partitions_taocp():\n        assert list(multiset_partitions_taocp([1, 1, 2], 2)) == [[[1, 1], [2]], [[1, 2], [1]]]\n        assert list(multiset_partitions_taocp([1, 1, 1, 2], 2)) == [[[1, 1, 1], [2]], [[1, 1], [1, 2]], [[1, 2], [1, 1]]]\n"], "sample_1136": ["    def test_ExpressionDomain_gcd_lcm(self):\n        P = EX\n\n        f = P('x**2 + y')\n        g = P('x + y**2')\n\n        assert _strict_eq(P.gcd(f, g), P('1'))\n        assert _strict_eq(P.lcm(f, g), P('x**2*y**2 + x*y**3 + x**3 + y**4'))\n", "def test_EX_gcd_lcm():\n    f = EX(x**2 + 1)\n    g = EX(x + 1)\n\n    assert f.gcd(g) == EX(1)\n    assert f.lcm(g) == EX((x**2 + 1)*(x + 1))\n", "    def test_ExpressionDomain_gcd_lcm(self):\n        EXx = EX.x\n        EXy = EX.y\n\n        assert EX(2).gcd(EX(4)) == EX(2)\n        assert EX(4).gcd(EX(2)) == EX(2)\n        assert EX(6).gcd(EX(9)) == EX(3)\n\n        assert EX(2).lcm(EX(4)) == EX(4)\n        assert EX(4).lcm(EX(2)) == EX(4)\n        assert EX(6).lcm(EX(9)) == EX(18)\n\n        assert EX(EXx**2 + EXx).gcd(EX(EXx + 1)) == EX(EXx + 1)\n        assert EX(EXx**2 + EXx).lcm(EX(EXx + 1)) == EX(EXx*(EXx + 1))\n\n        assert EX(EXy**2 - 1).gcd(EX(EXy - 1)) == EX(EXy - 1)\n        assert EX(EXy**2 - 1).lcm(EX(EXy - 1)) == EX((EXy - 1)*(EXy + 1))\n", "def test_ExpressionDomain_gcd():\n    F = EX\n    assert F.gcd(F(x + 1), F(x)) == F(1)\n    assert F.gcd(F(x**2 + 2*x + 1), F(x + 1)) == F(x + 1)\n", "def test_ExpressionDomain_pickle():\n    a = EX(x**2 + 2*x + 1)\n    b = pickle.loads(pickle.dumps(a))\n    assert a == b\n"], "sample_36": ["    def test_biweight_midcorrelation_outlier_robustness():\n        with NumpyRNGContext(42):\n            x = normal(0, 1, size=100)\n            y = 2*x + randn(100)*0.1\n            x[0] = 100  # Outlier\n            corr_true = np.corrcoef(x, y)[0, 1]\n            corr_bicorr = biweight_midcorrelation(x, y)\n        assert_allclose(corr_bicorr, 2.0, rtol=0.1)\n        assert_not_allclose(corr_true, 2.0)\n", "compilation error", "    def test_biweight_location_empty_array():\n        with pytest.raises(ValueError):\n            biweight_location(np.array([]))\n", "    def test_biweight_midcovariance_nan():\n        with NumpyRNGContext(12345):\n            x = randn(100)\n            y = randn(100)\n        # Introduce NaN values\n        x[5] = np.nan\n        y[10] = np.nan\n        cov = biweight_midcovariance([x, y])\n        assert np.isnan(cov[0, 1])\n        assert np.isnan(cov[1, 0])\n\n", "    def test_biweight_midcorrelation_scalar_location():\n        with NumpyRNGContext(12345):\n            x = normal(0, 1, 200)\n            y = normal(0, 3, 200)\n            x[0] = 30.0\n        bicorr = biweight_midcorrelation(x, y, M=0)\n        assert_allclose(bicorr, biweight_midcorrelation(x, y), rtol=1e-6)\n"], "sample_575": ["    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        labels = formatter.format_ticks(locs)\n        assert all(len(l) <= 5 for l in labels)\n\n", "    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        labels = formatter.format_ticks(locs)\n        assert labels == [\"1\", \"3\", \"9\"]\n\n", "    def test_label_formatter(self, x):\n        a, locs = self.setup_labels(x, formatter=mpl.ticker.ScalarFormatter(useOffset=False))\n        labels = a.major.formatter.format_ticks(locs)\n        assert all(l.isdigit() for l in labels)\n\n", "    def test_label_concise(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        labels = formatter.format_ticks(locs)\n        assert len(labels) == len(locs)\n        # TODO more specific checks on label format?\n\n", "    def test_label_concise_formatter(self, x):\n        a, locs = self.setup_labels(x, concise=True)\n        formatter = a.major.formatter\n        assert isinstance(formatter, mpl.dates.ConciseDateFormatter)\n        # TODO actual assertions about the formatter's output\n"], "sample_608": ["compilation error", "    def test_diff_array_repr_with_dims(self):\n        a = xr.DataArray(np.ones((2, 3)), dims=[\"x\", \"y\"])\n        b = xr.DataArray(np.zeros((2, 3)), dims=[\"x\", \"yz\"])\n        with pytest.raises(AssertionError):\n            a.equals(b)\n\n        expected = dedent(\n            \"\"\"\n            Left and right DataArray objects are not equal\n            Differing dimensions:\n                ({}) != ({})\n            Differing values:\n            L\n                [[1. 1. 1.]\n                  [1. 1. 1.]]\n            R\n                [[0. 0. 0.]\n                  [0. 0. 0.]]\n            \"\"\"\n        ).strip()\n\n        assert formatting.diff_array_repr(a, b, \"equals\") == expected\n", "    def test_diff_array_repr_with_indexes(self):\n        a = xr.DataArray(\n            [1, 2, 3],\n            dims=[\"x\"],\n            coords={\"x\": xr.IndexVariable(\"x\", [0, 1, 2])},\n        )\n        b = xr.DataArray(\n            [1, 2, 4],\n            dims=[\"x\"],\n            coords={\"x\": xr.IndexVariable(\"x\", [0, 1, 2])},\n        )\n        expected = dedent(\n            \"\"\"\n            Left and right DataArray objects are not equal\n            Differing values:\n            L    [1 2 3]\n            R    [1 2 4]\n        \"\"\"\n        )\n        assert formatting.diff_array_repr(a, b, \"equals\") == expected\n", "    def test_diff_array_repr(self, compat_func):\n        a = xr.DataArray(np.arange(10), dims=\"x\")\n        b = xr.DataArray(np.arange(10) + 1, dims=\"x\")\n        expected = dedent(\n            f\"\"\"\n            Left and right DataArray objects are not {compat_func.__name__}\n            {a.dims} != {b.dims}\n            Differing values:\n            L\n            <xarray.DataArray (x: 10)>\n            array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n            R\n            <xarray.DataArray (x: 10)>\n            array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n        \"\"\"\n        )\n        assert formatting.diff_array_repr(a, b, compat_func) == expected\n", "    def test_diff_array_repr(self, compat):\n        a = xr.DataArray(np.arange(10), dims=\"x\")\n        b = xr.DataArray(np.arange(10) + 1, dims=\"x\")\n        expected = dedent(\n            f\"\"\"\n            Left and right DataArray objects are not {compat}\n            Differing values:\n                L\n                [ 0  1  2  3  4  5  6  7  8  9]\n                R\n                [ 1  2  3  4  5  6  7  8  9 10]\n        \"\"\"\n        )\n        assert formatting.diff_array_repr(a, b, compat) == expected\n"], "sample_490": ["    def test_unique_constraint_deferrable(self):\n        with atomic():\n            UniqueConstraintDeferrable.objects.create(name=\"foo\", value=1)\n            UniqueConstraintDeferrable.objects.create(name=\"bar\", value=2)\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintDeferrable.objects.create(name=\"foo\", value=1)\n", "    def test_unique_constraint_nulls_distinct(self):\n        with atomic():\n            UniqueConstraintProduct.objects.create(\n                name=\"Product 1\", price=100, description=\"Description 1\"\n            )\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=None, price=100, description=\"Description 1\"\n                )\n            UniqueConstraintProduct.objects.create(\n                name=None, price=200, description=\"Description 2\", nulls_distinct=True\n            )\n", "    def test_unique_constraint_null_field_in_include(self):\n        with atomic(using=self.database):\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintInclude.objects.create(\n                    name=\"test\", description=\"test\", included_field_null=None\n                )\n", "    def test_constraint_with_include(self):\n        with self.assertRaisesMessage(ValidationError, \"Constraint \u2018unique_with_include\u2019 is violated.\"):\n            UniqueConstraintInclude.objects.create(\n                name=\"test\", value=\"test\", included_field=\"test\"\n            )\n\n", "    def test_unique_constraint_with_expressions(self):\n        with atomic():\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", RemovedInDjango60Warning)\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 1\", price=10, description=\"Description 1\"\n                )\n\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Product 1\", price=20, description=\"Description 2\"\n                )\n\n            # Test that creating a product with the same name but different price\n            # does not raise an error.\n            UniqueConstraintProduct.objects.create(\n                name=\"Product 1\", price=10, description=\"Description 3\"\n            )\n"], "sample_65": ["    def test_set_language_with_invalid_language_code(self):\n        \"\"\"\n        Test that set_language returns a 204 response when given an invalid\n        language code.\n        \"\"\"\n        inactive_language_code = self._get_inactive_language_code()\n        response = self.client.post(reverse('set_language'), {\n            'next': '/',\n            LANGUAGE_QUERY_PARAMETER: inactive_language_code,\n        })\n        self.assertEqual(response.status_code, 204)\n\n", "    def test_set_language_with_invalid_language_code(self):\n        rf = RequestFactory()\n        post_data = {\n            'next': '/',\n            LANGUAGE_QUERY_PARAMETER: self._get_inactive_language_code(),\n        }\n        request = rf.post(reverse('set_language'), data=post_data)\n\n        response = set_language(request)\n\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/')\n        self.assertFalse(LANGUAGE_SESSION_KEY in request.session)\n", "    def test_set_language_next(self):\n        factory = RequestFactory()\n        request = factory.post(reverse('set_language'), {'next': '/test/', 'language': 'fr'})\n        response = self.client.post(reverse('set_language'), {'next': '/test/', 'language': 'fr'})\n\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], 'http://localhost/test/')\n\n        request = factory.post(reverse('set_language'), {'language': 'fr'})\n        response = self.client.post(reverse('set_language'), {'language': 'fr'})\n\n        self.assertEqual(response.status_code, 204)\n\n\n", "    def test_set_language_with_ajax_post_request(self):\n        rf = RequestFactory()\n        response = self.client.post(reverse('set_language'), {'next': '/', LANGUAGE_QUERY_PARAMETER: 'fr'})\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(response['Location'], '/')\n        self.assertEqual(self.client.session.get(LANGUAGE_SESSION_KEY), 'fr')\n        self.assertEqual(get_language(), 'fr')\n\n", "    def test_set_language_session_deprecation(self):\n        \"\"\"Test deprecation warning for session usage.\"\"\"\n        with ignore_warnings(category=RemovedInDjango40Warning) as warnings:\n            request = self.client.get(reverse('set_language'), {'next': '/'})\n            self.assertEqual(len(warnings), 1)\n        # Assert that the language cookie is set regardless of deprecation\n        self.assertEqual(request.COOKIES.get(settings.LANGUAGE_COOKIE_NAME, 'en'), 'en')\n\n"], "sample_1100": ["def test_Pow_as_content_primitive():\n    assert (2**x).as_content_primitive() == (1, 2**x)\n    assert (2**(x + 1)).as_content_primitive() == (2, 2**x)\n    assert (2*(x + 1)**2).as_content_primitive() == (2, (x + 1)**2)\n    assert ((2*x + 2)**2).as_content_primitive() == (4, (x + 1)**2)\n    assert (2**(2*x + 1)).as_content_primitive() == (2, 2**(2*x))\n    assert (2**(2*x + 2)).as_content_primitive() == (4, 2**(2*x))\n    assert (3**(2*x + 1)).as_content_primitive() == (3, 3**(2*x))\n    assert (3**(2*x + 2)).as_content_primitive() == (9, 3**(2*x))\n    assert ((2*x + 2)**y).as_content_primitive() == (1, (2*(x + 1))**y)\n", "    def test_as_content_primitive_Pow():\n        assert (x**2).as_content_primitive() == (1, x**2)\n        assert ((-x)**2).as_content_primitive() == (1, x**2)\n        assert ((-2*x)**2).as_content_primitive() == (4, x**2)\n        assert ((2*x)**2).as_content_primitive() == (4, x**2)\n        assert (\n            (2 + 2*x)**2).as_content_primitive() == (4, (x + 1)**2)\n", "def test_Pow_as_content_primitive():\n    assert Pow(4, x).as_content_primitive() == (1, Pow(4, x))\n    assert Pow(4 + 4*sqrt(2), 2).as_content_primitive() == (4, Pow(1 + sqrt(2), 2))\n    assert Pow(3 + 3*sqrt(2), 2).as_content_primitive() == (1, Pow(3, 2) * sqrt(1 + sqrt(2)))\n    assert Pow(2*(x + 1), y).as_content_primitive() == (1, Pow(2*(x + 1), y))\n    assert Pow(3*(x + 1), y).as_content_primitive() == (3, Pow(x + 1, y))\n", "    def test_as_content_primitive_rational_exponent():\n        assert (\n            (4**(3/2)).as_content_primitive() ==\n            (8, 4**(1/2))\n        )\n        assert (\n            (4**(3/2 + x)).as_content_primitive() ==\n            (8, 4**(x + 1/2))\n        )\n        assert (\n            (4**((1+y)/2)).as_content_primitive() ==\n            (2, 4**(y/2))\n        )\n\n", "def test_Pow_limit():\n    assert limit(x**2, x, oo) == oo\n    assert limit(x**(-2), x, oo) == 0\n    assert limit(2**x, x, oo) == oo\n    assert limit(2**(-x), x, oo) == 0\n    assert limit((1 + 1/x)**x, x, oo) == e\n    assert limit((1 + 1/x)**(x + 1), x, oo) == e\n    assert limit((1 + 2/x)**x, x, oo) == e**2\n\n    assert limit((1 - x)**(1/x), x, 0) == 1/e\n    assert limit((1 + x)**(1/x), x, 0) == e\n    assert limit((1 + 2*x)**(1/x), x, 0) == e**2\n\n    assert limit(x**x, x, 0) == 1\n"], "sample_737": ["    def test_tfidf_vectorizer_vocabulary_(self):\n        # Test accessing vocabulary_ after fitting\n        vectorizer = TfidfVectorizer(analyzer='word')\n        vectorizer.fit(ALL_FOOD_DOCS)\n        vocabulary = vectorizer.vocabulary_\n        assert isinstance(vocabulary, dict)\n        assert len(vocabulary) > 0\n\n        # Test if vocabulary contains expected terms\n        assert 'pizza' in vocabulary\n        assert 'salad' in vocabulary\n        assert 'copyright' in vocabulary\n", "    def test_tfidf_vectorizer_vocabulary_setter(self):\n        tfidf = TfidfVectorizer(vocabulary=[\"apple\", \"banana\", \"cherry\"])\n        assert_equal(tfidf.vocabulary, {\"apple\": 0, \"banana\": 1, \"cherry\": 2})\n\n        # Check that setting vocabulary after fitting raises error\n        tfidf.fit(ALL_FOOD_DOCS)\n        with pytest.raises(ValueError):\n            tfidf.vocabulary = [\"grape\", \"orange\", \"kiwi\"]\n\n", "    def test_tfidfvectorizer_vocabulary_consistency(self):\n        \"\"\"Test vocabulary consistency between CountVectorizer and TfidfVectorizer\n        when vocabulary is specified.\n        \"\"\"\n        corpus = JUNK_FOOD_DOCS\n\n        vectorizer = CountVectorizer(vocabulary={'pizza': 0, 'beer': 1})\n        vectorizer.fit(corpus)\n        expected_vocabulary = vectorizer.vocabulary_\n\n        tfidf_vectorizer = TfidfVectorizer(vocabulary=expected_vocabulary)\n        tfidf_vectorizer.fit(corpus)\n        assert_equal(tfidf_vectorizer.vocabulary_, expected_vocabulary)\n\n", "    def test_tfidfvectorizer_vocabulary_updates(self):\n        corpus = [\n            \"This is the first document.\",\n            \"This document is the second document.\",\n            \"And this is the third one.\",\n            \"Is this the first document?\",\n        ]\n\n        vectorizer = TfidfVectorizer(analyzer=\"word\", lowercase=False)\n        vectorizer.fit(corpus)\n\n        # Check initial vocabulary\n        expected_vocabulary = {\n            \"This\": 0,\n            \"is\": 1,\n            \"the\": 2,\n            \"first\": 3,\n            \"document.\": 4,\n            \"second\": 5,\n            \"And\": 6,\n            \"third\": 7,\n            \"one.\": 8,\n        }\n        assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n\n        # Update vocabulary with new corpus\n        new_corpus = [\n            \"This is a new document.\",\n            \"It is the fourth document.\",\n        ]\n        vectorizer.fit(new_corpus)\n\n        # Check updated vocabulary\n        expected_vocabulary.update(\n            {\"a\": 9, \"new\": 10, \"It\": 11, \"fourth\": 12}\n        )\n        assert_equal(vectorizer.vocabulary_, expected_vocabulary)\n\n\n\n", "    def test_tfidfvectorizer_pickle(self):\n        tfidf = TfidfVectorizer()\n        tfidf.fit(ALL_FOOD_DOCS)\n        # pickle and unpickle\n        pickle_pickled = pickle.dumps(tfidf)\n        tfidf_unpickled = pickle.loads(pickle_pickled)\n        assert_true(isinstance(tfidf_unpickled, TfidfVectorizer))\n        assert_array_equal(tfidf.vocabulary_, tfidf_unpickled.vocabulary_)\n\n"], "sample_961": ["def test_module_index(make_app):\n    app = make_app(\n        dict(\n            extensions=['sphinx.domains.python'],\n            python_use_unqualified_type_names=False,\n        )\n    )\n    app.build()\n\n    # Check for the existence of the module index\n    assert \"modindex\" in app.builder.env.indices\n    # Check if it's a PythonModuleIndex\n    assert isinstance(app.builder.env.indices[\"modindex\"], PythonModuleIndex)\n\n\n\n", "    def test_parse_annotation(app):\n        # Check if _parse_annotation handles basic annotations correctly\n        result = _parse_annotation('int', app.env)\n        assert result == [('literal', 'int')]\n\n        result = _parse_annotation('List[str]', app.env)\n        assert result == [('identifier', 'List'), ('punctuation', '['),\n                         ('literal', 'str'), ('punctuation', ']')]\n\n        result = _parse_annotation('Dict[str, int]', app.env)\n        assert result == [('identifier', 'Dict'), ('punctuation', '['),\n                         ('literal', 'str'), ('punctuation', ','), ('literal', 'int'),\n                         ('punctuation', ']')]\n", "def test_parse_annotation_nested():\n    assert_node(\n        _parse_annotation(\"List[Tuple[str, int]]\"),\n        desc_annotation(\n            None,\n            [\n                desc_name(None, 'List'),\n                desc_sig_punctuation(None, '['),\n                desc_content(\n                    None,\n                    [\n                        desc_annotation(\n                            None,\n                            [\n                                desc_name(None, 'Tuple'),\n                                desc_sig_punctuation(None, '['),\n                                desc_content(None, [desc_name(None, 'str'), desc_sig_punctuation(None, ','), desc_name(None, 'int')]),\n                                desc_sig_punctuation(None, ']'),\n                            ]\n                        ),\n                    ]\n                ),\n                desc_sig_punctuation(None, ']'),\n            ]\n        )\n    )\n\n", "def test_parse_annotation():\n    assert _parse_annotation('foo') == ('foo', None)\n    assert _parse_annotation(': int') == (None, 'int')\n    assert _parse_annotation(':List[str]') == (None, 'List[str]')\n    assert _parse_annotation('foo: int') == ('foo', 'int')\n    assert _parse_annotation('foo: List[str]') == ('foo', 'List[str]')\n", "def test_parse_annotation():\n    assert _parse_annotation('int: x') == ('int', 'x')\n    assert _parse_annotation('list[str]') == ('list', 'str')\n    assert _parse_annotation('dict[str, int]') == ('dict', 'str, int')\n    assert _parse_annotation('Tuple[int, ...]') == ('Tuple', 'int, ...')\n    assert _parse_annotation('Callable[[int], str]') == ('Callable', '[[int], str]')\n    assert _parse_annotation('Union[int, str]') == ('Union', 'int, str')\n"], "sample_688": ["    def test_bestrelpath_absolute(self, tmpdir_factory, testdir):\n        p1 = tmpdir_factory.mktemp(\"p1\").join(\"a/b/c\")\n        p2 = tmpdir_factory.mktemp(\"p2\").join(\"a/b/d\")\n        assert testdir.make_absolute_path(p1) == testdir.make_absolute_path(p1)\n\n        relpath = bestrelpath(p1, p2)\n        assert relpath == os.path.join(\"..\", \"d\")\n\n        p3 = tmpdir_factory.mktemp(\"p3\").join(\"x/y/z\")\n        with pytest.raises(ValueError):\n            bestrelpath(p1, p3)\n", "    def test_bestrelpath_mixed_cases(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import os\n            from _pytest.pathlib import bestrelpath\n\n                p1 = Path('a/b/c')\n                p2 = Path('D:/a/b/c')\n                assert bestrelpath(p1, p2) == str(p2)\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n", "    def test_bestrelpath_different_drives(self, testdir: Testdir) -> None:\n        testdir.makepyfile(\"a.py\")\n        testdir.chdir()\n        drive = os.path.splitdrive(os.getcwd())[0]\n        other_drive = \"C:\" if drive.startswith(\"D:\") else \"D:\"\n        other_path = os.path.join(other_drive, \"path/to/a.py\")\n        assert bestrelpath(testdir.path, Path(other_path)) == str(\n            Path(other_path)\n        )\n\n", "    def test_bestrelpath_mix(self, testdir):\n        p = Path(testdir.tmpdir)\n        outside = Path(os.path.join(str(p), \"outside\"))\n        outside.mkdir()\n        path = Path(\n            str(testdir.tmpdir)\n            + os.sep\n            + \"something\"\n            + os.sep\n            + \"deeply\"\n            + os.sep\n            + \"nested\"\n        )\n        path.mkdir(parents=True)\n        result = bestrelpath(p, path)\n        assert result == \"something\" + os.sep + \"deeply\" + os.sep + \"nested\"\n", "def test_bestrelpath(testdir: Testdir) -> None:\n    testdir.makefile(\".txt\", content=\"something\")\n    p = testdir.tmpdir / \"something.txt\"\n    result = bestrelpath(testdir.tmpdir, p)\n    assert result == \"something.txt\"\n\n"], "sample_405": ["    def test_add_foreign_key_constraint_on_existing_field(self):\n        # Create a model with an existing field\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE migrations_test_model_1 (id SERIAL PRIMARY KEY, name VARCHAR(100));\"\n            )\n        with atomic():\n            class TestModel(models.Model):\n                name = models.CharField(max_length=100)\n\n        # Add a foreign key constraint to an existing field\n        class Migration(migrations.Migration):\n            dependencies = [\n                ('migrations_test', '0001_initial'),\n            ]\n\n            operations = [\n                migrations.AlterModelTable(\n                    name='testmodel',\n                    table='migrations_test_model_1',\n                ),\n                migrations.AddField(\n                    model_name='testmodel',\n                    name='related_model',\n                    field=models.ForeignKey(\n                        on_delete=models.CASCADE,\n                        to='related_model',\n                    ),\n                    preserve_default=False,\n                )\n            ]\n\n        # Apply the migration\n        migrator = migrations.Migration migrator.from_module(Migration)\n        migrator.run(ProjectState.from_apps(self.apps), self.schema_editor)\n\n        # Verify the foreign key constraint is created\n\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"SELECT * FROM migrations_test_model_1 WHERE name = 'Test Value';\"\n            )\n\n\n        cursor.fetchall()\n\n", "    def test_rename_model_ordering(self):\n        with atomic(savepoint=False), self.swap_models(\n            from_model=Food, to_model=UnicodeModel\n        ):\n            self.assertEqual(\n                self.project_state.models[\"food\"].options[\"ordering\"],\n                (\"name\",),\n            )\n\n            operation = RenameModel(\"Food\", \"RenamedFood\")\n            migration = Migration(operation)\n\n            # State forwards\n            self.assertEqual(\n                migration.apply(self.project_state).models[\"renamedfood\"].options[\n                    \"ordering\"\n                ],\n                (\"name\",),\n            )\n\n            # Database forwards\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"SELECT name FROM sqlite_master WHERE type='table' AND name='renamedfood'\"\n                )\n                self.assertTrue(cursor.fetchone())\n\n            # Database backwards\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"SELECT name FROM sqlite_master WHERE type='table' AND name='food'\"\n                )\n                self.assertTrue(cursor.fetchone())\n\n            # State backwards\n            self.assertEqual(\n                migration.unapply(self.project_state).models[\"food\"].options[\n                    \"ordering\"\n                ],\n                (\"name\",),\n            )\n\n", "    def test_add_field_with_default(self):\n        initial_state = ProjectState.from_apps(self.apps)\n        operation = migrations.AddField(\n            model_name='UnicodeModel',\n            name='some_field',\n            field=models.IntegerField(default=5),\n        )\n        migration = Migration('test', 'test', operations=[operation])\n\n        # First, check what the state would look like\n        final_state = migration.apply(initial_state, self.schema_editor)\n        self.assertEqual(final_state.models['test_app', 'unicodemodel'].fields, [\n            models.CharField(model='test_app.unicodemodel', name='name', primary_key=True, max_length=255, unique=True),\n            models.IntegerField(model='test_app.unicodemodel', name='some_field', default=5),\n        ])\n\n        # Then apply it to a real database\n        with self.assertRaises(FieldDoesNotExist):\n            UnicodeModel.objects.get().some_field\n\n        with atomic(using=self.connection.alias):\n            with transaction.commit_on_success(using=self.connection.alias):\n                self.create_test_table()\n                operation.database_forwards(self.connection, self.schema_editor)\n        self.assertTrue(UnicodeModel._meta.get_field('some_field'))\n        self.assertEqual(UnicodeModel.objects.create(name='test').some_field, 5)\n\n        # Now roll it back\n        with atomic(using=self.connection.alias):\n            with transaction.commit_on_success(using=self.connection.alias):\n                operation.database_backwards(self.connection, self.schema_editor)\n\n        with self.assertRaises(FieldDoesNotExist):\n            UnicodeModel.objects.get().some_field\n", "    def test_rename_index(self):\n        with self.connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE 'food_food' (\"\n                \"id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, \"\n                '\"name' TEXT NOT NULL UNIQUE, '\n                '\"calories' INTEGER NOT NULL, '\n                '\"added' DATETIME, INDEX 'food_name_idx' ('name')\"\n                \")\"\n            )\n\n        state = ProjectState.from_apps(self.apps)\n        before_migration = state.clone()\n        model_state = state.models[\"food\"]\n        original_index = model_state.indexes[0]\n        self.assertEqual(original_index.name, \"food_name_idx\")\n        migration = Migration(\n            \"initial\",\n            \"Initial migration for Food\",\n            operations=[\n                RenameIndex(\n                    \"food\", new_name=\"food_name_new_idx\", old_name=\"food_name_idx\"\n                )\n            ],\n        )\n        self.assertEqual(len(migration.operations), 1)\n        migration.apply(before_migration)\n        after_migration = before_migration.clone()\n        \n\n        # Check state changes\n        self.assertEqual(len(after_migration.models[\"food\"].indexes), 1)\n        self.assertEqual(\n            after_migration.models[\"food\"].indexes[0].name, \"food_name_new_idx\"\n        )\n\n        # Check database changes\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='index'\")\n            indexes = [row[0] for row in cursor.fetchall()]\n            self.assertIn(\"food_name_new_idx\", indexes)\n            self.assertNotIn(\"food_name_idx\", indexes)\n\n", "    def test_add_deferrable_unique_constraint(self):\n        with connection.cursor() as cursor:\n            cursor.execute(\n                \"CREATE TABLE deferrable_unique_test (id SERIAL PRIMARY KEY, value VARCHAR(100) UNIQUE DEFERRABLE)\"\n            )\n\n        with atomic(using='default'):\n            before_state = self.get_state()\n            after_state = Migration(\n                \"test_add_deferrable_unique_constraint\",\n                operations=[\n                    AddConstraint(\n                        \"deferrable_unique_test\",\n                        models.UniqueConstraint(\n                            fields=[\"value\"],\n                            name=\"deferrable_unique\",\n                            deferrable=True,\n                        ),\n                    ),\n                ],\n            ).apply(self.project_state.apps)\n            self.assertStateEqual(after_state, before_state)\n\n            # Ensure the constraint exists in the database\n            cursor.execute(\"SELECT * FROM pg_constraint WHERE conname = 'deferrable_unique'\")\n            self.assertEqual(cursor.fetchone(), (4, 'deferrable_unique', 'u', 't', 'p', \n                                                'deferrable_unique_test', 'value', \n                                                'u', 't', 'n', 'NULL', 'NULL'))\n\n            with self.assertRaises(IntegrityError):\n                with transaction.atomic():\n                    self.create_model('deferrable_unique_test', {'value': 'test'})\n                    self.create_model('deferrable_unique_test', {'value': 'test'})\n"], "sample_515": ["def test_colorbar_extension_length_lognorm():\n    \"\"\"Test the length of extensions on a log-scaled colorbar.\"\"\"\n    cmap = mpl.colormaps[\"viridis\"].resampled(10)\n    norm = LogNorm(vmin=0.1, vmax=10)\n    fig, axs = plt.subplots(2, 2)\n    for ax in axs.flat:\n        Colorbar(ax, cmap=cmap, norm=norm, extend='both',\n                 extendfrac=[0.05, 0.1], orientation='vertical')\n        ax.tick_params(left=False, labelleft=False,\n                       bottom=False, labelbottom=False)\n    return fig\n", "def test_colorbar_orientation():\n    fig, ax = plt.subplots()\n    im = ax.imshow(np.arange(100).reshape(10, 10), cmap=\"viridis\")\n    cax = fig.add_axes([0.9, 0.1, 0.03, 0.8])\n    cbar = fig.colorbar(im, cax=cax, orientation='horizontal')\n    assert cbar.orientation == 'horizontal'\n    \n    cax = fig.add_axes([0.1, 0.9, 0.8, 0.03])\n    cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n    assert cbar.orientation == 'vertical'\n", "def test_colorbar_tick_formatters():\n    fig, ax = plt.subplots()\n    data = np.random.rand(10, 10)\n    im = ax.imshow(data)\n\n    # Test default formatter\n    cbar = fig.colorbar(im)\n\n    # Test custom formatter\n        return f'{x:.2f}'\n\n    cbar.set_major_formatter(plt.FuncFormatter(my_formatter))\n    cbar.update_ticks()\n\n    # Test LogFormatter with scaled values\n    cbar.set_norm(LogNorm(vmin=1e-3, vmax=1e2))\n    cbar.set_major_formatter(LogFormatter(labelOnlyBase=False))\n    cbar.update_ticks()\n\n    # Test FixedLocator\n    cbar.set_ticks(FixedLocator([0.1, 0.5, 0.9]))\n    cbar.update_ticks()\n\n", "def test_colorbar_extend_frac_auto():\n    cmap = mpl.colormaps[\"RdBu\"].resampled(5)\n    clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n    norm = BoundaryNorm(clevs, len(clevs) - 1)\n    fig, ax = plt.subplots()\n    img = ax.imshow([[1, 2], [3, 4]], cmap=cmap, norm=norm)\n    cb = fig.colorbar(img, ax=ax, extend='both', extendfrac='auto')\n\n    assert cb.ax. Locator.get_bounds()[0] > cb._boundaries[0]\n\n", "    def test_colorbar_extendfrac(self):\n        cmap = mpl.colormaps[\"RdBu\"].resampled(5)\n        clevs = [-5., -2.5, -.5, .5, 1.5, 3.5]\n        norm = BoundaryNorm(clevs, len(clevs) - 1)\n\n        fig, ax = plt.subplots()\n        im = ax.imshow([[1, 2], [3, 4]], cmap=cmap, norm=norm)\n        cbar = fig.colorbar(im, extend='both', extendfrac=0.2)\n        cbar.set_ticks(clevs)\n\n        # Check that the extend fraction is applied correctly\n        assert cbar.ax.get_position().width == 0.02\n\n        # Check that the extend lines are drawn correctly\n        assert len(cbar.ax.lines) == 2\n        for line in cbar.ax.lines:\n            assert line.get_color() == 'k'\n\n\n\n"], "sample_728": ["    def test_make_checkerboard_shuffle(random_state=42):\n        X, rows, cols = make_checkerboard((100, 50), (2, 3), shuffle=True,\n                                        random_state=random_state)\n        assert_equal(X.shape, (100, 50))\n        assert_equal(rows.shape, (6, 100))\n        assert_equal(cols.shape, (6, 50))\n        assert_true(np.all(np.sum(rows, axis=0) == 1))\n        assert_true(np.all(np.sum(cols, axis=0) == 1))\n\n", "    def test_make_biclusters_shuffle(self):\n        X, rows, cols = make_biclusters((100, 50), 5)\n        assert_equal(X.shape, (100, 50))\n        assert_equal(rows.shape, (5, 100))\n        assert_equal(cols.shape, (5, 50))\n        assert_true(np.any(X != 0))\n\n", "def test_make_checkerboard_shuffle():\n    X, rows, cols = make_checkerboard(shape=(10, 12), n_clusters=(2, 3),\n                                     shuffle=True, random_state=42)\n    assert X.shape == (10, 12)\n    assert rows.shape == (6, 10)\n    assert cols.shape == (6, 12)\n    assert_true(np.any(X != 0))\n", "def test_make_biclusters_shape():\n    # Test that the output shape matches the specified shape\n    n_rows, n_cols = 100, 50\n    X, rows, cols = make_biclusters((n_rows, n_cols), n_clusters=5)\n    assert_equal(X.shape, (n_rows, n_cols))\n", "def test_make_checkerboard():\n    # Test basic checkerboard generation\n    X, rows, cols = make_checkerboard((50, 40), 2)\n    assert_equal(X.shape, (50, 40))\n    assert_equal(rows.shape, (2, 50))\n    assert_equal(cols.shape, (2, 40))\n\n    # Test with noise\n    X, rows, cols = make_checkerboard((50, 40), 2, noise=0.1)\n    assert_equal(X.shape, (50, 40))\n    assert_true(np.any(np.abs(X) > 10))\n\n    # Test with different cluster sizes\n    X, rows, cols = make_checkerboard((50, 40), (2, 3))\n    assert_equal(rows.shape, (2, 50))\n    assert_equal(cols.shape, (3, 40))\n\n    # Test with shuffle\n    # Check that rows and cols are shuffled\n    X1, rows1, cols1 = make_checkerboard((50, 40), 2)\n    X2, rows2, cols2 = make_checkerboard((50, 40), 2, shuffle=False)\n    assert_true(np.any(rows1 != rows2))\n    assert_true(np.any(cols1 != cols2))\n\n"], "sample_722": ["    def test_minibatch_kmeans_sparse_input(self):\n        kmeans = MiniBatchKMeans(n_clusters=3, random_state=42)\n        kmeans.fit(X_csr)\n        labels = kmeans.labels_\n        assert_true(np.all(labels >= 0))\n        assert_true(np.all(labels < 3))\n\n\n\n", "    def test_minibatch_kmeans_sparse_input(self):\n        # Test MiniBatchKMeans with sparse input\n        kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42)\n        kmeans.fit(X_csr)\n\n        labels = kmeans.labels_\n        assert_equal(labels.shape[0], n_samples)\n        assert_greater(v_measure_score(true_labels, labels), 0.9)\n", "    def test_partial_fit_mini_batch(self):\n        \"\"\"Test partial_fit on MiniBatchKMeans.\"\"\"\n        # Test that partial_fit works correctly with a batch size equal to\n        # the total data size\n        mbkmeans = MiniBatchKMeans(n_clusters=3, batch_size=n_samples,\n                                   n_init=1, random_state=42)\n        mbkmeans.partial_fit(X)\n        labels = mbkmeans.labels_\n        assert_equal(len(labels), n_samples)\n        assert_true(np.all(np.unique(labels) == np.arange(3)))\n        # Now test with smaller batch size\n        mbkmeans = MiniBatchKMeans(n_clusters=3, batch_size=20,\n                                   n_init=1, random_state=42)\n        for _ in range(5):\n            mbkmeans.partial_fit(X)\n        labels = mbkmeans.labels_\n        assert_equal(len(labels), n_samples)\n        assert_true(np.all(np.unique(labels) == np.arange(3)))\n\n", "def test_mini_batch_kmeans_n_init():\n  \"\"\"Test the n_init parameter of MiniBatchKMeans.\"\"\"\n  kmeans = MiniBatchKMeans(n_clusters=3, n_init=2, random_state=0)\n\n  kmeans.fit(X)\n  assert_equal(len(kmeans.inertia_), 2)\n\n  kmeans = MiniBatchKMeans(n_clusters=3, n_init=1, random_state=0)\n  kmeans.fit(X)\n  assert_equal(len(kmeans.inertia_), 1)\n", "    def test_mini_batch_kmeans_sparse(self):\n        # Test MiniBatchKMeans with sparse input\n        kmeans = MiniBatchKMeans(n_clusters=3, random_state=42).fit(X_csr)\n        assert_equal(kmeans.cluster_centers_.shape, (3, 5))\n"], "sample_699": ["    def test_doctest_module_with_mocked_function(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import mock\n\n            @mock.patch('some_module.some_function')\n                mock_some_function.return_value = 'mocked result'\n                assert some_module.some_function() == 'mocked result'\n\n            \"\"\"\n        )\n        result = pytester.runpytest(p)\n        assert result.ret == 0\n        \n        # Test if the doctest module correctly identifies and runs the doctest\n        # within the mock patch context\n        doctest_item = list(result.nodeid_to_item.values())[0]\n        assert isinstance(doctest_item, DoctestModule)\n", "    def test_doctest_collect_with_mocked_object(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n                pass\n                >>> my_mocked_function()\n            \"\"\"\n        )\n        with _patch_unwrap_mock_aware():\n            result = pytester.runpytest()\n        assert result.ret == 0\n        assert \"1 passed\" in result.stdout.str()\n", "    def test_setup_fixtures_with_mocked_object(\n        self, pytester: Pytester, monkeypatch", "    def test_doctest_setup_teardown(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n                mod.x = 1\n                del mod.x\n\n            class MyClass(object):\n                    self.y = 2\n\n                    del self.y\n\n                    import doctest\n                    doctest.testmod(\n                        globs={'MyClass': MyClass}, verbose=True, optionflags=doctest.ELLIPSIS)\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n", "    def test_doctest_with_mocked_object(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import pytest\n\n                return 42\n\n            @pytest.mark.parametrize(\"input,expected\", [(1, 43), (2, 44)])\n                assert mocked_func() + input == expected\n            \"\"\"\n        )\n        pytester.runpytest(p)\n        assert \"1 passed\" in pytester.stdout.str()\n"], "sample_455": ["    def test_unique_constraint_with_expressions_and_opclasses(self):\n        with self.assertRaises(ValueError):\n            UniqueConstraint(\n                F(\"name\") | F(\"price\"),\n                opclasses=(models.IntegerRangeField.opclasses['range']),\n            )\n", "    def test_unique_constraint_condition_with_expressions(self):\n        with atomic():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", category=RemovedInDjango60Warning\n                )\n                UniqueConstraintConditionProduct.objects.create(name='Product1', price='10.00')\n                with self.assertRaises(IntegrityError):\n                    UniqueConstraintConditionProduct.objects.create(name='Product2', price='10.00')\n", "    def test_unique_constraint_deferrable_deletion(self):\n        with atomic():\n            UniqueConstraintDeferrable.objects.create(name='product_name')\n            with self.assertRaises(IntegrityError):\n                UniqueConstraintDeferrable.objects.create(name='product_name')\n            # Deferrable constraint should not raise an error on deletion\n            UniqueConstraintDeferrable.objects.filter(name='product_name').delete()\n            UniqueConstraintDeferrable.objects.create(name='product_name')\n", "    def test_unique_constraint_with_expressions_postgresql(self):\n        with self.connect_to_postgresql() as connection:\n            with connection.cursor() as cursor:\n                with atomic(using=connection.alias):\n                    product = UniqueConstraintProduct.objects.create(\n                        name=\"Foo\",\n                        sku=Lower(\"name\"),\n                        price=10,\n                    )\n                    with self.assertRaises(IntegrityError):\n                        UniqueConstraintProduct.objects.create(\n                            name=\"foo\",\n                            sku=Lower(\"name\"),\n                            price=20,\n                        )\n", "    def test_unique_constraint_deferrable_creation_and_removal(self):\n        # Create a deferred UniqueConstraint.\n        with atomic():\n            with ignore_warnings(category=RemovedInDjango60Warning):\n                UniqueConstraintProduct.objects.create(\n                    name=\"Deferred Product\", value=1),\n                model = UniqueConstraintDeferrable\n                model.objects.create(\n                    name='Deferred UniqueConstraintDeferrable',\n                    value=1\n                    )\n            constraints = get_constraints(model._meta.db_table)\n            self.assertEqual(len(constraints), 1)\n            deferred_constraint = constraints[0]\n            self.assertEqual(deferred_constraint['name'], 'deferred_unique')\n            self.assertTrue(deferred_constraint['deferrable'])\n\n        # Remove the UniqueConstraint and check if it's gone.\n        with atomic():\n            model._meta.get_field('name').constraints.remove(model._meta.unique_together[0])  # Removing the constraint\n\n            model._meta.concrete_model.objects.create(\n                name='Deferred UniqueConstraintDeferrable', value=2\n            )\n            constraints = get_constraints(model._meta.db_table)\n            self.assertEqual(len(constraints), 0)\n"], "sample_155": ["    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n            f.write(b'some content')\n            filename = f.name\n        with open(filename, 'rb') as f:\n            response = FileResponse(f, as_attachment=True, filename='uploaded_file.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"uploaded_file.txt\"')\n        os.unlink(filename)\n", "    def test_file_response_with_block_size(self):\n        with tempfile.TemporaryFile() as f:\n            f.write(b'This is a test file' * 10)\n            f.seek(0)\n            response = FileResponse(f, block_size=5)\n            content = b''\n            for chunk in response:\n                content += chunk\n        self.assertEqual(content, b'This is a test file' * 10)\n", "    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(mode='w+b', delete=False) as f:\n            f.write(b'This is a test file.')\n            f.seek(0)\n            response = FileResponse(f, filename='test.txt')\n        self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n", "    def test_file_response_with_filename(self):\n        with tempfile.NamedTemporaryFile(mode='w+t', delete=False) as f:\n            f.write('Test content')\n            f.flush()\n            filename = f.name\n\n        response = FileResponse(\n            open(filename, 'rb'), filename=os.path.basename(filename)\n        )\n        self.assertEqual(response['Content-Disposition'], 'inline; filename=\"testfile.txt\"')\n        os.remove(filename)\n", "    def test_streaming_file_response_with_filename_and_as_attachment(self):\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(b\"test content\")\n            f.seek(0)\n            response = FileResponse(f, as_attachment=True, filename='test.txt')\n            self.assertEqual(response['Content-Disposition'], 'attachment; filename=\"test.txt\"')\n"], "sample_452": ["    def test_add_field_with_default_ref_model(self):\n        model_name = 'UnicodeModel'\n        new_field_name = 'referenced_unicode'\n        new_field_type = models.ForeignKey(UnicodeModel, on_delete=models.CASCADE, default='initial_value')\n\n        before_state = self.get_state()\n        operation = AddField(model_name=model_name, name=new_field_name, field=new_field_type)\n        after_state = operation.state_forwards(self.app_label, before_state)\n        \n        # Check if the new field is added to the model\n        self.assertIn(new_field_name, after_state.apps[self.app_label].models[model_name].fields)\n", "    def test_add_unique_together(self):\n        class Model(models.Model):\n            name = models.CharField(max_length=100)\n            food = models.CharField(max_length=100)\n            pass\n\n        with atomic():\n            # Before migration\n            with CaptureQueriesContext(connection) as queries:\n                state = ProjectState.from_apps(self.apps)\n                operation = AddUniqueTogether(\n                    \"appname\", \"Model\", [\"name\", \"food\"]\n                )\n                operation.state_forwards(\n                    \"appname\", state\n                )\n                operation.database_forwards(\n                    \"appname\", connection.schema_editor(), state, state\n                )\n                self.assertEqual(\n                    len(queries.captured_queries), 1\n                )\n                self.assertIn(\n                    'CREATE UNIQUE INDEX', queries.captured_queries[0][\"sql\"].upper()\n                )\n            # After migration\n            state = ProjectState.from_apps(self.apps)\n            model_state = state.models[\"appname\", \"model\"]\n            self.assertIn((\"name\", \"food\"), model_state.unique_together)\n            with self.assertRaises(IntegrityError):\n                Model.objects.create(name=\"spam\", food=\"eggs\")\n                Model.objects.create(name=\"spam\", food=\"eggs\")\n\n\n", "    def test_rename_field_with_related_model(self):\n        # Rename a field in a model that has a OneToOneField pointing to it.\n        with connection.cursor() as cursor:\n            cursor.execute(\"CREATE TABLE related_model (id SERIAL PRIMARY KEY, name VARCHAR(100) NOT NULL)\")\n        self._create_model(\n            \"RelatedModel\", \n            [models.CharField(\"name\", max_length=100)],\n            related_models=[\n                ('related_model', 'OneToOneField', 'related_model_id'),\n            ],\n        )\n\n        with atomic():\n            model_class = self.create_model(\n                'RelatedModel',\n                [\n                    models.CharField(\n                        \"old_name\",\n                        max_length=100,\n                    )\n                ],\n\n            )\n\n            instance = model_class.objects.create(old_name='foo')\n            related_instance = models.RelatedModel.objects.create(related_model_id=instance.pk)\n            # Now, rename the field.\n            self.apply_operation(\n                RenameField('RelatedModel', 'old_name', 'new_name')\n            )\n            # Make sure it's reflected in the DB\n            self.assertColumnExists('RelatedModel', 'new_name')\n            self.assertNotColumnExists('RelatedModel', 'old_name')\n            # Ensure the related model still points to the renamed field\n            self.assertEqual(related_instance.related_model.new_name, 'foo')\n\n            # Now, undo the rename.\n\n", "    def test_alter_field_with_related_name(self):\n        # Regression test for #27712: Changing a related_name should not\n        # raise an error when it's a backwards relationship.\n        with self.connection.cursor() as cursor:\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE \"operationstests_parent\" (\n                    \"id\" INTEGER PRIMARY KEY,\n                    \"name\" VARCHAR(255)\n                );\n                \"\"\"\n            )\n            cursor.execute(\n                \"\"\"\n                CREATE TABLE \"operationstests_child\" (\n                    \"id\" INTEGER PRIMARY KEY,\n                    \"parent_id\" INTEGER REFERENCES \"operationstests_parent\"(\"id\"),\n                    \"name\" VARCHAR(255)\n                );\n                \"\"\"\n            )\n\n        class Parent(models.Model):\n            name = models.CharField(max_length=255)\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, on_delete=models.CASCADE, related_name='children')\n            name = models.CharField(max_length=255)\n\n        parent_state = self.make_project_state(\n            apps=[\n                'operationstests',\n            ],\n            models={\n                'operationstests': {\n                    'parent': Parent._meta.get_fields(),\n                    'child': Child._meta.get_fields(),\n                }\n            }\n        )\n        self.assertEqual(Child._meta.get_field('parent').related_name, 'children')\n\n        migration = Migration(\n            'test_migration',\n            [\n                migrations.AlterField(\n                    model_name='child',\n                    name='parent',\n                    field=models.ForeignKey(Parent, on_delete=models.CASCADE, related_name='other_children'),\n                ),\n            ],\n            operation=migrations.CreateModel(\n                name='Parent',\n                fields=[\n                    ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                    ('name', models.CharField(max_length=255)),\n                ])\n\n        )\n\n        ", "    def test_alter_field_foreign_key_to_field_with_null(self):\n        # Create a model and a related model\n        class Author(models.Model):\n            name = models.CharField(max_length=100)\n\n        class Book(models.Model):\n            title = models.CharField(max_length=100)\n            author = models.ForeignKey(Author, on_delete=models.CASCADE)\n\n        # Create an initial migration\n        initial_migration = Migration(\n            'inital',\n            [\n                migrations.CreateModel(\n                    name='Author',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('name', models.CharField(max_length=100)),\n                    ],\n                    options={\n                        'managed': False,\n                    },\n                ),\n                migrations.CreateModel(\n                    name='Book',\n                    fields=[\n                        ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                        ('title', models.CharField(max_length=100)),\n                        ('author', models.ForeignKey(on_delete=models.CASCADE, to='tests.author')),\n                    ],\n                    options={\n                        'managed': False,\n                    },\n                ),\n            ],\n            dependencies=[]\n        )\n        self.apply_migration(initial_migration)\n\n        # Add a new non-null field to Author\n        with atomic():\n            self.apply_migration(\n                Migration(\n                    'change_author',\n                    [\n                        migrations.AddField(\n                            model_name='author',\n                            name='email',\n                            field=models.EmailField(null=False),\n                        ),\n                    ],\n                    dependencies=[\n                        ('tests', 'inital'),\n                    ],\n                )\n            )\n\n        # Create a new migration for altering the foreign key field to point to the new non-null field\n        second_migration = Migration(\n            'alter_book_author',\n            [\n                AlterField(\n                    model_name"], "sample_466": ["    def test_serialize_enum_args(self):\n        class CustomMigration(migrations.Migration):\n            operations = [\n                migrations.AddField(\n                    model_name=\"testmodel1\",\n                    name=\"my_enum\",\n                    field=models.CharField(\n                        choices=[(tag.value, tag.name) for tag in TextEnum],\n                        default=TextEnum.A,\n                        max_length=20,\n                    ),\n                ),\n            ]\n\n        writer = OperationWriter(CustomMigration.operations[0])\n        serialized, _ = writer.serialize()\n\n        self.assertIn(\n            \"field=models.CharField(choices=[('a-value', 'A'), ('value-b', 'B')], default='a-value', max_length=20),\",\n            serialized,\n        )\n\n", "    def test_operation_writer_timezone(self):\n        operation = migrations.RunPython(\n            lambda apps, schema_editor: None,\n            lambda apps, schema_editor: None,\n            elidable=True,\n            reverse_elidable=True,\n        )\n        operation_writer = OperationWriter(operation)\n        serialized, _ = operation_writer.serialize()\n        self.assertEqual(\n            serialized,\n            \"migrations.RunPython(\\n\"\n            \"    lambda apps, schema_editor: None,\\n\"\n            \"    lambda apps, schema_editor: None,\\n\"\n            \"    elidable=True,\\n\"\n            \"    reverse_elidable=True,\\n\"\n            \")\",\n        )\n", "    def test_serialize_enum_field(self):\n        class Model(models.Model):\n            field = models.CharField(\n                choices=[(TextEnum.A.value, TextEnum.A.name)], max_length=10\n            )\n\n        operation = migrations.AlterField(\n            model_name=\"model\", name=\"field\", field=models.CharField(choices=[(TextEnum.A.value, TextEnum.A.name)], max_length=10)\n        )\n        writer = OperationWriter(operation)\n        serialized, _ = writer.serialize()\n        expected = (\n            \"migrations.AlterField(\\n\"\n            \"    model_name='model',\\n\"\n            \"    name='field',\\n\"\n            \"    field=models.CharField(choices=[('a-value', 'A')], max_length=10),\\n\"\n            \"),\"\n        )\n        self.assertEqual(serialized, expected)\n", "    def test_operation_writer_with_enum(self):\n        class MyMigration(migrations.Migration):\n            operations = [\n                migrations.CreateModel(\n                    name=\"MyModel\",\n                    fields=[\n                        (\"enum_field\", models.CharField(choices=[(e.name, e.value) for e in TextEnum], max_length=20)),\n                    ],\n                )\n            ]\n\n        migration_writer = MigrationWriter(MyMigration())\n        migration_string = migration_writer.as_string()\n        self.assertIn('models.CharField(choices=[(\"A\", \"a-value\"), (\"B\", \"value-b\")], max_length=20)', migration_string)\n\n", "    def test_serialize_operation_with_default_value(self):\n        class MyOperation(migrations.Operation):\n                super().__init__()\n                self.name = name\n                self.default = default\n\n                return f'{self.name}({self.default!r})'\n\n                return (\n                    self.__class__.__name__,\n                    [self.name],\n                    {'default': self.default},\n                )\n\n        operation = MyOperation(\"test_field\", default='some_default')\n        writer = OperationWriter(operation)\n        serialized, _ = writer.serialize()\n        self.assertEqual(serialized, 'migrations.MyOperation(\"test_field\", default=\"some_default\"),')\n\n"], "sample_140": ["    def test_sensitive_post_parameters_all(self):\n        request = RequestFactory().post('/', {'password': '123', 'credit_card': '4111-1111-1111-1111'})\n        @sensitive_post_parameters()\n            return request.sensitive_post_parameters\n        response = view(request)\n        self.assertEqual(response, '__ALL__')\n\n", "    def test_sensitive_post_parameters_all(self):\n        request = RequestFactory().post('/', {'password': 'secret', 'credit_card': '1234'})\n        with mock.patch('django.views.decorators.debug.sensitive_post_parameters') as mock_decorator:\n            sensitive_post_parameters()(lambda request: None)(request)\n            mock_decorator.assert_called_once_with(request)\n            self.assertEqual(request.sensitive_post_parameters, '__ALL__')\n", "    def test_sensitive_variables_no_args(self):\n            pass\n        \n        decorated_function = sensitive_variables()(my_function)\n        self.assertEqual(decorated_function.sensitive_variables, '__ALL__')\n", "    def test_sensitive_variables_decorator_no_args(self):\n        @sensitive_variables()\n            pass\n        result = my_function(User(), 'password123')\n        self.assertEqual(my_function.sensitive_variables, '__ALL__')\n", "    def test_sensitive_variables_decorator_with_specific_variables(self):\n        @sensitive_variables('username', 'password')\n            return username, password\n        my_function('testuser', 'testpassword')\n        self.assertEqual(my_function.sensitive_variables, ('username', 'password'))\n\n"], "sample_623": ["def test_to_zarr_append_dim_conflict():\n    ds = xr.Dataset({\"foo\": ([\"x\"], [1, 2])})\n    with pytest.raises(ValueError, match=\"append_dim=x does not match\"):\n        ds.to_zarr(\"test.zarr\", mode=\"a\", append_dim=\"x\")\n\n", "    def test_to_netcdf_invalid_netcdf_h5netcdf_engine(self) -> None:\n        ds = xr.Dataset({\"foo\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf'\"):\n            ds.to_netcdf(\n                \"test.nc\", engine=\"h5netcdf\", invalid_netcdf=False, compute=False\n            )\n\n", "    def test_to_netcdf_invalid_netcdf(self) -> None:\n        ds = xr.Dataset({\"a\": (\"x\", [1, 2])}, coords={\"x\": [0, 1]})\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf' for engine h5netcdf\"):\n            ds.to_netcdf(\"test.nc\", engine=\"h5netcdf\", invalid_netcdf=True)\n", "    def test_to_netcdf_invalid_netcdf(self) -> None:\n        ds = xr.Dataset(\n            {\"foo\": ((\"x\", \"y\"), np.arange(6).reshape(2, 3))},\n            coords={\"x\": [0, 1], \"y\": [2, 3, 4]},\n        )\n        with pytest.raises(ValueError, match=\"unrecognized option 'invalid_netcdf'\"):\n            ds.to_netcdf(\n                \"test.nc\", invalid_netcdf=False, engine=\"h5netcdf\"\n            )\n", "    def test_to_zarr_append_with_existing_variables_and_region(self):\n\n        ds = xr.Dataset({\"a\": (\"x\", np.arange(10)), \"b\": (\"x\", np.arange(10))})\n        \n        path = \"test_append.zarr\"\n\n        # Write the dataset to a zarr store\n        ds.to_zarr(path, mode=\"w\")\n\n        existing_ds = xr.open_zarr(path)\n\n        # Create a new dataset with a smaller region\n        ds2 = xr.Dataset({\"a\": (\"x\", np.arange(5)), \"b\": (\"x\", np.arange(5))})\n\n        # Append the new dataset to the zarr store using the region argument\n\n        ds2.to_zarr(\n            path, mode=\"a\", region={\"x\": slice(0, 5)}\n        )\n\n        loaded_ds = xr.open_zarr(path)\n\n        assert_identical(loaded_ds[\"a\"], np.concatenate((existing_ds[\"a\"].values, ds2[\"a\"].values)))\n        assert_identical(loaded_ds[\"b\"], np.concatenate((existing_ds[\"b\"].values, ds2[\"b\"].values)))\n\n"], "sample_320": ["    def test_alter_field_to_null(self):\n        app_label = 'test_migrations'\n        # Create a model with a field\n        with atomic():\n            class Food(models.Model):\n                name = models.CharField(max_length=100)\n                calories = models.IntegerField()\n        self.assertTableExists('test_migrations_food')\n        # Change the field to nullable\n        operation = AlterField(\n            'test_migrations', 'Food', 'calories', field=models.IntegerField(null=True),\n        )\n        # Apply the operation\n        with connection.schema_editor() as editor:\n            operation.database_forwards(app_label, editor, ProjectState(apps=self.apps), ProjectState(apps=self.apps))\n        # Verify the field is now nullable\n        self.assertEqual(Food._meta.get_field('calories').null, True)\n        # Rollback the operation\n        with connection.schema_editor() as editor:\n            operation.database_backwards(app_label, editor, ProjectState(apps=self.apps), ProjectState(apps=self.apps))\n        # Verify the field is back to non-nullable\n        self.assertEqual(Food._meta.get_field('calories').null, False)\n", "    def test_alter_model_options_check_constraints(self):\n        app_label = 'test_migration_operations'\n        model_name = 'UnicodeModel'\n\n        # Create a model with a check constraint\n        with atomic():\n            with connection.cursor() as cursor:\n                cursor.execute(\n                    \"\"\"\n                    CREATE TABLE %s (\n                        id SERIAL PRIMARY KEY,\n                        name VARCHAR(100) NOT NULL CHECK (name <> '')\n                    );\n                    \"\"\"\n                    % self.table_name(model_name)\n                )\n        class UnicodeModel(models.Model):\n            name = models.CharField(max_length=100)\n\n            class Meta:\n                db_table = self.table_name(model_name)\n                constraints = [\n                    models.CheckConstraint(check=models.Q(name__neq=''), name='name_not_empty')\n                ]\n\n        before_state = self.get_state(app_label, [UnicodeModel])\n        operation = AlterModelOptions(\n            name=model_name,\n            options={\n                'constraints': [\n                    models.CheckConstraint(check=models.Q(name__startswith='A'), name='name_starts_with_a'),\n                ]\n            },\n        )\n        after_state = self.apply_migration(\n            operation, before_state, app_label, operation_type='alter_model_options'\n        )\n\n        self.assertEqual(len(after_state.models[app_label, model_name].constraints), 1)\n        self.assertEqual(after_state.models[app_label, model_name].constraints[0].name, 'name_starts_with_a')\n\n        self.assertColumnExists(after_state, model_name, 'name')\n", "    def test_alter_field_type(self):\n        \"Test altering the type of a field.\"\n        with atomic(using=self.using):\n            with transaction.atomic(using=self.using):\n                # Create a model with an integer field.\n                self.create_model(\n                    models.Model,\n                    name='NumberModel',\n                    fields=[\n                        ('number', models.IntegerField(default=12)),\n                    ],\n                )\n                # Get the initial state\n                app_label = \"test\"\n                model_state = ProjectState.from_apps(self.apps).models[app_label, 'numbermodel']\n\n                # Change the field to a FloatField\n\n                old_field = model_state.fields['number']\n                new_field = models.FloatField(default=12.0)\n                operation = AlterField(\n                    model_name='NumberModel',\n                    name='number',\n                    field=new_field,\n                    old_field=old_field,\n                )\n\n                # Assert the operation\n\n                self.assertEqual(operation.describe(), \"Alter field number on NumberModel to FloatField\")  \n                # Apply the migration\n\n                migration = Migration(\n                    '0010_alter_numbermodel_number', [operation]\n                )\n                migration.apply(self.connection, '')\n\n                # Verify that the field has been changed from IntegerField to FloatField\n\n                updated_model_state = ProjectState.from_apps(self.apps).models[app_label, 'numbermodel']\n                self.assertEqual(updated_model_state.fields['number'].get_internal_type(), 'FloatField')\n\n                # Rollback the migration\n                \n                migration.unapply(self.connection, '')\n\n                # Verify that the field has been changed back to IntegerField\n\n                final_model_state = ProjectState.from_apps(self.apps).models[app_label, 'numbermodel']\n                self.assertEqual(final_model_state.fields['number'].get_internal_type(), 'IntegerField')\n", "    def test_add_field_foreign_key_constraint(self):\n        with self.swap_schema_editor_for_test(self.schema_editor):\n            # Create a simple model\n            self.add_model(\n                'app', 'Food', fields=['name', 'calories'])\n\n            # Add a foreign key field\n            self.add_field('app', 'Food', 'restaurant', models.ForeignKey('app.Restaurant',\n                                                                         on_delete=models.CASCADE))\n\n            # Ensure the constraint is created in the database\n            with connection.cursor() as cursor:\n                cursor.execute(\"SELECT CONSTRAINT_NAME FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_NAME = %s AND CONSTRAINT_TYPE = %s\",\n                               [\"app_food\", 'FOREIGN KEY'])\n                result = cursor.fetchall()\n                self.assertEqual(len(result), 1)\n\n            # Test if the constraint is enforced\n            with transaction.atomic():\n                try:\n                    self.create_object('app', 'Food', restaurant_id=10)\n                    self.fail(\"Should raise IntegrityError\")\n                except IntegrityError:\n                    pass\n\n", "    def test_add_index_expression_with_nulls(self):\n        with self.connection.cursor() as cursor:\n            cursor.execute(\"DELETE FROM test_unicodemodel WHERE id = 1\")\n        self.assertEqual(UnicodeModel.objects.count(), 0)\n\n        with atomic(using=self.db_name), self.assertRaises(IntegrityError):\n            UnicodeModel.objects.create(name='test')\n\n        # Verify that the initial index creation fails\n\n        migration = Migration(\n            app=\"tests\",\n            name='Add index on expression for unicode model',\n            operations=[\n                migrations.AddIndex(\n                    model_name='unicodemodel',\n                    index=models.Index(fields=['name'], name='test_unicodemodel_name_idx'),\n                ),\n            ],\n            replaces=[(\n                \"tests\",\n                \"0001_initial\",\n            )],\n        )\n        # Create a migration that\n        state = self.get_state()\n        project_state = ProjectState.from_apps(apps=['tests'])\n        with atomic(using=self.db_name):\n            # Apply the migration\n            self.apply_migration(migration, project_state)\n\n        # Verify that the index is created successfully\n        self.assertQuerySetEqual(models.Index.objects.filter(\n            table_name='tests_unicodemodel',\n            name='test_unicodemodel_name_idx'\n        ),\n                                 [\n                                     models.Index(\n                                         table_name='tests_unicodemodel',\n                                         name='test_unicodemodel_name_idx',\n                                     )\n                                 ])\n"], "sample_693": ["    def test_inject_setup_teardown_fixtures(self, testdir: Pytester) -> None:\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                    self.setup_called = True\n\n                    assert self.setup_called\n\n            \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(passed=1)\n", "def test_unittest_skip(testdir, monkeypatch):\n    testdir.makepyfile(\n        \"\"\"\n        import unittest\n\n        class TestClass(unittest.TestCase):\n            @unittest.skip(\"skip this\")\n                pass\n        \"\"\"\n    )\n    monkeypatch.setattr(sys, \"argv\", [\"py.test\", \"-v\"])\n    result = testdir.runpytest()\n    result.assert_outcomes(skipped=1)\n    assert \"skip this\" in result.stdout.str()\n", "    def test_pytest_pycollect_makeitem_finds_unittest_testcase(self, pytester: Pytester) -> None:\n        p = pytester.makepyfile(\n            \"\"\"\n            import unittest\n\n            class MyTests(unittest.TestCase):\n                    pass\n        \"\"\"\n        )\n        items = pytester.collect(p)\n        assert len(items) == 1\n        item = items[0]\n        assert isinstance(item, UnitTestCase)\n        assert item.name == \"MyTests\"\n        assert item.obj.__class__.__name__ == \"MyTests\"\n", "    def test_pytest_runtest_makereport_handles_unittest_skip(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                @unittest.skip(\"skip reason\")\n                    pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == ExitCode.SKIPPED\n        assert result.stdout.str().lines[-1] == \"SKIP [1] reason: skip reason\"\n", "    def test_unittest_skip_with_reason(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import unittest\n\n            class TestClass(unittest.TestCase):\n                @unittest.skip(\"this is a skip reason\")\n                    pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.assert_outcomes(skipped=1)\n        assert \"this is a skip reason\" in result.stdout.str()\n"], "sample_838": ["    def test_column_transformer_mixed_sparse_dense(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n        X_sparse = sparse.csr_matrix(X)\n\n        ct = ColumnTransformer(\n            transformers=[\n                ('num', 'passthrough', 0),\n                ('sparse', StandardScaler(), 1),\n            ]\n        )\n\n        Xt = ct.fit_transform(X_sparse)\n        assert_allclose_dense_sparse(Xt[:, 0], X[:, 0])\n        assert_allclose_dense_sparse(Xt[:, 1], StandardScaler().fit_transform(X[:, 1].reshape(-1, 1)))\n\n        Xt = ct.fit_transform(X)\n        assert_allclose_dense_sparse(Xt[:, 0], X[:, 0])\n        assert_allclose_dense_sparse(Xt[:, 1], StandardScaler().fit_transform(X[:, 1].reshape(-1, 1)))\n\n", "    def test_transformer_weights(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n\n        transformer = ColumnTransformer(\n            transformers=[\n                ('std', StandardScaler(), [0]),\n                ('norm', Normalizer(), [1]),\n            ],\n            transformer_weights={'std': 2, 'norm': 1},\n        )\n        y = np.array([0, 1, 2])\n        transformer.fit(X, y)\n        Xt = transformer.transform(X)\n        # Check that the transformed data reflects the weights\n        assert_allclose(Xt[:, 0] * 2, Xt[:, 1])\n", "def test_column_transformer_sparse_output():\n    X = np.array([[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]])\n    \n    ct = ColumnTransformer(\n        transformers=[\n            ('num', StandardScaler(), [0, 1]),\n            ('cat', OneHotEncoder(handle_unknown='ignore'), [2])\n        ],\n        remainder='drop',\n        sparse_threshold=0.5\n    )\n\n    Xt = ct.fit_transform(X)\n    assert sparse.issparse(Xt)\n", "    def test_remainder_sparse_output(self):\n        X = sparse.csr_matrix([[1, 2], [3, 4]])\n        ct = ColumnTransformer([('scaler', StandardScaler(), [0])],\n                               remainder='passthrough',\n                               sparse_threshold=0.5)\n        X_t = ct.fit_transform(X)\n        assert isinstance(X_t, sparse.csr_matrix)\n\n", "    def test_drop_passthrough_callable(self):\n        X = np.array([[1, 2], [3, 4], [5, 6]])\n\n        ct = ColumnTransformer(\n            transformers=[\n                ('drop', 'drop', 0),\n                ('passthrough', 'passthrough', 1),\n                ('trans', Trans(), [lambda x: x[:, 1]])\n            ])\n\n        Xt = ct.fit_transform(X)\n\n        assert_array_equal(Xt, np.array([[2], [4], [6]]))\n"], "sample_797": ["def test_power_transform_inverse():\n\n    # Check inverse power transformation\n\n    # Box-Cox\n    X = np.array([1, 2, 3, 4, 5])\n    pt = PowerTransformer(method='box-cox')\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n\n    # Yeo-Johnson\n    X = np.array([-1, 0, 1, 2, 3])\n    pt = PowerTransformer(method='yeo-johnson')\n    X_trans = pt.fit_transform(X)\n    X_inv = pt.inverse_transform(X_trans)\n    assert_array_almost_equal(X, X_inv)\n\n", "    def test_power_transform_yeo_johnson_sparse_input(self):\n        X = sparse_random(100, 10, density=0.5, random_state=42)\n        X = X.tocsr()\n\n        for standardize in [True, False]:\n            pt = PowerTransformer(method='yeo-johnson', standardize=standardize,\n                                 copy=True)\n            X_trans = pt.fit_transform(X)\n            assert_allclose(X_trans.toarray() , pt.inverse_transform(X_trans))\n            assert isinstance(X_trans, sparse.csr_matrix)\n", "    def test_power_transform_yeo_johnson_sparse():\n        # Test the PowerTransformer with the yeo-johnson method\n        # on sparse data\n        rng = np.random.RandomState(0)\n        # Generate sparse data with both positive and negative values\n        n_samples = 100\n        n_features = 5\n        X = sparse_random(n_samples, n_features, density=0.5,\n                          random_state=rng).tocsr()\n        X_dense = X.toarray()\n        X_dense[:, 2] *= -1  # make some values negative\n        X = sparse.csr_matrix(X_dense)\n\n        # Transform the data\n        pt = PowerTransformer(method='yeo-johnson')\n        X_trans = pt.fit_transform(X)\n\n        # Check that the transformed data has the expected shape\n        assert_equal(X_trans.shape, X.shape)\n\n        # Check that the transformed data is dense\n        assert_equal(type(X_trans), np.ndarray)\n", "    def test_quantile_transform_ignores_nan(self):\n        \"\"\"Check that NaN values are ignored during quantile transformation.\"\"\"\n        # Create an array with NaNs\n        X = np.array([[1, 2], [3, np.nan], [np.nan, 5]])\n        # Apply quantile transformation\n        Xt = quantile_transform(X)\n        # Check that the transformed array has the same shape\n        assert_equal(Xt.shape, X.shape)\n        # Check that the NaN values are maintained\n        assert np.isnan(Xt[1, 1])\n        assert np.isnan(Xt[2, 0])\n", "    def test_power_transform_y_eo_johnson_zero_neg_samples(self):\n        # Check if Yeo-Johnson transform correctly handles samples with zero values\n        X = np.array([[-1, 0, 1],\n                      [0, -2, 3],\n                      [1, 0, 0]])\n        pt = PowerTransformer(method='yeo-johnson')\n        X_trans = pt.fit_transform(X)\n        assert_no_warnings(pt.fit_transform, X)\n        assert_array_equal(X_trans.shape, X.shape)\n\n"], "sample_806": ["    def test_classification_duplicate_features(self):\n        # Test that the algorithm does not break when there are duplicate features.\n        X = np.array([[1, 2, 1], [3, 4, 3], [5, 6, 5]]).reshape(-1, 3)\n        y = np.array([0, 1, 0])\n        clf = GradientBoostingClassifier(random_state=0)\n        clf.fit(X, y)\n        predictions = clf.predict(X)\n        assert_array_equal(predictions, np.array([0, 1, 0]))\n\n", "    def test_classification_sparse(self):\n        # Check if sparse matrices work as input\n        X_sparse = csr_matrix(iris.data)\n        y = iris.target\n\n        clf = GradientBoostingClassifier(n_estimators=10, random_state=0)\n        clf.fit(X_sparse, y)\n        score_sparse = clf.score(X_sparse, y)\n\n        clf.fit(iris.data, y)\n        score_dense = clf.score(iris.data, y)\n\n        assert_almost_equal(score_sparse, score_dense)\n\n", "    def test_classification_sparse_input(self):\n        \"\"\"Check classification on a dataset with sparse input.\"\"\"\n        X, y = make_classification(n_samples=100, n_features=10,\n                                   random_state=0, sparse=True)\n        clf = GradientBoostingClassifier(loss='deviance', n_estimators=10,\n                                        random_state=1)\n        clf.fit(X, y)\n        score = clf.score(X, y)\n        assert score > 0.8\n", "    def test_classification_sparse_input(self):\n        # Check if sparse input is handled correctly for classification\n        X_sparse = csr_matrix(iris.data)\n        y = iris.target\n\n        clf = GradientBoostingClassifier(n_estimators=10, random_state=0)\n        clf.fit(X_sparse, y)\n        score = clf.score(X_sparse, y)\n        assert score > 0.9\n\n\n", "    def test_gradient_boosting_classifier_random_state(self):\n        # Test random_state with GradientBoostingClassifier\n        clf = GradientBoostingClassifier(n_estimators=10, random_state=42)\n        clf.fit(X, y)\n        predictions = clf.predict(X)\n        clf2 = GradientBoostingClassifier(n_estimators=10, random_state=42)\n        clf2.fit(X, y)\n        predictions2 = clf2.predict(X)\n        assert_array_equal(predictions, predictions2)\n"], "sample_503": ["    def test_drawstyle(self, drawstyle):\n        fig, ax = plt.subplots()\n        x = np.arange(10)\n        y = np.sin(x)\n        line = ax.plot(x, y, drawstyle=drawstyle)[0]\n        assert line.get_drawstyle() == drawstyle\n\n", "    def test_lines_drawstyle(self, style):\n        fig, ax = plt.subplots()\n        x = np.arange(5)\n        y = np.arange(5)\n        line = ax.plot(x, y, drawstyle=style)[0]\n        fig.canvas.draw()  \n        assert line.get_drawstyle() == style \n", "    def test_marker_style_setter_getter(self):\n        style = MarkerStyle(marker='o', fillstyle='none')\n        l = mlines.Line2D([0, 1], [0, 1], marker=style)\n        assert l.get_marker() == 'o'\n        assert l.get_markerfacecolor() == 'none'\n        assert l.get_markeredgecolor() == 'k'\n", "    def test_path_effects_linestyle(self, linestyle, expected):\n        fig, ax = plt.subplots()\n        line = mlines.Line2D(\n            [0, 1], [0, 1], linestyle=linestyle,\n        )\n        ax.add_line(line)\n        fig.canvas.draw()\n        path = line.get_path()\n        assert path.codes == expected\n\n", "    def test_drawstyle(self, drawstyle):\n        fig, ax = plt.subplots()\n        x = np.arange(10)\n        y = np.arange(10)\n        line = ax.plot(x, y, drawstyle=drawstyle)[0]\n        data, = line.get_data()\n        assert_array_equal(data[0], x)\n        assert_array_equal(data[1], y)\n        if drawstyle == 'default' or drawstyle == '-':\n            assert len(line.get_xydata()) == len(x)\n        else:\n            assert len(line.get_xydata()) == len(x) + len(x) - 1\n"], "sample_224": ["    def test_values_list_with_annotation(self):\n        # Test for ticket #17446\n        result = Book.objects.annotate(\n            price_in_cents=F('price') * 100\n        ).values_list('price', 'price_in_cents')\n        self.assertEqual(\n            list(result),\n            [\n                (Decimal('30.00'), 3000),\n                (Decimal('23.09'), 2309),\n                (Decimal('29.69'), 2969),\n                (Decimal('29.69'), 2969),\n                (Decimal('82.80'), 8280),\n                (Decimal('75.00'), 7500),\n            ]\n        )\n", "    def test_values_list_with_annotation(self):\n        qs = Book.objects.values('name').annotate(\n            avg_rating=Avg('rating'),\n            max_price=Max('price')\n        ).order_by('-avg_rating')\n        self.assertEqual(\n            list(qs),\n            [\n                {'name': 'Paradigms of Artificial Intelligence Programming: Case Studies in Common Lisp', 'avg_rating': 5.0, 'max_price': Decimal('75.00')},\n                {'name': 'Practical Django Projects', 'avg_rating': 4.0, 'max_price': Decimal('29.69')},\n                {'name': 'Python Web Development with Django', 'avg_rating': 4.0, 'max_price': Decimal('29.69')},\n                {'name': 'Artificial Intelligence: A Modern Approach', 'avg_rating': 4.0, 'max_price': Decimal('82.80')},\n                {'name': 'The Definitive Guide to Django: Web Development Done Right', 'avg_rating': 4.5, 'max_price': Decimal('30.00')},\n                {'name': 'Sams Teach Yourself Django in 24 Hours', 'avg_rating': 3.0, 'max_price': Decimal('23.09')},\n            ]\n        )\n\n", "    def test_values_list_with_aggregate(self):\n        books = Book.objects.values_list('price', flat=True).annotate(avg_price=Avg('price'))\n        self.assertEqual(len(books), 6)\n        for book, avg_price in zip(self.b1, self.b2, self.b3, self.b4, self.b5, self.b6, books):\n            self.assertEqual(avg_price, book.price)\n\n\n\n", "    def test_subquery(self):\n        with CaptureQueriesContext(connection) as ctx:\n            qs = Book.objects.annotate(\n                avg_rating=Subquery(\n                    Book.objects.filter(publisher=OuterRef('publisher'),\n                                          rating__isnull=False).values('rating').annotate(avg=Avg('rating'))\n                    .values('avg')[:1]\n                )\n            ).filter(avg_rating__gt=4)\n        self.assertEqual(len(ctx.captured_queries), 2)\n        self.assertQuerysetEqual(qs, [self.b1, self.b3, self.b4])\n", "    def test_exists_annotation(self):\n        # Test using Exists to filter authors who have written books\n        authors_with_books = Author.objects.annotate(\n            has_books=Exists(Book.objects.filter(contact=OuterRef('id')))\n        ).filter(has_books=True)\n        self.assertEqual(authors_with_books.count(), 8)\n\n"], "sample_601": ["    def test_strftime(self):\n        expected = self.times.strftime(\"%Y-%m-%d\")\n        actual = self.data.dt.strftime(\"%Y-%m-%d\").data\n        assert_array_equal(actual, expected.values)\n\n", "    def test_strftime(self):\n        expected = self.times.strftime('%Y-%m-%d')\n        actual = self.times_data.dt.strftime('%Y-%m-%d')\n        assert_array_equal(expected, actual)\n\n", "    def test_strftime(self):\n        expected = self.times_data.dt.strftime('%Y-%m-%d %H:%M:%S')\n        assert_identical(self.times_data.dt.strftime('%Y-%m-%d %H:%M:%S'), expected)\n", "    def test_strftime(self):\n        expected = self.times.strftime(\"%Y-%m-%d %H:%M:%S\")\n        actual = self.data.dt.strftime(\"%Y-%m-%d %H:%M:%S\").values\n        assert_array_equal(actual, expected)\n", "    def test_strftime(self):\n        expected = self.times.strftime(\"%Y-%m-%d\")\n        actual = self.times_data.dt.strftime(\"%Y-%m-%d\").values\n        assert_array_equal(actual, expected)\n"], "sample_254": ["    def test_tabular_inline_new_object_creation(self):\n        response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(self.holder.id,)))\n        self.assertContains(response, INLINE_CHANGELINK_HTML, count=1)\n\n        # Add a new Inner object\n        # Note: We use the 'add' button which is inside the inner formset\n        data = {\n            'form-0-dummy': 'new inner dummy',\n            'add-inner': 'Add another Inner',\n            'save': 'Save'\n        }\n        response = self.client.post(reverse('admin:admin_inlines_holder_change', args=(self.holder.id,)), data)\n\n        self.assertEqual(response.status_code, 302)  # Redirect\n        self.assertRedirects(response, reverse('admin:admin_inlines_holder_change', args=(self.holder.id,)))\n        self.assertEqual(Holder.objects.get(pk=self.holder.id).inner_set.count(), 2)\n\n", "    def test_inline_formset_extra(self):\n        response = self.client.get(reverse('admin:admin_inlines_holder_change', args=[self.holder.pk]))\n        self.assertEqual(response.status_code, 200)\n\n        # Check that the formset has the expected number of extra forms.\n        formset_template = response.context['inline_admin_formsets'][0].template\n        self.assertContains(formset_template, 'extra=\"3\"')\n\n", "    def test_inline_formset_initial_data(self):\n        request = self.factory.get('/admin/admin_inlines/holder/')\n        request.user = self.superuser\n        inline_admin = admin_site._registry[Inner].admin\n        formset = inline_admin.get_formset(request, self.holder)\n        self.assertEqual(len(formset.initial_forms), 1)\n        self.assertEqual(formset.initial_forms[0].instance.pk, Inner.objects.first().pk)\n", "    def test_inline_formset_extra_can_change_permission(self):\n        # Check if extra forms display correctly when the user has change\n        # permission but not add permission.\n\n        # Create a user with change permission but not add permission.\n        permission = Permission.objects.get(codename='change_holder')\n        self.superuser.user_permissions.add(permission)\n        user = User.objects.create_user(username='user', password='password')\n        user.user_permissions.add(permission)\n\n        self.client.force_login(user)\n        response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(self.holder.id,)))\n        self.assertEqual(response.status_code, 200)\n        self.assertContains(response, INLINE_CHANGELINK_HTML, count=1)\n        self.assertContains(response, 'Add Inner', count=1)\n\n        # Check if the \"Add Inner\" button is disabled.\n        self.assertNotContains(response, '<button type=\"submit\" disabled', count=1)\n", "    def test_tabular_inline_without_explicit_formset(self):\n        response = self.client.get(reverse('admin:admin_inlines_holder_change', args=(self.holder.id,)))\n        self.assertContains(response, 'class=\"tabular\"')\n        self.assertContains(response, '<td>42</td>')\n        self.assertEqual(response.context['inline_instances'][0].formset.form.Meta.model, Inner)\n\n"], "sample_873": ["    def test_inverse_transform_sparse(self):\n        selector = StepSelector(step=2)\n        selector.fit(X)\n        X_t = selector.transform(X)\n        X_inv = selector.inverse_transform(X_t)\n        assert_array_equal(X_inv.toarray(), Xinv)\n", "def test_inverse_transform_sparse():\n    selector = StepSelector(step=2)\n    selector.fit(X)\n    Xt = selector.transform(X)\n    X_inv = selector.inverse_transform(Xt.tocsc())\n    assert_array_equal(X_inv.toarray(), Xinv)\n", "    def test_inverse_transform_sparse(self):\n        selector = StepSelector(step=2)\n        selector.fit(X)\n        Xt = selector.transform(X)\n        X_inv = selector.inverse_transform(Xt)\n        assert_array_equal(X_inv.toarray(), Xinv)\n", "def test_transform_preserve_dataframe():\n    sel = StepSelector(step=2)\n    df = pd.DataFrame(X, columns=feature_names)\n    sel.fit(df)\n    Xt = sel.transform(df)\n    assert isinstance(Xt, pd.DataFrame)\n    assert_array_equal(Xt.columns, feature_names_t)\n    assert_array_equal(Xt.values, Xt.to_numpy())\n\n", "    def test_inverse_transform_sparse(self):\n        selector = StepSelector(step=2)\n        selector.fit(X)\n        Xt = selector.transform(X)\n        Xt_sparse = sp.csr_matrix(Xt)\n        X_inv = selector.inverse_transform(Xt_sparse)\n        assert_array_equal(X_inv.toarray(), Xinv)\n"], "sample_1131": ["    def test_MatMul(self):\n        m1 = MatrixSymbol('m1', 2, 2)\n        m2 = MatrixSymbol('m2', 2, 2)\n        self.assertEqual(pycode(m1.dot(m2), language='python'), '(m1).dot(m2)')\n        self.assertEqual(pycode((m1*2).dot(m2), language='python'), '(2 * m1).dot(m2)')\n", "    def test_sympify_scalar_constant(self):\n        a = SymPyPrinter().doprint(S(1))\n        assert str(a) == \"S('1')\"\n\n", "    def test_scipy_special_functions(self):\n        from scipy.special import gamma as scipy_gamma\n        assert pycode(gamma(x), language=\"python with scipy\") ==  'scipy.special.gamma({})'.format(x)\n        assert pycode(loggamma(x), language=\"python with scipy\") == 'scipy.special.gammaln({})'.format(x)\n        assert pycode(erf(x), language=\"python with scipy\") == 'scipy.special.erf({})'.format(x)\n        # TODO: add more test cases for scipy.special functions.\n", "    def test_scipy_special_gammainc(self):\n        from scipy.special import gammainc\n        assert self.check_code(gammainc(x, y), 'scipy.special.gammainc(x, y)')\n", "    def test_scipy_special_gamma_function(self):\n        assert SciPyPrinter().doprint(gammaln(x)) == 'scipy.special.gammaln(x)'\n        assert SciPyPrinter().doprint(loggamma(x)) == 'scipy.special.gammaln(x)'\n"], "sample_425": ["    def test_serializer_for_deconstructible_instances(self):\n        instance = DeconstructibleInstances()\n        with self.assertRaisesMessage(\n            ValueError,\n            \"Cannot serialize: <DeconstructibleInstances instance>\\nThere are some values Django cannot serialize into migration files.\",\n        ):\n            serializer_factory(instance)\n", "    def test_serialize_enum_flag(self):\n        with self.settings(\n            INSTALLED_APPS=[\"custom_migration_operations\"], MIGRATION_MODULES={\"custom_migration_operations\": \"custom_migration_operations.migrations\"}\n        ):\n            operation = migrations.CreateModel(\n                name=\"TextTranslatedEnumModel\",\n                fields=[\n                    models.CharField(\n                        choices=[\n                            (IntFlagEnum.A.value, IntFlagEnum.A.name),\n                            (IntFlagEnum.B.value, IntFlagEnum.B.name),\n                            (IntFlagEnum.A | IntFlagEnum.B, \"A | B\"),\n                        ],\n                        max_length=10,\n                    )\n                ],\n            )\n            writer = OperationWriter(operation)\n            serialized = writer.serialize()\n            expected = (\n                \"migrations.CreateModel(\\n    name='TextTranslatedEnumModel',\\n    fields=[\\n        models.CharField(\\n            choices=[(1, 'A'), (2, 'B'), (3, 'A | B')],\\n            max_length=10,\\n        ),\\n    ],\\n)\"\n            )\n            self.assertEqual(serialized, expected)\n\n", "    def test_serialize_int_flag_enum(self):\n        operation = migrations.AlterField(\n            model_name=\"testmodel1\",\n            name=\"enum_field\",\n            field=models.PositiveSmallIntegerField(\n                choices=[(IntFlagEnum.A, \"A\"), (IntFlagEnum.B, \"B\")],\n                default=IntFlagEnum.A,\n            ),\n        )\n        writer = OperationWriter(operation)\n        serialized = writer.serialize()\n        self.assertEqual(\n            serialized[0],\n            \"AlterField(\\n    model_name='testmodel1',\\n    name='enum_field',\\n    field=models.PositiveSmallIntegerField(\\n        choices=[(IntFlagEnum.A, 'A'), (IntFlagEnum.B, 'B')],\\n        default=IntFlagEnum.A,\\n    ),\\n)\",\n        )\n", "    def test_serialize_datetime_aware_timezone(self):\n        operation = migrations.RunPython(\n            lambda apps, schema_editor: None,\n            lambda apps, schema_editor: None,\n            hints={\n                \"something\": datetime.datetime.now(tz=get_fixed_timezone(0)),\n            },\n        )\n        writer = OperationWriter(operation)\n        serialized = writer.serialize()\n        self.assertIn(\n            \"datetime.datetime(..., tzinfo=datetime.timezone.utc)\", serialized[0]\n        )\n", "    def test_serialize_settingsreference(self):\n        class MySetting(SettingsReference):\n            setting_name = 'MY_SETTING'\n\n        serializer = serializer_factory(MySetting())\n        self.assertEqual(serializer.serialize(), (\"settings.MY_SETTING\", {\"from django.conf import settings\"}))\n"], "sample_1190": ["    def test_get_dimensional_expr_function(self):\n        x = symbols('x')\n        y = symbols('y')\n        f = Function('f')(x)\n        g = Function('g')(y)\n        self.assertEqual(SI.get_dimensional_expr(f * g),\n                         SI.get_dimensional_expr(f) * SI.get_dimensional_expr(g))\n        self.assertEqual(SI.get_dimensional_expr(f + g), SI.get_dimensional_expr(f))\n        self.assertEqual(SI.get_dimensional_expr(f / g),\n                         SI.get_dimensional_expr(f) / SI.get_dimensional_expr(g))\n        self.assertEqual(SI.get_dimensional_expr(f**2),\n                         SI.get_dimensional_expr(f)**2)\n", "def test_get_dimensional_expr():\n    # Test get_dimensional_expr method\n    x = Symbol('x', real=True)\n    t = Symbol('t', real=True)\n    v = Symbol('v', real=True)\n\n    assert SI.get_dimensional_expr(x) == S.One\n    assert SI.get_dimensional_expr(meter*x) == length\n    assert SI.get_dimensional_expr(velocity) == length / time\n    assert SI.get_dimensional_expr(kinetic_energy) == mass * length**2 / time**2\n    assert SI.get_dimensional_expr(\n        kinetic_energy/(x**2)) == mass * length**2 / time**2 / (x**2)\n    assert SI.get_dimensional_expr(\n        kilometer/second + centimeter/minute) == length / time\n\n    with warns_deprecated_sympy(\n        \"The Quantity object is deprecated. Use sympy.physics.units.Quantity instead.\",\n        match='Quantity'\n    ):\n        assert SI.get_dimensional_expr(Quantity('1 meter')) == length\n\n", "def test_derived_units():\n    # Test unit system with derived units\n    m = meter\n    kg = kilogram\n    s = second\n\n    derived_units = {\n        Dimension(mass, length, -2, time, -2): Quantity(\"force\", kg*m/s**2),\n        Dimension(mass, length**2, time**-2): Quantity(\"energy\", kg*m**2/s**2),\n    }\n    custom_unit_system = UnitSystem(\n        base_units=(m, kg, s),\n        derived_units=derived_units)\n\n    force = Quantity(\"force\", 10 * kg * m / s**2)\n    energy = Quantity(\"energy\", 5 * kg * m**2 / s**2)\n\n    assert force.unit == custom_unit_system.get_quantity_dimension(\"force\")\n    assert energy.unit == custom_unit_system.get_quantity_dimension(\"energy\")\n\n", "    def test_get_dimensional_expr(self):\n        x = Symbol('x')\n        y = Symbol('y')\n        t = Symbol('t')\n\n        # Test basic units\n        assert SI.get_dimensional_expr(meter) == Dimension(length)\n        assert SI.get_dimensional_expr(kilogram) == Dimension(mass)\n        assert SI.get_dimensional_expr(second) == Dimension(time)\n\n        # Test combinations\n        assert SI.get_dimensional_expr(meter / second) == Dimension(length / time)\n        assert SI.get_dimensional_expr(meter**2) == Dimension(length**2)\n        assert SI.get_dimensional_expr(kilogram * meter**2 / second**2) == Dimension(\n            mass * length**2 / time**2)\n\n        # Test functions\n        assert SI.get_dimensional_expr(sin(x)) == S.One\n        assert SI.get_dimensional_expr(exp(x*meter/second)) == Dimension(\n            S.One)\n        assert SI.get_dimensional_expr(log(meter)) == S.One\n        assert SI.get_dimensional_expr(sqrt(meter)) == Dimension(length**(1/2))\n\n        # Test derivatives\n        assert SI.get_dimensional_expr(diff(meter*t, t)) == Dimension(length)\n        assert SI.get_dimensional_expr(diff(meter**2 / t, t)) == Dimension(length**2 / time)\n\n        # Test quantities\n        assert SI.get_dimensional_expr(Quantity('1 meter')) == Dimension(length)\n        assert SI.get_dimensional_expr(Quantity('1 kg*m/s')) == Dimension(\n            mass * length / time)\n\n", "    def test_derived_units():\n        assert SI.get_quantity_dimension(joule) == energy\n        assert SI.get_quantity_dimension(volt) == energy / charge\n        assert SI.get_quantity_dimension(ohm) == (energy / charge) / (charge / time)\n\n\n"], "sample_1071": ["def test_convert_to_mixed_dimensions():\n    x = 5*meter*kilogram/second**2\n    y = convert_to(x, [meter, kilogram, second])\n    assert y == 5*meter*kilogram/second**2 \n", "    def test_convert_to_same_units():\n        x = kilometer\n        assert convert_to(x, kilometer) == x\n        assert convert_to(2 * x, kilometer) == 2 * x\n        assert convert_to(x / 2, kilometer) == x / 2\n\n        assert convert_to(meter, kilometer) == meter / 1000 * kilometer\n", "def test_convert_to_complicated():\n    x = (meter**2*kilogram)/(second**3*ampere)\n    y = convert_to(x, [newton, meter, second])\n    assert y == 1000000*newton*meter**2/second**3\n\n\n\n", "def test_convert_to_multi_units():\n    assert convert_to(speed_of_light, [meter, second]) == 299792458*meter/second\n    assert convert_to(3*newton, [centimeter, gram, second]) == 300000*centimeter*gram/second**2\n    assert NS(convert_to(atomic_mass_constant, [gravitational_constant, speed_of_light, hbar]).n(), n=10) == NS(\n        '7.629630850e-20*gravitational_constant**(-0.5)*hbar**(0.5)*speed_of_light**(0.5)', n=10) \n", "    def test_convert_to_multiple_units_with_powers():\n        k = kilometer\n        m = meter\n        s = second\n        assert convert_to(speed_of_light**2, [m**2, s**2]) == 8.987551787368176e+16*m**2/s**2\n\n"], "sample_583": ["    def test_posify_mask_indexer(self):\n        # Test cases for posify_mask_indexer\n        cases = [\n            (\n                B([1, 2, -1, 4]),\n                B([1, 2, 2, 4]),\n            ),\n            (\n                B([1, -1, 3, -1]),\n                B([1, 1, 3, 3]),\n            ),\n            (\n                B([[-1, 2], [3, -1]]),\n                B([\n                    [0, 2],\n                    [3, 3]\n                ]),\n            ),\n        ]\n        for input_indexer, expected_indexer in cases:\n            result = indexing.posify_mask_indexer(input_indexer)\n            assert result == expected_indexer\n", "    def test_vectorized_indexing_with_mask(self):\n        data = np.arange(100).reshape(10, 10)\n        x = DataArray(data)\n        indexer = indexing.VectorizedIndexer(\n            (np.arange(10), slice(None), np.array([1, 3, 5])))\n        x[indexer] = np.nan\n        assert_array_equal(x[indexer], [np.nan] * 3)\n", "    def test_vectorized_indexer_with_slices_and_np_array(self):\n        data = np.arange(24).reshape(3, 8)\n        da = DataArray(data, dims=['x', 'y'])\n\n        i = B(slice(0, 2), np.array([0, 2, 4, 6]))\n        j = B(np.array([1, 3, 5, 7]), slice(1, 2))\n        idx = indexing.VectorizedIndexer((i, j))\n\n        actual = da[idx]\n        expected = da.loc[(slice(0, 2), [0, 2, 4, 6]),\n                          (np.array([1, 3, 5, 7]), slice(1, 2))]\n        assert_array_equal(actual, expected)\n", "    def test_dask_indexing_with_mask(self):\n        da = DataArray(np.arange(100).reshape((10, 10)), dims=['x', 'y'])\n        da = da.chunk({'x': 5, 'y': 5})\n        i = indexing.OuterIndexer((slice(None), slice(None, 5)))\n        actual = da[i].values\n        expected = da.isel(y=slice(None, 5)).values\n        assert_array_equal(actual, expected)\n", "    def test_posify_mask_indexer(self):\n        array = np.arange(10)\n        # Test with a basic indexer\n        indexer = B((np.array([-1, 1, 3, -1]), slice(None)))\n        posified_indexer = indexing.posify_mask_indexer(indexer)\n        expected_indexer = B((np.array([0, 1, 3, 1]), slice(None)))\n        assert posified_indexer == expected_indexer\n\n        # Test with a VectorizedIndexer\n        indexer = indexing.VectorizedIndexer((np.array([-1, 1, 3, -1]),))\n        posified_indexer = indexing.posify_mask_indexer(indexer)\n        expected_indexer = indexing.VectorizedIndexer((np.array([0, 1, 3, 1]),))\n        assert posified_indexer == expected_indexer\n"], "sample_911": ["def test_enum_anon_nested():\n    check(\"enum\", \"enum { A, B };\",\n          {},\n          output=\"enum { A, B };\")\n    check(\"enum\", \"enum { A, B } anonymous;\",\n          {},\n          output=\"enum { A, B } anonymous;\")\n", "def test_cpp_operators():\n    check(\"operator\", \"operator+(int a, int b)\",\n          {1: 'operator+', 2: 'operator_1'})\n\n\n", "def test_enum_scoped():\n    check(\"enum\", \"enum MyEnum\", {\n        1: \"MyEnum\",\n    })\n\n    check(\"enum\", \"enum class MyEnum\", {\n        1: \"MyEnum\",\n    })\n\n    check(\"enum\", \"enum struct MyEnum\", {\n        1: \"MyEnum\",\n    })\n", "    def test_enum_scoped(self):\n        check(\"enum\", \"enum E { A,B };\",\n              {1: \"E\"})\n", "def test_enum_class():\n    check('enum',\n          'enum class MyEnum : int { ONE, TWO };',\n          {1: \"MyEnum\", 2: \"MyEnum\", 3:\"MyEnum\"},  \n          'enum class MyEnum : int { ONE, TWO };')\n\n"], "sample_1109": ["    def test_frac_rewrite():\n        assert frac(x).rewrite(floor) == x - floor(x)\n        assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n", "def test_frac_rewrite_as_floor():\n    assert frac(x).rewrite(floor) == x - floor(x)\n", "def test_frac_rewrite():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n\n    assert frac(i).rewrite(floor) == i - floor(i)\n    assert frac(i).rewrite(ceiling) == i + ceiling(-i)\n\n", "def test_frac_rewrite():\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n", "def test_frac_real():\n    assert frac(Rational(4, 3)) == Rational(1, 3)\n    assert frac(-Rational(4, 3)) == Rational(2, 3)\n    assert frac(2*E) == frac(2*E - floor(2*E))\n    assert frac(pi) == pi - floor(pi)\n    assert frac(-pi) == -pi - floor(-pi)\n\n    assert frac(S.Zero) == S.Zero\n\n    assert frac(x).rewrite(floor) == x - floor(x)\n    assert frac(x).rewrite(ceiling) == x + ceiling(-x)\n\n"], "sample_545": ["    def test_figure_figsize(self):\n        fig, ax = plt.subplots(figsize=(3, 4))\n        assert fig.get_size_inches() == (3, 4)\n        \n        fig, ax = plt.subplots()\n        fig.set_size_inches(8, 6)\n        assert fig.get_size_inches() == (8, 6)\n", "    def test_constrained_layout_with_colorbar(self):\n        fig, ax = plt.subplots()\n        im = ax.imshow(np.arange(100).reshape((10, 10)))\n        cbar = fig.colorbar(im)\n        fig.constrained_layout()\n        fig.canvas.draw()\n        # Just making sure this doesn't raise an error\n        bbox = ax.get_position()\n        assert bbox.x0 >= 0 and bbox.x1 <= 1\n        assert bbox.y0 >= 0 and bbox.y1 <= 1\n\n", "    def test_figure_savefig_dates_format(self):\n        fig, ax = plt.subplots()\n        ax.plot([datetime(2023, 1, 1), datetime(2023, 1, 2)], [1, 2])\n        with io.BytesIO() as buf:\n            fig.savefig(buf, format='png')\n            buf.seek(0)\n            img = Image.open(buf)\n            assert img.format == 'PNG'\n", "    def test_figure_set_size_inches(self):\n        fig, ax = plt.subplots()\n        fig.set_size_inches(10, 5)\n        assert fig.get_figwidth() == 10\n        assert fig.get_figheight() == 5\n", "def test_figure_dpi_set_at_creation():\n    fig = Figure(dpi=120)\n    assert fig.dpi == 120\n\n"]}, "raw_scored_generations": {"sample_4": [true, true, true, true, true], "sample_5": [false, false, false, false, false], "sample_3": [false, false, false, false, true], "sample_0": [true, false, true, true, false], "sample_2": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_6": [false, true, false, false, false], "sample_7": [false, false, false, true, false], "sample_8": [false, false, false, false, false], "sample_9": [false, false, false, false, false], "sample_10": [false, false, false, false, false], "sample_11": [false, false, false, false, false], "sample_12": [false, false, false, false, true], "sample_13": [false, false, false, false, false], "sample_14": [false, false, false, false, false], "sample_15": [false, false, false, false, false], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, false, false], "sample_18": [true, false, false, false, true], "sample_19": [false, false, false, false, true], "sample_20": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_22": [false, false, false, false, false], "sample_23": [false, false, false, false, false], "sample_24": [false, false, false, false, false], "sample_25": [false, false, false, false, false], "sample_26": [false, false, false, false, false], "sample_27": [false, false, false, false, false], "sample_28": [false, false, false, false, false], "sample_29": [true, true, true, true, false], "sample_30": [false, false, false, false, false], "sample_31": [false, false, false, false, false], "sample_32": [true, true, true, true, true], "sample_33": [false, false, false, false, false], "sample_34": [false, false, false, false, false], "sample_35": [false, false, false, false, false], "sample_36": [false, false, false, false, false], "sample_37": [false, false, false, false, true], "sample_38": [false, false, false, false, false], "sample_39": [false, false, false, false, false], "sample_40": [false, false, false, false, false], "sample_41": [false, true, false, false, false], "sample_42": [false, false, false, false, false], "sample_43": [false, false, false, false, false], "sample_44": [false, false, false, false, false], "sample_45": [false, true, false, true, false], "sample_46": [false, false, false, false, false], "sample_47": [false, false, false, false, false], "sample_48": [false, false, false, false, false], "sample_49": [false, false, false, false, false], "sample_50": [true, true, true, true, true], "sample_51": [true, true, true, true, true], "sample_52": [false, false, false, false, false], "sample_54": [false, true, true, false, false], "sample_53": [false, false, false, false, false], "sample_55": [false, false, false, false, false], "sample_58": [false, false, false, false, false], "sample_56": [false, false, false, false, false], "sample_57": [true, true, false, false, false], "sample_59": [false, false, false, false, false], "sample_60": [false, true, false, false, false], "sample_61": [false, false, false, false, false], "sample_62": [false, false, false, false, false], "sample_63": [true, false, true, false, false], "sample_64": [true, false, false, true, true], "sample_65": [false, false, false, false, false], "sample_67": [true, false, false, true, false], "sample_66": [false, true, true, false, true], "sample_68": [false, false, false, false, false], "sample_69": [false, false, false, false, false], "sample_70": [false, false, false, false, false], "sample_71": [false, false, false, false, true], "sample_72": [false, false, false, false, false], "sample_73": [true, true, true, true, true], "sample_75": [false, false, false, false, false], "sample_74": [true, true, true, true, true], "sample_76": [false, false, false, true, false], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [true, true, false, true, false], "sample_81": [true, false, false, false, false], "sample_83": [false, false, false, false, false], "sample_85": [false, false, false, false, false], "sample_84": [false, false, false, false, false], "sample_86": [false, false, false, false, false], "sample_88": [false, true, false, false, false], "sample_87": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [true, false, false, false, false], "sample_92": [false, false, true, true, false], "sample_93": [false, false, false, false, false], "sample_94": [false, false, false, false, false], "sample_95": [true, true, false, false, false], "sample_98": [false, false, false, false, false], "sample_96": [false, false, false, false, false], "sample_99": [false, false, true, false, false], "sample_97": [false, false, true, false, false], "sample_100": [false, false, false, false, false], "sample_102": [false, false, false, false, false], "sample_101": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [true, true, true, true, true], "sample_107": [false, false, false, false, false], "sample_106": [false, true, false, false, false], "sample_105": [false, false, false, false, false], "sample_108": [false, false, false, false, false], "sample_109": [false, false, true, false, false], "sample_111": [false, false, false, false, false], "sample_110": [false, false, false, false, false], "sample_112": [false, false, false, false, false], "sample_113": [false, true, false, false, false], "sample_114": [false, false, false, false, true], "sample_115": [true, false, false, false, false], "sample_116": [false, true, false, false, true], "sample_117": [false, false, false, false, true], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, false], "sample_121": [false, true, false, false, true], "sample_122": [false, false, false, false, false], "sample_123": [false, false, false, false, false], "sample_124": [false, false, false, false, true], "sample_125": [false, false, false, false, true], "sample_126": [false, true, false, false, false], "sample_127": [false, false, false, false, false], "sample_128": [false, false, false, false, false], "sample_129": [false, false, false, false, false], "sample_130": [false, false, false, false, false], "sample_131": [true, true, true, true, true], "sample_132": [false, false, false, true, false], "sample_133": [false, false, false, false, false], "sample_135": [true, true, false, false, false], "sample_134": [false, false, false, false, false], "sample_136": [true, true, false, false, true], "sample_139": [false, false, false, false, false], "sample_137": [false, false, false, false, false], "sample_138": [true, true, true, true, true], "sample_140": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_142": [false, false, false, false, false], "sample_143": [true, true, false, false, false], "sample_144": [true, false, false, false, false], "sample_145": [false, false, false, false, false], "sample_146": [false, false, true, true, true], "sample_147": [true, false, false, false, false], "sample_148": [false, false, false, false, false], "sample_151": [false, false, false, false, false], "sample_149": [false, false, false, false, false], "sample_152": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, false, false], "sample_155": [false, false, false, false, true], "sample_156": [true, false, false, false, false], "sample_157": [true, true, false, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false], "sample_160": [false, false, false, true, false], "sample_161": [false, false, false, true, false], "sample_162": [true, true, true, false, true], "sample_163": [false, true, false, false, false], "sample_164": [false, false, false, false, false], "sample_165": [false, false, false, false, false], "sample_166": [true, false, true, true, false], "sample_167": [true, true, false, false, false], "sample_168": [false, false, false, false, false], "sample_169": [false, false, false, false, false], "sample_171": [false, false, false, false, false], "sample_170": [false, false, false, false, false], "sample_172": [false, false, false, false, false], "sample_173": [false, false, false, true, true], "sample_174": [false, false, false, false, false], "sample_175": [false, false, false, false, false], "sample_176": [false, false, false, false, false], "sample_177": [false, false, false, false, false], "sample_178": [false, false, false, false, true], "sample_180": [false, false, false, true, false], "sample_179": [false, false, false, false, false], "sample_182": [false, false, false, false, true], "sample_181": [false, false, false, false, false], "sample_183": [false, false, false, false, false], "sample_184": [true, false, true, false, false], "sample_185": [false, true, false, false, false], "sample_186": [false, false, false, false, false], "sample_187": [false, false, false, false, true], "sample_188": [false, false, false, false, false], "sample_189": [false, false, false, false, false], "sample_190": [false, false, false, false, false], "sample_191": [false, false, true, false, false], "sample_192": [false, false, false, false, false], "sample_193": [false, false, false, false, false], "sample_194": [false, false, false, false, false], "sample_195": [false, false, false, false, false], "sample_196": [false, false, false, false, true], "sample_198": [false, false, false, false, true], "sample_197": [false, false, false, false, false], "sample_199": [false, false, false, false, false], "sample_200": [true, false, false, false, false], "sample_201": [false, false, false, false, false], "sample_202": [false, false, false, false, false], "sample_203": [false, true, false, false, false], "sample_204": [false, false, false, false, false], "sample_205": [true, false, false, false, false], "sample_206": [false, false, false, false, false], "sample_207": [false, false, false, false, false], "sample_208": [false, false, false, false, false], "sample_209": [false, false, false, false, false], "sample_210": [true, false, false, false, true], "sample_211": [false, false, false, false, false], "sample_213": [false, false, false, false, false], "sample_212": [false, false, false, false, false], "sample_214": [false, false, false, false, false], "sample_215": [false, false, false, false, false], "sample_216": [false, false, false, false, true], "sample_217": [false, false, false, false, false], "sample_218": [false, true, false, false, false], "sample_219": [false, true, false, false, false], "sample_220": [false, false, true, false, false], "sample_221": [false, false, false, false, false], "sample_222": [false, true, true, true, true], "sample_223": [false, false, false, true, false], "sample_224": [true, false, false, false, false], "sample_225": [true, true, true, false, true], "sample_226": [true, true, true, true, true], "sample_227": [false, false, true, false, false], "sample_228": [false, false, false, false, false], "sample_229": [false, false, false, false, false], "sample_230": [false, false, false, false, true], "sample_231": [true, true, false, false, false], "sample_232": [false, false, false, false, false], "sample_233": [false, false, false, true, false], "sample_234": [false, false, false, false, false], "sample_235": [false, true, false, false, true], "sample_236": [false, false, false, false, false], "sample_237": [false, false, false, false, false], "sample_238": [false, false, false, false, false], "sample_239": [false, false, false, false, false], "sample_240": [true, true, false, false, false], "sample_241": [false, false, false, false, true], "sample_242": [false, false, false, false, false], "sample_243": [false, false, false, false, false], "sample_244": [false, false, false, false, false], "sample_245": [true, true, false, true, true], "sample_246": [true, true, true, true, true], "sample_247": [false, false, false, false, false], "sample_248": [false, true, false, false, true], "sample_249": [true, false, false, true, true], "sample_250": [true, false, true, false, true], "sample_251": [false, false, false, false, true], "sample_252": [false, false, false, false, false], "sample_253": [false, false, false, false, false], "sample_254": [false, false, false, false, false], "sample_256": [true, false, false, false, false], "sample_255": [false, false, false, false, false], "sample_257": [false, false, false, false, false], "sample_258": [false, false, false, false, false], "sample_259": [false, false, false, true, true], "sample_260": [false, false, false, false, true], "sample_261": [true, true, true, true, true], "sample_262": [false, false, false, false, false], "sample_263": [false, false, false, false, false], "sample_264": [false, false, false, false, false], "sample_265": [false, false, false, false, false], "sample_266": [false, false, false, false, false], "sample_267": [true, false, false, false, false], "sample_268": [false, false, false, false, false], "sample_269": [false, false, false, false, false], "sample_270": [false, false, false, false, false], "sample_271": [true, false, false, false, false], "sample_272": [false, false, false, false, false], "sample_273": [false, false, false, false, false], "sample_274": [false, false, false, false, true], "sample_275": [true, true, true, false, true], "sample_276": [false, true, false, false, false], "sample_277": [false, true, true, false, false], "sample_278": [false, false, false, false, false], "sample_279": [false, false, false, false, false], "sample_280": [false, false, false, false, false], "sample_281": [true, false, false, false, false], "sample_282": [true, false, false, false, false], "sample_283": [false, false, false, false, false], "sample_284": [false, true, false, true, true], "sample_285": [false, false, false, false, false], "sample_286": [false, false, false, false, false], "sample_287": [false, false, false, false, false], "sample_288": [false, false, false, false, false], "sample_289": [false, true, false, true, false], "sample_290": [false, false, false, false, false], "sample_291": [true, false, false, false, false], "sample_292": [true, true, true, true, true], "sample_293": [false, false, false, false, false], "sample_294": [true, true, true, true, true], "sample_295": [false, false, false, false, true], "sample_296": [false, false, false, false, false], "sample_297": [false, false, false, false, true], "sample_298": [false, true, false, false, false], "sample_299": [false, false, true, true, true], "sample_300": [false, false, false, false, false], "sample_301": [false, false, false, false, false], "sample_302": [false, false, false, false, false], "sample_303": [false, true, false, true, true], "sample_304": [true, true, true, false, false], "sample_305": [false, false, false, false, true], "sample_306": [false, true, true, true, true], "sample_307": [false, true, true, false, false], "sample_308": [false, true, true, false, false], "sample_309": [false, false, false, false, false], "sample_310": [false, false, false, false, false], "sample_312": [false, false, false, false, false], "sample_311": [false, false, false, false, false], "sample_313": [false, false, false, false, false], "sample_314": [true, false, false, false, true], "sample_315": [false, false, false, false, false], "sample_316": [false, false, false, false, true], "sample_317": [false, false, false, false, false], "sample_318": [false, false, false, false, false], "sample_319": [false, false, false, false, false], "sample_320": [false, false, false, false, false], "sample_321": [true, true, true, true, true], "sample_322": [false, false, false, false, false], "sample_323": [false, false, false, false, false], "sample_324": [true, true, true, false, true], "sample_325": [true, false, false, true, false], "sample_326": [false, false, false, false, false], "sample_327": [false, false, false, false, false], "sample_328": [false, false, false, false, false], "sample_329": [false, false, false, false, false], "sample_330": [false, false, false, false, false], "sample_331": [true, true, false, true, true], "sample_332": [false, true, false, false, false], "sample_333": [false, true, false, false, false], "sample_334": [false, false, false, true, true], "sample_335": [false, false, false, true, false], "sample_336": [false, false, false, false, false], "sample_337": [false, true, false, false, false], "sample_338": [false, false, false, false, false], "sample_339": [false, false, false, false, false], "sample_340": [false, false, true, false, false], "sample_341": [false, false, false, false, false], "sample_342": [false, true, false, false, false], "sample_343": [false, false, false, false, false], "sample_344": [false, false, false, false, false], "sample_345": [false, false, false, false, false], "sample_346": [false, false, false, false, false], "sample_347": [true, true, false, false, true], "sample_348": [false, false, true, false, false], "sample_349": [false, false, true, true, true], "sample_350": [false, false, false, false, false], "sample_351": [false, false, false, false, true], "sample_352": [false, false, false, false, false], "sample_353": [false, false, false, false, false], "sample_354": [false, false, false, false, false], "sample_355": [true, false, true, false, false], "sample_356": [false, false, false, false, false], "sample_357": [false, false, false, false, false], "sample_358": [true, false, false, false, true], "sample_359": [false, false, false, false, false], "sample_360": [false, false, false, false, false], "sample_361": [false, false, false, false, false], "sample_362": [false, false, false, false, true], "sample_363": [false, false, false, false, false], "sample_364": [false, false, false, false, false], "sample_365": [false, false, false, false, false], "sample_366": [true, true, false, false, true], "sample_367": [false, false, false, false, false], "sample_368": [false, false, false, false, false], "sample_369": [false, false, false, false, false], "sample_371": [false, false, false, false, false], "sample_370": [false, true, false, false, false], "sample_372": [false, false, false, false, false], "sample_373": [false, false, false, false, false], "sample_374": [false, true, false, false, false], "sample_375": [false, false, true, false, false], "sample_376": [false, false, false, false, false], "sample_377": [false, false, false, true, false], "sample_378": [false, false, false, false, false], "sample_379": [true, true, false, false, true], "sample_380": [false, false, false, false, false], "sample_381": [false, false, false, false, false], "sample_382": [true, false, false, false, false], "sample_383": [true, false, false, false, false], "sample_384": [false, false, false, false, false], "sample_385": [true, false, false, false, true], "sample_386": [true, true, true, false, false], "sample_387": [false, false, false, false, false], "sample_388": [false, false, false, false, false], "sample_389": [true, true, true, true, true], "sample_390": [false, false, false, false, false], "sample_391": [true, false, false, false, false], "sample_392": [false, false, false, false, false], "sample_393": [true, true, true, true, true], "sample_394": [false, false, false, false, false], "sample_395": [false, false, false, false, false], "sample_396": [false, false, false, false, false], "sample_397": [false, false, false, false, false], "sample_398": [false, false, false, false, false], "sample_399": [false, false, false, false, false], "sample_400": [false, false, false, false, false], "sample_401": [false, false, false, false, false], "sample_402": [false, false, false, false, false], "sample_403": [false, false, false, false, false], "sample_404": [true, true, true, true, true], "sample_405": [false, false, false, false, false], "sample_406": [false, false, false, false, false], "sample_407": [false, true, true, true, true], "sample_408": [false, false, false, true, false], "sample_409": [false, false, false, false, false], "sample_410": [true, false, false, false, true], "sample_411": [true, false, false, false, false], "sample_412": [false, false, false, false, false], "sample_413": [true, true, true, true, true], "sample_414": [false, false, false, false, false], "sample_415": [false, false, false, false, false], "sample_416": [false, false, false, false, false], "sample_417": [false, false, false, false, false], "sample_418": [false, false, false, false, false], "sample_419": [false, false, false, false, false], "sample_420": [false, false, true, false, false], "sample_421": [false, false, true, false, false], "sample_422": [false, false, false, false, false], "sample_423": [false, false, false, false, false], "sample_424": [false, false, false, false, false], "sample_425": [false, false, false, false, false], "sample_426": [false, false, false, false, false], "sample_427": [false, false, true, false, false], "sample_428": [false, false, true, false, false], "sample_429": [false, false, false, false, false], "sample_430": [false, false, false, false, false], "sample_431": [false, false, false, false, false], "sample_432": [false, false, false, false, false], "sample_433": [false, false, false, false, false], "sample_434": [false, true, false, false, false], "sample_435": [true, false, false, false, true], "sample_436": [false, false, false, false, false], "sample_437": [false, false, false, false, false], "sample_438": [false, false, false, false, false], "sample_439": [false, false, false, false, true], "sample_440": [false, false, false, true, false], "sample_441": [true, false, true, false, true], "sample_442": [false, true, false, false, false], "sample_443": [false, false, false, false, false], "sample_444": [true, true, true, true, true], "sample_445": [false, false, false, false, false], "sample_446": [false, false, false, false, false], "sample_447": [false, true, false, false, false], "sample_448": [false, false, false, false, false], "sample_449": [false, false, false, false, false], "sample_450": [false, false, false, false, false], "sample_451": [false, false, false, false, false], "sample_453": [false, false, false, false, false], "sample_452": [false, false, false, false, false], "sample_454": [false, false, false, false, false], "sample_455": [false, false, false, false, false], "sample_456": [false, false, false, false, false], "sample_457": [false, false, false, false, false], "sample_458": [false, false, false, false, false], "sample_459": [true, false, true, false, false], "sample_460": [false, false, false, false, false], "sample_461": [false, true, false, true, false], "sample_462": [false, false, false, false, false], "sample_463": [false, false, false, false, false], "sample_464": [true, false, false, false, false], "sample_465": [false, false, false, false, false], "sample_466": [false, false, false, false, false], "sample_467": [false, true, false, false, false], "sample_469": [false, false, false, false, false], "sample_468": [true, false, false, false, false], "sample_470": [false, false, true, false, false], "sample_471": [true, true, false, false, false], "sample_472": [false, false, false, false, false], "sample_473": [false, false, false, false, false], "sample_474": [false, false, false, false, false], "sample_475": [false, false, false, false, false], "sample_476": [false, false, false, false, false], "sample_477": [true, false, false, false, false], "sample_478": [false, false, false, false, false], "sample_479": [false, false, false, false, false], "sample_480": [false, false, false, false, false], "sample_481": [true, false, false, false, false], "sample_482": [false, false, false, false, false], "sample_483": [false, false, false, false, false], "sample_484": [false, false, false, false, false], "sample_485": [false, false, false, true, true], "sample_486": [false, false, false, false, false], "sample_487": [false, false, false, false, false], "sample_488": [false, false, false, false, false], "sample_489": [false, false, false, false, false], "sample_490": [false, false, false, false, false], "sample_491": [false, false, false, false, false], "sample_492": [false, false, false, false, false], "sample_493": [false, false, false, false, false], "sample_494": [false, false, false, false, false], "sample_495": [false, false, false, false, false], "sample_496": [false, false, false, false, false], "sample_497": [false, false, false, false, false], "sample_498": [false, false, false, false, false], "sample_499": [false, false, false, false, false], "sample_500": [false, false, false, false, false], "sample_501": [false, false, false, false, false], "sample_502": [false, false, false, true, false], "sample_503": [false, false, false, false, false], "sample_504": [false, false, false, false, false], "sample_505": [false, false, false, false, false], "sample_506": [false, false, false, false, false], "sample_507": [false, false, false, false, false], "sample_508": [false, false, false, false, false], "sample_509": [false, false, false, false, false], "sample_510": [false, false, false, false, false], "sample_511": [false, false, false, true, false], "sample_512": [false, false, false, false, false], "sample_513": [false, false, false, false, false], "sample_514": [false, false, false, false, false], "sample_515": [false, false, false, false, false], "sample_516": [false, false, false, false, false], "sample_517": [false, false, false, false, false], "sample_518": [false, false, false, false, false], "sample_519": [false, false, false, false, false], "sample_520": [false, false, false, false, false], "sample_521": [false, false, false, false, false], "sample_522": [false, false, false, false, false], "sample_523": [false, false, false, false, false], "sample_524": [false, false, false, false, false], "sample_525": [false, false, false, false, false], "sample_526": [false, false, false, false, false], "sample_527": [false, false, false, false, false], "sample_528": [false, true, false, true, true], "sample_529": [false, false, false, false, false], "sample_530": [false, false, false, false, false], "sample_531": [false, false, false, false, false], "sample_532": [false, false, false, false, false], "sample_533": [false, false, false, false, false], "sample_534": [false, false, false, false, false], "sample_535": [false, false, false, false, false], "sample_536": [false, false, false, false, false], "sample_537": [false, false, false, false, false], "sample_538": [false, false, false, false, false], "sample_539": [false, false, false, false, false], "sample_540": [false, false, false, false, false], "sample_541": [false, false, false, false, false], "sample_542": [false, false, false, false, false], "sample_543": [false, false, false, false, false], "sample_544": [false, false, false, false, false], "sample_545": [false, false, false, false, false], "sample_546": [false, false, false, false, false], "sample_547": [false, false, false, false, false], "sample_548": [false, false, false, false, false], "sample_549": [false, false, false, false, false], "sample_550": [false, false, false, false, false], "sample_551": [false, false, false, false, false], "sample_552": [false, false, false, false, false], "sample_553": [false, false, false, false, false], "sample_554": [false, false, false, false, false], "sample_555": [false, false, false, false, false], "sample_556": [false, false, false, false, false], "sample_557": [false, false, false, false, false], "sample_558": [false, false, false, false, false], "sample_559": [false, false, false, false, false], "sample_560": [false, false, false, false, false], "sample_561": [false, false, false, false, false], "sample_562": [false, false, false, false, false], "sample_563": [false, false, false, false, false], "sample_564": [false, false, false, false, false], "sample_565": [false, false, false, false, false], "sample_566": [false, false, false, false, false], "sample_567": [false, false, false, false, false], "sample_568": [false, false, false, false, false], "sample_569": [false, false, false, false, false], "sample_570": [false, false, false, false, false], "sample_571": [false, false, false, true, false], "sample_572": [false, false, false, false, false], "sample_573": [true, false, false, false, false], "sample_574": [false, false, false, false, false], "sample_575": [false, false, false, false, false], "sample_576": [false, false, false, false, false], "sample_577": [false, false, false, false, false], "sample_578": [false, false, false, false, false], "sample_579": [false, false, false, false, false], "sample_580": [false, false, false, false, false], "sample_581": [false, false, false, false, false], "sample_582": [false, false, false, false, false], "sample_583": [false, false, false, false, false], "sample_584": [true, false, false, false, false], "sample_585": [false, false, false, false, false], "sample_586": [false, false, false, false, false], "sample_587": [false, false, false, false, false], "sample_588": [false, false, false, false, false], "sample_589": [false, false, false, false, false], "sample_590": [false, false, false, false, false], "sample_591": [false, false, false, false, false], "sample_592": [false, false, false, false, false], "sample_593": [false, false, false, false, false], "sample_594": [false, false, false, false, true], "sample_595": [false, false, false, false, false], "sample_596": [false, false, false, false, false], "sample_597": [false, false, false, false, false], "sample_598": [false, false, false, true, false], "sample_599": [false, false, false, false, false], "sample_600": [false, false, false, false, false], "sample_601": [false, false, false, false, false], "sample_602": [false, false, false, false, false], "sample_603": [false, false, false, true, true], "sample_604": [false, false, false, true, false], "sample_605": [false, false, false, true, false], "sample_606": [false, false, false, false, false], "sample_607": [false, true, false, false, true], "sample_608": [false, false, false, false, false], "sample_609": [false, false, false, false, false], "sample_610": [false, false, false, false, false], "sample_611": [false, false, false, false, false], "sample_612": [false, false, false, false, false], "sample_613": [false, false, false, false, false], "sample_614": [false, false, false, false, false], "sample_615": [false, false, false, false, false], "sample_616": [false, false, false, false, false], "sample_617": [false, false, false, false, false], "sample_618": [false, false, false, false, false], "sample_619": [false, false, false, false, false], "sample_620": [false, false, false, false, false], "sample_621": [false, false, false, false, false], "sample_622": [false, false, false, false, false], "sample_623": [false, false, false, false, false], "sample_624": [false, false, false, false, false], "sample_625": [false, false, false, false, false], "sample_626": [false, false, false, false, false], "sample_627": [false, false, false, false, false], "sample_628": [false, true, true, false, true], "sample_629": [false, false, false, false, false], "sample_630": [false, true, false, false, false], "sample_631": [false, false, false, false, false], "sample_632": [false, false, false, false, false], "sample_633": [false, false, false, false, false], "sample_634": [false, false, false, false, false], "sample_635": [false, false, false, false, false], "sample_636": [false, false, false, false, false], "sample_637": [false, false, false, false, false], "sample_638": [false, false, false, true, false], "sample_639": [false, false, false, false, false], "sample_640": [false, false, false, false, false], "sample_641": [false, false, false, false, true], "sample_642": [false, false, false, false, false], "sample_643": [false, false, false, false, false], "sample_644": [false, false, false, false, false], "sample_645": [false, false, false, false, false], "sample_646": [false, false, false, false, false], "sample_647": [false, false, false, false, false], "sample_648": [false, false, false, false, false], "sample_649": [false, false, false, false, false], "sample_650": [false, false, false, false, false], "sample_651": [false, false, false, false, false], "sample_652": [false, false, false, false, false], "sample_653": [false, false, false, false, false], "sample_654": [false, false, false, false, false], "sample_655": [false, false, false, false, false], "sample_656": [false, false, false, false, false], "sample_657": [false, false, false, false, false], "sample_658": [false, false, false, false, false], "sample_659": [false, false, false, false, true], "sample_660": [false, false, false, false, false], "sample_661": [false, false, false, false, false], "sample_662": [false, false, false, false, false], "sample_663": [false, false, false, false, false], "sample_664": [false, false, false, false, false], "sample_665": [false, false, false, false, false], "sample_666": [false, false, false, false, false], "sample_667": [false, false, false, false, false], "sample_668": [false, false, false, false, false], "sample_669": [false, false, false, false, false], "sample_670": [false, false, false, false, false], "sample_671": [false, false, false, false, false], "sample_672": [false, false, false, false, false], "sample_673": [false, false, false, false, false], "sample_674": [false, false, false, false, false], "sample_675": [false, false, false, false, false], "sample_676": [false, false, false, false, false], "sample_677": [false, false, false, false, false], "sample_678": [false, false, false, false, false], "sample_679": [false, false, false, false, false], "sample_680": [false, false, false, false, false], "sample_681": [false, false, false, false, false], "sample_682": [false, false, false, false, false], "sample_683": [false, false, false, false, false], "sample_684": [false, false, false, false, false], "sample_685": [false, false, false, false, false], "sample_686": [false, false, false, false, false], "sample_687": [false, false, false, false, false], "sample_688": [false, false, false, false, false], "sample_689": [false, false, false, false, false], "sample_690": [false, false, false, false, false], "sample_691": [false, false, false, false, false], "sample_692": [false, false, false, false, false], "sample_693": [false, false, false, false, false], "sample_694": [false, false, false, false, false], "sample_695": [false, false, false, true, false], "sample_696": [false, false, false, false, false], "sample_697": [false, false, false, false, false], "sample_698": [false, false, false, false, false], "sample_699": [false, false, false, false, false], "sample_700": [false, false, false, false, false], "sample_701": [false, false, false, false, false], "sample_702": [false, false, false, false, false], "sample_703": [false, false, false, false, false], "sample_704": [false, false, false, false, false], "sample_705": [true, false, false, false, false], "sample_706": [true, true, true, true, true], "sample_707": [false, false, false, false, false], "sample_708": [false, false, false, false, false], "sample_709": [false, false, false, false, false], "sample_710": [false, false, false, false, false], "sample_711": [false, false, false, false, false], "sample_712": [false, false, false, false, false], "sample_713": [false, false, false, false, false], "sample_714": [false, false, false, false, false], "sample_715": [false, false, false, false, false], "sample_716": [false, false, false, false, false], "sample_717": [true, true, true, true, true], "sample_718": [false, false, false, false, false], "sample_719": [false, false, false, false, true], "sample_720": [false, false, false, false, false], "sample_721": [false, false, false, false, false], "sample_722": [false, false, false, false, false], "sample_723": [false, false, false, false, true], "sample_724": [false, false, false, false, false], "sample_725": [false, false, false, false, false], "sample_726": [false, false, false, false, false], "sample_727": [false, false, false, false, true], "sample_728": [false, false, true, true, false], "sample_729": [false, false, false, false, false], "sample_730": [false, false, false, false, false], "sample_731": [false, true, false, false, false], "sample_732": [false, false, false, false, false], "sample_733": [false, false, true, false, false], "sample_734": [false, false, false, false, false], "sample_735": [false, false, false, false, false], "sample_736": [true, true, true, true, false], "sample_737": [false, false, false, false, false], "sample_738": [true, false, false, false, false], "sample_739": [false, false, false, false, false], "sample_740": [false, false, false, false, false], "sample_741": [false, false, false, false, false], "sample_742": [false, false, false, false, false], "sample_743": [false, false, false, false, true], "sample_744": [false, false, false, false, false], "sample_745": [false, true, false, false, false], "sample_746": [false, false, false, false, false], "sample_747": [false, true, false, false, false], "sample_748": [false, false, false, false, false], "sample_749": [false, false, false, false, false], "sample_750": [false, false, false, true, false], "sample_751": [true, false, false, false, false], "sample_752": [false, false, false, false, false], "sample_753": [false, false, false, false, false], "sample_754": [false, false, false, false, false], "sample_755": [false, false, false, false, false], "sample_756": [false, false, false, false, false], "sample_757": [false, false, false, false, false], "sample_758": [false, false, false, false, false], "sample_759": [false, false, false, false, false], "sample_760": [false, true, false, false, false], "sample_761": [false, true, false, false, false], "sample_762": [false, false, false, false, false], "sample_763": [false, false, false, false, false], "sample_764": [false, false, false, false, false], "sample_765": [true, false, true, false, false], "sample_766": [false, false, false, false, false], "sample_767": [false, false, false, false, false], "sample_768": [false, false, false, false, false], "sample_769": [false, false, false, false, false], "sample_770": [false, false, false, false, false], "sample_771": [false, false, false, false, false], "sample_772": [false, false, false, false, false], "sample_773": [false, false, false, false, true], "sample_774": [false, false, false, false, false], "sample_775": [false, false, false, false, false], "sample_776": [false, false, false, false, false], "sample_777": [false, false, true, false, false], "sample_778": [false, false, false, false, false], "sample_779": [true, true, true, true, true], "sample_780": [true, true, true, true, true], "sample_781": [true, true, true, false, true], "sample_782": [false, false, false, false, false], "sample_783": [true, false, false, false, false], "sample_784": [true, false, false, false, false], "sample_785": [false, false, false, true, false], "sample_786": [false, false, false, false, false], "sample_787": [true, false, false, false, false], "sample_788": [true, false, false, false, false], "sample_789": [false, false, false, true, false], "sample_790": [false, false, false, false, false], "sample_791": [false, false, false, false, false], "sample_792": [false, false, false, false, false], "sample_793": [false, true, false, false, false], "sample_794": [false, false, false, false, false], "sample_795": [false, false, false, false, false], "sample_796": [false, false, false, false, false], "sample_797": [false, false, false, false, false], "sample_798": [false, false, false, false, false], "sample_799": [false, false, false, false, false], "sample_800": [true, false, true, true, true], "sample_801": [false, false, false, false, false], "sample_802": [false, false, false, false, false], "sample_803": [false, false, false, false, false], "sample_804": [false, false, false, false, false], "sample_805": [false, false, true, false, false], "sample_806": [false, false, false, false, false], "sample_807": [false, false, false, false, false], "sample_808": [false, false, false, false, false], "sample_809": [false, false, false, false, false], "sample_810": [false, false, false, false, false], "sample_811": [false, false, false, false, false], "sample_812": [false, false, false, false, false], "sample_813": [false, false, false, false, false], "sample_814": [false, false, false, false, false], "sample_815": [true, false, false, false, false], "sample_816": [false, false, false, false, false], "sample_817": [false, false, false, false, false], "sample_818": [false, false, false, false, false], "sample_819": [false, false, false, false, false], "sample_820": [false, false, false, false, false], "sample_821": [false, false, false, false, false], "sample_822": [false, false, false, false, false], "sample_823": [false, false, false, false, false], "sample_824": [false, false, false, false, false], "sample_825": [false, false, false, false, false], "sample_826": [true, true, true, true, true], "sample_827": [false, false, true, false, false], "sample_828": [false, false, false, false, false], "sample_829": [false, false, false, false, false], "sample_830": [false, false, false, false, false], "sample_831": [false, false, false, false, false], "sample_832": [false, false, false, false, false], "sample_833": [false, false, false, false, false], "sample_834": [false, false, false, false, true], "sample_835": [false, true, false, false, true], "sample_836": [true, false, false, false, false], "sample_837": [false, false, false, false, false], "sample_838": [false, false, false, false, false], "sample_839": [true, false, false, false, true], "sample_840": [false, false, false, false, false], "sample_841": [false, false, false, false, false], "sample_842": [false, true, true, false, false], "sample_843": [false, true, false, true, false], "sample_844": [false, false, true, false, false], "sample_845": [false, false, false, false, false], "sample_846": [false, false, false, false, false], "sample_847": [false, false, false, false, false], "sample_848": [false, false, false, false, false], "sample_849": [false, false, true, false, false], "sample_850": [false, false, false, false, false], "sample_851": [false, false, false, false, false], "sample_852": [false, false, false, false, false], "sample_853": [false, false, false, false, false], "sample_854": [false, false, false, false, false], "sample_855": [false, false, false, false, false], "sample_856": [false, false, false, false, false], "sample_857": [true, true, true, true, false], "sample_858": [false, false, false, false, false], "sample_859": [false, false, false, false, false], "sample_860": [false, false, false, false, false], "sample_861": [true, false, false, false, false], "sample_862": [false, false, false, false, false], "sample_863": [false, false, false, false, false], "sample_864": [false, false, false, false, false], "sample_865": [false, false, false, false, false], "sample_866": [false, false, false, false, false], "sample_867": [false, false, true, false, false], "sample_868": [false, false, false, true, false], "sample_869": [false, false, false, false, false], "sample_870": [true, false, false, false, false], "sample_871": [false, true, false, false, false], "sample_872": [false, false, false, false, false], "sample_873": [false, false, false, false, false], "sample_874": [false, false, false, false, false], "sample_875": [false, false, false, false, false], "sample_876": [false, false, false, false, false], "sample_877": [false, false, false, false, false], "sample_878": [false, false, false, false, false], "sample_879": [false, false, false, false, false], "sample_880": [false, false, false, true, true], "sample_881": [false, false, false, false, false], "sample_882": [false, false, false, false, false], "sample_883": [false, false, false, false, false], "sample_884": [true, false, true, false, true], "sample_885": [false, false, false, false, false], "sample_886": [false, false, false, false, false], "sample_887": [false, false, false, false, false], "sample_888": [false, false, false, true, false], "sample_889": [false, false, false, false, false], "sample_890": [false, false, false, false, false], "sample_891": [false, false, false, false, false], "sample_892": [false, false, false, false, false], "sample_893": [true, false, false, false, false], "sample_894": [false, false, false, false, false], "sample_895": [false, false, false, false, false], "sample_896": [false, false, false, false, false], "sample_897": [false, false, false, false, false], "sample_898": [false, false, false, false, false], "sample_899": [false, false, false, false, false], "sample_900": [false, false, false, false, false], "sample_901": [false, false, false, false, false], "sample_902": [false, false, false, false, false], "sample_903": [false, false, false, false, false], "sample_904": [false, false, false, false, false], "sample_905": [false, false, false, false, false], "sample_906": [false, false, false, false, false], "sample_907": [false, false, false, false, false], "sample_908": [false, false, false, false, false], "sample_909": [false, false, false, false, false], "sample_910": [false, true, false, false, false], "sample_911": [false, false, false, false, false], "sample_912": [false, false, false, false, false], "sample_913": [false, false, false, false, false], "sample_914": [false, false, false, false, false], "sample_915": [false, false, false, false, false], "sample_916": [false, false, false, false, false], "sample_917": [false, false, false, false, false], "sample_918": [false, false, false, false, false], "sample_919": [false, false, false, false, false], "sample_920": [false, false, false, false, false], "sample_921": [false, false, false, false, false], "sample_922": [false, false, false, false, false], "sample_923": [false, false, true, false, false], "sample_924": [false, false, false, false, false], "sample_925": [false, false, false, false, false], "sample_926": [false, false, false, false, false], "sample_927": [false, false, false, false, false], "sample_928": [false, false, false, false, false], "sample_929": [false, false, false, false, false], "sample_930": [false, false, false, false, false], "sample_931": [false, false, false, false, false], "sample_932": [false, false, false, false, false], "sample_933": [false, false, false, false, false], "sample_934": [false, false, false, false, false], "sample_935": [false, false, false, false, false], "sample_936": [false, false, false, false, false], "sample_937": [false, false, false, false, false], "sample_938": [false, true, false, false, false], "sample_939": [false, false, false, false, false], "sample_940": [false, false, false, true, false], "sample_941": [false, false, false, false, false], "sample_942": [false, false, false, false, false], "sample_943": [false, true, false, false, true], "sample_944": [false, false, false, false, false], "sample_945": [false, false, false, false, false], "sample_946": [false, false, false, false, false], "sample_947": [false, false, false, false, false], "sample_948": [false, false, false, false, false], "sample_949": [false, false, false, false, false], "sample_950": [false, false, false, false, false], "sample_951": [false, false, false, false, false], "sample_952": [false, false, false, false, false], "sample_953": [true, false, false, false, false], "sample_954": [false, false, false, false, false], "sample_955": [false, false, false, false, false], "sample_956": [false, false, false, false, false], "sample_957": [false, false, false, false, false], "sample_958": [false, false, false, false, false], "sample_959": [false, false, false, false, false], "sample_960": [false, false, false, false, false], "sample_961": [false, false, false, false, false], "sample_962": [false, false, false, false, false], "sample_963": [false, true, false, false, false], "sample_964": [false, false, false, false, false], "sample_965": [false, false, false, false, false], "sample_966": [false, false, false, false, false], "sample_967": [true, true, false, true, false], "sample_968": [false, false, false, false, false], "sample_969": [true, true, true, true, false], "sample_970": [false, false, false, false, false], "sample_971": [false, false, false, false, false], "sample_972": [true, true, true, false, false], "sample_973": [false, false, false, false, false], "sample_974": [false, false, false, false, true], "sample_975": [false, false, true, false, false], "sample_976": [true, false, false, false, true], "sample_977": [false, false, true, false, false], "sample_978": [false, false, false, false, true], "sample_979": [false, false, false, false, true], "sample_980": [false, false, false, false, false], "sample_981": [false, false, false, false, false], "sample_982": [true, true, true, true, true], "sample_983": [false, false, true, false, true], "sample_984": [false, false, false, true, false], "sample_985": [true, true, false, false, true], "sample_986": [true, true, false, false, true], "sample_987": [false, false, true, true, true], "sample_988": [false, false, false, false, false], "sample_989": [false, true, true, true, false], "sample_990": [false, false, true, false, false], "sample_991": [true, true, true, true, true], "sample_992": [false, false, false, false, false], "sample_993": [false, false, false, false, true], "sample_994": [true, false, true, true, true], "sample_995": [true, true, true, true, false], "sample_996": [true, false, false, true, true], "sample_997": [false, false, false, false, false], "sample_998": [false, false, false, false, false], "sample_999": [false, false, false, false, false], "sample_1000": [false, true, false, false, true], "sample_1001": [false, false, false, false, false], "sample_1002": [true, true, false, false, false], "sample_1003": [false, false, false, false, false], "sample_1004": [true, true, false, true, true], "sample_1005": [false, false, false, false, false], "sample_1006": [false, false, false, false, false], "sample_1007": [false, false, true, false, true], "sample_1008": [false, false, false, false, false], "sample_1009": [true, false, true, false, false], "sample_1010": [false, false, false, false, false], "sample_1011": [false, true, false, false, true], "sample_1012": [false, false, false, true, false], "sample_1013": [true, false, false, false, true], "sample_1014": [false, false, true, true, false], "sample_1015": [false, false, false, false, false], "sample_1016": [false, true, false, false, true], "sample_1017": [false, false, false, true, false], "sample_1018": [false, false, false, true, false], "sample_1019": [false, false, false, true, false], "sample_1020": [false, false, false, false, false], "sample_1021": [true, false, false, false, false], "sample_1022": [false, false, false, false, false], "sample_1023": [false, false, false, false, false], "sample_1024": [true, true, true, true, false], "sample_1025": [false, true, false, false, false], "sample_1026": [false, true, true, true, false], "sample_1027": [false, false, true, false, false], "sample_1028": [true, true, true, true, true], "sample_1029": [false, false, false, false, false], "sample_1030": [false, false, false, true, false], "sample_1031": [false, false, false, false, false], "sample_1032": [false, false, false, false, false], "sample_1033": [true, false, true, true, true], "sample_1034": [true, true, true, true, true], "sample_1035": [true, false, false, false, true], "sample_1036": [true, false, false, false, false], "sample_1037": [false, false, false, false, false], "sample_1038": [false, false, false, false, true], "sample_1039": [true, true, false, false, true], "sample_1040": [false, false, true, true, true], "sample_1041": [false, false, false, false, false], "sample_1042": [true, false, true, false, false], "sample_1043": [false, false, false, false, false], "sample_1044": [false, false, false, false, false], "sample_1045": [true, true, true, false, false], "sample_1046": [true, false, false, true, true], "sample_1047": [false, false, true, true, true], "sample_1048": [false, false, true, true, false], "sample_1049": [true, false, false, true, false], "sample_1050": [false, false, false, false, false], "sample_1051": [false, false, false, false, false], "sample_1052": [false, false, false, false, false], "sample_1053": [true, true, false, false, false], "sample_1054": [false, false, false, false, true], "sample_1055": [false, false, true, false, false], "sample_1056": [false, false, false, false, false], "sample_1057": [false, false, false, false, false], "sample_1058": [false, false, false, false, false], "sample_1059": [false, false, false, true, false], "sample_1060": [false, true, false, false, false], "sample_1061": [false, false, true, false, true], "sample_1062": [false, false, false, false, false], "sample_1063": [false, false, false, false, true], "sample_1064": [false, false, true, false, false], "sample_1065": [false, false, false, false, false], "sample_1066": [false, false, false, false, false], "sample_1067": [false, false, false, false, true], "sample_1068": [true, true, false, false, true], "sample_1069": [false, false, false, false, false], "sample_1070": [true, false, false, false, false], "sample_1071": [true, false, false, false, false], "sample_1072": [false, false, true, true, true], "sample_1073": [false, true, false, true, false], "sample_1074": [false, false, false, false, false], "sample_1075": [false, false, false, false, false], "sample_1076": [false, false, false, false, false], "sample_1077": [true, true, false, false, false], "sample_1078": [false, false, false, false, true], "sample_1079": [false, true, true, false, true], "sample_1080": [true, false, false, true, false], "sample_1081": [true, true, true, true, true], "sample_1082": [true, true, true, true, false], "sample_1083": [false, false, false, false, false], "sample_1084": [false, false, false, false, false], "sample_1085": [true, true, false, true, true], "sample_1086": [false, false, false, false, false], "sample_1087": [false, false, false, false, false], "sample_1088": [false, false, true, false, false], "sample_1089": [false, true, false, true, true], "sample_1090": [false, false, false, false, false], "sample_1091": [true, true, true, false, false], "sample_1092": [false, false, false, false, false], "sample_1093": [false, false, false, false, false], "sample_1094": [false, false, false, false, true], "sample_1095": [false, false, true, false, false], "sample_1096": [false, true, false, false, false], "sample_1097": [true, true, false, false, false], "sample_1098": [false, false, false, false, false], "sample_1099": [false, false, false, false, false], "sample_1100": [false, true, false, true, false], "sample_1101": [false, false, false, false, false], "sample_1102": [false, true, true, true, false], "sample_1103": [true, true, false, true, false], "sample_1104": [false, false, false, false, false], "sample_1105": [false, false, false, false, false], "sample_1106": [false, false, false, false, false], "sample_1107": [true, false, false, false, false], "sample_1108": [false, false, false, true, false], "sample_1109": [false, true, true, true, true], "sample_1110": [false, false, false, false, false], "sample_1111": [false, false, false, false, false], "sample_1112": [false, false, false, false, false], "sample_1113": [false, false, false, false, false], "sample_1114": [false, true, false, false, false], "sample_1115": [true, true, true, false, true], "sample_1116": [false, false, false, false, false], "sample_1117": [false, true, false, false, false], "sample_1118": [false, false, false, false, false], "sample_1119": [false, false, false, false, false], "sample_1120": [false, false, true, false, false], "sample_1121": [true, false, false, false, false], "sample_1122": [false, true, true, false, true], "sample_1123": [true, false, false, false, false], "sample_1124": [true, false, true, false, false], "sample_1125": [false, false, false, false, false], "sample_1126": [false, false, false, false, false], "sample_1127": [false, false, false, false, false], "sample_1128": [false, false, true, true, false], "sample_1129": [false, false, false, false, false], "sample_1130": [true, false, true, true, false], "sample_1131": [false, false, false, false, false], "sample_1132": [false, false, true, false, false], "sample_1133": [false, false, false, false, false], "sample_1134": [false, false, false, false, false], "sample_1135": [false, false, true, false, true], "sample_1136": [true, false, true, false, true], "sample_1137": [false, false, false, true, false], "sample_1138": [false, false, false, false, false], "sample_1139": [true, true, true, false, false], "sample_1140": [false, false, false, false, false], "sample_1141": [true, false, false, false, true], "sample_1142": [false, false, false, false, true], "sample_1143": [false, false, false, true, true], "sample_1144": [false, false, false, false, false], "sample_1145": [true, false, false, false, false], "sample_1146": [false, false, false, false, false], "sample_1147": [false, false, true, false, true], "sample_1148": [false, true, false, false, false], "sample_1149": [false, false, false, false, false], "sample_1150": [false, false, false, false, false], "sample_1151": [true, true, true, true, true], "sample_1152": [false, false, false, false, true], "sample_1153": [false, true, false, false, true], "sample_1154": [false, false, false, false, false], "sample_1155": [false, false, false, false, false], "sample_1156": [false, true, false, false, true], "sample_1157": [false, false, false, false, false], "sample_1158": [false, true, true, false, false], "sample_1159": [false, true, false, true, false], "sample_1160": [false, false, false, false, false], "sample_1161": [false, false, false, false, false], "sample_1162": [false, false, false, false, false], "sample_1163": [false, false, true, true, false], "sample_1164": [false, true, false, false, false], "sample_1165": [true, false, false, false, false], "sample_1166": [true, true, true, false, false], "sample_1167": [false, false, false, false, false], "sample_1168": [false, false, false, false, false], "sample_1169": [true, false, false, true, true], "sample_1170": [false, false, false, true, false], "sample_1171": [true, false, false, false, false], "sample_1172": [false, false, false, false, false], "sample_1173": [true, false, false, false, false], "sample_1174": [false, true, false, true, false], "sample_1175": [false, false, false, true, false], "sample_1176": [true, true, false, false, false], "sample_1177": [true, false, true, false, false], "sample_1178": [false, false, false, false, false], "sample_1179": [false, false, false, false, false], "sample_1180": [false, false, true, true, false], "sample_1181": [false, false, false, false, false], "sample_1182": [false, false, false, false, false], "sample_1183": [false, false, false, true, false], "sample_1184": [true, false, true, false, false], "sample_1185": [true, false, true, false, false], "sample_1186": [true, false, true, false, false], "sample_1187": [false, false, false, false, false], "sample_1188": [false, false, false, false, false], "sample_1189": [false, true, false, false, true], "sample_1190": [false, false, false, false, false], "sample_1191": [true, false, false, false, false], "sample_1192": [false, false, true, false, false], "sample_1193": [false, false, true, false, true], "sample_1194": [false, false, false, false, false], "sample_1195": [false, false, false, false, false], "sample_1196": [false, false, false, false, false], "sample_1197": [false, false, false, false, false], "sample_1198": [false, false, false, false, false], "sample_1199": [false, false, false, false, false], "sample_1200": [false, false, false, false, false], "sample_1201": [false, false, false, false, false], "sample_1202": [true, false, true, false, true], "sample_1203": [true, false, false, false, true], "sample_1204": [false, true, false, false, false], "sample_1205": [false, false, false, false, false], "sample_1206": [true, true, true, true, true], "sample_1207": [false, false, false, false, false], "sample_1208": [false, false, false, false, false], "sample_1209": [false, false, false, false, false]}}