{"raw_generations": {"sample_33": ["def test_disconnect_uid_multiple_receivers(self):\n        pass\n\n        pass\n\n    a_signal.connect(uid_based_receiver_1, dispatch_uid=\"uid\")\n    a_signal.connect(uid_based_receiver_2, dispatch_uid=\"uid\")\n    self.assertEqual(len(a_signal.receivers), 1)\n    a_signal.disconnect(dispatch_uid=\"uid\")\n    self.assertTestIsClean(a_signal)\n", "def test_disconnect_multiple_registration(self):\n        return kwargs\n\n        return kwargs\n\n        return kwargs\n\n    a_signal.connect(receiver_1)\n    a_signal.connect(receiver_2)\n    a_signal.connect(receiver_3)\n    result = a_signal.send(sender=self, val=\"test\")\n    self.assertEqual(len(result), 3)\n    self.assertEqual(len(a_signal.receivers), 3)\n    a_signal.disconnect(receiver_2)\n    a_signal.disconnect(receiver_2)\n    del receiver_3\n    garbage_collect()\n    self.assertEqual(len(a_signal.receivers), 1)\n    self.assertTestIsClean(a_signal)\n", "def test_receiver_disconnect_while_receiving(self):\n        self.receivers_connected = True\n        # Simulate a receiver disconnecting during the execution of the signal\n        a_signal.disconnect(receiver_disconnect_while_receiving, sender=self)\n    a_signal.connect(receiver_disconnect_while_receiving, sender=self)\n    self.assertFalse(hasattr(self, 'receivers_connected'))\n    a_signal.send(sender=self, val='test')\n    self.assertTrue(hasattr(self, 'receivers_connected'))\n    self.assertTestIsClean(a_signal)\n", "def test_weak_reference_cleanup(self):\n    \"\"\"\n    Ensure that weak references are properly cleaned up when the object is garbage collected\n    \"\"\"\n    a = Callable()\n    b = Callable()\n\n    # Connect both receivers with weak and strong references\n    a_signal.connect(a.a, sender=self, weak=False)\n    a_signal.connect(b.a, sender=self, weak=True)\n\n    # Delete the weak referenced receiver\n    del b\n    garbage_collect()\n\n    # Check that the weak reference was properly cleaned up\n    self.assertEqual(a_signal._live_receivers(sender=self), [a.a])\n\n    # Check that the strong reference is still present\n    self.assertEqual(len(a_signal._live_receivers(sender=self)), 1)\n    self.assertEqual(a_signal._live_receivers(sender=self)[0], a.a)\n\n    # Disconnect the strong reference\n    a_signal.disconnect(a.a, sender=self)\n\n    # Check that the strong reference was properly removed\n    self.assertEqual(a_signal._live_receivers(sender=self), [])\n    self.assertTestIsClean(a_signal)\n", "    def test_garbage_collected_sender(self):\n        \"\"\"Test that garbage collection is handled for the sender even when signal is not cached\"\"\"\n        class Sender:\n            pass\n\n        sender = Sender()\n        a_signal.connect(receiver_1_arg, sender=sender)\n        del sender\n        garbage_collect()\n        result = a_signal.send(sender=self, val=\"test\")\n        self.assertEqual(result, [])\n        self.assertTestIsClean(a_signal)\n"], "sample_42": ["    def test_serialize_regex_flags(self):\n        \"\"\"\n        Make sure compiled regex flags can be serialized.\n        \"\"\"\n        regex = re.compile(r'^\\w+$', flags=re.IGNORECASE)\n        self.assertSerializedEqual(regex)\n", "    def test_serialize_uuid_with_choices(self):\n        uuid_a = uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8')\n        uuid_b = uuid.UUID('c7853ec1-2ea3-4359-b02d-b54e8f1bcee2')\n        self.assertSerializedResultEqual(\n            models.UUIDField(choices=((uuid_a, 'UUID A'), (uuid_b, 'UUID B')), default=uuid_a),\n            (\n                \"models.UUIDField(choices=[(uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'), 'UUID A'), (uuid.UUID('c7853ec1-2ea3-4359-b02d-b54e8f1bcee2'), 'UUID B')], default=uuid.UUID('5c859437-d061-4847-b3f7-e6b78852f8c8'))\",\n                {'import uuid'},\n            )\n        )\n", "def test_serialize_uuid_field(self):\n    \"\"\"\n    Tests serialization of UUIDField.\n    \"\"\"\n    field = models.UUIDField(default=uuid.uuid4)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.UUIDField(default=uuid.UUID('uuid uuid4 value'))\"\n    )\n    self.serialize_round_trip(field)\n\n    # Test choices\n    choices = [(uuid.uuid1(), 'uuid1'), (uuid.uuid4(), 'uuid4')]\n    field = models.UUIDField(choices=choices, default=uuid.uuid4)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.UUIDField(choices=[(uuid.UUID('uuid1 value'), 'uuid1'), (uuid.UUID('uuid4 value'), 'uuid4')], \"\n        \"default=uuid.UUID('uuid4 value'))\"\n    )\n    self.serialize_round_trip(field)\n\n    # Test null and blank\n    field = models.UUIDField(null=True, blank=True)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.UUIDField(null=True, blank=True)\"\n    )\n    self.serialize_round_trip(field)\n", "    def test_fails_with_type_not_found(self):\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize: int\\n'):\n            serializer_factory(1)\n", "    def test_register_serializer(self):\n        # Test that serializers are registered in the correct order\n        class BaseSerializer(BaseSerializer):\n            pass\n\n        class DerivedSerializer(BaseSerializer):\n            pass\n\n            return 'test(%r)' % value, {}\n\n        MigrationWriter.register_serializer(int, BaseSerializer)\n        MigrationWriter.register_serializer(int, DerivedSerializer)\n\n        self.assertEqual(MigrationWriter._registry[int], BaseSerializer)\n        self.assertEqual(MigrationWriter._registry[int] is not DerivedSerializer, True)\n"], "sample_112": ["def test_isotonic_regression_out_of_bounds_nan():\n    # Set y and x\n    y = np.array([3, 7, 5, 9, 8, 7, 10])\n    x = np.arange(len(y))\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=\"auto\", out_of_bounds=\"nan\")\n    ir.fit(x, y)\n\n    # Predict from  training and test x and check that we have two NaNs.\n    y1 = ir.predict([min(x) - 10, max(x) + 10])\n    assert sum(np.isnan(y1)) == 2\n\n    # Predict from training x and check that we don't have any NaNs\n    y2 = ir.predict(x)\n    assert not np.any(np.isnan(y2))\n", "def test_isotonic_regression_bounds_warning():\n    # check that it warns when y_min or y_max is set with increasing=False\n    y = np.array([10, 9, 10, 7, 6, 6.1, 5])\n    x = np.arange(len(y))\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=False, y_min=0)\n    with pytest.warns(UserWarning, match=\"isotropic\"):\n        ir.fit(x, y)\n\n    # Create model and fit\n    ir = IsotonicRegression(increasing=False, y_max=10)\n    with pytest.warns(UserWarning, match=\"isotropic\"):\n        ir.fit(x, y)\n", "def test_isotonic_regression_with_equal_weights_and_equal_predicted_values():\n    y = np.array([3, 7, 5, 9, 8, 7, 10])\n    y_ = np.array([3, 6, 6, 8, 8, 8, 10])\n\n    ir = IsotonicRegression()\n    ir.fit(np.arange(len(y)), y)\n    assert_array_equal(ir.fit_transform(np.arange(len(y)), y), y_)\n    assert_array_equal(ir.transform(np.arange(len(y))), y_)\n\n    # Check that equal weights lead to equal predictions\n    ir = IsotonicRegression()\n    ir.fit(np.arange(len(y)), y, sample_weight=[1.0]*len(y))\n    assert_array_equal(ir.fit_transform(np.arange(len(y)), y, sample_weight=[1.0]*len(y)), y_)\n    assert_array_equal(ir.transform(np.arange(len(y)), sample_weight=[1.0]*len(y)), y_)\n", "def test_isotonic_regression_dtype_warning():\n    # Check that a warning is raised when the dtype of y and y_thresholds don't match\n    reg = IsotonicRegression()\n    y = np.array([2, 1, 4, 3, 5], dtype=np.int32)\n    X = np.arange(len(y)).astype(np.float64)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        reg.fit(X, y)\n        assert len(w) == 1\n        assert \"Input data type not supported\" in str(w[-1].message)\n        assert reg.y_thresholds_.dtype == np.float64\n\n    # Check that a warning is raised when the dtype of X and X_thresholds don't match\n    reg = IsotonicRegression()\n    X = np.arange(len(y)).astype(np.float32)\n    y = np.array([2, 1, 4, 3, 5], dtype=np.int32)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        reg.fit(X, y)\n        assert len(w) == 1\n        assert \"Input data type not supported\" in str(w[-1].message)\n        assert reg.X_thresholds_.dtype == np.float32\n\n    # Check that a warning is raised when the dtype of y_thresholds and sample_weight don't match\n    reg = IsotonicRegression()\n    y = np.array([2, 1, 4, 3, 5], dtype=np.int32)\n    X = np.arange(len(y)).astype(np.float64)\n    sample_weight = np.ones(len(y), dtype=np.float32)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(\"always\")\n        reg.fit(X, y, sample_weight=sample_weight)\n        assert len(w) == 1\n        assert \"Input data type not supported\" in str(w[-1].message)\n        assert reg.y_thresholds_.dtype == np.float64\n\n    # Check that a warning is not raised when the dtype of X, y, and sample_weight are the same\n    reg = IsotonicRegression()\n    X = np.arange(len(y)).astype(np.float64)\n    y = np.array([2, 1, 4, 3, 5], dtype=np.float64)\n    sample_weight = np.ones(len(y), dtype=np.float64)\n    reg", "def test_isotonic_regression_1darray_zero_length():\n    # check that it is possible to fit a model with an empty 1d array\n    ir = IsotonicRegression()\n    assert ir.fit(np.array([], dtype=float)).predict(np.array([], dtype=float)).shape == (0,)\n\n    # check that it is possible to fit a model with a 1d array of 1 element\n    ir = IsotonicRegression()\n    assert ir.fit(np.array([1], dtype=float)).predict(np.array([1], dtype=float)).shape == (1,)\n\n"], "sample_84": ["    def test_scope_metadata(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture(scope=\"session\")\n                return request.scope_metadata\n                assert arg == \"session\"\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reprec.assertoutcome(passed=1)\n", "    def test_autouse_in_conftest(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n            @pytest.fixture(autouse=True)\n                pass\n\n            @pytest.fixture()\n                pass\n        \"\"\"\n        )\n        reprec = testdir.inline_run()\n        reprec.assertoutcome(passed=1)\n", "    def test_getfixturevalue_sets_request(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.fixture\n                pass\n            @pytest.fixture\n                pass\n            @pytest.fixture\n                pass\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        assert result.ret == 0\n        config = result.config\n        item = config.get_terminal_writer().getmaster().session.items[0]\n        request = fixtures.FixtureRequest(item)\n        request.getfixturevalue(\"arg1\")\n        assert request.keywords == {'arg1': 1}\n        request.getfixturevalue(\"arg2\")\n        assert request.keywords == {'arg1': 1, 'arg2': 2}\n        request.getfixturevalue(\"arg3\")\n        assert request.keywords == {'arg1': 1, 'arg2': 2, 'arg3': 3}\n", "    def test_overriden_fixture_missing_param(self, testdir):\n        testdir.makeconftest(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture(params=[1,2])\n                return request.param\n\n            @pytest.fixture()\n                return request.getfixturevalue('fix')\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest()\n        result.stdout.fnmatch_lines(\n            [\"*The requested fixture has no parameter defined for test:.*\",\n             \"    test_getfixturevalue_error.py::test_missing\",\n             \"Requested fixture 'fix' defined in:.*\",\n             \"test_getfixturevalue_error.py:6\",\n             \"Requested here:.*\",\n             \"test_getfixturevalue_error.py:12\",\n             \"*1 error*\",\n             ]\n        )\n", "    def test_fixturevalue_on_failure(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                raise ValueError\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest(\"-s\")\n        assert result.ret != 0\n        result.stdout.fnmatch_lines(\n            [\n                \"*ERROR*test_fixturevalue_on_failure*\",\n                \"    def test_foo(fix):\",\n                \"*ValueError*\",\n            ]\n        )\n"], "sample_85": ["def test_log_file_unicode_encoding(testdir):\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            # -*- coding: utf-8 -*-\n            from __future__ import unicode_literals\n            import logging\n\n                logging.getLogger('catchlog').info(u\"Normal message\")\n                logging.getLogger('catchlog').info(u\"Normal message\")\n                logging.getLogger('catchlog').info(u\"\u251c\")\n                logging.getLogger('catchlog').info(u\"Normal message\")\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert u\"Normal message\" in contents\n        assert u\"\u251c\" in contents\n        assert u\"Normal message\" in contents\n", "def test_log_cli_level_custom_format(testdir):\n    # Default log file level\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.level == logging.INFO\n            logging.getLogger('catchlog').debug(\"This log message won't be shown\")\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-cli-format='%(asctime)s %(levelname)s %(name)s: %(message)s'\")\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_level_custom_format.py*2019-01-01 12:00:00,000 INFO catchlog: This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    assert \"This log message won't be shown\" not in result.stdout.str()\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_file_multiple_handlers(testdir):\n    # Default log file level\n    log_file = testdir.tmpdir.join(\"pytest.log\").strpath\n\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_file={}\n        log_file_level = INFO\n        log_file_format = '%(asctime)s %(levelname)-8s %(name)s:%(filename)s:%(lineno)d %(message)s'\n        log_file_date_format = '%Y-%m-%d %H:%M:%S'\n        \"\"\".format(\n            log_file\n        )\n    )\n    testdir.makepyfile(\n        \"\"\"\n            import logging\n            logger1 = logging.getLogger('logger1')\n            logger2 = logging.getLogger('logger2')\n\n                logger1.info(\"This is a log message from logger1\")\n                logger2.warning(\"This is a log message from logger2\")\n                print('PASSED')\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n    assert os.path.isfile(log_file)\n    with open(log_file, encoding=\"utf-8\") as rfh:\n        contents = rfh.read()\n        assert \"This is a log message from logger1\" in contents\n        assert \"This is a log message from logger2\" in contents\n", "def test_log_cli_format(testdir):\n    # Default log file level\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter.format(\"test\").startswith(\"levelname\")\n            logging.getLogger('catchlog').info(\"This log message won't be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n        log_cli_format = %(levelname)-8s %(message)s\n        \"\"\"\n    )\n\n    result = testdir.runpytest(\"-s\", \"--log-cli-format=%(levelname)s\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_format.py*warning\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    assert \"This log message will be shown\" not in result.stdout.str()\n    assert \"This log message won't be shown\" not in result.stdout.str()\n\n    # make sure that that we get a '0' exit code for the testsuite\n    assert result.ret == 0\n", "def test_log_cli_formatter(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import logging\n        import logging.config\n\n            logging.config.dictConfig({'version': 1,\n                                     'formatters': {\n                                         'f1': {'format': \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"}\n                                     },\n                                     'handlers': {\n                                         'h1': {'class': 'logging.StreamHandler',\n                                                'formatter': 'f1',\n                                                'level': 'INFO'},\n                                     },\n                                     'root': {'level': 'DEBUG',\n                                              'handlers': ['h1']}})\n            plugin = request.config.pluginmanager.getplugin('logging-plugin')\n            assert plugin.log_cli_handler.formatter.format_string == \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n            logging.getLogger('catchlog').info(\"This log message will be shown\")\n            logging.getLogger('catchlog').warning(\"This log message will be shown\")\n            print('PASSED')\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        log_cli=true\n    \"\"\"\n    )\n\n    result = testdir.runpytest()\n\n    # fnmatch_lines does an assertion internally\n    result.stdout.fnmatch_lines(\n        [\n            \"*test_log_cli_formatter.py*2023-07-26 14:30:00,000 - test_log_cli_formatter - INFO - This log message will be shown\",\n            \"*test_log_cli_formatter.py*2023-07-26 14:30:00,000 - test_log_cli_formatter - WARNING - This log message will be shown\",\n            \"PASSED\",  # 'PASSED' on its own line because the log message prints a new line\n        ]\n    )\n    assert \"2023-07-26 14:30:00,000 - test_log_cli_formatter - WARNING - This log message won't be shown\" not in result.stdout.str()\n"], "sample_4": ["    def test_content_type_default_charset(self):\n        r = HttpResponse()\n        self.assertEqual(r.charset, settings.DEFAULT_CHARSET)\n        r['Content-Type'] = 'application/json'\n        self.assertEqual(r.charset, 'utf-8')\n", "    def test_non_string_charset(self):\n        # Test that the charset is set correctly when the content type doesn't specify one\n        r = HttpResponse()\n        r['Content-Type'] = 'application/json'\n        self.assertEqual(r.charset, settings.DEFAULT_CHARSET)\n\n        # Test that the charset is set correctly when the content type specifies a different charset\n        r = HttpResponse()\n        r['Content-Type'] = 'application/json; charset=UTF-8'\n        self.assertEqual(r.charset, 'UTF-8')\n", "    def test_streaming_response_with_iterable_of_iterables(self):\n        r = StreamingHttpResponse([['hello', 'world'], ['foo', 'bar']])\n        chunks = list(r)\n        self.assertEqual(chunks, [b'hello', b'world', b'foo', b'bar'])\n        for chunk in chunks:\n            self.assertIsInstance(chunk, bytes)\n\n        # and the response can only be iterated once.\n        self.assertEqual(list(r), [])\n\n        # even when a sequence that can be iterated many times, like a list,\n        # is given as content.\n        r = StreamingHttpResponse([['abc', 'def'], ['ghi', 'jkl']])\n        self.assertEqual(list(r), [b'abcdef', b'jkl'])\n        self.assertEqual(list(r), [])\n\n        # and you can't accidentally assign to a `content` attribute.\n        with self.assertRaises(AttributeError):\n            r.content = 'xyz'\n\n        # but they do have a `streaming_content` attribute.\n        self.assertTrue(hasattr(r, 'streaming_content'))\n\n        # that exists so we can check if a response is streaming, and wrap or\n        # replace the content iterator.\n        r.streaming_content = [['abc', 'def'], ['ghi', 'jkl']]\n        self.assertEqual(list(r), [b'abcdef', b'jkl'])\n\n        # coercing a streaming response to bytes doesn't return a complete HTTP\n        # message like a regular response does. it only gives us the headers.\n        r = StreamingHttpResponse([['hello', 'world']])\n        self.assertEqual(bytes(r), b'Content-Type: text/html; charset=utf-8')\n\n        # and this won't consume its content.\n        self.assertEqual(list(r), [b'hello', b'world'])\n\n        # additional content cannot be written to the response.\n        r = StreamingHttpResponse([['hello', 'world']])\n        with self.assertRaises(Exception):\n            r.write('!')\n", "    def test_iterable_content_with_non_string_chunks(self):\n        # Test if HttpResponse correctly handles iterable content with non-string chunks.\n        r = HttpResponse([1, 2, 'hello', b'world'])\n        self.assertEqual(r.content, b'123hello' + b'world')\n\n        # Test if HttpResponse correctly handles iterable content with non-string chunks when iterated.\n        r = HttpResponse([1, 2, 'hello', b'world'])\n        chunks = list(r)\n        self.assertEqual(chunks, [b'1', b'2', b'hello', b'world'])\n        for chunk in chunks:\n            self.assertIsInstance(chunk, bytes)\n", "    def test_status_code_integrity(self):\n        \"\"\"\n        Test if the status code is not modified when setting other attributes.\n        \"\"\"\n        r = HttpResponse()\n        self.assertEqual(r.status_code, 200)\n        r['Content-Type'] = 'text/plain'\n        self.assertEqual(r.status_code, 200)\n        r['Content-Type'] = 'application/json'\n        r.reason_phrase = 'Not Found'\n        self.assertEqual(r.status_code, 200)\n        r.charset = 'utf-8'\n        self.assertEqual(r.status_code, 200)\n        r.content = 'Hello'\n        self.assertEqual(r.status_code, 200)\n        r['X-Test'] = 'value'\n        self.assertEqual(r.status_code, 200)\n"], "sample_66": ["    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            exclude = 10\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'exclude' must be a list or tuple.\",\n            \"admin.E014\",\n        )\n", "    def test_multiple_prepopulated_fields(self):\n        class TestModelAdmin(ModelAdmin):\n            prepopulated_fields = {\"slug\": (\"name\", \"bio\"), \"sign_date\": (\"name\",)}\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'prepopulated_fields' refers to 'name', which must not be \"\n            \"a DateTimeField, a ForeignKey, a OneToOneField, or a ManyToManyField.\",\n            \"admin.E028\",\n        )\n", "    def test_not_iterable(self):\n        class TestModelAdmin(ModelAdmin):\n            prepopulated_fields = {\"slug\": \"test\"}\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'prepopulated_fields[\\\"slug\\\"]' must be a list or tuple.\",\n            \"admin.E029\",\n        )\n", "    def test_prepopulated_fields_contain_non_field(self):\n        class TestModelAdmin(ModelAdmin):\n            prepopulated_fields = {\"slug\": (\"non_existent_field\",)}\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'prepopulated_fields[\\\"slug\\\"][0]' refers to \"\n            \"'non_existent_field', which is not a field of \"\n            \"'modeladmin.ValidationTestModel'.\",\n            \"admin.E027\",\n        )\n", "    def test_invalid_related_field_type(self):\n        class TestModelAdmin(ModelAdmin):\n            date_hierarchy = \"band__name\"\n\n        self.assertIsInvalid(\n            TestModelAdmin,\n            ValidationTestModel,\n            \"The value of 'date_hierarchy' must be a DateField or DateTimeField.\",\n            \"admin.E128\",\n        )\n"], "sample_116": ["def test_create_triple_index_with_multiple_categories(app):\n    text = (\".. index:: triple: foo; bar; baz\\n\"\n            \".. index:: triple: Python; Sphinx; reST\\n\"\n            \".. index:: triple: Python; Sphinx; Python\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 6\n    assert index[0] == ('B', [('bar', [[], [('baz, foo', [('', '#index-0')])], None),\n                              ('baz', [[], [('foo bar', [('', '#index-0')])], None])])\n    assert index[1] == ('F', [('foo', [[], [('bar baz', [('', '#index-0')])], None)])\n    assert index[2] == ('P', [('Python', [[], [('Sphinx reST', [('', '#index-1')])], None)])\n    assert index[3] == ('P', [('Python', [[], [('Python', [('', '#index-2')])], None]])\n    assert index[4] == ('R', [('reST', [[], [('Python Sphinx', [('', '#index-1')])], None)])\n    assert index[5] == ('S', [('Sphinx', [[], [('reST, Python', [('', '#index-1')])], None]])\n", "def test_create_triple_index_with_name(app):\n    text = (\".. index:: triple: foo; bar; baz\\n\"\n            \"   :name: ref1\\n\"\n            \".. index:: triple: Python; Sphinx; reST\\n\"\n            \"   :name: ref2\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('B', [('bar', [[], [('baz, foo', [('', '#index-0')]), ('foo, baz', [('', '#index-1')])], None]])\n    assert index[1] == ('F', [('foo', [[], [('bar baz', [('', '#index-0')]), ('baz, foo', [('', '#index-1')])], None]])\n    assert index[2] == ('P', [('Python', [[], [('reST, Sphinx', [('', '#index-2')]), ('Sphinx, reST', [('', '#index-3')])], None]])\n    assert index[3] == ('R', [('reST', [[], [('Python Sphinx', [('', '#index-2')]), ('Sphinx, reST', [('', '#index-3')])], None]])\n    assert index[4] == ('S', [('Sphinx', [[], [('foo, baz', [('', '#index-1')]), ('baz, foo', [('', '#index-1')]), ('Python, reST', [('', '#index-3')]), ('reST, Sphinx', [('', '#index-2')])], None]]))\n", "def test_create_triple_index_with_name(app):\n    text = (\".. index:: triple: foo; bar; baz\\n\"\n            \"   :name: ref1\\n\"\n            \".. index:: triple: Python; Sphinx; reST\\n\"\n            \"   :name: ref2\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('B', [('bar', [[], [('baz, foo', [('', '#index-0')]), ('baz foo', [('', '#index-0')])], None],\n                                ('ref1', []))]\n    assert index[1] == ('F', [('foo', [[], [('bar baz', [('', '#index-0')]), ('foo bar', [('', '#index-0')])], None,\n                                            ('ref1', [])]])\n    assert index[2] == ('P', [('Python', [[], [('Sphinx reST', [('', '#index-1')]), ('Sphinx reST, Python', [('', '#index-1')])], None,\n                                              ('ref2', [])]])\n    assert index[3] == ('R', [('reST', [[], [('Python Sphinx', [('', '#index-1')]), ('reST, Python', [('', '#index-1')])], None,\n                                             ('ref2', [])]])\n    assert index[4] == ('S', [('Sphinx', [[], [('reST, Python', [('', '#index-1')]), ('reST, Python, Sphinx', [('', '#index-1')])], None,\n                                              ('ref2', [])]])\n", "def test_create_index_with_multi_type_index_entries(app):\n    text = (\".. index:: single: docutils\\n\"\n            \".. index:: single: Python\\n\"\n            \".. index:: pair: Python; interpreter\\n\"\n            \".. index:: pair: Sphinx; documentation tool\\n\"\n            \".. index:: triple: Python; Sphinx; reST\\n\"\n            \".. index:: see: Python; interpreter\\n\"\n            \".. index:: seealso: Sphinx; documentation tool\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 5\n    assert index[0] == ('D',\n                        [('docutils', [[('', '#index-0')], [], None],\n                                     [('see interpreter', [])])])\n    assert index[1] == ('I',\n                        [('interpreter', [[], [('Python', [('', '#index-1')])], None])])\n    assert index[2] == ('P',\n                        [('Python',\n                          [[], [('interpreter', [('', '#index-1')])],\n                               [('see documentation tool', [])]],\n                          None),\n                          [('Sphinx', [[], [('reST', [('', '#index-4')])], None])])\n    assert index[3] == ('R', [('reST', [[], [('Python Sphinx', [('', '#index-3')])], None)])\n    assert index[4] == ('S',\n                        [('documentation tool', [[], [('Sphinx', [('', '#index-2')])], None]),\n                         ('Sphinx', [[], [('reST', [('', '#index-4')])], None])])\n", "def test_create_index_single_with_subentry(app):\n    text = (\".. index:: single: foo\\n\"\n            \".. index:: single: bar baz\\n\")\n    restructuredtext.parse(app, text)\n    index = IndexEntries(app.env).create_index(app.builder)\n    assert len(index) == 2\n    assert index[0] == ('F', [('bar baz', [[], [], None],\n                              ('foo', [[], [], None])])\n    assert index[1] == ('S', [('foo', [[], [], None]]))\n"], "sample_52": ["    def test_alter_unique_together(self):\n        \"\"\"\n        Test the AlterUniqueTogether operation.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_alunto\", unique_together=True)\n        # Test the state alteration\n        operation = migrations.AlterUniqueTogether(\"Pony\", [(\"pink\", \"weight\")])\n        self.assertEqual(\n            operation.describe(), \"Alter unique_together for Pony (1 constraint(s))\"\n        )\n        self.assertEqual(\n            operation.migration_name_fragment,\n            \"alter_pony_unique_together\",\n        )\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_alunto\", new_state)\n        self.assertEqual(\n            len(\n                project_state.models[\"test_alunto\", \"pony\"].options.get(\n                    \"unique_together\", set()\n                )\n            ),\n            0,\n        )\n        self.assertEqual(\n            len(\n                new_state.models[\"test_alunto\", \"pony\"].options.get(\n                    \"unique_together\", set()\n                )\n            ),\n            1,\n        )\n        # Make sure we can insert duplicate rows\n        with connection.cursor() as cursor:\n            cursor.execute(\"INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)\")\n            cursor.execute(\"INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)\")\n            cursor.execute(\"DELETE FROM test_alunto_pony\")\n            # Test the database alteration\n            with connection.schema_editor() as editor:\n                operation.database_forwards(\n                    \"test_alunto\", editor, project_state, new_state\n                )\n            cursor.execute(\"INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)\")\n            with self.assertRaises(IntegrityError):\n                with atomic():\n                    cursor.execute(\n                        \"INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)\"\n                    )\n            cursor.execute(\"DELETE FROM test_alunto_pony\")\n            # And test reversal\n            with connection.schema_editor() as editor:\n                operation.database_backwards(\n                    \"test_alunto\", editor, new_state, project_state\n                )\n            cursor.execute(\"INSERT INTO test_alunto_pony (pink, weight) VALUES (1, 1)\")\n            cursor.execute(\"INSERT INTO test_alunto_pony (pink, weight)", "def test_rename_field_with_db_column_references(self):\n    \"\"\"\n    Tests the RenameField operation when renaming a field with db_column.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rfwdbc\", index=True)\n    operation = migrations.RenameField(\"Pony\", \"field\", \"renamed_field\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rfwdbc\", new_state)\n    self.assertIn(\"renamed_field\", new_state.models[\"test_rfwdbc\", \"pony\"].fields)\n    self.assertNotIn(\"field\", new_state.models[\"test_rfwdbc\", \"pony\"].fields)\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_field\")\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(0):\n            operation.database_forwards(\n                \"test_rfwdbc\", editor, project_state, new_state\n            )\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_field\")\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(0):\n            operation.database_backwards(\n                \"test_rfwdbc\", editor, new_state, project_state\n            )\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_field\")\n\n    new_state = project_state.clone()\n    operation = migrations.RenameField(\"Pony\", \"fk_field\", \"renamed_fk_field\")\n    operation.state_forwards(\"test_rfwdbc\", new_state)\n    self.assertIn(\n        \"renamed_fk_field\", new_state.models[\"test_rfwdbc\", \"pony\"].fields\n    )\n    self.assertNotIn(\"fk_field\", new_state.models[\"test_rfwdbc\", \"pony\"].fields)\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_fk_field\")\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(0):\n            operation.database_forwards(\n                \"test_rfwdbc\", editor, project_state, new_state\n            )\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_fk_field\")\n    with connection.schema_editor() as editor:\n        with self.assertNumQueries(0):\n            operation.database_backwards(\n                \"test_rfwdbc\", editor, new_state, project_state\n            )\n    self.assertColumnExists(\"test_rfwdbc_pony\", \"db_fk_field\")\n\n    # Test that the db_column references", "def test_rename_field_to_field_name_conflict(self):\n    \"\"\"\n    Tests the RenameField operation raises a ValueError when renaming a field\n    to a field name already present on the model.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_rnflconf\", fields=[(\"id\", models.AutoField(primary_key=True)), (\"field\", models.IntegerField())])\n    with self.assertRaisesMessage(ValueError, \"Pony has a field named 'field'\"):\n        migrations.RenameField(\"Pony\", \"field\", \"field\")\n", "def test_rename_field_preserve_db_column(self):\n    \"\"\"\n    Tests the RenameField operation's state alteration\n    when preserve_db_column = False.\n    \"\"\"\n    project_state = self.set_up_test_model(\n        \"test_rnflpd\", db_column=True\n    )\n    # Test the state alteration\n    operation = migrations.RenameField(\"Pony\", \"pink\", \"blue\")\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_rnflpd\", new_state)\n    self.assertIn(\"blue\", new_state.models[\"test_rnflpd\", \"pony\"].fields)\n    self.assertNotIn(\"pink\", new_state.models[\"test_rnflpd\", \"pony\"].fields)\n    # Test the database alteration\n    self.assertColumnExists(\"test_rnflpd_pony\", \"pink\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_rnflpd\", editor, project_state, new_state)\n    self.assertColumnNotExists(\"test_rnflpd_pony\", \"pink\")\n    self.assertColumnExists(\"test_rnflpd_pony\", \"blue\")\n    self.assertColumnNotExists(\"test_rnflpd_pony\", \"db_pink\")\n    # And test reversal\n    with connection.schema_editor() as editor:\n        operation.database_backwards(\n            \"test_rnflpd\", editor, new_state, project_state\n        )\n    self.assertColumnNotExists(\"test_rnflpd_pony\", \"blue\")\n    self.assertColumnExists(\"test_rnflpd_pony\", \"pink\")\n    self.assertColumnExists(\"test_rnflpd_pony\", \"db_pink\")\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"RenameField\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(\n        definition[2],\n        {\"model_name\": \"Pony\", \"old_name\": \"pink\", \"new_name\": \"blue\"},\n    )\n", "def test_alter_field_with_db_column_and_nullable(self):\n    \"\"\"\n    AlterField with a db_column argument should not change the nullable status\n    of the field.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_alflbdcbn\", related_model=True)\n    new_state = project_state.clone()\n    operation = migrations.AlterField(\n        \"Rider\",\n        \"pony\",\n        models.ForeignKey(\n            \"Pony\", models.CASCADE, db_column=\"pony_id\", null=True\n        ),\n    )\n    operation.state_forwards(\"test_alflbdcbn\", new_state)\n    self.assertIs(\n        new_state.models[\"test_alflbdcbn\", \"rider\"].fields[\"pony\"].null, True\n    )\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_alflbdcbn\", editor, project_state, new_state)\n    self.assertIs(\n        project_state.models[\"test_alflbdcbn\", \"rider\"].fields[\"pony\"].null, True\n    )\n    self.assertIs(\n        new_state.models[\"test_alflbdcbn\", \"rider\"].fields[\"pony\"].null, True\n    )\n"], "sample_69": ["def test_get_mouseover():\n    \"\"\"Test if get_mouseover returns True or False.\"\"\"\n    art = martist.Artist()\n    assert art.get_mouseover() is False\n    art.set_mouseover(True)\n    assert art.get_mouseover() is True\n    art.set_mouseover(False)\n    assert art.get_mouseover() is False\n", "def test_get_url():\n    art = martist.Artist()\n    assert art.get_url() is None\n    art.set_url('https://example.com')\n    assert art.get_url() == 'https://example.com'\n    art.set_url(None)\n    assert art.get_url() is None\n\n", "def test_sticky_edges():\n    \"\"\"Test sticky edges autoscaling.\"\"\"\n    ax = plt.subplots()[1]\n    line, = ax.plot([0, 0.5, 1])\n    line.set_sticky_edges(y=[0, 1])\n    line2, = ax.plot([0, 0.5, 1])\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 2)\n    ax.set_yticks([0, 1])\n    ax.set_yticks([0.5, 1.5])\n    assert np.allclose(line.sticky_edges.y, [0, 1])\n    assert np.allclose(line2.sticky_edges.y, [0, 1])\n", "def test_figure_property():\n    fig = martist.Figure()\n    assert fig.axes is None\n    fig.add_subplot(111)\n    assert fig.axes is not None\n    assert fig.stale\n\n    class TestArtist(martist.Artist):\n            super().__init__()\n            self.figure = fig\n\n    artist = TestArtist()\n    assert artist.figure is fig\n\n    artist.figure = martist.Figure()\n    assert artist.figure is not fig\n    assert artist.stale\n\n    with pytest.raises(RuntimeError, match='cannot remove artist'):\n        artist.remove()\n\n    artist.set_figure(fig)\n    assert artist.figure is fig\n    assert artist.stale\n\n    artist = martist.Artist()\n    artist.set_figure(fig)\n    artist.remove()\n    assert artist.figure is None\n", "def test_sticky_edges():\n    \"\"\"Test sticky_edges property.\"\"\"\n    fig, ax = plt.subplots()\n    ln, = ax.plot([1, 2, 3])\n\n    # Default sticky_edges values\n    assert ln.sticky_edges.x == []\n    assert ln.sticky_edges.y == []\n\n    # Set sticky_edges to a specific value\n    ln.sticky_edges.x = [1]\n    assert ln.sticky_edges.x == [1]\n    ln.sticky_edges.y = [2]\n    assert ln.sticky_edges.y == [2]\n\n    # Update sticky_edges value\n    ln.sticky_edges.x[0] = 10\n    assert ln.sticky_edges.x == [10]\n\n    # Test that sticky_edges does not change when setting properties\n    ln.set_linewidth(2)\n    assert ln.sticky_edges.x == [10]\n    assert ln.sticky_edges.y == [2]\n\n    # Test that sticky_edges is not mutable when using update_from\n    ln2 = mlines.Line2D([], [])\n    ln.update_from(ln2)\n    ln.sticky_edges.x.append(20)\n    assert ln.sticky_edges.x == [10]\n    assert ln.sticky_edges.y == [2]\n\n    # Test that sticky_edges is correctly updated when using update\n    ln.sticky_edges.x = [30]\n    ln.update({'sticky_edges': {'x': [10], 'y': [20]}})\n    assert ln.sticky_edges.x == [10]\n    assert ln.sticky_edges.y == [20]\n"], "sample_127": ["def test_latex_ConditionSet():\n    x, y = symbols('x, y')\n    assert latex(ConditionSet(x, Eq(x**2 + y, 1), S.Reals)) == \\\n        r\"\\left\\{x\\; |\\; x \\in \\mathbb{R} \\wedge x^{2} + y = 1 \\right\\}\"\n", "def test_latex_MinMax():\n    assert latex(Min(x, y, z)) == r\"\\min\\left(x, y, z\\right)\"\n    assert latex(Max(x, y, z)) == r\"\\max\\left(x, y, z\\right)\"\n    assert latex(Min(x, y, z)**2) == r\"\\min\\left(x, y, z\\right)^{2}\"\n    assert latex(Max(x, y, z)**2) == r\"\\max\\left(x, y, z\\right)^{2}\"\n", "def test_latex_Transpose():\n    from sympy import MatrixSymbol, Transpose\n    M = MatrixSymbol('M', 5, 5)\n    assert latex(Transpose(M)) == r\"\\left(M\\right)^T\"\n    M2 = MatrixSymbol('M', 1, 1)\n    assert latex(Transpose(M2)) == r\"M^{T}\"\n", "def test_latex_UnitInterval():\n    from sympy import Interval, oo\n    assert latex(Interval(0, 1)) == r\"\\left[0, 1\\right]\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True)) == r\"\\left(0, 1\\right)\"\n    assert latex(Interval(0, 1, left_open=True)) == r\"\\left(0, 1\\right]\"\n    assert latex(Interval(0, 1, right_open=True)) == r\"\\left[0, 1\\right)\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True, bounds='neither')) == r\"\\left(0, 1\\right)\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True, bounds='right')) == r\"\\left(0, 1\\right]\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True, bounds='left')) == r\"\\left[0, 1\\right)\"\n    assert latex(Interval(0, 1, left_open=True, right_open=True, bounds='both')) == r\"\\left(0, 1\\right)\"\n    assert latex(Interval(0, oo, right_open=True)) == r\"\\left[0, \\infty\\right)\"\n    assert latex(Interval(0, oo, right_open=False)) == r\"\\left[0, \\infty\\right]\"\n    assert latex(Interval(-oo, 0, right_open=True)) == r\"\\left(-\\infty, 0\\right]\"\n    assert latex(Interval(-oo, 0, right_open=False)) == r\"\\left(-\\infty, 0\\right]\"\n    assert latex(Interval(-oo, 0, right_open=True, left_open=True)) == r\"\\left(-\\infty, 0\\right)\"\n    assert latex(Interval(-oo, 0, right_open=False, left_open=True)) == r\"\\left(-\\infty, 0\\right]\"\n    assert latex(Interval(-oo, 0, right_open=True, left_open=False)) == r\"\\left(0, -\\infty\\right)\"\n    assert latex(Interval(-oo, 0, right_open=False, left_open=False)) == r\"\\left[0, -\\infty\\right]\"\n", "def test_latex_SingularityFunction_expansion():\n    from sympy import sin, cos\n    from sympy.abc import x\n    from sympy.tensor.tensor import TensorProduct, TensorHead\n\n    T = TensorHead('T')\n    u = TensorProduct(T, x, x)\n\n    f = SingularityFunction(u, 0, 2)\n\n    latex_str = r'{\\langle 0 \\rangle}^{2}'\n\n    assert latex(f) == latex_str\n    assert latex(u.diff(x)) == latex_str\n\n    f = SingularityFunction(u, 2, 1)\n    latex_str = r'{\\langle 2 \\rangle}^{1}'\n\n    assert latex(f) == latex_str\n    assert latex(u.diff(x, x)) == latex_str\n"], "sample_65": ["    def test_stringformat01(self):\n        output = self.engine.render_to_string(\"stringformat01\", {\"a\": \"Hello World!\"})\n        self.assertEqual(output, \"Hello World!\")\n", "    def test_date01(self):\n        output = self.engine.render_to_string(\"date01\", {\"date\": \"2022-07-25\"})\n        self.assertEqual(output, \"25 07 2022\")\n", "    def test_slugify01(self):\n        output = self.engine.render_to_string(\"slugify01\", {\"a\": \"Hello World!\"})\n        self.assertEqual(output, \"hello-world\")\n", "    def test_stringformat01(self):\n        self.assertEqual(stringformat(\"hello\", \"s\"), \"hello\")\n        self.assertEqual(stringformat(\"hello\", \"f\"), \"hello\")\n        self.assertEqual(stringformat(\"hello\", \"%s\"), \"hello\")\n        self.assertEqual(stringformat(\"hello\", \"%s, %s\"), \"hello, hello\")\n", "    def test_addslashes(self):\n        self.assertEqual(addslashes(\"abc'def\"), \"abc\\\\'def\")\n"], "sample_28": ["    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUp(self):\n        self.request_factory = RequestFactory()\n        self.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        self.request = self.request_factory.get(reverse('test_adminsite:index'))\n        self.request.user = self.u1\n", "    def setUp(self):\n        self.site = admin.AdminSite()\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.u2 = User.objects.create_user(username='user', password='password', email='user@example.com')\n        cls.article = Article.objects.create(title='test article', content='test content')\n", "    def setUpTestData(cls):\n        cls.u1 = User.objects.create_superuser(username='super', password='secret', email='super@example.com')\n        cls.article = Article.objects.create(title='Test', content='This is a test')\n"], "sample_89": ["def test_get_fslocation_from_item():\n    # Test when item has a location attribute\n    item = nodes.Item(\"test_name\")\n    item.location = (\"path/to/file.py\", 10, \"\")\n    expected = (\"path/to/file.py\", 10, \"\")\n    assert nodes.get_fslocation_from_item(item) == expected\n\n    # Test when item has a fspath attribute\n    item = nodes.Item(\"test_name\")\n    item.fspath = py.path.local(\"path/to/file.py\")\n    expected = (\"path/to/file.py\", None, \"\")\n    assert nodes.get_fslocation_from_item(item) == expected\n\n    # Test when item has an obj attribute\n    item = nodes.Item(\"test_name\")\n    item.obj = object()\n    expected = getfslineno(item.obj)\n    assert nodes.get_fslocation_from_item(item) == expected\n", "def test_repr_failure_py_fulltrace(testdir):\n    \"\"\"Ensure that repr_failure_py returns the full traceback with fulltrace option.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n            raise ValueError(\"test error\")\n    \"\"\"\n    )\n    result = testdir.runpytest(\"--fulltrace\")\n    result.stdout.fnmatch_lines([\"*ValueError: test error*\"])\n    result.stdout.fnmatch_lines([\"*test_show_full_trace*\"])\n\n", "def test_repr_failure_py_config_fulltrace(testdir):\n    \"\"\"Test the `repr_failure_py` method when the fulltrace option is on.\"\"\"\n    p = testdir.makepyfile(\n        \"\"\"\n            raise ValueError\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-v\", \"--fulltrace\")\n    result.stdout.fnmatch_lines(\n        [\n            \"*raise ValueError\",\n            \"*test_show_full_trace.*test_show_full_trace.py\",\n        ]\n    )\n\n", "def test_repr_failure_py_short_style_with_showlocals():\n    \"\"\"Test that repr_failure_py returns short style with showlocals.\"\"\"\n    class CustomException(Exception):\n        pass\n\n    excinfo = ExceptionInfo(CustomException())\n    excinfo.traceback = py.path.local(\"test_file.py\").relto(py.path.local(\".\"))\n    excinfo.value.pytrace = True\n    node = Node(\"test_function\", config=Config({\"showlocals\": True}))\n    result = node._repr_failure_py(excinfo, style=\"short\")\n    assert isinstance(result, str)\n    assert \"showlocals=True\" in result\n    assert \"short\" in result\n", "def test_iter_markers_with_node():\n    \"\"\"Ensure that iter_markers_with_node returns all nodes with the specified marker.\"\"\"\n    class TestNode(Node):\n            super().__init__(name, parent, **kwargs)\n            self.own_markers = [Mark(\"marker1\"), Mark(\"marker2\")]\n\n    item = TestNode(\"test_name\")\n    expected_nodes = [item, item.parent]\n    expected_markers = [(item, Mark(\"marker1\")), (item, Mark(\"marker2\"))]\n    nodes = list(item.iter_markers_with_node(name=\"marker1\"))\n    assert nodes == expected_nodes\n    assert all((n, m) in expected_markers for n, m in nodes)\n\n    # Test that it doesn't include nodes without the marker\n    item.parent.own_markers = []\n    nodes = list(item.iter_markers_with_node(name=\"marker1\"))\n    assert len(nodes) == 1\n\n    # Test that it doesn't include nodes from parents of parents\n    node = TestNode(\"parent_name\", parent=TestNode(\"grandparent_name\"))\n    node.own_markers = [Mark(\"marker1\")]\n    item.parent = node\n    nodes = list(item.iter_markers_with_node(name=\"marker1\"))\n    assert len(nodes) == 1\n\n    # Test that it raises an error if the node is None\n    item = None\n    with pytest.raises(TypeError):\n        list(item.iter_markers_with_node())\n"], "sample_80": ["def test_inline_dask_repr(self):\n    dask_array = np.array([1, 2, 3]).to_dask_array()\n    actual = formatting.inline_dask_repr(dask_array)\n    expected = f\"dask.array<chunksize=(3,)>\"\n    assert actual == expected\n", "def test_inline_dask_repr(self):\n    da = xr.DataArray(np.random.randn(10, 5))\n    da = da.chunk((2, 5))\n    expected = f\"dask.array<chunksize={da.chunks}>\" if sys.version_info >= (3, 8) else \"dask.array<chunksize=(2, 5)>\"\n    actual = formatting.inline_dask_repr(da)\n    assert expected == actual\n\n    da = xr.DataArray(np.random.randn(10, 5))\n    da = da.chunk((10, 5))\n    with raises_regex(ValueError, \"chunksize\"):\n        formatting.inline_dask_repr(da)\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    da_a = da.from_array(np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"))\n\n    expected = \"dask.array<chunksize=(2, 3)>\"\n    actual = formatting.inline_dask_repr(da_a)\n    assert expected == actual\n\n    expected = \"dask.array<chunksize=(3, 2)>\"\n    actual = formatting.inline_dask_repr(da_a.rechunk(chunks={\"x\": 3, \"y\": 2}))\n    assert expected == actual\n\n    da_b = da.from_array(np.array([[1], [2], [3]], dtype=\"int64\"))\n    expected = \"dask.array<chunksize=(2,)>\"\n    actual = formatting.inline_dask_repr(da_b)\n    assert expected == actual\n\n    da_c = da.from_array(np.array([[1, 2, 3], [4, 5, 6]], dtype=\"float64\"))\n    expected = \"dask.array<chunksize=(2, 3)>\"\n    actual = formatting.inline_dask_repr(da_c)\n    assert expected == actual\n\n    da_d = da.from_array(np.array([[1, 2, 3], [4, 5, 6]], dtype=\"int64\"), chunks=(2, 3))\n    expected = \"dask.array<chunksize=(2, 3)>\"\n    actual = formatting.inline_dask_repr(da_d)\n    assert expected == actual\n", "def test_inline_dask_repr():\n    import dask.array as da\n\n    array = da.from_array(np.arange(10), chunks=(5,))\n\n    expected = \"dask.array<chunksize=(5,)>\"\n    actual = formatting.inline_dask_repr(array)\n    assert expected == actual\n\n    array = da.from_array(np.arange(10), chunks=(3, 3, 1))\n    expected = \"dask.array<chunksize=(3, 3, 1)>\"\n    actual = formatting.inline_dask_repr(array)\n    assert expected == actual\n\n    class CustomArray:\n            self.value = value\n\n        @property\n            return (10,)\n\n    array = CustomArray(np.arange(10))\n    with pytest.raises(AssertionError):\n        formatting.inline_dask_repr(array)\n\n    array = da.from_array(np.arange(10), chunks=(5,))\n    array._meta = \"custom\"\n    with pytest.raises(AttributeError):\n        formatting.inline_dask_repr(array)\n\n    class CustomArray:\n            self.value = value\n\n            return \"CustomArray\"\n\n        @property\n            return (10,)\n\n    array = CustomArray(np.arange(10))\n    with pytest.raises(AttributeError):\n        formatting.inline_dask_repr(array)\n", "def test_inline_dask_repr():\n    from dask.array.core import from_array\n    from dask.array.core import zeros\n    import dask\n\n    da = from_array(np.random.randn(1000))\n    chunksize = (100, 100)\n    with dask.config.set(options={'array.chunk-size': '1MiB'}):\n        dask_array = da.rechunk(chunks=chunksize)\n\n    assert formatting.inline_dask_repr(dask_array) == f\"dask.array<chunksize={(100, 100)}>\"\n\n"], "sample_124": ["def test_acsch_series():\n    x = Symbol('x')\n    assert acsch(x).series(x, 0, 10) == \\\n       1/ x - x/6 + 7*x**3/360 - 31*x**5/15120 + 127*x**7/604800 \\\n          - 73*x**9/3421440 + O(x**10)\n", "def test_acsch_series():\n    x = Symbol('x')\n    assert acsch(x).series(x, 0, 10) == \\\n        1/x - x/6 + 7*x**3/360 - 31*x**5/15120 + 127*x**7/604800 + O(x**8)\n    t7 = acsch(x).taylor_term(7, x)\n    assert t7 == 127*x**7/604800\n    assert acsch(x).taylor_term(9, x, t7, 0) == -73*x**9/3421440\n", "def test_asech_csch_acosh():\n    x = Symbol('x')\n    assert asech(x).series(x, 0, 10) == \\\n        1/x - x**2/2 + 5*x**4/24 - 61*x**6/720 + 277*x**8/8064 + O(x**10)\n    t6 = asech(x).expansion_term(6, x)\n    assert t6 == -5*x**6/96\n    assert asech(x).expansion_term(8, x, t6, 0) == -35*x**8/1024\n", "def test_acsch_expansion():\n    x, y = symbols('x,y')\n    assert acsch(x+y).expand(trig=True) == acsch(x)*acsch(y) + acsch(x)*acoth(x)*acoth(y)\n    assert acsch(2*x).expand(trig=True) == (2*x)**(-1) + (2*x)**(-1)*acoth(2*x)\n    assert acsch(3*x).expand(trig=True).expand() == \\\n        3*x**(-2) + 3*x**(-2)*acoth(3*x)\n", "def test_acsch_expansion():\n    x, y = symbols('x,y')\n    assert acsch(x+y).expand(trig=True) == acsch(x)*acsch(y) - acsch(x)*csch(y)\n    assert acsch(2*x).expand(trig=True) == -csch(x**2)/sinh(x**2)\n    assert acsch(3*x).expand(trig=True).expand() == \\\n        -csch(x**3)/sinh(x**3) + 2*csch(x)/sinh(x)\n    assert acsch(2*x).as_real_imag(deep=True) == \\\n        (-csch(x**2)/sinh(x**2), 0)\n"], "sample_64": ["    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should pass prepopulated fields.\n        \"\"\"\n        request = self.request_factory.get(\n            reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n        )\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        admin.prepopulated_fields = (\n            [(\"username\", \"email\"), (\"first_name\", \"last_name\")]\n        )\n        extra_context = {\"extra\": True}\n        response = admin.change_view(\n            request, str(self.superuser.pk), extra_context=extra_context\n        )\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertEqual(len(template_context[\"prepopulated_fields\"]), 2)\n        self.assertEqual(\n            json.loads(template_context[\"prepopulated_fields_json\"]),\n            [\n                {\n                    \"id\": \"#id_username\",\n                    \"name\": \"username\",\n                    \"dependency_ids\": [],\n                    \"dependency_list\": [],\n                    \"maxLength\": 254,\n                    \"allowUnicode\": False,\n                },\n                {\n                    \"id\": \"#id_email\",\n                    \"name\": \"email\",\n                    \"dependency_ids\": [],\n                    \"dependency_list\": [],\n                    \"maxLength\": 254,\n                    \"allowUnicode\": False,\n                },\n                {\n                    \"id\": \"#id_first_name\",\n                    \"name\": \"first_name\",\n                    \"dependency_ids\": [],\n                    \"dependency_list\": [],\n                    \"maxLength\": 150,\n                    \"allowUnicode\": True,\n                },\n                {\n                    \"id\": \"#id_last_name\",\n                    \"name\": \"last_name\",\n                    \"dependency_ids\": [],\n                    \"dependency_list\": [],\n                    \"maxLength\": 150,\n                    \"allowUnicode\": True,\n                },\n            ],\n        )\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should pass whole context and correct JSON data.\n        \"\"\"\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        extra_context = {\"extra\": True}\n        response = admin.change_view(\n            request, str(article.pk), extra_context=extra_context\n        )\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIs(template_context[\"extra\"], True)\n        self.assertIsInstance(template_context[\"prepopulated_fields\"], list)\n        self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n        self.assertIn('id', json.loads(template_context[\"prepopulated_fields_json\"][0]))\n", "    def setUpTestData(cls):\n        cls.superuser = User.objects.create_superuser(\n            username=\"super\", password=\"secret\", email=\"super@example.com\"\n        )\n        cls.model_admin = ModelAdmin(Article, site)\n        cls.model_admin.prepopulated_fields = (\n            (\"title\", \"slug\"),\n            (\"content\",),\n        )\n        cls.model_admin.save_as = True\n", "    def test_prepopulated_fields_js(self):\n        \"\"\"\n        prepopulated_fields_js template tag should pass the correct prepopulated fields to the template.\n        \"\"\"\n        request = self.request_factory.get(\n            reverse(\"admin:auth_user_change\", args=[self.superuser.pk])\n        )\n        request.user = self.superuser\n        admin = UserAdmin(User, site)\n        extra_context = {}\n        response = admin.change_view(request, str(self.superuser.pk), extra_context=extra_context)\n        template_context = prepopulated_fields_js(response.context_data)\n        expected_prepopulated_fields_json = [\n            {\n                \"id\": \"#id_username\",\n                \"name\": \"username\",\n                \"dependency_ids\": [\"#id_email\", \"#id_password\", \"#id_password1\", \"#id_password2\"],\n                \"dependency_list\": [\"email\", \"password\", \"password1\", \"password2\"],\n                \"maxLength\": 150,\n                \"allowUnicode\": False,\n            }\n        ]\n        self.assertEqual(json.loads(template_context[\"prepopulated_fields_json\"]), expected_prepopulated_fields_json)\n", "    def test_prepopulated_fields_js(self):\n        article = Article.objects.all()[0]\n        request = self.request_factory.get(\n            reverse(\"admin:admin_views_article_change\", args=[article.pk])\n        )\n        request.user = self.superuser\n        admin = ArticleAdmin(Article, site)\n        response = admin.change_view(request, str(article.pk))\n        template_context = prepopulated_fields_js(response.context_data)\n        self.assertIsInstance(template_context[\"prepopulated_fields\"], list)\n        self.assertIsInstance(template_context[\"prepopulated_fields_json\"], str)\n        self.assertContains(response, 'id=\"id_title\"')\n"], "sample_15": ["    def test_inconsistent_language_settings_bidi(self):\n        msg = 'You have provided a value for the LANGUAGE_CODE setting that is not in the LANGUAGES_BIDI setting.'\n        for tag in ['en', 'en-US', 'en-357', 'fr-CA', 'fr', 'fr-357']:\n            with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n                self.assertEqual(check_language_settings_consistent(None), [\n                    Error(msg, id='translation.E004'),\n                ])\n", "def test_inconsistent_language_settings_bidi(self):\n    \"\"\"Test that check_language_settings_consistent raises error when LANGUAGE_CODE is set\n    to a language code that is in LANGUAGES_BIDI but not in LANGUAGES.\"\"\"\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is not in '\n        'the LANGUAGES setting.'\n    )\n    tests = [\n        'en', 'en-CA', 'en-357'\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag, LANGUAGES_BIDI=['en']):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n", "def test_valid_language_settings_consistent_with_multiple_languages(self):\n    tests = [\n        # language + script.\n        'zh-Hans',\n        # language + region + variant.\n        'ca-ES-valencia',\n    ]\n    for tag in tests:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_valid_language_code_in_languages(self):\n    self.assertEqual(check_language_settings_consistent(None), [])\n", "def test_inconsistent_language_settings(self):\n    msg = (\n        'You have provided a value for the LANGUAGE_CODE setting that is '\n        'not in the LANGUAGES_BIDI setting.'\n    )\n    for tag in ['fr', 'fr-CA', 'fr-357', 'en_US']:\n        with self.subTest(tag), self.settings(LANGUAGE_CODE=tag):\n            self.assertEqual(check_language_settings_consistent(None), [\n                Error(msg, id='translation.E004'),\n            ])\n"], "sample_2": ["def test_more_sip_broken():\n    \"\"\"\n    Test that WCS can handle a broken SIP header\n    \"\"\"\n    hdr = get_pkg_data_contents(\"data/sip-broken2.hdr\")\n\n    w = wcs.WCS(hdr)\n", "def test_all_world2pix_false_maxiter():\n    \"\"\"Test for #2288\"\"\"\n    header = get_pkg_data_contents('data/j94f05bgq_flt.fits', encoding='binary')\n    w = wcs.WCS(header)\n    maxiter = 1\n\n    with pytest.raises(wcs.wcs.NoConvergence):\n        w.all_world2pix(maxiter=maxiter)\n", "def test_sip_with_det2im():\n    \"\"\"\n    Test that when creating a WCS object with SIP distortion and\n    detector to image plane correction, WCS.all_world2pix will work\n    without errors.\n    \"\"\"\n    header = get_pkg_data_contents('data/sip-d2im.hdr', encoding='binary')\n    w = wcs.WCS(header)\n    with NumpyRNGContext(123456789):\n        data = np.random.rand(100, 2)\n        try:\n            all_pix = w.all_world2pix(data, origin=1)\n        except wcs.wcs.NoConvergence as e:\n            print(e)\n            raise\n    assert all_pix.shape == (100, 2)\n", "def test_sip_calculated_from_header():\n    \"\"\"\n    This test checks that the header values are correctly read from a\n    FITS file with a SIP distortion, and that the WCS is correctly\n    initialized.\n    \"\"\"\n    # Create a test FITS file with a SIP distortion\n    with fits.open(get_pkg_data_filename('data/sip.fits')) as f:\n        header = f[0].header\n        # Make a copy of the header\n        header_copy = fits.Header(header)\n        # Delete the CRPIX keywords to check that the WCS can be read\n        # from the header even without them\n        del header_copy['CRPIX1']\n        del header_copy['CRPIX2']\n\n    w = wcs.WCS(header_copy)\n    # Check that the SIP distortion has been read from the header\n    assert isinstance(w.sip, wcs.Sip)\n    # Check that the CRPIX values have been calculated from the\n    # header\n    assert_array_almost_equal(w.sip.crpix, [2048, 1024])\n\n", "def test_p2s_s2p_singularity():\n    \"\"\"\n    Test for #4597\n    \"\"\"\n    # WCS with 2D, 3x3 matrix\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cd = [[0.5, 0.5], [0.5, 0.5]]\n    w.wcs.crval = [0, 0]\n    w.wcs.crpix = [1, 1]\n    w.wcs.cunit = [\"deg\", \"deg\"]\n    w.wcs.cdelt = [1, 1]\n    w.wcs.wcslib_version = \"wcslib-5.18.0\"\n    # Invertible matrix\n    w.wcs.set()\n    assert np.linalg.det(w.wcs.cd) != 0\n    w2 = wcs.WCS(w.to_header())\n    w2.wcs.set()\n    assert np.linalg.det(w2.wcs.cd) != 0\n\n    # WCS with 2D, singular matrix\n    w = wcs.WCS(naxis=2)\n    w.wcs.ctype = [\"RA---TAN\", \"DEC--TAN\"]\n    w.wcs.cd = [[0.5, 0.5], [0.5, 0]]\n    w.wcs.crval = [0, 0]\n    w.wcs.crpix = [1, 1]\n    w.wcs.cunit = [\"deg\", \"deg\"]\n    w.wcs.cdelt = [1, 1]\n    w.wcs.wcslib_version = \"wcslib-5.18.0\"\n    # Singular matrix\n    with pytest.raises(wcs.SingularMatrixError):\n        w.wcs.set()\n    # Check that WCS not set still produces a SingularMatrixError\n    with pytest.raises(wcs.SingularMatrixError):\n        w.wcs.p2s([[1, 2]], 0)\n    # Check that WCS with SingularMatrixError produces a SingularMatrixError\n    with pytest.raises(wcs.SingularMatrixError):\n        wcs.WCS(w.to_header()).wcs.set()\n    # Check that WCS with SingularMatrixError produces a SingularMatrixError\n    # in pix2world\n    with pytest.raises(wcs.SingularMatrixError):\n        wcs.WCS(w.to_header()).wcs.pix2s([[1"], "sample_41": ["    def test_iterator_retains_errors(self):\n        \"\"\"Iterator retains errors.\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '0',\n            'choices-MIN_NUM_FORMS': '0',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',\n        }\n        ChoiceFormSet = formset_factory(Choice)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertFalse(formset.is_valid())\n        self.assertEqual(formset.errors, [{'votes': ['This field is required.']}])\n        forms = list(formset)\n        self.assertEqual(formset.errors, [{'votes': ['This field is required.']}])\n        self.assertEqual(forms[0].errors, [{'votes': ['This field is required.']}])\n        self.assertEqual(forms[1].errors, [])\n", "    def test_custom_error_messages(self):\n        formset = FavoriteDrinksFormSet({}, prefix='drinks', error_messages={\n            'missing_management_form': 'Custom error message',\n        })\n        self.assertEqual(formset.non_form_errors(), ['Custom error message'])\n", "    def test_validate_min_counts_forms(self):\n        \"\"\"Formset validation shouldn't consider unchanged forms with initial data as \"empty\".\"\"\"\n        data = {\n            'choices-TOTAL_FORMS': '2',\n            'choices-INITIAL_FORMS': '2',\n            'choices-MIN_NUM_FORMS': '1',\n            'choices-MAX_NUM_FORMS': '2',\n            'choices-0-choice': 'Zero',\n            'choices-0-votes': '0',\n            'choices-0-ORDER': '1',\n            'choices-1-choice': 'One',\n            'choices-1-votes': '1',  # changed from initial\n            'choices-1-ORDER': '2',\n        }\n        ChoiceFormSet = formset_factory(Choice, min_num=2, validate_min=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix='choices')\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.forms[0].has_changed(), False)\n        self.assertTrue(formset.forms[1].has_changed())\n", "    def make_choiceformset(\n            self, formset_data=None, formset_class=ChoiceFormSet,\n            total_forms=None, initial_forms=0, max_num_forms=0, min_num_forms=0, **kwargs):\n        \"\"\"\n        Make a ChoiceFormset from the given formset_data.\n        The data should be given as a list of (choice, votes) tuples.\n        \"\"\"\n        kwargs.setdefault('prefix', 'choices')\n        kwargs.setdefault('auto_id', False)\n\n        if formset_data is None:\n            return formset_class(**kwargs)\n\n        if total_forms is None:\n            total_forms = len(formset_data)\n\n            args = (kwargs['prefix'],) + args\n            return '-'.join(args)\n\n        data = {\n            prefixed('TOTAL_FORMS'): str(total_forms),\n            prefixed('INITIAL_FORMS'): str(initial_forms),\n            prefixed('MAX_NUM_FORMS'): str(max_num_forms),\n            prefixed('MIN_NUM_FORMS'): str(min_num_forms),\n        }\n        for i, (choice, votes) in enumerate(formset_data):\n            data[prefixed(str(i), 'choice')] = choice\n            data[prefixed(str(i), 'votes')] = votes\n\n        return formset_class(data, **kwargs)\n", "    def test_absolute_max_factory(self):\n        # absolute_max has no effect when max_num is provided\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, max_num=2, absolute_max=3000)\n        formset = LimitedFavoriteDrinkFormSet()\n        self.assertEqual(len(formset.forms), 2)\n        \n        # absolute_max has no effect when min_num is provided\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, min_num=2, absolute_max=3000)\n        formset = LimitedFavoriteDrinkFormSet()\n        self.assertEqual(len(formset.forms), 2)\n        \n        # absolute_max overrides max_num when max_num is not provided\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, absolute_max=3)\n        formset = LimitedFavoriteDrinkFormSet()\n        self.assertEqual(len(formset.forms), 3)\n        \n        # absolute_max validation is done in is_valid\n        data = {\n            'form-TOTAL_FORMS': '2001',\n            'form-INITIAL_FORMS': '0',\n            'form-MAX_NUM_FORMS': '0',\n        }\n        formset = LimitedFavoriteDrinkFormSet(data=data, absolute_max=1000)\n        self.assertIs(formset.is_valid(), False)\n        \n        # absolute_max can be set to a lower value than max_num\n        LimitedFavoriteDrinkFormSet = formset_factory(FavoriteDrinkForm, max_num=1000, absolute_max=800)\n        formset = LimitedFavoriteDrinkFormSet()\n        self.assertEqual(len(formset.forms), 800)\n"], "sample_132": ["def test_are_coplanar():\n    from sympy.geometry import Point, Point3D, Line3D, Plane\n    from sympy.geometry.util import are_coplanar\n\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert are_coplanar(a, b, c) == False\n    assert are_coplanar(a, Plane((0, 0, 0), (1, -1, 1), (0, 1, 0))) == False\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0)) == True\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 1)) == False\n\n    # 2D objects\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(0, 1)) == True\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(0, 1, 0)) == True\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(0, 1, 0), Point(0, 1, 1)) == False\n\n    # Non-coplanar entities with more than 3D\n    assert are_coplanar(Point(0, 0, 0), Point(1, 0, 0), Point(0, 1, 0), Point(0, 0, 1), Point(1, 1, 1)) == False\n", "def test_intersection_list():\n    # test with a list of lists\n    circle = Circle((0, 0), 1)\n    points = [Point(0, 0), Point(1, 0), Point(2, 0), Point(3, 0)]\n    assert intersection(points, pairwise=True) == [Point(0, 0), Segment(Point(0, 0), Point(1, 0))]\n\n    # test with a single argument\n    assert intersection([Point(0, 0), Point(1, 0)]) == []\n\n    # test with a non-geometry entity\n    raises(TypeError, lambda: intersection([Point(0, 0), 3]))\n\n    # test with non-Point arguments\n    ray = Ray((0, 0), (1, 0))\n    assert intersection([ray, ray]) == [Segment(Point(0, 0), Point(1, 0))]\n", "def test_are_coplanar():\n    from sympy.geometry import Point, Line, Plane, Point3D\n    from sympy.geometry.util import are_coplanar\n    a = Point(5, 0, 0)\n    b = Point(1, -1, 1)\n    c = Point(0, -1, 0)\n    d = Point(5, -1, 9)\n    assert are_coplanar(a, b, c, d)\n    e = Point(0, 0, 0)\n    f = Point(0, 0, 1)\n    assert not are_coplanar(e, b, c, d)\n    # Test with 2D objects\n    g = Line(0, 0, 1, 1)\n    h = Line(0, 0, 1, -1)\n    i = Line(1, 0, 0, 1)\n    assert are_coplanar(g, h, i)\n    # Test with non-coplanar 2D objects\n    j = Line(0, 0, 1, 2)\n    assert not are_coplanar(g, h, j)\n    # Test with 3D points and a plane\n    k = Plane(0, 0, 0, 0, 0, 1)\n    assert are_coplanar(a, b, k)\n    l = Plane(0, 0, 0, 1, 0, 0)\n    assert not are_coplanar(a, b, l)\n", "def test_are_coplanar():\n    from sympy.geometry import Point, Point3D, Line3D, Plane, are_coplanar\n    # The following test cases were provided in the docstring\n    a = Line3D(Point3D(5, 0, 0), Point3D(1, -1, 1))\n    b = Line3D(Point3D(0, -2, 0), Point3D(3, 1, 1))\n    c = Line3D(Point3D(0, -1, 0), Point3D(5, -1, 9))\n    assert not are_coplanar(a, b, c)\n\n    # Test points\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 0))\n    assert are_coplanar(Point3D(0, 0, 0), Point3D(1, 0, 0), Point3D(0, 1, 1))\n\n    # Test 2D points\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(0, 1))\n    assert are_coplanar(Point(0, 0), Point(1, 0), Point(0, 2))\n\n    # Test various combinations of entities\n    assert are_coplanar(Point3D(0, 0, 0), Line3D(Point3D(1, 0, 0), Point3D(0, 1, 0)))\n    assert are_coplanar(Point(0, 0), Line(Point(1, 0), Point(0, 1)))\n    assert are_coplanar(Plane(0, 0, 0), Point3D(1, 0, 0))\n    assert not are_coplanar(Plane(0, 0, 0), Line3D(Point3D(1, 0, 0), Point3D(0, 1, 0)))\n\n    # Test cases with collinear points\n    p = [Point(0, 0), Point(1, 0), Point(2, 0)]\n    assert are_coplanar(*p)\n    p = [Point3D(", "def test_are_similar():\n    from sympy.geometry import Point, Point2D, Line, Circle, Triangle, are_similar\n    from sympy.geometry.util import are_coplanar\n    # check the basics\n    t = Triangle(Point2D(0, 0), Point2D(0, 2), Point2D(2, 2))\n    c = Circle(Point2D(1, 1), 1)\n    assert not are_similar(t, c)\n    assert are_similar(t, Triangle(Point2D(0, 0), Point2D(0, 2), Point2D(2, 2)))\n    # scaling\n    c2 = c.scale(2)\n    assert are_similar(t, c2)\n    # non-similar triangles\n    assert not are_similar(Triangle(Point2D(0, 0), Point2D(1, 0), Point2D(1, 1)), Triangle(Point2D(0, 0), Point2D(0, 2), Point2D(2, 2)))\n    # collinear\n    l1 = Line(Point2D(0, 0), Point2D(1, 0))\n    l2 = Line(Point2D(0, 0), Point2D(2, 0))\n    assert are_similar(l1, l2)\n    # same entity\n    assert are_similar(c, c)\n    # check that are_coplanar is called correctly\n    assert are_coplanar(l1, l2) == are_similar(l1, l2)\n    assert are_coplanar(c, Triangle(Point2D(0, 0), Point2D(0, 2), Point2D(2, 2)))\n    # non-similar coplanar entities\n    l3 = Line(Point2D(0, 0), Point2D(1, 1))\n    assert not are_similar(l1, l3)\n"], "sample_152": ["def test_array_transpose():\n    for array_type in array_types:\n        A = array_type([[1, 2], [3, 4]])\n        A_t = A.transpose()\n        assert A_t.shape == (2, 2)\n        assert A_t[0, 0] == 1\n        assert A_t[0, 1] == 3\n        assert A_t[1, 0] == 2\n        assert A_t[1, 1] == 4\n        assert A_t == Array([[1, 3], [2, 4]])\n", "def test_issue_20766():\n    for array_type in array_types:\n        A = array_type([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        assert A.shape == (2, 5)\n        assert A.rank() == 2\n        assert len(A) == 10\n        assert A[0, 0] == 1\n        assert A[0, 1] == 2\n        assert A[0, 2] == 3\n        assert A[0, 3] == 4\n        assert A[0, 4] == 5\n        assert A[1, 0] == 6\n        assert A[1, 1] == 7\n        assert A[1, 2] == 8\n        assert A[1, 3] == 9\n        assert A[1, 4] == 10\n\n        A[0, 0] = 0\n        assert A[0, 0] == 0\n        A[0, 0] = \"a\"\n        raises(TypeError, lambda: A[0, 0])\n", "def test_array_assignment_negative_indices():\n    for ArrayType in array_types:\n        test_array = ArrayType([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n        test_array[:, -1] = Array([10, 20])\n        assert test_array == Array([[1, 2, 3, 4, 10], [6, 7, 8, 9, 20]])\n\n        test_array[:, -2] = Array([4, 9])\n        assert test_array == Array([[1, 2, 3, 4, 10], [6, 7, 8, 9, 20]])\n\n        test_array[:, 0] = Array([1, 6])\n        assert test_array == Array([[1, 2, 3, 4, 10], [6, 7, 8, 9, 20]])\n\n        raises(ValueError, lambda: test_array[:, -6] = Array([10, 20]))\n        raises(ValueError, lambda: test_array[-3, :] = Array([10, 20]))\n        raises(ValueError, lambda: test_array[:, 6] = Array([10, 20]))\n        raises(ValueError, lambda: test_array[-1, -1] = Array([10, 20]))\n\n        test_array[:, -1] = Array([5, 10])\n        assert test_array == Array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n\n        test_array[:, 0] = Array([1, 6])\n        assert test_array == Array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n", "def test_issue_18673():\n    A = ImmutableDenseNDimArray([[1, 2, 3], [4, 5, 6]])\n    B = ImmutableDenseNDimArray([[7, 8, 9], [10, 11, 12]])\n    assert (A + B).shape == (2, 3)\n    assert (A + B).tolist() == [[8, 10, 12], [14, 16, 18]]\n    A[1, 1] = 0\n    assert A.tolist() == [[1, 2, 3], [4, 0, 6]]\n    assert A.shape == (2, 3)\n    A[:, 0] = 10\n    assert A.tolist() == [[10, 2, 3], [10, 0, 6]]\n    A[:, 1] = 20\n    assert A.tolist() == [[10, 20, 3], [10, 20, 6]]\n    assert A[:, 2] = 30, \"Assignment of slice not working\"\n    assert A.tolist() == [[10, 20, 30], [10, 20, 30]]\n\n    A[:] = [[40, 50, 60], [70, 80, 90]]\n    assert A.tolist() == [[40, 50, 60], [70, 80, 90]]\n\n    raises(ValueError, lambda: A[0, 1, 2])\n\n    B = ImmutableDenseNDimArray([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n    A[0, -1] = 20\n    assert B[0, -1] == 5\n    assert A[0, -1] == 20\n", "def test_issue_13533():\n    # test that applying a function to a single element of an array creates a new instance\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    for func in [lambda x: x**2, lambda x: x + 1, sympify(lambda x: x**2 + 1)]:\n        B = A.applyfunc(func)\n        assert not B.is_MutableDenseNDimArray\n        assert isinstance(B, ImmutableDenseNDimArray)\n\n    # test that applying a function that returns a scalar to an array creates a new instance\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    for func in [lambda: 5, lambda: 3.14, sympify(lambda: 2.71)]:\n        B = A.applyfunc(func)\n        assert B.shape == (2, 2)\n        assert all(i == func() for i in B.aslist())\n\n    # test that applying a function to a non-array and a scalar creates a new instance\n    A = ImmutableDenseNDimArray([[1, 2], [3, 4]])\n    for func in [lambda x: x**2, lambda x: x + 1, sympify(lambda x: x**2 + 1)]:\n        B = func(A) * A\n        assert isinstance(B, ImmutableDenseNDimArray)\n"], "sample_51": ["def test_directory_index_template_context(self):\n    \"\"\"Ensure the directory index template gets the correct context\"\"\"\n    file_list = [\"file1.txt\", \"file2.txt\"]\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertEqual(response.context[\"file_list\"], file_list)\n    self.assertEqual(response.context[\"directory\"], \"/%s/\" % self.prefix)\n", "def test_directory_index_template_error(self):\n    \"\"\"Test that a TemplateDoesNotExist is raised when the default template is missing.\"\"\"\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertIsInstance(response, Http404)\n    self.assertEqual(response.status_code, 404)\n", "def test_directory_index_template_translatable(self):\n    \"\"\"Template translatable variables in the directory index template are translated\"\"\"\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertEqual(response.status_code, 200)\n    # The 'Index of' string is translated\n    self.assertIn(self.prefix + \"/\", response.content)\n    # The 'directory' variable is translated\n    self.assertIn(self.prefix + \"/\", response.content)\n    # The 'file_list' variable is not translated\n    self.assertIn(\"visible\", response.content)\n    # The 'file_list' variable is not translated, it's a list of strings\n    self.assertNotIn(_(\"list\"), response.content)\n", "def test_directory_index_http_error(self):\n    \"\"\"\n    The directory index view should return 403 when accessing a directory\n    without the show_indexes parameter\n    \"\"\"\n    response = self.client.get(\"/%s/\" % self.prefix)\n    self.assertEqual(403, response.status_code)\n", "    def test_directory_index_no_show_indexes(self):\n        \"\"\"Test that directory index raises 404 when show_indexes=False\"\"\"\n        response = self.client.get(\"/%s/\" % self.prefix)\n        self.assertEqual(404, response.status_code)\n"], "sample_134": ["def test_matrix_power():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n    f = lambdify((M,), M**2, 'numpy')\n    ma = np.matrix([[1, 2], [3, 4]])\n    assert (f(ma) == np.linalg.matrix_power(ma, 2)).all()\n", "def test_issue_17006_2():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 2, 2)\n\n    f = lambdify(M, M.T + Identity(2))\n    ma = np.array([[1, 2], [3, 4]])\n    mr = np.array([[1, 3], [2, 4]])\n\n    assert (f(ma) == mr).all()\n\n    from sympy import symbols\n    n = symbols('n', integer=True)\n    N = MatrixSymbol(\"N\", n, n)\n    raises(NotImplementedError, lambda: lambdify(N, N.T + Identity(n)))\n", "def test_block_matrix():\n    if not np:\n        skip(\"NumPy not installed\")\n\n    a_1 = MatrixSymbol('a_1', 10, 3)\n    a_2 = MatrixSymbol('a_2', 10, 3)\n    a_3 = MatrixSymbol('a_3', 10, 3)\n    a_4 = MatrixSymbol('a_4', 10, 3)\n    A = BlockMatrix([[a_1, a_2], [a_3, a_4]])\n    assert A.shape == (20, 6)\n\n    B = BlockMatrix([[a_1, a_2], [a_3, a_4]], [a_1, a_3])\n    assert B.shape == (20, 6)\n\n    printer = NumPyPrinter()\n    assert printer.doprint(A) == 'numpy.block([[a_1, a_2], [a_3, a_4]])'\n\n    assert printer.doprint(B) == 'numpy.block([[a_1, a_2], [a_3, a_4]])'\n", "def test_sign():\n    \"\"\"\n    NumPyPrinter needs to print sign() correctly.\n    \"\"\"\n    if not np:\n        skip(\"NumPy not installed\")\n\n    f = lambdify((a,), cbrt(a), 'numpy')\n    assert abs(f(-1)) < 1e-16\n\n    f = lambdify((a,), sign(a), 'numpy')\n    assert np.array_equal(f([-1, 0, 1]), [-1, 0, 1])\n\n    e = cbrt(-a)\n    f = lambdify((a,), e, 'numpy')\n    assert abs(f(-1)) < 1e-16\n", "def test_CodgenArrayDiagonal_regression():\n    \"\"\"\n    CodegenArrayDiagonal should print with diagonal_indices\n    with a shape of (2, 3) as a 3x3 matrix.\n    \"\"\"\n    if not np:\n        skip(\"NumPy not installed\")\n\n    M = MatrixSymbol(\"M\", 3, 3)\n    cg = CodegenArrayDiagonal(CodegenArrayTensorProduct(M, M), (1, 2))\n    f = lambdify((M,), cg, 'numpy')\n    ma = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    assert (f(ma).all() == np.allclose(np.diag(ma))).all()\n"], "sample_55": ["def test_required_argument(self):\n    \"\"\"\n    Test that a command raises a CommandError when a required argument\n    is missing.\n    \"\"\"\n    msg = (\n        \"Error: one of the arguments --foo-id --foo-name --foo-list is \"\n        \"required\"\n    )\n    with self.assertRaisesMessage(CommandError, msg):\n        management.call_command(\"mutually_exclusive_required\")\n", "def test_handle_default_options(self):\n    \"\"\"\n    handle_default_options should modify sys.path and os.environ['DJANGO_SETTINGS_MODULE']\n    \"\"\"\n    original_sys_path = list(sys.path)\n    original_settings_module = os.environ.get(\"DJANGO_SETTINGS_MODULE\")\n    management.call_command(\"dance\", settings=\"settings\")\n    self.assertEqual(sys.path[0], \"settings\")\n    self.assertEqual(os.environ[\"DJANGO_SETTINGS_MODULE\"], \"settings\")\n    sys.path[:] = original_sys_path\n    del os.environ[\"DJANGO_SETTINGS_MODULE\"]\n    if original_settings_module:\n        os.environ[\"DJANGO_SETTINGS_MODULE\"] = original_settings_module\n", "    def test_handle_default_options(self):\n        \"\"\"\n        handle_default_options() does not fail when not given options\n        \"\"\"\n        out = StringIO()\n        management.call_command(\"dance\", stdout=out)\n        self.assertNotIn(\"DJANGO_SETTINGS_MODULE\", out.getvalue())\n", "def test_check_migrations_options(self):\n    \"\"\"\n    The check_migrations method should handle database options.\n    \"\"\"\n    requires_migrations_checks = dance.Command.requires_migrations_checks\n    self.assertIs(requires_migrations_checks, False)\n    try:\n        with mock.patch.object(BaseCommand, \"check_migrations\") as check_migrations:\n            management.call_command(\"dance\", verbosity=0)\n            self.assertFalse(check_migrations.called)\n            dance.Command.requires_migrations_checks = True\n            management.call_command(\"dance\", verbosity=0, database=\"other_db\")\n            self.assertTrue(check_migrations.called)\n    finally:\n        dance.Command.requires_migrations_checks = requires_migrations_checks\n    try:\n        with mock.patch.object(BaseCommand, \"check_migrations\") as check_migrations:\n            management.call_command(\"dance\", verbosity=0, database=\"other_db\")\n            self.assertFalse(check_migrations.called)\n            management.call_command(\"dance\", verbosity=0, skip_checks=True, database=\"other_db\")\n            self.assertFalse(check_migrations.called)\n    finally:\n        dance.Command.requires_migrations_checks = requires_migrations_checks\n", "def test_create_parser_defaults(self):\n    \"\"\"\n    The create_parser method should include any default options that all commands should accept\n    \"\"\"\n    class TestCommand(BaseCommand):\n            pass\n\n    parser = TestCommand().create_parser(\"prog_name\", \"subcommand\")\n    self.assertIn(\"--version\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--verbosity\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--settings\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--pythonpath\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--traceback\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--no-color\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--force-color\", [arg.dest for arg in parser._option_string_actions])\n    self.assertIn(\"--skip-checks\", [arg.dest for arg in parser._option_string_actions])\n"], "sample_49": ["    def test_template_changed_from_different_directory(self, mock_file_changed):\n        template_path = Path(__file__).parent / 'templates' / 'index.html'\n        mock_file_changed.return_value = True\n        self.assertTrue(autoreload.template_changed(None, template_path))\n        mock_file_changed.assert_called_once_with(None, template_path)\n", "    def test_watch_for_template_changes_with_invalid_directory(self):\n        mock_reloader = mock.MagicMock()\n        autoreload.watch_for_template_changes(mock_reloader)\n        self.assertEqual(mock_reloader.watch_dir.call_count, 0)\n", "def test_template_dirs_with_django_contrib(self):\n    self.assertSetEqual(\n        autoreload.get_template_directories(),\n        {\n            Path('path/to/another/absolute_str'),\n            Path.cwd() / 'relative_str',\n            Path.cwd() / 'another/relative_path',\n            Path.cwd() / 'django' / 'contrib' / 'admin'\n        }\n    )\n", "    def test_template_change_detection_in_absolute_path(self, mock_file_changed):\n        mock_file_changed.return_value = True\n        template_path = Path('/absolute/path/to/template.html')\n        self.assertTrue(autoreload.template_changed(None, template_path))\n", "    def test_get_template_directories_with_absolute_path(self):\n        self.assertIn(\n            Path('/absolute/path'),\n            autoreload.get_template_directories()\n        )\n"], "sample_13": ["    def test_invalid_date(self):\n        with self.assertRaises(ValueError):\n            parse_http_date('invalid_date')\n", "    def test_valid_date(self):\n        date_str = 'Sun, 06 Nov 1994 08:49:37 GMT'\n        self.assertIsNotNone(parse_http_date_safe(date_str))\n", "    def test_empty_url(self):\n        self.assertEqual(escape_leading_slashes(''), '')\n", "    def test_current_time(self):\n        t = datetime.datetime.utcnow()\n        self.assertEqual(http_date(), t.strftime('%a, %d %b %Y %H:%M:%S GMT'))\n", "    def test_empty_string(self):\n        self.assertEqual(parse_etags(''), [])\n"], "sample_48": ["def test_add_field_preserve_default_true(self):\n    \"\"\"\n    Tests the AddField operation's state alteration\n    when preserve_default = True.\n    \"\"\"\n    project_state = self.set_up_test_model(\"test_adflpd\")\n    # Test the state alteration\n    operation = migrations.AddField(\n        \"Pony\",\n        \"height\",\n        models.FloatField(null=True, default=4),\n        preserve_default=True,\n    )\n    new_state = project_state.clone()\n    operation.state_forwards(\"test_adflpd\", new_state)\n    self.assertEqual(len(new_state.models[\"test_adflpd\", \"pony\"].fields), 4)\n    field = new_state.models['test_adflpd', 'pony'].fields['height']\n    self.assertEqual(field.default, 4)\n    # Test the database alteration\n    project_state.apps.get_model(\"test_adflpd\", \"pony\").objects.create(\n        weight=4,\n    )\n    self.assertColumnNotExists(\"test_adflpd_pony\", \"height\")\n    with connection.schema_editor() as editor:\n        operation.database_forwards(\"test_adflpd\", editor, project_state, new_state)\n    self.assertColumnExists(\"test_adflpd_pony\", \"height\")\n    # And deconstruction\n    definition = operation.deconstruct()\n    self.assertEqual(definition[0], \"AddField\")\n    self.assertEqual(definition[1], [])\n    self.assertEqual(sorted(definition[2]), [\"field\", \"model_name\", \"name\", \"preserve_default\"])\n\n", "    def test_references_field_by_through_model(self):\n        operation = FieldOperation('Model', 'field', models.ManyToManyField('Other', through='Through'))\n        self.assertIs(operation.references_field('Other', 'whatever', 'migrations'), True)\n        self.assertIs(operation.references_field('Through', 'whatever', 'migrations'), False)\n        self.assertIs(operation.references_field('Model', 'field', 'migrations'), True)\n", "    def test_alter_model_table_noop_no_db_table(self):\n        \"\"\"\n        AlterModelTable operation is a noop when no table name is given.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_almota\")\n        # Test the state alteration\n        operation = migrations.AlterModelTable(\"Pony\", None)\n        self.assertEqual(operation.describe(), \"Rename table for Pony to (default)\")\n        self.assertEqual(operation.migration_name_fragment, 'alter_pony_table')\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_almota\", new_state)\n        self.assertEqual(new_state.models[\"test_almota\", \"pony\"].options[\"db_table\"], \"test_almota_pony\")\n        # Test the database alteration\n        self.assertTableExists(\"test_almota_pony\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_almota\", editor, project_state, new_state)\n        self.assertTableExists(\"test_almota_pony\")\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_almota\", editor, new_state, project_state)\n        self.assertTableExists(\"test_almota_pony\")\n", "    def test_references_field_on_table_rename(self):\n        operation = FieldOperation('Model', 'field', models.CharField(max_length=100))\n        # Rename the table.\n        new_model = migration.RenameModel('Model', 'NewModel').reduce(operation, 'migrations')\n        self.assertIs(operation.references_field('NewModel', 'field', 'migrations'), False)\n        # Rename the field.\n        new_model = migration.AlterField('Model', 'field', models.CharField(max_length=100)).reduce(operation, 'migrations')\n        self.assertIs(operation.references_field('NewModel', 'field', 'migrations'), False)\n        new_model = migration.AlterField('Model', 'field', models.CharField(max_length=200)).reduce(operation, 'migrations')\n        self.assertIs(operation.references_field('Model', 'field', 'migrations'), True)\n", "    def test_add_unique_index(self):\n        \"\"\"\n        Tests the AlterTableIndex operation with unique=True.\n        \"\"\"\n        project_state = self.set_up_test_model(\"test_altridx\")\n        operation = migrations.AlterTableIndex(\"Pony\", \"pink\", unique=True)\n        self.assertEqual(operation.describe(), \"Add index pink on field(s) pink of model Pony\")\n        self.assertEqual(operation.migration_name_fragment, 'pony_pink')\n        # Test the state alteration\n        new_state = project_state.clone()\n        operation.state_forwards(\"test_altridx\", new_state)\n        self.assertEqual(len(new_state.models[\"test_altridx\", \"pony\"].options['indexes']), 1)\n        self.assertIndexNotExists(\"test_altridx_pony\", [\"pink\"])\n        # Test the database alteration\n        self.assertColumnNotNull(\"test_altridx_pony\", \"pink\")\n        with connection.schema_editor() as editor:\n            operation.database_forwards(\"test_altridx\", editor, project_state, new_state)\n        self.assertIndexExists(\"test_altridx_pony\", [\"pink\"])\n        self.assertIndexIsUnique(\"test_altridx_pony\", [\"pink\"])\n        # And test reversal\n        with connection.schema_editor() as editor:\n            operation.database_backwards(\"test_altridx\", editor, new_state, project_state)\n        self.assertIndexNotExists(\"test_altridx_pony\", [\"pink\"])\n        # And deconstruction\n        definition = operation.deconstruct()\n        self.assertEqual(definition[0], \"AlterTableIndex\")\n        self.assertEqual(definition[1], [])\n        self.assertEqual(definition[2], {'model_name': \"Pony\", 'fields': [\"pink\"], 'unique': True})\n"], "sample_12": ["def test_rename_field_with_remote_field(self):\n    \"\"\"Tests autodetection of renamed fields with a remote field.\"\"\"\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.IntegerField(unique=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='field')),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.IntegerField(unique=True)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.ForeignKey('app.Foo', models.CASCADE, to_field='renamed_field')),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, model_name='foo', old_name='field', new_name='renamed_field')\n    self.assertEqual(changes['app'][0].operations[0].field.remote_field.to_field.name, 'renamed_field')\n", "def test_add_field_with_auto_now_add_not_asking_for_default(self):\n    \"\"\"\n    #23609 - Tests autodetection of nullable to non-nullable alterations.\n    \"\"\"\n    changes = self.get_changes([self.author_empty], [self.author_dates_of_birth_auto_now_add])\n    self.assertEqual(changes, {})\n\n    changes = self.get_changes([self.author_dates_of_birth_auto_now_add], [self.author_dates_of_birth_auto_now_add])\n    self.assertEqual(changes, {})\n\n    changes = self.get_changes([self.author_empty, self.author_dates_of_birth_auto_now_add], [self.author_empty])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=True)\n\n    changes = self.get_changes([self.author_empty, self.author_dates_of_birth_auto_now_add], [self.author_empty])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=True)\n\n    changes = self.get_changes([self.author_empty, self.author_dates_of_birth_auto_now_add], [self.author_dates_of_birth_auto_now])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=True, default=None)\n\n    changes = self.get_changes([self.author_empty, self.author_dates_of_birth_auto_now], [self.author_empty])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"RemoveField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=None)\n\n    changes = self.get_changes([self.author_empty, self.author_dates_of_birth_auto_now], [self.author_dates_of_birth_auto_now_add])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AddField\"])\n    self.assertOperationFieldAttributes(changes, \"testapp\", 0, 0, auto_now_add=True)\n", "def test_add_order_with_respect_to(self):\n    \"\"\"Setting order_with_respect_to adds a field.\"\"\"\n    changes = self.get_changes([self.author_name], [self.author_with_book_order_wrt])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AddField\", \"AlterOrderWithRespectTo\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, model_name=\"author\", name=\"book\")\n    self.assertOperationAttributes(changes, 'testapp', 0, 1, name=\"author\", order_with_respect_to=\"book\")\n", "    def test_index_add(self):\n        \"\"\"Tests autodetection of new indexes.\"\"\"\n        changes = self.get_changes", "def test_create_unique_together_with_default_value(self):\n    \"\"\"\n    #23405 - When creating a unique together constraint with a field that\n    set to have a default value, the autodetector should not prompt for a default.\n    \"\"\"\n    model_state = ModelState(\"app\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200, default='John Doe')),\n        (\"email\", models.EmailField(max_length=254)),\n    ], {\n        \"unique_together\": {(\"name\", \"email\")},\n    })\n    changes = self.get_changes([], [model_state])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"app\", 1)\n    self.assertOperationTypes(changes, \"app\", 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, \"app\", 0, 0, name=\"Model\", unique_together={(\"name\", \"email\")})\n    self.assertEqual(model_state.options[\"unique_together\"], changes['app'][0].operations[0].options[\"unique_together\"])\n\n    model_state = ModelState(\"app\", \"Model\", [\n        (\"id\", models.AutoField(primary_key=True)),\n        (\"name\", models.CharField(max_length=200)),\n        (\"email\", models.EmailField(max_length=254, default='john.doe@example.com')),\n    ], {\n        \"unique_together\": {(\"name\", \"email\")},\n    })\n    changes = self.get_changes([model_state], [model_state])\n    # Right number of migrations?\n    self.assertEqual(len(changes), 0)\n"], "sample_6": ["    def test_unicode_validator_with_flags(self):\n        valid_usernames = ['joe', 'Ren\u00e9', '\u1d2e\u1d35\u1d33\u1d2e\u1d35\u1d3f\u1d30', '\u0623\u062d\u0645\u062f']\n        invalid_usernames = [\n            \"o'connell\", \"\u0639\u0628\u062f \u0627\u0644\",\n            \"zerowidth\\u200Bspace\", \"nonbreaking\\u00A0space\",\n            \"en\\u2013dash\", 'trailingnewline\\u000A',\n        ]\n        v = validators.UnicodeUsernameValidator(flags=re.IGNORECASE)\n        for valid in valid_usernames:\n            with self.subTest(valid=valid):\n                v(valid)\n        for invalid in invalid_usernames:\n            with self.subTest(invalid=invalid):\n                with self.assertRaises(ValidationError):\n                    v(invalid)\n", "    def test_flags(self):\n        v = validators.UnicodeUsernameValidator(flags=re.UNICODE_CASE)\n        self.assertTrue(v('a').has_match())\n        self.assertTrue(v('A').has_match())\n\n        v = validators.UnicodeUsernameValidator(flags=re.ASCII)\n        self.assertTrue(v('a').has_match())\n        self.assertFalse(v('A').has_match())\n\n        v = validators.UnicodeUsernameValidator(flags=0)\n        self.assertTrue(v('a').has_match())\n        self.assertFalse(v('A').has_match())\n", "    def test_deconstruct(self):\n        validator = ASCIIUsernameValidator()\n        self.assertEqual(validator.regex, ASCIIUsernameValidator.regex)\n        self.assertEqual(validator.message, ASCIIUsernameValidator.message)\n        self.assertEqual(validator.flags, ASCIIUsernameValidator.flags)\n\n        deconstructed = validators.deconstruct(validator)\n        self.assertEqual(deconstructed['dependencies'], [])\n        self.assertEqual(deconstructed['code'], 'validators.ASCIIUsernameValidator')\n\n        # Test deconstruction with an instance of UnicodeUsernameValidator\n        validator = UnicodeUsernameValidator()\n        deconstructed = validators.deconstruct(validator)\n        self.assertEqual(deconstructed['dependencies'], [])\n        self.assertEqual(deconstructed['code'], 'validators.UnicodeUsernameValidator')\n", "    def test_deconstruction(self):\n        validator = ASCIIUsernameValidator()\n        expected_attrs = ['regex', 'flags', 'message']\n        self.assertEqual(set(dir(validator)), set(expected_attrs))\n        self.assertEqual(validator.regex, ASCIIUsernameValidator.regex)\n        self.assertEqual(validator.flags, ASCIIUsernameValidator.flags)\n        self.assertEqual(validator.message, ASCIIUsernameValidator.message)\n        deconstructed_validator = validator.deconstruct()\n        self.assertEqual(deconstructed_validator['constructor'], validator.__class__)\n        self.assertEqual(deconstructed_validator['args'], [])\n        self.assertEqual(deconstructed_validator['kwargs'], {'regex': validator.regex, 'flags': validator.flags, 'message': validator.message})\n", "    def test_validate(self):\n        class CustomValidator(validators.RegexValidator):\n            regex = r'^test-regex-\\d+$'\n            message = _('This password is not a custom validator.')\n            flags = 0\n\n        validator = CustomValidator()\n        self.assertIsNone(validator.validate('test-regex-123'))\n\n        with self.assertRaises(ValidationError) as cm:\n            validator.validate('invalid')\n        self.assertEqual(cm.exception.messages, [validator.message])\n        self.assertEqual(cm.exception.error_list[0].code, 'custom_validator')\n"], "sample_153": ["def test_issue_12345():\n    from sympy import symbols, sin, cos, pi, UnevaluatedExpr\n\n    delop = Del()\n    CC_   = CoordSys3D(\"C\")\n    y     = CC_.y\n    xhat  = CC_.i\n\n    t = symbols(\"t\")\n    ten = symbols(\"10\", positive=True)\n    eps, mu = 4*pi*ten**(-11), ten**(-5)\n\n    Bx = 2 * ten**(-4) * cos(ten**5 * t) * sin(ten**(-3) * y)\n    vecB = Bx * xhat\n    vecE = (1/eps) * Integral(delop.cross(vecB/mu).doit(), t)\n    vecE = vecE.doit()\n\n    vecB_str = \"\"\"\\", "def test_pretty_printing_NegInfinity():\n    assert pretty(-float('inf')) == '-oo'\n    assert pretty(float('inf')) == 'oo'\n    assert upretty(-float('inf')) == '\u221e'\n    assert upretty(float('inf')) == '-\u221e'\n", "def test_RootSum_printing():\n    from sympy.functions.elementary.integers import RootSum\n    from sympy import symbols\n    x, y = symbols('x y')\n    expr = RootSum(x**2 + x*y, x)\n    assert upretty(expr) == 'x**2 + x*y | x'\n", "def test_Choice_and_Mul():\n    from sympy.combinatorics.permutations import Permutation\n    from sympy.printing.pretty import pretty as xpretty\n\n    C = Permutation([1, 2, 3])\n    M = Mul(C, C)\n\n    assert pretty(M) == '(2)(3)'\n    assert xpretty(M, use_unicode=False, wrap_line=False) == '(2)(3)'\n    assert xpretty(M, use_unicode=True, wrap_line=False) == '2\u22c53'\n\n    C = Permutation([4, 5, 6])\n    M = Mul(M, C)\n\n    assert pretty(M) == '(2)(3)(4)(5)(6)'\n    assert xpretty(M, use_unicode=False, wrap_line=False) == '(2)(3)(4)(5)(6)'\n    assert xpretty(M, use_unicode=True, wrap_line=False) == '2\u22c53\u22c54\u22c55\u22c56'\n", "def test_issue_16518():\n    from sympy import sin, symbols\n    # Test that multivariable expressions are properly formatted.\n    x, y, z = symbols('x y z')\n    e = sin(x)**2 + sin(y)**2 + sin(z)**2\n    upretty_e = \"\"\"\\\n        2        2       2\\n\\"], "sample_140": ["def test_auto_point_vel_inconsistent_position():\n    t = dynamicsymbols._t\n    q, q1, q2, u = dynamicsymbols('q q1 q2 u')\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    O.set_vel(N, u * N.x)\n    P = Point('P')\n    P.set_pos(O, q1 * N.x + q2 * B.y)\n    P1 = Point('P1')\n    P1.set_pos(P, -q1 * N.x - q2 * B.y)\n    raises(ValueError, lambda: P1.vel(N)) # Inconsistent position of P1 with respect to P\n    N.orient(B, 'Axis', (q, B.x))\n    assert P.vel(N) == (u + q.diff(t)) * N.x + q2.diff(t) * B.y - q2 * q.diff(t) * B.z\n", "def test_set_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    B.set_ang_vel(N, qd * B.z)\n    O = Point('O')\n    P = O.locatenew('P', B.x)\n    O.set_vel(N, 0)\n    assert P.acc(B) == 0\n    P.set_acc(B, q2d * B.z)\n    assert P.acc(B) == q2d * B.z\n    O.set_vel(N, 10 * N.x)\n    P.set_acc(B, q2d * B.z)\n    assert P.acc(B) == q2d * B.z\n    P.set_acc(N, 10 * N.x)\n    assert P.acc(N) == 10\n    P.set_acc(B, 5 * B.y)\n    assert P.acc(B) == q2d * B.z + 5 * B.y\n", "def test_set_acc():\n    q, q2 = dynamicsymbols('q q2')\n    qd, q2d = dynamicsymbols('q q2', 1)\n    N = ReferenceFrame('N')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = O.locatenew('P', 0)\n    O.set_vel(N, 0)\n    P.set_vel(B, qd * B.x + q2d * B.y)\n    O.set_acc(N, 10 * N.x)\n    assert P.acc(N) == 10 * N.x\n    P.set_acc(N, 20 * N.x)\n    assert P.acc(N) == 20 * N.x\n    O.set_vel(N, 5 * N.y)\n    P.set_acc(N, 10 * N.x + 5 * N.y)\n    assert P.acc(N) == 10 * N.x + 5 * N.y\n    P.set_acc(N, 0)\n    assert P.acc(N) == 5 * N.y\n", "def test_auto_point_pos_shortest_path():\n    t = dynamicsymbols._t\n    q1, q2, q3 = dynamicsymbols('q1 q2 q3')\n    B = ReferenceFrame('B')\n    O = Point('O')\n    P = O.locatenew('P', q1 * B.x)\n    P1 = O.locatenew('P1', q2 * B.y)\n    P2 = O.locatenew('P2', q3 * B.z)\n    Q = P.locatenew('Q', q1 * B.x + q2 * B.y)\n    assert Q.pos_from(O) == q1 * B.x + q2 * B.y\n    assert Q.pos_from(P) == q2 * B.y + (q3 - q1) * B.z\n    assert Q.pos_from(P1) == (q3 - q1) * B.z\n    assert Q.pos_from(P2) == q1 * B.x + (q3 - q1) * B.z\n    Q2 = P1.locatenew('Q2', 10 * B.z)\n    assert Q2.pos_from(Q) == -10 * B.z\n    assert Q2.pos_from(O) == 10 * q2 * B.y + (q3 - q1) * B.z\n", "def test_set_acc():\n    t = dynamicsymbols._t\n    q1, q2 = dynamicsymbols('q1 q2')\n    N = ReferenceFrame('N')\n    P = Point('P')\n    P.set_vel(N, q1 * N.x + q2 * N.y)\n    P.set_acc(N, 0)\n    assert P.acc(N) == 0\n    P.set_acc(N, q1 * N.x + q2 * N.y)\n    assert P.acc(N) == q1 * N.x + q2 * N.y\n    P.set_acc(N, q1 * N.x)\n    assert P.acc(N) == q1 * N.x\n    assert P.acc(N) == q1 * N.x\n    P.set_acc(N, 0)\n    assert P.acc(N) == 0\n"], "sample_19": ["    def test_non_sensitive_request(self):\n        \"\"\"\n        Everything (request info and frame variables) can bee seen\n        in the default error reports for non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(non_sensitive_view)\n\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(non_sensitive_view)\n", "    def test_sensitive_variables(self):\n        \"\"\"\n        The sensitive_variables decorator works with function wrappers.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(sensitive_variables(lambda x: x)(lambda password: password))\n            self.verify_unsafe_email(sensitive_variables(lambda x: x)(lambda password: password))\n\n        with self.settings(DEBUG=False):\n            self.verify_safe_response(sensitive_variables(lambda x: x)(lambda password: password))\n            self.verify_safe_email(sensitive_variables(lambda x: x)(lambda password: password))\n", "    def test_setting_custom_exception_reporter_class(self):\n        with self.settings(DEBUG=True):\n            response = self.client.get('/custom_reporter_class_view/')\n            self.assertContains(response, 'custom traceback text', status_code=500)\n", "    def test_non_sensitive_request(self):\n        \"\"\"\n        Everything (request info and frame variables) can bee seen\n        in the default error reports for non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(non_sensitive_view, check_for_vars=True,\n                                       check_for_POST_params=True)\n\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(non_sensitive_view, check_for_vars=True,\n                                       check_for_POST_params=True)\n", "    def test_non_sensitive_request(self):\n        \"\"\"\n        Request info can bee seen in the default error reports for\n        non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            self.verify_unsafe_response(non_sensitive_view, check_for_vars=False)\n\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(non_sensitive_view, check_for_vars=False)\n"], "sample_119": ["def test_Add():\n    assert mcode(x + y) == \"x + y\"\n    assert mcode(x + y + z) == \"x + y + z\"\n    assert mcode(x + y - z) == \"x + y - z\"\n    assert mcode(x - y - z) == \"x - y - z\"\n    assert mcode(x*y - 2*z) == \"x*y - 2*z\"\n    assert mcode(x + 2*y - 3*z) == \"x + 2*y - 3*z\"\n    assert mcode(3 + 2*x - x) == \"3 + 2*x - x\"\n", "def test_FuncWithArgs():\n    assert mcode(sin(x, x)) == \"Sin[x, x]\"\n    assert mcode(sin(x, x, x)) == \"Sin[x, x, x]\"\n    assert mcode(sin(x, y)) == \"Sin[x, y]\"\n    assert mcode(x**sin(x, x)) == \"x^(Sin[x, x])\"\n", "def test_Abs():\n    assert mcode(Abs(x)) == \"Abs[x]\"\n    assert mcode(-Abs(x)) == \"-Abs[x]\"\n    assert mcode(Abs(x + 1)) == \"Abs[x + 1]\"\n    assert mcode(Abs(x) + Abs(y)) == \"Abs[x] + Abs[y]\"\n", "def test_MCodePrinter_settings():\n    settings = {\n        'order': 'hzn',\n        'full_prec': 'none',\n        'precision': 20,\n        'user_functions': {\n            'my_function': 'myFunction',\n            'my_function2': 'myFunction2'\n        },\n        'human': False,\n    }\n    expr = f(x, y, z)\n    printer = MCodePrinter(settings)\n    assert printer.doprint(expr) == \"myFunction[x, y, z]\"\n    assert printer.doprint(sin(x) ** cos(x)) == \"myFunction2[Sin[x], Cos[x]]\"\n    assert printer.doprint(Integral(sin(x), x)) == \"Hold[Integrate[Sin[x], x]]\"\n    assert printer.doprint(Sum(sin(x), (x, 0, 10))) == \"Hold[Sum[Sin[x], {x, 0, 10}]]\"\n    assert printer.doprint(Derivative(sin(x), x)) == \"Hold[D[Sin[x], x]]\"\n", "def test_Add():\n    assert mcode(x + y) == \"x + y\"\n    assert mcode(x - y) == \"x - y\"\n    assert mcode(x + y + z) == \"x + y + z\"\n    assert mcode(x + 1/2) == \"x + 1/2\"\n    assert mcode(x + 1) == \"x + 1\"\n    assert mcode(1/2 + x) == \"1/2 + x\"\n    assert mcode(1 + x) == \"1 + x\"\n    assert mcode(x + Rational(3, 7)) == \"x + 3/7\"\n    assert mcode(x - Rational(3, 7)) == \"x - 3/7\"\n    assert mcode(x - y - z) == \"x - y - z\"\n    assert mcode(1 + 1/2) == \"3/2\"\n    assert mcode(x + Rational(3, 7)*y) == \"x + (3/7)*y\"\n    assert mcode(1 + Rational(3, 7)*y) == \"1 + (3/7)*y\"\n    assert mcode(x + x) == \"2*x\"\n    assert mcode(x + y + x) == \"x + y + x\"\n"], "sample_133": ["def test_rust_codegen():\n    x, y, z = symbols('x,y,z')\n    expr = (x + y)*z\n    routine = make_routine(\"test\", expr)\n    code_gen = RustCodeGen()\n    source = get_string(code_gen.dump_rs, [routine])\n    expected = (\n        \"// file.rs\\n\"\n        \"// See http://www.sympy.org/ for more information.\\n\"\n        \"// This file is part of 'project'\\n\"\n        \"\\n\"\n        \"fn test(double x, double y, double z) -> double {\\n\"\n        \"   let test_result;\\n\"\n        \"   test_result = z*(x + y);\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"\n    )\n    assert source == expected\n", "def test_rust_with_external_header():\n    #issue #13586\n    from sympy.printing.rust import RustCodePrinter\n    class CustomPrinter(RustCodePrinter):\n            return \"{}**{}\".format(self._print(expr.base), self._print(expr.exp))\n\n    x = symbols('x')\n    expr = x**3\n    expected = [\n        (\"file.rs\",\n        \"use std::f64;\\n\"\n        \"fn test(x: f64) -> f64 {\\n\"\n        \"   let test_result = x**3;\\n\"\n        \"   test_result\\n\"\n        \"}\\n\"),\n        (\"file.h\",\n        \"#ifndef PROJECT__FILE__H\\n\"\n        \"#define PROJECT__FILE__H\\n\"\n        \"extern \\\"C\\\" fn test(x: f64) -> f64;\\n\"\n        \"#endif\\n\")\n    ]\n    result = codegen((\"test\", expr), \"RUST\", \"file\", header=False, empty=False, printer=CustomPrinter())\n    assert result == expected\n", "def test_cse_matrices():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    m, n = symbols('m,n', integer=True)\n    A = IndexedBase('A')\n    B = IndexedBase('B')\n    i = Idx('i', m)\n    j = Idx('j', n)\n    routine = make_routine(\n        \"test\",\n        [Equality(B[i, j], A[i, j]*A[i, j])]\n    )\n    code_gen = CCodeGen(cse=True)\n    source = get_string(code_gen.dump_c, [routine])\n    expected = (\n        '#include \"test.h\"\\n'\n        '#include <math.h>\\n'\n        'double test(double *A, int m, int n) {\\n'\n        '   const double x0 = A[0];\\n'\n        '   double test_result;\\n'\n        '   test_result = 0;\\n'\n        '   for (int i=0; i<m; i++){\\n'\n        '      for (int j=0; j<n; j++){\\n'\n        '         test_result += x0*x0;\\n'\n        '      }\\n'\n        '   }\\n'\n        '   return test_result;\\n'\n        '}\\n'\n    )\n    assert source == expected\n", "def test_matrix_output_arg():\n    from sympy import symbols, MatrixSymbol\n    from sympy.utilities.codegen import CodeGenArgumentListError\n    x, y, z = symbols('x,y,z')\n    A = MatrixSymbol('A', 3, 3)\n    name_expr = (\"test\", (x + y)*z, Matrix(A))\n    result = codegen(name_expr, \"f95\", \"test\", header=False, empty=False)\n    source = result[0][1]\n    expected = (\n        \"subroutine test(x, y, z, out_%(hash)s)\\n\"\n        \"implicit none\\n\"\n        \"REAL*8, intent(in) :: x\\n\"\n        \"REAL*8, intent(in) :: y\\n\"\n        \"REAL*8, intent(in) :: z\\n\"\n        \"REAL*8, intent(out), dimension(1:3, 1:3) :: out_%(hash)s\\n\"\n        \"out_%(hash)s(1, 1) = x + y\\n\"\n        \"out_%(hash)s(2, 1) = x + y\\n\"\n        \"out_%(hash)s(3, 1) = x + y\\n\"\n        \"out_%(hash)s(1, 2) = x + y\\n\"\n        \"out_%(hash)s(2, 2) = x + y\\n\"\n        \"out_%(hash)s(3, 2) = x + y\\n\"\n        \"out_%(hash)s(1, 3) = x + y\\n\"\n        \"out_%(hash)s(2, 3) = x + y\\n\"\n        \"out_%(hash)s(3, 3) = x + y\\n\"\n        \"end subroutine\\n\"\n    )\n    # look for the magic number\n    a = source.splitlines()[5]\n    b = a.split('_')\n    out = b[1]\n    expected = expected % {'hash': out}\n    assert source == expected\n\n    # test that we raise the error when output args are not specified\n    raises(CodeGenArgumentListError, lambda: codegen((\"test\", (x + y)*z, A), \"f95\", \"test\", header=False, empty=False))\n", "def test_CCodeGen_with_preprocessor():\n    # This is a test of the preprocessor_statements method in CCodeGen\n    x, y, z = symbols('x,y,z')\n    routine = make_routine('test', x + y)\n    code_gen = CCodeGen(preprocessor_statements=['#define MY_CONST 1',\n                                                 '#define MY_FUNC(x) x'])\n    result = code_gen.write([routine], \"test\", header=False, empty=False)\n    assert result[0][0] == \"test.c\"\n    expected = (\n        \"#include \\\"test.h\\\"\\n\"\n        \"#include <math.h>\\n\"\n        \"#define MY_CONST 1\\n\"\n        \"#define MY_FUNC(x) x\\n\"\n        \"double test(double x, double y) {\\n\"\n        \"   double test_result;\\n\"\n        \"   test_result = x + y;\\n\"\n        \"   return test_result;\\n\"\n        \"}\\n\"\n    )\n    assert result[0][1] == expected\n\n    result = codegen(('test', x + y), \"C\", \"test\", header=False, empty=False,\n                     code_gen=code_gen)\n    assert result[0][1] == expected\n"], "sample_148": ["def test_polar_lift_real():\n    from sympy import polar_lift, S\n    assert polar_lift(2).is_real is True\n    assert polar_lift(2).is_extended_real is True\n    assert polar_lift(2).is_rational is True\n    assert polar_lift(2).is_positive is True\n    assert polar_lift(2).is_nonnegative is True\n    assert polar_lift(2).is_extended_positive is True\n    assert polar_lift(2).is_zero is False\n    assert polar_lift(2).is_finite is True\n    assert polar_lift(2).is_integer is True\n    assert polar_lift(2).is_even is None\n    assert polar_lift(2).is_odd is None\n    assert polar_lift(2).is_algebraic is False\n    assert polar_lift(2).is_rational is True\n\n    r = Symbol('r', real=True)\n    assert polar_lift(2*r).is_real is True\n    assert polar_lift(2*r).is_extended_real is True\n    assert polar_lift(2*r).is_rational is None\n    assert polar_lift(2*r).is_positive is None\n    assert polar_lift(2*r).is_nonnegative is None\n    assert polar_lift(2*r).is_extended_positive is None\n    assert polar_lift(2*r).is_zero is None\n    assert polar_lift(2*r).is_finite is True\n    assert polar_lift(2*r).is_integer is None\n    assert polar_lift(2*r).is_even is None\n    assert polar_lift(2*r).is_odd is None\n    assert polar_lift(2*r).is_algebraic is None\n\n    i = Symbol('i', imaginary=True)\n    assert polar_lift(2*i).is_real is False\n    assert polar_lift(2*i).is_extended_real is False\n    assert polar_lift(2*i).is_rational is None\n    assert polar_lift(2*i).is_positive is None\n    assert polar_lift(2*i).is_nonnegative is None\n    assert polar_lift(2*i).is_extended_positive is None\n    assert polar_lift(2*i).is_zero is None\n    assert polar_lift(2*i).is_finite is True\n    assert polar_lift(2*i).is_integer is None\n    assert polar_lift(2*i).is_even is None\n    assert polar_lift(2*i).is_odd", "def test_conjugate_transpose():\n    from sympy import conjugate, transpose, adjoint, MatrixSymbol, Matrix\n    x, y = symbols('x y')\n    A = MatrixSymbol('A')\n    assert conjugate(transpose(A)) == adjoint(A)\n    assert transpose(conjugate(A)) == adjoint(A)\n    assert adjoint(transpose(A)) == conjugate(A)\n    assert transpose(adjoint(A)) == conjugate(A)\n    assert adjoint(conjugate(A)) == transpose(A)\n    assert conjugate(adjoint(A)) == transpose(A)\n\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n\n    assert conjugate(transpose(A)) == transpose(adjoint(A))\n    assert transpose(conjugate(A)) == transpose(adjoint(A))\n    assert adjoint(transpose(A)) == conjugate(A)\n    assert transpose(adjoint(A)) == conjugate(A)\n    assert adjoint(conjugate(A)) == transpose(A)\n    assert conjugate(adjoint(A)) == transpose(A)\n\n    assert conjugate(A * B) == conjugate(B) * conjugate(A)\n    assert conjugate(A + B) == conjugate(A) + conjugate(B)\n    assert conjugate(A - B) == conjugate(A) - conjugate(B)\n    assert conjugate(A * B) == conjugate(B) * conjugate(A)\n\n    assert transpose(A * B) == transpose(B) * transpose(A)\n    assert transpose(A + B) == transpose(A) + transpose(B)\n    assert transpose(A - B) == transpose(A) - transpose(B)\n    assert transpose(A * B) == transpose(B) * transpose(A)\n", "def test_zero_arguments():\n    n = pi**1000\n    i = int(n)\n    assert sign(n - i).is_zero\n    assert sign(1 - i).is_zero\n    assert Abs(n - i).is_zero\n    assert sign(n - i).doit() == 1\n    assert sign(1 - i).doit() == 1\n    assert Abs(n - i).doit() == n - i\n    x = Symbol('x')\n    assert sign(x).is_zero is None\n    assert sign(x).doit().is_zero is None\n    assert Abs(x).is_zero is None\n    assert Abs(x).doit().is_zero is None\n", "def test_unpolarify_issue_2356():\n    from sympy import unpolarify, exp_polar, exp\n    from sympy.abc import x\n    p = exp_polar(7*I) + 1\n    u = exp(7*I) + 1\n\n    # Check that unpolarify handles expressions with repeated exponentiation of exp_polar\n    assert unpolarify(p**2) == u**2\n    assert unpolarify(p**3) == u**3\n\n    # Check that unpolarify handles expressions with functions\n    f = Function('f')\n    assert unpolarify(f(p)) == f(u)\n    assert unpolarify(f(p)**2) == f(u)**2\n\n    # Check that unpolarify handles expressions with complex numbers and symbols\n    assert unpolarify(2 + 3*p) == 2 + 3*u\n    assert unpolarify(x + p) == x + u\n\n    # Check that unpolarify handles expressions with conjugates\n    assert unpolarify(conjugate(p)) == conjugate(u)\n", "def test_issue_17243():\n    from sympy import PolarFunction, polar_lift, oo\n\n    a = Symbol('a')\n    z = Symbol('z', polar=True)\n    p = Symbol('p', extended_positive=True)\n    w = Symbol('w', polar=True, extended_positive=True)\n\n    # sanity checks\n    assert PolarFunction(polar_lift(a)).is_polar is True\n    assert PolarFunction(polar_lift(a)).is_comparable is False\n\n    assert PolarFunction(polar_lift(a)*p) == (polar_lift(a)*p)\n    assert PolarFunction(p) == p\n\n    # PolarFunction() is the identity\n    assert (PolarFunction(polar_lift(a)) * w) == polar_lift(a) * w\n    assert (polar_lift(a) * PolarFunction(w)) == polar_lift(a) * w\n    assert PolarFunction(polar_lift(a) * w) == polar_lift(a) * w\n    assert PolarFunction(w * polar_lift(a)) == polar_lift(a) * w\n    assert PolarFunction(PolarFunction(polar_lift(a))) == polar_lift(a)\n\n    # Test the principal_branch\n    assert PolarFunction(principal_branch(a, 2*pi)) == principal_branch(a, 2*pi)\n    assert PolarFunction(principal_branch(a, 2*pi)*p) == principal_branch(a, 2*pi)*p\n    assert PolarFunction(principal_branch(p, 2*pi)) == principal_branch(p, 2*pi)\n    assert PolarFunction(principal_branch(p, 2*pi)*w) == principal_branch(p, 2*pi)*w\n\n    # Test periodic_argument\n    assert PolarFunction(periodic_argument(a, 2*pi)) == periodic_argument(a, 2*pi)\n    assert PolarFunction(periodic_argument(a, 2*pi)*p) == periodic_argument(a, 2*pi)*p\n    assert PolarFunction(periodic_argument(p, 2*pi)) == periodic_argument(p, 2*pi)\n    assert PolarFunction(periodic_argument(p, 2*pi)*w) == periodic_argument(p, 2*pi)*w\n\n    # Test with oo\n    assert PolarFunction(oo) == oo\n    assert PolarFunction(p * oo) == oo\n    assert PolarFunction(w * oo) == oo\n"], "sample_23": ["def test_union_with_distinct(self):\n    qs1 = Number.objects.all()\n    qs2 = Number.objects.filter(num=9)\n    self.assertEqual(len(list(qs1.union(qs2, distinct=True))), 10)\n", "    def setUpTestData(cls):\n        Number.objects.bulk_create(Number(num=i, other_num=10 - i) for i in range(10))\n", "def test_intersection_with_annotation(self):\n    ReservedName.objects.create(name='a', order=2)\n    qs1 = ReservedName.objects.all()\n    qs2 = ReservedName.objects.filter(order__gt=0)\n    reserved_name = qs1.intersection(qs2).values('name', 'order', 'id').get()\n    self.assertEqual(reserved_name['name'], 'a')\n    self.assertEqual(reserved_name['order'], 2)\n    reserved_name = qs1.intersection(qs2).values_list('name', 'order', 'id').get()\n    self.assertEqual(reserved_name[:2], ('a', 2))\n", "def test_difference_with_values_list_on_annotated_and_unannotated(self):\n    ReservedName.objects.create(name='rn1', order=1)\n    qs1 = Number.objects.annotate(\n        has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n    ).filter(has_reserved_name=True)\n    qs2 = Number.objects.filter(num=9)\n    self.assertCountEqual(qs1.difference(qs2).values_list('num', flat=True), [1])\n\n    ReservedName.objects.create(name='rn1', order=1)\n    qs1 = Number.objects.annotate(\n        has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n    ).filter(has_reserved_name=True)\n    qs2 = Number.objects.filter(num=9)\n    qs2 = qs2.values('pk').annotate(\n        num=F('num')\n    ).annotate(\n        reserved_name=Value(1, IntegerField())\n    )\n    self.assertCountEqual(qs1.difference(qs2).values_list('num', flat=True), [1])\n", "def test_union_with_empty_qs_and_annotation(self):\n    qs1 = Number.objects.annotate(\n        has_reserved_name=Exists(ReservedName.objects.filter(order=OuterRef('num')))\n    ).filter(has_reserved_name=True)\n    qs2 = Number.objects.none()\n    union_qs = qs1.union(qs2)\n    self.assertEqual(len(list(union_qs.values_list('num', flat=True))), 1)\n    self.assertEqual(union_qs.count(), 1)\n"], "sample_146": ["def test_BlockMatrix():\n    A = Matrix([[1, 2], [3, 4]])\n    B = Matrix([[5, 6], [7, 8]])\n    BM = BlockMatrix([[A, A], [B, B]])\n    assert str(BM) == \"Matrix([[Matrix([[1, 2], [3, 4]]), Matrix([[1, 2], [3, 4]])], [Matrix([[5, 6], [7, 8]]), Matrix([[5, 6], [7, 8]])])\"\n", "def test_BlockMatrix_printing():\n    from sympy import Matrix, BlockMatrix\n    from sympy.tensor.array.expressions import BlockMatrixBase\n    block_matrix = BlockMatrix([[Matrix([[1, 2], [3, 4]]), Matrix([[5, 6], [7, 8]])])\n    assert str(block_matrix) == \"Matrix([[1, 2], [3, 4], [5, 6], [7, 8]])\"\n    block_matrix = BlockMatrixBase([[Matrix([[1, 2], [3, 4]]), Matrix([[5, 6], [7, 8]])])\n    assert str(block_matrix) == \"Matrix([[1, 2], [3, 4], [5, 6], [7, 8]])\"\n", "def test_DimensionalQuantity():\n    from sympy.physics.units import second, joule, meters\n    u = second + 2 * joule\n    assert sstr(u) == \"s + 2 J\"\n    assert sstr(meters) == \"m\"\n", "def test_TensorMul():\n    from sympy.tensor import NDimArray, TensorIndex, TensorHead, Tensor\n    A = TensorHead('A')\n    t = TensorIndex('t')\n    u = TensorIndex('u')\n    B = TensorHead('B')\n    assert str(A * B) == \"A*B\"\n    assert str(NDimArray(1) * NDimArray(2)) == \"1*2\"\n    assert str(A * 2) == \"A*2\"\n    assert str(2 * A) == \"2*A\"\n    assert str(NDimArray(1) * 2) == \"1*2\"\n    assert str(2 * NDimArray(1)) == \"2*1\"\n    assert str(A * B * 2) == \"A*B*2\"\n    assert str(2 * A * B) == \"2*A*B\"\n    assert str(NDimArray(1) * NDimArray(2) * 3) == \"1*2*3\"\n    assert str(3 * NDimArray(1) * NDimArray(2)) == \"3*1*2\"\n    assert str(A * B * C) == \"A*B*C\"\n    assert str(NDimArray(1) * NDimArray(2) * NDimArray(3)) == \"1*2*3\"\n", "def test_issue_26649():\n    from sympy.core import sympify\n    from sympy.core.numbers import One\n\n    e = sqrt(sqrt(x))\n    assert str(e) == \"x**(1/4)\"\n    assert str(sympify(e)) == \"x**(1/4)\"\n    assert str(sympify(e)) == \"x**(1/4)\"\n    assert str(sympify(e)) == \"x**(1/4)\"\n    assert str(One()**e) == \"1\"\n    assert str(e.as_numer_denom()[0]) == \"x**(1/4)\"\n    assert str(e.as_numer_denom()[1]) == \"1\"\n"], "sample_17": ["    def test_destroy_test_db_prefix(self, mocked_ensure_connection):\n        # A test db name prefixed with TEST_DATABASE_PREFIX is set.\n        test_name = TEST_DATABASE_PREFIX + 'hodor'\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = test_name\n        creation = test_connection.creation_class(test_connection)\n        creation.destroy_test_db(verbosity=0, keepdb=False)\n        mocked_ensure_connection.assert_called_once()\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['NAME'] = 'prod_name'\n        creation = test_connection.creation_class(test_connection)\n        test_database_name = creation._get_test_db_name()\n        creation.clone_test_db('suffix', verbosity=0, autoclobber=False, keepdb=False)\n        self.assertNotEqual(test_connection.settings_dict['NAME'], 'prod_name')\n        self.assertEqual(test_connection.settings_dict['NAME'], 'test_' + 'prod_name_suffix')\n        self.assertEqual(creation.test_db_signature()[3], 'test_' + 'prod_name_suffix')\n        creation.destroy_test_db(test_connection.settings_dict['NAME'], verbosity=0, keepdb=False)\n        creation.clone_test_db('suffix', verbosity=0, autoclobber=False, keepdb=True)\n        self.assertEqual(test_connection.settings_dict['NAME'], 'prod_name')\n", "    def test_serialize_and_deserialize(self):\n        # Serialize a single object, then deserialize it back into a new database.\n        obj = Object.objects.create()\n        obj_ref = ObjectReference.objects.create(obj=obj)\n        with self.settings(DATABASES={\n            'default': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'test.sqlite',\n            },\n            'test': {\n                'ENGINE': 'django.db.backends.sqlite3',\n                'NAME': 'test.sqlite',\n            }\n        }):\n            creation = connections['test'].creation_class(connections['test'])\n            data = creation.serialize_db_to_string()\n            creation.deserialize_db_from_string(data)\n            deserialized_obj = Object.objects.get()\n            deserialized_obj_ref = ObjectReference.objects.get()\n            self.assertEqual(deserialized_obj.pk, obj.pk)\n            self.assertEqual(deserialized_obj_ref.pk, obj_ref.pk)\n            self.assertEqual(deserialized_obj.obj_ref.pk, deserialized_obj_ref.obj.pk)\n            self.assertEqual(deserialized_obj_ref.obj.pk, deserialized_obj.pk)\n", "    def test_empty_database(self):\n        # serialize_db_to_string() handles empty databases.\n        test_connection = get_connection_copy()\n        connection.creation.serialize_db_to_string(test_connection)\n        self.assertEqual(test_connection._test_serialized_contents, '')\n", "    def test_backward_references(self, mocked_ensure_connection):\n        # deserialize_db_from_string() handles backward references.\n        data = \"\"\"\n        [\n            {\n                \"model\": \"backends.object\",\n                \"pk\": 1,\n                \"fields\": {\"obj_ref\": 1, \"related_objects\": []}\n            },\n            {\n                \"model\": \"backends.objectreference\",\n                \"pk\": 1,\n                \"fields\": {\"obj\": 1}\n            }\n        ]\n        \"\"\"\n        connection.creation.deserialize_db_from_string(data)\n        obj = Object.objects.get()\n        obj_ref = ObjectReference.objects.get()\n        obj_ref.obj_id = 2\n        obj_ref.save()\n        self.assertEqual(obj.obj_ref, obj_ref)\n        self.assertEqual(obj_ref.obj_id, obj.pk)\n"], "sample_99": ["def test_KNeighborsRegressor_sparse_precomputed():\n    # Test KNeighborsRegressor on sparse precomputed distances\n    rng = np.random.RandomState(42)\n    X = rng.rand(12, 4)\n    D = pairwise_distances(X, metric='euclidean')\n    knn = neighbors.KNeighborsRegressor(n_neighbors=3, metric='precomputed')\n    knn.fit(D)\n    assert_raises(ValueError, knn.predict, csr_matrix(D))\n", "def test_kneighbors_regressor_sparse_precomputed():\n    # Test k-neighbors regression on sparse precomputed matrices\n    rng = np.random.RandomState(0)\n    X = rng.rand(10, 5)\n    y = rng.rand(10)\n    Xcsr = pairwise_distances(X, metric='euclidean')\n    knn = neighbors.KNeighborsRegressor(n_neighbors=3, algorithm='brute',\n                                        metric='precomputed')\n    knn.fit(Xcsr, y)\n    knn_pre = neighbors.KNeighborsRegressor(n_neighbors=3,\n                                            metric='precomputed')\n    knn_pre.fit(pairwise_distances(X, metric='euclidean'), y)\n    for sparsev in SPARSE_OR_DENSE:\n        X2 = sparsev(pairwise_distances(X, metric='euclidean'))\n        if issparse(sparsev(X2)):\n            assert_raises(ValueError, knn_pre.predict, X2)\n        else:\n            assert_true(\n                np.mean(knn_pre.predict(X2).round() == y) > 0.95)\n\n        X2csr = sparsev(Xcsr)\n        assert_true(np.mean(knn.predict(X2csr).round() == y) > 0.95)\n", "def test_radius_neighbors_regressor_sparse():\n    # Test radius-based regression on sparse matrices\n    rng = np.random.RandomState(0)\n    X = 2 * rng.rand(40, 5) - 1\n    y = np.sqrt((X ** 2).sum(1))\n    y /= y.max()\n    y = np.vstack([y, y]).T\n\n    X_sparse = csr_matrix(X)\n    y_sparse = csr_matrix(y)\n\n    weights = ['uniform', 'distance']\n\n    for algorithm, weights in product(ALGORITHMS, weights):\n        rnn = neighbors.RadiusNeighborsRegressor(n_neighbors=3,\n                                                 weights=weights,\n                                                 algorithm=algorithm)\n        rnn.fit(X_sparse, y_sparse)\n        epsilon = 1E-5 * (2 * rng.rand(1, 5) - 1)\n        y_pred = rnn.predict(X_sparse[:10] + epsilon)\n        assert_equal(y_pred.shape, y[:10].shape)\n        assert_true(np.all(np.abs(y_pred - y[:10]) < 0.3))\n\n        # Check that it works with the density matrix\n        for sparsev in SPARSE_OR_DENSE:\n            X2 = sparsev(X[:10] + epsilon)\n            y_pred = rnn.predict(X2)\n            assert_equal(y_pred.shape, y[:10].shape)\n            assert_true(np.all(np.abs(y_pred - y[:10]) < 0.3))\n", "def test_RadiusNeighborsRegressor_outlier_labeling():\n    # Test radius-based regressor when no neighbors found and outliers are labeled.\n\n    X = np.array([[1.0, 1.0], [2.0, 2.0], [0.99, 0.99],\n                  [0.98, 0.98], [2.01, 2.01]])\n    y = np.array([1.0, 2.0, 1.0, 1.0, 2.0])\n    radius = 0.2\n    z = np.array([[1.1, 1.1], [2.0, 2.0]])\n\n    z1 = np.array([[1.01, 1.01], [2.01, 2.01]])  # no outliers\n    correct_labels1 = np.array([1.0, 2.0])\n\n    weight_func = _weight_func\n\n    for algorithm in ALGORITHMS:\n        for weights in ['uniform', 'distance', weight_func]:\n            rnn = neighbors.RadiusNeighborsRegressor(radius=radius,\n                                                      weights=weights,\n                                                      algorithm=algorithm,\n                                                      outlier_label=-1)\n            rnn.fit(X, y)\n            assert_array_almost_equal(rnn.predict(z), correct_labels1)\n\n            # test that one outlier is labeled as -1\n            z[1] = [2.01, 2.01]\n            assert_array_almost_equal(rnn.predict(z), [-1.0, 2.0])\n", "def test_NeighborsBase_fit():\n    # Test NeighborsBase to make sure it fits and checks algorithm and metric\n    X = np.array([[0, 1], [1.01, 1.], [2, 0]])\n\n        assert_equal(self._fit_method, 'brute')\n        assert_equal(self.effective_metric_, 'euclidean')\n        assert_equal(self.n_neighbors, 5)\n\n    # Test BallTree\n    nbrs_ball_tree = neighbors.NearestNeighbors(n_neighbors=5,\n                                                algorithm='ball_tree')\n    nbrs_ball_tree.fit(X)\n    check_base_fit(nbrs_ball_tree)\n\n    # Test KDTree\n    nbrs_kdtree = neighbors.NearestNeighbors(n_neighbors=5,\n                                             algorithm='kd_tree')\n    nbrs_kdtree.fit(X)\n    check_base_fit(nbrs_kdtree)\n\n    # Test Brute\n    nbrs_brute = neighbors.NearestNeighbors(n_neighbors=5,\n                                            algorithm='brute')\n    nbrs_brute.fit(X)\n    check_base_fit(nbrs_brute)\n\n    # Test auto\n    nbrs_auto = neighbors.NearestNeighbors(n_neighbors=5, algorithm='auto')\n    nbrs_auto.fit(X)\n    check_base_fit(nbrs_auto)\n\n    # Test no fit\n    nbrs_no_fit = neighbors.NearestNeighbors(n_neighbors=5, algorithm='brute')\n    check_raises_msg(ValueError, nbrs_no_fit._check_algorithm_metric,\n                     \"unrecognized algorithm: 'brute'\")\n"], "sample_34": ["    def test_collision_in_same_model(self):\n        class Model(models.Model):\n            class Meta:\n                unique_together = (('id', 'name'),)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"'id' and 'name' are not unique together for model check_framework.Model.\",\n                id='models.E011',\n            ),\n        ])\n", "    def test_auto_created_pk_with_pk(self):\n        class Model(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_explicit_inherited_pk_overridden_app_config(self):\n        class Parent(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        class Child(Parent):\n            pass\n\n        class ChildConfig(models.apps.AppConfig):\n            default_auto_field = 'django.db.models.BigAutoField'\n\n        class ParentConfig(models.apps.AppConfig):\n            default_auto_field = 'django.db.models.AutoField'\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n", "    def test_proxy_model_clashes(self):\n        class Model(models.Model):\n            class Meta:\n                db_table = 'test_table'\n\n        class ProxyModel(Model):\n            class Meta:\n                proxy = True\n                db_table = 'test_table'\n\n        self.assertEqual(Model._meta.db_table, ProxyModel._meta.db_table)\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [\n            Error(\n                \"db_table 'test_table' is not unique for model \"\n                \"check_framework.Model.\",\n                id='models.E028',\n            ),\n        ])\n", "    def test_non_null_foreign_key_on_non_abstract_model(self):\n        class Model1(models.Model):\n            id = models.AutoField(primary_key=True)\n\n        class Model2(models.Model):\n            model1 = models.ForeignKey(Model1, null=False, on_delete=models.CASCADE)\n\n        class Model3(models.Model):\n            model1 = models.ForeignKey(Model2, null=False, on_delete=models.CASCADE)\n\n        self.assertEqual(checks.run_checks(app_configs=self.apps.get_app_configs()), [])\n"], "sample_123": ["def test_seterr_warnings():\n    assert seterr(divide=False)\n    assert seterr(divide=True)\n    assert seterr(divide=False)\n    # seterr(divide=False) should have reset the cache\n    assert clear_cache() == set()\n", "def test_Infinity_as_coeff():\n    # issue 6255\n    assert oo.as_coeff_Mul(rational=False) == (oo, 1)\n    assert oo.as_coeff_Add(rational=False) == (oo, S.Zero)\n    assert oo.as_coeff_Mul(rational=True) == (1, oo)\n    assert oo.as_coeff_Add(rational=True) == (S.Zero, oo)\n    assert oo.as_coeff_Mul() == (oo, 1)\n    assert oo.as_coeff_Add() == (oo, S.Zero)\n\n    assert -oo.as_coeff_Mul(rational=False) == (1, -oo)\n    assert -oo.as_coeff_Add(rational=False) == (oo, S.Zero)\n    assert -oo.as_coeff_Mul(rational=True) == (-1, oo)\n    assert -oo.as_coeff_Add(rational=True) == (oo, S.Zero)\n    assert -oo.as_coeff_Mul() == (1, -oo)\n    assert -oo.as_coeff_Add() == (oo, S.Zero)\n\n    assert oo.as_coeff_Mul(rational=False) == (oo, 1)\n    assert oo.as_coeff_Add(rational=False) == (oo, S.Zero)\n    assert oo.as_coeff_Mul(rational=True) == (1, oo)\n    assert oo.as_coeff_Add(rational=True) == (S.Zero, oo)\n    assert oo.as_coeff_Mul() == (oo, 1)\n    assert oo.as_coeff_Add() == (oo, S.Zero)\n", "def test_issue_5964():\n    x = Symbol('x', real=True)\n    assert Pow(x, -oo).subs(x, -1) == S.Infinity\n    assert Pow(x, oo).subs(x, -1) == S.Zero\n    assert Pow(x, -oo).subs(x, 1) == S.Infinity\n    assert Pow(x, oo).subs(x, 1) == S.Infinity\n    assert Pow(x, -oo).subs(x, 0) == S.NaN\n    assert Pow(x, oo).subs(x, 0) == S.NaN\n    assert Pow(x, -oo).subs(x, 2) == S.Infinity\n    assert Pow(x, oo).subs(x, 2) == S.Infinity\n    assert Pow(x, -oo).subs(x, -2) == S.Zero\n    assert Pow(x, oo).subs(x, -2) == S.Zero\n    assert Pow(x, oo).subs(x, S.Infinity) == S.Infinity\n    assert Pow(x, -oo).subs(x, S.Infinity) == S.Zero\n    assert Pow(x, oo).subs(x, S.NegativeInfinity) == S.Zero\n    assert Pow(x, -oo).subs(x, S.NegativeInfinity) == S.Infinity\n", "def test_Float_precision():\n    assert Float('1.0', dps=10)._prec == 15\n    assert Float('1.0', precision=10)._prec == 10\n    assert Float('1.0', dps=10, precision=20)._prec == 20\n    assert Float('1.0', precision=10, dps=20)._prec == 20\n    assert Float('1.0', precision=10)._prec == 10\n    assert Float('1.0', precision=None)._prec == 15\n    assert Float('1.0', dps=None)._prec == 15\n    assert Float('1.0', dps=10, precision=None)._prec == 20\n    assert Float('1.0', precision=10, dps=None)._prec == 20\n    assert Float('1.0', dps=10, precision=10)._prec == 10\n    assert Float('1.0', precision=10, dps=10)._prec == 10\n    assert Float('1.0', dps=10, precision=10)._prec == 10\n    assert Float('1.0', dps='10', precision=None)._prec == 20\n    assert Float('1.0', precision='10', dps=None)._prec == 20\n    assert Float('1.0', dps='10', precision=None)._prec == 20\n    assert Float('1.0', precision='10', dps=None)._prec == 20\n    assert Float('1.0', dps='10', precision='20')._prec == 20\n    assert Float('1.0', precision='10', dps='20')._prec == 20\n    assert Float('1.0', dps='10', precision='20')._prec == 20\n    assert Float('1.0', precision='10', dps='20')._prec == 20\n", "def test_Rational_n():\n    assert Rational(1, 2).n(0) == Rational(0)\n    assert Rational(1, 2).n(1) == Rational(1, 2)\n    assert Rational(1, 2).n(2) == Rational(1, 2)\n    assert Rational(1, 2).n(3) == Rational(1, 2)\n    assert Rational(1, 2).n(4) == Rational(1, 2)\n    assert Rational(1, 2).n(5) == Rational(1, 2)\n    assert Rational(1, 2).n(50) == Rational(1, 2)\n    assert Rational(1, 2).n(53) == Rational(1, 2)\n    assert Rational(1, 2).n(100) == Rational(1, 2)\n\n    assert Rational(1, 2).n(0, Rational(3, 10)) == Rational(0)\n    assert Rational(1, 2).n(1, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(2, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(3, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(4, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(5, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(50, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(53, Rational(3, 10)) == Rational(1, 2)\n    assert Rational(1, 2).n(100, Rational(3, 10)) == Rational(1, 2)\n\n    assert Rational(1, 1000000).n(53) == Rational(1, 1000000)\n    assert Rational(1, 1000000).n(100) == Rational(1, 1000000)\n\n    assert Rational(1, 2).n(0, Float(1.2))"], "sample_149": ["def test_Monomial_div():\n    m = Monomial((3, 4, 1), (x, y, z))\n    n = Monomial((1, 2, 0), (x, y, z))\n\n    assert m.div(n) == (2, 2, 1)\n", "def test_MonomialOps():\n    ops = MonomialOps(3)\n    assert ops.mul() == \"monomial_mul\"\n    assert ops.pow() == \"monomial_pow\"\n    assert ops.ldiv() == \"monomial_ldiv\"\n    assert ops.div() == \"monomial_div\"\n    assert ops.lcm() == \"monomial_lcm\"\n    assert ops.gcd() == \"monomial_gcd\"\n\n    assert ops.mul()(A=(1, 2, 3), B=(4, 5, 6)) == (5, 7, 9)\n    assert ops.pow()(A=(1, 2, 3), k=4) == (4, 8, 12)\n    assert ops.ldiv()(A=(1, 2, 3), B=(4, 5, 6)) is None\n    assert ops.div()(A=(1, 2, 3), B=(4, 5, 6)) is None\n    assert ops.lcm()(A=(1, 2, 3), B=(4, 5, 6)) == (4, 6, 6)\n    assert ops.gcd()(A=(1, 2, 3), B=(4, 5, 6)) == (1, 2, 3)\n", "def test_Monomial_pow():\n    m = Monomial((3, 4, 1), (x, y, z))\n    assert m.as_expr() == x**3*y**4*z\n    assert m.pow(3) == Monomial((9, 12, 3))\n    assert m.pow(-1) == Monomial((0, 0, 0))\n", "def test_Monomial_pow_div():\n    m = Monomial((3, 4, 1), (x, y, z))\n    assert m**(-1) == Monomial((-3, -4, -1))\n    m = Monomial((3, 4, 1))\n    assert m**(-1) == Monomial((-3, -4, -1))\n", "def test_Monomial_pow():\n    m = Monomial((3, 4, 1), (x, y, z))\n    assert m.pow() == (0, 0, 0)\n    assert m**0 == Monomial((0, 0, 0))\n\n    assert monomial_pow((1, 2, 3), -1) == (0, 0, -2)\n\n    m = Monomial((1, 2, 3), (x, y, z))\n    assert m**(-1) == Monomial((-1, -2, -3))\n    assert m**(-2) == Monomial((-2, -4, -6))\n\n    assert m**(1/2) == Monomial((1/2, 1, 3/2))\n    assert m**(1/3) == Monomial((1/3, 2/3, 1))\n"], "sample_46": ["    def setUp(self):\n        self.reference = TableColumns('table', ['first_column', 'second_column'])\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'],\n            lambda column: column.upper(),\n            col_suffixes=['_suffix'],\n            opclasses=['opclass']\n        )\n", "    def setUp(self):\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), col_suffixes=['_suf']\n        )\n", "    def setUp(self):\n            return ', '.join(\"%s_%s_%s\" % (table_name, column_name, suffix) for column_name in column_names)\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), col_suffixes=['_suffix']\n        )\n        self.reference2 = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper()\n        )\n        self.reference3 = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), col_suffixes=['_suffix'], opclasses=['text_pattern_ops']\n        )\n", "    def setUp(self):\n            return '_'.join(column_names) + '_' + suffix\n\n        self.reference = IndexColumns(\n            'table', ['first_column', 'second_column'], lambda column: column.upper(), ('text', 'varchar'), ('varchar', 'text')\n        )\n"], "sample_93": ["def test_getbasetemp_fallback_to_tempdir(pytester: Pytester, monkeypatch) -> None:\n    \"\"\"Test that getbasetemp() fallback to the system's tempdir if basetemp is not given and can't find the given basetemp\"\"\"\n    from tempfile import gettempdir\n    monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", \"/wrong/path\")\n    config = cast(Config, FakeConfig(\"/wrong/path\"))\n    t = TempPathFactory.from_config(config, _ispytest=True)\n    assert t.getbasetemp() == Path(gettempdir()).resolve()\n", "    def test_getbasetemp_with_given_basetemp_clears_it(self, tmp_path_factory: TempPathFactory):\n        config = cast(Config, FakeConfig(tmp_path_factory.getbasetemp()))\n        t = TempPathFactory.from_config(config, _ispytest=True)\n        assert t.getbasetemp() == config.option.basetemp\n        assert not t.getbasetemp().exists()\n        tmp_path_factory.getbasetemp()\n        assert t.getbasetemp() == config.option.basetemp\n        assert not t.getbasetemp().exists()\n", "def test_tmpdir_factory_custom_basetemp_before_session(pytester: Pytester) -> None:\n    \"\"\"Test that a custom basetemp given via --basetemp before session is used instead of creating a new one.\"\"\"\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n            assert tmpdir == '/path/to/custom/basetemp'\n    \"\"\"\n    )\n    pytester.makepyfile(\n        \"\"\"\n        import pytest\n        pytest.main(['--basetemp', '/path/to/custom/basetemp'])\n        pytest.main(['-q', '-ra'])\n    \"\"\"\n    )\n    assert pytester.sysinfo.basetemp == '/path/to/custom/basetemp'\n", "    def test_getbasetemp_from_given_basetemp(self, tmp_path_factory: TempPathFactory, monkeypatch):\n        # Test that when given_basetemp is set, the provided value is used\n        # and the directory is cleared each session\n        original_basetemp = tmp_path_factory.getbasetemp()\n        monkeypatch.setenv(\"PYTEST_DEBUG_TEMPROOT\", str(tmp_path_factory._given_basetemp))\n        new_basetemp = tmp_path_factory.getbasetemp()\n        assert new_basetemp == tmp_path_factory._given_basetemp\n        assert new_basetemp != original_basetemp\n\n        # Test that when given_basetemp is set and it's a symlink, the resolved\n        # path is used\n        symlinked_basetemp = tmp_path_factory._given_basetemp.joinpath(\"symlink\")\n        attempt_symlink_to(symlinked_basetemp, tmp_path_factory._given_basetemp)\n        new_basetemp = tmp_path_factory.getbasetemp()\n        assert new_basetemp.resolve() == tmp_path_factory._given_basetemp.resolve()\n", "def test_tmp_path_factory_handles_invalid_dir_characters_reproducibly(\n    tmp_path_factory: TempPathFactory, monkeypatch: MonkeyPatch"], "sample_16": ["    def test_quote_with_none_value(self):\n        \"\"\"\n        Regression test for #13141: quote() should handle None value.\n        \"\"\"\n        self.assertEqual(quote(None), '_00')\n", "def test_label_for_field_callable_with_form(self):\n    \"\"\"\n    Regression test for #12321\n    \"\"\"\n    class ArticleForm(forms.ModelForm):\n        extra_form_field = forms.BooleanField()\n\n        class Meta:\n            fields = '__all__'\n            model = Article\n\n        return \"callable field\"\n\n    self.assertEqual(\n        label_for_field('extra_form_field', Article, form=ArticleForm()),\n        'Callable field'\n    )\n    self.assertEqual(\n        label_for_field('extra_form_field', Article, form=ArticleForm(), return_attr=True),\n        ('Callable field', test_callable_form_field)\n    )\n", "def test_help_text_for_field(self):\n    \"\"\"\n    Regression test for #14001: help_text_for_field should return help text from model field.\n    \"\"\"\n    field = models.CharField()\n    self.assertEqual(help_text_for_field('test', models.Model), field.help_text)\n\n    class Article(models.Model):\n        test = models.CharField(help_text='Test help text')\n\n    self.assertEqual(help_text_for_field('test', Article), 'Test help text')\n\n    class ArticleForm(forms.ModelForm):\n        class Meta:\n            model = Article\n            fields = '__all__'\n\n    self.assertEqual(help_text_for_field('test', ArticleForm()), 'Test help text')\n\n    class MockModelAdmin:\n            return 'mock model admin help text'\n\n    self.assertEqual(help_text_for_field('test', Article, model_admin=MockModelAdmin()), 'mock model admin help text')\n", "    def test_display_for_field_datetime(self):\n        \"\"\"\n        Regression test for #12550: display_for_field should handle DateTimeField.\n        \"\"\"\n        from django.db import models\n\n        display_value = display_for_field(datetime.datetime.min, models.DateTimeField(), self.empty_value)\n        self.assertEqual(display_value, '1970-01-01 00:00:00')\n", "    def test_quote(self):\n        \"\"\"\n        Test that quote() handles Unicode characters correctly.\n        \"\"\"\n        self.assertEqual(quote('\u00e2'), '\u00e2_00E2')\n        self.assertEqual(quote(''), '_____')\n        self.assertEqual(quote('abc def'), 'abc_def')\n"], "sample_82": ["def test_groupby_assign_kwargs():\n    array = xr.DataArray([1, 2, 3], [(\"x\", [1, 1, 2])])\n    expected = xr.DataArray([3, 3], [(\"x\", [1, 2])])\n    actual = array.groupby(\"x\").assign_coords({\"x\": [0, 0]})\n    assert_identical(expected, actual)\n", "def test_groupby_quantile_out_of_range():\n    # test groupby quantile out of range\n    array = xr.DataArray([1, 2, 3, 4, 5, 6], dims=\"x\")\n    with raises_regex(ValueError, \"quantile must be between 0 and 1\"):\n        array.groupby(\"x\").quantile(2)\n", "def test_groupby_quantile_empty_group():\n    array = xr.DataArray(np.array([1, 2, 3, 4, 5, 6]), dims=[\"x\"], coords={\"x\": []})\n    with pytest.raises(ValueError):\n        array.groupby(\"x\").quantile(0)\n", "def test_groupby_bins_datetime():\n    # test groupby with bins of type datetime\n    ds = xr.Dataset()\n    ds[\"time\"] = xr.DataArray(\n        pd.date_range(\"2010-08-01\", \"2010-08-15\", freq=\"15min\"), dims=\"time\"\n    )\n    ds[\"val\"] = xr.DataArray(np.ones(*ds[\"time\"].shape), dims=\"time\")\n    time_bins = pd.date_range(start=\"2010-08-01\", end=\"2010-08-15\", freq=\"H\")\n    actual = ds.groupby_bins(\"time\", time_bins).sum()\n    expected = xr.DataArray(\n        48 * np.ones((13,)),\n        dims=[\"time_bins\"],\n        coords={\"time_bins\": pd.cut(ds.time, time_bins).categories},\n    ).to_dataset(name=\"val\")\n    assert_identical(actual, expected)\n", "def test_groupby_fillna():\n    array = xr.DataArray([1, 2, np.nan, 4, 5, np.nan], [(\"x\", [1, 1, 1, 2, 2, 2])])\n    expected = xr.DataArray([1, 2, 2, 4, 5, 5], [(\"x\", [1, 1, 1, 2, 2, 2]])\n    actual = array.groupby(\"x\").fillna(2)\n    assert_identical(expected, actual)\n\n    array = xr.Dataset({\"a\": (\"x\", [1, 2, np.nan, 4, 5, np.nan])}, {\"x\": [1, 1, 1, 2, 2, 2]})\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 2, 4, 5, 5])}, {\"x\": [1, 1, 1, 2, 2, 2]})\n    actual = array.groupby(\"x\").fillna(2)\n    assert_identical(expected, actual)\n\n    array = xr.DataArray([1, 2, np.nan, 4, 5, np.nan], [(\"x\", [1, 1, 1, 2, 2, 2]])\n    array[\"a\"] = (\"x\", [1, 2, np.nan, 4, 5, np.nan])\n    expected = xr.Dataset(\n        {\"a\": (\"x\", [1, 2, 2, 4, 5, 5])}, coords={\"x\": [1, 1, 1, 2, 2, 2]}\n    )\n    actual = array.groupby(\"x\").fillna(2)\n    assert_identical(expected, actual)\n\n    array = xr.Dataset({\"a\": (\"x\", [1, 2, np.nan, 4, 5, np.nan])}, {\"x\": [1, 1, 1, 2, 2, 2]})\n    array[\"a\"] = array[\"a\"].where(array[\"a\"] != np.nan, 2)\n    expected = xr.Dataset({\"a\": (\"x\", [1, 2, 2, 4, 5, 5])}, {\"x\": [1, 1, 1,"], "sample_20": ["    def test_unique_constraint_pointing_to_non_existing_field(self):\n        class Model(models.Model):\n            field1 = models.IntegerField()\n\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(fields=['field2', 'field3'], name='name'),\n                ]\n\n        self.assertEqual(Model.check(databases=self.databases), [\n            Error(\n                \"'constraints' refers to the nonexistent field 'field2'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n            Error(\n                \"'constraints' refers to the nonexistent field 'field3'.\",\n                obj=Model,\n                id='models.E012',\n            ),\n        ])\n", "    def test_ordering_pointing_to_related_model_field_with_lookup(self):\n        class Parent(models.Model):\n            pass\n\n        class Child(models.Model):\n            parent = models.ForeignKey(Parent, models.CASCADE)\n\n            class Meta:\n                ordering = ['parent__name__exact']\n\n        self.assertEqual(Child.check(), [\n            Error(\n                \"'ordering' refers to the nonexistent field, related field, \"\n                \"or lookup 'parent__name__exact'.\",\n                obj=Child,\n                id='models.E015',\n            ),\n        ])\n", "    def test_ordering_pointing_to_lookup_transform(self):\n        class Model(models.Model):\n            test = models.CharField(max_length=100)\n\n            class Meta:\n                ordering = ('test__isnull',)\n\n        self.assertEqual(Model.check(), [])\n", "    def test_check_field(self):\n        class Model(models.Model):\n            field = models.CharField(max_length=255)\n\n        self.assertEqual(Model.check(), [])\n", "    def test_multiple_primary_keys(self):\n        class Model(models.Model):\n            foo = models.IntegerField(primary_key=True)\n            bar = models.IntegerField(primary_key=True)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"The model cannot have more than one field with 'primary_key=True'.\",\n                obj=Model,\n                id='models.E026',\n            ),\n        ])\n"], "sample_136": ["def test_BlockMatrix_as_real_imag():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    X = BlockMatrix([[A, B]])\n    real_matrices, im_matrices = X.as_real_imag()\n    assert isinstance(real_matrices, Matrix)\n    assert isinstance(im_matrices, Matrix)\n    assert real_matrices == Matrix([[re(A), re(B)]])\n    assert im_matrices == Matrix([[im(A), im(B)]])\n", "def test_deblock_block_matrix():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', m, m)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = deblock(X)\n\n    assert Y.shape == X.shape\n    assert Y.blockshape == (2, 2)\n    assert Y.blocks[0, 0] == A\n    assert Y.blocks[0, 1] == B\n    assert Y.blocks[1, 0] == C\n    assert Y.blocks[1, 1] == D\n", "def test_BlockDiagMatrix_blockmul():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', m, m)\n    D = MatrixSymbol('D', m, m)\n    X = BlockDiagMatrix(A, B)\n    Y = BlockDiagMatrix(C, D)\n\n    assert bc_matmul(X * Y).equals(BlockDiagMatrix(A*C, B*D))\n    assert block_collapse(X * Y).equals(BlockDiagMatrix(A*C, B*D))\n\n    X = BlockDiagMatrix(A)\n    Y = BlockDiagMatrix(C)\n    assert bc_matmul(X * Y).equals(BlockDiagMatrix(A*C))\n    assert block_collapse(X * Y).equals(BlockDiagMatrix(A*C))\n\n    X = BlockDiagMatrix(A)\n    Y = BlockMatrix([[C]])\n    assert bc_matmul(X * Y).equals(BlockDiagMatrix(A*C))\n    assert block_collapse(X * Y).equals(BlockDiagMatrix(A*C))\n", "def test_BlockMatrix_colblock():\n    A = MatrixSymbol('A', m, n)\n    B = MatrixSymbol('B', m, n)\n    C = MatrixSymbol('C', l, n)\n    D = MatrixSymbol('D', l, n)\n    X = BlockMatrix([[A, C], [B, D]])\n    assert X.colblocksizes == [n, n]\n    assert X.blocks[0, 1].cols == n\n    assert X.blocks[1, 0].cols == n\n", "def test_BlockMatrix_invertibility():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 2, 2)\n    D = MatrixSymbol('D', 2, 2)\n\n    X = BlockMatrix([[A, ZeroMatrix(3, 2)], [ZeroMatrix(2, 3), BlockDiagMatrix(B, C)]])\n    assert isinstance(X.inverse(), Inverse)\n\n    Y = BlockMatrix([[A, ZeroMatrix(3, 2)], [ZeroMatrix(2, 3), BlockDiagMatrix(B, C).inverse()]])\n    assert isinstance(Y.inverse(), Inverse)\n\n    Z = BlockMatrix([[A, ZeroMatrix(3, 2)], [ZeroMatrix(2, 3), BlockDiagMatrix(B, C, D).inverse()]])\n    assert isinstance(Z.inverse(), Inverse)\n\n    W = BlockMatrix([[A, ZeroMatrix(3, 2)], [ZeroMatrix(2, 3), BlockDiagMatrix(B, C).inverse(), D]])\n    assert isinstance(W.inverse(), Inverse)\n\n    # Test case when one of the diagonal blocks is a zero matrix\n    Z2 = BlockMatrix([[A, ZeroMatrix(3, 2)], [ZeroMatrix(2, 3), BlockDiagMatrix(B, ZeroMatrix(2, 2))])\n    assert isinstance(Z2.inverse(), Inverse)\n\n    # Test case when one of the diagonal blocks is a matrix with determinant zero\n    with assuming(Q.invertible(B)):\n        assert Z2.inverse() != BlockDiagMatrix(A.inverse(), ZeroMatrix(2, 2), ZeroMatrix(2, 2))\n"], "sample_91": ["def test_xfail_run_setup_error(testdir):\n    testdir.makeconftest(\n        \"\"\"\n                0 / 0\n        \"\"\"\n    )\n    p = testdir.makepyfile(\n        \"\"\"\n            import pytest\n            @pytest.mark.xfail\n                assert 0\n        \"\"\"\n    )\n    result = testdir.runpytest(p)\n    assert result.ret == 0\n    assert \"xfailed\" in result.stdout.str()\n    result.stdout.no_fnmatch_line(\"*xpassed*\")\n", "def test_xfail_runxfail_interaction(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n            import pytest\n            @pytest.mark.xfail\n                pass\n    \"\"\"\n    )\n    result = testdir.runpytest(p, \"--runxfail\", \"-rxf\")\n    result.stdout.fnmatch_lines([\"*1 pass*\"])\n", "    def test_raises_with_expected_exception(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=AssertionError, reason=\"invalid value\")\n                assert 0\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"-rx\")\n        result.stdout.fnmatch_lines([\"*XPASS*test_func*\", \"*invalid value*\"])\n        assert result.ret == 0\n", "    def test_xfail_with_expected_raises(self, testdir, raises, expected):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.mark.xfail(raises=%s)\n                raise Exception()\n\n        \"\"\" % raises\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n        assert expected in result.stdout.str()\n", "    def test_xfail_raises(self, testdir, raises, expected):\n        item = testdir.getitem(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=%s)\n                raise ValueError()\n        \"\"\"\n            % raises\n        )\n        reports = runtestprotocol(item, log=False)\n        callreport = reports[1]\n        assert callreport.outcome == \"skipped\"\n        assert callreport.wasxfail == \"condition: %s\" % raises\n        assert callreport.longrepr == \"Expected exception ValueError but got: TypeError\"\n        assert not hasattr(callreport, \"raises\")\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n"], "sample_118": ["compilation error", "def test_ccode_TensorContraction():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols\n    n, m = symbols('n m', integer=True)\n    a = IndexedBase('a')\n    b = IndexedBase('b')\n    i = Idx('i', m)\n    j = Idx('j', n)\n    k = Idx('k', m)\n    c = IndexedBase('c')\n\n    s = (\n        'for (int i=0; i<m; i++){\\n'\n        '   c[i] = 0;\\n'\n        '}\\n'\n        'for (int i=0; i<m; i++){\\n'\n        '   for (int j=0; j<n; j++){\\n'\n        '      for (int k=0; k<m; k++){\\n'\n        '         c[i] += a[%s]*b[%s];\\n' % (i*n + j, i*n + k) +\\\n        '      }\\n'\n        '   }\\n'\n        '}'\n    )\n    c = ccode(a[i, j]*b[i, k], assign_to=c[i])\n    assert c == s\n", "def test_ccode_MatrixElement():\n    A = Matrix([[1, 2], [3, 4]])\n    A_symbol = MatrixSymbol('A', 2, 2)\n    expr = A[1, 1]\n    assert ccode(expr, A_symbol) == 'A[1, 1]'\n\n    expr = A[0, 0] + A[1, 1]\n    assert ccode(expr, A_symbol) == 'A[0, 0] + A[1, 1]'\n\n    expr = A[0, 0] + A[1, 0]\n    assert ccode(expr, A_symbol) == 'A[0, 0] + A[1, 0]'\n\n    expr = A[0, 0] + 1\n    assert ccode(expr, A_symbol) == 'A[0, 0] + 1'\n", "def test_ccode_MatrixElement():\n    from sympy import Matrix, MatrixSymbol, symbols\n    x, y = symbols('x, y')\n    A = MatrixSymbol('A', 3, 3)\n    assert ccode(A[1, 2]) == \"A[1]\"\n    assert ccode(A[1, 2] + 3) == \"A[1] + 3\"\n    assert ccode(A[0, 0] + A[1, 1] + A[2, 2]) == \"A[0] + A[1] + A[2]\"\n", "def test_ccode_For_inline_functions():\n    from sympy.tensor import IndexedBase, Idx\n    from sympy import symbols, exp, sin, cos, log\n    n = symbols('n', integer=True)\n    A = IndexedBase('A')\n    x = IndexedBase('x')\n    i = Idx('i', n)\n    f = For(x, Range(0, n), [aug_assign(A[i], '*', exp(x) + sin(x) + cos(x) + log(x))])\n    sol = ccode(f)\n    expected = (\n        'for (int x=0; x<n; x++){\\n'\n        '   A[x] = pow(exp(x) + sin(x) + cos(x) + log(x), x);\\n'\n        '}\\n'\n    )\n    assert sol == expected\n"], "sample_62": ["    def setUp(self):\n        super().setUp()\n        self.dirname = self.mkdtemp()\n        # Caches location cannot be modified through override_settings /\n        # modify_settings, hence settings are manipulated directly here and the\n        # setting_changed signal is triggered manually.\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = self.mkdtemp()\n        # Caches location cannot be modified through override_settings /\n        # modify_settings, hence settings are manipulated directly here and the\n        # setting_changed signal is triggered manually.\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = self.mkdtemp()\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def setUp(self):\n        self.dirname = self.mkdtemp()\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n", "    def setUp(self):\n        super().setUp()\n        self.dirname = tempfile.mkdtemp()\n        # Caches location cannot be modified through override_settings /\n        # modify_settings, hence settings are manipulated directly here and the\n        # setting_changed signal is triggered manually.\n        for cache_params in settings.CACHES.values():\n            cache_params[\"LOCATION\"] = self.dirname\n        setting_changed.send(self.__class__, setting=\"CACHES\", enter=False)\n"], "sample_8": ["    def test_callable_setting_as_setting(self):\n        class CallableSetting:\n                return \"This should not be displayed\"\n                return \"repr from the wrapped callable\"\n        with self.settings(DEBUG=True, FOOBAR=CallableSetting()):\n            response = self.client.get('/raises500/')\n            self.assertNotContains(response, \"This should not be displayed\", status_code=500)\n", "    def test_debug_path_encoding(self):\n        \"\"\"\n        Path objects are opened with the encoding specified in the template engine.\n        \"\"\"\n        with mock.patch.object(Path, 'open') as m:\n            with self.settings(DEBUG=True, TEMPLATES=[{'BACKEND': 'django.template.backends.django.DjangoTemplates', 'OPTIONS': {'loaders': ['django.template.loaders.locmem.Loader'], 'DEFAULT_CHARSET': 'ISO-8859-1'}}]):\n                default_urlconf(None)\n                m.assert_called_once_with(encoding='iso-8859-1')\n                m.reset_mock()\n                technical_404_response(mock.MagicMock(), mock.Mock())\n                m.assert_called_once_with(encoding='iso-8859-1')\n", "    def test_non_sensitive(self):\n        \"\"\"\n        Everything (request info, frame variables and exception value) can bee seen\n        in the default 500 error page for non-sensitive requests.\n        \"\"\"\n        with self.settings(DEBUG=True):\n            response = self.client.get('/raises500/')\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'Request Method:', status_code=500)\n            self.assertContains(response, 'Request URL:', status_code=500)\n            self.assertContains(response, 'Exception Type:', status_code=500)\n            self.assertContains(response, 'Exception Value:', status_code=500)\n            self.assertContains(response, 'Traceback (most recent call last):', status_code=500)\n\n        with self.settings(DEBUG=False):\n            response = self.client.get('/raises500/')\n            self.assertEqual(response.status_code, 500)\n            self.assertContains(response, 'Request Method:', status_code=500)\n            self.assertContains(response, 'Request URL:', status_code=500)\n            self.assertContains(response, 'Exception Type:', status_code=500)\n            self.assertContains(response, 'Exception Value:', status_code=500)\n            self.assertContains(response, 'Traceback (most recent call last):', status_code=500)\n", "    def test_reporting_of_frames_without_source_with_cyclic_reference(self):\n        \"\"\"\n        When generating a traceback report, frames without source should not\n        cause a cyclic reference exception (#29393).\n        \"\"\"\n        try:\n                try:\n                    raise RuntimeError('outer') from RuntimeError('inner')\n                except RuntimeError as explicit:\n                    raise exc.__cause__ = RuntimeError('cyclic')\n                    RuntimeError('inner')\n            test_func()\n        except Exception:\n            exc_type, exc_value, tb = sys.exc_info()\n\n        reporter = ExceptionReporter(self.rf.get('/test_view/'), exc_type, exc_value, tb)\n\n            nonlocal tb_frames\n            tb_frames = reporter.get_traceback_frames()\n\n        tb_frames = None\n        tb_generator = threading.Thread(target=generate_traceback_frames, daemon=True)\n        tb_generator.start()\n        tb_generator.join(timeout=5)\n        if tb_generator.is_alive():\n            # tb_generator is a daemon that runs until the main thread/process\n            # exits. This is resource heavy when running the full test suite.\n            # Setting the following values to None makes\n            # reporter.get_traceback_frames() exit early.\n            exc_value.__traceback__ = exc_value.__context__ = exc_value.__cause__ = None\n            tb_generator.join()\n            self.fail('Cyclic reference in Exception Reporter.get_traceback_frames()')\n        if tb_frames is None:\n            # can happen if the thread generating traceback got killed\n            # or exception while generating the traceback\n            self.fail('Traceback generation failed')\n        last_frame = tb_frames[-1]\n        self.assertIn('raise exc.__cause__', last_frame['context_line'])\n        self.assertEqual(last_frame['filename'], __file__)\n        self.assertEqual(last_frame['function'], 'test_func')\n", "    def test_sensitive_post_parameters_without_sensitive_post_parameters_list(self):\n        \"\"\"\n        If no sensitive_post_parameters is specified, all POST parameters\n        are shown in the error reports, even in production environments.\n        \"\"\"\n        with self.settings(DEBUG=False):\n            self.verify_unsafe_response(non_sensitive_view)\n            self.verify_unsafe_email(non_sensitive_view)\n"], "sample_101": ["def test_pipeline_inverse_transform_without_final_estimator():\n    iris = load_iris()\n    X = iris.data\n    pca = PCA(n_components=2, svd_solver='full')\n    pipeline = Pipeline([('pca', pca)])\n    X_trans = pipeline.fit_transform(X)\n    # Check that inverse_transform behaves as expected\n    assert_array_equal(X, pipeline.inverse_transform(X_trans))\n    # Check that fit_transform behaves as expected\n    assert_array_almost_equal(X_trans, pca.fit_transform(X))\n", "def test_pipeline_fit_transform_cache():\n    # Test that fit_transform uses the cache\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    transf = DummyTransf()\n    pca = PCA(svd_solver='full')\n    pipe = Pipeline([('transf', transf), ('pca', pca)],\n                    memory=DummyMemory())\n    # get the timestamp of the transformer\n    ts = transf.timestamp_\n    # Fit the pipeline\n    pipe.fit(X, y)\n    # The timestamp should be updated\n    assert pipe.named_steps['transf'].timestamp_ != ts\n    # The timestamp should not be updated again\n    pipe.fit(X, y)\n    assert pipe.named_steps['transf'].timestamp_ == ts\n    # The timestamp should be updated again after changing the memory\n    pipe.memory = DummyMemory()\n    pipe.fit(X, y)\n    assert pipe.named_steps['transf'].timestamp_ != ts\n", "def test_pipeline_unfitted_transformer():\n    # Test that the pipeline raises an error if a transformer is called\n    # without being fitted\n    iris = load_iris()\n    X = iris.data\n    pipeline = Pipeline([('transf', Transf())])\n    assert_raises_regex(\n        ValueError,\n        \"This pipeline instance has a transformer (transf) that is '\n        'unfitted. You must fit it before calling transform.\",\n        pipeline.transform, X\n    )\n", "def test_transformer_name_validation():\n    # Test that transformer name validation works for FeatureUnion and Pipeline\n    # Test that the names are valid\n    X = np.array([[1, 2]])\n    trans1 = Transf()\n    trans2 = Transf()\n    trans3 = Transf()\n    trans4 = Transf()\n\n    # Test pipeline\n    pipe = Pipeline([('transf1', trans1), ('transf2', trans2)])\n    assert_equal(pipe.named_steps['transf1'], trans1)\n    assert_equal(pipe.named_steps['transf2'], trans2)\n    # Test that invalid name raises an error\n    assert_raise_message(ValueError,\n                        \"Estimator names must not contain __: got ['transf__1']\",\n                        pipe.set_params, transf1='transf__1')\n\n    # Test FeatureUnion\n    fu = FeatureUnion([('transf1', trans1), ('transf2', trans2)])\n    assert_equal(fu.transformer_list[0][0], 'transf1')\n    assert_equal(fu.transformer_list[1][0], 'transf2')\n    # Test that invalid name raises an error\n    assert_raise_message(ValueError,\n                        \"Estimator names must not contain __: got ['transf__1']\",\n                        fu.set_params, transformer_list=[('transf__1', trans3)])\n    # Test that invalid name raises an error\n    assert_raise_message(ValueError,\n                        \"Estimator names must not contain __: got ['transf__2']\",\n                        fu.set_params, transformer_list=[('transf1', trans4),\n                                                        ('transf2', trans3),\n                                                        ('transf__2', trans3)])\n\n    # Test that conflict raises an error\n    assert_raise_message(ValueError,\n                        \"Names provided are not unique: ['transf1', 'transf1']\",\n                        fu.set_params, transformer_list=[('transf1', trans1),\n                                                        ('transf1', trans2)])\n", "def test_feature_union_transformer_types():\n    iris = load_iris()\n    X = iris.data\n    y = iris.target\n    pca = PCA(n_components=2, svd_solver='full')\n    select = SelectKBest(k=1)\n    fs = FeatureUnion([(\"svd\", pca), (\"select\", select)])\n    fs.fit(X, y)\n    X_transformed = fs.transform(X)\n\n    # Test that FeatureUnion correctly detects transformer types\n    assert hasattr(pca, 'fit_transform') and hasattr(pca, 'transform')\n    assert hasattr(select, 'fit_transform') and hasattr(select, 'transform')\n    assert not hasattr(pca, 'fit_predict')\n    assert hasattr(select, 'fit_predict')\n\n    # Test that FeatureUnion raises an error if transformers do not implement fit and transform\n    assert_raises_regex(TypeError,\n                        'All estimators should implement fit and transform.*',\n                        FeatureUnion,\n                        [(\"svd\", pca), (\"select\", NoInvTransf())])\n\n    # Test that FeatureUnion raises an error if some transformers do not implement get_feature_names\n    assert_raises_regex(TypeError,\n                        'Transformer svd (type PCA) does not provide get_feature_names',\n                        FeatureUnion,\n                        [(\"svd\", pca), (\"select\", Transf())])\n\n    # Test that FeatureUnion works correctly with different types of transformers\n    # (transformers with fit_transform, transformers without fit_transform,\n    # transformers with fit_predict)\n    fs = FeatureUnion([(\"svd\", pca), (\"select\", select)])\n    assert_array_almost_equal(X_transformed[:, :-1], pca.fit_transform(X))\n    assert_array_equal(X_transformed[:, -1], select.fit_transform(X, y).ravel())\n\n    # Test that FeatureUnion correctly handles sparse matrices\n    X_sp = sparse.csr_matrix(X)\n    fs = FeatureUnion([(\"svd\", pca), (\"select\", select)])\n    X_sp_transformed = fs.fit_transform(X_sp, y)\n    assert_array_almost_equal(X_transformed, X_sp_transformed.toarray())\n\n    # Test that FeatureUnion raises an error if all transformers are None\n    assert_raises_regex(TypeError,\n                        'All transformers should be objects and not None',\n                        FeatureUnion, [(\"None\", None), (\"None\", None)])\n"], "sample_11": ["def test_serialize_float_infinity_nan(self):\n    self.assertSerializedEqual(float('inf'))\n    self.assertSerializedEqual(float('-inf'))\n    self.assertSerializedEqual(float('nan'))\n    self.assertSerializedEqual(float('inf'))\n    self.assertSerializedResultEqual(\n        float('inf'),\n        (\"float('inf')\", set())\n    )\n    self.assertSerializedResultEqual(\n        float('-inf'),\n        (\"float('-inf')\", set())\n    )\n    self.assertSerializedResultEqual(\n        float('nan'),\n        (\"float('nan')\", set())\n    )\n", "    def test_serialize_functools_partial_with_nested_operations(self):\n        \"\"\"A functools.partial with nested operations.\"\"\"\n        operation = custom_migration_operations.operations.ArgsOperation(\n            custom_migration_operations.operations.ArgsOperation(1, 2),\n            functools.partial(datetime.timedelta, 1, seconds=2)\n        )\n        buff, imports = OperationWriter(operation, indentation=0).serialize()\n        self.assertEqual(imports, {'import custom_migration_operations.operations', 'import datetime', 'import functools'})\n        self.assertEqual(\n            buff,\n            'custom_migration_operations.operations.ArgsOperation(\\n'\n            '    arg1=custom_migration_operations.operations.ArgsOperation(\\n'\n            '        arg1=1,\\n'\n            '        arg2=2,\\n'\n            '    ),\\n'\n            '    arg2=datetime.timedelta(seconds=2),\\n'\n            '),'\n        )\n", "def test_serialize_deconstructible_class(self):\n    \"\"\"Test serialization of a deconstructible class with arguments.\"\"\"\n    class DeconstructibleClass:\n            return ('DeconstructibleClass', ['arg1', 'arg2'], {'kwarg1': 'value1', 'kwarg2': 'value2'})\n\n    class DeconstructibleSerializer(BaseSerializer):\n            return 'deconstructible_class(%r, %r, %r)' % (self.value.args, self.value.kwarg1, self.value.kwarg2), set()\n\n    MigrationWriter.register_serializer(DeconstructibleClass, DeconstructibleSerializer)\n    self.assertSerializedEqual(DeconstructibleClass('arg1', 'arg2', kwarg1='value1', kwarg2='value2'))\n    MigrationWriter.unregister_serializer(DeconstructibleClass)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: DeconstructibleClass'):\n        self.assertSerializedEqual(DeconstructibleClass('arg1', 'arg2', kwarg1='value1', kwarg2='value2'))\n", "def test_serialize_datetime_tzinfo(self):\n    \"\"\"Test serialization of datetime with tzinfo.\"\"\"\n    # Test that tzinfo is correctly serialized\n    self.assertSerializedResultEqual(\n        datetime.datetime(2014, 1, 1, 1, 1, tzinfo=get_default_timezone()),\n        (\"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=get_default_timezone())\", {'import datetime'})\n    )\n    # Test that tzinfo is correctly serialized as None\n    self.assertSerializedResultEqual(\n        datetime.datetime(2012, 1, 1, 1, 1),\n        (\"datetime.datetime(2012, 1, 1, 1, 1)\", {'import datetime'})\n    )\n", "def test_register_serializer_with_order_dependent_type(self):\n    class OrderDependentSerializer(BaseSerializer):\n            return 'order_dependent(%r)' % self.value, {}\n\n    MigrationWriter.register_serializer(tuple, OrderDependentSerializer)\n    self.assertSerializedEqual((1, 2))\n    self.assertSerializedEqual((3, 4))\n    MigrationWriter.unregister_serializer(tuple)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: (5, 6)'):\n        self.assertSerializedEqual((5, 6))\n\n    class OrderDependentSerializer2(BaseSerializer):\n            return 'order_dependent2(%r)' % self.value, {}\n\n    MigrationWriter.register_serializer(int, OrderDependentSerializer2, replace=True)\n    self.assertSerializedEqual(7)\n    self.assertSerializedEqual(8)\n    MigrationWriter.unregister_serializer(int)\n    with self.assertRaisesMessage(ValueError, 'Cannot serialize: 9'):\n        self.assertSerializedEqual(9)\n"], "sample_122": ["def test_simplify():\n    m = SparseMatrix(((1, 0), (0, 1)))\n    assert m.simplify() == m\n    m = SparseMatrix(((x, 0), (0, 1)))\n    assert m.simplify() == m\n    m = SparseMatrix(((x, 0), (0, 0)))\n    assert m.simplify() == SparseMatrix(((0, 0), (0, 0)))\n    m = SparseMatrix(((x, 0), (0, x)))\n    assert m.simplify() == SparseMatrix(((x, 0), (0, x)))\n    m = SparseMatrix(((x, 0), (0, x**2)))\n    assert m.simplify() == SparseMatrix(((x, 0), (0, x**2)))\n    m = SparseMatrix(((x, 0), (0, x + 1)))\n    assert m.simplify() == SparseMatrix(((x, 0), (0, x + 1)))\n    m = SparseMatrix(((x, 0), (0, x + 1)).as_mutable())\n    m = m.simplify()\n    assert m == SparseMatrix(((x, 0), (0, x + 1)))\n", "def test_scalar_multiply():\n    A = SparseMatrix(((25, 15, -5), (15, 18, 0), (-5, 0, 11)))\n    assert A.scalar_multiply(2) == SparseMatrix(((50, 30, -10), (30, 36, 0), (-10, 0, 22)))\n    A = SparseMatrix(((1, 2), (3, 4)))\n    assert A.scalar_multiply(-1) == SparseMatrix((-1, -2), (-3, -4))\n    A = SparseMatrix(((1, 2), (3, 4)))\n    assert A.scalar_multiply(0) == SparseMatrix.zeros(2, 2)\n    A = SparseMatrix(((1, 2), (3, 4)))\n    assert A.scalar_multiply(0) == SparseMatrix.zeros(2, 2)\n    A = SparseMatrix(((1, 2), (3, 4)))\n    assert A.scalar_multiply(0) == SparseMatrix.zeros(2, 2)\n", "def test_diagonal():\n    a = SparseMatrix(((1, 2, 0), (0, 1, 0), (0, 0, 1)))\n    assert a.is_diagonal\n    a[0, 1] = 0\n    assert a.is_diagonal is False\n    a = SparseMatrix(((1, 2, 0), (0, 1, 0), (0, 0, 1)))\n    assert a.diag() == SparseMatrix([1, 1, 1])\n    assert a.diag(1, 1) == SparseMatrix([1, 1, 1])\n    assert a.diag(1, 0) == SparseMatrix([1, 0, 0])\n    a = SparseMatrix(((1, 0, 0), (0, 2, 0), (0, 0, 3)))\n    assert a.diag(1, 0) == SparseMatrix([1, 0, 0])\n    a = SparseMatrix(((1, 2, 3), (4, 5, 6), (7, 8, 9)))\n    assert a.diag() == SparseMatrix([1, 5, 9])\n", "compilation error", "def test_upper_triangular_solve():\n    # test_upper_triangular_solve\n    A = SparseMatrix([[0, -2, 1, 0], [-3, 3, -1, 0], [0, 6, -4, 1], [0, 0, 0, 5]])\n    b = SparseMatrix([[6], [14], [3], [5]])\n    assert A._upper_triangular_solve(b) == SparseMatrix([[0], [2], [3], [1]])\n    A = SparseMatrix([[0, 2, 2], [1, 0, 1], [0, -2, 0]])\n    b = SparseMatrix([[4], [0], [1]])\n    assert A._upper_triangular_solve(b) == SparseMatrix([[0], [2], [-1]])\n\n    # test_upper_triangular_solve with singular matrix\n    A = SparseMatrix([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n    b = SparseMatrix([[1], [1], [1]])\n    try:\n        A._upper_triangular_solve(b)\n        assert False\n    except ValueError:\n        assert True\n\n    # test_upper_triangular_solve with zero matrix\n    A = SparseMatrix.zeros(3, 3)\n    b = SparseMatrix([[1], [2], [3]])\n    assert A._upper_triangular_solve(b) == SparseMatrix([[1], [2], [3]])\n"], "sample_54": ["def test_avoid_wrapping(self):\n    tests = (\n        (\"This is a <b>test</b> with a long word that wraps\", \"This is a <b>test</b> with a long word that wraps\"),\n        (\"This is a <b>test</b> with a short word\", \"This is a <b>test</b> with a short word\"),\n        (\"This is a <b>test</b> with a short word that wraps\", \"This is a <b>test</b> with a short word that wraps\"),\n        (\"This is a <b>test</b> with a short word that wraps and some <i>italic</i> text\", \"This is a <b>test</b> with a short word that wraps and some <i>italic</i> text\"),\n    )\n    for value, output in tests:\n        with self.subTest(value=value, output=output):\n            self.assertEqual(avoid_wrapping(value), output)\n            self.assertEqual(avoid_wrapping(lazystr(value)), output)\n", "    def test_urlizer_trim_url_limit(self):\n        urlizer = Urlizer(trim_url_limit=10)\n        self.assertEqual(urlizer(\"very long url that should be shortened\"), \"very long...</\")\n", "def test_urlize_custom_url(self):\n    urlizer = Urlizer()\n    tests = (\n        (\n            \"http://example.com\",\n            '<a href=\"http://example.com\">http://example.com</a>',\n        ),\n        (\n            \"http://www.example.com\",\n            '<a href=\"http://www.example.com\">http://www.example.com</a>',\n        ),\n        (\n            \"http://example.com/path/to/page\",\n            '<a href=\"http://example.com/path/to/page\">http://example.com/path/to/page</a>',\n        ),\n        (\n            \"http://example.com/path/to/page\",\n            '<a href=\"http://example.com/path/to/page\">http://example.com/path/to/page</a>',\n        ),\n        (\"mailto:foo@example.com\", '<a href=\"mailto:foo@example.com\">mailto:foo@example.com</a>'),\n        (\"ftp://example.com\", '<a href=\"ftp://example.com\">ftp://example.com</a>'),\n    )\n    for value, output in tests:\n        with self.subTest(value=value):\n            self.assertEqual(urlizer(value), output)\n", "def test_avoid_wrapping(self):\n    self.assertEqual(\n        avoid_wrapping(\"This is a test.  This test will have   spaces and tabs  \\t in it.\"),\n        \"This is a test.  This test will have   spaces and tabs  \\t in it.\",\n    )\n\n    self.assertEqual(\n        avoid_wrapping(\"This test has multiple lines.\\nThis line should be\\n next to that line.\"),\n        \"This test has multiple lines.<br>This line should be<br> next to that line.\",\n    )\n\n    self.assertEqual(\n        avoid_wrapping(\"This test has an anchor.<a href='#'>anchor</a> This should be next to the anchor.\"),\n        \"This test has an anchor.<a href='#'>anchor</a> This should be next to the anchor.\",\n    )\n\n    self.assertEqual(\n        avoid_wrapping(\"This test has a span.<span>This should be next to the span.</span>\"),\n        \"This test has a span.<span>This should be next to the span.</span>\",\n    )\n", "def test_avoid_wrapping(self):\n    self.assertEqual(avoid_wrapping(\"foo bar\"), \"foo bar\")\n    self.assertEqual(avoid_wrapping(\"foo  bar\"), \"foo  bar\")\n    self.assertEqual(avoid_wrapping(\" foo bar\"), \" foo bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\n bar\"), \"foo \\n bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\t bar\"), \"foo \\t bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\r\\n bar\"), \"foo \\r\\n bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\v bar\"), \"foo \\v bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\b bar\"), \"foo \\b bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\f bar\"), \"foo \\f bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\u0000 bar\"), \"foo \\u0000 bar\")\n\n    self.assertEqual(avoid_wrapping(\"foo  \\n bar\"), \"foo  \\n bar\")\n    self.assertEqual(avoid_wrapping(\"foo \\t \\n bar\"), \"foo \\t \\n bar\")\n\n    self.assertEqual(avoid_wrapping(\"\\nfoo bar\\n\"), \" \\nfoo bar\\n \")\n\n    # Test that avoid wrapping doesn't double escape existing newlines\n    self.assertEqual(avoid_wrapping(\" foo\\nbar\\n baz\\n\"), \" foo\\nbar\\n baz\\n \")\n\n    # Test that avoid wrapping doesn't mess with the output of urlize\n    self.assertEqual(avoid_wrapping(urlize(\" foo\\nbar\\n baz\\n \")), \" <a href=\\\"http://foo\\nbar\\n baz\\n \\\">foo\\nbar\\n baz\\n </a>\")\n\n    # Test that avoid wrapping doesn't mess with the output of strip_tags\n    self.assertEqual(avoid_wrapping(strip_tags(\" foo\\nbar\\n baz\\n \")), \" foo\\nbar\\n baz\\n \")\n"], "sample_29": ["    def test_output_field_conversions(self):\n        value = Value(10)\n        self.assertEqual(value.output_field, IntegerField())\n\n        value = Value('name')\n        self.assertEqual(value.output_field, CharField())\n\n        value = Value(True)\n        self.assertEqual(value.output_field, BooleanField())\n\n        value = Value(3.14)\n        self.assertEqual(value.output_field, FloatField())\n\n        value = Value(datetime.date(2019, 5, 15))\n        self.assertEqual(value.output_field, DateField())\n\n        value = Value(datetime.datetime(2019, 5, 15, 12, 30, 0))\n        self.assertEqual(value.output_field, DateTimeField())\n\n        value = Value(datetime.time(12, 30, 0))\n        self.assertEqual(value.output_field, TimeField())\n\n        value = Value(datetime.timedelta(1))\n        self.assertEqual(value.output_field, DurationField())\n\n        value = Value(Decimal('3.14'))\n        self.assertEqual(value.output_field, DecimalField())\n\n        value = Value(b'hello')\n        self.assertEqual(value.output_field, BinaryField())\n\n        value = Value(uuid.uuid4())\n        self.assertEqual(value.output_field, UUIDField())\n\n        value = Value(None)\n        self.assertEqual(value.output_field, Field())\n\n        msg = 'Cannot resolve expression type, unknown output_field'\n        with self.assertRaisesMessage(FieldError, msg):\n            Value(object())\n", "    def setUpTestData(cls):\n        cls.example_inc = Company.objects.create(\n            name=\"Example Inc.\", num_employees=2300, num_chairs=5,\n            ceo=Employee.objects.create(firstname=\"Joe\", lastname=\"Smith\", salary=10)\n        )\n        cls.foobar_ltd = Company.objects.create(\n            name=\"Foobar Ltd.\", num_employees=3, num_chairs=4, based_in_eu=True,\n            ceo=Employee.objects.create(firstname=\"Frank\", lastname=\"Meyer\", salary=20)\n        )\n        cls.max = Employee.objects.create(firstname='Max', lastname='Mustermann', salary=30)\n        cls.gmbh = Company.objects.create(name='Test GmbH', num_employees=32, num_chairs=1, ceo=cls.max)\n", "    def setUpTestData(cls):\n        cls.c1 = Company.objects.create(name=\"Company 1\", num_employees=1, num_chairs=1, ceo=Employee.objects.create(firstname=\"John\", lastname=\"Doe\", salary=10))\n        cls.c2 = Company.objects.create(name=\"Company 2\", num_employees=2, num_chairs=2, ceo=Employee.objects.create(firstname=\"Jane\", lastname=\"Smith\", salary=20))\n", "    def setUpTestData(cls):\n        cls.datetime = datetime.datetime(2019, 12, 25, 14, 30, 0)\n        cls.time = datetime.time(14, 30)\n        cls.date = datetime.date(2019, 12, 25)\n        cls.ex = Experiment.objects.create(\n            name='Test Experiment',\n            assigned=cls.date,\n            start=cls.datetime,\n            end=cls.datetime + datetime.timedelta(hours=1),\n            completed=cls.date,\n            estimated_time=datetime.timedelta(hours=1),\n        )\n", "    def setUpTestData(cls):\n        cls.number = Number.objects.create(integer=10, float=123.45)\n"], "sample_37": ["    def test_update(self):\n        Number.objects.create(integer=1)\n        Number.objects.create(integer=2)\n        Number.objects.update(integer=ExpressionList(F('integer') + 1, F('integer') + 2))\n        self.assertQuerysetEqual(\n            Number.objects.all(),\n            [1, 2, 3],\n            lambda n: n.integer,\n            ordered=False\n        )\n", "    def test_equal(self):\n        self.assertEqual(ExpressionList(F('a')), ExpressionList(F('a')))\n        self.assertEqual(ExpressionList(F('a'), F('b')), ExpressionList(F('a'), F('b')))\n        self.assertEqual(ExpressionList(), ExpressionList())\n        self.assertNotEqual(ExpressionList(F('a')), ExpressionList(F('b')))\n        self.assertNotEqual(ExpressionList(F('a'), F('b')), ExpressionList(F('a'), F('c')))\n", "    def setUpTestData(cls):\n        cls.now = datetime.datetime.now()\n        cls.now_minus_1_day = cls.now - datetime.timedelta(days=1)\n        cls.now_minus_1_hour = cls.now - datetime.timedelta(hours=1)\n        cls.now_minus_1_minute = cls.now - datetime.timedelta(minutes=1)\n        cls.now_minus_1_second = cls.now - datetime.timedelta(seconds=1)\n        cls.now_minus_1_microsecond = cls.now - datetime.timedelta(microseconds=1)\n", "    def test_add_expression(self):\n        self.assertEqual(str(ExpressionList(F('a'), F('b'))), '(F(a), F(b))')\n", "    def test_func_update(self):\n        self.assertEqual(Employee.objects.create(firstname='John', lastname='Doe').salary, 10)\n        Employee.objects.update(salary=Func('ceo__salary', function='COALESCE'))\n        self.assertEqual(Employee.objects.get(pk=1).salary, 10)\n"], "sample_56": ["    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error1 = copy(E002)\n        cls.error2 = copy(E002)\n        string_if_invalid1 = cls.TEMPLATES_STRING_IF_INVALID[0][\"OPTIONS\"][\n            \"string_if_invalid\"\n        ]\n        string_if_invalid2 = cls.TEMPLATES_STRING_IF_INVALID[1][\"OPTIONS\"][\n            \"string_if_invalid\"\n        ]\n        cls.error1.msg = cls.error1.msg.format(\n            string_if_invalid1, type(string_if_invalid1).__name__\n        )\n        cls.error2.msg = cls.error2.msg.format(\n            string_if_invalid2, type(string_if_invalid2).__name__\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_same_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_2.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n", "    def setUpClass(cls):\n        super().setUpClass()\n        cls.error_different_library_name_tags = Error(\n            E003.msg.format(\n                \"'same_tags'\",\n                \"'check_framework.template_test_apps.same_tags_app_1.\"\n                \"templatetags.same_tags', \"\n                \"'check_framework.template_test_apps.same_tags_app_3.\"\n                \"templatetags.same_tags'\",\n            ),\n            id=E003.id,\n        )\n"], "sample_88": ["def test_empty_repr():\n    class EmptyClass:\n            pass\n\n    assert saferepr(EmptyClass()) == \"<[unpresentable exception (<class '__main__.EmptyClass'>) \" \\\n                                   \"raised in repr()] EmptyClass object at 0x{:x}>\".format(\n                                       id(EmptyClass())\n                                   )\n", "def test_repr_on_module():\n    class ModuleWithBrokenRepr:\n            raise ValueError(\"broken repr\")\n\n    m = ModuleWithBrokenRepr()\n    s = saferepr(m)\n    assert \"ModuleWithBrokenRepr\" in s\n    assert \"broken\" in s\n", "def test_saferepr_maxsize_with_instance():\n    class A:\n            self.x = x\n\n            return repr(self.x) + \" instance\"\n\n    assert saferepr(A(100), maxsize=25) == _ellipsize(repr(100) + \" instance\", 25)\n", "def test_repr_on_magic_attributes():\n    \"\"\"Test saferepr() with special attributes like __dict__ and __weakref__.\"\"\"\n\n    class MagicObj:\n            raise ValueError(\"Buggy repr!\")\n\n    obj = MagicObj()\n    obj.__dict__ = { \"key\": \"value\" }\n    obj.__weakref__ = weakref.ref(obj)\n\n    assert \"weakref\" in saferepr(obj)\n    assert \"dict\" in saferepr(obj)\n", "def test_repr_on_oldstyle():\n    \"\"\"Test saferepr() with old-style classes, which includes __repr__ method but\n    does not have a __repr__ method in its class info.\n    \"\"\"\n\n    class OldClass:\n            return \"<%s>\" % (self.name)\n\n    oldobj = OldClass()\n    oldobj.name = \"old\"\n    assert saferepr(oldobj) == \"<old>\"\n\n"], "sample_74": ["def test_colorbar_empty_cax():\n    fig, ax = plt.subplots()\n    cax = ax.inset_axes([0, 1.05, 1, 0.05])\n    fig.colorbar(cax=cax)\n", "def test_colorbar_contour_orientation():\n    fig, ax = plt.subplots(figsize=(6, 2))\n    data = np.arange(1200).reshape(30, 40) - 500\n    levels = np.array([0, 200, 400, 600, 800, 1000, 1200]) - 500\n\n    CS = ax.contour(data, levels=levels, extend='both')\n    fig.colorbar(CS, orientation='vertical')\n    fig.colorbar(CS, orientation='horizontal')\n", "def test_colorbar_no_alpha_auto_alpha():\n    \"\"\"\n    Test that the alpha is None when the alpha of the mappable is an array\n    \"\"\"\n    fig, ax = plt.subplots()\n    x = np.arange(1, 5).reshape(2, 2)/4\n    pc = ax.pcolormesh(x, alpha=x)\n    cb = fig.colorbar(pc)\n    assert cb.alpha is None\n", "def test_colorbar_discrete():\n    fig, ax = plt.subplots()\n    colors = ['red', 'green', 'blue']\n    np.random.seed(19680808)\n    Z = np.random.choice(len(colors), size=(10, 10))\n    pc = ax.pcolormesh(Z, cmap='viridis', rasterized=True, norm=NoNorm())\n    cb = fig.colorbar(pc)\n    cb.set_ticks(range(len(colors)))\n    cb.set_ticklabels(colors)\n", "def test_colorbar_get_major_formatter_set_major_formatter(orientation):\n    fig, ax = plt.subplots()\n    pc = ax.pcolormesh([[.05, .95]])\n    cb = fig.colorbar(pc, ax=ax, orientation=orientation)\n    fmt = LogFormatter()\n    cb.formatter = fmt\n    cb.majorformatter = fmt\n    assert cb.formatter is cb.ax.yaxis.get_major_formatter()\n    assert cb.majorformatter is cb.ax.yaxis.get_major_formatter()\n    assert cb.ax.yaxis.get_major_formatter().__class__ is fmt.__class__\n    # test that these don't go out of sync with each other:\n    cb.ax.yaxis.set_major_formatter(LogFormatter())\n    assert cb.formatter is not cb.ax.yaxis.get_major_formatter()\n    cb.formatter = fmt\n    assert cb.majorformatter is not cb.ax.yaxis.get_major_formatter()\n    cb.majorformatter = fmt\n    assert cb.formatter is cb.ax.yaxis.get_major_formatter()\n    cb.formatter = None\n    cb.formatter = fmt\n    assert cb.ax.yaxis.get_major_formatter().__class__ is fmt.__class__\n    # test that setting the formatter to None doesn't trigger a null locator\n    cb.formatter = None\n    assert cb.ax.yaxis.get_major_formatter() is None\n    cb.formatter = fmt\n    cb.majorformatter = None\n    assert cb.ax.yaxis.get_major_formatter() is None\n    cb.formatter = None\n    cb.majorformatter = fmt\n    assert cb.ax.yaxis.get_major_formatter().__class__ is fmt.__class__\n"], "sample_111": ["def test_perfect_match(metric):\n    y_true = [0, 0, 1, 1, 2, 2]\n    y_pred = y_true.copy()\n    assert metric(y_true, y_pred) == pytest.approx(1.0)\n", "def test_fowlkes_mallows_output_range():\n    # Test if Fowlkes-Mallows score returns values in range [0, 1]\n    assert 0 <= fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1]) <= 1\n    assert 0 <= fowlkes_mallows_score([0, 0, 1, 2], [0, 0, 1, 1]) <= 1\n    assert 0 <= fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3]) <= 1\n", "def test_out_of_range_input(metric_name):\n    # All clustering metrics do not change score due to permutations of labels\n    # that is when 0 and 1 exchanged.\n    y_label = np.array([0, 0, 0, 1, 1, 0, 1])\n    y_pred = np.array([1, 0, 1, 0, 1, 1, 0])\n    if metric_name in SUPERVISED_METRICS:\n        metric = SUPERVISED_METRICS[metric_name]\n        score_1 = metric(y_pred, y_label)\n        assert score_1 == pytest.approx(metric(y_pred, y_label * 100))\n        assert score_1 == pytest.approx(metric(y_pred, y_label * 1000))\n    else:\n        metric = UNSUPERVISED_METRICS[metric_name]\n        X = np.random.randint(10, size=(7, 10))\n        score_1 = metric(X, y_pred)\n        assert score_1 == pytest.approx(metric(X, y_pred * 100))\n        assert score_1 == pytest.approx(metric(X, y_pred * 1000))\n\n    if metric_name in SUPERVISED_METRICS:\n        # Check that labels are integers\n        with pytest.raises(ValueError, match='labels must be integers'):\n            metric(y_pred, y_label * 2.5)\n        with pytest.raises(ValueError, match='labels must be integers'):\n            metric(y_pred, y_label + 2.5)\n    else:\n        # Check that X has the correct shape\n        X = np.random.rand(7, 10)\n        with pytest.raises(ValueError, match='Unexpected number of samples'):\n            metric(X, y_pred)\n\n        # Check that X has correct type\n        X = np.array([1, 2, 3], dtype=object)\n        with pytest.raises(TypeError, match='Expected a 2D array-like'):\n            metric(X, y_pred)\n\n", "def test_raised_on_type_error(metric_name):\n    # all clustering metrics are type agnostic\n    metric = SUPERVISED_METRICS[metric_name] if metric_name in SUPERVISED_METRICS \\\n        else UNSUPERVISED_METRICS[metric_name]\n    with pytest.raises(TypeError):\n        metric(\"a\", [1, 2, 3])\n", "def test_input_shape(metric_name):\n    # Only some metrics accept 1D arrays or single values\n    metric = SUPERVISED_METRICS[metric_name]\n    valid_input_shape = (1,)\n    invalid_input_shape = (2,)\n    valid_single_input = 1\n    invalid_single_input = 2\n    # Test if the function accepts 1D arrays\n    if metric_name in [\"adjusted_mutual_info_score\",\n                      \"adjusted_rand_score\",\n                      \"completeness_score\",\n                      \"homogeneity_score\",\n                      \"mutual_info_score\",\n                      \"normalized_mutual_info_score\",\n                      \"v_measure_score\"]:\n        assert metric(valid_input_shape) == metric(np.array([0, 0]))\n    # Test if the function accepts single values\n    if metric_name in [\"adjusted_mutual_info_score\",\n                      \"adjusted_rand_score\",\n                      \"completeness_score\",\n                      \"homogeneity_score\",\n                      \"mutual_info_score\",\n                      \"normalized_mutual_info_score\",\n                      \"v_measure_score\"]:\n        assert metric(valid_single_input) == metric([valid_single_input])\n    # Test if the function raises an error for 2D input shape\n    with pytest.raises(ValueError):\n        metric(invalid_input_shape)\n    # Test if the function raises an error for 2D single input\n    with pytest.raises(ValueError):\n        metric(invalid_single_input)\n"], "sample_47": ["def test_migrate_backward_to_squashed_migration_and_then_forwards(self):\n    executor = MigrationExecutor(connection)\n    try:\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        executor.migrate([('migrations', '0001_squashed_0002')])\n        self.assertTableExists('migrations_author')\n        self.assertTableExists('migrations_book')\n        executor.loader.build_graph()\n        # Migrate backward to a squashed migration.\n        executor.migrate([('migrations', '0001_initial')])\n        self.assertTableExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        # Now migrate forwards again.\n        executor.migrate([('migrations', '0002_second')])\n        self.assertTableExists('migrations_author')\n        self.assertTableExists('migrations_book')\n        self.assertTableNotExists('migrations_tribble')\n    finally:\n        # Unmigrate everything.\n        executor = MigrationExecutor(connection)\n        executor.migrate([('migrations', None)])\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        self.assertTableNotExists('migrations_tribble')\n", "def test_migrate_with_cyclic_dependencies(self):\n    executor = MigrationExecutor(connection)\n    self.assertTableNotExists('migrations_dependency_cyclic_c')\n    self.assertTableNotExists('migrations_dependency_cyclic_b')\n    self.assertTableNotExists('migrations_dependency_cyclic_a')\n    executor.migrate([\n        ('migrations', '0001_initial'),\n        ('migrations', '0002_add_dependency_b'),\n        ('migrations', '0003_add_dependency_c'),\n    ])\n    self.assertTableExists('migrations_dependency_cyclic_c')\n    self.assertTableExists('migrations_dependency_cyclic_b')\n    self.assertTableExists('migrations_dependency_cyclic_a')\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    executor.migrate([\n        ('migrations', None),\n    ])\n    self.assertTableNotExists('migrations_dependency_cyclic_c')\n    self.assertTableNotExists('migrations_dependency_cyclic_b')\n    self.assertTableNotExists('migrations_dependency_cyclic_a')\n", "def test_migration_dependencies(self):\n    executor = MigrationExecutor(connection)\n    try:\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        # Migrate forwards\n        executor.migrate([('migrations', '0001_initial')])\n        self.assertTableExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n\n        # Migrate to a migration that depends on the migration that was just\n        # applied.\n        with self.assertRaisesMessage(DatabaseError, 'Cannot resolve dependency to migration '\n                                             'path() because some dependencies are missing'):\n            executor.migrate([('migrations', '0002_dependencies')])\n\n        # Rebuild the graph to reflect the new DB state\n        executor.loader.build_graph()\n    finally:\n        # Cleanup\n        executor.migrate([('migrations', None)])\n        self.assertTableNotExists('migrations_author')\n        self.assertTableNotExists('migrations_book')\n", "def test_empty_plan_with_replacements(self):\n    \"\"\"\n    An empty plan should be generated when there are replacements and not much to do.\n\n    This is somewhat tricky because even if a replacement doesn't do anything,\n    it still shows up in the plan.  But when there are replacements and not much\n    else to do, that means an empty plan is the right thing to do.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Make the initial plan, check it\n    plan = executor.migration_plan([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertEqual(\n        plan,\n        [\n            (executor.loader.graph.nodes[\"migrations\", \"0001_initial\"], False),\n            (executor.loader.graph.nodes[\"migrations\", \"0002_second\"], False),\n            (executor.loader.graph.nodes[\"migrations2\", \"0001_initial\"], False),\n        ],\n    )\n    # Fake-apply all migrations\n    executor.migrate([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\")\n    ], fake=True)\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Make a replacement squashed migration\n    executor.recorder.record_applied(\"migrations\", \"0002_second\")\n    # Now plan a second time and make sure it's empty\n    plan = executor.migration_plan([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertEqual(plan, [])\n    # The resulting state should include applied migrations.\n    state = executor.migrate([\n        (\"migrations\", \"0002_second\"),\n        (\"migrations2\", \"0001_initial\"),\n    ])\n    self.assertIn(('migrations', 'book'), state.models)\n    self.assertIn(('migrations', 'author'), state.models)\n    self.assertIn(('migrations2', 'otherauthor'), state.models)\n    # Erase all the fake records\n    executor.recorder.record_unapplied(\"migrations2\", \"0001_initial\")\n    executor.recorder.record_unapplied(\"migrations\", \"0002_second\")\n    executor.recorder.record_unapplied(\"migrations", "def test_migrate_handles_dependencies_with_earlier_unapplied_migrations(self):\n    \"\"\"\n    Test that the migration executor correctly handles the case where an app has\n    dependencies with earlier unapplied migrations.\n    \"\"\"\n    executor = MigrationExecutor(connection)\n    # Were the tables there before?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n    # Apply migrations up to the point where the dependencies are added\n    executor.migrate([(\"migrations\", \"0001_dependencies\")])\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n\n    # Migrate forward. This should not raise a LookupError because we're\n    # migrating to a migration that has no dependencies yet, or where all\n    # dependencies are already applied.\n    executor.migrate([(\"migrations\", \"0002_dependencies\")])\n\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Migrate backward to the point where the dependencies are added. This\n    # should not raise a LookupError because we're migrating to a migration that\n    # has dependencies with earlier unapplied migrations.\n    executor.migrate([(\"migrations\", \"0001_dependencies\")])\n\n    # Rebuild the graph to reflect the new DB state\n    executor.loader.build_graph()\n    # Migrate backward to the initial migration. This should not raise a\n    # LookupError because we're migrating to a migration that has no dependencies.\n    executor.migrate([(\"migrations\", None)])\n    # Are the tables gone?\n    self.assertTableNotExists(\"migrations_author\")\n    self.assertTableNotExists(\"migrations_tribble\")\n"], "sample_75": ["def test_imagegrid_cbar_location_top_bottom():\n    arr = np.arange(16).reshape((4, 4))\n\n    fig = plt.figure(figsize=(18, 9))\n\n    directions = ['row', 'column']\n    cbar_locations = ['top', 'bottom']\n\n    for direction, location in zip(directions, cbar_locations):\n        grid = ImageGrid(fig, 111,\n                         nrows_ncols=(2, 2),\n                         direction=direction,\n                         cbar_location=location,\n                         cbar_size='20%',\n                         cbar_mode='edge')\n        ax1, ax2, ax3, ax4 = grid\n\n        ax1.imshow(arr, cmap='nipy_spectral')\n        ax2.imshow(arr.T, cmap='hot')\n        ax3.imshow(np.hypot(arr, arr.T), cmap='jet')\n        ax4.imshow(np.arctan2(arr, arr.T), cmap='hsv')\n\n        # In each column/row, the \"first\" colorbars must be overwritten by the\n        # \"second\" ones.  To achieve this, clear out the axes first.\n        for ax in grid:\n            ax.cax.cla()\n            cb = ax.cax.colorbar(ax.images[0])\n", "def test_axesgrid_colorbar_mode_each_label_mode_L():\n    fig = plt.figure()\n    grid = AxesGrid(fig, 111, nrows_ncols=(1, 1),\n                    label_mode=\"L\", cbar_mode=\"each\", cbar_location=\"left\")\n    Z = 10000 * np.random.rand(10, 10)\n    im = grid[0].imshow(Z, interpolation=\"nearest\", norm=LogNorm())\n    for ax, cax in zip(grid, grid.cbar_axes):\n        ax.imshow(np.zeros((10, 10)), interpolation=\"nearest\")\n        cax.colorbar(im)\n        assert cax.get_visible()\n", "def test_axesgrid_colorbar_single_location_deprecation_warning():\n    imdata = np.arange(100).reshape((10, 10))\n\n    fig = plt.figure()\n    with pytest.warns(mpl.MatplotlibDeprecationWarning, match=\"Passing an undefined\"):\n        grid = ImageGrid(fig, 111, nrows_ncols=(1, 1), cbar_mode=\"single\",\n                         cbar_location=\"top\", cbar_size=\"10%\",\n                         label_mode=None)\n    for ax, cax in zip(grid, grid.cbar_axes):\n        im = ax.imshow(imdata, interpolation='nearest')\n        cax.colorbar(im)\n", "def test_axesgrid_aspect():\n    fig = plt.figure()\n    grid = AxesGrid(fig, 111, nrows_ncols=(2, 2), aspect=False)\n    ax = grid[0]\n    ax.imshow([[1, 2]], interpolation='nearest')\n    with pytest.raises(mpl.errorlocker.RuntimeError):\n        ax.imshow([[1, 2]], aspect='auto')\n", "def test_grid_axes_pad_errors(axes_pad, error, message):\n    fig = plt.figure()\n    with pytest.raises(error, match=message):\n        Grid(fig, 111, (2, 2), axes_pad=axes_pad)\n"], "sample_147": ["def test_Function_kind():\n    assert Function('f').kind is UndefinedKind\n    assert Function('f', nargs=1).kind is UndefinedKind\n    assert Function('f', nargs=(1, 2)).kind is UndefinedKind\n    assert Function('f', nargs=None).kind is UndefinedKind\n    assert Function('f', commutative=True).kind is UndefinedKind\n    assert Function('f', commutative=False).kind is UndefinedKind\n    assert Function(comm_x).kind is NumberKind\n    assert Function(noncomm_x).kind is UndefinedKind\n", "def test_Derivative_func():\n    assert Derivative(comm_x).func is S\n    assert Derivative(comm_x, comm_x).func is S\n    assert Derivative(noncomm_x).func is noncomm_x\n    assert Derivative(noncomm_x, noncomm_x).func is noncomm_x\n", "def test_WildFunction_kind():\n    F = WildFunction('F')\n    assert F(1).kind is UndefinedKind\n    assert F(noncomm_x).kind is UndefinedKind\n    assert F(comm_x).kind is NumberKind\n    assert F(1 + 2).kind is UndefinedKind\n    assert F(noncomm_x + 1).kind is UndefinedKind\n    assert F(comm_x + 1).kind is NumberKind\n", "compilation error", "def test_WildFunction():\n    from sympy.core.function import Function, WildFunction, FunctionClass\n    from sympy.abc import x\n    F = WildFunction('F')\n    f = Function('f')\n    g = Function('g')\n\n    assert F.match(F) == {F: F}\n    assert f(x).match(F) == {F: f(x)}\n    assert g(x).match(F) == {F: g(x)}\n\n    F_2 = WildFunction('F', nargs=2)\n    assert F_2.match(F) is None\n    assert f(x).match(F_2) == {F_2: f(x)}\n    assert g(x, y).match(F_2) == {F_2: g(x, y)}\n    assert f(x, y).match(F_2) == {F_2: f(x, y)}\n\n    F_ = WildFunction('F', nargs=(1, 2))\n    assert F_.match(F) is None\n    assert f(x).match(F_) == {F_: f(x)}\n    assert f(x, y).match(F_) == {F_: f(x, y)}\n    assert f(x, y, z).match(F_) is None\n\n    assert WildFunction('F').matches(F) is None\n    assert WildFunction('F', nargs=1).matches(f(x)) == {F: f(x)}\n    assert WildFunction('F', nargs=1).matches(g(x)) == {F: g(x)}\n    assert WildFunction('F', nargs=2).matches(f(x)) is None\n    assert WildFunction('F', nargs=1).matches(f(x, y)) is None\n    assert WildFunction('F', nargs=(1, 2)).matches(f(x)) == {F: f(x)}\n    assert WildFunction('F', nargs=(1, 2)).matches(f(x, y)) == {F: f(x, y)}\n\n    assert WildFunction('F').nargs is S.Naturals0\n    assert WildFunction('F', nargs=1).nargs == FiniteSet(1)\n    assert WildFunction('F', nargs=2).nargs == FiniteSet(2)\n    assert WildFunction('F', nargs=(1, 2)).nargs == FiniteSet(1, 2)\n"], "sample_115": ["def test_set_output_pandas_overrides_index():\n    \"\"\"Check that set_output overrides index.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=[0, 1])\n    est = EstimatorWithSetOutput().set_output(transform=\"pandas\")\n    est.fit(X)\n\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, range(X.shape[0]))\n\n", "def test__wrap_in_pandas_container_callable_columns():\n    \"\"\"Check that columns callable without error.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n\n        return np.asarray([\"f0\", \"f1\", \"f2\"], dtype=object)\n\n    X = np.asarray([[1, 0, 3], [0, 0, 1]])\n    X_wrapped = _wrap_in_pandas_container(X, columns=get_columns)\n    assert isinstance(X_wrapped, pd.DataFrame)\n    assert_array_equal(X_wrapped.columns, get_columns())\n", "def test_set_output_mixin_multiple_transforms():\n    \"\"\"Check that multiple wrapped methods do not interfere with each other.\"\"\"\n\n    class EstimatorWithMultipleTransforms(_SetOutputMixin):\n            return X + 1\n\n            return X + 2\n\n            return np.asarray([f\"X{i}\" for i in range(3)], dtype=object)\n\n    est = EstimatorWithMultipleTransforms().fit(np.asarray([[1, 2, 3]]))\n    est.set_output(transform=\"pandas\")\n\n    X_trans = est.transform(np.asarray([[1, 2, 3]]))\n    assert isinstance(X_trans, np.ndarray)\n    assert_array_equal(X_trans, np.asarray([[2, 3, 4]]))\n\n    X_trans_pd = est.fit_transform(np.asarray([[1, 2, 3]]))\n    assert isinstance(X_trans_pd, np.ndarray)\n    assert_array_equal(X_trans_pd, np.asarray([[2, 3, 4]]))\n", "def test_set_output_mixin_index_errors():\n    \"\"\"Check that `set_output` does not override index.\n\n    Non-regression test for gh-25730.\n    \"\"\"\n\n    pd = pytest.importorskip(\"pandas\")\n\n    class EstimatorWithSetOutputIndex(_SetOutputMixin):\n            self.n_features_in_ = X.shape[1]\n            return self\n\n            return X\n\n            return np.asarray([f\"X{i}\" for i in range(self.n_features_in_)], dtype=object)\n\n    X = pd.DataFrame([[1, 2, 3], [4, 5, 6]], index=[0, 1])\n    est = EstimatorWithSetOutputIndex()\n\n    with pytest.raises(ValueError, match=\"output config must be 'default' or 'pandas'\"):\n        est.set_output(transform=\"bad\")\n\n    with pytest.raises(ValueError, match=\"output config must be 'default' or 'pandas'\"):\n        est.set_output(transform=123)\n\n    est.set_output(transform=None)\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, X.index)\n\n    est.set_output(transform=\"pandas\")\n    X_trans = est.transform(X)\n    assert_array_equal(X_trans.index, [\"s0\", \"s1\"])\n", "def test__get_output_config_custom_config():\n    \"\"\"Check _get_output_config works with custom config.\"\"\"\n    global_config = get_config()[\"transform_output\"]\n    config = _get_output_config(\"transform\")\n    assert config[\"dense\"] == global_config\n\n    # with estimator=None, the global config is used\n    config = _get_output_config(\"transform\")\n    assert config[\"dense\"] == global_config\n\n    # Set a custom config for the estimator\n    class CustomEstimator(_SetOutputMixin):\n            self._sklearn_output_config = {\"transform\": \"custom\"}\n\n            return X\n\n            return input_features\n\n    est = CustomEstimator()\n    config = _get_output_config(\"transform\", est)\n    assert config[\"dense\"] == \"custom\"\n\n    # Changing the config in the estimator should update the config\n    est.set_output(transform=\"pandas\")\n    config = _get_output_config(\"transform\", est)\n    assert config[\"dense\"] == \"pandas\"\n"], "sample_126": ["def test_issue_9347():\n    # test if -oo**-oo returns oo\n    assert (-oo)**(-oo) == oo\n", "compilation error", "def test_Rational_str():\n    assert str(Rational(1, 1)) == \"1\"\n    assert str(Rational(-1, -1)) == \"1\"\n    assert str(Rational(1, -1)) == \"-1\"\n    assert str(Rational(-1, 1)) == \"-1\"\n    assert str(Rational(1, 2)) == \"1/2\"\n    assert str(Rational(-1, 2)) == \"-1/2\"\n    assert str(Rational(2, 1)) == \"2\"\n    assert str(Rational(-2, 1)) == \"-2\"\n    assert str(Rational(-1, 2)) == \"-1/2\"\n    assert str(Rational(2, -2)) == \"-1\"\n    assert str(Rational(2, -1)) == \"-2\"\n    assert str(Rational(2, 3)) == \"2/3\"\n    assert str(Rational(-2, 3)) == \"-2/3\"\n    assert str(Rational(-2, -3)) == \"2/3\"\n    assert str(Rational(-2, 1)) == \"-2\"\n    assert str(Rational(-2, -1)) == \"2\"\n    assert str(Rational(1, 1/2)) == \"2\"\n    assert str(Rational(-1, 1/2)) == \"-2\"\n    assert str(Rational(1, -1/2)) == \"-2\"\n    assert str(Rational(-1, -1/2)) == \"2\"\n    assert str(Rational(1, 3/2)) == \"2/3\"\n    assert str(Rational(-1, 3/2)) == \"-2/3\"\n    assert str(Rational(1, -3/2)) == \"-2/3\"\n    assert str(Rational(-1, -3/2)) == \"2/3\"\n    assert str(Rational(1, 1)) == \"1\"\n    assert str(Rational(-1, 1)) == \"-1\"\n    assert str(Rational(1, 0)) == \"oo\"\n    assert str(Rational(-1, 0)) == \"-oo\"\n    assert str(Rational(0, 1)) == \"0\"\n    assert str(Rational(0, -1)) == \"0\"\n    assert str(Rational(0, 2))", "def test_issue_10215():\n    from sympy import S, Rational\n    from sympy.core.numbers import RationalConstant\n    from sympy.core.numbers import IntegerConstant\n    from sympy.core.numbers import Zero\n    from sympy.core.numbers import One\n    from sympy.core.numbers import NegativeOne\n    from sympy.core.numbers import Half\n\n    assert issubclass(RationalConstant, Zero)\n    assert issubclass(IntegerConstant, Zero)\n    assert Zero.is_number is True\n    assert One.is_number is True\n    assert NegativeOne.is_number is True\n    assert Half.is_number is True\n    assert issubclass(One, IntegerConstant)\n    assert issubclass(NegativeOne, IntegerConstant)\n    assert RationalConstant.__new__(RationalConstant) is S.Half\n    assert IntegerConstant.__new__(IntegerConstant) is S.One\n    assert Zero.__new__(Zero) is S.Zero\n    assert One.__new__(One) is S.One\n    assert NegativeOne.__new__(NegativeOne) is S.NegativeOne\n\n    assert Zero.is_positive is False\n    assert Zero.is_negative is False\n    assert Zero.is_zero is True\n    assert Zero.is_real is True\n\n    assert One.is_positive is True\n    assert One.is_negative is False\n    assert One.is_zero is False\n    assert One.is_real is True\n\n    assert NegativeOne.is_positive is False\n    assert NegativeOne.is_negative is True\n    assert NegativeOne.is_zero is False\n    assert NegativeOne.is_real is True\n\n    assert Half.is_positive is True\n    assert Half.is_negative is False\n    assert Half.is_zero is False\n    assert Half.is_real is True\n\n    assert Zero.is_integer is True\n    assert Zero.is_rational is True\n    assert Zero.is_irrational is False\n\n    assert One.is_integer is True\n    assert One.is_rational is True\n    assert One.is_irrational is False\n\n    assert NegativeOne.is_integer is True\n    assert NegativeOne.is_rational is True\n    assert NegativeOne.is_irrational is False\n\n    assert Half.is_integer is False\n    assert Half.is_rational is True\n    assert Half.is_irrational is False\n\n    assert Zero.is_algebraic is False\n    assert Zero.is_transcendental is True\n\n    assert One.is_algebraic is False\n    assert One.is_transcendental is False\n\n    assert", "def test_Float_rounding_mode():\n    import decimal\n    from decimal import getcontext, ROUND_HALF_UP, ROUND_HALF_EVEN\n    # Default rounding mode should be ROUND_HALF_UP\n    assert getcontext().getrounding() == ROUND_HALF_UP\n    a = Float(0.2, 10)\n    assert str(a.evalf(10, rounding=ROUND_HALF_UP)) == str(0.2)\n    # No rounding should be done on Float's used as arguments to round\n    assert round(a, 10) == a\n    # This should round to 0.2, not 0.2\n    assert round(a, 10, ROUND_HALF_EVEN) == 0.2\n    # Test that round can handle Float's with more than 15 digits of precision\n    a = Float(0.1, 100)\n    assert round(a, 10) == 0.1\n    assert round(a, 10, ROUND_HALF_EVEN) == 0.1\n"], "sample_138": ["def test_BlockMatrix_determinant_2x2():\n    A = MatrixSymbol('A', 2, 2)\n    B = MatrixSymbol('B', 2, 2)\n    X = BlockMatrix([[A, B]])\n    assert det(X) == det(A)*det(B - A.I*B)\n", "def test_BlockMatrix_Identity():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', m, m)\n    C = MatrixSymbol('C', l, l)\n    D = MatrixSymbol('D', l, l)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[Identity(n), B], [C, D]])\n\n    assert X.is_Identity == False\n    assert Y.is_Identity == True\n\n    assert Y.is_structurally_symmetric\n    assert not X.is_structurally_symmetric\n\n    # Test that equals method works\n    assert X.equals(Y)\n    assert Y.equals(X)\n\n    assert X.structurally_equal(Y)\n    assert Y.structurally_equal(X)\n", "def test_BlockMatrix_equality():\n    A = MatrixSymbol('A', n, m)\n    B = MatrixSymbol('B', n, m)\n    C = MatrixSymbol('C', m, n)\n    D = MatrixSymbol('D', m, m)\n    X = BlockMatrix([[A, B], [C, D]])\n    Y = BlockMatrix([[A, B], [C, D]])\n\n    assert X.equals(Y)\n    assert Y.equals(X)\n\n    Z = BlockMatrix([[A, B], [D, C]])\n    assert not X.equals(Z)\n\n    W = BlockMatrix([[A+B, B], [C, D]])\n    assert not X.equals(W)\n\n    assert (X.equals(X + X))\n    assert (X.equals(X - X))\n    assert (X.equals(X * X))\n\n    assert (X.equals(X.transpose()))\n    assert (X.equals(X.inverse()))\n", "def test_BlockMatrix_det():\n    A = MatrixSymbol('A', 3, 3)\n    B = MatrixSymbol('B', 2, 2)\n    C = MatrixSymbol('C', 3, 3)\n    X = BlockMatrix([[A, B], [C, ZeroMatrix(2, 2)]])\n    assert det(X) == det(A) * det(ZeroMatrix(2, 2))\n", "def test_blockmul_BlockDiagMatrix():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', n, n)\n    E = MatrixSymbol('E', n, n)\n    F = MatrixSymbol('F', n, n)\n\n    # Test that BlockDiagMatrix can multiply with a MatrixExpr\n    X = BlockDiagMatrix(A, B, C)\n    Y = BlockDiagMatrix(D, E, F)\n    assert (X * Y).equals(BlockDiagMatrix(A * D, B * E, C * F))\n\n    # Test that BlockDiagMatrix can multiply with a BlockMatrix\n    X = BlockDiagMatrix(A, B, C)\n    Y = BlockMatrix([[D], [E], [F]])\n    assert (X * Y).equals(BlockDiagMatrix(A * D, B * E, C * F))\n\n    # Test that BlockDiagMatrix can multiply with a BlockDiagMatrix\n    X = BlockDiagMatrix(A, B, C)\n    Y = BlockDiagMatrix(D, E, F)\n    assert (X * Y).equals(BlockDiagMatrix(A * D, B * E, C * F))\n\n    # Test that BlockDiagMatrix can multiply with a BlockDiagMatrix with different block sizes\n    X = BlockDiagMatrix(A, B)\n    Y = BlockDiagMatrix(C, D, E)\n    assert (X * Y).equals(BlockDiagMatrix(A * C, B * D, 0))\n\n    # Test that BlockDiagMatrix can multiply with a BlockDiagMatrix and a normal MatrixExpr\n    X = BlockDiagMatrix(A, B)\n    Y = BlockDiagMatrix(C, D, E)\n    Z = MatrixSymbol('Z', n, n)\n    assert (X * Y * Z).equals(BlockDiagMatrix(A * C * Z, B * D * Z, 0))\n"], "sample_117": ["def test_restify_type_hints_NewType():\n    MyInt = NewType('MyInt', int)\n    assert restify(MyInt) == \":class:`MyInt`\"\n", "def test_restify_type_hints_NewType():\n    MyInt = NewType('MyInt', int)\n    assert restify(MyInt) == \":class:`MyInt`\"\n    assert restify(NewType) == \":class:`NewType`\"\n    assert restify(NewType[int]) == \":class:`NewType`\"\n", "def test_stringify_type_hints_GenericMeta():\n    from typing import GenericMeta  # type: ignore\n\n    class BrokenGenericMeta(GenericMeta):\n        pass\n\n    assert stringify(BrokenGenericMeta) == 'tests.test_util_typing.BrokenGenericMeta'\n\n    class MyGenericMeta(GenericMeta, int):\n        pass\n\n    assert stringify(MyGenericMeta) == 'tests.test_util_typing.MyGenericMeta'\n", "def test_restify_type_hints_ParameterizedType():\n    T = TypeVar('T')\n    assert restify(Parameterized) == \":obj:`Parameterized`\"\n    assert restify(Parameterized[int, T]) == \":class:`Parameterized`\\\\ [[:class:`int`, :obj:`tests.test_util_typing.T`]]\"\n", "def test_stringify_type_hints_Subscript():\n    assert stringify(List[str]) == \"List[str]\"\n    assert stringify(Tuple[int, int]) == \"Tuple[int, int]\"\n    assert stringify(Dict[str, int]) == \"Dict[str, int]\"\n    assert stringify(Tuple[int, ...]) == \"Tuple[int, ...]\"\n    assert stringify(List[Dict[str, int]]) == \"List[Dict[str, int]]\"\n    assert stringify(MyList[Tuple[str, int]]) == \"tests.test_util_typing.MyList[Tuple[str, int]]\"\n    assert stringify(Generator[None, str, int]) == \"Generator[None, str, int]\"\n"], "sample_63": ["    def test_invalid_password(self):\n        user = User.objects.get(username=\"testclient\")\n        data = {\"password1\": \"test123\", \"password2\": \"test\"}\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(\n            form[\"password2\"].errors,\n            [str(form.error_messages[\"password_mismatch\"])],\n        )\n", "    def test_custom_form(self):\n        class CustomAdminPasswordChangeForm(AdminPasswordChangeForm):\n            class Meta(AdminPasswordChangeForm.Meta):\n                model = ExtensionUser\n                fields = (\"password\",)\n\n        user = User.objects.get(username=\"testclient\")\n        data = {\n            \"password1\": \"test123\",\n            \"password2\": \"test123\",\n        }\n        form = CustomAdminPasswordChangeForm(user, data)\n        self.assertTrue(form.is_valid())\n        form.save(commit=False)\n        self.assertEqual(form.cleaned_data[\"password\"], data[\"password1\"])\n        self.assertEqual(form.changed_data, [\"password\"])\n", "    def test_render_with_unknown_algorithm(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"unknown_algorithm$unknown_data\"\n        self.assertHTMLEqual(\n            widget.render(\"name\", value, {\"id\": \"id_password\"}),\n            '<div id=\"id_password\">'\n            \"    <strong>algorithm</strong>: <bdi>unknown_algorithm</bdi>\"\n            \"    <strong>hash</strong>: \"\n            \"       <bdi>unknown_data</bdi>\"\n            \"</div>\",\n        )\n", "    def test_bug_19349_bound_password_field_rendering(self):\n        user = User.objects.get(username=\"testclient\")\n        form = UserChangeForm(instance=user)\n        password_field = form[\"password\"]\n        self.assertEqual(password_field.value(), \"\")\n        self.assertEqual(password_field.widget.attrs[\"readonly\"], \"readonly\")\n        password_field.initial = \"old password\"\n        self.assertEqual(password_field.value(), \"old password\")\n        self.assertEqual(password_field.widget.attrs[\"readonly\"], \"readonly\")\n", "    def test_render_with_usable_password(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = \"pbkdf2_sha256$100000$a6Pucb1qSFcD$WmCkn9Hqidj48NVe5x0FEM6A9YiOqQcl/83m2Z5u\"\n        self.assertHTMLEqual(\n            widget.render(\"name\", value, {\"id\": \"id_password\"}),\n            '<div id=\"id_password\">'\n            \"    <strong>algorithm</strong>: <bdi>pbkdf2_sha256</bdi>\"\n            \"    <strong>iterations</strong>: <bdi>100000</bdi>\"\n            \"    <strong>salt</strong>: <bdi>a6Pucb******</bdi>\"\n            \"    <strong>hash</strong>: \"\n            \"       <bdi>WmCkn9**************************************</bdi>\"\n            \"</div>\",\n        )\n"], "sample_31": ["    def test_command_with_no_startup(self, select):\n        with captured_stdout() as stdout:\n            call_command('shell', interface='python', no_startup=True)\n        self.assertEqual(stdout.getvalue().strip(), 'True')\n", "def test_shell_with_python_not_installed(self, select):\n    select.return_value = ([], [], [])\n    with self.assertRaisesMessage(CommandError, \"Couldn't import python interface.\"):\n        call_command('shell', interface='python')\n", "    def test_shell_with_ipython_not_installed_and_startup_option(self):\n        select.return_value = ([], [], [])\n        with self.assertRaisesMessage(CommandError, \"Couldn't import ipython interface.\"):\n            call_command('shell', interface='ipython', no_startup=True)\n", "    def test_no_startup_files(self):\n        with captured_stdout() as stdout, captured_stdin() as stdin:\n            with mock.patch.dict('os.environ', {'PYTHONSTARTUP': ''}):\n                call_command('shell')\n        self.assertEqual(stdout.getvalue().strip(), '')\n", "def test_shell_with_python_interface_found(self, select):\n    select.return_value = ([], [], [])\n    with captured_stdout() as stdout:\n        call_command('shell', interface='python')\n    self.assertEqual(stdout.getvalue().strip(), 'True')\n"], "sample_81": ["def test_notes_rgx_with_message(self) -> None:\n    code = \"\"\"a = 1\n            # @TODO this should trigger a fixme\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"TODO this should trigger a fixme\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "    def test_ascii_encoding(self) -> None:\n        code = \"\"\"a = 1\n                # FIXME message\n                \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_module(self._ast_from_source(code))\n", "def test_notes_rgx(self) -> None:\n    code = \"\"\"a = 1\n            # ???\n            \"\"\"\n    with self.assertAddsMessages(\n        MessageTest(msg_id=\"fixme\", line=2, args=\"???\", col_offset=17)\n    ):\n        self.checker.process_tokens(_tokenize_str(code))\n", "    def test_dont_trigger_on_pylint_disable_clause(self) -> None:\n        code = \"\"\"\n        # pylint: disable=FIXME\n        a = 1\n        \"\"\"\n        with self.assertNoMessages():\n            self.checker.process_tokens(_tokenize_str(code))\n", "    def test_tokenizing_comment_with_newline(self) -> None:\n        code = \"\"\"a = 1"], "sample_114": ["def test_class_distribution_with_multilabel_sequences():\n    y = np.array([[\"a\", \"b\"], [\"c\", \"d\"]])\n    # Define the sparse matrix with a mix of implicit and explicit zeros\n    data = np.array([1, 2, 1, 4, 2, 1, 0, 2, 3, 2, 3, 1, 1, 1, 1, 1, 1])\n    indices = np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 5, 0, 1, 2, 3, 4, 5])\n    indptr = np.array([0, 6, 11, 11, 17])\n    y_sp = sp.csc_matrix((data, indices, indptr), shape=(6, 4))\n\n    with pytest.raises(ValueError):\n        class_distribution(y)\n    with pytest.raises(ValueError):\n        class_distribution(y_sp)\n", "def test_class_distribution_empty_class():\n    # Test empty class\n    y = np.array([[1, 0], [1, 0], [1, 0]])\n    (classes, n_classes, class_prior) = class_distribution(y)\n    expected_classes = [[1], [0]]\n    expected_n_classes = [1, 1]\n    expected_class_prior = [[1.0], [1.0]]\n\n    for k in range(y.shape[1]):\n        assert_array_equal(classes[k], expected_classes[k])\n        assert_array_equal(n_classes[k], expected_n_classes[k])\n        assert_array_equal(class_prior[k], expected_class_prior[k])\n\n    y = np.array([[1, 0], [0, 0], [1, 1]])\n    (classes, n_classes, class_prior) = class_distribution(y)\n    expected_classes = [[1], [0], [1]]\n    expected_n_classes = [1, 1, 1]\n    expected_class_prior = [[1.0], [1.0], [1.0]]\n\n    for k in range(y.shape[1]):\n        assert_array_equal(classes[k], expected_classes[k])\n        assert_array_equal(n_classes[k], expected_n_classes[k])\n        assert_array_equal(class_prior[k], expected_class_prior[k])\n\n    # Test empty class with sample weights\n    y = np.array([[1, 0], [1, 0], [1, 0]])\n    (classes, n_classes, class_prior) = class_distribution(y, [1.0, 2.0, 1.0])\n    expected_classes = [[1], [0]]\n    expected_n_classes = [1, 1]\n    expected_class_prior = [[2 / 3], [1 / 3]]\n\n    for k in range(y.shape[1]):\n        assert_array_equal(classes[k], expected_classes[k])\n        assert_array_equal(n_classes[k], expected_n_classes[k])\n        assert_array_equal(class_prior[k], expected_class_prior[k])\n", "def test_type_of_target_invalid_input_shape():\n    # Test that type_of_target raises an error when the input array has a shape\n    # that does not match the expected types.\n    for y_type in EXAMPLES.keys():\n        for example in EXAMPLES[y_type]:\n            # Check that type_of_target raises a ValueError when the input array\n            # has more than 3 dimensions.\n            if issparse(example) or hasattr(example, \"__array__\"):\n                with pytest.raises(ValueError):\n                    type_of_target(example.reshape(-1, -1, -1))\n            else:\n                with pytest.raises(ValueError):\n                    type_of_target(np.asarray(example).reshape(-1, -1, -1))\n\n            # Check that type_of_target raises a ValueError when the input array\n            # has a 0-dimensional shape.\n            with pytest.raises(ValueError):\n                type_of_target(np.asarray(example).reshape(()))\n\n            # Check that type_of_target raises a ValueError when the input array\n            # has a shape with a single dimension of size 0.\n            with pytest.raises(ValueError):\n                type_of_target(np.asarray(example).reshape((0,)))\n", "def test_type_of_target_multiclass_multioutput():\n    for group, group_examples in EXAMPLES.items():\n        if group != \"multiclass-multioutput\":\n            for example in group_examples:\n                msg = \"type_of_target should return 'multiclass-multioutput'\"\n                with pytest.raises(AssertionError, match=msg):\n                    assert type_of_target(example) == \"multiclass-multioutput\"\n        else:\n            for example in group_examples:\n                assert type_of_target(example) == \"multiclass-multioutput\"\n\n    y_true = np.array([[1, 0, 0, 2], [1, 4, 2, 4]])\n    y_true[np.array([1, 2, 3])] = 0\n    assert type_of_target(y_true) == \"multiclass-multioutput\"\n\n    y_true = np.array([[1, 0, 0, 2]])\n    y_true[np.array([1, 2, 3])] = 0\n    assert type_of_target(y_true) == \"multiclass\"\n\n    y_true = np.array([[1, 0, 2, 2]])\n    y_true[np.array([1, 3])] = 0\n    assert type_of_target(y_true) == \"multiclass\"\n\n    y_true = np.array([[1, 0, 2, 2], [1, 4, 2, 4]])\n    y_true[np.array([1, 3])] = 0\n    assert type_of_target(y_true) == \"multiclass-multioutput\"\n\n    y_true = np.array([[1, 0, 2, 2]])\n    y_true[np.array([2, 3])] = 0\n    assert type_of_target(y_true) == \"binary\"\n\n    y_true = np.array([[1, 0, 2, 2], [1, 4, 2, 4]])\n    y_true[np.array([2, 3])] = 0\n    assert type_of_target(y_true) == \"binary\"\n\n    y_true = np.array([[1, 0, 2, 2], [1, 4, 2, 4]])\n    y_true[np.array([2, 3])] = 0\n    y_true[:, 2] = 0\n    assert type_of_target(y_true) == \"binary\"\n\n    y_true = np.array([[1, ", "def test_type_of_target_invalid_input_type():\n    # Test type_of_target with invalid input types\n\n    for invalid_type in [\n        True,\n        False,\n        None,\n        5,\n        3.14,\n        \"a\",\n        lambda x: x,\n        [1, 2, 3],\n        [1, 2, [3]],\n        [[1], [2], [3]],\n        {1, 2, 3},\n        {\"a\": 1, \"b\": 2},\n        {\"a\": 1, \"b\": 2, \"c\": [3]},\n        {1: 1, 2: 2, 3: 3},\n        (1, 2, 3),\n        frozenset([1, 2, 3]),\n        [frozenset([1, 2, 3]), frozenset([4, 5])],\n        np.array(1),\n        np.array([1, 2]),\n        np.array([[1], [2], [3]]),\n        np.array([1, 2, [3]]),\n        np.array([1, 2, {3: 3}]),\n        np.array([1, 2, (3,)]),\n        np.array([1, 2, frozenset([3])]),\n    ]:\n        msg_regex = r\"Expected array-like \\(array or non-string sequence\\).*\"\n        with pytest.raises(ValueError, match=msg_regex):\n            type_of_target(invalid_type)\n\n    # Test type_of_target with invalid input shapes\n    for invalid_shape in [\n        (),\n        [],\n        [[]],\n        [[[]]],\n        [[[]], [[]]],\n        [[[], []]],\n        [[1], [1], [1]],\n        [[1, 2], [2, 2]],\n        [[1, 2], [2, 3]],\n        [[1, 2], [3, 4]],\n        [[1, 2, 3], [4, 5, 6]],\n        [[1, 2, 3, 4], [5, 6, 7, 8]],\n    ]:\n        msg_regex = r\"Expected array-like \\(array or non-string sequence\\).*\"\n        with pytest.raises(ValueError, match=msg_regex):\n            type_of_target(invalid_shape)\n\n    # Test type_of_target with invalid input dtypes\n    for invalid_dtype in [\""], "sample_130": ["def test_lambdify_sympy_zero():\n    if not sympy:\n        skip(\"sympy not installed.\")\n    f = lambdify(x, 1/sympy.sqrt(x))\n    raises(ZeroDivisionError, lambda: f(0))\n", "def test_lambdify_undefined_function():\n    # Test that an UndefinedFunction instance with no _imp_ attribute raises a ValueError\n    f = sympy.Function('f')\n    raises(ValueError, lambda: lambdify(x, f(x)))\n    # Test that an UndefinedFunction instance with an _imp_ attribute is properly handled by lambdify\n    f = implemented_function('f', lambda x: x+1)\n    f_impl = lambdify(x, f(x))\n    assert f_impl(4) == 5\n", "def test_lambdify_overriding_numpy_functions():\n    # Test if numpy functions are overridden by user provided functions\n    if not numpy:\n        skip(\"numpy not installed.\")\n\n    f = lambdify(x, sin(x), {'sin': lambda x: x**2})\n    assert f(0) == 0\n    assert f(1) == 1\n    f = lambdify(x, cos(x), {'cos': lambda x: 1})\n    assert f(0) == 1\n    f = lambdify(x, tan(x), {'tan': lambda x: 1})\n    assert f(0) == 1\n", "def test_translations_order():\n    # Test that the translation order is correct.\n    # See issue #12092\n    from sympy.utilities.lambdify import implemented_function, lambdify\n\n        return x + y\n    f = implemented_function('f', add)\n    g = implemented_function('g', add)\n    func = lambdify(x, f(x))\n    assert func(3) == 6\n    func = lambdify(x, g(x))\n    assert func(3) == 6\n    func = lambdify(x, f(g(x)))\n    assert func(3) == 9\n    func = lambdify(x, g(f(x)))\n    assert func(3) == 6\n", "def test_lambdify_with_iterable():\n    # Test that lambdify handles iterable arguments correctly\n    from sympy import symbols, sin, sqrt, Matrix\n\n    x, y, z = symbols('x y z')\n    expr = sin(x)*sqrt(y) + Matrix([[x, y], [z, z**2]])\n\n    f = lambdify((x, y, z), expr)\n    assert f(1, 2, 3) == (sin(1)*sqrt(2), Matrix([[1, 2], [3, 9]]))\n\n    f = lambdify((x, y, z), expr, modules=\"numpy\")\n    assert f(1, 2, 3) == (numpy.sin(1)*numpy.sqrt(2), numpy.array([[1, 2], [3, 9]]))\n\n    f = lambdify((x, y, z), expr, modules=\"mpmath\")\n    assert f(1, 2, 3) == (mpmath.sin(1)*mpmath.sqrt(2), mpmath.matrix([[1, 2], [3, 9]]))\n\n    f = lambdify((x, y, z), expr, modules=\"sympy\")\n    assert f(1, 2, 3) == (sin(1)*sqrt(2), Matrix([[1, 2], [3, 9]]))\n\n    f = lambdify((x, y, z), expr, dummify=True)\n    assert f(1, 2, 3) == (sin(Dummy()), Matrix([[Dummy(), 2], [3, Dummy()**2]]))\n"], "sample_131": ["def test_Precedence():\n    assert mcode(x - y + z) == \"x - y + z\"\n    assert mcode(x + y * z) == \"x + y*z\"\n    assert mcode(x - y / z) == \"x - y/z\"\n    assert mcode(x + y**z) == \"x + y^z\"\n    assert mcode(x + y**z**x) == \"x + y^(z^x)\"\n", "def test_Equality():\n    assert mcode(x == y) == \"x == y\"\n    assert mcode(x != y) == \"x != y\"\n    assert mcode(x == y * x) == \"x == y*x\"\n    assert mcode(x == y + x) == \"x == y + x\"\n", "def test_NegativeNumber():\n    assert mcode(-x) == \"-x\"\n    assert mcode(-x + 3) == \"-x + 3\"\n    assert mcode(-x + y) == \"-x + y\"\n    assert mcode(-x * 2) == \"-2*x\"\n    assert mcode(-x / 3) == \"-x/3\"\n    assert mcode(-x**2) == \"-x^2\"\n    assert mcode(-x**2.5) == \"-x^(2.5)\"\n    assert mcode(-1.5 * x) == \"-1.5*x\"\n", "def test_Operator():  # test operators\n    assert mcode(x + y) == \"x + y\"\n    assert mcode(x - y) == \"x - y\"\n    assert mcode(x * y) == \"x*y\"\n    assert mcode(x / y) == \"x/y\"\n    assert mcode(x**y) == \"x^y\"\n    assert mcode(sin(x)*cos(y)) == \"Sin[x]*Cos[y]\"\n    assert mcode((x + 3) - (x + 2)) == \"(x + 3) - (x + 2)\"\n", "def test_Precedence():\n    assert mcode(x + (y**3 + z)) == \"x + y^3 + z\"\n    assert mcode((y**3 + z) + x) == \"y^3 + z + x\"\n    assert mcode(x*y + (z**4 + a)) == \"x*y + z^4 + a\"\n    assert mcode(x - y + z) == \"x - y + z\"\n    assert mcode(x*y - (z**4 + a)) == \"x*y - z^4 - a\"\n    assert mcode(x*y + (z**4 + a) - 3) == \"x*y + z^4 + a - 3\"\n    assert mcode(3 - (x*y + (z**4 + a))) == \"3 - (x*y + z^4 + a)\"\n    assert mcode(x*y + z**4 - a + 3) == \"x*y + z^4 - a + 3\"\n    assert mcode(3 + (x*y + (z**4 + a))) == \"3 + (x*y + z^4 + a)\"\n"], "sample_32": ["    def test_lookup(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.annotate(\n                key=KeyTransform('d', 'value'),\n                expr=KeyTransform('0', 'key'),\n            ).filter(expr__gt=0),\n            [self.objs[4]],\n        )\n", "    def test_json_field_special_values(self):\n        field = models.JSONField()\n        value = '{\"key\": \"value\"}'\n        self.assertEqual(field.to_python(value), {'key': 'value'})\n        value = '{\"key\": null}'\n        self.assertEqual(field.to_python(value), {'key': None})\n        value = '{\"key\": true}'\n        self.assertEqual(field.to_python(value), {'key': True})\n        value = '{\"key\": false}'\n        self.assertEqual(field.to_python(value), {'key': False})\n        value = '{\"key\": 123}'\n        self.assertEqual(field.to_python(value), {'key': 123})\n        value = '{\"key\": 123.456}'\n        self.assertEqual(field.to_python(value), {'key': 123.456})\n        value = '{\"key\": \"123\"}'\n        self.assertEqual(field.to_python(value), {'key': '123'})\n        value = '{\"key\": \"123.456\"}'\n        self.assertEqual(field.to_python(value), {'key': '123.456'})\n        value = '{\"key\": [\"value1\", \"value2\"]}'\n        self.assertEqual(field.to_python(value), {'key': ['value1', 'value2']})\n        value = '{\"key\": {\"subkey\": \"value\"}}'\n        self.assertEqual(field.to_python(value), {'key': {'subkey': 'value'}})\n        value = '{\"key\": {\"subkey\": {\"subsubkey\": \"value\"}}}'\n        self.assertEqual(field.to_python(value), {'key': {'subkey': {'subsubkey': 'value'}}})\n        value = '{\"key\": {}}'\n        self.assertEqual(field.to_python(value), {'key': {}})\n        value = '{\"key\": []}'\n        self.assertEqual(field.to_python(value), {'key': []})\n        value = '{\"key\": {}}'\n        self.assertEqual(field.to_python(value), {'key': {}})\n", "    def setUpTestData(cls):\n        cls.obj1 = NullableJSONModel.objects.create(value={'d': ['e', 'f', 'g']})\n        cls.obj2 = NullableJSONModel.objects.create(value={'d': ['e', 'f', 'h']})\n        cls.obj3 = NullableJSONModel.objects.create(value={'d': ['e', 'i', 'j']})\n", "    def test_key_transform_or(self):\n        self.assertSequenceEqual(\n            NullableJSONModel.objects.filter(value__a__isnull=False | value__a__isnull=True),\n            self.objs[:5]\n        )\n", "    def setUpTestData(cls):\n        cls.objs = [\n            NullableJSONModel.objects.create(value={'f': 'g'}),\n            NullableJSONModel.objects.create(value={'f': 'h'}),\n            NullableJSONModel.objects.create(value={'f': {'g': 'i'}}),\n        ]\n        cls.objs.extend([\n            NullableJSONModel.objects.create(value={'h': 'i'}),\n            NullableJSONModel.objects.create(value={'h': {'i': 'j'}}),\n        ])\n"], "sample_128": ["def test_BuildOptions():\n    opt = Options((x, y), {'domain': 'ZZ'})\n\n    assert opt.gens == (x, y)\n    assert opt.domain == ZZ\n    assert ('order' in opt) is False\n\n    new_opt = build_options((x, y), {'domain': 'ZZ'})\n\n    assert new_opt.gens == (x, y)\n    assert new_opt.domain == ZZ\n    assert ('order' in new_opt) is False\n\n\n    new_opt = build_options({'opt': {'domain': 'ZZ'}})\n\n    assert new_opt.gens == ()\n    assert new_opt.domain == ZZ\n    assert ('order' in new_opt) is False\n", "def test_BuildOptions_preprocess():\n    assert build_options((x, y, z)) == Options((x, y, z), {})\n    assert build_options((x, y, z), {}) == Options((x, y, z), {})\n\n    opt = Options((x, y, z), {'domain': ZZ})\n    assert build_options((x, y, z), {'opt': opt}) is opt\n    assert build_options((x, y, z), {'opt': opt, 'a': 1}) == Options((x, y, z), {'opt': opt, 'a': 1})\n    assert build_options((x, y, z), {'opt': opt, 'a': 1, 'b': 2}) == Options((x, y, z), {'opt': opt, 'a': 1, 'b': 2})\n\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': x}))\n\n    opt = Options((x, y, z), {'domain': ZZ})\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': opt, 'gens': (x, y, z)}))\n\n    opt = Options((x, y, z), {'domain': ZZ})\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': opt, 'gens': (x, y)}))\n", "def test_build_options():\n    opt = {'gens': (x, y, z), 'expand': False, 'order': 'lex', 'field': True}\n\n    assert build_options(opt) == {'gens': (x, y, z), 'expand': False, 'order': lex, 'field': True}\n    assert build_options(opt, {'opt': {'gens': (x, y, z), 'expand': False, 'order': 'lex', 'field': True}}) == opt\n\n    raises(OptionError, lambda: build_options(gens=(x, y, z), args={'expand': False, 'order': 'lex', 'field': True}))\n    raises(OptionError, lambda: build_options(gens=(x, y, z), args={'gens': (x, y, z, w), 'expand': False, 'order': 'lex', 'field': True}))\n", "def test_BuildOptions_preprocess():\n    opt = Options((x, y, z), {'domain': 'ZZ'})\n\n    assert build_options((x, y, z), {'domain': 'ZZ'}) == opt\n\n    assert build_options((x, y, z), {'domain': 'ZZ', 'opt': {'gens': (x, y)}}) == {'gens': (x, y), 'domain': ZZ}\n\n    assert build_options((x, y, z), {'opt': {'gens': (x, y)}}) == {'gens': (x, y)}\n\n    raises(OptionError, lambda: build_options((x, y, z), {'opt': {'gens': x})))\n", "def test_BuildOptions_preprocess():\n    # Test all possible combinations of keyword arguments and ... options\n    # The following tests check if the Options constructor handles all possible\n    # combinations correctly.\n\n    # Test valid keyword arguments\n    opt = Options((x, y, z), {'domain': ZZ})\n    assert opt.domain == ZZ\n\n    opt = Options((x, y, z), {'gens': (x, y, z), 'domain': ZZ})\n    assert opt.domain == ZZ\n\n    opt = Options((x, y, z), {'gens': (x, y), 'domain': ZZ, 'field': True})\n    assert opt.domain == QQ\n\n    opt = Options((x, y, z), {'gens': (x, y, z), 'domain': ZZ, 'field': True, 'greedy': True})\n    assert opt.domain == ZZ\n\n    opt = Options((x, y, z), {'gens': (x, y, z), 'domain': ZZ, 'field': True, 'extension': {sqrt(2)}})\n    assert opt.domain == QQ.algebraic_field(sqrt(2))\n\n    opt = Options((x, y, z), {'gens': (x, y, z), 'domain': ZZ, 'field': True, 'extension': {sqrt(2)}, 'symmetric': True})\n    assert opt.domain == QQ.algebraic_field(sqrt(2), extension={sqrt(2)}, symmetric=True)\n\n    # Test valid ... options\n    opt = build_options((x, y, z), {'domain': ZZ})\n    assert opt.domain == ZZ\n\n    opt = build_options((x, y, z), {'gens': (x, y, z), 'domain': ZZ})\n    assert opt.domain == ZZ\n\n    opt = build_options((x, y, z), {'gens': (x, y), 'domain': ZZ, 'field': True})\n    assert opt.domain == QQ\n\n    opt = build_options((x, y, z), {'gens': (x, y, z), 'domain': ZZ, 'field': True, 'greedy': True})\n    assert opt.domain == ZZ\n\n    opt = build_options((x, y, z), {'gens': (x, y, z), 'domain': ZZ, 'field': True, 'extension': {sqrt(2)}})\n    assert opt.domain == QQ.algebraic_field(sqrt(2))\n\n    opt ="], "sample_144": ["def test_abs_with_mul():\n    assert refine(Abs(x * y), Q.real(x) & Q.real(y)) == Abs(x*y)\n    assert refine(Abs(x * y), Q.real(x) & Q.imaginary(y)) == Abs(x)\n    assert refine(Abs(x * y), Q.imaginary(x) & Q.real(y)) == Abs(y)\n    assert refine(Abs(x * y), Q.imaginary(x) & Q.imaginary(y)) == Abs(x * y)\n\n    assert refine(Abs(x * y), Q.real(x)) == Abs(x*y)\n    assert refine(Abs(x * y), Q.imaginary(x)) == Abs(y)\n", "def test_Pow_issue():\n    x = Symbol('x', real = True)\n    assert refine(x**2, Q.real(x)) == x**2\n    assert refine((-1)**x, Q.even(x)) == 1\n    assert refine((-1)**x, Q.odd(x)) == -1\n    assert refine((-1)**(x + 2), Q.odd(x)) == (-1)**(x + 1)\n    assert refine((-1)**(x + 3), Q.odd(x)) == (-1)**x\n\n    # nested powers\n    assert refine(sqrt(x**2)) != Abs(x)\n    assert refine(sqrt(x**2), Q.complex(x)) != Abs(x)\n    assert refine(sqrt(x**2), Q.real(x)) == Abs(x)\n    assert refine(sqrt(x**2), Q.positive(x)) == x\n    assert refine((x**3)**Rational(1, 3)) != x\n\n    assert refine((x**3)**Rational(1, 3), Q.real(x)) != x\n    assert refine((x**3)**Rational(1, 3), Q.positive(x)) == x\n\n    assert refine(sqrt(1/x), Q.real(x)) != 1/sqrt(x)\n    assert refine(sqrt(1/x), Q.positive(x)) == 1/sqrt(x)\n\n    # powers of (-1)\n    assert refine((-1)**(x + y), Q.even(x)) == (-1)**y\n    assert refine((-1)**(x + y + z), Q.odd(x) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x + y + 1), Q.odd(x)) == (-1)**y\n    assert refine((-1)**(x + y + 2), Q.odd(x)) == (-1)**(y + 1)\n    assert refine((-1)**(x + 3)) == (-1)**(x + 1)\n\n    # continuation\n    assert refine((-1)**((-1)**x/2 - S.Half), Q.integer(x)) == (-1)**x\n    assert refine((-1)**((-1)**x/2 + S.Half), Q.integer(x)) == (-1)**(x + 1)\n    assert refine((-1)**((-1)**x/2 + 5*S.Half), Q.integer(x)) ==", "def test_refine_sqrt():\n    x = Symbol('x', real = True)\n    assert refine(sqrt(x), Q.positive(x)) == x\n    assert refine(sqrt(x), Q.negative(x)) == -x\n    assert refine(sqrt(x**2), Q.complex(x)) == Abs(x)\n    assert refine(sqrt(x**2), Q.real(x)) == Abs(x)\n\n    # Test with expressions as arguments\n    assert refine(sqrt(x + 1), Q.real(x)) == sqrt(x + 1)\n    assert refine(sqrt(x + 1), Q.positive(x)) == sqrt(x + 1)\n    assert refine(sqrt(x + 1), Q.negative(x)) == -sqrt(x + 1)\n\n    # Test with complex arguments\n    assert refine(sqrt(x + I*y), Q.real(x)) == sqrt(x + I*y)\n    assert refine(sqrt(x + I*y), Q.positive(x)) == sqrt(x + I*y)\n    assert refine(sqrt(x + I*y), Q.negative(x)) == -sqrt(x + I*y)\n", "def test_refine_Pow():\n    assert refine((-1)**(x + y + z), Q.real(x) & Q.even(y) & Q.odd(z)) == (-1)**y\n    assert refine((-1)**(x + y + z), Q.real(x) & Q.odd(y) & Q.even(z)) == -(-1)**(y + 1)\n    assert refine((-1)**(x + y + z), Q.real(x) & Q.real(y)) == refine((-1)**(y + z), Q.real(x))\n    assert refine((-1)**(x + y + z), Q.real(x) & Q.real(y) & Q.real(z)) == 1\n\n    assert refine((-1)**(x + y + z), Q.even(x) & Q.even(y) & Q.odd(z)) == refine((-1)**(y + z), Q.even(x))\n    assert refine((-1)**(x + y + z), Q.even(x) & Q.odd(y) & Q.even(z)) == refine((-1)**(y + z), Q.even(x))\n    assert refine((-1)**(x + y + z), Q.even(x) & Q.even(y) & Q.real(z)) == refine((-1)**(y + z), Q.even(x))\n    assert refine((-1)**(x + y + z), Q.even(x) & Q.even(y) & Q.even(z)) == refine((-1)**(y + z), Q.even(x))\n", "def test_Pow_inverse():\n    # Test that the inverse of a power is handled correctly\n    assert refine((-1)**(-x), Q.even(x)) == 1\n    assert refine((-1)**(-x), Q.odd(x)) == 1\n    assert refine((-1)**(-2*x), Q.even(x)) == 1/x**2\n    assert refine((-1)**(-3*x), Q.even(x)) == (-1)**(-x)/x**3\n    assert refine((-1)**(-2*x), Q.odd(x)) == -1/x**2\n    assert refine((-1)**(-3*x), Q.odd(x)) == (-1)**(-x)/(-x)**3\n\n    # Test that the inverse of a power of a rational is handled correctly\n    assert refine((x**3)**(-1/3), Q.real(x)) == 1/x\n    assert refine((x**3)**(-1/3), Q.positive(x)) == 1/x\n\n    # Test that the inverse of a power of an irrational number is handled correctly\n    assert refine((x**3)**Rational(-1,3), Q.real(x)) != 1/x\n\n    # Test that the inverse of a power of a negative number is handled correctly\n    assert refine((-x)**(-2), Q.real(x)) == 1/x**2\n    assert refine((-x)**(-2), Q.positive(x)) == 1/x**2\n    assert refine((-x)**(-3), Q.real(x)) == (-1)**2/x**3\n    assert refine((-x)**(-3), Q.positive(x)) == (-1)**2/x**3\n"], "sample_35": ["    def test_modelchoicefield_with_to_field(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        # ModelChoiceField with to_field\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = ModelChoiceField(\n            queryset=ChoiceModel.objects.all(),\n            to_field_name='name',\n            error_messages=e,\n        )\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n", "    def test_inline_foreign_keyfield(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        # InlineForeignKeyField\n        e = {\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        form = InlineForeignKeyField(parent_instance=ChoiceModel.objects.get(pk=1), error_messages=e)\n        self.assertFormErrors(['INVALID CHOICE'], form.clean, '2')\n        self.assertFormErrors([], form.clean, '1')\n\n        # Test that an InlineForeignKeyField raises an error when it's not attached to a parent instance\n        with self.assertRaises(ValueError):\n            InlineForeignKeyField(error_messages=e)\n", "    def test_modelform(self):\n        f = ModelForm(ChoiceModel)\n        self.assertEqual(f.fields['name'].label, 'name')\n        self.assertEqual(f.fields['name'].help_text, ChoiceModel._meta.get_field('name').help_text)\n", "    def test_modelformset(self):\n        # Create choices for the model choice field tests below.\n        ChoiceModel.objects.create(pk=1, name='a')\n        ChoiceModel.objects.create(pk=2, name='b')\n        ChoiceModel.objects.create(pk=3, name='c')\n\n        # ModelFormSet\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': 'INVALID CHOICE',\n        }\n        f = modelformset_factory(ChoiceModel, error_messages=e)\n        formset = f(formset=BaseModelFormSet, queryset=ChoiceModel.objects.all())\n        self.assertFormErrors(['REQUIRED'], formset.save, formset.initial_data)\n\n        # InlineFormSet\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n            'invalid_list': 'NOT A LIST OF VALUES',\n        }\n        f = inlineformset_factory(ChoiceModel, ChoiceModel, error_messages=e)\n        formset = f(extra=2, instance=ChoiceModel.objects.get(pk=1))\n        self.assertFormErrors(['REQUIRED'], formset.save, formset.initial_data)\n", "    def test_modelform(self):\n        e = {\n            'required': 'REQUIRED',\n            'invalid_choice': '%(value)s IS INVALID CHOICE',\n        }\n        f = modelform_factory(ChoiceModel, error_messages=e)\n        self.assertFormErrors(['REQUIRED'], f.clean, '')\n        self.assertFormErrors(['INVALID CHOICE'], f.clean, '4')\n\n        # Test that fields are not included if not specified in Meta.fields\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, fields=['name'], error_messages=e)\n        self.assertIn('name', f.fields)\n        self.assertNotIn('pk', f.fields)\n\n        # Test that fields are not included if specified in Meta.exclude\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, exclude=['name'], error_messages=e)\n        self.assertNotIn('name', f.fields)\n        self.assertIn('pk', f.fields)\n\n        # Test that fields are correctly sorted\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, fields=['name', 'pk'], error_messages=e)\n        self.assertEqual(list(f.fields.keys()), ['pk', 'name'])\n\n        # Test that ModelChoiceField is used for related fields\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, fields=['name', 'related'], error_messages=e)\n        self.assertIsInstance(f.fields['related'], ModelChoiceField)\n\n        # Test that ModelMultipleChoiceField is used for related fields\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, fields=['name', 'relateds'], error_messages=e)\n        self.assertIsInstance(f.fields['relateds'], ModelMultipleChoiceField)\n\n        # Test that ModelChoiceField is used for related fields when to_field is specified\n        e = {\n            'required': 'REQUIRED',\n        }\n        f = modelform_factory(ChoiceModel, fields=['name', 'related', 'related_to_field'], error_messages=e)\n        self.assertIsInstance(f.fields['related'], ModelChoiceField)\n        self.assertEqual(f.fields['related'].to_field_name, 'related_to_field')\n\n        # Test that ModelMultipleChoiceField is used for related fields when"], "sample_61": ["def test_decimal_sep(self):\n    # Decimal separator.\n    self.assertEqual(nformat(1234, \",\"), \"1234\")\n    self.assertEqual(nformat(1234.2, \",\"), \"1234,2\")\n    self.assertEqual(nformat(1234, \",\", decimal_pos=2), \"1234,00\")\n    self.assertEqual(nformat(1234, \",\", grouping=2, thousand_sep=\".\"), \"1234\")\n    self.assertEqual(\n        nformat(1234, \",\", grouping=2, thousand_sep=\".\", force_grouping=True),\n        \"1,234\",\n    )\n    self.assertEqual(nformat(-1234.33, \",\", decimal_pos=1), \"-1234,3\")\n    # The use_l10n parameter can force thousand grouping behavior.\n    with self.settings(USE_THOUSAND_SEPARATOR=False):\n        self.assertEqual(\n            nformat(1234, \",\", grouping=3, thousand_sep=\".\", use_l10n=False), \"1234\"\n        )\n        self.assertEqual(\n            nformat(1234, \",\", grouping=3, thousand_sep=\".\", use_l10n=True), \"1,234\"\n        )\n", "def test_force_grouping_without_decimal_sep(self):\n    self.assertEqual(\n        nformat(12345, \"\", grouping=3, force_grouping=True),\n        \"12345\",\n    )\n    self.assertEqual(\n        nformat(12345.67, \"\", grouping=3, force_grouping=True),\n        \"12345.670\",\n    )\n    self.assertEqual(\n        nformat(12345, \"\", grouping=3, decimal_pos=2, force_grouping=True),\n        \"12345.00\",\n    )\n    self.assertEqual(\n        nformat(12345.67, \"\", grouping=3, decimal_pos=2, force_grouping=True),\n        \"12345.67\",\n    )\n    self.assertEqual(\n        nformat(12345.6789, \"\", grouping=3, decimal_pos=4, force_grouping=True),\n        \"12345.6789\",\n    )\n    self.assertEqual(\n        nformat(12345678901234567890, \"\", grouping=3, force_grouping=True),\n        \"12,345,678,901,234,567,890\",\n    )\n    self.assertEqual(\n        nformat(12345678901234567890.123, \"\", grouping=3, decimal_pos=2, force_grouping=True),\n        \"12,345,678,901,234,567,890.12\",\n    )\n", "def test_decimal_separator(self):\n    # Test that the decimal separator is correctly used.\n    self.assertEqual(nformat(1234, \",\"), \"1234\")\n    self.assertEqual(nformat(1234.2, \",\"), \"1234,2\")\n    self.assertEqual(nformat(1234, \",\", decimal_pos=2), \"1234,00\")\n    self.assertEqual(\n        nformat(1234, \",\", grouping=2, thousand_sep=\",\", force_grouping=True),\n        \"1,234\",\n    )\n    self.assertEqual(nformat(\"-1234.33\", \",\", decimal_pos=1), \"-1234,3\")\n\n    # Test that the decimal separator is correctly used when using a custom thousand separator.\n    self.assertEqual(\n        nformat(1234, \".\", grouping=2, thousand_sep=\",\", force_grouping=True),\n        \"1,234\",\n    )\n    self.assertEqual(\n        nformat(-1234.33, \".\", grouping=2, thousand_sep=\",\", force_grouping=True),\n        \"-1,234,33\",\n    )\n\n    # Test that the decimal separator is correctly used when using a custom grouping.\n    self.assertEqual(\n        nformat(1234, \".\", grouping=(3, 2), force_grouping=True),\n        \"1,234\",\n    )\n    self.assertEqual(\n        nformat(1234, \".\", grouping=(3, 2), decimal_pos=2, force_grouping=True),\n        \"1,234,00\",\n    )\n", "def test_format_large_number_precision(self):\n    # Test that the format function doesn't exceed the precision of the input number.\n    large_number = Decimal(\"1.234567890123456789012345678901234567890123456789012345678901234567890\")\n    self.assertEqual(nformat(large_number, \".\", decimal_pos=10), \"1.2345678901\")\n    self.assertEqual(nformat(large_number, \".\", decimal_pos=20), \"1.2345678901234567890\")\n    self.assertEqual(nformat(large_number, \".\", decimal_pos=30), \"1.2345678901234567890123456789\")\n    # Test that the format function rounds correctly when the decimal_pos is greater than the number of decimal places.\n    self.assertEqual(nformat(1.2345, \".\", decimal_pos=10), \"1.2\")\n    self.assertEqual(nformat(1.2345, \".\", decimal_pos=20), \"1.2\")\n    self.assertEqual(nformat(1.2345, \".\", decimal_pos=30), \"1.2\")\n", "def test_locale_decimal_sep(self):\n    class LocaleDecimalSeparator(Decimal):\n        \"\"\"\n        Wrapper for Decimal which uses a different decimal separator.\n        \"\"\"\n\n            amount = super().__format__(specifier, **kwargs)\n            return amount.replace(\".\", \",\")\n\n    price = LocaleDecimalSeparator(\"12,34\")\n    self.assertEqual(nformat(price, \",\"), \"12,34\")\n    price = LocaleDecimalSeparator(\"1234\")\n    self.assertEqual(nformat(price, \",\"), \"1.234\")\n    price = LocaleDecimalSeparator(\"1234.56\")\n    self.assertEqual(nformat(price, \",\"), \"1,234.56\")\n\n    # Test for a decimal separator different than the default and a different\n    # thousand separator.\n    self.assertEqual(\n        nformat(\n            LocaleDecimalSeparator(\"1,234.56\"), \".\", grouping=3, thousand_sep=\",\"\n        ),\n        \"1 234,56\",\n    )\n\n    # Test for a case where the input is a float with a decimal separator not equal\n    # to the default.\n    self.assertEqual(\n        nformat(1.234, \",\", decimal_sep=\",\"),\n        \"1,234\",\n    )\n"], "sample_108": ["def test_nu_svr():\n    # Test Support Vector Regression with nu parameter\n\n    diabetes = datasets.load_diabetes()\n    for clf in (svm.NuSVR(kernel='linear', nu=0.5, C=1.0),\n                svm.NuSVR(kernel='linear', nu=0.5, C=10.),\n                svm.SVR(kernel='linear', C=10.)):\n        clf.fit(diabetes.data, diabetes.target)\n        assert clf.score(diabetes.data, diabetes.target) > 0.02\n\n    # non-regression test; previously, BaseLibSVM would check that\n    # len(np.unique(y)) < 2, which must only be done for SVC\n    svm.NuSVR().fit(diabetes.data, np.ones(len(diabetes.data)))\n", "def test_libsvm_fit_intercept_scaling():\n    # Test that fit_intercept=False and intercept_scaling=0 raises an error\n    # in LinearSVC.\n\n    lsvc = svm.LinearSVC(fit_intercept=False, intercept_scaling=0)\n    with pytest.raises(ValueError, match=\"Intercept scaling is .* but needs to be greater than 0.\"):\n        lsvc.fit(X, Y)\n\n    # Test that fit_intercept=False and intercept_scaling>0 doesn't raise an error.\n    lsvc = svm.LinearSVC(fit_intercept=False, intercept_scaling=1)\n    lsvc.fit(X, Y)\n", "def test_libsvm_sparse():\n    # Test that libsvm handles sparse data\n    X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    y = np.array([1, 2, 3])\n    model = svm.libsvm_sparse.fit(X, y)\n\n    # Test prediction\n    assert_array_equal(svm.libsvm_sparse.predict(X, *model), y)\n\n    # Test that libsvm_sparse handles sparse input\n    X_sparse = sparse.csr_matrix(X)\n    assert_array_equal(svm.libsvm_sparse.predict(X_sparse, *model), y)\n\n    # Test that libsvm_sparse handles precomputed kernel\n    K = np.dot(X, X.T)\n    assert_array_equal(svm.libsvm_sparse.predict(K, *model, kernel='precomputed'), y)\n\n    # Test that libsvm_sparse handles precomputed kernel with sparse input\n    K_sparse = sparse.csr_matrix(K)\n    assert_array_equal(svm.libsvm_sparse.predict(K_sparse, *model, kernel='precomputed'), y)\n", "def test_sparse_libsvm_train_shape():\n    # Test sparse LibSVM train method input validation\n\n    # create a very sparse matrix\n    data = [1, 2, 3]\n    indices = [0, 0, 1]\n    indptr = [0, 2, 3]\n    X = sparse.csr_matrix((data, indices, indptr), shape=(3, 3))\n    y = np.array([0, 1, 2])\n\n    with pytest.raises(ValueError):\n        libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices,\n                                          X.indptr, y, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0)\n\n    with pytest.raises(ValueError):\n        libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices,\n                                          X.indptr, y, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0)\n\n    with pytest.raises(ValueError):\n        libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices,\n                                          X.indptr, y, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                                          0, 0, 0, 0)\n\n    with pytest.raises(ValueError):\n        libsvm_sparse.libsvm_sparse_train(X.shape[1], X.data, X.indices,\n                                          X.indptr, y, 0, 0, 0, ", "def test_sparse_kernel():\n    # Test that sparse kernel works with a sparse input and a sparse support vector\n\n    X = np.array([[1, 2], [3, 4]])\n    X_sparse = sparse.csr_matrix(X)\n    y = np.array([0, 1])\n\n    clf = svm.SVC(kernel='rbf', gamma=1)\n    clf.fit(X_sparse, y)\n\n    # Test that the sparse kernel is used\n    assert clf.support_vectors_.dtype == np.float64\n    assert sp.isspmatrix(clf.support_vectors_)\n    assert clf._sparse\n\n    # Test that the predict method works with a sparse input\n    y_pred = clf.predict(sparse.csr_matrix([[1, 2], [3, 4]]))\n    assert_array_equal(y_pred, y)\n\n    # Test that the decision_function method works with a sparse input\n    dec_func = clf.decision_function(sparse.csr_matrix([[1, 2], [3, 4]]))\n    assert_array_equal(dec_func, clf.decision_function(X))\n\n    # Test that the predict_proba method works with a sparse input\n    proba = clf.predict_proba(sparse.csr_matrix([[1, 2], [3, 4]]))\n    assert_array_almost_equal(proba, clf.predict_proba(X))\n"], "sample_141": ["def test_issue_19566():\n    # The root issue is that an argument with dimensions should\n    # not raise an error when the the `arg - 1` calculation is\n    # performed in the assumptions system\n    from sympy.physics.units import meter, second\n    from sympy import Eq\n    assert meter**2.is_positive is None\n    assert meter**2.is_nonnegative is None\n    assert meter**2.is_negative is None\n    assert meter**2.is_algebraic is None\n    assert meter**2.is_rational is None\n    # doesn't raise error\n    assert Eq(meter**2, second**2) is not None  # might be False or unevaluated\n\n    x = Symbol('x')\n    e = meter + x\n    assert e.is_Add and set(e.args) == {meter, x}\n    e = meter + 1\n    assert e.is_Add and set(e.args) == {meter, 1}\n", "def test_quantity_simplify_with_prefixes():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    km = Quantity(\"km\")\n    km.set_global_relative_scale_factor(kilo, meter)\n    u.set_global_relative_scale_factor(kilo, meter)\n    v.set_global_relative_scale_factor(mega, meter)\n\n    assert quantity_simplify(u) == meter\n    assert quantity_simplify(v) == meter\n    assert quantity_simplify(km) == kilo*meter\n    assert quantity_simplify(u * v) == kilo**2 * meter**2\n    assert quantity_simplify(u + v) == (kilo + mega)*meter\n", "def test_quantity_simplify_with_prefixes():\n    u = Quantity('u')\n    u.set_global_relative_scale_factor(1e-3, kilogram)\n    v = Quantity('v')\n    v.set_global_relative_scale_factor(kilo, meter)\n\n    expr = u * v\n    assert quantity_simplify(expr) == 1e-3 * kilogram * kilo * meter\n    assert quantity_simplify(expr).simplify() == 1e-3 * meter**2 / 100\n\n    expr = u + v\n    assert quantity_simplify(expr) == 1e-3 * kilogram + kilo * meter\n    assert quantity_simplify(expr).simplify() == (1e-3 + kilo) * meter\n\n    expr = u / v\n    assert quantity_simplify(expr) == 1e-3 * kilogram / kilo / meter\n    assert quantity_simplify(expr).simplify() == 1e-6\n", "def test_quantity_simplify_with_powers():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert quantity_simplify(u**2) == (10**2)*meter**2\n    assert quantity_simplify(v**-1) == 1/(5*meter)\n    assert quantity_simplify(u*v**2) == (10*5**2)*meter*meter**2\n    assert quantity_simplify(2*u*v) == 2*10*5*meter*meter\n    assert quantity_simplify(2*u**2/v) == (2*10**2)/(5*meter)\n    assert quantity_simplify(w**-2) == 1/(2**-2*second**2)\n    assert quantity_simplify(u*w**-2) == (10*2**-2)*meter/second**2\n\n    u = Quantity(\"u\", abbrev=\"dam\")\n    v = Quantity(\"v\", abbrev=\"dm\")\n    w = Quantity(\"w\", abbrev=\"cm\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), meter)\n\n    assert quantity_simplify(u**2) == 100*meter**2\n    assert quantity_simplify(v**-1) == 1/(5*meter)\n    assert quantity_simplify(u*v**2) == 500*meter**3\n    assert quantity_simplify(2*u*v) == 50*meter**3\n    assert quantity_simplify(2*u**2/v) == 200*meter**3\n    assert quantity_simplify(w**-2) == 1/(2**-2*meter**2)\n    assert quantity_simplify(u*w**-2) == 50*meter**2/meter**2\n", "def test_quantity_simplify_with_units():\n    from sympy.physics.units.prefixes import Prefix\n    from sympy.physics.units.definitions import kilogram, meter, second\n    from sympy.physics.units.quantities import Quantity\n\n    q1 = Quantity(\"q1\")\n    q1.set_global_relative_scale_factor(2, meter**2/second**2)\n    q2 = Quantity(\"q2\")\n    q2.set_global_relative_scale_factor(3, kilogram*meter**2/second**2)\n    q3 = Quantity(\"q3\")\n    q3.set_global_relative_scale_factor(4, kilogram**2/meter**2)\n\n    assert quantity_simplify(q1 + q2) == 6*kilogram*meter**2/second**2\n    assert quantity_simplify(q1 - q2) == -kilogram*meter**2/second**2\n    assert quantity_simplify(q2 + q3) == 15*kilogram**2/meter**2\n\n    # test with prefixes\n    kilo = Prefix(\"kilo\")\n    q4 = Quantity(\"q4\")\n    q4.set_global_relative_scale_factor(kilo, meter)\n    assert quantity_simplify(q1 + q4) == 2*meter**2/second**2\n\n    # test with complex unit\n    q5 = Quantity(\"q5\")\n    q5.set_global_relative_scale_factor(10, kilogram**2 / meter)\n    assert quantity_simplify(q5) == 10*kilogram**2/meter\n"], "sample_142": ["def test_signed_permutations():\n    assert list(signed_permutations((0, 1, 2, 0))) == [\n        (0, 1, 2, 0), (0, 1, -2, 0), (0, 1, 2, 0), (0, 1, -2, 0),\n        (0, -1, 2, 0), (0, -1, -2, 0), (0, -1, 2, 0), (0, -1, -2, 0),\n        (0, 2, 1, 0), (0, 2, -1, 0), (0, 2, 1, 0), (0, 2, -1, 0),\n        (0, -2, 1, 0), (0, -2, -1, 0), (0, -2, 1, 0), (0, -2, -1, 0),\n        (1, 0, 2, 0), (1, 0, -2, 0), (1, 0, 2, 0), (1, 0, -2, 0),\n        (-1, 0, 2, 0), (-1, 0, -2, 0), (-1, 0, 2, 0), (-1, 0, -2, 0),\n        (1, 2, 0, 0), (1, -2, 0, 0), (1, 2, 0, 0), (1, -2, 0, 0),\n        (-1, 2, 0, 0), (-1, -2, 0, 0), (-1, 2, 0, 0), (-1, -2, 0, 0),\n        (1, 2, -0, 0), (1, -2, 0, 0), (1, 2, 0, 0), (1, -2, 0, 0),\n        (2, 1, 0, 0), (2, -1, 0, 0), (2, 1, 0, 0), (2, -", "def test_permute_signs():\n    assert list(permute_signs((0, 1, 2))) == [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs((1, 0, 2))) == [(1, 0, 2), (1, 0, -2), (1, -0, -2), (1, -0, 2)]\n    assert list(permute_signs((0, 0, 0))) == [(0, 0, 0)]\n    assert list(permute_signs((0, 0, 2))) == [(0, 0, 2), (0, 0, -2)]\n    assert list(signed_permutations((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2), (0, 2, 1),\n        (0, -2, 1), (0, 2, -1), (0, -2, -1), (1, 0, 2), (-1, 0, 2),\n        (1, 0, -2), (-1, 0, -2), (1, 2, 0), (-1, 2, 0), (1, -2, 0),\n        (-1, -2, 0), (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0), (2, -1, 0), (-2, -1, 0)]\n", "def test_signed_permutations():\n    from sympy.utilities.iterables import signed_permutations\n    s = (1, 2, 3)\n    assert list(signed_permutations(s)) == [\n        (1, 2, 3), (1, -2, 3), (1, 2, -3), (1, -2, -3),\n        (2, 1, 3), (-2, 1, 3), (2, 1, -3), (-2, 1, -3),\n        (2, -1, 3), (2, 1, 3), (-2, -1, 3), (-2, 1, 3),\n        (3, 1, 2), (-3, 1, 2), (3, 1, -2), (-3, 1, -2),\n        (3, -1, 2), (3, 1, 2), (-3, -1, 2), (-3, 1, 2),\n        (-3, -1, 2), (-3, 1, 2), (3, -1, 2), (3, 1, 2),\n        (2, -1, -3), (2, 1, -3), (-2, -1, -3), (-2, 1, -3)]\n", "def test_ibin_str():\n    assert list(ibin(2)) == [[1, 0], [0, 1]]\n    assert list(ibin(2, 4)) == [[0, 0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 0], [1, 0, 0, 0]]\n    assert list(ibin(2, str=True)) == ['10', '01']\n    assert list(ibin(2, 4, str=True)) == ['0001', '0010', '0100', '1000']\n    assert list(ibin(3, 'all')) == [(1, 1), (1, 0), (0, 1), (0, 0), (0, 0)]\n    assert list(ibin(3, 'all', str=True)) == ['11', '10', '01', '00', '00']\n    assert list(ibin(3, '', str=True)) == ['110', '101', '011', '001']\n    assert list(ibin(3, str=True)) == ['110', '101', '011', '001']\n    assert list(ibin(3, 3, str=True)) == ['011', '010', '001', '000']\n    assert list(ibin(4, str=True)) == ['1111', '1110', '1101', '1100', '1011', '1010', '1001', '1000', '0111', '0110', '0101', '0100', '0011', '0010', '0001', '0000']\n    assert list(ibin(4, 4, str=True)) == ['0011', '0010', '0001', '0000', '0110', '0101', '0100', '0000', '1101', '1100', '1001', '0000', '1110', '1110', '1010', '0000', '1011', '1000', '1000', '0000', '0111', '0000', '0110', '0000', '0011', '0000', '0001', '0000', '0000']\n    raises(ValueError, lambda: ibin(-2))\n   ", "def test_ordered_partitions_with_multiple():\n    from sympy.functions.combinatorial.numbers import nT\n    f = ordered_partitions\n    assert list(f(7, 3, sort=False)) == [[1, 1, 5], [1, 2, 4], [1, 3, 3], [2, 2, 3]]\n    assert list(f(7, 3)) == [[1, 1, 5], [1, 2, 4], [2, 2, 3]]\n    assert list(f(7, 3, sort=True)) == [[1, 1, 5], [1, 2, 4], [2, 2, 3]]\n    assert sum(1 for p in f(7, 3)) == nT(7, 3)\n    assert sum(1 for p in f(7, 3, 0)) == nT(7, 3)\n    assert sum(1 for p in f(7, 3, 1)) == nT(7, 3)\n\n    # TODO: How does `f(7, 3, 1)` behave with multiple-part partitions?\n    # Should it return all partitions with 3 parts or those with at least 3 parts?\n"], "sample_105": ["def test_predict_proba_on_binary_problem():\n    \"\"\"Calculate predicted probabilities on binary toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4]])\n    y = np.array([1, 1])\n\n    clf1_res = np.array([[0.59790391, 0.40209609],\n                         [0.57622162, 0.42377838]])\n\n    clf2_res = np.array([[0.8, 0.2],\n                         [0.8, 0.2]])\n\n    clf3_res = np.array([[0.9985082, 0.0014918],\n                         [0.99845843, 0.00154157]])\n\n    t00 = (2*clf1_res[0][0] + clf2_res[0][0] + clf3_res[0][0]) / 4\n    t11 = (2*clf1_res[1][1] + clf2_res[1][1] + clf3_res[1][1]) / 4\n\n    eclf = VotingClassifier(estimators=[\n                            ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                            voting='soft',\n                            weights=[2, 1, 1])\n    eclf_res = eclf.fit(X, y).predict_proba(X)\n\n    assert_almost_equal(t00, eclf_res[0][0], decimal=1)\n    assert_almost_equal(t11, eclf_res[1][1], decimal=1)\n\n    with pytest.raises(\n            AttributeError,\n            match=\"predict_proba is not available when voting='hard'\"):\n        eclf = VotingClassifier(estimators=[\n                                ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                                voting='hard')\n        eclf.fit(X, y).predict_proba(X)\n", "def test_transform_hard_voting():\n    \"\"\"Check transform method of VotingClassifier on toy dataset with hard voting.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='hard',\n        flatten_transform=True).fit(X, y)\n    assert_array_equal(eclf.transform(X).shape, (4, 3))\n    assert_array_equal(eclf.transform(X),\n                      np.array([[1, 1, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1]]))\n", "def test_transform_softvoting_with_same_class():\n    \"\"\"Check transform method of VotingClassifier with soft voting and same class.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 1, 1])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    assert_array_almost_equal(\n            eclf.transform(X).mean(axis=1),\n            np.array([1., 1., 1., 1.])\n    )\n", "def test_transform_on_toy_problem():\n    \"\"\"Check transform method of VotingClassifier on toy dataset.\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[-1.1, -1.5], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\n    y = np.array([1, 1, 2, 2])\n\n    eclf1 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft').fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    eclf3 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n\n    eclf1_transform = eclf1.transform(X)\n    eclf2_transform = eclf2.transform(X)\n    eclf3_transform = eclf3.transform(X)\n\n    # Test if the shape of transform output is correct\n    assert_array_equal(eclf1_transform.shape, (4, 6))\n    assert_array_equal(eclf2_transform.shape, (4, 6))\n    assert_array_equal(eclf3_transform.shape, (3, 4, 2))\n\n    # Test if the result of transform is correct\n    assert_array_almost_equal(eclf1_transform,\n                              eclf2_transform)\n    assert_array_almost_equal(\n            eclf3_transform.swapaxes(0, 1).reshape((4, 6)),\n            eclf2_transform\n    )\n\n    # Test if the result of transform is correct when voting is 'hard'\n    eclf1.set_params(voting='hard')\n    eclf2.set_params(voting='hard')\n    eclf3.set_params(voting='hard')\n    eclf1_transform_hard = eclf1.transform(X)\n    eclf2_transform_hard", "def test_transform_shape_with_flat_transform():\n    \"\"\"Check that flatten_transform has effect on shape of transform output\"\"\"\n    clf1 = LogisticRegression(random_state=123)\n    clf2 = RandomForestClassifier(random_state=123)\n    clf3 = GaussianNB()\n    X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n    y = np.array([1, 1, 1, 2, 2, 2])\n\n    eclf = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=True).fit(X, y)\n    eclf2 = VotingClassifier(estimators=[\n        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n        voting='soft',\n        flatten_transform=False).fit(X, y)\n\n    assert_array_equal(eclf.transform(X).shape, (6, 6))\n    assert_array_equal(eclf2.transform(X).shape, (3, 6, 2))\n    assert_array_almost_equal(eclf.transform(X), eclf2.transform(X).reshape((6, 6)))\n"], "sample_53": ["def test_remove_proxy_order_with_respect_to(self):\n    \"\"\"Removing the model used in order_with_respect_to does things in the right order.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.author_with_book_order_wrt],\n        [self.author_empty],\n    )\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterOrderWithRespectTo\", \"RemoveField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"author\", order_with_respect_to=None\n    )\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 1, model_name=\"author\", name=\"book\"\n    )\n\n    changes = self.get_changes(\n        [self.author_empty, self.book, self.author_with_book_order_wrt],\n        [self.book, self.author_empty],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"RemoveField\", \"AlterOrderWithRespectTo\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"author\", order_with_respect_to=None\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"book\"\n    )\n\n    changes = self.get_changes(\n        [self.author_empty, self.book, self.author_empty, self.author_with_book_order_wrt],\n        [self.book, self.author_empty],\n    )\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"RemoveField\", \"AlterOrderWithRespectTo\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"author\", order_with_respect_to=None\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"book\"\n    )\n", "def test_add_alter_managers_with_remove(self):\n    \"\"\"Changing the model managers with a migration that also removes the model\"\"\"\n    before = self.make_project_state([self.author_empty, self.author_empty])\n    after = self.make_project_state(\n        [\n            self.author_empty,\n            self.author_empty,\n            ModelState(\n                \"testapp\",\n                \"Author\",\n                [\n                    (\"id\", models.AutoField(primary_key=True)),\n                    (\"name\", models.CharField(max_length=200)),\n                    (\"book\", models.ForeignKey(\"otherapp.Book\", models.CASCADE)),\n                ],\n                managers=[\n                    (\"food_qs\", FoodQuerySet.as_manager()),\n                    (\"food_mgr\", FoodManager(\"a\", \"b\")),\n                    (\"food_mgr_kwargs\", FoodManager(\"x\", \"y\", 3, 4)),\n                ],\n            ),\n        ]\n    )\n    autodetector = MigrationAutodetector(before, after)\n    changes = autodetector._detect_changes()\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterModelManagers\", \"DeleteModel\"])\n    self.assertOperationAttributes(changes, \"testapp\", 0, 0, name=\"author\")\n    self.assertEqual(\n        [name for name, mgr in changes[\"testapp\"][0].operations[0].managers],\n        [\"food_qs\", \"food_mgr\", \"food_mgr_kwargs\"],\n    )\n    self.assertOperationAttributes(changes, \"testapp\", 0, 1, name=\"Author\")\n\n", "def test_remove_field_with_renamed_field(self):\n    \"\"\"\n    When a field is removed and the field is renamed at the same time, the\n    removal operation should depend on the alteration of the field before it is\n    removed.\n    \"\"\"\n    changes = self.get_changes(\n        [self.author_empty, self.author_name],\n        [self.author_empty, self.author_renamed],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"testapp\", 1)\n    self.assertOperationTypes(changes, \"testapp\", 0, [\"AlterField\", \"RemoveField\"])\n    self.assertOperationAttributes(\n        changes, \"testapp\", 0, 0, name=\"name\", old_name=\"names\"\n    )\n    self.assertMigrationDependencies(\n        changes, \"testapp\", 0, [(\"testapp\", \"auto_1\")]\n    )\n", "def test_destructurable_objects_default(self):\n    \"\"\"\n    #26253 - Destructable objects that deconstruct to the same value should\n    not be considered a change.\n    \"\"\"\n    author_name_default = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Some Name\")),\n        ],\n    )\n    author_name_same_default = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Some Name\")),\n        ],\n    )\n    changes = self.get_changes([author_name_default], [author_name_same_default])\n    self.assertEqual(changes, {})\n\n    author_name_diff_default = ModelState(\n        \"testapp\",\n        \"Author\",\n        [\n            (\"id\", models.AutoField(primary_key=True)),\n            (\"name\", models.CharField(max_length=200, default=\"Different Name\")),\n        ],\n    )\n    changes = self.get_changes([author_name_default], [author_name_diff_default])\n    self.assertEqual(len(changes), 1)\n", "def test_add_db_table_and_index_together(self):\n    \"\"\"Adding a db_table to a model and index_together simultaneously should not cause a circular dependency.\"\"\"\n    changes = self.get_changes(\n        [self.author_empty],\n        [self.author_with_db_table_options, self.book_foo_together],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AddIndex\", \"AlterModelTable\"]\n    )\n    added_index = models.Index(fields=[\"author\", \"title\"], name=\"book_title_author_idx\")\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, model_name=\"book\", index=added_index\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, name=\"author\", table=\"author_one\"\n    )\n\n    # Check that the addition of a db_table to a model with an index_together doesn't cause a circular dependency.\n    changes = self.get_changes(\n        [self.author_empty, self.book_foo_together],\n        [self.author_with_db_table_options, self.book_foo_together],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterModelTable\", \"AlterIndexTogether\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", unique_together={(\"author\", \"title\")}\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 1, name=\"author\", table=\"author_one\"\n    )\n\n    # Check that the addition of a db_table to a model with a unique_together doesn't cause a circular dependency.\n    changes = self.get_changes(\n        [self.author_empty, self.book_unique_together],\n        [self.author_with_db_table_options, self.book_unique_together],\n    )\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(\n        changes, \"otherapp\", 0, [\"AlterModelTable\", \"AlterUniqueTogether\"]\n    )\n    self.assertOperationAttributes(\n        changes, \"otherapp\", 0, 0, name=\"book\", unique"], "sample_137": ["def test_signed_permutations():\n    assert list(signed_permutations((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2),\n        (0, 2, 1), (0, -2, 1), (0, 2, -1), (0, -2, -1),\n        (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2),\n        (1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0),\n        (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0), (2, -1, 0), (-2, -1, 0)\n    ]\n    assert list(signed_permutations((-1, -1, -1))) == [\n        (-1, -1, -1)]\n    assert list(signed_permutations((0, 0, 0))) == [\n        (0, 0, 0)]\n", "def test_signed_permutations():\n    assert list(signed_permutations((0, 1, 2))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2),\n        (0, 2, 1), (0, -2, 1), (0, 2, -1), (0, -2, -1),\n        (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2),\n        (1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0),\n        (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0), (2, -1, 0), (-2, -1, 0)]\n    assert list(signed_permutations((0, 1))) == [\n        (0, 1), (0, -1), (1, 0), (-1, 0)]\n\n    assert list(signed_permutations(Matrix([0, 1, 2]))) == [\n        (0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2),\n        (0, 2, 1), (0, -2, 1), (0, 2, -1), (0, -2, -1),\n        (1, 0, 2), (-1, 0, 2), (1, 0, -2), (-1, 0, -2),\n        (1, 2, 0), (-1, 2, 0), (1, -2, 0), (-1, -2, 0),\n        (2, 0, 1), (-2, 0, 1), (2, 0, -1), (-2, 0, -1),\n        (2, 1, 0), (-2, 1, 0),", "def test_permute_signs():\n    assert list(permute_signs([1, 0, -1, 0, 1])) == \\\n        [(1, 0, -1, 0, 1), (1, 0, 1, 0, -1), (1, 0, -1, 1, 0), (1, 0, 1, 1, 0),\n        (1, 0, -1, -1, 0), (-1, 0, 1, 0, 1), (-1, 0, -1, 0, 1),\n        (-1, 0, 1, 1, 0), (-1, 0, -1, 1, 0), (1, -1, 0, 0, 1),\n        (1, -1, 0, 1, 0), (1, -1, 0, -1, 0), (1, -1, 1, 0, 0),\n        (1, -1, 0, 0, -1), (1, -1, -1, 0, 0), (-1, -1, 0, 0, 1),\n        (-1, -1, 0, 1, 0), (-1, -1, 0, -1, 0), (-1, -1, 1, 0, 0),\n        (-1, -1, 0, 0, -1), (-1, -1, -1, 0, 0)]\n    assert list(permute_signs((0, 1, 2))) == \\\n        [(0, 1, 2), (0, -1, 2), (0, 1, -2), (0, -1, -2)]\n    assert list(permute_signs([0, 1, 2, 3, 4])) == \\\n        [(0, 1, 2, 3, 4), (0, 1, 2, 4, 3), (0, 1, -2, 3, 4), (0, 1, 2, -4, 3),\n        (0, 1, -2, 4,", "def test_permute_signs():\n    from sympy.abc import x, y, z\n    assert list(permute_signs(x + y + z)) == [\n        x + y + z, x + y - z, x - y + z, x - y - z, -x + y + z, -x + y - z,\n        -x - y + z, -x - y - z\n    ]\n    assert list(permute_signs(-x)) == [-x]\n    assert list(permute_signs(x)) == [x]\n    assert list(permute_signs((x + y) * (y + z))) == [\n        (x + y) * (y + z), -x * (y + z), x * (-y + z), (x + y) * (-y + z),\n        (x + y) * (-z), (x - y) * (y + z), (x - y) * (-z), (-x + y) * (y + z),\n        (-x + y) * (-z), (-x - y) * (y + z), (-x - y) * (-z), (x + y) * z,\n        (x + y) * (-y + z), (x + y) * (-y - z), (-x + y) * z, (-x + y) * (-y + z),\n        (-x + y) * (-y - z), (-x - y) * z, (-x - y) * (-y + z), (-x - y) * (-y - z)\n    ]\n    assert list(signed_permutations(x + y + z)) == [\n        x + y + z, x + y - z, x - y + z, x - y - z, -x + y + z, -x + y - z,\n        -x - y + z, -x - y - z\n    ]\n    assert list(signed_permutations(-x)) == [-x]\n    assert list(signed_permutations(x)) == [x]\n    assert list(signed_permutations((x + y) * (y + z))) == [\n        (x + y) * (y + z), -x * (y + z), x * (-y + z), (x + y) * (-y + z),\n        (x + y) * (-", "compilation error"], "sample_86": ["def test_junit_duration_report_total_duration(testdir, monkeypatch):\n    # mock LogXML.node_reporter so it always sets a known duration to each test report object\n    original_node_reporter = LogXML.node_reporter\n\n        report.duration = 3.0\n        reporter = original_node_reporter(s, report)\n        return reporter\n\n    monkeypatch.setattr(LogXML, \"node_reporter\", node_reporter_wrapper)\n\n    testdir.makepyfile(\n        \"\"\"\n                pass\n    \"\"\"\n    )\n    result, dom = runandparse(\n        testdir, \"-o\", \"junit_duration_report=total\"\n    )\n    node = dom.find_first_by_tag(\"testsuite\")\n    tnode = node.find_first_by_tag(\"testcase\")\n    val = tnode[\"time\"]\n    assert val == \"3.000\"\n", "def test_add_global_property_multiple_times(testdir):\n    path = testdir.tmpdir.join(\"test_add_global_property_multiple_times.xml\")\n    log = LogXML(str(path), None)\n\n    class Report(BaseReport):\n        sections = []\n        nodeid = \"test_node_id\"\n\n    log.pytest_sessionstart()\n    log.add_global_property(\"foo\", 1)\n    log.add_global_property(\"foo\", 2)  # should overwrite previous property\n    log.add_global_property(\"bar\", 3)\n    log.pytest_sessionfinish()\n\n    dom = minidom.parse(str(path))\n\n    properties = dom.getElementsByTagName(\"properties\")\n\n    assert properties.length == 1, \"There must be one <properties> node\"\n\n    property_list = dom.getElementsByTagName(\"property\")\n\n    assert property_list.length == 2, \"There most be only 2 property nodes\"\n\n    expected = {\"foo\": \"2\", \"bar\": \"3\"}\n    actual = {}\n\n    for p in property_list:\n        k = str(p.getAttribute(\"name\"))\n        v = str(p.getAttribute(\"value\"))\n        actual[k] = v\n\n    assert actual == expected\n", "def test_logxml_log_passing_tests_default(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            pass\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret == 0\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(log_passing_tests=\"true\")\n    node.assert_attr(failures=0, skipped=0, tests=1)\n\n    testdir.makepyfile(\n        \"\"\"\n            assert 0\n    \"\"\"\n    )\n    result, dom = runandparse(testdir)\n    assert result.ret\n    node = dom.find_first_by_tag(\"testsuite\")\n    node.assert_attr(log_passing_tests=\"true\")\n    node.assert_attr(failures=1, skipped=0, tests=2)\n", "def test_record_property_junit_disabled_property_value_type_checking(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            record_property(\"foo\", 42)\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stdout.fnmatch_lines(\n        [\"*record_property is incompatible with junit_family 'xunit2' (use 'legacy' or 'xunit1')\"]\n    )\n    result.stdout.fnmatch_lines(\n        [\"*name parameter needs to be a string, but int given\"]\n    )\n", "        def test_function(record_property):\n            record_property(\"example_key\", 1)"], "sample_83": ["def test_colorized_text_reporter(tmp_path):\n    \"\"\"Test the colorized text reporter.\"\"\"\n    output = StringIO()\n    reporter = ColorizedTextReporter(output=output)\n    linter = PyLinter(reporter=reporter)\n    checkers.initialize(linter)\n\n    # Test with default color mapping\n    linter.config.colorized_report = True\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\n\")\n    assert out_lines[1] == f\"\\033[32m************* Module my_mod\\033[0m\"\n    assert (\n        out_lines[2] == \"\\033[34mmy_mod:1:0: C0301: Line too long (1/2) (line-too-long)\\033[0m\"\n    )\n    assert (\n        out_lines[3] == \"\\033[34mmy_mod:2:0: C0301: Line too long (3/4) (line-too-long)\\033[0m\"\n    )\n\n    # Test with custom color mapping\n    reporter = ColorizedTextReporter(output=output, color_mapping={\"C\": \"blue\"})\n    linter = PyLinter(reporter=reporter)\n    checkers.initialize(linter)\n    linter.config.colorized_report = True\n    linter.open()\n    linter.set_current_module(\"my_mod\")\n    linter.add_message(\"C0301\", line=1, args=(1, 2))\n    linter.add_message(\"line-too-long\", line=2, args=(3, 4))\n\n    out_lines = output.getvalue().split(\"\\033[0m\\n\")\n    assert len(out_lines) == 7\n    assert out_lines[1] == \"************* Module my_mod\"\n    assert (\n        out_lines[2] == \"my_mod:1:0: C0301: Line too long (1/2) (line-too-long)\"\n    )\n    assert (\n        out_lines[3] == \"my_mod:2:0: C0301: Line too long (3/4) (line-too-long)\"\n    )\n    assert out_lines[4] == \"\"\n    assert out_lines[5] == \"\"\n    assert out_lines[6]", "def test_colorized_reporter_deprecation():\n    \"\"\"TODO remove in 3.0.\"\"\"\n    reporter = ColorizedTextReporter()\n    # noinspection PyDeprecation\n    reporter.set_output(sys.stdout)\n    reporter = ColorizedTextReporter()\n    # noinspection PyDeprecation\n    reporter.set_output(sys.stdout)\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        reporter._get_decoration(\"W\")\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n", "def test_display_results_is_renamed_parseable():\n    class CustomReporter(ParseableTextReporter):\n            return None\n\n    reporter = CustomReporter()\n    with pytest.raises(AttributeError) as exc:\n        # pylint: disable=no-member\n        reporter.display_results()  # type: ignore[attr-defined]\n    assert \"no attribute 'display_results'\" in str(exc)\n\n", "def test_colorize_ansi_deprecation_warning():\n    \"\"\"Test the colorize_ansi function to ensure it emits a deprecation warning when MessageStyle is not passed as parameter.\"\"\"\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        assert colorize_ansi(\"message\") is not None\n        assert colorize_ansi(\"message\", \"red\") is not None\n        assert len(cm) == 2\n        assert isinstance(cm[0].message, DeprecationWarning)\n        assert isinstance(cm[1].message, DeprecationWarning)\n", "def test_colorized_reporter_deprecation():\n    \"\"\"Test the colorized output reporter with deprecation warnings.\"\"\"\n    with warnings.catch_warnings(record=True) as cm:\n        warnings.simplefilter(\"always\")\n        ColorizedTextReporter()\n\n    assert len(cm) == 1\n    assert isinstance(cm[0].message, DeprecationWarning)\n\n"], "sample_7": ["    def test_common_roots_multiple_roots(self):\n        paths = (\n            Path('/first/second'),\n            Path('/first/second/third'),\n            Path('/first/'),\n            Path('/root/first/'),\n            Path('/root/second/'),\n        )\n        results = autoreload.common_roots(paths)\n        self.assertCountEqual(results, [Path('/first/'), Path('/root/')])\n", "    def setUp(self):\n        self._directory = tempfile.TemporaryDirectory()\n        self.directory = Path(self._directory.name).resolve().absolute()\n        self.file = self.directory / 'test.py'\n        self.file.touch()\n        self.symlink = self.directory / 'symlink'\n        self.symlink.symlink_to(self.file)\n", "    def test_check_availability(self):\n        self.assertTrue(self.reloader.check_availability())\n        self.assertTrue(autoreload.check_availability())\n", "    def test_subscription_removal(self, mocked_pywatchman):\n        fake_client = mock.MagicMock()\n        fake_client.subs = {'sub1': mock.MagicMock(), 'sub2': mock.MagicMock()}\n        mocked_pywatchman.client.return_value = fake_client\n        self.reloader = autoreload.WatchmanReloader()\n        self.reloader.client = fake_client\n        self.reloader.client_timeout = 0.1\n        with self.tick_twice():\n            self.reloader.tick()\n        self.assertEqual(fake_client.subs.keys(), ['sub1', 'sub2'])\n        with self.tick_twice():\n            self.reloader.stop()\n            self.reloader.tick()\n        self.assertEqual(fake_client.subs.keys(), [])\n", "    def test_invalid_module_type(self):\n        \"\"\"\n        Invalid module types are ignored.\n        \"\"\"\n        invalid_module = object()\n        self.clear_autoreload_caches()\n        self.assertNotIn(invalid_module, list(autoreload.iter_all_python_module_files()))\n        self.assertEqual(autoreload.iter_modules_and_files.cache_info().hits, 0)\n"], "sample_22": ["def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('Hello World'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('HelloWorldWithSpaces'), 'hello world with spaces')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('HelloWorld')), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hello world'), 'hello world')\n", "def test_camel_case_to_spaces(self):\n    self.assertEqual(text.camel_case_to_spaces('HelloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('helloWorld'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hELLoWoRlD'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces(''), '')\n    self.assertEqual(text.camel_case_to_spaces('Hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('hello'), 'hello')\n    self.assertEqual(text.camel_case_to_spaces('Hello World'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('hello world'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces('Hello World'), 'hello world')\n    self.assertEqual(text.camel_case_to_spaces(lazystr('hello world')), 'hello world')\n", "def test_compress_string(self):\n    self.assertEqual(b'\\x1f\\x9d\\x90\\x03\\x00\\x00\\x00\\x03Hello, World!\\x00\\x00', text.compress_string('Hello, World!'))\n    self.assertEqual(b'\\x1f\\x9d\\x90\\x03\\x00\\x00\\x00\\x02The quick brown fox jumped over the lazy dog.\\x00', text.compress_string('The quick brown fox jumped over the lazy dog.'))\n    self.assertEqual(b'\\x1f\\x9d\\x90\\x03\\x00\\x00\\x00\\x01\\0', text.compress_string(''))\n    self.assertEqual(b'\\x1f\\x9d\\x90\\x03\\x00\\x00\\x00\\x00', text.compress_string(None))\n    # Test with non-ASCII characters\n    self.assertEqual(\n        b'\\x1f\\x9d\\x90\\x03\\x00\\x00\\x00\\x04B\\0\\0\\0\\xfc\\xdf\\x04',\n        text.compress_string('o\u00fco\u00fco\u00fco\u00fc')\n    )\n\n    # Test with a long string\n    long_string = 'a' * 1000000\n    self.assertLess(len(text.compress_string(long_string)), len(long_string))\n", "def test_camel_case_to_spaces(self):\n    items = (\n        ('HelloWorld', 'hello world'),\n        ('myLongString', 'my long string'),\n        ('my ShortString', 'my short string'),\n        ('my MixedString', 'my mixed string'),\n        ('my MixedString123', 'my mixed string123'),\n        ('my StringWithCamelCase', 'my string with camel case'),\n        ('my String With Camel Case', 'my string with camel case'),\n    )\n    for value, expected in items:\n        self.assertEqual(camel_case_to_spaces(value), expected)\n        self.assertEqual(camel_case_to_spaces(lazystr(value)), expected)\n", "    def test_capfirst(self):\n        self.assertEqual(text.capfirst('test'), 'Test')\n        self.assertEqual(text.capfirst(''), '')\n        self.assertEqual(text.capfirst('test Test'), 'test Test')\n        self.assertEqual(text.capfirst('test test'), 'test test')\n"], "sample_72": ["def test_figure_add_subplot_with_projection():\n    fig = plt.figure()\n    fig.add_subplot(111, projection='polar')\n    ax = fig.gca()\n    assert ax.get_projection_name() == 'polar'\n    assert isinstance(ax, projections.PolarAxes)\n", "def test_tight_layout():\n    fig, ax = plt.subplots()\n    fig.tight_layout()\n\n    # Check that the axes are still present in the figure\n    assert len(fig.axes) > 0\n\n    # Check that the tight layout has been applied\n    assert fig.get_layout_engine().is_tight_layout\n\n    # Check that the layout engine is a TightLayoutEngine instance\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n", "def test_figure_get_tightbbox():\n    fig, ax = plt.subplots()\n    fig.autofmt_xdate()\n    fig.autofmt_xdate(bottom=0.3)\n    fig.autofmt_xdate(bottom=0.3, rotation=60)\n    fig.autofmt_xdate(bottom=0.3, rotation=90)\n    bbox = fig.get_tightbbox(fig.canvas.get_renderer())\n    assert isinstance(bbox, transforms.BboxBase)\n    assert bbox.size != 0\n", "def test_figure_subplot_adjust():\n    fig, axs = plt.subplots(2, 2)\n    fig.tight_layout()\n    ax = fig.add_subplot(221)\n    ax.set_title(\"Axis 1\")\n    ax.set_xlabel(\"X-axis 1\")\n    ax.set_ylabel(\"Y-axis 1\")\n    ax = fig.add_subplot(222)\n    ax.set_title(\"Axis 2\")\n    ax.set_xlabel(\"X-axis 2\")\n    ax.set_ylabel(\"Y-axis 2\")\n    fig.subplots_adjust(left=0.1, right=0.9)\n    assert len(fig.get_layout_engine()) == 1\n    assert isinstance(fig.get_layout_engine(), TightLayoutEngine)\n    assert fig.subplotpars.left == 0.1\n    assert fig.subplotpars.right == 0.9\n    assert fig.get_constrained_layout_pads(relative=False) is None\n", "def test_subplot_polar_masking():\n    fig, axs = plt.subplots(1, 2, subplot_kw={'projection': 'polar'})\n    fig.suptitle('Figure')\n    ax1, ax2 = axs\n    ax1.set_title('Axes', loc='left', fontstyle='oblique', fontsize='medium')\n    ax2.set_title('Axes', loc='left', fontstyle='oblique', fontsize='medium')\n    ax1.set_rlim(0, 1)\n    ax2.set_rlim(0, 2)\n    assert ax1.get_rlim().start == 0.0\n    assert ax1.get_rlim().stop == 1.0\n    assert ax2.get_rlim().start == 0.0\n    assert ax2.get_rlim().stop == 2.0\n    assert len(axs) == 2\n    fig.sca(ax1)\n    ax1.set_rlim(0, 1)\n    assert ax1.get_rlim().start == 0.0\n    assert ax1.get_rlim().stop == 1.0\n    fig.sca(ax2)\n    ax2.set_rlim(0, 2)\n    assert ax2.get_rlim().start == 0.0\n    assert ax2.get_rlim().stop == 2.0\n    fig.delaxes(ax1)\n    fig.delaxes(ax2)\n    assert len(fig.axes) == 0\n"], "sample_150": ["def test_solve_generic():\n    x, y, z = symbols('x y z')\n    assert solve_generic([x + y + z - 1, x + y + z - 2], x, y, z) == []\n\n    f_1 = x**2 + y - 1\n    f_2 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x + y + z - 1\n    f_2 = x + y + z - 2\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x**2 + y - 1\n    f_2 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x + y + z - 1\n    f_2 = x**2 + y + z - 2\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x**2 + y - 1\n    f_2 = x**2 + y + z - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x**2 + y - 1\n    f_2 = x + y + z - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x + y + z - 1\n    f_2 = x**2 + y**2 + z - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x + y + z - 1\n    f_2 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 = x + y + z - 1\n    f_2 = x + y + z - 2\n\n    assert solve_generic([f_1, f_2], x, y, z) == []\n\n    f_1 =", "def test_solve_generic():\n    f_1 = x**2 + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    # test for generic case\n    solutions = solve_generic([f_1, f_2, f_3], (x, y, z))\n\n    expected_solutions = [(0, 0, 1), (0, 1, 0), (1, 0, 0), (sqrt(2) - 1, sqrt(2) - 1, sqrt(2) - 1), (-sqrt(2) - 1, -sqrt(2) - 1, -sqrt(2) - 1)]\n\n    assert solutions == expected_solutions\n\n    # test for non-zero-dimensional case\n    assert solve_generic([x + y - 1, x + y + 2], (x, y)) is None\n\n    # test for non-zero-dimensional case with entry=True\n    assert solve_generic([x + y - 1, x + y + 2], (x, y), entry=True) == None\n", "def test_solve_generic():\n    x, y, z = symbols('x y z')\n\n    # Generic system of polynomial equations\n    f_1 = x + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2, f_3], x, y, z) == \\\n        [(0, 0, 1), (0, 1, 0), (1, 0, 0), (-sqrt(2), sqrt(2), sqrt(2)),\n         (sqrt(2), -sqrt(2), sqrt(2)), (-sqrt(2), -sqrt(2), -sqrt(2)),\n         (sqrt(2), sqrt(2), -sqrt(2))]\n\n    # Test with a non-zero dimensional system\n    f_1 = x + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 + 1\n\n    assert solve_generic([f_1, f_2, f_3], x, y, z) is None\n\n    # Test with an empty set of solutions\n    f_1 = x**2 + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2, f_3], x, y, z) == []\n\n    # Test with a system of linear equations\n    f_1 = x + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x + y + z**2 - 1\n\n    assert solve_generic([f_1, f_2, f_3], x, y, z) == \\\n        [(0, 0, 1), (0, 1, 0), (1, 0, 0)]\n\n    # Test with a system of polynomial equations with repeated variables\n    f_1 = x**2 + y + z - 1\n    f_2 = x + y**2 + z - 1\n    f_3 = x", "def test_solve_generic():\n    x, y, z = symbols('x y z')\n\n    # Test that the system with the zero-dimensional solution [1, 2, 3] is solved\n    assert solve_poly_system([x - 1, y - 2, z - 3], x, y, z) == [(1, 2, 3)]\n\n    # Test that the system with the zero-dimensional solution [-1, -2, -3] is solved\n    assert solve_poly_system([x + 1, y + 2, z + 3], x, y, z) == [(-1, -2, -3)]\n\n    # Test that the system with multiple zero-dimensional solutions is solved\n    assert solve_poly_system([x**2 - 1, y**2 - 1, z**2 - 1], x, y, z) == [\n        (-1, 0, 0),\n        (1, 0, 0),\n        (0, -1, 0),\n        (0, 1, 0),\n        (0, 0, -1),\n        (0, 0, 1)\n    ]\n\n    # Test that a system with a non-zero-dimensional solution is not solved\n    assert solve_poly_system([x**2 + y**2 + z**2 - 1], x, y, z) is None\n\n    # Test that a system with no solutions is not solved\n    assert solve_poly_system([x**2 + y**2 + z**2 + 1], x, y, z) is None\n\n    # Test that a system with a variable not in the sequence is not solved\n    assert solve_poly_system([x + 1, y + 1, z + 1], x, y) is None\n\n    # Test that a system with a variable not in the sequence raises an error\n    raises(NotImplementedError, lambda: solve_poly_system([x + 1, y + 1, z + 1], x, y, z))\n\n    # Test that a system with a variable not in the sequence raises an error\n    raises(NotImplementedError, lambda: solve_poly_system([x + 1, y + 1, z + 1], x))\n\n    # Test that a system with a non-ground polynomial raises an error\n    raises(PolynomialError, lambda: solve_poly_system([", "def test_solve_generic():\n    from sympy.abc import x, y\n    from sympy.polys import Poly\n    from sympy.solvers.polysys import solve_generic\n    from sympy.polys.polytools import parallel_poly_from_expr\n\n    f = Poly(x**2 + y**2 - 2, x, y)\n    g = Poly(x**2 - 1, x, y)\n    gens = (x, y)\n    (f, g), opt = parallel_poly_from_expr([f, g], *gens)\n    result = solve_generic([f, g], opt)\n\n    assert result == [(0, -1), (0, 1), (2, -1), (2, 1)]\n\n    f = Poly(x**3 + y**3 - 3*x*y, x, y)\n    g = Poly(x**2 + y**2 - 1, x, y)\n    (f, g), opt = parallel_poly_from_expr([f, g], *gens)\n    result = solve_generic([f, g], opt)\n\n    assert result is None\n\n    f = Poly(x**2 + y**2 + 1, x, y)\n    g = Poly(x + y, x, y)\n    (f, g), opt = parallel_poly_from_expr([f, g], *gens)\n    result = solve_generic([f, g], opt)\n\n    assert result is None\n\n    f = Poly(x**3 + y**2 - 3*x*y - 2*x + 1, x, y)\n    g = Poly(x**2 + y**2 + 1, x, y)\n    (f, g), opt = parallel_poly_from_expr([f, g], *gens)\n    result = solve_generic([f, g], opt)\n\n    assert result == [(-1, 1)]\n\n    # Test that a univariate polynomial in its last variable is handled correctly\n    f = Poly(x**2 + y - 1, x, y)\n    g = Poly(y - 1, x, y)\n    (f, g), opt = parallel_poly_from_expr([f, g], *gens)\n    result = solve_generic([f, g], opt)\n\n    assert result == [(1, 1)]\n\n    # Test that the variable order is correct\n    f = Poly(x**2 + y**2 - 2, x, y)\n   "], "sample_40": ["def test_boundfield_id_for_label_overridden_by_subwidgets(self):\n    class SomeForm(Form):\n        field = ChoiceField(choices=[('a', 'A'), ('b', 'B')], widget=RadioSelect)\n\n    form = SomeForm()\n    field = form['field']\n    subwidgets = field.subwidgets\n    self.assertEqual(subwidgets[0].id_for_label, 'id_field_0')\n    self.assertEqual(subwidgets[1].id_for_label, 'id_field_1')\n    field = form['field']\n    self.assertEqual(field.id_for_label, 'id_field_0')\n", "def test_subclassing_boundfield(self):\n    class CustomBoundField(BoundField):\n            return 'Custom rendered field'\n\n    class MyForm(Form):\n        field = CharField()\n\n            super().__init__(*args, **kwargs)\n            self.fields['field'].widget = CustomBoundField()\n\n    form = MyForm()\n    self.assertEqual(form['field'], CustomBoundField(form, 'field', form.renderer))\n", "def test_boundfield_choose_widget(self):\n    # BoundField's as_text() and as_textarea() are shortcuts that change the widget type.\n    class ContactForm(Form):\n        subject = CharField()\n        message = CharField(widget=Textarea)\n\n    f = ContactForm(auto_id=False)\n    self.assertHTMLEqual(f.as_text(), '<input type=\"text\" name=\"subject\" required>')\n    self.assertHTMLEqual(f.as_textarea(), '<textarea name=\"message\" rows=\"10\" cols=\"40\" required></textarea>')\n\n    # You can also pass a widget instance to the as_text() and as_textarea() methods.\n    class ContactForm(Form):\n        subject = CharField()\n        message = CharField(widget=Textarea(attrs={'rows': 80, 'cols': 20}))\n\n    f = ContactForm(auto_id=False)\n    self.assertHTMLEqual(f.as_text(), '<input type=\"text\" name=\"message\" required>')\n    self.assertHTMLEqual(f.as_textarea(), '<textarea rows=\"80\" cols=\"20\" name=\"message\" required></textarea>')\n\n    # The 'widget' parameter to as_text() and as_textarea() will not carry over to as_hidden().\n    f = ContactForm({'subject': 'Hello', 'message': 'I love you.'}, auto_id=False)\n    self.assertHTMLEqual(f.as_hidden(), '<input type=\"hidden\" name=\"subject\" value=\"Hello\"><input type=\"hidden\" name=\"message\" value=\"I love you.\">')\n", "def test_boundfield_value_callable_initial_invalidation(self):\n    class PersonForm(Form):\n        name = CharField(initial=lambda: 'John', disabled=True)\n\n    # Without form data.\n    form = PersonForm()\n    form.data = {'name': 'Joe'}\n    self.assertEqual(form['name'].value(), 'John')\n\n    # With form data. As the field is disabled, the value should not be\n    # affected by the form data.\n    form = PersonForm({'name': 'Joe'})\n    self.assertEqual(form['name'].value(), 'John')\n\n    # Check that changing the field value in a Form instance does not affect the\n    # BoundField's value.\n    form['name'] = 'Jane'\n    self.assertEqual(form['name'].value(), 'John')\n\n    # Now, check that a BoundField's value remains the same when\n    # form.cleaned_data is modified.\n    form.cleaned_data = {'name': 'Bob'}\n    self.assertEqual(form['name'].value(), 'John')\n", "def test_boundfield_errors(self):\n    class SomeForm(Form):\n        field = CharField()\n        field2 = CharField(widget=TextInput)\n\n    form = SomeForm({'field': 'hello'})\n    boundfield = form['field']\n    self.assertEqual(boundfield.errors, {})\n    boundfield.errors = ['error1']\n    self.assertEqual(boundfield.errors, ['error1'])\n    self.assertEqual(form.errors, {'field': ['error1']})\n\n    boundfield = SomeForm()['field']\n    self.assertEqual(boundfield.errors, {'field': ['error1']})\n    boundfield.errors = {'field': 'error2'}\n    self.assertEqual(boundfield.errors, {'field': 'error2'})\n    self.assertEqual(form.errors, {'field': 'error2'})\n\n    # Subwidgets have errors\n    class SomeForm(Form):\n        field = MultipleChoiceField(widget=CheckboxSelectMultiple)\n\n    form = SomeForm()\n    subwidgets = form['field'].subwidgets\n    subwidgets[0].errors = ['error1']\n    self.assertEqual(form.errors, {'field': ['error1']})\n\n    # Test if boundfield.__bool__ works correctly with subwidgets having errors\n    self.assertTrue(form['field'])\n\n    # Test if boundfield.errors and form.errors work correctly with subwidgets having errors\n    class SomeForm(Form):\n        field = MultipleChoiceField(widget=CheckboxSelectMultiple)\n\n    form = SomeForm({'field': ['option1', 'option2']})\n    subwidgets = form['field'].subwidgets\n    subwidgets[0].errors = ['error1']\n    self.assertEqual(form.errors, {'field': ['error1']})\n    self.assertEqual(form['field'].errors, ['error1'])\n"], "sample_155": ["def test_get_dimensional_dependencies():\n    v_w1 = Quantity('v_w1')\n    v_w2 = Quantity('v_w2')\n\n    v_w1.set_global_relative_scale_factor(1, meter/second)\n    v_w2.set_global_relative_scale_factor(1, meter/second)\n\n    expr = v_w1 - v_w2\n    Dq = Dimension(SI.get_dimensional_expr(expr))\n    assert SI.get_dimension_system().get_dimensional_dependencies(Dq) == {\n        length: 0,\n        time: -1,\n    }\n", "def test_unit_system_get_unit_system():\n    \"\"\"\n    Test that UnitSystem.get_unit_system() returns the correct unit system\n    instance given a unit.\n    \"\"\"\n    # Test that it returns the unit system instance for a valid unit\n    assert UnitSystem.get_unit_system(meter) is UnitSystem.get_default_unit_system()\n    # Test that it raises a ValueError for an invalid unit\n    with raises(ValueError):\n        UnitSystem.get_unit_system(\"invalid_unit\")\n", "def test_dimensional_expr_with_Pow():\n    q1 = Quantity('q1')\n    q2 = Quantity('q2')\n\n    SI.set_quantity_dimension(q1, length**2)\n    SI.set_quantity_dimension(q2, length)\n\n    assert SI.get_dimensional_expr(q1/q2) == (length**2/length).name\n    assert SI.get_dimensional_expr(q1**2) == (length**2)**2\n    assert SI.get_dimensional_expr(q2**-1) == 1/(length)\n\n    with warns_deprecated_sympy():\n        assert SI.get_dimensional_expr((q1**2 + q2**2)**3) == (length**2)**3\n        assert SI.get_dimensional_expr(q1**-2) == 1/(length**2)\n", "def test_issue_24560():\n    \"\"\"\n    This test ensures that the `Dimension` object is correctly generated from\n    the expression when the `Dimension` object is created with a non-scalar\n    argument.\n\n    This was a regression introduced in the fix of issue 20288.\n    \"\"\"\n    from sympy.physics.units import centimeter, kilogram, meter, second\n    expr = (kilogram / meter**2).convert_to(centimeter)\n    dim = SI.get_dimensional_expr(expr)\n    assert isinstance(dim, Dimension)\n    assert dim == length**-1\n", "def test_issue_23845():\n    from sympy.physics.units import definitions\n\n    # Create a unit system with different base units\n    unit_system = UnitSystem(('m', 's'), units=(), name='my_system', dimension_system=Dimension('length', 'time'))\n    my_length = unit_system.get_quantity_dimension('m')\n    my_time = unit_system.get_quantity_dimension('s')\n    my_system = unit_system\n\n    # Define a new quantity in this unit system\n    Quantity('my_quantity', abbrev='mq', dimension=my_length**2 / my_time**3)\n\n    # Check that the quantity has the correct dimension\n    assert my_quantity.scale_factor == S.One\n    assert my_quantity.dimension == my_length**2 / my_time**3\n\n    # Check that the unit system has the correct number of base units\n    assert my_system.dim == 2\n\n    # Check that the unit system has the correct derived units\n    assert my_system.derived_units == {my_length**2 / my_time**3: my_quantity}\n\n    # Check that the unit system can handle prefix units correctly\n    kilo_mq = Quantity('kilo_mq', abbrev='kmq', dimension=my_length**2 / my_time**3)\n    kilo_mq.set_global_relative_scale_factor(kilo, my_quantity)\n    assert kilo_mq.scale_factor == 1000\n    assert kilo_mq.dimension == my_length**2 / my_time**3\n    assert kilo_mq.abbrev == Symbol('kmq')\n"], "sample_21": ["    def test_sorting(self):\n        collector = Collector(using='default')\n        # Simulate a complex dependency graph.\n        collector.add([RChild.objects.create()], source=None, nullable=False)\n        collector.add([RChildChild.objects.create()], source=None, nullable=False)\n        collector.add([R.objects.create()], source=None, nullable=False)\n        collector.sort()\n        self.assertEqual(\n            [model._meta.concrete_model for model in collector.data.keys()],\n            [RChildChild._meta.concrete_model, RChild._meta.concrete_model, R._meta.concrete_model]\n        )\n", "    def test_sort_dependencies(self):\n        collector = Collector(using='default')\n        collector.data['model1'] = [object()]\n        collector.data['model2'] = [object()]\n        collector.dependencies['model1'].add('model2')\n        collector.dependencies['model2'].add('model1')\n        collector.sort()\n        self.assertEqual(list(collector.data.keys()), ['model2', 'model1'])\n", "    def test_sort_dependencies(self):\n        r = R.objects.create()\n        s1 = S.objects.create(r=r)\n        s2 = S.objects.create(r=r)\n        T.objects.create(pk=1, s=s1)\n        T.objects.create(pk=2, s=s2)\n        RChild.objects.create(r_ptr=r)\n        r.delete()\n        self.assertEqual(\n            self.collector.sort(),\n            [(RChild, r.pk), (T, 2), (T, 1), (R, r.pk), (S, 2), (S, 1)]\n        )\n", "    def test_delete_with_soft_deletion(self):\n        \"\"\"\n        Test that delete with a model that uses soft deletion works as expected.\n        \"\"\"\n        m = M.objects.create()\n        r = R.objects.create()\n        MR.objects.create(m=m, r=r)\n        with transaction.atomic():\n            m.delete()\n        # The objects should still exist in the database, but be marked as deleted.\n        self.assertTrue(MR.objects.exists())\n        self.assertTrue(M.objects.exists())\n        self.assertTrue(R.objects.exists())\n        # The soft-deleted objects should be returned by a normal QuerySet.\n        self.assertEqual(MR.objects.count(), 1)\n        self.assertEqual(M.objects.count(), 1)\n        self.assertEqual(R.objects.count(), 1)\n        # A QuerySet with delete() should return the number of deleted rows.\n        deleted, rows_count = M.objects.all().delete()\n        self.assertEqual(deleted, 0)\n        self.assertEqual(rows_count, {'M': 1})\n        # A model with delete() should return the number of deleted rows.\n        deleted, rows_count = m.delete()\n        self.assertEqual(deleted, 0)\n        self.assertEqual(rows_count, {'M': 1})\n", "    def test_fast_deletes_prefers_related_objects_with_select_related(self):\n        # django.db.models.sql.constants.GET_ITERATOR_CHUNK_SIZE = 1000\n        # So `q1` and `q2` are the same, but with different iteration sizes\n        q1 = R.objects.filter(m__in=[R.objects.create() for _ in range(2000)])\n        q2 = R.objects.filter(m__in=[R.objects.create() for _ in range(1000)])\n        c1 = Collector(using='default')\n        c2 = Collector(using='default')\n        c1.fast_deletes.append(q1)\n        c2.fast_deletes.append(q2)\n        self.assertLess(len(c1.get_del_batches(q1, [])), len(c2.get_del_batches(q2, [])))\n"], "sample_71": ["def test_context_reset():\n    original_value = 'gray'\n    other_value = 'blue'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_style_reload_context():\n    original_value = 'gray'\n    mpl.rcParams[PARAM] = original_value\n    with style.reload_library():\n        with style.context('test'):\n            assert mpl.rcParams[PARAM] == VALUE\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_style_reload():\n    # Test that style.reload_library() resets mpl.rcParams to default values.\n    original_value = 'gray'\n    mpl.rcParams[PARAM] = original_value\n    style.reload_library()\n    assert mpl.rcParams == rcParamsDefault\n", "def test_use_after_reset():\n    original_value = 'gray'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context('test', after_reset=True):\n            assert mpl.rcParams[PARAM] == VALUE\n    # Check that this value is reset after the exiting the context.\n    assert mpl.rcParams[PARAM] == original_value\n", "def test_context_with_multiple_styles():\n    # Test context with multiple style names and/or dictionaries.\n    original_value = 'gray'\n    mpl.rcParams[PARAM] = original_value\n    with temp_style('test', DUMMY_SETTINGS):\n        with style.context(['test', {'text.usetex': True, 'font.family': 'sans-serif'}, 'classic']):\n            assert mpl.rcParams[PARAM] == VALUE\n            assert mpl.rcParams['text.usetex'] == True\n            assert mpl.rcParams['font.family'] == 'sans-serif'\n    assert mpl.rcParams[PARAM] == original_value\n    assert mpl.rcParams['text.usetex'] == (not True)\n    assert mpl.rcParams['font.family'] == 'sans-serif'  # will be default family\n"], "sample_10": ["def test_year_lookup_bounds(self):\n    season = Season.objects.create(year=2012)\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__year=2012),\n        ['<Season: 2012>'],\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__gt=2012),\n        []\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__lt=2012),\n        []\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__gte=2012),\n        ['<Season: 2012>']\n    )\n    self.assertQuerysetEqual(\n        Season.objects.filter(year__lte=2012),\n        ['<Season: 2012>']\n    )\n", "def test_year_lookup_exact(self):\n    \"\"\"\n    year_lookup_exact works correctly with a direct value.\n    \"\"\"\n    self.assertQuerysetEqual(\n        Article.objects.filter(pub_date__year__exact=2005),\n        ['<Article: Article 1>', '<Article: Article 2>', '<Article: Article 3>',\n         '<Article: Article 4>', '<Article: Article 5>', '<Article: Article 6>',\n         '<Article: Article 7>']\n    )\n", "def test_custom_lookup_not_none_rhs(self):\n    \"\"\"\n    A custom lookup which has Lookup.can_use_none_as_rhs=False does not allow None as a lookup value.\n    \"\"\"\n    msg = \"None value is not allowed in lookup query for 'isnull_none_rhs'\"\n    with self.assertRaisesMessage(ValueError, msg):\n        Season.objects.filter(nulled_text_field__isnull_none_rhs=None)\n", "def test_transform_ordering(self):\n    # Test ordering of transforms\n    class MyTransform(Transform):\n        bilateral = True\n        arity = 1\n\n    class MyLookup(Lookup):\n            return [MyTransform]\n\n    field = Field()\n    lookup = MyLookup(field, 'value')\n    transformed = lookup.apply_bilateral_transforms('value')\n    self.assertEqual(transformed, 'value')\n\n    lookup = MyLookup(field, lookup)\n    transformed = lookup.apply_bilateral_transforms('value')\n    self.assertEqual(transformed, 'value')\n\n    lookup = MyLookup(field, [lookup])\n    transformed = lookup.apply_bilateral_transforms('value')\n    self.assertEqual(transformed, 'value')\n\n    lookup = MyLookup(field, [lookup, lookup])\n    transformed = lookup.apply_bilateral_transforms('value')\n    self.assertEqual(transformed, 'value')\n", "def test_transforms_on_nested_querysets(self):\n    \"\"\"Test that transforms work on nested QuerySets.\"\"\"\n    from django.db.models import Subquery\n    article_queryset = Article.objects.filter(author=Author.objects.get(name='Author 1'))\n    self.assertEqual(\n        Article.objects.filter(pub_date__year__gte=2005).filter(id__in=article_queryset),\n        [self.a1, self.a2, self.a3, self.a4, self.a5, self.a6, self.a7]\n    )\n    article_queryset = Article.objects.filter(author=Author.objects.get(name='Author 2'))\n    self.assertEqual(\n        Article.objects.filter(pub_date__year__gte=2005).filter(id__in=Subquery(article_queryset.values('id'))),\n        [self.a5, self.a6, self.a7]\n    )\n"], "sample_25": ["def test_alter_unique_together_with_removed_field(self):\n    \"\"\"\n    Altering unique_together when a field is removed should still work.\n    \"\"\"\n    changes = self.get_changes([self.author_empty, self.book_foo_together], [self.author_empty, self.book])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, \"otherapp\", 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"AlterUniqueTogether\", \"RemoveField\"])\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 0, name=\"book\", unique_together=set())\n    self.assertOperationAttributes(changes, \"otherapp\", 0, 1, model_name=\"book\", name=\"author\")\n\n", "def test_rename_field_with_different_type(self):\n    \"\"\"Tests autodetection of renamed fields with different types.\"\"\"\n    before = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('field', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.IntegerField()),\n        ]),\n    ]\n    after = [\n        ModelState('app', 'Foo', [\n            ('id', models.AutoField(primary_key=True)),\n            ('renamed_field', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'Bar', [\n            ('id', models.AutoField(primary_key=True)),\n            ('foo', models.IntegerField()),\n        ]),\n    ]\n    changes = self.get_changes(before, after, MigrationQuestioner({'ask_rename': True}))\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RenameField'])\n    self.assertOperationAttributes(\n        changes, 'app', 0, 0, model_name='bar', old_name='foo', new_name='foo'\n    )\n    self.assertEqual(changes['app'][0].operations[0].field.deconstruct(), (\n        'foo',\n        'django.db.models.IntegerField',\n        [],\n        {},\n    ))\n", "def test_add_model_with_field_removed_from_base_model(self):\n    \"\"\"\n    Removing a base field takes place before adding a new inherited model\n    that has a field with the same name.\n    \"\"\"\n    before = [\n        ModelState('app', 'readable', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n            ('extra', models.CharField(max_length=200)),\n        ]),\n        ModelState('app', 'readable_book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('extra', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    after = [\n        ModelState('app', 'readable_book', [\n            ('id', models.AutoField(primary_key=True)),\n            ('title', models.CharField(max_length=200)),\n        ], bases=('app.readable',)),\n    ]\n    changes = self.get_changes(before, after)\n    self.assertNumberMigrations(changes, 'app', 1)\n    self.assertOperationTypes(changes, 'app', 0, ['RemoveField', 'CreateModel'])\n    self.assertOperationAttributes(changes, 'app', 0, 0, name='extra', model_name='readable_book')\n    self.assertOperationAttributes(changes, 'app', 0, 1, name='readable_book')\n", "def test_alter_field_to_set_null_with_default(self):\n    \"\"\"\n    #23609 - Tests autodetection of nullable to non-nullable alterations.\n    \"\"\"\n    changes = self.get_changes([self.author_name], [self.author_name_null_default])\n    self.assertEqual(changes, {})\n    changes = self.get_changes([self.author_name_null], [self.author_name_null_default])\n    self.assertEqual(changes, {})\n    changes = self.get_changes([self.author_name_null], [self.author_empty])\n    self.assertEqual(changes, {})\n    changes = self.get_changes([self.author_name_null_default], [self.author_empty])\n    self.assertEqual(len(changes), 1)\n    self.assertOperationTypes(changes, 'testapp', 0, [\"AlterField\"])\n    self.assertOperationAttributes(changes, 'testapp', 0, 0, name=\"name\", null=False)\n    self.assertOperationFieldAttributes(changes, 'testapp', 0, 0, default='Ada Lovelace')\n", "def test_alter_manager(self):\n    \"\"\"Changing the managers of a model adds a new operation.\"\"\"\n    changes = self.get_changes([self.author_empty, self.book], [self.author_empty, self.book, self.author_with_managers])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 0)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"CreateModel\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Book\")\n    self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                     ['book_qs', 'book_mgr'])\n\n    # Changing them back to the original should also make a change\n    changes = self.get_changes([self.author_empty, self.book, self.author_with_managers], [self.author_empty, self.book])\n    # Right number/type of migrations?\n    self.assertNumberMigrations(changes, 'testapp', 0)\n    self.assertNumberMigrations(changes, 'otherapp', 1)\n    self.assertOperationTypes(changes, 'otherapp', 0, [\"DeleteModel\", \"AlterModelManagers\"])\n    self.assertOperationAttributes(changes, 'otherapp', 0, 0, name=\"Book\")\n    self.assertEqual([name for name, mgr in changes['otherapp'][0].operations[0].managers],\n                     ['book_qs', 'book_mgr_kwargs'])\n    self.assertEqual(changes['otherapp'][0].operations[1].managers[1][1].args, ('a', 'b'))\n"], "sample_9": ["    def test_empty_module_spec(self):\n        \"\"\"Module without a __spec__ is not included in the result.\"\"\"\n        module = types.ModuleType('test_module')\n        self.assertEqual(autoreload.iter_modules_and_files((module,), frozenset()), frozenset())\n", "    def patch_autoreload(self, argv):\n        patch_call = mock.patch('django.utils.autoreload.subprocess.call', return_value=0)\n        patches = [\n            mock.patch('django.utils.autoreload.sys.argv', argv),\n            mock.patch('django.utils.autoreload.sys.executable', self.executable),\n            mock.patch('django.utils.autoreload.sys.warnoptions', ['all']),\n        ]\n        for p in patches:\n            p.start()\n            self.addCleanup(p.stop)\n        mock_call = patch_call.start()\n        self.addCleanup(patch_call.stop)\n        return mock_call\n", "    def test_modules_with_no_origin(self):\n        module = types.ModuleType('test_module')\n        self.assertEqual(autoreload.iter_modules_and_files((module,), frozenset()), frozenset())\n", "    def test_common_roots_empty_list(self):\n        \"\"\"\n        When an empty list of paths is passed to common_roots(), it returns an\n        empty list.\n        \"\"\"\n        self.assertEqual(autoreload.common_roots([]), ())\n", "    def test_file_added_after_import(self):\n        \"\"\"\n        When a file is added after a module has been imported, it's returned by\n        iter_all_python_module_files().\n        \"\"\"\n        filename = self.temporary_file('test_added_after_import.py')\n        filename.touch()\n\n        with extend_sys_path(str(filename.parent)):\n            self.import_and_cleanup('test_added_after_import')\n\n        self.assertFileFound(filename)\n        self.assertFileFound(filename)\n\n        filename.write_text('import os')\n        with extend_sys_path(str(filename.parent)):\n            self.import_and_cleanup('test_added_after_import')\n        self.assertFileNotFound(filename)\n"], "sample_96": ["def test_dtype_match_lsqr():\n    # Test dtype consistency of Ridge coefficients and predictions when using\n    # the lsqr solver.\n\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_64 = rng.randn(n_samples, n_features)\n    y_64 = rng.randn(n_samples)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    ridge_32 = Ridge(alpha=alpha, solver='lsqr')\n    ridge_32.fit(X_32, y_32)\n    coef_32 = ridge_32.coef_\n    prediction_32 = ridge_32.predict(X_32)\n\n    ridge_64 = Ridge(alpha=alpha, solver='lsqr')\n    ridge_64.fit(X_64, y_64)\n    coef_64 = ridge_64.coef_\n    prediction_64 = ridge_64.predict(X_64)\n\n    assert coef_32.dtype == X_32.dtype\n    assert coef_64.dtype == X_64.dtype\n    assert prediction_32.dtype == X_32.dtype\n    assert prediction_64.dtype == X_64.dtype\n    assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n    assert_almost_equal(ridge_32.predict(X_32), ridge_64.predict(X_64), decimal=5)\n", "def test_ridge_solver_precision():\n    # Test that the solver returns the correct coefficients and intercept\n    # for various precision types.\n\n    rng = np.random.RandomState(0)\n    alpha = 1.0\n\n    n_samples, n_features = 6, 5\n    X_64 = rng.randn(n_samples, n_features)\n    y_64 = rng.randn(n_samples)\n    X_32 = X_64.astype(np.float32)\n    y_32 = y_64.astype(np.float32)\n\n    solvers = [\"svd\", \"sparse_cg\", \"cholesky\", \"lsqr\", \"sag\", \"saga\"]\n\n    for solver in solvers:\n        # Check type consistency 32bits\n        ridge_32 = Ridge(alpha=alpha, solver=solver)\n        ridge_32.fit(X_32, y_32)\n        coef_32 = ridge_32.coef_\n        intercept_32 = ridge_32.intercept_\n\n        # Check type consistency 64 bits\n        ridge_64 = Ridge(alpha=alpha, solver=solver)\n        ridge_64.fit(X_64, y_64)\n        coef_64 = ridge_64.coef_\n        intercept_64 = ridge_64.intercept_\n\n        # Do the actual checks at once for easier debug\n        assert coef_32.dtype == X_32.dtype\n        assert coef_64.dtype == X_64.dtype\n        assert intercept_32.dtype == X_32.dtype\n        assert intercept_64.dtype == X_64.dtype\n        assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)\n        assert_almost_equal(ridge_32.intercept_, ridge_64.intercept_, decimal=5)\n", "def test_ridge_solver_consistency():\n    # Test that ridge_regression returns the same results for different solvers.\n    rng = np.random.RandomState(0)\n\n    n_samples, n_features = 5, 10\n    y = rng.randn(n_samples)\n    X = rng.randn(n_samples, n_features)\n    alpha = 1.0\n\n    for solver in ('svd', 'cholesky', 'sparse_cg', 'lsqr'):\n        ridge1 = Ridge(alpha=alpha, solver=solver)\n        ridge2 = Ridge(alpha=alpha, solver='auto')\n\n        ridge1.fit(X, y)\n        ridge2.fit(X, y)\n\n        assert_array_almost_equal(ridge1.coef_, ridge2.coef_)\n        assert_equal(ridge1.score(X, y), ridge2.score(X, y))\n", "def test_ridge_solver_change():\n    # Tests whether setting solver='auto' forces usage of the cholesky solver for sparse data\n    # and sparse_cg for non-sparse data\n\n    # Test case 1: sparse data\n    X_sparse = sp.csr_matrix(np.eye(10))\n    y = np.ones(10)\n    ridge = Ridge(solver='auto')\n    assert_equal(ridge.solver, 'cholesky')\n    ridge.fit(X_sparse, y)\n\n    # Test case 2: dense data\n    X_dense = np.eye(10)\n    y = np.ones(10)\n    ridge = Ridge(solver='auto')\n    assert_equal(ridge.solver, 'sparse_cg')\n    ridge.fit(X_dense, y)\n\n    # Test case 3: sparse data, sample_weight is provided\n    X_sparse = sp.csr_matrix(np.eye(10))\n    y = np.ones(10)\n    ridge = Ridge(solver='auto')\n    assert_equal(ridge.solver, 'cholesky')\n    ridge.fit(X_sparse, y, sample_weight=np.ones(10))\n\n    # Test case 4: dense data, sample_weight is provided\n    X_dense = np.eye(10)\n    y = np.ones(10)\n    ridge = Ridge(solver='auto')\n    assert_equal(ridge.solver, 'sparse_cg')\n    ridge.fit(X_dense, y, sample_weight=np.ones(10))\n", "def test_ridge_invalid_input():\n    \"\"\"Test invalid input to ridge regression\n\n    Tests that the function correctly raises errors for invalid inputs.\n    \"\"\"\n\n    # Test that an error is raised when X is not a 2D array\n        X = [1, 2, 3]\n        y = [1, 2, 3]\n        assert_raise_message(TypeError, \"X must be a 2D array\",\n                             ridge_regression, X, y, alpha=1.0)\n    test_non_2d_input()\n\n    # Test that an error is raised when y is not a 1D array\n        X = [[1, 2], [3, 4]]\n        y = [1, 2, 3]\n        assert_raise_message(TypeError, \"y must be a 1D array\",\n                             ridge_regression, X, y, alpha=1.0)\n    test_non_1d_input()\n\n    # Test that an error is raised when alpha is not a positive number\n        X = [[1, 2], [3, 4]]\n        y = [1, 2]\n        assert_raise_message(ValueError, \"alpha must be a positive number\",\n                             ridge_regression, X, y, alpha=-1.0)\n    test_non_positive_alpha()\n\n    # Test that an error is raised when X and y do not have the same number of samples\n        X = [[1, 2], [3, 4]]\n        y = [1, 2, 3]\n        assert_raise_message(ValueError, \"Number of samples in X and y does not correspond:\",\n                             ridge_regression, X, y, alpha=1.0)\n    test_diff_sample_size()\n\n    # Test that a warning is raised when the solver is 'sag' and fit_intercept is True\n        X = sp.csr_matrix([[1, 2], [3, 4]])\n        y = [1, 2]\n        assert_warns(UserWarning, Ridge, alpha=1.0, fit_intercept=True, solver='sag')\n    test_sag_intercept_warning()\n"], "sample_94": ["def test_getstatementrange_ast_bug3() -> None:\n    source = Source(\n        \"\"\"\\", "    def setup_class(self) -> None:\n        self.source = \"\"\"\\", "def test_getstatementrange_within_nested_try_finally() -> None:\n    source = Source(\n        \"\"\"\n        try:\n            try:\n                pass\n            finally:\n                42\n        finally:\n            raise ValueError\n        \"\"\"\n    )\n    assert len(source) == 6\n    assert source.getstatementrange(1) == (0, 6)\n    assert source.getstatementrange(2) == (1, 3)\n    assert source.getstatementrange(3) == (2, 4)\n    assert source.getstatementrange(4) == (5, 6)\n    assert source.getstatementrange(5) == (4, 6)\n    assert source.getstatementrange(6) == (0, 6)\n", "def test_getstatementrange_with_triple_quoted_string_issue96() -> None:\n    source = Source(\n        \"\"\"\\", "def test_getstatementrange_empty_source() -> None:\n    source = Source(\"\")\n    pytest.raises(IndexError, lambda: source.getstatementrange(0))\n"], "sample_0": ["def test_build_attrs_empty_label(self):\n    form = AlbumForm()\n    attrs = form['featuring'].field.widget.get_context(name='name', value=None, attrs={})['widget']['attrs']\n    self.assertIn('data-placeholder', attrs)\n    self.assertEqual(attrs['data-placeholder'], self.empty_option.split()[1])\n", "    def test_render_checkbox(self):\n        # Check that the 'clear' checkbox is rendered with the correct\n        # attributes and text.\n        form = RequiredBandForm()\n        output = form['band'].field.widget.render('band', None)\n        checkbox = form['band'].field.widget.get_context('band', None, {})['widget']['subwidgets'][0]\n        self.assertIn('type=\"checkbox\"', checkbox['widget']['attrs'])\n        self.assertIn('id=\"id_band-clear\"', checkbox['widget']['attrs'])\n        self.assertEqual(checkbox['widget']['checkbox_name'], 'band-clear')\n        self.assertEqual(checkbox['widget']['label'], 'Clear')\n        self.assertEqual(checkbox['widget']['input_text'], 'Change')\n        self.assertEqual(checkbox['widget']['initial_text'], 'Currently')\n        self.assertEqual(checkbox['widget']['clear_checkbox_label'], 'Clear')\n", "    def test_build_attrs_no_data_ajax_params(self):\n        form = AlbumForm()\n        attrs = form['band'].field.widget.get_context(name='my_field', value=None, attrs={})['widget']['attrs']\n        self.assertNotIn('data-ajax--cache', attrs)\n        self.assertNotIn('data-ajax--type', attrs)\n        self.assertNotIn('data-ajax--url', attrs)\n        self.assertNotIn('data-ajax--method', attrs)\n", "    def test_empty_option_disabled(self):\n        \"\"\"Empty option is present and disabled if the field is required.\"\"\"\n        form = RequiredBandForm()\n        output = form.as_table()\n        self.assertIn(self.empty_option, output)\n        self.assertIn('disabled', output)\n        self.assertIn('style=\"opacity: 0.5;\"', output)\n", "    def test_render_options_not_required_field_multiple_values(self):\n        \"\"\"Test multiple values for not required fields.\"\"\"\n        beatles = Band.objects.create(name='The Beatles', style='rock')\n        who = Band.objects.create(name='The Who', style='rock')\n        album = Album.objects.create(band=beatles, featuring=[beatles, who])\n        form = AlbumForm(instance=album)\n        output = form.as_table()\n        selected_option = '<option value=\"%s\" selected>The Beatles</option>' % beatles.pk\n        option = '<option value=\"%s\" selected>The Who</option>' % who.pk\n        self.assertIn(selected_option, output)\n        self.assertIn(option, output)\n"], "sample_27": ["def test_token_with_different_password(self):\n    \"\"\"Updating the user password invalidates the token.\"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"\n    Changing the user password invalidates the token.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "def test_token_with_different_password(self):\n    \"\"\"\n    A valid token can be created with a new password, but not with the old one.\n    \"\"\"\n    user = User.objects.create_user('tokentestuser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpassword')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n", "    def test_hash_value_generation(self):\n        user = User.objects.create_user('hashuser', 'test@example.com', 'testpw')\n        p0 = PasswordResetTokenGenerator()\n        hash_value = p0._make_hash_value(user, p0._num_seconds(p0._now()))\n        user_with_different_password = User.objects.create_user('hashuser2', 'test@example.com', 'newpw')\n        hash_value_different_password = p0._make_hash_value(user_with_different_password, p0._num_seconds(p0._now()))\n        self.assertNotEqual(hash_value, hash_value_different_password)\n\n        user_with_same_password = User.objects.create_user('hashuser3', 'test@example.com', 'testpw')\n        hash_value_same_password = p0._make_hash_value(user_with_same_password, p0._num_seconds(p0._now()))\n        self.assertEqual(hash_value, hash_value_same_password)\n", "def test_token_with_different_user_state(self):\n    \"\"\"The token is invalidated when the user's password changes.\"\"\"\n    user = User.objects.create_user('passwordchangeduser', 'test2@example.com', 'testpw')\n    p0 = PasswordResetTokenGenerator()\n    tk1 = p0.make_token(user)\n    self.assertIs(p0.check_token(user, tk1), True)\n    user.set_password('newpw')\n    user.save()\n    self.assertIs(p0.check_token(user, tk1), False)\n"], "sample_145": ["def test_latex_Dict():\n    d = {Symbol('x'): 1, Symbol('y'): 2, Symbol('z'): 3}\n    assert latex(d) == r'\\left\\{ x : 1, \\  y : 2, \\  z : 3\\right\\}'\n", "def test_latex_Dict():\n    from sympy import symbols\n    from sympy.abc import x, y\n    d = {x: 1, y: 2}\n    assert latex(d) == r'\\left\\{ x : 1, \\  y : 2\\right\\}'\n", "def test_latex_unified_transforms():\n    from sympy import symbols, sin\n    x = symbols('x')\n    assert latex(UnifiedTransform(f(x), x, x)) == \\\n        r'\\mathcal{{{}}}{}_{{{}}}\\left[{}\\right]\\left({}\\right)'\n    assert latex(UnifiedTransform(f(x), x, x, name='M')) == \\\n        r'\\mathcal{M}\\left[{}\\right]\\left({}\\right)'\n    assert latex(UnifiedTransform(f(x), x, x, inverse=True)) == \\\n        r'\\mathcal{{{}}}^{(-1)}\\left[{}\\right]\\left({}\\right)'\n    assert latex(UnifiedTransform(f(x), x, x, name='M', inverse=True)) == \\\n        r'\\mathcal{M}^{(-1)}\\left[{}\\right]\\left({}\\right)'\n", "def test_latex_Morphism():\n    from sympy.categories import NamedMorphism\n    from sympy import sin\n    A = Object(\"A\")\n    B = Object(\"B\")\n    f = NamedMorphism(A, B, \"f\")\n    assert latex(f) == r\"f:A\\rightarrow B\"\n    assert latex(f.domain) == r\"A\"\n    assert latex(f.codomain) == r\"B\"\n    assert latex(f.name) == r\"f\"\n    assert latex(f.components) == r\"f\"\n    assert latex(f**2) == r\"f^{2}:A\\rightarrow B\"\n    assert latex(f*A) == r\"A \\left(f\\right)\"\n    assert latex(f * sin) == r\"f \\left(s \\mapsto \\sin{\\left(s \\right)}\\right)\"\n", "def test_latex_Float_numbers():\n    from sympy import Float\n    f = Float('0.5')\n    assert latex(f) == r\"0.5\"\n    f = Float('0.50')\n    assert latex(f) == r\"0.50\"\n    f = Float('0.500')\n    assert latex(f) == r\"0.500\"\n    f = Float('5e-1')\n    assert latex(f) == r\"5 \\cdot 10^{-1}\"\n    f = Float('5e-01')\n    assert latex(f) == r\"5 \\cdot 10^{-1}\"\n    f = Float('5e-001')\n    assert latex(f) == r\"5 \\cdot 10^{-3}\"\n    f = Float('5e-2')\n    assert latex(f) == r\"0.05\"\n    f = Float('5e-02')\n    assert latex(f) == r\"0.05\"\n    f = Float('5e-020')\n    assert latex(f) == r\"0.00005\"\n    f = Float('5e+2')\n    assert latex(f) == r\"500.0\"\n    f = Float('5e+02')\n    assert latex(f) == r\"500.0\"\n    f = Float('5e+020')\n    assert latex(f) == r\"50000000.0\"\n\n    f = Float('123456.78901234')\n    assert latex(f) == r\"1.23456789012334 \\cdot 10^{2}\"\n    f = Float('1234.56789012334e-2')\n    assert latex(f) == r\"1.23456789012334 \\cdot 10^{-1}\"\n    f = Float('1234.56789012334e-03')\n    assert latex(f) == r\"1.23456789012334 \\cdot 10^{-3}\"\n    f = Float('1234.56789012334e-04')\n    assert latex(f) == r\"1.23456789012334 \\cdot 10^{-4}\"\n    f = Float('1234.56789012334e-05')\n    assert latex(f) == r\"1.23456789012334 \\cdot 10^{-5}\"\n    f = Float('1234.56789012334e-06')\n    assert latex(f) == r\"1.23456789012334 \\cdot"], "sample_1": ["def test_write_table_qdp_write_comments(tmp_path):\n    example_qdp = \"\"\"\n    ! Initial comment line 1\n    ! Initial comment line 2\n    READ TERR 1\n    READ SERR 3\n    ! Table 0 comment\n    !a a(pos) a(neg) b c ce d\n    53000.5   0.25  -0.5   1  1.5  3.5 2\n    54000.5   1.25  -1.5   2  2.5  4.5 3\n    NO NO NO NO NO\n    ! Table 1 comment\n    !a a(pos) a(neg) b c ce d\n    54000.5   2.25  -2.5   NO  3.5  5.5 5\n    55000.5   3.25  -3.5   4  4.5  6.5 nan\n    \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n\n    t = Table.read(example_qdp, format=\"ascii.qdp\", table_id=1, names=[\"a\", \"b\", \"c\", \"d\"])\n    _write_table_qdp(t, test_file)\n\n    with open(test_file, \"r\") as fp:\n        lines = fp.readlines()\n\n    assert lines[0] == \"! Initial comment line 1\\n\"\n    assert lines[1] == \"! Initial comment line 2\\n\"\n    assert lines[2] == \"READ TERR 1\\n\"\n    assert lines[3] == \"READ SERR 3\\n\"\n    assert lines[4] == \"Table 0 comment\\n\"\n    assert lines[5] == \"a a(pos) a(neg) b c ce d\\n\"\n    assert lines[6] == \"53000.5   0.25  -0.5   1  1.5  3.5 2\\n\"\n    assert lines[7] == \"54000.5   1.25  -1.5   2  2.5  4.5 3\\n\"\n    assert lines[8] == \"NO NO NO NO NO\\n\"\n    assert lines[9] == \"Table 1 comment\\n\"\n", "def test_write_table_qdp_simple(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t = Table()\n    t.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t.add_column(\n        MaskedColumn(\n            data=[4.0, np.nan, 3.0], name=\"b\", mask=[False, True, False]\n        )\n    )\n    _write_table_qdp(t, filename=test_file)\n    with open(test_file, \"r\") as fobj:\n        lines = fobj.readlines()\n    assert lines[2].startswith(\"READ\")\n    assert lines[4].startswith(\"a\")\n    assert lines[5].startswith(\"1\")\n    assert lines[6].startswith(\"2\")\n    assert lines[7].startswith(\"3\")\n    assert lines[8].startswith(\"b\")\n    assert lines[9].startswith(\"4.0\")\n    assert lines[10].startswith(\"NO\")\n", "def test_read_write_empty(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t = Table()\n    with pytest.raises(UserWarning) as record:\n        t.write(test_file, format=\"ascii.qdp\")\n    assert \"Warning: Writing an empty table\" in str(record.value)\n    with pytest.raises(ValueError) as record:\n        _read_table_qdp(test_file)\n    assert \"table_id not specified\" in str(record.value)\n", "def test_roundtrip_with_comments(tmp_path):\n    example_qdp = \"\"\"\n        ! Initial comment line 1\n        ! Initial comment line 2\n        READ TERR 1\n        READ SERR 3\n        ! Table 0 comment\n        !a a(pos) a(neg) b c ce d\n        53000.5   0.25  -0.5   1  1.5  3.5 2\n        54000.5   1.25  -1.5   2  2.5  4.5 3\n        NO NO NO NO NO\n        ! Table 1 comment\n        !a a(pos) a(neg) b c ce d\n        54000.5   2.25  -2.5   NO  3.5  5.5 5\n        55000.5   3.25  -3.5   4  4.5  6.5 nan\n        \"\"\"\n    test_file = tmp_path / \"test.qdp\"\n    test_file2 = tmp_path / \"test2.qdp\"\n\n    t = Table.read(\n        example_qdp, format=\"ascii.qdp\", table_id=1, names=[\"a\", \"b\", \"c\", \"d\"]\n    )\n    t.write(test_file, err_specs={\"terr\": [1], \"serr\": [3]})\n\n    with pytest.raises(ValueError):\n        t.write(test_file2, err_specs={\"terr\": [1, 2, 3], \"serr\": [3]})\n\n    t2 = Table.read(test_file, names=[\"a\", \"b\", \"c\", \"d\"], table_id=0)\n    for col1, col2 in zip(t.itercols(), t2.itercols()):\n        assert np.allclose(col1, col2, equal_nan=True)\n\n    t3 = Table.read(test_file, names=[\"a\", \"b\", \"c\", \"d\", \"ce\"], table_id=0)\n    assert \"ce\" not in t3.colnames\n", "def test_read_write_error_specification(tmp_path):\n    test_file = tmp_path / \"test.qdp\"\n    t1 = Table()\n    t1.add_column(Column(name=\"a\", data=[1, 2, 3]))\n    t1.add_column(Column(name=\"b\", data=[1.0, 2.0, 3.0]))\n    t1.add_column(Column(name=\"c\", data=[1.0, 2.0, 3.0]))\n    t1.add_column(Column(name=\"d\", data=[1.0, 2.0, 3.0]))\n    t1.add_column(Column(name=\"e\", data=[1.0, 2.0, 3.0]))\n    t1.add_column(Column(name=\"f\", data=[1.0, 2.0, 3.0]))\n    t1.write(test_file, err_specs={\"serr\": [3, 4], \"terr\": [1]})\n\n    t2 = Table.read(test_file, table_id=0, names=[\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"])\n    assert np.allclose(t2[\"a\"], [1, 2, 3])\n    assert np.allclose(t2[\"a_perr\"], [3, 4, 0])\n    assert np.allclose(t2[\"a_nerr\"], [0, 0, 0])\n    assert np.allclose(t2[\"b\"], [1.0, 2.0, 3.0])\n    assert np.allclose(t2[\"b_perr\"], [0, 0, 0])\n    assert np.allclose(t2[\"b_nerr\"], [0, 0, 0])\n    assert np.allclose(t2[\"c\"], [1.0, 2.0, 3.0])\n    assert np.allclose(t2[\"c_perr\"], [0, 0, 0])\n    assert np.allclose(t2[\"c_nerr\"], [0, 0, 0])\n    assert np.allclose(t2[\"d\"], [1.0, 2.0, 3.0])\n    assert np.allclose(t2[\"d\"], [1.0, 2.0, 3.0])\n    assert np.allclose(t2[\"d_perr\"], [0, 0, 0])\n    assert np.allclose(t"], "sample_156": ["def test_parser_mathematica_parser_special_characters():\n    parser = MathematicaParser()\n\n    # Test special characters\n    assert parser.parse_mathematica(\"x^2 + y^2\") == x**2 + y**2\n    assert parser.parse_mathematica(\"x**2 + y**2\") == x**2 + y**2\n    assert parser.parse_mathematica(\"x_ + y_\") == x + y\n    assert parser.parse_mathematica(\"x__ + y__\") == x + y\n    assert parser.parse_mathematica(\"x___ + y___\") == x + y\n    assert parser.parse_mathematica(\"x_ + y__\") == x + y\n    assert parser.parse_mathematica(\"x___ + y__\") == x + y\n    assert parser.parse_mathematica(\"x_ + y___\") == x + y\n    assert parser.parse_mathematica(\"x__ + y___\") == x + y\n    assert parser.parse_mathematica(\"x_ + y\") == x + y\n    assert parser.parse_mathematica(\"x__ + y\") == x + y\n    assert parser.parse_mathematica(\"x___ + y\") == x + y\n    assert parser.parse_mathematica(\"x + y_\") == x + y\n    assert parser.parse_mathematica(\"x + y__\") == x + y\n    assert parser.parse_mathematica(\"x + y___\") == x + y\n    assert parser.parse_mathematica(\"x + y\") == x + y\n    assert parser.parse_mathematica(\"x*y\") == x*y\n    assert parser.parse_mathematica(\"x**y\") == x**y\n    assert parser.parse_mathematica(\"x_**y_\") == x**y\n    assert parser.parse_mathematica(\"x__**y__\") == x**y\n    assert parser.parse_mathematica(\"x___**y___\") == x**y\n    assert parser.parse_mathematica(\"x_**y__\") == x**y\n    assert parser.parse_mathematica(\"x___**y__\") == x**y\n    assert parser.parse_mathematica(\"x__**y___\") == x**y\n    assert parser.parse_mathematica(\"x_**y___\") == x**y\n    assert parser.parse_mathematica(\"x**y_\")", "def test_parser_mathematica_postfix():\n    parser = MathematicaParser()\n\n    convert_chain = lambda expr: parser._from_fullformlist_to_fullformsympy(parser._from_fullform_to_fullformlist(expr))\n    convert_chain2 = lambda expr: parser._from_fullformsympy_to_sympy(convert_chain(expr))\n\n    # Test Postfix operators\n    assert convert_chain(\"Cos[x]''\") == cos(x)**2  # Test case from the issue 12036\n    assert convert_chain(\"Derivative[x][x]\") == diff(x, x)  # Test case from the issue 13435\n    assert convert_chain(\"Factorial[5]\") == factorial(5)\n    assert convert_chain(\"Factorial2[5]\") == factorial(5, 2)\n    assert convert_chain(\"Decrement[x]\") == x - 1\n    assert convert_chain(\"Increment[x]\") == x + 1\n    assert convert_chain(\"DecrementBy[x, y]\") == x - y\n    assert convert_chain(\"IncrementBy[x, y]\") == x + y\n\n    # Test invalid Postfix operators\n    raises(SyntaxError, lambda: convert_chain(\"Invalid[x]\"))\n", "def test_parser_mathematica_function_conversion():\n    parser = MathematicaParser()\n\n    # Test conversion of Mathematica functions\n    d = {\n        'IntegerDigits[12345678901234567890]': 'IntegerDigits(12345678901234567890)',\n        'IntegerPart[123.456]': 'IntegerPart(123.456)',\n        'FractionalPart[123.456]': 'FractionalPart(123.456)',\n        'Quotient[123, 456]': 'Quotient(123, 456)',\n        'Mod[123, 456]': 'Mod(123, 456)',\n        'Mod[123, 456, 7]': 'Mod(123, 456, 7)',\n        'FactorInteger[12345678901234567890]': 'FactorInteger(12345678901234567890)',\n        'Apart[123/456]': 'Apart(123/456)',\n        'Apart[123/(456+7)]': 'Apart(123/(456+7))',\n        'Apart[(123/456)*Sin[x]]': 'Apart((123/456)*sin(x))',\n        'Apart[123/(456+7)*(x+1)]': 'Apart((123/(456+7))*(x+1))',\n        'Apart[(123/456)*(x+1)]': 'Apart((123/456)*(x+1))',\n        'Apart[123/(456*x)]': 'Apart(123/(456*x))',\n        'Apart[123/(456*x + 7)]': 'Apart(123/(456*x+7))',\n        'Apart[123/(456*x + 7*x)]': 'Apart(123/(456*x+7*x))',\n        'Apart[123/(456*x + 7*x + 1)]': 'Apart(123/(456*x+7*x+1))',\n        'Apart[(123/456)*x]': 'Apart((123/456)*x)',\n        'Apart[(123/456)*x + (789/234)]': 'Apart((123/456)*x+(789/234))',\n        'Apart[123/(456*x) + 789/234]': 'Apart(123/(456*x)+789/234)',\n        'Apart[123/(456*x + 7)] + 789/234': 'Apart(123/(456*x+7))+789/234',\n        '", "def test_parser_mathematica_edge_cases():\n    parser = MathematicaParser()\n\n    chain = lambda expr: parser._from_tokens_to_fullformlist(parser._from_mathematica_to_tokens(expr))\n\n    # Empty expressions\n    assert chain(\"\") == []\n    assert chain(\"\") == []\n    assert chain(\"\") == []\n\n    # Single character\n    assert chain(\"a\") == \"a\"\n    assert chain(\"A\") == \"A\"\n    assert chain(\"\u03b1\") == \"\u03b1\"\n    assert chain(\"\u03b2\") == \"\u03b2\"\n\n    # Single characters without whitespace\n    assert chain(\"a\") == \"a\"\n    assert chain(\"b\") == \"b\"\n    assert chain(\"c\") == \"c\"\n\n    # Single operator\n    assert chain(\"+\") == \"+\"\n    assert chain(\"-\") == \"-\"\n    assert chain(\"*\") == \"*\"\n    assert chain(\"/\") == \"/\"\n    assert chain(\"^\") == \"^\"\n\n    # Double operators\n    assert chain(\"++\") == \"+\"\n    assert chain(\"+-\") == \"-\"\n    assert chain(\"*+\") == \"*\"\n    assert chain(\"*/\") == \"/\"\n    assert chain(\"*^\") == \"^\"\n\n    # Multiple characters\n    assert chain(\"a b\") == \"a*b\"\n    assert chain(\"ab\") == \"a*b\"\n    assert chain(\"a,b\") == \"a,b\"\n    assert chain(\"ab,\") == \"a,b\"\n    assert chain(\"a,bc\") == \"a,b*c\"\n\n    # Enclosures\n    assert chain(\"(x)\") == \"x\"\n    assert chain(\"(a + b)\") == \"a + b\"\n    assert chain(\"(a + b + c)\") == \"a + b + c\"\n    assert chain(\"(a + b + (c + d))\") == \"a + b + c + d\"\n    assert chain(\"(a + (b + c) + d)\") == \"a + b + c + d\"\n\n    # Matrix\n    assert chain(\"{a, b, c}\") == [\"List\", \"a\", \"b\", \"c\"]\n    assert chain(\"{a, {b, c}}\") == [\"List\", \"a\", [\"List\", \"b\", \"c\"]]\n    assert chain(\"{{a, b}, {c, d}}\") == [\"List\", [\"List\", \"a\", \"b\"], [\"List\", \"c\", \"d\"]]\n\n    # Slot\n    assert chain(\"#\") == [\"Slot\", \"1\"]\n    assert chain(\"#2", "def test_parser_mathematica_invalid_function_form():\n    parser = MathematicaParser()\n\n    # Invalid function forms\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f(x,y)'\": \"f(x,y)\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f()'\": \"f()\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f(x,y,z'\": \"f(x,y,z\")})\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f(,x'\": \"f(,x\")})\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f()'\": \"f(,\"}))\n\n    # Invalid arguments for the function\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y,z'\": \"f(x,y,z)\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y\" : \"f(x,y\" }))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y]'\": \"f(x,y\")})\n\n    # Invalid cases\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y]'\": \"f(x,y)\"})\n\n    # Invalid arguments for variable-length arguments\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x_,y,*z]'\": \"f(x,y,z)\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x_,*y,z]'\": \"f(x,y,z)\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y_,*z]'\": \"f(x,y,z)\"}))\n\n    # Invalid bracket count\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y]]'\": \"f(x,y)\"}))\n    assert raises(ValueError, lambda: parser._compile_dictionary({\"'f[x,y'\": \"f(x,y)\"}))\n"], "sample_143": ["def test_pretty_AssignmentBase():\n    from sympy import Assignment\n    from sympy.core.expr import UnevaluatedExpr\n    a, b = symbols('a b')\n    expr = UnevaluatedExpr(Assignment(a, b))\n    ascii_str = \\", "def test_issue_18628():\n    from sympy import rootof\n    expr = rootof(x**4 + 1, 0)\n    ascii_str = \\", "def test_pretty_Series():\n    n = symbols('n')\n    a = symbols('a')\n    f = symbols('f')\n    g = symbols('g')\n    h = symbols('h')\n\n    tf1 = TransferFunction(n, n + 1, n)\n    tf2 = TransferFunction(a, a + 1, a)\n    tf3 = TransferFunction(f, f + 1, f)\n    tf4 = TransferFunction(g, g + 1, g)\n    tf5 = TransferFunction(h, h + 1, h)\n\n    expected1 = \\", "compilation error", "def test_pretty_Series_classes():\n    from sympy import Product, Sum\n    s1 = Sum(x, (x, 0, 1))\n    s2 = Sum(x, (x, 0, oo))\n    s3 = Sum(x, (x, oo, 1))\n    s4 = Sum(x**2, (x, 0, 1))\n\n    p1 = Product(x, (x, 1, 2))\n    p2 = Product(x, (x, oo, 1))\n    p3 = Product(x, (x, 1, oo))\n    p4 = Product(x**2, (x, 1, 2))\n\n    assert pretty(s1) == \"1\"\n    assert pretty(s2) == \"oo\"\n    assert pretty(s3) == \"oo\"\n    assert pretty(s4) == \"oo\"\n\n    assert pretty(p1) == \"2\"\n    assert pretty(p2) == \"oo\"\n    assert pretty(p3) == \"oo\"\n    assert pretty(p4) == \"4\"\n\n    assert upretty(s1) == \"\u2211\u2080\u00b9 x\"\n    assert upretty(s2) == \"\u2211\u2080\u221e x\"\n    assert upretty(s3) == \"\u2211\u207b\u221e\u00b9 x\"\n    assert upretty(s4) == \"\u2211\u2080\u00b9 x\u00b2\"\n\n    assert upretty(p1) == \"\u220f\u2081\u00b2 x\"\n    assert upretty(p2) == \"\u220f\u2080\u221e x\"\n    assert upretty(p3) == \"\u220f\u00b9\u221e x\"\n    assert upretty(p4) == \"\u220f\u2081\u00b2 x\u00b2\"\n\n    s5 = Sum(x, (x, 1, 2), (x, 3, 4))\n    p5 = Product(x, (x, 1, 2), (x, 3, 4))\n    assert pretty(s5) == \"6\"\n    assert upretty(s5) == \"\u2045\u2081\u00b2\u2046\u2046\u2083\u2044\u2044 x\"\n    assert pretty(p5) == \"24\"\n    assert upretty(p5) == \"\u220f\u2081\u00b2\u22c5\u220f\u2083\u2044\u2044 x\"\n\n    s6 = Sum(x**2, (x, 1, 2), (x, 3, 4))\n    p6 = Product(x**2, (x, 1, "], "sample_106": ["def test_warm_start_consistency():\n    \"\"\"Test that warm-starting does not change the learned transformation\n\n    Warm-starting is supposed to use the previously learned transformation as the\n    initial transformation. This test checks that it does not change the learned\n    transformation.\n    \"\"\"\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n\n    nca = NeighborhoodComponentsAnalysis(warm_start=True, max_iter=5)\n    nca.fit(X, y)\n    transformation_1 = nca.components_\n\n    nca.max_iter = 30\n    nca.fit(X, y)\n    transformation_2 = nca.components_\n\n    assert_array_almost_equal(transformation_1, transformation_2)\n", "def test_warm_start_same_data():\n    \"\"\"Test that warm starting with the same data does not raise an error.\"\"\"\n    X, y = make_classification(n_samples=30, n_features=5, n_classes=4,\n                               n_redundant=0, n_informative=5, random_state=0)\n\n    nca = NeighborhoodComponentsAnalysis(warm_start=True)\n    nca.fit(X, y)\n\n    nca_warm_start = NeighborhoodComponentsAnalysis(warm_start=True)\n    nca_warm_start.fit(X, y)\n\n    # Test that the second fit did not raise an error and the transformation\n    # is the same as the one from the first fit\n    assert not hasattr(nca_warm_start, 'components_')\n    nca_warm_start.fit(X, y)\n    assert_array_equal(nca.components_, nca_warm_start.components_)\n", "def test_init_and_warm_start():\n    # Test that the warm_start parameter does not overwrite the init parameter\n\n    rng = np.random.RandomState(42)\n    X, y = make_blobs(n_samples=30, n_features=5, n_classes=4,\n                       n_redundant=0, n_informative=5, random_state=0)\n\n    nca = NeighborhoodComponentsAnalysis(warm_start=True, max_iter=5)\n    nca.fit(X, y)\n\n    # Try to fit with a different init\n    init = rng.rand(X.shape[1], X.shape[1])\n    nca.fit(X, y)\n\n    assert not np.array_equal(nca.components_, init)\n", "def test_no_labels():\n    \"\"\"Test that an error is raised when no labels are provided.\"\"\"\n    X = np.arange(12).reshape(4, 3)\n\n    # No labels\n    assert_raise_message(ValueError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, None)\n\n    # None\n    assert_raise_message(ValueError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, None)\n\n    # Not an array\n    assert_raise_message(TypeError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, object())\n\n    # Empty array\n    assert_raise_message(ValueError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, np.array([]))\n\n    # One-dimensional array\n    assert_raise_message(ValueError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, np.array([1]))\n\n    # Three-dimensional array\n    assert_raise_message(ValueError, 'y has 4 features, expected 1',\n                         NeighborhoodComponentsAnalysis().fit, X, np.array([[[1]]]))\n", "def test_init_with_pca_before_fit():\n    \"\"\"Test that initialization with PCA is possible when fit is called after fit\n\n    The test checks that the `init='pca'` initialization option works when\n    calling fit twice.\n    \"\"\"\n    X, y = make_blobs(n_samples=30, centers=6, n_features=5, random_state=0)\n    nca = NeighborhoodComponentsAnalysis(init='pca', n_components=5)\n    nca.fit(X, y)\n    # Now, fit should be able to use the previously learned PCA\n    nca.fit(X, y)\n"], "sample_103": ["def test_mutual_info_regression_mixed():\n    # Here the target is continuous and there are two continuous and one\n    # discrete feature. The idea of this test is clear from the code.\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    X[:, 2] = (X[:, 2] > 0.5).astype(int)\n\n    y = X[:, 0] + 0.5 * X[:, 1] + 0.1 * X[:, 2]\n\n    mi = mutual_info_regression(X, y, random_state=0)\n    assert_array_equal(np.argsort(-mi), [2, 1, 0])\n    for n_neighbors in [5, 7, 9]:\n        mi_nn = mutual_info_regression(X, y, n_neighbors=n_neighbors, random_state=0)\n        # Check that the continuous values have an higher MI with greater\n        # n_neighbors\n        assert_greater(mi_nn[0], mi[0])\n        assert_greater(mi_nn[1], mi[1])\n        # The n_neighbors should not have any effect on the discrete value\n        # The MI should be the same\n        assert_equal(mi_nn[2], mi[2])\n", "def test_mutual_info_regression_mixed():\n    # Here the target is continuous and there are one continuous and two\n    # discrete features. The idea of this test is clear from the code.\n    rng = check_random_state(0)\n    X = rng.rand(1000, 3)\n    X[:, 1] += X[:, 0]\n    y = X[:, 1] + X[:, 2] + rng.randn(1000)\n\n    mi = mutual_info_regression(X, y, discrete_features=[1, 2], n_neighbors=3,\n                               random_state=0)\n    assert_array_equal(np.argsort(-mi), [1, 2, 0])\n    for n_neighbors in [5, 7, 9]:\n        mi_nn = mutual_info_regression(X, y, discrete_features=[1, 2],\n                                      n_neighbors=n_neighbors, random_state=0)\n        # Check that the continuous values have an higher MI with greater\n        # n_neighbors\n        assert_greater(mi_nn[0], mi[0])\n        assert_greater(mi_nn[1], mi[1])\n        # The n_neighbors should not have any effect on the discrete value\n        # The MI should be the same\n        assert_equal(mi_nn[2], mi[2])\n\n", "def test_mutual_info_classif_auto_discrete_features():\n    # Test that discrete_features='auto' is correctly assigned for a sparse matrix.\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=float)\n    y = np.array([0, 1, 2, 2, 1], dtype=float)\n\n    X_csr = csr_matrix(X)\n    assert mutual_info_classif(X_csr, y, discrete_features='auto') is not None\n\n    # Test that discrete_features='auto' is correctly assigned for a dense matrix.\n    assert mutual_info_classif(X, y, discrete_features='auto') is not None\n\n    # Test that discrete_features='auto' is correctly assigned for a sparse matrix\n    # with discrete features.\n    X_csr = csr_matrix(np.array([[0, 1, 0],\n                                 [1, 0, 1],\n                                 [0, 1, 0],\n                                 [0, 1, 0],\n                                 [0, 1, 0]], dtype=int))\n    y = np.array([0, 1, 2, 2, 1], dtype=int)\n    assert mutual_info_classif(X_csr, y, discrete_features='auto') is not None\n\n    # Test that discrete_features='auto' is correctly assigned for a dense matrix\n    # with discrete features.\n    assert mutual_info_classif(np.array([[0, 1, 0],\n                                        [1, 0, 1],\n                                        [0, 1, 0],\n                                        [0, 1, 0],\n                                        [0, 1, 0]], dtype=int),\n                              y, discrete_features='auto') is not None\n\n    # Test that discrete_features='auto' is correctly assigned for a sparse matrix\n    # with a mix of discrete and continuous features.\n    X_csr = csr_matrix(np.array([[0, 0.5, 0],\n                                 [1, 1, 1],\n                                 [0, 0.5, 0],\n                                 [0, 0.5, 0],\n                                 [0, 0.5, 0]], dtype=float))\n    y = np.array([0, 1, 2,", "def test_estimate_mi_discrete_target():\n    # Test that the function works with a discrete target variable.\n\n    # Test case 1: all features discrete\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=int)\n    y = np.array([0, 1, 2, 2, 1])\n\n    mi = _estimate_mi(X, y, discrete_features=True, discrete_target=True)\n    assert_array_equal(mi, np.array([0, 0, 0]))\n\n    # Test case 2: one feature continuous, rest are discrete\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=float)\n    y = np.array([0, 1, 2, 2, 1])\n\n    mi = _estimate_mi(X, y, discrete_features=True, discrete_target=True)\n    assert_array_equal(mi, np.array([0, 0, 0]))\n\n    # Test case 3: all features continuous\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=float)\n    y = np.array([0, 1, 2, 2, 1])\n\n    mi = _estimate_mi(X, y, discrete_features=False, discrete_target=True)\n    assert_array_equal(mi, np.array([0, 0, 0]))\n\n    # Test case 4: some features continuous, others discrete\n    X = np.array([[0, 0, 0],\n                  [1, 1, 0],\n                  [2, 0, 1],\n                  [2, 0, 1],\n                  [2, 0, 1]], dtype=float)\n    y = np.array([0, 1, 2, 2, 1])\n\n    mi = _estimate_mi(X, y, discrete_features=[True, False, False], discrete_target=True)\n   ", "def test_mutual_info_regression_mixed():\n    # We generate sample from multivariate normal distribution, using\n    # transformation from initially uncorrelated variables. The zero\n    # variables after transformation is selected as the target vector,\n    # it has the strongest correlation with the variable 2, and\n    # the weakest correlation with the variable 1. This test is similar to\n    # test_mutual_info_regression but it checks mixed continuous and\n    # discrete features.\n    T = np.array([\n        [1, 0.5, 2, 1],\n        [0, 1, 0.1, 0.0],\n        [0, 0.1, 1, 0.1],\n        [0, 0.1, 0.1, 1]\n    ])\n    cov = T.dot(T.T)\n    mean = np.zeros(4)\n\n    rng = check_random_state(0)\n    Z = rng.multivariate_normal(mean, cov, size=1000)\n    X = Z[:, 1:]\n    X_discrete = Z[:, 2] > 0.5\n    y = Z[:, 0]\n\n    mi = mutual_info_regression(X, y, discrete_features=X_discrete, random_state=0)\n    assert_array_equal(np.argsort(-mi), np.array([2, 0, 1]))\n    for n_neighbors in [5, 7, 9]:\n        mi_nn = mutual_info_regression(X, y, discrete_features=X_discrete,\n                                      n_neighbors=n_neighbors, random_state=0)\n        # Check that the continuous values have an higher MI with greater\n        # n_neighbors\n        assert_greater(mi_nn[0], mi[0])\n        # The n_neighbors should not have any effect on the discrete value\n        # The MI should be the same\n        assert_equal(mi_nn[1], mi[1])\n\n    # Test that the `n_neighbors` parameter does not affect the MI when\n    # `discrete_features` is a mask.\n    mi_nn = mutual_info_regression(X, y, discrete_features=np.array([True, False, True]),\n                                  n_neighbors=5, random_state=0)\n    mi_nn_2 = mutual_info_regression(X, y, discrete_features=np.array([True, False, True]),\n                                     n_neighbors=15, random_state=0)\n    assert_array_equal(mi_nn, mi_nn_2)\n"], "sample_113": ["def test_column_transformer_sparse_transformer():\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_res_first = np.array([[0], [1]])\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])], sparse_threshold=0.8)\n    assert sparse.issparse(ct.fit_transform(X_array))\n    assert_array_equal(ct.fit_transform(X_array), X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), X_res_first)\n\n    ct = ColumnTransformer(\n        [(\"trans\", Trans(), [0])], transformer_weights={\"trans\": 0.1}, sparse_threshold=0.8\n    )\n    assert sparse.issparse(ct.fit_transform(X_array))\n    assert_array_equal(ct.fit_transform(X_array), 0.1 * X_res_first)\n    assert_array_equal(ct.fit(X_array).transform(X_array), 0.1 * X_res_first)\n", "def test_column_transformer_output_dtype():\n    \"\"\"Checks for the output dtype of ColumnTransformer.\"\"\"\n    X_array = np.array([[0, 1, 2], [2, 4, 6], [8, 6, 4]])\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", StandardScaler(), [1])],\n        sparse_threshold=0.8,\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", StandardScaler(), [1])],\n        sparse_threshold=0.1,\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0]), (\"trans2\", StandardScaler(), [1])],\n        sparse_threshold=0.8,\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0])], remainder=\"passthrough\", sparse_threshold=0.8\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [0])], remainder=\"passthrough\", sparse_threshold=0.1\n    )\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0])], remainder=\"passthrough\")\n    X_trans = ct.fit_transform(X_array)\n    assert X_trans.dtype == float\n", "def test_column_transformer_get_feature_names_with_pandas_object():\n    \"\"\"Checks for the interaction between ColumnTransformer and pd objects.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_array = np.array([[0, 1, 2], [2, 4, 6]]).T\n    X_df = pd.DataFrame(X_array, columns=[\"first\", \"second\"])\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0])])\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.get_feature_names_out(X_df)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit_transform(X_df)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit(X_df).transform(X_df)\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), \"first\")])\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.get_feature_names_out(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit_transform(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit(X_array).transform(X_array)\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), \"first\")], remainder=\"passthrough\")\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.get_feature_names_out(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit_transform(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit(X_array).transform(X_array)\n\n    ct = ColumnTransformer(\n        [(\"trans1\", Trans(), [\"first\"])], remainder=\"passthrough\"\n    )\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.get_feature_names_out(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit_transform(X_array)\n    with pytest.raises(ValueError, match=\"get_feature_names_out\"):\n        ct.fit(X_array).transform(X_array)\n", "def test_column_transformer_transformer_output_dtype():\n    \"\"\"Check that ColumnTransformer passes through the output dtype of the\n    transformers.\"\"\"\n    X = np.array([[0, 1, 2], [2, 4, 6]], dtype=float).T\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])])\n    X_trans = ct.fit_transform(X)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer([(\"trans1\", Trans(), [0, 1]), (\"trans2\", Trans(), [0])])\n    X_trans = ct.fit_transform(X)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])], remainder=\"passthrough\")\n    X_trans = ct.fit_transform(X)\n    assert X_trans.dtype == float\n\n    ct = ColumnTransformer(\n        [(\"trans\", Trans(), [0, 1])], remainder=\"passthrough\", sparse_threshold=0.8\n    )\n    X_trans = ct.fit_transform(X)\n    assert X_trans.dtype == float\n\n    # test with different dtypes\n    X = np.array([[0, 1, 2], [2, 4, 6]], dtype=float).T\n    X_float32 = X.astype(np.float32)\n    X_int32 = X.astype(np.int32)\n    X_uint32 = X.astype(np.uint32)\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])])\n    X_trans_float32 = ct.fit_transform(X_float32)\n    X_trans_int32 = ct.fit_transform(X_int32)\n    X_trans_uint32 = ct.fit_transform(X_uint32)\n\n    assert X_trans_float32.dtype == np.float32\n    assert X_trans_int32.dtype == np.int32\n    assert X_trans_uint32.dtype == np.uint32\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1]), (\"trans2\", Trans(), [0])])\n    X_trans_float32 = ct.fit_transform(X_float32)\n    X_trans_int32 = ct.fit_transform(X_int32)\n    X_trans_uint32 = ct.fit_transform(X_uint32)\n\n    assert X_trans_float32.dtype == np.float32\n    assert X_trans_int32.dtype == np.int32\n    assert X_trans_uint32.dtype == np.uint32\n\n    ct = ColumnTransformer([(\"trans\", Trans(), [0, 1])], remainder=\"", "def test_column_transformer_n_jobs():\n    \"\"\"Check column transformer behavior with n_jobs.\"\"\"\n    pd = pytest.importorskip(\"pandas\")\n    X_df = pd.DataFrame(\n        {\n            \"col1\": [1, 2, 3],\n            \"col2\": [4, 5, 6],\n            \"col3\": [7, 8, 9],\n        },\n        columns=[\"col1\", \"col2\", \"col3\"],\n    )\n    ct = ColumnTransformer(\n        [\n            (\"bycol1\", \"passthrough\", [\"col1\", \"col2\"]),\n            (\"bycol2\", \"passthrough\", [\"col3\"]),\n        ]\n    )\n    ct.set_params(n_jobs=-1)\n\n    # with n_jobs=-1 (number of cores), transform() must raise an error if\n    # transformers[0] and transformers[1] do not support parallelized fit_transform.\n    with pytest.raises(NotFittedError):\n        ct.fit_transform(X_df)\n    with pytest.raises(NotFittedError):\n        ct.fit(X_df).transform(X_df)\n\n    # with n_jobs=-1 (number of cores), transform() must raise an error if\n    # transformers[0] and transformers[1] do not support parallelized transform.\n    ct = ColumnTransformer(\n        [\n            (\"bycol1\", \"passthrough\", [\"col1\", \"col2\"]),\n            (\"bycol2\", TransNo2D(), [\"col3\"]),\n        ]\n    )\n    with pytest.raises(NotFittedError):\n        ct.transform(X_df)\n\n    # check that only the input data is used in the error message\n    with pytest.raises(NotFittedError, match=\"at ColumnTransformer\"):\n        ct.fit_transform(X_df)\n    with pytest.raises(NotFittedError, match=\"at ColumnTransformer\"):\n        ct.fit(X_df).transform(X_df)\n\n    # test fit_transform with n_jobs=None\n    ct.set_params(n_jobs=None)\n    ct.fit_transform(X_df)\n\n    # test fit with n_jobs=None\n    ct.set_params(n_jobs=None)\n    ct.fit(X_df)\n\n    # test transform with n_jobs=None\n    ct.set_params(n_jobs=None)\n    ct.transform(X_df)\n\n    # test fit_transform with n_jobs=1\n    ct.set_params(n_jobs=1)\n    ct.fit_transform(X_df)\n\n    # test fit with n_jobs=1\n    ct.set_params(n_jobs=1)\n    ct"], "sample_97": ["def test_label_binarizer_class_order():\n    y = [2, 6]\n    classes = [1, 2, 4, 6]\n    lb = LabelBinarizer()\n    lb.fit(y)\n    lb.classes_ = np.array([2, 6, 4, 1])\n\n    expected = np.array([[0, 0, 0, 1]])\n    got = lb.transform(y)\n    assert_array_equal(expected, got)\n\n    expected = np.array([2])\n    got = lb.inverse_transform(got)\n    assert_array_equal(expected, got)\n\n    y = [2, 6]\n    classes = [2, 6, 4, 1]\n    lb = LabelBinarizer()\n    lb.fit(y)\n    lb.classes_ = np.array([1, 2, 6, 4])\n\n    expected = np.array([[0, 0, 1, 0]])\n    got = lb.transform(y)\n    assert_array_equal(expected, got)\n\n    expected = np.array([2])\n    got = lb.inverse_transform(got)\n    assert_array_equal(expected, got)\n", "def test_inverse_binarize_thresholding():\n    # Test _inverse_binarize_thresholding function\n\n    # Test binary case\n    y = np.array([[0, 1], [0.5, 0.5]])\n    output_type = 'binary'\n    classes = [0, 1]\n    threshold = 0.5\n    expected = np.array([0, 1])\n    got = _inverse_binarize_thresholding(y, output_type, classes, threshold)\n    assert_array_equal(got, expected)\n\n    # Test multiclass case\n    y = np.array([[0.2, 0.8], [0.3, 0.7]])\n    output_type = 'multiclass'\n    classes = [0, 1]\n    threshold = 0.6\n    expected = np.array([1, 0])\n    got = _inverse_binarize_thresholding(y, output_type, classes, threshold)\n    assert_array_equal(got, expected)\n\n    # Test multilabel-indicator case\n    y = np.array([[0, 1, 0], [1, 0, 1]])\n    output_type = 'multilabel-indicator'\n    classes = [0, 1, 2]\n    threshold = 0.5\n    expected = np.array([[0, 1, 0], [1, 0, 1]])\n    got = _inverse_binarize_thresholding(y, output_type, classes, threshold)\n    assert_array_equal(got, expected)\n\n    # Test invalid output_type\n    y = np.array([[0, 1], [0, 1]])\n    output_type = 'invalid'\n    classes = [0, 1]\n    threshold = 0.5\n    assert_raises(ValueError, _inverse_binarize_thresholding, y, output_type,\n                  classes, threshold)\n\n    # Test invalid threshold\n    y = np.array([[0, 1], [0, 1]])\n    output_type = 'binary'\n    classes = [0, 1]\n    threshold = 2\n    assert_raises(ValueError, _inverse_binarize_thresholding, y, output_type,\n                  classes, threshold)\n\n    # Test invalid classes\n    y = np.array([[0, 1], [0, 1]])\n    output_type = 'binary'\n    classes = [0, 1, 2]\n    threshold = 0.", "def test_label_binarizer_multilabel_sparse_output():\n    # test multilabel case\n    lb = LabelBinarizer(sparse_output=True)\n    y = [[1, 2], [0, 1]]\n    expected = csr_matrix([[1, 1, 0],\n                          [0, 0, 1]])\n\n    got = lb.fit_transform(y)\n    assert_equal(issparse(got), True)\n    assert_array_equal(got.indices.dtype, got.indptr.dtype)\n    got = got.toarray()\n    assert_array_equal(expected.toarray(), got)\n\n    # test inverse_transform\n    inverse = lb.inverse_transform(got)\n    expected_inverse = np.array([[1, 2], [0, 1]])\n    assert_array_equal(inverse, expected_inverse)\n", "def test_multilabel_binarizer_empty_classes():\n    # Empty class list should not be allowed\n    assert_raises(ValueError, MultiLabelBinarizer().fit_transform, [[1, 2], [3, 4]])\n    assert_raises(ValueError, MultiLabelBinarizer().fit, [[1, 2], [3, 4]])\n    assert_raises(ValueError, MultiLabelBinarizer(classes=[]).fit_transform, [[1, 2], [3, 4]])\n    assert_raises(ValueError, MultiLabelBinarizer(classes=[]).fit, [[1, 2], [3, 4]])\n\n    # Check that fit_transform does not add new classes\n    # (i.e., it should just pass through the input data)\n    mlb = MultiLabelBinarizer(classes=[1, 2])\n    inp = [[3], [2], [1]]\n    expected = np.array([[0, 1], [0, 0], [1, 0]])\n    assert_array_equal(mlb.fit_transform(inp), expected)\n    assert_array_equal(mlb.classes_, [1, 2])\n    assert_equal(mlb.inverse_transform(expected), inp)\n\n    # Check that fit does not add new classes\n    # (i.e., it should just pass through the input data)\n    mlb = MultiLabelBinarizer(classes=[1, 2])\n    inp = [[3], [2], [1]]\n    expected = np.array([[0, 1], [0, 0], [1, 0]])\n    assert_array_equal(mlb.fit(inp).transform(inp), expected)\n    assert_array_equal(mlb.classes_, [1, 2])\n    assert_equal(mlb.inverse_transform(expected), inp)\n", "def test_inverse_binarize_thresholding():\n    # test thresholding logic\n    y_type = \"binary\"\n    y = np.array([[1, 2], [2, 1]])\n\n    threshold_01 = 0.5\n    threshold_1 = 1\n    threshold_0 = 0\n\n    classes = [0, 1]\n    expected = _inverse_binarize_thresholding(y, y_type, classes,\n                                             threshold_01)\n    assert_array_equal(expected, np.array([0, 1]))\n\n    expected = _inverse_binarize_thresholding(y, y_type, classes,\n                                             threshold_1)\n    assert_array_equal(expected, np.array([0, 1]))\n\n    expected = _inverse_binarize_thresholding(y, y_type, classes,\n                                             threshold_0)\n    assert_array_equal(expected, np.array([0, 1]))\n\n    # test edge case when all values are 0 or all values are 1\n    y = np.array([[1, 1], [1, 1]])\n    expected = _inverse_binarize_thresholding(y, y_type, classes,\n                                             threshold_01)\n    assert_array_equal(expected, np.array([1, 1]))\n\n    y = np.array([[0, 0], [0, 0]])\n    expected = _inverse_binarize_thresholding(y, y_type, classes,\n                                             threshold_01)\n    assert_array_equal(expected, np.array([0, 0]))\n\n    # test output_type\n    y = np.array([[1, 2], [2, 1]])\n    expected = _inverse_binarize_thresholding(y, \"multiclass\", classes,\n                                             threshold_01)\n    assert_array_equal(expected, np.array([1, 2]))\n\n    expected = _inverse_binarize_thresholding(y, \"multiclass\", classes,\n                                             threshold_1)\n    assert_array_equal(expected, np.array([1, 2]))\n\n    expected = _inverse_binarize_thresholding(y, \"multiclass\", classes,\n                                             threshold_0)\n    assert_array_equal(expected, np.array([1, 2]))\n\n    # test output_type failure\n    assert_raises(ValueError, _inverse_binarize_thresholding, y, \"foo\",\n                 classes, threshold_0)\n"], "sample_26": ["    def test_keepdb_false(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        test_connection.settings_dict['TEST']['NAME'] = 'my_test_db'\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False, keepdb=False)\n            # Migrations run.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # App is not synced.\n            mocked_sync_apps.assert_not_called()\n            # Test database is recreated.\n            self.assertEqual(connection.settings_dict['NAME'], 'test_my_test_db')\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_migrate_apps(self, mocked_sync_apps, mocked_migrate, *mocked_objects):\n        # Migrate apps during database creation.\n        test_connection = get_connection_copy()\n        test_connection.settings_dict['TEST']['MIGRATE'] = True\n        creation = test_connection.creation_class(test_connection)\n        if connection.vendor == 'oracle':\n            # Don't close connection on Oracle.\n            creation.connection.close = mock.Mock()\n        old_database_name = test_connection.settings_dict['NAME']\n        try:\n            with mock.patch.object(creation, '_create_test_db'):\n                creation.create_test_db(verbosity=0, autoclobber=True, serialize=False)\n            # Migrations run.\n            mocked_migrate.assert_called()\n            args, kwargs = mocked_migrate.call_args\n            self.assertEqual(args, ([('app_unmigrated', '0001_initial')],))\n            self.assertEqual(len(kwargs['plan']), 1)\n            # Apps are migrated.\n            mocked_sync_apps.assert_called()\n            mocked_args, _ = mocked_sync_apps.call_args\n            self.assertEqual(mocked_args[1], {'app_unmigrated'})\n        finally:\n            with mock.patch.object(creation, '_destroy_test_db'):\n                creation.destroy_test_db(old_database_name, verbosity=0)\n", "    def test_deserialize_db_from_string_error(self, mocked_sync_apps, mocked_has_table, *mocked_objects):\n        # deserialize_db_from_string() raises an error when it receives invalid data.\n        data = \"\"\"\n        {\n            \"model\": \"backends.object\",\n            \"pk\": 1,\n            \"fields\": {\"obj_ref\": 2}  # Missing related_objects\n        }\n        \"\"\"\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        with self.assertRaises(serializers.SerializationError):\n            creation.deserialize_db_from_string(data)\n", "    def test_serialize_large_data(self):\n        # serialize_db_to_string() handles large data.\n        obj = Object.objects.create(obj_ref=1)\n        for _ in range(1000):\n            Object.objects.create(obj_ref=obj)\n        data = connection.creation.serialize_db_to_string()\n        self.assertGreater(len(data), 0)\n", "    def test_clone_test_db(self):\n        test_connection = get_connection_copy()\n        creation = test_connection.creation_class(test_connection)\n        old_database_name = test_connection.settings_dict['NAME']\n        test_db_clone_settings = creation.get_test_db_clone_settings('clone1')\n        test_connection.settings_dict = test_db_clone_settings\n        creation.clone_test_db('clone1', verbosity=0, autoclobber=False)\n        new_database_name = test_connection.settings_dict['NAME']\n        self.assertEqual(new_database_name, test_db_clone_settings['NAME'])\n        self.assertEqual(creation.get_test_db_clone_settings('clone1')['NAME'], new_database_name)\n\n        # Test cloning with existing database.\n        creation.clone_test_db('clone2', verbosity=0, autoclobber=False, keepdb=True)\n        self.assertEqual(test_connection.settings_dict['NAME'], new_database_name)\n\n        with mock.patch.object(creation, '_destroy_test_db'):\n            creation.destroy_test_db(old_database_name, verbosity=0)\n"], "sample_50": ["def test_max_cookie_length_empty(self):\n    \"\"\"\n    If the data to be stored exceeds what is allowed in a cookie and the\n    remove_oldest parameter is False, older messages are added at the start of\n    the list instead of being removed.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n\n    # When storing as a cookie, the cookie has constant overhead of approx\n    # 54 chars, and each message has a constant overhead of about 37 chars\n    # and a variable overhead of zero in the best case. We aim for a message\n    # size which will fit 4 messages into the cookie, but not 5.\n    # See also FallbackTest.test_session_fallback\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    first_msg = None\n    # Generate the same (tested) content every time that does not get run\n    # through zlib compression.\n    random.seed(42)\n    for i in range(5):\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n        if i == 0:\n            first_msg = msg\n    unstored_messages = storage._store([self.not_finished] + storage._loaded_data, response, remove_oldest=False)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 4)\n\n    self.assertEqual(len(unstored_messages), 1)\n    self.assertEqual(unstored_messages[0].message, first_msg)\n\n    # The oldest message should be at the start of the list\n    self.assertEqual(storage._loaded_data[0].message, unstored_messages[0].message)\n", "def test_update_and_consume(self):\n    \"\"\"\n    Test that consuming messages and then adding new ones works correctly.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    storage.add(constants.INFO, 'test1')\n    storage.update(response)\n    self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n    messages = []\n    for msg in storage:\n        messages.append(msg.message)\n    self.assertEqual(messages, ['test1'])\n    # Add new messages to the storage and update the cookie\n    storage.add(constants.INFO, 'test2')\n    storage.add(constants.INFO, 'test3')\n    storage.update(response)\n    # Only one message should be in the cookie\n    self.assertEqual(stored_cookie_messages_count(storage, response), 1)\n    # Consume the message in the cookie\n    for msg in storage:\n        messages.append(msg.message)\n    # Now both new messages should be in the cookie\n    self.assertEqual(stored_cookie_messages_count(storage, response), 2)\n    self.assertEqual(storage._decode(response.cookies['messages'].value), ['test2', 'test3'])\n", "    def test_compress(self):\n        storage = self.get_storage()\n        message = Message(constants.INFO, 'large message that is longer than the compression threshold')\n        encoded = storage._encode([message], encode_empty=False)\n        self.assertGreater(len(encoded), len(json.dumps(message).encode('latin-1')))\n        # Check that the message was actually compressed\n        self.assertLess(storage._decode(encoded).pop().message, json.dumps(message).encode('latin-1'))\n", "def test_update_max_cookie_size_sentinel(self):\n    \"\"\"\n    The sentinel value is correctly handled when a message is larger than the\n    max_cookie_size, and the messages are truncated.\n    \"\"\"\n    storage = self.get_storage()\n    response = self.get_response()\n    # The first message will be the one that gets truncated.\n    msg_size = int((CookieStorage.max_cookie_size - 54) / 4.5 - 37)\n    for i in range(5):\n        if i == 3:\n            # Make the last message larger than the max size.\n            msg_size += 100\n        msg = get_random_string(msg_size)\n        storage.add(constants.INFO, msg)\n\n    # Update with a smaller max cookie size.\n    with override_settings(CookieStorage.max_cookie_size=512):\n        response = self.get_response()\n        unstored_messages = storage.update(response)\n\n    cookie_storing = self.stored_messages_count(storage, response)\n    self.assertEqual(cookie_storing, 3)\n\n    self.assertEqual(len(unstored_messages), 2)\n    self.assertEqual(unstored_messages[0].message, storage._loaded_data[0].message)\n    self.assertEqual(unstored_messages[1].message, storage._loaded_data[1].message)\n    # The sentinel value was added.\n    self.assertEqual(stored_cookie_messages_count(storage, response), 3)\n", "    def test_repeated_encoding(self):\n        \"\"\"\n        Test that repeated calls to ``_encode`` and ``_decode`` on the same\n        message data work correctly.\n        \"\"\"\n        storage = self.get_storage()\n        message = Message(constants.INFO, 'Test message')\n        encoded = storage._encode(message)\n        self.assertEqual(self._decode(encoded), message)\n        encoded = storage._encode(message)\n        self.assertEqual(self._decode(encoded), message)\n"], "sample_90": ["def test_istrue_multiple_expressions():\n    testdir = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.foo\n        @pytest.mark.bar\n            pass\n    \"\"\"\n    )\n    reprec = testdir.inline_run(\"-m\", \"foo and bar\")\n    passed, skipped, failed = reprec.listoutcomes()\n    assert passed == 1\n    assert skipped == failed == 0\n", "def test_mark_eval_invalid_syntax(testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.xyz\n            pass\n    \"\"\"\n    )\n    reprec = testdir.inline_run(\"-m\", \"xyz or\")\n    err = reprec.getfailure()\n    assert err.outcome == \"failed\"\n    assert err.location == \"pytest.mark.istrue\"\n    assert \"expected not OR left parenthesis OR identifier; got or\" in err.longrepr\n", "def test_invalid_parametrize_mark_name(testdir):\n    p = testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.parametrize(\"arg\", \"tuple\")\n            pass\n    \"\"\"\n    )\n\n    result = testdir.runpytest(p)\n    result.stdout.fnmatch_lines(\n        [\n            \"test_invalid_parametrize_mark_name.py::test_func: in 'parametrize' the first value must be an iterable,\",\n            \"  got a string: 'tuple'\",\n        ]\n    )\n    assert result.ret == ExitCode.USAGE_ERROR\n", "def test_mark_invalid_syntax(self, testdir):\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        @pytest.mark.invalidsyntax(\"hello\"\n            \"world\"\n        )\n            pass\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.stderr.fnmatch_lines([\"Invalid syntax in '-m': 'hello\\\"\\\"world\\\"\\\"'\"])\n    assert result.ret == ExitCode.USAGE_ERROR\n", "def test_invalid_marker_name(testdir):\n    testdir.makepyfile(\n        \"\"\"\n            import pytest\n            @pytest.mark.hello_world\n                pass\n    \"\"\"\n    )\n    reprec = testdir.inline_run()\n    reprec.assertoutcome(passed=0)\n    reprec.assert_outcomes(UNCOLLECTED=1)\n    dlist = reprec.getcalls(\"pytest_unmatchedmarker\")\n    assert len(dlist) == 1\n    assert len(dlist[0].items) == 1\n    assert dlist[0].items[0].nodeid.split(\"::\")[-1] == \"test_func\"\n"], "sample_125": ["def test_issue_2120():\n    assert -oo / oo == nan\n    assert oo / oo == nan\n    assert oo / -oo == nan\n    assert -oo / -oo == nan\n", "def test_number_symbols():\n    assert pi.is_irrational\n    assert pi.is_positive\n    assert pi.is_real\n    assert pi.is_number\n    assert pi.is_AlgebraicNumber is False\n    assert pi.is_Float is False\n    assert pi.is_Rational is False\n    assert pi.is_integer is False\n    assert pi.is_zero is False\n    assert pi.is_infinite is False\n    assert pi.is_transcendental\n    assert pi.is_complex is False\n\n    assert E.is_irrational\n    assert E.is_positive\n    assert E.is_real\n    assert E.is_number\n    assert E.is_AlgebraicNumber is False\n    assert E.is_Float is False\n    assert E.is_Rational is False\n    assert E.is_integer is False\n    assert E.is_zero is False\n    assert E.is_infinite is False\n    assert E.is_transcendental\n    assert E.is_complex is False\n\n    assert GoldenRatio.is_irrational\n    assert GoldenRatio.is_positive\n    assert GoldenRatio.is_real\n    assert GoldenRatio.is_number\n    assert GoldenRatio.is_AlgebraicNumber\n    assert GoldenRatio.is_Float is False\n    assert GoldenRatio.is_Rational is False\n    assert GoldenRatio.is_integer is False\n    assert GoldenRatio.is_zero is False\n    assert GoldenRatio.is_infinite is False\n    assert GoldenRatio.is_transcendental is False\n    assert GoldenRatio.is_complex is False\n\n    assert EulerGamma.is_real\n    assert EulerGamma.is_positive\n    assert EulerGamma.is_number\n    assert EulerGamma.is_AlgebraicNumber is False\n    assert EulerGamma.is_Float is False\n    assert EulerGamma.is_Rational is False\n    assert EulerGamma.is_integer is False\n    assert EulerGamma.is_zero is False\n    assert EulerGamma.is_infinite is False\n    assert EulerGamma.is_transcendental\n    assert EulerGamma.is_complex is False\n\n    assert Catalan.is_real\n    assert Catalan.is_positive\n    assert Catalan.is_number\n    assert Catalan.is_AlgebraicNumber is False\n    assert Catalan.is_Float is False\n    assert Catalan.is_Rational is False\n    assert Catalan.is_integer is False\n    assert Catalan.is_zero is False\n    assert Catalan.is_infinite is False\n    assert Catalan.is_transcendental\n    assert Catalan.is_complex is False\n\n    assert I.is_imaginary\n    assert I.is_number\n    assert I.is", "def test_issue_13956():\n    from sympy import sin, cos, tan, tanh\n    from sympy.abc import x\n    assert sin(x).series(x, 0, 10).removeO() == \\\n        x - x**3/6 + x**5/120 - x**7/5040 + x**9/362880\n    assert cos(x).series(x, 0, 10).removeO() == \\\n        1 - x**2/2 + x**4/24 - x**6/720 + x**8/40320\n    assert tan(x).series(x, 0, 10).removeO() == \\\n        x + x**3/3 + x**5/5 + x**7/63 + x**9/135 + x**11/3465\n    assert tanh(x).series(x, 0, 10).removeO() == \\\n        x + x**3/3 + x**5/15 + x**7/105 + x**9/945 + x**11/10395\n", "def test_issue_18971():\n    # test that Integer from strings works correctly\n    assert Integer('12345678901234567890') == Integer(12345678901234567890)\n    assert Integer('12345678901234567890') is Integer(Integer('12345678901234567890'))\n    assert Integer('0') == Integer(0)\n    assert Integer('0') is Integer(Integer('0'))\n    # test that Integer from strings with negative sign works correctly\n    assert Integer('-12345678901234567890') == Integer(-12345678901234567890)\n    assert Integer('-12345678901234567890') is Integer(Integer('-12345678901234567890'))\n    # test that Integer from strings with a single digit works correctly\n    assert Integer('1') == Integer(1)\n    assert Integer('1') is Integer(Integer('1'))\n    assert Integer('-1') == Integer(-1)\n    assert Integer('-1') is Integer(Integer('-1'))\n    # test that Integer from strings with leading zeros works correctly\n    assert Integer('012345678901234567890') == Integer(12345678901234567890)\n    assert Integer('012345678901234567890') is Integer(Integer('012345678901234567890'))\n    # test that Integer from strings with a decimal point works correctly\n    assert Integer('123456789012345678.901234567890') == Integer(123456789012345678)\n    assert Integer('123456789012345678.901234567890') is Integer(Integer('123456789012345678.901234567890'))\n    assert Integer('-123456789012345678.901234567890') == Integer(-123456789012345678)\n    assert Integer('-123456789012345678.901234567890') is Integer(Integer('-123456789012345678.901234567890'))\n", "def test_mod_inverse_examples():\n    from sympy import Mod, oo, S, Rational, Float, oo, zoo, cos\n    assert mod_inverse(3, 11) == 4\n    assert mod_inverse(5, 11) == 9\n    assert mod_inverse(21124921, 521512) == 7713\n    assert mod_inverse(124215421, 5125) == 2981\n    assert mod_inverse(214, 12515) == 1579\n    assert mod_inverse(5823991, 3299) == 1442\n    assert mod_inverse(123, 44) == 39\n    assert mod_inverse(2, 5) == 3\n    assert mod_inverse(-2, 5) == -3\n    assert mod_inverse(2, 11) == 6\n    assert mod_inverse(6, 11) == 2\n    assert mod_inverse(10, 11) == 10\n    assert mod_inverse(0, 11) == oo\n\n    assert mod_inverse(1, 11) == 1\n    assert mod_inverse(-1, 11) == -1\n    assert mod_inverse(1, 3) == 1\n    assert mod_inverse(-1, 3) == -1\n    assert mod_inverse(1, 7) == 1\n    assert mod_inverse(-1, 7) == -1\n    assert mod_inverse(1, 13) == 1\n    assert mod_inverse(-1, 13) == -1\n\n    assert mod_inverse(1, oo) == 1\n    assert mod_inverse(-1, oo) == -1\n    assert mod_inverse(1, 0) == oo\n    assert mod_inverse(-1, 0) == oo\n\n    # in sympy, mod_inverse uses igcd and igcdex, so if the denominator and\n    # numerator are coprime, it will work as expected:\n    assert mod_inverse(7, 10) == 3\n    assert mod_inverse(11, 10) == 1\n    # but if they are not, then it will raise an exception\n    raises(ValueError, lambda: mod_inverse(7, 22))\n    raises(ValueError, lambda: mod_inverse(11, 22))\n\n    # test with floats\n    assert mod_inverse(2., 11)"], "sample_129": ["def test_latex_Equivalent():\n    assert latex(Equivalent(x, y)) == r\"x \\Leftrightarrow y\"\n    assert latex(Not(Equivalent(x, y))) == r\"x \\not\\Leftrightarrow y\"\n", "def test_latex_TensorProduct():\n    from sympy.tensor.tensor import TensorProduct, TensorHead, TensorIndex\n    from sympy.tensor.indexed import TensorIndex\n    i, j = TensorIndex('i'), TensorIndex('j')\n    A = TensorHead('A', (3,), [i])\n    B = TensorHead('B', (3,), [j])\n    t = TensorProduct(A, B)\n    assert latex(t) == r\"A_{i} \\otimes B_{j}\"\n\n    t = TensorProduct(A, B, evaluate=False)\n    assert latex(t) == r\"A_{i} \\otimes B_{j}\"\n\n    t = TensorProduct(A, B, [i, j], evaluate=False)\n    assert latex(t) == r\"A_{i} \\otimes B_{j}\"\n\n    t = TensorProduct(A, B, [i, j], evaluate=False)\n    assert latex(t) == r\"A_{i} \\otimes B_{j}\"\n", "def test_latex_finite_field():\n    from sympy.polys.domains import QQ\n    from sympy import FiniteField\n\n    assert latex(FiniteField(QQ, 11)) == r\"\\mathbb{F}_{11}\"\n    assert latex(FiniteField(QQ, 5, name='myfield')) == r\"\\mathbb{F}_{5}\"\n    assert latex(FiniteField(QQ, 5, name='myfield', modulus='my_m')) == r\"\\mathbb{F}_{\\mathrm{my\\_m}}\"\n\n", "def test_issue_16314():\n    from sympy import Add, Eq, Lambda, Matrix, MatrixSymbol, MatrixBase\n    from sympy.abc import x, y\n    from sympy.printing.latex import latex, greek_letters_set, tex_greek_dictionary\n    from sympy.printing.latex import LatexPrinter\n    from sympy.physics.vector import CoordSys3D\n\n    A = CoordSys3D('A')\n    f = Lambda((x, y), x**2 + y**2)\n    M = Matrix([[f(x, y), f(x, y + 1)], [f(x + 1, y), f(x + 1, y + 1)]])\n    assert latex(MatrixBase(M)) == r\"\\\\begin{pmatrix}x^{2} + y^{2} & x^{2} + y^{2} \\\\x^{2} + y^{2} & x^{2} + y^{2}\\end{pmatrix}\"\n", "def test_latex_Printer():\n    from sympy import Printer\n    from sympy.printing.latex import LatexPrinter\n    from sympy import symbols\n\n    x, y = symbols('x y')\n\n    # test default printer\n    p = LatexPrinter()\n    assert p.doprint(2*x) == r\"2 x\"\n    assert p.doprint(x**2) == r\"x^{2}\"\n    assert p.doprint(x**3) == r\"x^{3}\"\n\n    # test with custom printer\n    class MyLatexPrinter(Printer):\n            return \"custom_\" + self._print(expr)\n\n    assert MyLatexPrinter().doprint(2*x) == \"custom_2 x\"\n    assert MyLatexPrinter().doprint(x**2) == \"custom_x^{2}\"\n    assert MyLatexPrinter().doprint(x**3) == \"custom_x^{3}\"\n\n    # test with custom printer and latex printer\n    p = LatexPrinter()\n    assert p.doprint(MyLatexPrinter().doprint(2*x)) == \"custom_2 x\"\n    assert p.doprint(MyLatexPrinter().doprint(x**2)) == \"custom_x^{2}\"\n    assert p.doprint(MyLatexPrinter().doprint(x**3)) == \"custom_x^{3}\"\n"], "sample_70": ["def test_frameon_rcparam():\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    with mpl.rc_context(rc={'legend.frameon': False}):\n        leg = ax.legend()\n    assert not leg.get_frame_on()\n\n    with mpl.rc_context(rc={'legend.frameon': True}):\n        leg = ax.legend()\n    assert leg.get_frame_on()\n", "def test_bbox_transform(bbox_transform):\n    # Smoke test for Legend's bbox_transform kwarg.\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line')\n    leg = ax.legend(bbox_transform=bbox_transform)\n    assert leg.get_bbox_to_anchor().transform == bbox_transform\n", "def test_title_fontproperties():\n    # test that title_fontproperties can be set\n    fig, ax = plt.subplots()\n    ax.plot(range(10))\n    leg = ax.legend(title='Aardvark', title_fontproperties=FontProperties(\n                                        family='serif', size=22, style='italic'))\n    assert leg.get_title().get_fontproperties().get_family() == 'serif'\n    assert leg.get_title().get_fontproperties().get_size_in_points() == 22\n    assert leg.get_title().get_fontproperties().get_style() == 'italic'\n", "def test_legend_ncol_kwarg():\n    fig, ax = plt.subplots()\n    ax.plot([1, 2], [3, 4], label='line1')\n    ax.plot([5, 6], [7, 8], label='line2')\n    with pytest.warns(UserWarning) as record:\n        ax.legend(ncol=2)\n    assert len(record) == 0\n    ax.legend(ncols=2)\n    ax.legend(ncol=1)\n    with pytest.warns(UserWarning) as record:\n        ax.legend(ncol=1, ncols=2)\n    assert len(record) == 1\n    assert str(record[0].message) == (\n        \"You have mixed positional and keyword arguments, some input may be discarded.\")\n", "def test_legend_custom_font():\n    # Test custom font for legend title\n    fig, ax = plt.subplots()\n    ax.plot(range(10), label='test')\n    with rc_context({'font.family': 'serif',\n                    'font.serif': 'Computer Modern',\n                    'text.usetex': True}):\n        leg = ax.legend(title='Aardvark',\n                        title_fontproperties=FontProperties(family='sans-serif',\n                                                           size=22))\n        assert leg.get_title().get_fontname() == 'sans-serif'\n        assert leg.get_title().get_fontsize() == 22\n    with rc_context({'font.family': 'serif',\n                    'font.serif': 'Computer Modern',\n                    'text.usetex': True}):\n        leg = ax.legend(title='Aardvark',\n                        title_fontproperties={'family': 'serif', 'size': 22})\n        assert leg.get_title().get_fontname() == 'serif'\n        assert leg.get_title().get_fontsize() == 22\n    with rc_context({'font.family': 'serif',\n                    'font.serif': 'Computer Modern',\n                    'text.usetex': True}):\n        leg = ax.legend(title='Aardvark', title_fontsize=22)\n        assert leg.get_title().get_fontsize() == 22\n        assert leg.get_title().get_fontname() == mpl.rcParams['font.family']\n"], "sample_3": ["def test_separable_mapping():\n    map1 = Mapping((0, 1, 0, 1), name='map1')\n    assert np.all(is_separable(map1))\n    assert np.all(separability_matrix(map1))\n", "def test_separability_matrix_custom_model():\n    @custom_model\n        return x + y\n\n    assert np.all(separability_matrix(model_d()) == [[True, True]])\n\n    @custom_model\n        return x + y + z\n\n    assert np.all(separability_matrix(model_e()) == [[True, True, True]])\n\n    @custom_model\n        return x * y\n\n    assert np.all(separability_matrix(model_f()) == [[False, False]])\n", "def test_separable_mapping_separable():\n    map1 = Mapping((0, 1), name='map1')\n    map2 = Mapping((0, 1), name='map2')\n    map3 = Mapping((1, 0), name='map3')\n    assert is_separable(map1)\n    assert is_separable(map2)\n    assert is_separable(map3)\n\n    map4 = Mapping((0, 0), name='map4')\n    assert not is_separable(map4)\n\n    assert_allclose(separability_matrix(map1), np.array([[True, False], [False, True]]))\n    assert_allclose(separability_matrix(map2), np.array([[True, False], [False, True]]))\n    assert_allclose(separability_matrix(map3), np.array([[True, False], [False, True]]))\n    assert_allclose(separability_matrix(map4), np.ones((2, 2), dtype=np.bool_))\n", "def test_arith_oper_ndarray():\n    # Models as inputs\n    result = _arith_oper(rot, scl1)\n    assert_allclose(result, np.array([[1, 0], [0, 1]])\n\n    result = _arith_oper(rot, p2)\n    assert_allclose(result, np.array([[2, 2], [2, 2]])\n\n    # ndarray\n    result = _arith_oper(np.array([[1, 2], [3, 4]]), np.array([[5, 6], [7, 8]]))\n    assert_allclose(result, np.array([[6, 8], [15, 20]]))\n\n    result = _arith_oper(np.array([[1, 2], [3, 4]]), np.array([[0, 0], [0, 0]]))\n    assert_allclose(result, np.array([[0, 0], [0, 0]])\n\n    result = _arith_oper(np.array([[1, 2], [3, 4]]), np.array([[5, 0], [0, 5]]))\n    assert_allclose(result, np.array([[6, 2], [12, 5]])\n\n    # Error\n    with pytest.raises(ModelDefinitionError, match=r\"Unsupported operands for arithmetic operator: .*\"):\n        _arith_oper(sh1, map1)\n", "def test_separability_of_mapping_models():\n    # Test Mapping model with separable input\n    map1_separable = Mapping((1, 0), name='map1')\n    assert is_separable(map1_separable)\n    assert_allclose(separability_matrix(map1_separable), np.array([[True, False]]))\n\n    # Test Mapping model with non-separable input\n    map2_non_separable = Mapping((0, 1), name='map2')\n    assert not is_separable(map2_non_separable)\n    assert_allclose(separability_matrix(map2_non_separable), np.array([[False, True]]))\n\n    # Test Mapping model with no input\n    map3_no_input = Mapping((0,), name='map3')\n    assert not is_separable(map3_no_input)\n    assert_allclose(separability_matrix(map3_no_input), np.array([False]))\n"], "sample_157": ["def test_tensor_product_powers():\n    A, B, C = symbols('A,B,C', commutative=False)\n    assert tensor_product_simp(TP(A, B)**3) == TP(A**3, B**3)\n    assert tensor_product_simp(2*TP(A, B)**2) == 2*TP(A**2, B**2)\n    assert tensor_product_simp(TP(A, B)**2 + TP(C, D)) == TP(A**2, B**2) + TP(C, D)\n    assert tensor_product_simp(2*TP(A, B) + TP(A, B)**2) == 2*TP(A, B) + TP(A**2, B**2)\n", "def test_tensor_product_combine():\n    assert tensor_product_simp(TensorProduct(A, B) + TensorProduct(B, C)) == \\\n        TensorProduct(A + B, C)\n\n", "def test_tensor_product_distributivity():\n    A, B, C, D = symbols('A,B,C,D', commutative=False)\n    assert (TP(TP(A, B), TP(C, D)).expand(tensorproduct=True) == \n            TP(TP(A, C), TP(B, D)))\n    assert (TP(TP(A, B), C).expand(tensorproduct=True) == \n            TP(A, TP(B, C)))\n    assert (C * TP(A, B).expand(tensorproduct=True) == \n            TP(C, A) * TP(C, B))\n", "def test_tensor_product_simp_with_Add_inside_TensorProduct():\n    A, B, C, D = symbols('A,B,C,D', commutative=False)\n    assert tensor_product_simp(TP(TP(A, B) + C, D)) == \\\n        TP(A, B) + TP(C, D)\n    assert tensor_product_simp(TP(A + C, B) + D) == \\\n        TP(A, B) + TP(C, B) + D\n    assert tensor_product_simp(TP(A, B + C) + D) == \\\n        TP(A, B) + TP(A, C) + D\n", "def test_issue_9407():\n    # Test that tensor_product_simp handles Muls with no TensorProducts correctly\n    assert tensor_product_simp(Mul(A, B)) == Mul(A, B)\n    # Test that tensor_product_simp handles powers of Muls correctly\n    assert tensor_product_simp(Mul(A, B)**2) == Mul(A, B)**2\n    # Test that tensor_product_simp handles nested Muls correctly\n    assert tensor_product_simp(Mul(Mul(A, B), Mul(C, D))) == Mul(A, B, C, D)\n"], "sample_139": ["def test_polarify_conjugate():\n    from sympy import conjugate, polar_lift, exp_polar\n    x = Symbol('x')\n    z = Symbol('z', polar=True)\n    f = Function('f')\n    ES = {}\n\n    assert polarify(conjugate(-1)) == (conjugate(polar_lift(-1)), ES)\n    assert polarify(conjugate(1 + I)) == (conjugate(polar_lift(1 + I)), ES)\n\n    assert polarify(conjugate(exp(x)), subs=False) == conjugate(exp(x))\n    assert polarify(conjugate(1 + x), subs=False) == conjugate(1 + x)\n    assert polarify(conjugate(f(I) + x), subs=False) == conjugate(f(polar_lift(I)) + x)\n\n    assert polarify(conjugate(x), lift=True) == conjugate(polar_lift(x))\n    assert polarify(conjugate(z), lift=True) == conjugate(z)\n    assert polarify(conjugate(f(x)), lift=True) == conjugate(f(polar_lift(x)))\n    assert polarify(conjugate(1 + x), lift=True) == conjugate(polar_lift(1 + x))\n    assert polarify(conjugate(1 + f(x)), lift=True) == conjugate(polar_lift(1 + f(polar_lift(x))))\n\n    newex, subs = polarify(conjugate(f(x) + z))\n    assert newex.subs(subs) == conjugate(f(x) + z)\n\n    mu = Symbol(\"mu\")\n    sigma = Symbol(\"sigma\", positive=True)\n\n    # Make sure polarify(lift=True) doesn't try to lift the integration\n    # variable\n    assert polarify(\n        Integral(sqrt(2)*x*exp(-(-mu + x)**2/(2*sigma**2))/(2*sqrt(pi)*sigma),\n        (x, -oo, oo)), lift=True) == Integral(sqrt(2)*(sigma*exp_polar(0))**exp_polar(I*pi)*\n        exp((sigma*exp_polar(0))**(2*exp_polar(I*pi))*exp_polar(I*pi)*polar_lift(-mu + x)**\n        (2*exp_polar(0))/2)*exp_polar(0)*polar_lift(x)/(2*sqrt(pi)), (x, -oo, oo))\n\n\n    assert polarify(conjugate", "def test_periodic_argument_zero():\n    from sympy import periodic_argument, exp_polar, I, oo, pi\n\n    assert periodic_argument(exp_polar(0)) is oo\n    assert periodic_argument(exp_polar(0), oo) is oo\n    assert periodic_argument(1, 2*pi) == 0\n    assert periodic_argument(-1, 2*pi) == pi\n    assert periodic_argument(1, pi) == 0\n    assert periodic_argument(-1, pi) == pi\n    assert periodic_argument(1 + I, 2*pi) == pi/4\n    assert periodic_argument(1 - I, 2*pi) == -pi/4\n    assert periodic_argument(1 + I, pi) == pi/4\n    assert periodic_argument(1 - I, pi) == -pi/4\n    assert periodic_argument(I, 2*pi) == pi/2\n    assert periodic_argument(-I, 2*pi) == -pi/2\n    assert periodic_argument(I, pi) == pi/2\n    assert periodic_argument(-I, pi) == -pi/2\n    assert periodic_argument(0, 2*pi) == 0\n    assert periodic_argument(exp_polar(pi), 2*pi) is oo\n", "def test_polarify_issue_17922():\n    from sympy import polarify, symbols, oo, exp_polar, sqrt\n    x, y = symbols('x y')\n    assert polarify(x + y) == (x + y, {})\n    assert polarify(x - y) == (x - y, {})\n    assert polarify(x*y) == (x*y, {})\n    assert polarify(x/y) == (x/y, {})\n    assert polarify(x/y) == (x/y, {})\n    assert polarify(x**y) == (x**y, {})\n    assert polarify(exp_polar(3*I)*x) == (polar_lift(exp_polar(3*I))*x, {})\n    assert polarify(sqrt(x)) == (sqrt(x), {})\n    assert polarify(exp_polar(3*I) + y) == (polar_lift(exp_polar(3*I)) + y, {})\n    assert polarify(exp_polar(3*I) - y) == (polar_lift(exp_polar(3*I)) - y, {})\n    assert polarify(exp_polar(3*I)*y) == (polar_lift(exp_polar(3*I))*y, {})\n    assert polarify(exp_polar(3*I)/y) == (polar_lift(exp_polar(3*I))/y, {})\n    assert polarify(exp_polar(3*I)**y) == (polar_lift(exp_polar(3*I))**y, {})\n", "def test_issue_12115():\n    from sympy import unpolarify, polar_lift, pi, exp, sqrt\n    from sympy.abc import x, y, z\n    from sympy import Eq, Ne, Interval\n\n    # test with complex arguments\n    assert unpolarify(polar_lift(2 + 3*I)) == 2 + 3*I\n    assert unpolarify(exp(7 + 3*I)) == exp(7 + 3*I)\n    assert unpolarify(sqrt(polar_lift(2 + 3*I))) == sqrt(2 + 3*I)\n    assert unpolarify(sqrt(exp(7 + 3*I))) == sqrt(exp(7 + 3*I))\n    assert unpolarify(sqrt(polar_lift(2 + 3*I)**(7 + 3*I))) == sqrt((2 + 3*I)**(7 + 3*I))\n\n    # test with single and multi variable expressions\n    assert unpolarify(2 + 3*x) == 2 + 3*x\n    assert unpolarify((2 + 3*I)*(4 + 5*x)) == (2 + 3*I)*(4 + 5*x)\n    assert unpolarify((2 + 3*x)*y) == (2 + 3*x)*y\n    assert unpolarify((2 + 3*I)*x*y) == (2 + 3*I)*x*y\n    assert unpolarify((2 + 3*x)*(4 + 5*y)) == (2 + 3*x)*(4 + 5*y)\n    assert unpolarify((2 + 3*I)*(4 + 5*x*y)) == (2 + 3*I)*(4 + 5*x*y)\n    assert unpolarify((2 + 3*x)*(4 + 5*y)*z) == (2 + 3*x)*(4 + 5*y)*z\n    assert unpolarify((2 + 3*I)*(4 + 5*x*y)*z) == (2 + 3*I)*(4 + 5*x*y)*z\n\n    # test with algebraic and transcendental functions\n    assert unpolarify(polar_lift(2 + 3*I).sin()) == (2 + 3*I).sin()\n    assert unpolarify(exp(7 + 3*I).sin()) == exp(7 + 3*I).sin()\n    assert unpolar", "def test_issue_17962():\n    from sympy import polar_lift, oo\n    x = Symbol('x')\n    p = Symbol('p', positive=True)\n    assert polar_lift(p**2) == 2*polar_lift(p)\n    assert polar_lift(4*p) == 4*polar_lift(p)\n    assert polar_lift(1 + 2*p) == polar_lift(1 + 2*p)\n    assert polar_lift(polar_lift(x)) == polar_lift(x)\n    assert polar_lift(polar_lift(2*x)) == polar_lift(2*x)\n    assert polar_lift(polar_lift(2)*polar_lift(x)) == polar_lift(2)*polar_lift(x)\n    assert polar_lift(polar_lift(p)) == p\n    assert polar_lift(x*2*p) == x*2*polar_lift(p)\n    assert polar_lift(p*p) == p*polar_lift(p)\n    assert polar_lift(x*p**2) == x*polar_lift(p)**2\n    assert polar_lift(p*x**2) == p*x**2\n    assert polar_lift(2*x**p) == 2*x**polar_lift(p)\n    assert polar_lift(p*x**p) == p*x**polar_lift(p)\n\n    # test that the integer coefficients are lifted\n    assert polar_lift(p**2 + p + 1) == (polar_lift(p))**2 + polar_lift(p) + 1\n    assert polar_lift(p**2 + p**(-1) + p**(-2)) == \\\n        (polar_lift(p))**2 + polar_lift(p)**(-1) + polar_lift(p)**(-2)\n\n    assert polar_lift(x + 2) == polar_lift(x) + 2\n    assert polar_lift(x + 2*p) == polar_lift(x) + 2*polar_lift(p)\n    assert polar_lift(x + 2*polar_lift(p)) == x + 2*polar_lift(p)\n\n    assert polar_lift(1 + x + p) == 1 + polar_lift(x) + polar_lift(p)\n    assert polar_lift(1 + x + 2*p) == 1 + polar_lift(x) + 2*polar_lift(p)\n    assert polar_lift(1 + x + polar_lift(2*p)) == 1 + x + 2*polar_lift(p)\n\n    assert polar_lift(x*p + 2*p) == polar_lift(x)*"], "sample_95": ["    def test_class_level_skipif(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            pytestmark = pytest.mark.skipif(True, reason=\"True123\")\n            class TestClass(object):\n                    pass\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        result.stdout.fnmatch_lines(\n            \"\"\"\n            *1 passed*1 skipped*\n        \"\"\"\n        )\n", "    def test_importorskip_false(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n                pytest.importorskip(\"builtins\")\n        \"\"\"\n        )\n        result = pytester.runpytest(\"-rxs\")\n        result.stdout.fnmatch_lines([\"*1 passed*\"])\n", "    def test_importorskip_imports(self, pytester: Pytester) -> None:\n        pytester.makepyfile(\n            \"\"\"\n            import pytest\n            pytest.importorskip(\"os\")\n        \"\"\"\n        )\n        result = pytester.runpytest()\n        assert result.ret == 0\n", "    def test_parametrize_no_args(self, pytester: Pytester) -> None:\n        item = pytester.getitem(\"def test_func(a, b): pass\")\n        pytest.raises(\n            ValueError,\n            item._parametrize,\n            argnames=\"a, b\",\n            argvalues=[],\n        )\n", "    def test_default_markers(self, pytester: Pytester) -> None:\n        result = pytester.runpytest(\"--markers\", \"--strict-markers\")\n        result.stdout.fnmatch_lines(\n            [\n                \"*skip(condition, ..., [*], reason=...)*skip*\",\n                \"*xfail(condition, ..., [*], reason=..., run=True, raises=None, strict=xfail_strict)*expected failure*\",\n                \"*xfail(condition, ..., [*], reason=..., run=False, raises=None, strict=xfail_strict)*xfail*\",\n            ]\n        )\n"], "sample_44": ["    def setUpTestData(cls):\n        cls.c1 = Category.objects.create(name='Entertainment', slug='entertainment', url='entertainment')\n        cls.c2 = Category.objects.create(name='A test', slug='test', url='test')\n        cls.c3 = Category.objects.create(name='Third', slug='third-test', url='third')\n        cls.c4 = Category.objects.create(name='Fourth', slug='4th', url='4th')\n", "def test_choice_label_from_instance(self):\n    \"\"\"\n    The ModelChoiceField label_from_instance() method can be overridden to\n    customize the label for the choice presented by the object.\n    \"\"\"\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(f.label_from_instance(self.c1), 'Entertainment')\n    f.label_from_instance = lambda obj: obj.name + ' label'\n    self.assertEqual(f.label_from_instance(self.c1), 'Entertainment label')\n    self.assertEqual(f.label_from_instance(self.c2), 'A test label')\n    self.assertEqual(f.label_from_instance(self.c3), 'Third label')\n", "def test_choice_iterator_with_to_field_name(self):\n    \"\"\"\n    Test that choice_iterator passes the to_field_name to the object's\n    serializable_value method.\n    \"\"\"\n    field = forms.ModelChoiceField(Category.objects.all(), to_field_name='url')\n    self.assertIsInstance(field.choices, ModelChoiceIterator)\n\n    class TestModelChoiceIterator(ModelChoiceIterator):\n            value, label = super().choice(obj)\n            return value, self.label_from_instance(obj)\n\n            return str(obj.url)\n\n    class TestModelChoiceField(forms.ModelChoiceField):\n        iterator = TestModelChoiceIterator\n\n    field = TestModelChoiceField(Category.objects.all(), to_field_name='url')\n    self.assertIsInstance(field.choices, TestModelChoiceIterator)\n    self.assertEqual(field.choices.label_from_instance(self.c1), 'entertainment')\n    self.assertEqual(field.choices.label_from_instance(self.c2), 'test')\n    self.assertEqual(field.choices.label_from_instance(self.c3), 'third-test')\n\n    field.queryset = Category.objects.filter(slug__contains='test')\n    self.assertEqual(list(field.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'entertainment'),\n        (self.c2.pk, 'test'),\n    ])\n", "def test_choices_iterability_after_delete(self):\n    f = forms.ModelChoiceField(Category.objects.all())\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n        (self.c3.pk, 'Third'),\n    ])\n    self.c3.delete()\n    self.assertEqual(list(f.choices), [\n        ('', '---------'),\n        (self.c1.pk, 'Entertainment'),\n        (self.c2.pk, 'A test'),\n    ])\n    self.assertEqual(f.clean(self.c3.pk), None)\n", "def test_choices_frozen(self):\n    \"\"\"ModelChoiceField.choices can't be modified after instantiation.\"\"\"\n    f = forms.ModelChoiceField(Category.objects.all())\n    original_choices = list(f.choices)\n    f.queryset = Category.objects.all()  # Try to change the queryset.\n    with self.assertRaises(AttributeError):\n        list(f.choices)\n    self.assertEqual(list(f.choices), original_choices)\n"], "sample_76": ["def test_invalid_order(self, df):\n\n    groupby = GroupBy([\"group\"])\n    with pytest.raises(ValueError):\n        PolyFit(order=-1)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n", "def test_order(self, df):\n\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=5)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n\n    grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n    assert_array_almost_equal(\n        res[\"y\"], np.polyval(np.polyfit(df[\"x\"], df[\"y\"], 5), grid)\n    )\n", "def test_order_change(self, df):\n\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=3, gridsize=100)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n    assert_array_equal(res.columns, [\"x\", \"y\"])\n\n    grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n    assert_array_almost_equal(\n        res[\"y\"], np.polyval(np.polyfit(df[\"x\"], df[\"y\"], 3), grid)\n    )\n", "    def test_order(self, df):\n\n        groupby = GroupBy([\"group\"])\n        res = PolyFit(order=3)(df[[\"x\", \"y\"]], groupby, \"x\", {})\n\n        assert_array_equal(res.columns, [\"x\", \"y\"])\n\n        grid = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 100)\n        assert_array_almost_equal(\n            res[\"y\"].diff().diff().dropna(), np.zeros(grid.size - 2)\n        )\n\n        # Check that the fitted polynomial is of the correct order\n        for _, part in res.groupby(\"group\"):\n            assert np.polyfit(part[\"x\"], part[\"y\"], 3).shape == (4,)\n", "def test_gridsize_greater_than_x_nunique(self, df):\n\n    groupby = GroupBy([\"group\"])\n    res = PolyFit(order=5, gridsize=100)(df, groupby, \"x\", {})\n\n    assert res.columns.to_list() == [\"x\", \"y\", \"group\"]\n\n    ngroups = df[\"group\"].nunique()\n    assert_array_equal(res.index, np.arange(ngroups * 100))\n\n    for _, part in res.groupby(\"group\"):\n        grid = np.linspace(part[\"x\"].min(), part[\"x\"].max(), 100)\n        assert_array_equal(part[\"x\"], grid)\n        assert part[\"y\"].diff().diff().dropna().abs().gt(0).all()\n"], "sample_24": ["def test_str(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError('message', code='my_code1')\n    error3 = ValidationError('message', code='my_code2')\n    error4 = ValidationError(\n        'error %(parm1)s %(parm2)s',\n        code='my_code1',\n        params={'parm1': 'val1', 'parm2': 'val2'},\n    )\n    error5 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error6 = ValidationError({'field1': 'message'})\n    error7 = ValidationError([\n        ValidationError({'field1': 'field error', 'field2': 'other'}),\n        'message',\n    ])\n\n    self.assertEqual(str(error1), 'ValidationError(message)')\n    self.assertEqual(str(error2), 'ValidationError(message)')\n    self.assertEqual(str(error3), 'ValidationError(message)')\n    self.assertEqual(str(error4), \"ValidationError(error val1 val2)\")\n    self.assertEqual(str(error5), \"ValidationError({'field1': 'message', 'field2': 'other'})\")\n    self.assertEqual(str(error6), \"ValidationError({'field1': 'message'})\")\n    self.assertEqual(str(error7), \"ValidationError(['message', ValidationError({'field1': 'field error', 'field2': 'other'})])\")\n\n    # Test error_dict representation\n    error_dict = {\n        'field1': ValidationError(\n            'error %(parm1)s %(parm2)s',\n            code='my_code',\n            params={'parm2': 'val2', 'parm1': 'val1'},\n        ),\n        'field2': 'other',\n    }\n    error = ValidationError(error_dict)\n    self.assertEqual(\n        str(error),\n        \"ValidationError({'field1': ValidationError('error val1 val2'), 'field2': 'other'})\"\n    )\n\n    # Test error_list representation\n    error_list = [\n        ValidationError({'field1': 'field error', 'field2': 'other'}),\n        'message',\n    ]\n    error = ValidationError(error_list)\n    self.assertEqual(\n        str(error),\n        \"ValidationError(['message', ValidationError({'field1': 'field error', 'field2': 'other'})])\"\n    )\n", "def test_str(self):\n    error1 = ValidationError('message')\n    error2 = ValidationError('message', code='my_code1')\n    error3 = ValidationError('message', code='my_code2')\n    error4 = ValidationError(\n        'error %(parm1)s %(parm2)s',\n        code='my_code1',\n        params={'parm1': 'val1', 'parm2': 'val2'},\n    )\n    error5 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error6 = ValidationError({'field1': 'message'})\n    error7 = ValidationError([\n        ValidationError({'field1': 'field error', 'field2': 'other'}),\n        'message',\n    ])\n\n    self.assertEqual(str(error1), 'message')\n    self.assertEqual(str(error2), 'message')\n    self.assertEqual(str(error3), 'message')\n    self.assertEqual(str(error4), \"error val1 val2\")\n    self.assertEqual(str(error5), \"{'field1': 'message', 'field2': 'other'}\")\n    self.assertEqual(str(error6), \"{'field1': 'message'}\")\n    self.assertEqual(str(error7), \"['message', {'field1': 'field error', 'field2': 'other'}]\")\n\n    error8 = ValidationError({\n        'field1': 'error %(parm1)s %(parm2)s',\n        'field2': 'other',\n    }, code='my_code')\n    error9 = ValidationError({\n        'field1': 'error %(parm1)s %(parm2)s',\n        'field2': 'other',\n    }, code='my_code')\n    error10 = ValidationError({\n        'field1': 'error %(parm1)s val2',\n        'field2': 'other',\n    }, code='my_code')\n\n    self.assertEqual(str(error8), \"{'field1': 'error %(parm1)s %(parm2)s', 'field2': 'other'}\")\n    self.assertEqual(str(error9), \"{'field1': 'error %(parm1)s %(parm2)s', 'field2': 'other'}\")\n    self.assertEqual(str(error10), \"{'field1': 'error %(parm1)s val2', 'field2': 'other'}\")\n", "    def test_update_error_dict(self):\n        error = ValidationError('message')\n        error_dict = {}\n        error.update_error_dict(error_dict)\n        self.assertIn('message', error_dict[NON_FIELD_ERRORS])\n        self.assertEqual(len(error_dict[NON_FIELD_ERRORS]), 1)\n\n        error_dict = {'field1': 'message1'}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'message1', NON_FIELD_ERRORS: ['message']})\n\n        error_dict = {'field1': 'message1', NON_FIELD_ERRORS: ['message2']}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'message1', NON_FIELD_ERRORS: ['message', 'message2']})\n\n        error_dict = {'field1': 'message1', 'field2': 'message2'}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'message1', 'field2': 'message2'})\n\n        error_dict = {'field1': 'message1', NON_FIELD_ERRORS: ['message2']}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'message1', NON_FIELD_ERRORS: ['message', 'message2']})\n\n        error_dict = {'field1': 'message1'}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': 'message1'})\n\n        error_dict = {}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {NON_FIELD_ERRORS: ['message']})\n\n        error_dict = {'field1': ValidationError('message1')}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ValidationError('message1'), NON_FIELD_ERRORS: ['message']})\n\n        error_dict = {'field1': ValidationError('message1'), NON_FIELD_ERRORS: ['message2']}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ValidationError('message1'), NON_FIELD_ERRORS: ['message', 'message2']})\n\n        error_dict = {'field1': ValidationError('message1'), 'field2': 'message2'}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict, {'field1': ValidationError('message1'), 'field2': 'message2'})\n\n        error_dict = {'field1': ValidationError('message1'), NON_FIELD_ERRORS: ['message2']}\n        error.update_error_dict(error_dict)\n        self.assertEqual(error_dict", "    def test_string_formatting(self):\n        error = ValidationError('error %(parm1)s %(parm2)s', params={'parm1': 'val1', 'parm2': 'val2'})\n        self.assertEqual(str(error), 'error val1 val2')\n        self.assertEqual(repr(error), \"ValidationError('error %(parm1)s %(parm2)s', code=None, params={'parm1': 'val1', 'parm2': 'val2'})\")\n", "def test_hash_dict_values(self):\n    error1 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error2 = ValidationError({'field1': 'message', 'field2': 'other'})\n    error3 = ValidationError({'field1': 'message2', 'field2': 'other'})\n\n    self.assertEqual(hash(error1), hash(error2))\n    self.assertNotEqual(hash(error1), hash(error3))\n    # Hashing of dictionary values is case sensitive.\n    self.assertNotEqual(hash(error1), hash(ValidationError({'field1': 'message', 'field2': 'Other'})))\n    # Hashing of dictionary values is order insensitive.\n    self.assertEqual(hash(error1), hash(ValidationError({'field2': 'other', 'field1': 'message'})))\n\n    error4 = ValidationError(\n        {'field1': 'error %(parm1)s %(parm2)s', 'field2': 'other'},\n        code='my_code1',\n        params={'parm1': 'val1', 'parm2': 'val2'},\n    )\n    error5 = ValidationError(\n        {'field1': 'error %(parm1)s %(parm2)s', 'field2': 'other'},\n        code='my_code1',\n        params={'parm2': 'val2', 'parm1': 'val1'},\n    )\n\n    self.assertEqual(hash(error4), hash(ValidationError(dict(error4.error_dict))))\n    self.assertEqual(hash(error4), hash(ValidationError({\n        'field1': ValidationError(\n            'error %(parm1)s %(parm2)s',\n            code='my_code1',\n            params={'parm1': 'val1', 'parm2': 'val2'},\n        ),\n        'field2': 'other',\n    })))\n    self.assertNotEqual(hash(error4), hash(ValidationError({\n        **error4.error_dict, 'field2': 'message'\n    })))\n    self.assertNotEqual(hash(error4), hash(ValidationError({\n        'field1': ValidationError(\n            'error %(parm1)s val2',\n            code='my_code1',\n            params={'parm1': 'val1'},\n        ),\n        'field2': 'other',\n    })))\n    self.assertEqual(hash(error4), hash(error5))\n"], "sample_36": ["def test_resolve_expression(self):\n    from django.db.models import F\n    q = Q(price__gt=F('discounted_price'))\n    query = object()\n    self.assertEqual(q.resolve_expression(query), query._add_q(q))\n", "def test_deconstruct_multiple_nested(self):\n    q = Q(Q(Q(price__gt=F('discounted_price'))))\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (Q(Q(price__gt=F('discounted_price')),),))\n    self.assertEqual(kwargs, {})\n", "def test_deconstruct_transform(self):\n    from django.db.models import Transform\n    class MyTransform(Transform):\n        lookup_name = 'my_transform'\n\n    field = 'price'\n    transform = MyTransform(field)\n    lookup = transform()\n    q = Q(field__my_transform=lookup)\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (field,))\n    self.assertEqual(kwargs, {'_transform': 'my_transform'})\n\n    q = ~Q(field__my_transform=lookup)\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, (field,))\n    self.assertEqual(kwargs, {'_transform': 'my_transform', '_negated': True})\n\n    q = Q(field__my_transform=lookup) | Q(field=F('price'))\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, ((field,), (field, F('price'))))\n    self.assertEqual(kwargs, {'_connector': 'OR', '_transform': 'my_transform'})\n\n    q = Q(field=F('price')) & Q(field__my_transform=lookup)\n    path, args, kwargs = q.deconstruct()\n    self.assertEqual(args, ((field, F('price')), (field,)))\n    self.assertEqual(kwargs, {'_connector': 'AND', '_transform': 'my_transform'})\n", "    def test_resolve_expression(self):\n        # Test resolve_expression with a simple Q object\n        q = Q(x__gt=10)\n        query = self.setup_query()\n        clause, joins = query._add_q(q, reuse=None, allow_joins=True, split_subq=False, check_filterable=False)\n        self.assertEqual(clause.sql, 'x > %s')\n        self.assertEqual(clause.params, [10])\n\n        # Test resolve_expression with a Q object that has a filter\n        q = ~Q(x__lt=10)\n        query = self.setup_query()\n        clause, joins = query._add_q(q, reuse=None, allow_joins=True, split_subq=False, check_filterable=False)\n        self.assertEqual(clause.sql, '(NOT x < %s)')\n        self.assertEqual(clause.params, [10])\n\n        # Test resolve_expression with a Q object that has a join\n        q = Q(x__gt=10) & Q(y__lt=5)\n        query = self.setup_query()\n        clause, joins = query._add_q(q, reuse=None, allow_joins=True, split_subq=False, check_filterable=False)\n        self.assertEqual(clause.sql, '(x > %s AND y < %s)')\n        self.assertEqual(clause.params, [10, 5])\n", "def test_combine_and_with_q_object(self):\n    q1 = Q(x=1)\n    q2 = Q(y=2)\n    with self.assertRaisesMessage(TypeError, str(q2)):\n        q1 & q2\n\n    q3 = ~Q(x=1)\n    with self.assertRaisesMessage(TypeError, str(q3)):\n        q1 | q3\n"], "sample_67": ["    def test_serialize_pathlib_posix_path(self):\n        # Pure path objects work in all platforms.\n        self.assertSerializedEqual(pathlib.PosixPath())\n        self.assertSerializedEqual(pathlib.PosixPath(\"/path/file.txt\"))\n        path = pathlib.PosixPath(\"/path/file.txt\")\n        expected = (\"pathlib.PosixPath('/path/file.txt')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n\n        field = models.FilePathField(path=pathlib.PosixPath(\"/home/user\"))\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"models.FilePathField(path=pathlib.PosixPath('/home/user'))\",\n        )\n        self.assertIn(\"import pathlib\", imports)\n", "    def test_serialize_regex_flags(self):\n        regex = re.compile(r\"^\\w+$\", re.I)\n        string, imports = MigrationWriter.serialize(regex)\n        self.assertEqual(string, \"re.compile('^\\\\\\\\w+$', flags=re.RegexFlag['IGNORECASE'])\")\n        self.assertEqual(imports, {\"import re\"})\n        self.serialize_round_trip(regex)\n\n        regex = re.compile(r\"^\\w+$\", re.I | re.M)\n        string, imports = MigrationWriter.serialize(regex)\n        self.assertEqual(string, \"re.compile('^\\\\\\\\w+$', flags=re.RegexFlag['IGNORECASE'] | re.RegexFlag['MULTILINE'])\")\n        self.assertEqual(imports, {\"import re\"})\n        self.serialize_round_trip(regex)\n", "    def test_register_serializer_instance(self):\n        class ComplexSerializer(BaseSerializer):\n                return \"complex(%r)\" % self.value, {}\n\n        serializer_instance = ComplexSerializer(1, 2)\n        MigrationWriter.register_serializer(complex, serializer_instance)\n        self.assertSerializedEqual(complex(1, 2))\n        MigrationWriter.unregister_serializer(complex)\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize: (1+2j)\"):\n            self.assertSerializedEqual(complex(1, 2))\n", "    def test_serialize_pathlib_join(self):\n        \"\"\"Test serialization of pathlib.Path objects joined using / operator.\"\"\"\n        path = pathlib.Path(\"/home/user\") / pathlib.Path(\"/Documents\")\n        expected = (\"pathlib.PurePosixPath('/home/user/Documents')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n\n        path = pathlib.Path(\"/home/user\") / pathlib.Path(\"Documents\")\n        expected = (\"pathlib.PurePosixPath('/home/user/Documents')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n\n        path = pathlib.Path(\"/home/user\") / pathlib.Path()\n        expected = (\"pathlib.PurePosixPath('/home/user')\", {\"import pathlib\"})\n        self.assertSerializedResultEqual(path, expected)\n\n        field = models.FilePathField(path=pathlib.Path(\"/home/user\") / \"Documents\")\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"models.FilePathField(path=pathlib.PurePosixPath('/home/user/Documents'))\",\n        )\n        self.assertIn(\"import pathlib\", imports)\n\n        field = models.FilePathField(path=pathlib.Path(\"/home/user\") / \"Documents\")\n        string, imports = MigrationWriter.serialize(field)\n        self.assertEqual(\n            string,\n            \"models.FilePathField(path=pathlib.PurePosixPath('/home/user/Documents'))\",\n        )\n        self.assertIn(\"import pathlib\", imports)\n", "def test_serialize_timezone(self):\n    \"\"\"\n    Tests serializing a timezone object.\n    \"\"\"\n    self.assertSerializedEqual(datetime.timezone())\n    self.assertSerializedEqual(datetime.timezone.utc)\n    self.assertSerializedResultEqual(\n        get_fixed_timezone(180),\n        (\"datetime.timezone(timedelta(hours=3))\", {\"import datetime\"}),\n    )\n    timezone = get_default_timezone()\n    self.assertSerializedResultEqual(\n        timezone,\n        (\"datetime.timezone(timedelta(hours=0))\", {\"import datetime\"}),\n    )\n    self.assertSerializedResultEqual(\n        zoneinfo.ZoneInfo(\"Europe/Paris\"),\n        (\"datetime.timezone(timedelta(hours=1))\", {\"import datetime\"}),\n    )\n\n    field = models.DateTimeField(default=datetime.datetime.now, tz_info=timezone)\n    string = MigrationWriter.serialize(field)[0]\n    self.assertEqual(\n        string,\n        \"models.DateTimeField(default=datetime.datetime(2024, 7, 26, 15, 55, 38.461954), \"\n        \"tz_info=datetime.timezone(timedelta(hours=0)))\",\n    )\n"], "sample_5": ["    def test_fast_delete_queryset_with_related_objects(self):\n        u = User.objects.create()\n        a = Avatar.objects.create()\n        u.avatar = a\n        # 1 to fast-delete the user, 1 to fast-delete the related avatar\n        self.assertNumQueries(2, u.delete)\n        self.assertFalse(User.objects.exists())\n        self.assertFalse(Avatar.objects.exists())\n", "def test_fast_delete_joined_qs_related(self):\n    \"\"\"\n    Test that deletion of a joined query with a related model results in a single\n    query if the related model can be fast-deleted.\n    \"\"\"\n    a = Avatar.objects.create(desc='a')\n    u1 = User.objects.create(avatar=a)\n    u2 = User.objects.create()\n    expected_queries = 1 if connection.features.update_can_self_select else 2\n    self.assertNumQueries(expected_queries,\n                          User.objects.filter(avatar__desc='a').delete)\n    self.assertEqual(User.objects.count(), 1)\n    self.assertTrue(User.objects.filter(pk=u2.pk).exists())\n", "    def test_fast_delete_single_instance(self):\n        u = User.objects.create()\n        # 1 query to fast-delete the user\n        deleted, rows_count = u.delete()\n        self.assertEqual(deleted, 1)\n        self.assertEqual(rows_count, {'delete.User': 1})\n", "    def test_get_del_batches(self):\n        collector = Collector(using='default')\n        objs = [A() for _ in range(100)]\n        collector.add(objs)\n        batches = collector.get_del_batches(objs, A._meta.pk)\n        self.assertEqual(len(batches), 1)\n        self.assertEqual(len(batches[0]), 100)\n        collector = Collector(using='default')\n        objs = [A() for _ in range(5)]\n        collector.add(objs)\n        batches = collector.get_del_batches(objs, A._meta.pk)\n        self.assertEqual(len(batches), 1)\n        self.assertEqual(len(batches[0]), 5)\n", "    def test_inheritance_cascade_from_proxy_model(self):\n        child = RChild.objects.create()\n        child.delete()\n        self.assertFalse(R.objects.filter(pk=child.pk).exists())\n"], "sample_98": ["def test_check_X_y():\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    X, y = check_X_y(X, y)\n    assert_array_equal(X, X)\n    assert_array_equal(y, y)\n\n    X = [[1, 2], [3, 4]]\n    y = [1, 2]\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array(['a', 'b'])\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, 4]])\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    check_X_y(X, y, multi_output=True)\n    check_X_y(X, y, multi_output=True, allow_nd=True)\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y, multi_output=True)\n\n    X = [[1, 2], [3, 4]]\n    y = [1, 2]\n    check_X_y(X, y, multi_output=True)\n    check_X_y(X, y, multi_output=True, allow_nd=True)\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y, multi_output=True)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([1, 2])\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y, multi_output=True, y_numeric=False)\n    assert_warns_regex(ValueError, 'Input contains non-numeric values',\n                       check_X_y, X, y, multi_output=True, y_numeric=True)\n\n    X = np.array([[1, 2], [3, 4]])\n    y = np.array([[1, 2], [3, 4", "def test_check_X_y_on_mock_dataframe():\n    # Smoke test for mock dataframe on check_X_y\n    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    mock_df = MockDataFrame(arr)\n    X, y = check_X_y(mock_df, [1, 2])\n    assert_equal(X.dtype, arr.dtype)\n    assert_equal(y.dtype, [1, 2].dtype)\n\n    # Test that a non-numeric y array raises an error\n    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    mock_df = MockDataFrame(arr)\n    assert_raises_regex(ValueError, \"y has dtype object\",\n                        check_X_y, mock_df, ['a', 'b'])\n\n    # Test that a non-numeric y array raises an error when y_numeric=True\n    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    mock_df = MockDataFrame(arr)\n    assert_raises_regex(ValueError, \"y has dtype object\",\n                        check_X_y, mock_df, ['a', 'b'], y_numeric=True)\n\n    # Test that a non-numeric y array is converted to numeric when y_numeric=True\n    arr = np.array([[0.2, 0.7], [0.6, 0.5], [0.4, 0.1], [0.7, 0.2]])\n    mock_df = MockDataFrame(arr)\n    X, y = check_X_y(mock_df, ['a', 'b'], y_numeric=True)\n    assert_equal(y.dtype, np.float64)\n", "def test_check_array_column_order():\n    # check that input validation preserves column order when\n    # copy=False.\n    # https://github.com/scikit-learn/scikit-learn/issues/10137\n    X = np.array([[1, 2], [3, 4]], order='F')\n    X_checked = check_array(X, copy=False)\n    assert_true(X_checked.flags['F_CONTIGUOUS'])\n    assert_false(X_checked.flags['C_CONTIGUOUS'])\n    assert_equal(X_checked, X)\n\n    X_sparse = sp.csr_matrix([[1, 2], [3, 4]])\n    X_checked = check_array(X_sparse, copy=False)\n    assert_true(X_checked.format == X_sparse.format)\n    assert_false(np.may_share_memory(X_checked.data, X_sparse.data))\n    assert_array_equal(X_checked.toarray(), X_sparse.toarray())\n", "def test_check_X_y():\n    # check that y must be 1-d\n    X = np.ones((10, 10))\n    y = np.ones((10, 10))\n    assert_raises(ValueError, check_X_y, X, y)\n\n    # check that multi_output=True allows 2d y\n    X = np.ones((10, 10))\n    y = np.ones((10, 10))\n    check_X_y(X, y, multi_output=True)\n\n    # check that y must be numeric when multi_output=False\n    X = np.ones((10, 10))\n    y = np.array(['a', 'b'])\n    assert_raises_regex(ValueError, \"can have np.inf or np.nan values\",\n                        check_X_y, X, y, multi_output=False)\n\n    # check that y can be numeric when multi_output=True\n    X = np.ones((10, 10))\n    y = np.array(['a', 'b'])\n    check_X_y(X, y, multi_output=True)\n\n    # check that y must not have inf or nan when multi_output=True\n    X = np.ones((10, 10))\n    y = np.array([np.inf, np.nan])\n    assert_raises_regex(ValueError, \"can have np.inf or np.nan values\",\n                        check_X_y, X, y, multi_output=True)\n\n    # check that y must be numeric when y_numeric=True\n    X = np.ones((10, 10))\n    y = np.array(['a', 'b'])\n    assert_raises_regex(ValueError, \"can have np.inf or np.nan values\",\n                        check_X_y, X, y, y_numeric=True)\n\n    # check that y can be numeric when y_numeric=False\n    X = np.ones((10, 10))\n    y = np.array(['a', 'b'])\n    check_X_y(X, y, y_numeric=False)\n\n    # check that y must not have inf or nan when y_numeric=True\n    X = np.ones((10, 10))\n    y = np.array([np.inf, np.nan])\n    assert_raises_regex(ValueError, \"can have np.inf or np.nan values\",\n                        check_X_y, X, y, y_numeric=True)\n\n    # check that warn_on_dtype works for check_X_y\n    X = np.ones((10, 10), dtype=np.float16)\n    y = np.ones((10, 10), dtype=np.float32)\n   ", "def test_check_array_memory_error():\n    X = np.memmap('temp_array', dtype=np.float32, mode='w+', shape=(10, 10))\n    X[:] = 1\n    assert_raises_regex(ValueError, \"Cannot convert a memory mapped object\",\n                         check_array, X)\n\n"], "sample_120": ["def test_Inverse():\n    A = MatrixSymbol('A', n, n)\n    I = Identity(n)\n    In = Inverse(A)\n    assert In.shape == (n, n)\n    assert In*A == A*I\n    assert A*In == I\n    assert (In)**-1 == In\n    assert In.conjugate() == In.inverse()\n", "def test_inverses():\n    A = MatrixSymbol('A', n, n)\n    B = MatrixSymbol('B', n, n)\n    C = MatrixSymbol('C', n, n)\n    D = MatrixSymbol('D', m, m)\n    E = MatrixSymbol('E', m, m)\n    X = MatrixSymbol('X', n, n)\n    Y = MatrixSymbol('Y', n, n)\n\n    assert (A**-1).is_Inverse\n    assert (A**-1).is_Inverse == A._eval_inverse()\n    assert (A*B)**-1 == Inverse(A)*Inverse(B)\n    assert (A**-1)**-1 == A\n    assert (A**-1)*A == Identity(n)\n    assert A*(A**-1) == Identity(n)\n    assert (A*B)**-1 == Inverse(B)*Inverse(A)\n    assert (B**-1)**-1 == B\n    assert (B**-1)*B == Identity(m)\n    assert B*(B**-1) == Identity(m)\n    assert (C**-1)*C == Identity(n)\n    assert C*(C**-1) == Identity(n)\n    assert (D**-1)*D == Identity(m)\n    assert D*(D**-1) == Identity(m)\n    assert (E**-1)*E == Identity(m)\n    assert E*(E**-1) == Identity(m)\n    assert (X**-1)*X == Identity(n)\n    assert X*(X**-1) == Identity(n)\n    assert (Y**-1)*Y == Identity(n)\n    assert Y*(Y**-1) == Identity(n)\n", "def test_KroneckerDelta():\n    A = MatrixSymbol('A', n, m)\n    assert KroneckerDelta(A, A) == n\n    raises(TypeError, lambda: KroneckerDelta(A, 1))\n", "def test_Transpose():\n    A = MatrixSymbol('A', n, m)\n    assert (A.T).shape == (m, n)\n    assert (A*B).T == B.T*A.T\n    assert (A.T).T == A\n    assert A.T*B == B.T*A\n    assert (A + B).T == A.T + B.T\n    assert (A - B).T == A.T - B.T\n", "def test_matrix_transpose():\n    A = MatrixSymbol('A', n, m)\n    assert transpose(A).is_Transpose\n    assert transpose(A).args[0] == A\n    assert (A.T).is_Transpose\n    assert (A.T).args[0] == A\n    A = MatrixSymbol('A', n, n)\n    assert (A.T).is_square\n    assert (A.T).T == A\n"], "sample_104": ["def test_count_vectorizer_grid_search():\n    # Test pprinting of a CountVectorizer object inside a GridSearchCV object\n    param_grid = [{'vocabulary': ['word1', 'word2', 'word3'], 'binary': [True, False]},\n                  {'vocabulary': ['word4', 'word5', 'word6'], 'binary': [True, False]}]\n    gs = GridSearchCV(CountVectorizer(), param_grid, cv=5)\n\n    expected = \"\"\"", "def test_n_max_elements_to_show_in_estimator():\n    # Test that the number of max elements to show in an estimator affects the\n    # number of parameters shown.\n    n_max_elements_to_show = 20\n    pp = _EstimatorPrettyPrinter(\n        compact=True, indent=1, indent_at_name=True,\n        n_max_elements_to_show=n_max_elements_to_show\n    )\n    svc = SVC(C=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], kernel='rbf')\n    expected = \"\"\"", "def test_repr_of_empty_objects():\n    # Check that empty estimators (e.g., with no parameters) are correctly\n    # represented\n    empty_lr = LogisticRegression()\n    expected = \"LogisticRegression()\"\n\n    expected = expected[1:]  # remove first \\n\n    assert empty_lr.__repr__() == expected\n\n    empty_imputer = SimpleImputer()\n    expected = \"SimpleImputer()\"\n\n    expected = expected[1:]  # remove first \\n\n    assert empty_imputer.__repr__() == expected\n\n    empty_svm = SVC()\n    expected = \"SVC()\"\n\n    expected = expected[1:]  # remove first \\n\n    assert empty_svm.__repr__() == expected\n\n    # Now check with a non-empty object that has a default parameter value\n    imputer = SimpleImputer(missing_values=0)\n    expected = \"SimpleImputer(missing_values=0)\"\n\n    expected = expected[1:]  # remove first \\n\n    assert imputer.__repr__() == expected\n", "def test_dict_parameters_repr():\n    # Test that parameters of dict type are not represented as KeyValTupleParam\n    # instances.\n    scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n    expected = \"\"\"StandardScaler(copy=True, with_mean=True, with_std=True)\"\"\"\n    assert scaler.__repr__() == expected\n\n    # Test that parameters of dict type are represented correctly\n    est = LogisticRegression(\n        penalty='l1',\n        class_weight={'auto': 0.5, 'balanced': 0.5}\n    )\n    expected = \"\"\"LogisticRegression(class_weight={'auto': 0.5, 'balanced': 0.5},\n                                penalty='l1')\"\"\"\n    assert est.__repr__() == expected\n\n    # Test that parameters of dict type with only default values are not shown\n    est = LogisticRegression(\n        penalty='l1',\n        class_weight={'auto': 0.5}\n    )\n    expected = \"\"\"LogisticRegression(penalty='l1')\"\"\"\n    assert est.__repr__() == expected\n\n    # Test that parameters of dict type with non-default values and default values\n    # are shown correctly\n    est = LogisticRegression(\n        penalty='l1',\n        class_weight={'auto': 0.5, 'balanced': 0.5, 'not_default': 0.0}\n    )\n    expected = \"\"\"LogisticRegression(class_weight={'auto': 0.5, 'balanced': 0.5,\n                                                'not_default': 0.0},\n                                penalty='l1')\"\"\"\n    assert est.__repr__() == expected\n", "def test_multiple_estimator_inheritance():\n    # Test if an estimator with multiple inheritance is properly rendered\n    # As of 0.20, we have: BaseEstimator -> TransformerMixin -> SimpleImputer\n\n    imputer = SimpleImputer(strategy=\"mean\")\n    expected = \"\"\""], "sample_87": ["    def test_parsearg_pyargs_relative_path(self, testdir):\n        p = testdir.makepyfile(\"def test_func(): pass\")\n        subdir = testdir.mkdir(\"sub\")\n        subdir.ensure(\"__init__.py\")\n        target = subdir.join(\"test_\" + p.basename)\n        p.move(target)\n        subdir.chdir()\n        config = testdir.parseconfig(p.basename)\n        rcol = Session(config=config)\n        assert rcol.fspath == subdir\n        parts = rcol._parsearg(p.basename)\n\n        assert parts[0] == target\n        assert len(parts) == 1\n        parts = rcol._parsearg(p.basename + \"::test_func\")\n        assert parts[0] == target\n        assert parts[1] == \"test_func\"\n        assert len(parts) == 2\n", "    def test_exit_status_update(self, testdir):\n        p = testdir.makepyfile(\"def test_func(): pass\")\n        id = \"::\".join([p.basename, \"test_func\"])\n\n        config = testdir.parseconfig(p.basename)\n        rcol = Session(config=config)\n        rcol.exitstatus = ExitCode.TESTS_FAILED\n        items, hookrec = testdir.inline_genitems(id)\n        assert rcol.exitstatus == ExitCode.TESTS_FAILED\n", "    def test_collect_protocol_multiple_functions(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n        \"\"\"\n        )\n        id = \"::\".join([p.basename, \"test_func1\", \"test_func2\"])\n\n        items, hookrec = testdir.inline_genitems(id)\n        assert len(items) == 2\n        item1, item2 = items\n        assert item1.name == \"test_func1\"\n        assert item1.nodeid == id\n        assert item2.name == \"test_func2\"\n        assert item2.nodeid == id\n\n        newid = item2.nodeid\n        items, hookrec = testdir.inline_genitems(newid)\n        assert len(items) == 1\n        item3, = items\n        assert item3.name == \"test_func2\"\n        assert item3.nodeid == newid\n\n        # ensure we are reporting the collection of the single test item (#2464)\n        assert [x.name for x in self.get_reported_items(hookrec)] == [\"test_func1\", \"test_func2\"]\n", "    def test_handle_duplicate_names_different_paths(self, testdir):\n        p1 = testdir.makepyfile(\"def test_func(): pass\")\n        p2 = testdir.makepyfile(\"def test_func(): pass\")\n        p2.move(p1.dirname.join(\"test_func.py\"))\n        config = testdir.parseconfig(p1.basename)\n        rcol = Session(config=config)\n        rcol.perform_collect([p1.basename], genitems=False)\n        colitems = rcol.perform_collect([p2.basename], genitems=False)\n        assert len(colitems) == 1\n        assert colitems[0].fspath == p2\n", "    def test_exit_after_maxfail(self, testdir):\n        testdir.makepyfile(\n            \"\"\"\n                pass\n                assert False\n                pass\n        \"\"\"\n        )\n        res = testdir.runpytest(\"--maxfail=1\")\n        assert res.ret == 1\n        res.stdout.fnmatch_lines([\"collected 3 items\", \"*1 failed*\"])\n"], "sample_78": ["def test_flaskgroup_load_plugins(runner):\n        app = Flask(__name__)\n        from flask import Blueprint\n\n        custom = Blueprint(\"custom\", __name__, cli_group=\"customized\")\n        custom.add_url_rule(\"/custom\", endpoint=\"custom\")\n\n        return app\n\n    cli = FlaskGroup(create_app=create_app)\n    result = runner.invoke(cli, [\"customized\", \"custom\"])\n    assert result.exit_code == 0\n    assert result.output == \"custom\\n\"\n", "def test_separated_path_type(runner):\n    @click.command()\n        click.echo(extra_files)\n\n    cli = FlaskGroup()\n    result = runner.invoke(cli, [\"testcmd\", \"--extra-files\", \"/tmp:/home/user\"])\n    assert result.exit_code == 0\n    assert result.output == \"/tmp:/home/user\\n\"\n\n    result = runner.invoke(cli, [\"testcmd\"])\n    assert result.exit_code == 2\n    assert \"Usage:\" in result.output\n", "def test_cli_group_callbacks(runner):\n    class TestGroup(FlaskGroup):\n            extra[\"obj\"] = \"custom_obj\"\n            return super().make_context(info_name, args, parent, **extra)\n\n    @click.group(cls=TestGroup)\n        pass\n\n    @cli.command()\n        assert click.get_current_context().obj == \"custom_obj\"\n\n    cli_runner = TestGroup()\n    result = runner.invoke(cli_runner, [\"test\"])\n    assert result.exit_code == 0\n    assert result.output == \"custom_obj\\n\"\n", "def test_cert_param_type():\n        click.testing.CliRunner().invoke([\"run\", \"--cert\", __file__])\n\n        click.testing.CliRunner().invoke([\"run\", \"--cert\", __file__, \"--key\", __file__])\n\n        click.testing.CliRunner().invoke([\"run\", \"--cert\", \"adhoc\"])\n\n        click.testing.CliRunner().invoke([\"run\", \"--cert\", \"flask\"])\n\n        class MockContext:\n                self.value = value\n\n                return self.value\n\n            return MockContext(\"ssl_context\")\n\n        click.testing.CliRunner().invoke([\"run\", \"--cert\", \"ssl_context\"], make_context)\n\n        class MockContext:\n                self.value = value\n\n                return self.value\n\n            return MockContext(\"not_here\")\n\n        with pytest.raises(click.BadParameter):\n            click.testing.CliRunner().invoke([\"run\", \"--cert\", \"ssl_context\"], make_context)\n\n        try:\n            import ssl\n        except ImportError:\n            pytest.skip(\"SSL not available\")\n        else:\n            class MockContext:\n                    self.value = value\n\n                    return self.value\n\n                return MockContext(\"not_here\")\n\n            with pytest.raises(click.BadParameter):\n                click.testing.CliRunner().invoke([\"run\", \"--cert\", \"not_here\"], make_context)\n", "def test_flaskgroup_params(runner, monkeypatch):\n    # check the params in the context\n    cli = click.Group(\"cli\")\n    flask_group = FlaskGroup(name=\"flask\")\n    cli.add_command(flask_group)\n\n    @flask_group.command()\n        click.echo(current_app.name)\n\n    obj = ScriptInfo()\n\n    result = runner.invoke(cli, [\"flask\", \"show\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"flask\\n\"\n\n    # check the extra params in the context\n    cli = FlaskGroup(name=\"flask\", create_app=lambda: Flask(\"flask\"))\n    cli.add_command(show)\n    result = runner.invoke(cli, [\"--env-file\", \"test.env\", \"flask\", \"show\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"flask\\n\"\n\n    # check the params in the context when the app cannot be loaded\n    class MyFlask(Flask):\n            raise Exception(\"oh no\")\n\n    cli = FlaskGroup(name=\"flask\", create_app=lambda: MyFlask(\"flask\"))\n    cli.add_command(show)\n    result = runner.invoke(cli, [\"flask\", \"show\"], obj=obj)\n    assert result.exit_code == 1\n    assert result.output == \"\"\n    assert \"oh no\" in result.stderr\n\n    # check the params in the context when the app is not the default\n    cli = FlaskGroup(name=\"flask\", create_app=lambda: Flask(\"app\"))\n    cli.add_command(show)\n    result = runner.invoke(cli, [\"--app\", \"testapp\", \"flask\", \"show\"], obj=obj)\n    assert result.exit_code == 0\n    assert result.output == \"testapp\\n\"\n"], "sample_92": ["    def test_xfail_unexpected_exception(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=RuntimeError)\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.assert_outcomes(xfailed=1)\n        result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n", "    def test_string_condition_with_syntax_error(self, testdir):\n        item = testdir.getitem(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(\"hasattr(os, 'invalid syntax')\")\n                pass\n        \"\"\"\n        )\n        with pytest.raises(pytest.fail.Exception) as excinfo:\n            evaluate_condition(item, pytest.mark.xfail(\"hasattr(os, 'invalid syntax')\"), \"\")\n        assert excinfo.value.msg is not None\n        assert \"invalid syntax\" in excinfo.value.msg\n", "    def test_dynamic_skip_during_setup(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n                pytest.mark.skip()(function)\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(p, \"-rs\")\n        result.stdout.fnmatch_lines([\"*1 skipped*\"])\n", "    def test_xfail_with_raises(self, testdir, raises):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n            @pytest.mark.xfail(raises=%s)\n                raise TypeError()\n        \"\"\"\n            % (raises, )\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\"*1 xfailed*\"])\n        result = testdir.runpytest(p, \"-rx\")\n        result.stdout.fnmatch_lines([\"*XFAIL*test_raises*\", \"*reason:*TypeError*\"])\n", "    def test_dynamic_xfail_strict(self, testdir):\n        p = testdir.makepyfile(\n            \"\"\"\n            import pytest\n\n            @pytest.fixture\n                request.add_marker(pytest.mark.xfail(reason='dynamic xfail', strict=True))\n                pass\n        \"\"\"\n        )\n        result = testdir.runpytest(p)\n        result.stdout.fnmatch_lines([\"*XPASS(dynamic xfail)*dynamic xfail*\"])\n"], "sample_107": ["def test_logistic_regression_no_intercept():\n    # Test that LogisticRegression with fit_intercept=False gives the same result as\n    # LogisticRegression with fit_intercept=True and intercept_ set to 0.\n    X, y = make_classification(random_state=0)\n    clf1 = LogisticRegression(fit_intercept=True, solver='lbfgs', max_iter=100,\n                             random_state=42, tol=1e-10)\n    clf2 = LogisticRegression(fit_intercept=False, solver='lbfgs', max_iter=100,\n                             random_state=42, tol=1e-10)\n    clf1.fit(X, y)\n    clf2.fit(X, y)\n    assert_array_almost_equal(clf1.coef_, clf2.coef_)\n    assert_array_almost_equal(clf1.intercept_, 0)\n    assert_equal(clf1.n_iter_, clf2.n_iter_)\n", "def test_logistic_regression_attrs():\n    X, y = make_classification(n_samples=10, n_features=5, random_state=0)\n\n    for solver in ['lbfgs', 'newton-cg', 'sag', 'saga']:\n        for penalty in ['l1', 'l2']:\n            if solver == 'liblinear' and penalty == 'l1':\n                continue\n            if solver == 'sag' and penalty == 'l2':\n                continue\n            for dual in [True, False]:\n                for fit_intercept in [True, False]:\n                    for multi_class in ['ovr', 'multinomial']:\n                        lr = LogisticRegression(solver=solver, penalty=penalty,\n                                                dual=dual, fit_intercept=fit_intercept,\n                                                multi_class=multi_class,\n                                                random_state=42)\n                        lr.fit(X, y)\n                        assert_equal(lr.coef_.shape, (1, X.shape[1]))\n                        if fit_intercept:\n                            assert_equal(lr.intercept_.shape, (1,))\n                            assert_equal(lr.n_iter_.shape, (1,))\n                        else:\n                            assert_equal(lr.intercept_, 0)\n                            assert_equal(lr.n_iter_.shape, (1,))\n", "def test_logistic_regression_l2_ridge():\n    # Test that LogisticRegression with l2 penalty and no regularization is\n    # equivalent to RidgeRegressor\n    X = iris.data\n    y = iris.target.copy()\n    y[y == 2] = 0\n\n    for solver in ['liblinear', 'sag', 'saga', 'newton-cg']:\n        penalty = 'l2' if solver in ['liblinear', 'sag', 'saga'] else 'none'\n        C = 1.\n        if solver == 'sag':\n            max_iter = 1000\n            tol = 1e-8\n        elif solver == 'newton-cg':\n            max_iter = 20\n            tol = 1e-8\n        else:\n            max_iter = 100\n            tol = 1e-10\n\n        lr = LogisticRegression(C=C, penalty=penalty, solver=solver,\n                               random_state=42, max_iter=max_iter,\n                               tol=tol, fit_intercept=False)\n        ridge = Ridge(alpha=C, solver=solver, random_state=42, max_iter=max_iter,\n                      tol=tol)\n\n        lr.fit(X, y)\n        ridge.fit(X, y)\n        assert_array_almost_equal(lr.coef_, ridge.coef_)\n", "def test_logistic_regression_fit_intercept_zero_intercept():\n    # Test that fit_intercept=False works with and without an intercept in the\n    # model.\n    X, y = make_classification(n_samples=20, n_features=5, n_informative=3,\n                               n_classes=2, random_state=0)\n    X_constant = np.ones(shape=(20, 2))\n    X = np.concatenate((X, X_constant), axis=1)\n    X_noise = np.random.normal(size=(20, 3))\n    X = np.concatenate((X, X_noise), axis=1)\n    X[X < 1] = 0\n\n    for solver in ['lbfgs', 'sag', 'saga']:\n        for fit_intercept in (True, False):\n            clf = LogisticRegression(fit_intercept=fit_intercept, solver=solver,\n                                    multi_class='ovr')\n            clf.fit(X, y)\n            if fit_intercept:\n                assert_array_equal(clf.coef_.shape, (1, X.shape[1]))\n                assert_array_equal(clf.intercept_.shape, (1,))\n            else:\n                assert_array_equal(clf.coef_.shape, (1, X.shape[1] - 1))\n                assert_array_equal(clf.intercept_, 0)\n\n            # Check that it can fit to the constant feature\n            constant_X = np.zeros((20, X.shape[1] - 2))\n            constant_X[:, -1] = 1\n            assert_array_equal(clf.predict(constant_X), y)\n", "def test_intercept_scaling_logistic_regression():\n    # Test the computation of the synthetic feature weight with intercept_scaling\n    # for the liblinear solver.\n\n    rng = np.random.RandomState(42)\n    X = np.vstack([rng.randn(100, 2), np.ones((100, 2))])\n    y = np.array([0] * 100 + [1] * 100)\n\n    for solver in ['liblinear', 'liblinear']:  # for liblinear solver only\n        for intercept_scaling in [1.0, 10.0, 100.0, 1000.0]:\n            for penalty in ['l1', 'l2']:\n                for dual in [False, True]:\n                    for tol in [1e-5, 1e-10]:\n                        for max_iter in [1, 100]:\n                            clf = LogisticRegression(solver=solver,\n                                                    penalty=penalty,\n                                                    C=1.0,\n                                                    fit_intercept=True,\n                                                    multi_class='ovr',\n                                                    tol=tol,\n                                                    max_iter=max_iter,\n                                                    intercept_scaling=intercept_scaling,\n                                                    dual=dual,\n                                                    random_state=42)\n                            clf.fit(X, y)\n                            intercept_scaling_w = clf.intercept_scaling * X[:, -1]\n                            synthetic_w = np.insert(clf.coef_, len(clf.coef_), 0)\n                            assert_array_almost_equal(intercept_scaling_w, synthetic_w)\n\n                            # Check that changing the intercept_scaling has the\n                            # expected effect on the weight of the synthetic\n                            # feature.\n                            new_intercept_scaling = 2 * intercept_scaling\n                            clf.intercept_scaling = new_intercept_scaling\n                            clf.fit(X, y)\n                            new_intercept_scaling_w = clf.intercept_scaling * X[:, -1]\n                            assert_array_almost_equal(new_intercept_scaling_w,\n                                                    synthetic_w)\n"], "sample_45": ["    def test_cache_page_decorator_http_request(self):\n        class MyClass:\n            @cache_page(60 * 15)\n                return HttpResponse()\n\n        msg = (\n            \"cache_page didn't receive an HttpRequest. If you are decorating \"\n            \"a classmethod, be sure to use @method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            MyClass().a_view(HttpRequest())\n", "    def test_vary_on_headers(self):\n        \"\"\"\n        @vary_on_headers properly sets the Vary header.\n        \"\"\"\n        @vary_on_headers('Accept-Encoding')\n            return HttpResponse()\n        r = a_view(HttpRequest())\n        self.assertEqual(r['Vary'], 'Accept-Encoding')\n", "def test_process_request_decorator(self):\n    \"\"\"\n    Ensures the @cache_page decorator properly sets cache headers on the response.\n    \"\"\"\n    @cache_page(60 * 15)\n        return HttpResponse()\n\n    r = a_view(HttpRequest())\n    self.assertEqual(\n        set(r.headers['Cache-Control'].split(', ')),\n        {'max-age=900', 'post-check=900', 'pre-check=0', 'public'},\n    )\n", "    def test_never_cache_decorator_http_method(self):\n        @never_cache\n            return HttpResponse()\n        msg = (\n            \"never_cache() only accepts a view function, not a method of a \"\n            \"view class. If you want to use it as a class decorator, use \"\n            \"@method_decorator.\"\n        )\n        with self.assertRaisesMessage(TypeError, msg):\n            class MyClass:\n                never_cache\n                    return HttpResponse()\n", "    def test_middleware_decorator(self):\n        class MyMiddleware:\n                self.view_func = view_func\n\n                return HttpResponse('pre_process')\n\n                return HttpResponse('pre_view')\n\n                return HttpResponse('pre_exception')\n\n                return HttpResponse('pre_response')\n\n                return HttpResponse('pre_template_response')\n\n        middleware_decorator = decorator_from_middleware(MyMiddleware)\n        view = middleware_decorator(fully_decorated)\n\n        request = HttpRequest()\n        response = view(request)\n        self.assertEqual(response.content, b'pre_process')\n        self.assertEqual(response.status_code, 200)\n\n        response = view(request)\n        self.assertEqual(response.content, b'pre_process')\n        self.assertEqual(response.status_code, 200)\n"], "sample_100": ["def test_one_hot_encoder_handle_unknown_strings():\n    X = np.array(['11111111', '22', '333', '4444']).reshape((-1, 1))\n    X2 = np.array(['55555', '22']).reshape((-1, 1))\n    # Non Regression test for the issue #12470\n    # Test the ignore option, when categories are numpy string dtype\n    # particularly when the known category strings are larger\n    # than the unknown category strings\n    oh = OneHotEncoder(handle_unknown='ignore')\n    oh.fit(X)\n    X2_passed = X2.copy()\n    assert_array_equal(\n        oh.transform(X2_passed).toarray(),\n        np.array([[0.,  0.,  0.,  0.], [0.,  1.,  0.,  0.]]))\n    # ensure transformed data was not modified in place\n    assert_array_equal(X2, X2_passed)\n", "def test_one_hot_encoder_invalid_n_values():\n    with pytest.raises(ValueError, match=\"handle_unknown should be either error or unknown got\"):\n        OneHotEncoder(handle_unknown='42').fit([[0, 1], [0, 1]])\n", "def test_one_hot_encoder_transform_dtype():\n    X = np.array([[1, 2], [3, 4]], dtype=np.int32).T\n    X2 = np.array([[0, 2], [1, 0]], dtype=np.float64).T\n\n    enc = OneHotEncoder(dtype=np.float32, sparse=False)\n    X_trans = enc.fit_transform(X)\n    assert X_trans.dtype == np.float32\n\n    # check that transform also returns the correct dtype\n    X_trans = enc.transform(X2)\n    assert X_trans.dtype == np.float32\n\n    # check that the inverse transform also returns the correct dtype\n    X_inv = enc.inverse_transform(X_trans)\n    assert X_inv.dtype == np.object_\n", "def test_one_hot_encoder_dtype_warning():\n    # Test that a warning is raised when the output dtype is not specified,\n    # but the input data type is not numpy.float64\n    X = np.array([[1, 2]], dtype=np.int32).T\n    enc = OneHotEncoder()\n    with ignore_warnings(category=FutureWarning):\n        enc.fit(X)\n        assert_warns(FutureWarning, enc.transform, X)\n    # Test that the warning is not raised when the output dtype is specified\n    # and is numpy.float64\n    X = np.array([[1, 2]], dtype=np.int32).T\n    enc = OneHotEncoder(dtype=np.float64)\n    assert_no_warnings(enc.transform, X)\n", "def test_ordinal_encoder_dtype():\n    X = np.array([[1, 2]], dtype='int64')\n    X2 = np.array([[1, 4]], dtype='int64')\n    X3 = np.array([[1, 'a']], dtype='object')\n    X4 = np.array([[1, 'a'], [3, 'b']])\n\n    enc = OrdinalEncoder(dtype=np.float32)\n    enc.fit(X)\n    assert np.issubdtype(enc.categories_[0].dtype, np.float32)\n    assert_array_equal(enc.transform(X), np.array([[0.], [1.]]))\n\n    enc = OrdinalEncoder(dtype=np.int32)\n    enc.fit(X2)\n    assert np.issubdtype(enc.categories_[0].dtype, np.integer)\n    assert_array_equal(enc.transform(X2), np.array([[0.], [1.]]))\n\n    enc = OrdinalEncoder(dtype=np.object)\n    enc.fit(X3)\n    assert enc.categories_[0].dtype == 'object'\n    assert_array_equal(enc.transform(X3), np.array([[0.], [1.]]))\n\n    enc = OrdinalEncoder(dtype=np.object)\n    enc.fit(X4)\n    assert all([enc.categories_[i].dtype == 'object' for i in range(2)])\n    assert_array_equal(enc.transform(X4), np.array([[0., 1.], [1., 0.]]))\n\n    # check default dtype\n    enc = OrdinalEncoder()\n    assert enc.dtype == np.float64\n    assert_array_equal(enc.fit_transform(X).astype('float64'), np.array([[0.], [1.]]))\n\n    # check that error is raised for wrong dtype\n    with pytest.raises(TypeError):\n        OrdinalEncoder(dtype='string')\n"], "sample_77": ["    def t(self):\n        dates = pd.to_datetime([\"1972-09-27\", \"1975-06-24\", \"1980-12-14\"])\n        return pd.Series(dates, name=\"x\")\n", "    def x(self):\n        return pd.Series([1, 1, 10, 100], name=\"x\", dtype=float)\n", "    def t(self):\n        dates = pd.to_datetime([\"1972-09-27\", \"1975-06-24\", \"1980-12-14\"])\n        return pd.Series(dates, name=\"x\")\n", "    def x(self):\n        return pd.Series([1e10, 1e-10, 1e5], name=\"x\", dtype=float)\n", "    def test_norm_transform(self, x):\n\n        s = Continuous(trans=\"pow3\")._setup(x, Coordinate())\n        assert_series_equal(s(x), np.power(x, 3))\n        s = Continuous(trans=\"pow3\")._setup(x, Coordinate())\n        s = s._setup(x, Coordinate(), norm=(1, 2))\n        expected = (np.power(1, 3) - np.power(2, 3)) / (np.power(2, 3) - np.power(1, 3))\n        assert_series_equal(s(x), expected * np.power(x, 3) + np.power(1, 3))\n"], "sample_68": ["def test_bulk_update(self):\n    TwoFields.objects.bulk_create([TwoFields(f1=i, f2=i) for i in range(0, 10)])\n    self.assertEqual(TwoFields.objects.count(), 10)\n    self.assertEqual(TwoFields.objects.filter(f1__gte=5, f1__lte=5).count(), 1)\n    self.assertEqual(TwoFields.objects.filter(f2__gte=10).count(), 1)\n    TwoFields.objects.bulk_update([TwoFields(f1=5, f2=5)], [\"f1\"])\n    self.assertEqual(TwoFields.objects.count(), 10)\n    self.assertEqual(TwoFields.objects.filter(f1__gte=5, f1__lte=5).count(), 1)\n    self.assertEqual(TwoFields.objects.filter(f2__gte=10).count(), 0)\n    self.assertEqual(TwoFields.objects.filter(f1__gte=0, f1__lte=5).count(), 5)\n", "def test_transaction_isolation(self):\n    with self.assertNumQueries(1):\n        Country.objects.bulk_create(self.data)\n    # Create a read-only transaction.\n    with connection.cursor() as cursor:\n        cursor.execute(\"BEGIN DEFERRED\")\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(self.data)\n    # Create a read-only transaction with a savepoint.\n    with connection.cursor() as cursor:\n        cursor.execute(\"BEGIN DEFERRED\")\n        cursor.execute(\"SAVEPOINT sp\")\n        with self.assertRaises(IntegrityError):\n            Country.objects.bulk_create(self.data)\n", "def test_batched_insert_single_item(self):\n    with self.assertNumQueries(1):\n        country = Country.objects.get(name=\"United States of America\")\n        Country.objects.bulk_create([country])\n    self.assertEqual(country.pk, Country.objects.get(name=\"United States of America\").pk)\n", "def test_update_conflicts_update_fields_invalid_pk(self):\n    self._test_update_conflicts_unique_two_fields([\"pk\"])\n", "def test_empty_query_set(self):\n    Country.objects.bulk_create([]).values_list(\"id\", \"name\", \"iso_two_letter\")\n    self.assertEqual(\n        Country.objects.filter(id__in=[1, 2, 3, 4]).values_list(\"id\", \"name\", \"iso_two_letter\"),\n        [\n            [1, \"United States of America\", \"US\"],\n            [2, \"The Netherlands\", \"NL\"],\n            [3, \"Germany\", \"DE\"],\n            [4, \"Czech Republic\", \"CZ\"],\n        ],\n    )\n"], "sample_14": ["def test_serialize_deconstructable_instances(self):\n    \"\"\"\n    Test that deconstructable instances are correctly serialized.\n    \"\"\"\n    instances = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(instances)\n    self.assertEqual(string, 'migrations.test_writer.DeconstructibleInstances()')\n    self.assertEqual(imports, {'import migrations.test_writer'})\n", "def test_serialize_datetime_timezone(self):\n    self.assertSerializedResultEqual(\n        datetime.datetime(2014, 1, 1, 1, 1, tzinfo=datetime.timezone(timedelta(hours=1))),\n        (\n            \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=datetime.timezone(datetime.timedelta(0, 36000)))\",\n            {'import datetime'},\n        ),\n    )\n\n    self.assertSerializedResultEqual(\n        datetime.datetime(2013, 12, 31, 22, 1, tzinfo=datetime.timezone(timedelta(days=1))),\n        (\n            \"datetime.datetime(2013, 12, 31, 22, 1, tzinfo=datetime.timezone(datetime.timedelta(days=1)))\",\n            {'import datetime'},\n        ),\n    )\n", "def test_serialize_datetime_object(self):\n    \"\"\"\n    Test that the serialize method handles a datetime object.\n    \"\"\"\n    dt = datetime.datetime(2014, 1, 1, 1, 1, tzinfo=get_default_timezone())\n    string, imports = MigrationWriter.serialize(dt)\n    self.assertEqual(string, \"datetime.datetime(2014, 1, 1, 1, 1, tzinfo=utc)\")\n    self.assertEqual(imports, {'import datetime', 'from django.utils.timezone import utc'})\n", "    def test_register_serializer_replace_existing(self):\n        \"\"\"Test that registering a serializer replaces an existing one.\"\"\"\n        class NewSerializer(BaseSerializer):\n                return 'new_serializer(%r)' % self.value, {}\n\n        MigrationWriter.register_serializer(int, NewSerializer)\n        self.assertSerializedEqual(1)\n        self.assertSerializedEqual(2)\n        self.assertSerializedEqual(3)\n        MigrationWriter.unregister_serializer(int)\n        with self.assertRaisesMessage(ValueError, 'Cannot serialize: 1'):\n            self.assertSerializedEqual(1)\n", "def test_serialize_deconstructible_instance(self):\n    \"\"\"\n    Test serialization of an instance of a class that has a deconstruct method,\n    but is not a Field or Manager instance.\n    \"\"\"\n    decon_instance = DeconstructibleInstances()\n    string, imports = MigrationWriter.serialize(decon_instance)\n    self.assertEqual(string, 'migrations.test_writer.DeconstructibleInstances()')\n    self.assertEqual(imports, {'import migrations.test_writer'})\n\n    class CustomSerializer(BaseSerializer):\n            return 'custom(%r)' % self.value, {}\n\n    MigrationWriter.register_serializer(DeconstructibleInstances, CustomSerializer)\n    string, imports = MigrationWriter.serialize(decon_instance)\n    self.assertEqual(string, 'custom(migrations.test_writer.DeconstructibleInstances())')\n    MigrationWriter.unregister_serializer(DeconstructibleInstances)\n\n    # Test that the custom serializer is not registered for a different class\n    string, imports = MigrationWriter.serialize(DeconstructibleInstances())\n    self.assertEqual(string, 'migrations.test_writer.DeconstructibleInstances()')\n    self.assertEqual(imports, {'import migrations.test_writer'})\n"], "sample_57": ["def test_ordered_forms_after_deletion(self):\n    \"\"\"\n    After a form is deleted, its ordering field is removed from self._ordering\n    and self.ordered_forms should reflect the new ordering.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, can_order=True, can_delete=True)\n    initial = [\n        {\"choice\": \"Calexico\", \"votes\": 100},\n        {\"choice\": \"Fergie\", \"votes\": 900},\n        {\"choice\": \"The Decemberists\", \"votes\": 500},\n    ]\n    formset = ChoiceFormSet(initial=initial, auto_id=False, prefix=\"choices\")\n    self.assertHTMLEqual(\n        \"\\n\".join(form.as_ul() for form in formset.forms),\n        '<li>Choice: <input type=\"text\" name=\"choices-0-choice\" value=\"Calexico\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-0-votes\" value=\"100\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-0-ORDER\" value=\"1\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-0-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-1-choice\" value=\"Fergie\">'\n        \"</li>\"\n        '<li>Votes: <input type=\"number\" name=\"choices-1-votes\" value=\"900\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-1-ORDER\" value=\"2\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-1-DELETE\"></li>'\n        '<li>Choice: <input type=\"text\" name=\"choices-2-choice\" '\n        'value=\"The Decemberists\"></li>'\n        '<li>Votes: <input type=\"number\" name=\"choices-2-votes\" value=\"500\"></li>'\n        '<li>Order: <input type=\"number\" name=\"choices-2-ORDER\" value=\"3\"></li>'\n        '<li>Delete: <input type=\"checkbox\" name=\"choices-2-DELETE\"></li>',\n    )\n    # Delete Fergie and put The Decemberists ahead of Calexico.\n    data = {\n        \"choices-TOTAL_FORMS\": \"4\",  # the number of forms rendered\n        \"choices-INITIAL", "def test_add_fields_with_custom_fields(self):\n    \"\"\"\n    Add_fields() should add custom fields in addition to the standard \"ORDER\" and \"DELETE\" fields.\n    \"\"\"\n    class CustomForm(Form):\n        custom_field = CharField()\n\n    class CustomFormSet(BaseFormSet):\n            return {\"custom_field\": \"default\"}\n\n    CustomFormSet = formset_factory(CustomForm, formset=CustomFormSet, extra=1)\n    formset = CustomFormSet(initial=[{\"name\": \"test\"}])\n    custom_form = formset.forms[0]\n    custom_form.fields[\"ORDER\"] = IntegerField(label=_(\"Order\"), required=False, widget=NumberInput())\n    custom_form.fields[\"DELETE\"] = BooleanField(label=_(\"Delete\"), required=False, widget=CheckboxInput())\n    custom_form.fields[\"custom_field\"] = CharField(label=_(\"Custom Field\"), required=False)\n    formset.add_fields(custom_form, 0)\n    self.assertIn(\"custom_field\", custom_form.fields)\n    self.assertEqual(custom_form.fields[\"custom_field\"].widget, CharField().widget)\n", "    def test_default_error_messages(self):\n        \"\"\"Default error messages should be used when not overridden.\"\"\"\n        data = {\n            \"form-TOTAL_FORMS\": \"1\",\n            \"form-INITIAL_FORMS\": \"0\",\n        }\n        formset = ArticleFormSet(data)\n        self.assertEqual(\n            formset.non_form_errors(),\n            [\n                \"ManagementForm data is missing or has been tampered with. \"\n                \"Missing fields: form-TOTAL_FORMS, form-INITIAL_FORMS. \"\n                \"You may need to file a bug report if the issue persists.\"\n            ],\n        )\n", "    def test_get_form_kwargs_empty_form(self):\n        \"\"\"\n        get_form_kwargs() returns an empty dictionary for an empty form.\n        \"\"\"\n        formset = ChoiceFormSet(extra=3)\n        self.assertEqual(formset.get_form_kwargs(None), {})\n", "    def test_formset_invalid(self):\n        \"\"\"\n        A formset's validation is triggered by calling its is_valid() method.\n        This method is True if all the forms in the formset are valid.\n        The forms are validated recursively by calling form.is_valid(), so\n        any validation errors will be stored in each form's errors attribute.\n        The formset's errors attribute is a list of lists of error messages,\n        each corresponding to one of the forms in the formset.\n        \"\"\"\n        # Create a formset with 3 forms, one of which has an invalid field\n        formset = self.make_choiceformset([(\"Calexico\", \"100\"), (\"\", \"\"), (\"\", \"100\")])\n        self.assertFalse(formset.is_valid())\n        # Check that the formset has 3 errors, one for each form\n        self.assertEqual(len(formset.errors), 3)\n        # Check that the errors are correctly attributed to each form\n        self.assertEqual(formset.errors, [[], {\"votes\": [\"This field is required.\"]}, []])\n"], "sample_151": ["def test_canberra_distance_on_zero_point():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    raises(ValueError, lambda: p1.canberra_distance(p2))\n\n", "def test_direction_ratio():\n    p1 = Point3D(0, 0, 0)\n    p2 = Point3D(1, 1, 1)\n    assert p1.direction_ratio(Point3D(1, 0, 0)) == [1, 0, 0]\n    assert p1.direction_ratio(Point3D(0, 1, 0)) == [0, 1, 0]\n    assert p1.direction_ratio(Point3D(0, 0, 1)) == [0, 0, 1]\n\n    assert p1.direction_ratio(Point3D(5, 0, 0)) == [5, 0, 0]\n    assert p1.direction_ratio(Point3D(0, sqrt(3), 0)) == [0, sqrt(3), 0]\n    assert p1.direction_ratio(Point3D(0, 0, 5)) == [0, 0, 5]\n\n    assert p1.direction_ratio(Point3D(2.4, 2.4, 0)) == [2.4, 2.4, 0]\n    assert p1.direction_ratio(Point3D(1, 1, 1)) == [1, 1, 1]\n    assert p1.direction_ratio(Point3D(-12, 0 -15)) == [-12, -15, 0]\n\n    assert p2.direction_ratio(Point3D(0, 0, 0)) == [-1, -1, -1]\n    assert p2.direction_ratio(Point3D(1, 1, 12)) == [0, 0, 12]\n    assert p2.direction_ratio(Point3D(12, 1, 12)) == [12, 0, 12]\n\n    with warns(UserWarning):\n        p3 = Point3D(0, 0, 0, 1)\n        assert p1.direction_ratio(p3) == [1, 1, 1, 0]\n\n    raises(TypeError, lambda: p1.direction_ratio(Point2D(1, 1)))\n\n", "def test_project():\n    \"\"\"Test the project function\"\"\"\n    p = Point(1, 1)\n    origin = Point(0, 0)\n    line = Point(1, 0)\n    assert Point.project(p, line) == Point(1, 0)\n    assert Point.project(p, origin) == p\n    assert Point.project(p, p) == p\n    raises(ValueError, lambda: Point.project(p, Point()))\n    raises(ValueError, lambda: Point.project(Point(), Point(1, 1)))\n", "def test_distance():\n    p1 = Point(1, 1)\n    p2 = Point(4, 5)\n    assert p1.distance(p2) == 5\n    p3 = Point(2, 3)\n    assert p1.distance(p3) == sqrt(8)\n    p4 = Point(2, 2)\n    assert p1.distance(p4) == 1\n    p5 = Point(1, 5)\n    assert p1.distance(p5) == 4\n    p6 = Point(4, 1)\n    assert p1.distance(p6) == 3\n    p7 = Point(4, 4)\n    assert p1.distance(p7) == 3\n    p8 = Point(1, 1, 1)\n    with warns(UserWarning):\n        assert p1.distance(p8) == sqrt(3)\n    p9 = Point(2, 2, 2)\n    with warns(UserWarning):\n        assert p1.distance(p9) == sqrt(6)\n    p10 = Point(3, 3, 3)\n    with warns(UserWarning):\n        assert p1.distance(p10) == sqrt(9)\n", "def test_issue_9606():\n    p = Point3D(1, 2, 3)\n    p3d = Point3D(3, 2, 1)\n    l = Line3D(p, p3d)\n    assert Point.are_coplanar(p, p3d, l.p1, l.p2) is True\n"], "sample_43": ["def test_autocomplete_view_permission_denied_for_unregistered_model(self):\n    request = self.factory.get(self.url, {'term': '', **self.opts})\n    request.user = self.user\n    class UnregisteredModelAdmin(admin.ModelAdmin):\n        pass\n    request.urlconf = 'admin_views.urls'\n    with model_admin(UnregisteredModel, UnregisteredModelAdmin):\n        with self.assertRaises(PermissionDenied):\n            AutocompleteJsonView.as_view(**self.as_view_args)(request)\n", "def test_get_queryset_limit_choices_to(self):\n    \"\"\"get_queryset() correctly applies limit_choices_to.\"\"\"\n    class QuestionAdmin(QuestionAdmin):\n        search_fields = ['question']\n\n    q1 = Question.objects.create(question='Question 1')\n    q2 = Question.objects.create(question='Question 2', question_with_to_field__question='Not a question.')\n    request = self.factory.get(self.url, {'term': 'is', **self.opts})\n    request.user = self.superuser\n    with model_admin(Question, QuestionAdmin):\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q1.pk), 'text': q1.question}],\n            'pagination': {'more': False},\n        })\n", "def test_custom_admin_search_fields(self):\n    \"\"\"\n    Test search results when the related model admin has custom search fields.\n    \"\"\"\n    class CustomQuestionAdmin(QuestionAdmin):\n        search_fields = ['related_questions__question', 'answer']\n\n    with model_admin(Question, CustomQuestionAdmin):\n        request = self.factory.get(self.url, {'term': 'question', **self.opts})\n        request.user = self.superuser\n        response = AutocompleteJsonView.as_view(**self.as_view_args)(request)\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.content.decode('utf-8'))\n        self.assertEqual(data, {\n            'results': [{'id': str(q.pk), 'text': q.question} for q in Question.objects.order_by('id')],\n            'pagination': {'more': False},\n        })\n", "    def setUpTestData(self):\n        super().setUpTestData()\n        Question.objects.create(question='Question 1', related_questions=[Question.objects.create(question='Related Question 1')])\n        Question.objects.create(question='Question 2', related_questions=[Question.objects.create(question='Related Question 2')])\n        Question.objects.create(question='Question 3', related_questions=[Question.objects.create(question='Related Question 3')])\n", "    def setUpTestData(cls):\n        cls.user = User.objects.create_user(\n            username='user', password='secret',\n            email='user@example.com', is_staff=True,\n        )\n        super().setUpTestData()\n"], "sample_38": ["    def test_render_with_unknown_hasher(self):\n        \"\"\"Rendering the widget with unknown hasher should raise an exception.\"\"\"\n        widget = ReadOnlyPasswordHashWidget()\n        # Simulate unknown hash format\n        value = 'unknown_hasher$unknown_data$unknown'\n        with self.assertRaises(ValueError):\n            widget.render('name', value, {'id': 'id_password'})\n", "    def test_render_invalid_hash(self):\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid hash'\n        with self.assertRaises(ValidationError):\n            widget.get_context('name', value, {'id': 'id_password'})\n", "    def test_render_with_unusable_password(self):\n        # Rendering the widget with unusable password\n        # should display the correct message.\n        widget = ReadOnlyPasswordHashWidget()\n        value = '!' + UNUSABLE_PASSWORD_PREFIX + '!!'\n        html = widget.render('name', value, {'id': 'id_password'})\n        self.assertIn(_(\"No password set.\"), html)\n", "    def test_validates_password(self):\n        user = User.objects.get(username='testclient')\n        data = {\n            'password1': 'testclient',\n            'password2': 'testclient',\n        }\n        form = AdminPasswordChangeForm(user, data)\n        self.assertFalse(form.is_valid())\n        self.assertEqual(len(form['password2'].errors), 1)\n        self.assertIn('This password is already in use.', form['password2'].errors)\n", "    def test_invalid_password_format(self):\n        # Rendering the widget with an invalid password format\n        # mustn't raise an exception.\n        widget = ReadOnlyPasswordHashWidget()\n        value = 'invalid'\n        self.assertEqual(widget.render('name', value, {'id': 'id_password'}), '')\n        self.assertIn(_(\"Invalid password format or unknown hashing algorithm.\"), widget.get_context('name', value, {'id': 'id_password'})['summary'][0]['label'])\n"], "sample_79": ["    def test_concat_merge_variables_present_in_all_datasets(self, data):\n        # coordinates present in all datasets\n        ds1 = Dataset(\n            data_vars={\"a\": (\"y\", [0.1])}, coords={\"x\": 0.1, \"y\": 0.2}\n        )\n        ds2 = Dataset(\n            data_vars={\"a\": (\"y\", [0.2])}, coords={\"x\": 0.3, \"y\": 0.4}\n        )\n        actual = concat([ds1, ds2], dim=\"y\", coords=\"minimal\")\n        expected = Dataset(\n            {\"a\": (\"y\", [0.1, 0.2])}, coords={\"x\": [0.1, 0.3], \"y\": [0.2, 0.4]}\n        )\n        assert_identical(expected, actual)\n\n        # data variables present in all datasets\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n        data0, data1 = deepcopy(split_data)\n        data0[\"foo\"] = (\"bar\", np.random.randn(10))\n        data1[\"foo\"] = (\"bar\", np.random.randn(10))\n        actual = concat([data0, data1], \"dim1\")\n        expected = data.copy().assign(foo=data0.foo)\n        assert_identical(expected, actual)\n", "    def test_concat_compat_override(self):\n        ds1 = Dataset(\n            {\"has_x_y\": ((\"y\", \"x\"), [[1, 2]]), \"has_x\": (\"x\", [1, 2])},\n            coords={\"x\": [0, 1], \"y\": [0]},\n        )\n        ds2 = Dataset(\n            {\"has_x_y\": ((\"y\", \"x\"), [[3, 4]]), \"has_x\": (\"x\", [1, 2])},\n            coords={\"x\": [0, 1], \"y\": [1]},\n        )\n\n        result = concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", compat=\"override\")\n        assert_equal(ds1.has_x, result.has_x)\n        assert_equal(ds2.has_x, result.has_x)\n\n        with raises_regex(ValueError, \"coordinates in some datasets but not others\"):\n            concat([ds1, ds2], dim=\"q\")\n", "    def test_concat_coords_var_dim_in_coord_names(self, coords):\n        data = create_test_data()\n        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n        coord = Variable(\"y\", (\"dim1\", [0, 1, 2]))\n        data.coords[\"y\"] = coord\n        actual = concat(split_data, \"dim1\", coords=coords)\n        expected = data.copy().assign_coords(x=(\"dim1\", [0, 1, 1, 2]))\n        assert_identical(expected, actual)\n", "def test_concat_groupby_names(self):\n    data = Dataset(\n        {\"foo\": (\"dim1\", [1, 2, 3])},\n        {\"dim1\": pd.Index([0, 1, 2])},\n    )\n    data[\"bar\"] = (\"dim1\", [1.0, 2.0, 3.0])\n\n    split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]\n    for names in [\"minimal\", \"different\", \"all\"]:\n        actual = concat(split_data, \"dim1\", coords=names)\n        expected = Dataset(\n            {\n                \"foo\": (\"dim1\", [1, 2, 3]),\n                \"bar\": (\"dim1\", [1.0, 2.0, 3.0]),\n            },\n            coords={\"dim1\": pd.Index([0, 1, 2])},\n        )\n        assert_identical(expected, actual)\n", "    def test_concat_compat_diffent_coord_name(self):\n        ds1 = Dataset({\"has_x_y\": ((\"y\", \"x\"), [[1, 2]])}, coords={\"x\": [0, 1], \"y\": [0]})\n        ds2 = Dataset({\"has_x_y\": ((\"y\", \"x\"), [[3, 4]])}, coords={\"new_x\": [0, 1], \"y\": [1]})\n        actual = concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", coords=\"different\", compat=\"broadcast_equals\")\n        assert_equal(ds2.has_x_y, actual.has_x_y.transpose())\n\n        for var in [\"has_x_y\"]:\n            assert \"y\" not in actual[var]\n\n        with raises_regex(ValueError, \"coordinates in some datasets but not others\"):\n            concat([ds1, ds2], dim=\"y\", data_vars=\"minimal\", coords=\"minimal\")\n"], "sample_135": ["def test_compare():\n    x = symbols('x')\n    y = symbols('y')\n    a, b, c = symbols('a b c')\n    f = Function('f')\n    assert Basic(x, y).compare(a, b) == (len(x) > len(a)) - (len(x) < len(a))\n    assert Basic(x, y).compare(a) == (len(x) > len(a)) - (len(x) < len(a))\n    assert Basic(x).compare(y) == (len(x) > len(y)) - (len(x) < len(y))\n    assert Basic(x, y).compare(Basic(x, y)) == 0\n    assert Basic(x, y).compare(Basic(y, x)) == 1\n    assert Basic(x, y).compare(x) == 1\n    assert Basic(x, y).compare(y) == 1\n    assert Basic(x).compare(x) == 0\n    assert Basic(x).compare(y) == 1\n    assert Basic().compare(x) == 1\n    assert Basic().compare(y) == 1\n\n    assert Basic(x, y).compare(f(a, b), c) == (len(x) > len(c)) - (len(x) < len(c))\n    assert Basic(x, y).compare(f(a, b)) == (len(x) > len(f(a, b))) - (len(x) < len(f(a, b)))\n\n    assert Basic(x, y).compare(Basic(x, y, c)) == (len(x) > len(x)) - (len(x) < len(x))\n\n    assert Basic(x, y).compare(Basic(x)) == (len(y) > len(x)) - (len(y) < len(x))\n\n    assert Basic(x, y).compare(f(a, b, c)) == (len(x) > len(c)) - (len(x) < len(c))\n\n    assert Basic(x, y).compare(x + y) == (len(x + y) > len(x)) - (len(x + y) < len(x))\n\n    assert Basic(x, y).compare(y + x) == (len(y + x) > len(x)) - (len(y + x) < len(x))\n\n    assert Basic(x, y).compare(x * y) == (len(x * y) > len(x)) - (len(x * y) < len(x))\n    assert", "def test_replace_without_equivalent_expression():\n    from sympy import sin, cos, gamma, symbols\n    x = symbols('x')\n    a, b, c = symbols('a:b')\n\n    f = sin(x) + cos(x)\n    assert f.replace(sin, cos, map=True) == (cos(x), {sin(x): cos(x)})\n\n    g = cos(x) + gamma(x)\n    assert g.replace(sin, cos, map=True) == (cos(x) + gamma(x), {sin(x): cos(x)})\n\n    h = cos(x) + cos(x)\n    assert h.replace(sin, cos, map=True) == (2*cos(x), {sin(x): cos(x)})\n\n    i = cos(x) + cos(x + x)\n    assert i.replace(sin, cos, map=True) == (2*cos(x) + 2*cos(2*x), {sin(x): cos(x)})\n\n    j = cos(x) + cos(x + a)\n    assert j.replace(sin, cos, map=True) == (cos(x) + cos(x + a), {sin(x): cos(x)})\n\n    k = sin(x + y) + cos(x + y)\n    assert k.replace(sin, cos, map=True) == (sin(x + y) + cos(x + y), {sin(x + y): cos(x + y)})\n\n    l = sin(x + y) + cos(x)\n    assert l.replace(sin, cos, map=True) == (sin(x + y) + cos(x), {sin(x): cos(x)})\n\n    m = sin(x + y) + cos(x + y + x)\n    assert m.replace(sin, cos, map=True) == (sin(x + y) + cos(x + y + x), {sin(x + y): cos(x + y)})\n\n    n = sin(x + y) + cos(x + y + a)\n    assert n.replace(sin, cos, map=True) == (sin(x + y) + cos(x + y + a), {sin(x + y): cos(x + y)})\n\n    o = sin(x + y) + sin(x + y)\n    assert o.replace(sin, cos, map=True) == (cos(x + y), {sin(x + y): cos(x + y)})\n\n    p = cos(x + y) + cos(x + y)\n    assert p.replace(sin, cos, map=True) == (2*cos(x +", "def test_sort_key():\n    from sympy.core.function import Function, AppliedUndef\n    from sympy.core.symbol import Symbol\n    from sympy.abc import x, y, z\n    from sympy import sin, cos, Wild, S\n\n    # Default sort key\n    assert Basic(x + y).sort_key() > Basic(x + z).sort_key()\n    assert Basic(x + y).sort_key() == Basic(x*y).sort_key()\n    assert Basic(x + y).sort_key() == Basic(y + x).sort_key()\n\n    # Custom sort key\n    a = Wild('a')\n    b = Wild('b')\n    assert (x + y).sort_key(order='lex') < (x + z).sort_key(order='lex')\n    assert (x + y).sort_key(order='lex') == (x*y).sort_key(order='lex')\n    assert (x + y).sort_key(order='lex') == (y + x).sort_key(order='lex')\n    assert (x + y).sort_key(order='lex') > (y + x).sort_key(order='lex')\n    assert (x*y).sort_key(order='lex') > (x + y).sort_key(order='lex')\n\n    # Test cacheit decorator\n    assert Basic(x + y).sort_key() == Basic(x + y).sort_key()\n\n    # Test multiple functions in args\n    f = Function('f')\n    g = Function('g')\n    assert f(x).sort_key() > g(x).sort_key()\n    assert f(x).sort_key() == f(y).sort_key()\n    assert f(x).sort_key() == g(y).sort_key()\n    assert f(x).sort_key() == f(x*y).sort_key()\n    assert f(x).sort_key() == f(x**2).sort_key()\n    assert f(x).sort_key() == f(x + y).sort_key()\n    assert f(x).sort_key() == f(x - y).sort_key()\n    assert f(x).sort_key() == f(x*y).sort_key()\n    assert f(x).sort_key() == f(x**2).sort_key()\n\n    # Test sort key for atoms\n    assert S(1).sort_key() == S(2).sort_key()\n    assert S(1).sort_key() == S(3).sort_key()\n\n    # Test sort key for various classes\n    assert AppliedUndef(x).", "def test_sort_key():\n    x, y = symbols('x y')\n    assert b21.sort_key() == (5, 0, 'Basic'), b21.sort_key()\n    assert (x + y).sort_key() == (5, 0, 'Basic'), (x + y).sort_key()\n    assert (S(1) + S(1)).sort_key() == (5, 0, 'Basic'), (S(1) + S(1)).sort_key()\n    assert S(1).sort_key() == (5, 1, 'One'), S(1).sort_key()\n\n    # XXX: this should not be here; we should remove this when issue 5169 is fixed\n    # see http://trac.sagemath.org/ticket/5169\n    assert (y*x).sort_key() == (5, 0, 'Mul'), (y*x).sort_key()\n    assert (x*y).sort_key() == (5, 0, 'Mul'), (x*y).sort_key()\n\n    # test the inner_key() used when sorting within _sorted_args\n    assert ((S(3) + S(2) + S(1)).sort_key()) == (1, (3, 2, 1))\n    assert ((S(3) + S(2) + S(1)).sort_key(order=1)) == (5, 0, 'Basic')\n    assert ((S(3) + S(2) + S(1)).sort_key(order=2)) == (5, 1, 'Basic')\n    assert ((S(3) + S(2) + S(1)).sort_key(order=3)) == (5, 1, 'Basic')\n    assert ((S(3) + S(2) + S(1)).sort_key(order=4)) == (5, 1, 'Basic')\n\n    from sympy import simplify\n    assert (simplify(x + y).sort_key()) == (5, 0, 'Basic')\n", "def test_dummies_in_replace():\n    from sympy import Wild, Function, sin, cos\n    from sympy.abc import x, y, z\n    a, b = map(Wild, 'ab')\n    f = Function('f')\n\n    expr = sin(f(x)*a + cos(f(y))*b)\n    replacement = sin(f(x)*b + cos(f(y))*a)\n    assert expr.replace(a, b, map=True) == (replacement, {a: b})\n    assert expr.replace(a, b) == replacement\n    assert expr.replace(a, b, simultaneous=False) == replacement\n\n    # When the expression is replaced, the mapping dictionary should include the dummies\n    expr = sin(f(x)*a + cos(f(y))*b)\n    replacement = sin(f(x)*b + cos(f(y))*a)\n    mapping = expr.replace(a, b, map=True, simultaneous=False, exact=False)\n    assert mapping[0] == replacement\n    assert mapping[1] == {a: b}\n\n    # Check that dummies are correctly replaced when the expression is replaced\n    # with a function of those dummies\n    expr = sin(f(x)*a + cos(f(y))*b)\n    replacement = sin(f(x)*b + cos(f(y))*a)\n    f_new = lambda expr: sin(expr*a + cos(expr*b)\n                            if expr.is_Mul else expr)\n    assert expr.replace(a, b, map=False, simultaneous=False) == f_new(replacement)\n\n    # Check that dummies are correctly replaced when the expression is replaced\n    # with a pattern containing wildcards\n    expr = sin(f(x)*a + cos(f(y))*b)\n    replacement = sin(a + b)\n    pattern = Wild('x', cls=Basic)\n    assert expr.replace(a + b, pattern) == replacement\n    mapping = expr.replace(a + b, pattern, map=True)\n    assert mapping[0] == replacement\n    assert mapping[1] == {pattern: a + b}\n"], "sample_159": ["def test_latex_repr():\n    assert kilo._latex(SympyPrinter()) == r'\\text{k}'\n    assert kibi._latex(SympyPrinter()) == r'\\text{Y}'\n    assert kibi._latex_repr == r'\\text{Y}'\n", "def test_prefix_latex():\n    assert kilo._latex(None) == r'\\text{k}'\n    assert kibi._latex(None) == r'\\text{Y}'\n    assert kibi._latex_repr == r\"\\mathrm{Y}\"\n    assert kibi._latex(None) == r'\\mathrm{Y}'\n    assert kibi._latex(None) == r'\\mathrm{Y}'\n\n    assert repr(kilo) == \"Prefix('kilo', 'k', 3)\"\n    assert repr(kibi) == \"Prefix('kibi', 'Y', 10, 2)\"\n\n    assert str(kilo) == \"k\"\n    assert str(kibi) == \"Y\"\n", "def test_prefix_unicode():\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    assert str(m) == 'milli'\n    assert repr(m) == \"Prefix('milli', 'm', -3)\"\n    assert m._latex_repr is None\n    m._latex_repr = r'\\mathrm{m}'\n    assert m._latex(m, 0) == r'\\mathrm{m}'\n\n    k._latex_repr = r'\\mathrm{k}'\n    assert k._latex(m) == r'\\mathrm{k}'\n\n    assert m._name == 'milli'\n    assert m._abbrev == 'm'\n    assert m._scale_factor == 1/1000\n    assert m._exponent == -3\n    assert m._base == 10\n", "def test_prefix_latex():\n    \"\"\"\n    Test that the latex representation of prefixes is correct.\n\n    \"\"\"\n    m = PREFIXES['m']\n    k = PREFIXES['k']\n    M = PREFIXES['M']\n    c = PREFIXES['c']\n\n    assert m._latex_repr is None\n    assert k._latex_repr is None\n    assert c._latex_repr == r\"\\mu\"\n    assert M._latex_repr is None\n\n    expr1 = m * m\n    assert m._latex_repr is None\n\n    expr2 = k / k\n    assert m._latex_repr is None\n\n    expr3 = c * c\n    assert c._latex_repr == r\"\\mu\"\n\n    expr4 = c / c\n    assert c._latex_repr == r\"\\mu\"\n\n    # Test with custom latex representation\n    kibi = Prefix('kibi', 'Ki', 10, 2, latex_repr=r'\\mathrm{Ki}')\n    assert kibi._latex_repr == r'\\mathrm{Ki}'\n", "def test_prefix_division_with_prefix():\n    k = PREFIXES['k']\n    m = PREFIXES['m']\n\n    # Test division with another prefix\n    result = k / m\n    assert result == PREFIXES['M']\n\n    # Test division with a non-Prefix quantity\n    result = k / m\n    assert result == PREFIXES['M']\n\n    # Test division with a non-Prefix and a non-numeric value\n    result = k / S(1)\n    assert result == k\n\n    # Test division with a non-Prefix and a non-numeric symbol\n    result = k / x\n    assert isinstance(result, Mul)\n    assert result.args == (1/x, k)\n\n    # Test division with two prefixes\n    result = k / dodeca\n    assert result == (1000 / 12)\n\n    # Test division with a prefix and a quantity with a global scale factor\n    m = Quantity(\"fake_meter\")\n    SI.set_quantity_dimension(m, S.One)\n    SI.set_quantity_scale_factor(m, S.One)\n    assert k / m == (1/12) * m\n\n    # Test division with two quantities with global scale factors\n    result = k / m\n    assert result == (1/12) * m\n"], "sample_30": ["    def test_inline_show_change_link_registered(self):\n        \"\"\"Inlines `show_change_link` for registered models when enabled.\"\"\"\n        parent = Parent.objects.create(name='Parent')\n        child = Child.objects.create(name='Child', parent=parent)\n        response = self.client.get(reverse('admin:admin_inlines_parent_change', args=(parent.pk,)))\n        self.assertTrue(response.context['inline_admin_formset'].opts.has_registered_model)\n        url = reverse('admin:admin_inlines_child_change', args=(child.pk,))\n        self.assertContains(response, '<a href=\"%s\" %s' % (url, INLINE_CHANGELINK_HTML))\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def setUp(self):\n        self.client.force_login(self.superuser)\n", "    def test_tabular_inline_filter_horizontal(self):\n        \"\"\"Test that the TabularInline with filter_horizontal works.\"\"\"\n        response = self.client.get(reverse('admin:admin_inlines_holder_add'))\n        self.assertContains(response, 'Add another Inner')\n        self.assertContains(response, 'id=\"id_inner_set-TOTAL_FORMS\"')\n        # The filter_horizontal field should be available.\n        self.assertContains(response, 'id=\"id_inner_set-0-dummy\"')\n"], "sample_154": ["def test_dtype():\n    f = lambdify(x, exp(x), modules=\"numpy\")\n    assert f(numpy.array([1, 2], dtype=numpy.float32)).dtype == numpy.float32\n    f = lambdify(x, exp(x), modules=\"numexpr\")\n    assert f(numpy.array([1, 2], dtype=numpy.float32)).dtype == numpy.float32\n    f = lambdify(x, exp(x), modules=\"tensorflow\")\n    with tensorflow.compat.v1.Session() as s:\n        assert f(tensorflow.constant(1, dtype=tensorflow.float32)).dtype == tensorflow.float32\n", "def test_lambdastr():\n    from sympy.abc import x, y, z\n    from sympy.utilities.lambdify import lambdastr\n\n    # Test that lambdastr creates a valid function\n    f = lambdastr(x, x**2)\n    assert f(4) == 16\n\n    # Test that lambdastr can handle tuples\n    g = lambdastr((x, y), (y, x))\n    assert g(1, 2) == (2, 1)\n\n    # Test that lambdastr can handle lists\n    h = lambdastr([x, y], [y, x])\n    assert h(1, 2) == [2, 1]\n\n    # Test that lambdastr can handle invalid Python variable names\n    i = lambdastr('\u03bb', \u03bb)\n    raises(SyntaxError, lambda: i(1))\n\n    # Test that lambdastr can handle dummification\n    j = lambdastr(x, x**2, dummify=True)\n    assert j(4) == 16\n\n    # Test that lambdastr can handle no dummification\n    k = lambdastr(x, x**2, dummify=False)\n    assert k(4) == 16\n", "def test_lambdastr():\n    # Test lambdastr with valid function arguments\n    f = lambdify(x, sin(x))\n    assert str(f) == 'lambda x: (sin(x))'\n\n    # Test lambdastr with invalid function arguments\n    raises(TypeError, lambda: lambdify(x, sin(x), dummify=False))\n\n    # Test lambdastr with valid function arguments and dummification\n    f = lambdify(x, sin(x), dummify=True)\n    assert str(f) == 'lambda x: (sin(_0))'\n\n    # Test lambdastr with valid function arguments and lambdastr\n    f = lambda x: 2*sin(x)\n    assert str(f) == 'lambda x: (2*sin(x))'\n\n    # Test lambdastr with valid function arguments and cse\n    f = lambdify(x, sin(x))\n    cses, expr = cse(f(x))\n    assert str(lambdastr(x, expr)) == 'lambda x: (sin(x))'\n\n    # Test lambdastr with valid function arguments and cse and dummification\n    f = lambdify(x, sin(x))\n    cses, expr = cse(f(x))\n    f = lambdify(x, expr, dummify=True)\n    assert str(f) == 'lambda x: (sin(_0))'\n\n    # Test lambdastr with valid function arguments and custom printer\n    class CustomPrinter:\n            return 'custom: ' + str(expr)\n    f = lambdify(x, sin(x), printer=CustomPrinter())\n    assert str(f) == 'custom: lambda x: (sin(x))'\n\n    # Test lambdastr with valid function arguments and custom printer and dummification\n    class CustomPrinter:\n            return 'custom: ' + str(expr)\n    f = lambdify(x, sin(x), printer=CustomPrinter(), dummify=True)\n    assert str(f) == 'custom: lambda x: (sin(_0))'\n\n    # Test lambdastr with valid function arguments and cse and custom printer\n    class CustomPrinter:\n            return 'custom: ' + str(expr)\n    f = lambdify(x, sin(x))\n    cses, expr = cse(f(x))\n    f = lambdify(x, expr, printer=CustomPrinter())\n    assert str(f)", "def test_issue_10933():\n    if not numexpr:\n        skip(\"numexpr not installed.\")\n\n    f = lambdify(x, sin(x), modules='numexpr')\n    assert f(1) == 0.8414709848078965\n    f = lambdify(x, 1/sin(x), modules='numexpr')\n    # Numexpr does not support division with complex numbers\n    assert f(1) == 1.5574344150337805\n    assert isinstance(f(0.1), float)\n    f = lambdify(x, sin(x)**2, modules='numexpr')\n    assert f(1) == 0.7080496575747384\n    f = lambdify(x, Abs(x), modules='numexpr')\n    assert f(-1) == 1.0\n    f = lambdify(x, tan(x), modules='numexpr')\n    assert f(0) == 0.0\n    # Issue 10933\n    f = lambdify((x, y), sin(x*y), modules='numexpr')\n    assert f(0, 0) == 0.0\n    assert f(pi/2, pi/2) == -1.0\n", "def test_get_namespace():\n    # Test that get_namespace can correctly return a namespace dictionary\n    # for different types of modules\n    m = 'numpy'\n    assert isinstance(_get_namespace(m), dict)\n    assert isinstance(_get_namespace(numpy), dict)\n    assert isinstance(_get_namespace(numpy.__dict__), dict)\n\n    # Test that get_namespace raises a TypeError for invalid input\n    m = 'non-existent-module'\n    raises(TypeError, lambda: _get_namespace(m))\n\n    m = 123  # test that non-string non-module input raises TypeError\n    raises(TypeError, lambda: _get_namespace(m))\n\n    # Test that get_namespace raises a ValueError for non-existent module\n    m = '__private_module__'\n    raises(ValueError, lambda: _get_namespace(m))\n\n    m = 'sympy'\n    assert _get_namespace(m) == sympy.__dict__\n\n    m = 'numpy'\n    assert isinstance(_get_namespace(m), dict)\n    assert _get_namespace(m) == numpy.__dict__\n\n    m = {'numpy': 'numpy'}\n    assert _get_namespace(m) == {'numpy': numpy.__dict__}\n    # test that dict's items have higher priority than module's __dict__\n    assert 'numpy' in _get_namespace(m)\n    assert _get_namespace(m) == {'numpy': numpy.__dict__}\n\n    m = {'numpy': 'numpy', 'sympy': sympy}\n    assert _get_namespace(m) == {'numpy': numpy.__dict__, 'sympy': sympy.__dict__}\n\n    m = {'numpy': 'numpy', 'sympy': sympy}\n    assert _get_namespace(m) == {'numpy': numpy.__dict__, 'sympy': sympy.__dict__}\n\n    # test that non-module input dictionaries override module dicts\n    m = {'numpy': 'numpy', 123: 456}\n    assert isinstance(_get_namespace(m), dict)\n    assert _get_namespace(m) == {123: 456, 'numpy': numpy.__dict__}\n\n    m = {'numpy': 'numpy', 123: 456}\n    assert isinstance(_get_namespace(m), dict)\n    assert _get_namespace(m) == {123: 456, 'numpy': numpy.__dict__}\n"], "sample_18": ["    def test_clash_between_accessors(self):\n        class Model(models.Model):\n            first_m2m = models.ManyToManyField('self', symmetrical=False, through='Intermediate')\n            second_m2m = models.ManyToManyField('self', symmetrical=False, through='Intermediate')\n\n        class Intermediate(models.Model):\n            first = models.ForeignKey(Model, models.CASCADE)\n            second = models.ForeignKey(Model, models.CASCADE)\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Reverse accessor for 'Model.first_m2m' clashes with reverse accessor for 'Model.second_m2m'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Model.first_m2m' or 'Model.second_m2m'.\"\n                ),\n                obj=Model._meta.get_field('first_m2m'),\n                id='fields.E304',\n            ),\n            Error(\n                \"Reverse accessor for 'Model.second_m2m' clashes with reverse accessor for 'Model.first_m2m'.\",\n                hint=(\n                    \"Add or change a related_name argument to the definition \"\n                    \"for 'Model.second_m2m' or 'Model.first_m2m'.\"\n                ),\n                obj=Model._meta.get_field('second_m2m'),\n                id='fields.E304',\n            ),\n        ])\n", "    def test_relationship_model_with_unique_constraint(self):\n        class Person(models.Model):\n            class Meta:\n                constraints = [\n                    models.UniqueConstraint(\n                        fields=['id'],\n                        name='unique_id',\n                    ),\n                ]\n\n        class Group(models.Model):\n            members = models.ManyToManyField('Person', through='GroupMember')\n\n        class GroupMember(models.Model):\n            group = models.ForeignKey(Group, models.CASCADE)\n            person = models.ForeignKey(Person, models.CASCADE)\n\n        field = Group._meta.get_field('members')\n        self.assertEqual(field.check(from_model=Group), [])\n", "    def test_clash_between_accessors(self):\n        class Target(models.Model):\n            model_set = models.CharField(max_length=10)\n\n        class Model(models.Model):\n            foreign = models.ForeignKey('Target', models.CASCADE, related_name='id')\n\n        self.assertEqual(Model.check(), [\n            Error(\n                \"Reverse accessor for 'Model.foreign' clashes with field name 'Target.model_set'.\",\n                hint=(\"Rename field 'Target.model_set', or add/change a related_name \"\n                      \"argument to the definition for field 'Model.foreign'.\"),\n                obj=Model._meta.get_field('foreign'),\n                id='fields.E302',\n            ),\n        ])\n", "    def test_foreign_key_repr(self):\n        class Model(models.Model):\n            field = models.ForeignKey('Target', models.CASCADE)\n\n        self.assertEqual(repr(Model._meta.get_field('field')), \"ForeignKey(Target, on_delete=models.CASCADE)\")\n", "    def test_related_field_swapped_model(self):\n        class SwappableModel(models.Model):\n            class Meta:\n                swappable = 'TEST_SWAPPABLE_MODEL'\n\n        class Model(models.Model):\n            foreign_key = models.ForeignKey('invalid_models_tests.SwappableModel', models.CASCADE)\n\n        field = Model._meta.get_field('foreign_key')\n        self.assertEqual(field.check(from_model=Model), [])\n"], "sample_58": ["def test_no_host(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\"NAME\": \"dbname\", \"USER\": \"someuser\", \"PORT\": \"444\"}\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-p\", \"444\", \"dbname\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_empty_options(self):\n    # Empty OPTIONS dictionary should not affect the output.\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"NAME\": \"dbname\", \"USER\": \"someuser\", \"PASSWORD\": \"somepassword\", \"HOST\": \"somehost\", \"PORT\": \"444\"}),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"dbname\"],\n            {\"PGPASSWORD\": \"somepassword\"},\n        ),\n    )\n", "def test_empty_settings(self):\n    with self.assertRaises(KeyError):\n        self.settings_to_cmd_args_env({})\n", "def test_default_database(self):\n    \"\"\"Test that the default database name is correctly handled.\"\"\"\n    # The default database should be set to 'postgres' when 'dbname' is not set.\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"NAME\": None}, []),\n        ([\"psql\"], None),\n    )\n    # The default database should be set to 'postgres' when 'dbname' is not provided.\n    self.assertEqual(\n        self.settings_to_cmd_args_env({\"OPTIONS\": {}}, []),\n        ([\"psql\"], None),\n    )\n", "def test_options(self):\n    self.assertEqual(\n        self.settings_to_cmd_args_env(\n            {\n                \"NAME\": \"dbname\",\n                \"USER\": \"someuser\",\n                \"HOST\": \"somehost\",\n                \"PORT\": \"444\",\n                \"OPTIONS\": {\n                    \"connect_timeout\": 5,\n                    \"keepalives_idle\": 10,\n                    \"keepalives_interval\": 5,\n                    \"keepalives_count\": 3,\n                    \"sslcompression\": True,\n                },\n            }\n        ),\n        (\n            [\"psql\", \"-U\", \"someuser\", \"-h\", \"somehost\", \"-p\", \"444\", \"dbname\"],\n            {\n                \"PGCONNECT_TIMEOUT\": \"5\",\n                \"PGKEEPALIVES_IDLE\": \"10\",\n                \"PGKEEPALIVES_INTERVAL\": \"5\",\n                \"PGKEEPALIVES_COUNT\": \"3\",\n                \"PGSSLCOMPRESSION\": \"1\",\n            },\n        ),\n    )\n"], "sample_73": ["def test_drawingarea_clip():\n    # - create a plot\n    # - put an AnchoredOffsetbox with a child DrawingArea\n    #   at the center of the axes\n    # - give the DrawingArea a gray background\n    # - put a black line across the bounds of the DrawingArea\n    # - see that the black line is clipped to the edges of\n    #   the DrawingArea.\n    fig, ax = plt.subplots()\n    size = 100\n    da = DrawingArea(size, size, clip=True)\n    assert da.clip_children\n    bg = mpatches.Rectangle((0, 0), size, size,\n                            facecolor='#CCCCCC',\n                            edgecolor='None',\n                            linewidth=0)\n    line = mlines.Line2D([-size*.5, size*1.5], [size/2, size/2],\n                         color='black',\n                         linewidth=10)\n    anchored_box = AnchoredOffsetbox(\n        loc='center',\n        child=da,\n        pad=0.,\n        frameon=False,\n        bbox_to_anchor=(.5, .5),\n        bbox_transform=ax.transAxes,\n        borderpad=0.)\n\n    da.add_artist(bg)\n    da.add_artist(line)\n    ax.add_artist(anchored_box)\n    ax.set_xlim((0, 1))\n    ax.set_ylim((0, 1))\n\n    # Test that the black line is clipped to the DrawingArea\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bb = line.get_window_extent(renderer)\n    assert bb.x0 > size\n    assert bb.x1 < -size\n    assert bb.y0 > size\n    assert bb.y1 < -size\n", "def test_offsetbox_arrows():\n    fig, ax = plt.subplots()\n    da = DrawingArea(50, 50, 0, 0, clip=True)\n    da.add_artist(mpatches.Circle((0, 0), 45))\n    ab = AnnotationBbox(da, (.5, .5), xybox=(-0.2, 0.5), xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                        arrowprops=dict(arrowstyle=\"->\",\n                                       connectionstyle=\"arc3,rad=.2\"))\n    ax.add_artist(ab)\n    fig.canvas.draw()\n\n    assert ab.arrow_patch.arrowstyle == '->'\n    assert ab.arrow_patch.connectionstyle == 'arc3,rad=.2'\n", "def test_get_bbox_offsetbox(child_type):\n    # Test that get_bbox works as expected for an OffsetBox\n    if child_type == 'draw':\n        picking_child = DrawingArea(5, 5)\n        picking_child.add_artist(mpatches.Rectangle((0, 0), 5, 5, linewidth=0))\n    elif child_type == 'image':\n        im = np.ones((5, 5))\n        im[2, 2] = 0\n        picking_child = OffsetImage(im)\n    elif child_type == 'text':\n        picking_child = TextArea('\\N{Black Square}', textprops={'fontsize': 5})\n    else:\n        assert False, f'Unknown child type {child_type}'\n\n    fig, ax = plt.subplots()\n    ab = AnnotationBbox(picking_child, (0.5, 0.5), xybox=(-0.2, 0.5),\n                        xycoords='data',\n                        boxcoords=\"axes fraction\", box_alignment=(0., .5),\n                        arrowprops=None)\n    ax.add_artist(ab)\n\n    # set the DPI to match points to make the math easier below\n    fig.set_dpi(72)\n    renderer = fig.canvas.get_renderer()\n\n    bbox = picking_child.get_bbox(renderer)\n    offset = picking_child.get_offset()\n    assert_allclose(bbox.bounds, (0, 0, 5, 5))\n    assert_allclose(offset, (0, 0))\n", "def test_packer_pad():\n    fig, ax = plt.subplots()\n    hp = HPacker(pad=10, sep=5, align='left')\n    hp.add_artist(DrawingArea(100, 100))\n    hp.add_artist(DrawingArea(50, 50))\n    ax.add_artist(hp)\n    fig.canvas.draw()\n    renderer = fig.canvas.get_renderer()\n    bbox = hp.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (10, -50, 155, 150))\n    hp.pad = 20\n    hp.sep = 10\n    hp.draw(renderer)\n    bbox = hp.get_bbox(renderer)\n    assert_allclose(bbox.bounds, (20, -50, 165, 160))\n", "def test_offsetimage():\n    # Test for issue #14304\n    # Check if the image gets clipped when the container box is\n    # larger than the figure\n    fig, ax = plt.subplots(figsize=(6, 6))\n    image = OffsetImage(np.ones((100, 100)))\n    image.set_zoom(5)\n    ax.add_artist(image)\n\n    fig.set_size_inches(2, 2)\n    fig.canvas.draw()\n\n    # The image should be clipped to the figure size\n    assert_allclose(image.get_bbox(fig.canvas.get_renderer()).bounds, (0, 0, 2, 2))\n"], "sample_121": ["def test_copy():\n    p = Permutation([0, 3, 1, 2])\n    q = p.copy()\n    assert q == p\n    q = p.copy()\n    q = q.copy()\n    assert q == p\n    assert q is not p\n    assert q is not q\n", "def test_commutes_with():\n    p = Permutation([1, 2, 0, 3])\n    q = Permutation([0, 1, 3, 2])\n    assert p.commutes_with(q) == False\n    assert Permutation([0, 1, 3, 2]).commutes_with(p) == False\n    assert p.commutes_with(p) == True\n    assert Permutation([]).commutes_with(p) == True\n    assert Permutation([]).commutes_with(Permutation([])) == True\n    assert p.commutes_with(Permutation([])) == True\n    assert Permutation([]).commutes_with(p) == True\n    assert Permutation([0, 1, 2, 3]).commutes_with(p) == True\n    q = Permutation([1, 0, 2, 3])\n    assert p.commutes_with(q) == False\n    q = Permutation([0, 2, 3, 1])\n    assert p.commutes_with(q) == False\n    assert Permutation([0, 1, 2, 3]).commutes_with(p) == True\n    q = Permutation([2, 1, 0, 3])\n    assert p.commutes_with(q) == False\n    q = Permutation([3, 1, 2, 0])\n    assert p.commutes_with(q) == False\n\n    p = Permutation([0, 2, 3, 1])\n    q = Permutation([0, 3, 1, 2])\n    assert p.commutes_with(q) == False\n    q = Permutation([0, 2, 1, 3])\n    assert p.commutes_with(q) == True\n    q = Permutation([0, 1, 2, 3])\n    assert p.commutes_with(q) == False\n    q = Permutation([0, 2, 3, 1])\n    assert p.commutes_with(q) == True\n    q = Permutation([0, 3, 1, 2])\n    assert p.commutes_with(q) == False\n    q = Permutation([1, 2, 3, 0])\n    assert p.commutes_with(q) == False\n    q = Permutation([1, 3, 2", "def test_matrix():\n    p = Permutation([0, 2, 1])\n    assert p.get_adjacency_matrix() == zeros(3)\n    assert p.get_adjacency_distance(p) == 1\n    assert p.get_adjacency_distance(Permutation(0, 1, 2, 3)) == 2\n    assert p.get_positional_distance(p) == 0\n    assert p.get_positional_distance(Permutation(0, 2, 1, 3)) == 2\n    assert p.get_precedence_distance(p) == 0\n    assert p.get_precedence_distance(Permutation(0, 1, 2, 3)) == 3\n    q = Permutation([[1], [0, 2, 3]])\n    assert q.get_adjacency_matrix() == zeros(4)\n    assert q.get_adjacency_distance(p) == 4\n    assert q.get_adjacency_distance(q) == 0\n    assert q.get_positional_distance(p) == 4\n    assert q.get_positional_distance(Permutation(0, 1, 2, 3)) == 4\n    assert q.get_precedence_distance(p) == 6\n    assert q.get_precedence_distance(q) == 0\n    p = Permutation([0, 1, 3, 2])\n    assert p.get_adjacency_distance(p) == 2\n    q = Permutation([0, 2, 1])\n    assert q.get_adjacency_distance(q) == 1\n    raises(ValueError, lambda: p.get_adjacency_distance(Permutation([])))\n    raises(ValueError, lambda: p.get_adjacency_distance(0))\n    raises(ValueError, lambda: p.get_adjacency_distance(1))\n    raises(ValueError, lambda: p.get_adjacency_distance(''))\n    raises(ValueError, lambda: p.get_adjacency_distance('a'))\n    raises(ValueError, lambda: p.get_positional_distance(Permutation([])))\n    raises(ValueError, lambda: p.get_positional_distance(0))\n    raises(ValueError, lambda: p.get_positional_distance(1))\n    raises(ValueError, lambda: p.get_positional_distance(''))\n    raises(ValueError, lambda: p.get_positional_distance('a'))\n    raises(ValueError, lambda: p.get_precedence_distance(Permutation([])))\n    raises(ValueError, lambda: p.get_precedence_distance(0))\n    raises(ValueError, lambda", "def test_identity():\n    I = Permutation(3)\n    assert I == Permutation([0, 1, 2])\n    assert I == Permutation([[0], [1], [2]])\n    assert I == Permutation([[0, 1], [2]])\n    assert I == Permutation([[0], [1], [2], [3]])\n    assert I == Permutation([0, 1, 2, 3])\n    assert I == Permutation(0, 1, 2)\n    assert I == Permutation([0, 1, 2], size=4)\n    assert I == Permutation([[0, 1, 2], [3]])\n    assert I == Permutation(0, 1, 2, size=4)\n    assert I == Permutation([[0], [1], [2], [3]], size=4)\n    assert I == Permutation([[0], [1, 2]], size=4)\n    assert I == Permutation([[0, 1], [2]], size=4)\n    assert I == Permutation([[0], [1], [2], [3, 4]], size=5)\n    assert I == Permutation([0, 1, 2, 3, 4], size=5)\n    assert I == Permutation(0, 1, 2, 3, 4)\n    assert I == Permutation([[0], [1, 2], [3, 4]], size=5)\n    assert I == Permutation([[0], [1], [2], [3, 4, 5]], size=6)\n    assert I == Permutation([[0], [1], [2], [3, 4]], size=5)\n\n    p = Permutation([[3], [2], [1]])\n    assert I * p == Permutation([0, 1, 2])\n    assert p * I == Permutation([0, 1, 2])\n\n    I = Permutation([])\n    assert I == Permutation()\n    assert I == Permutation(size=0)\n    assert I == Permutation([], size=0)\n    assert I == Permutation([[0], [1]], size=2)\n    assert I == Permutation([[0, 1]], size=2)\n    assert I == Permutation([[0], [1], [2]], size", "def test_lehmer_code():\n    p = Permutation([[1, 3], [2]])\n    assert p.Lehmer_code() == [1, 0, 1, 2]\n    assert Permutation(0, 1, 3, 2).Lehmer_code() == [1, 0, 1, 2]\n    assert Permutation(0, 1, 2, 3).Lehmer_code() == [0, 0, 0, 0]\n    assert Permutation([0, 2, 1, 3]).Lehmer_code() == [1, 0, 1, 2]\n    assert Permutation([]).Lehmer_code() == []\n    assert Permutation([0, 1, 2]).Lehmer_code() == [0, 0, 0]\n    assert Permutation([0, 1]).Lehmer_code() == [0, 0]\n    assert Permutation(1).Lehmer_code() == [0]\n    assert Permutation([0]).Lehmer_code() == []\n\n    p = Permutation(0, 2, 4, 3, 1)\n    assert p.Lehmer_code() == [0, 1, 0, 1, 0]\n    p = Permutation(1, 0, 3, 2, 4)\n    assert p.Lehmer_code() == [1, 0, 1, 0, 0]\n    p = Permutation(0, 3, 2, 1, 4)\n    assert p.Lehmer_code() == [1, 1, 0, 0, 0]\n    p = Permutation(0, 4, 3, 1, 2)\n    assert p.Lehmer_code() == [1, 0, 1, 0, 1]\n    p = Permutation(1, 0, 2, 3, 4)\n    assert p.Lehmer_code() == [1, 0, 0, 1, 0]\n    p = Permutation(0, 1, 3, 2, 4)\n    assert p.Lehmer_code() == [0, 1, 1, 0, 0]\n    p = Permutation(1,"], "sample_158": ["def test_get_dimension_system():\n    u = UnitSystem(\"custom\", base_units=(meter, second), units=(meter, second))\n    assert u.get_dimension_system() is u._dimension_system\n", "def test_get_quantity_scale_factor():\n    u = Quantity(\"u\")\n    v = Quantity(\"v\")\n    w = Quantity(\"w\")\n\n    u.set_global_relative_scale_factor(S(10), meter)\n    v.set_global_relative_scale_factor(S(5), meter)\n    w.set_global_relative_scale_factor(S(2), second)\n\n    assert SI.get_quantity_scale_factor(u) == 10\n    assert SI.get_quantity_scale_factor(v) == 5\n    assert SI.get_quantity_scale_factor(w) == 2\n    assert SI.get_quantity_scale_factor(u.convert_to(v)) == 2\n    assert SI.get_quantity_scale_factor(u.convert_to(w)) == 5\n\n    with warns_deprecated_sympy():\n        assert SI.get_quantity_scale_factor(v.convert_to(u))\n    with warns_deprecated_sympy():\n        assert SI.get_quantity_scale_factor(w.convert_to(u))\n", "def test_get_dimensional_dependencies():\n    from sympy.physics.units import meter, second, joule, force, length, time, mass\n    from sympy.physics.units.systems import SI\n    assert SI.get_dimensional_dependencies(joule) == {length: 2, mass: 1, time: -2}\n    assert SI.get_dimensional_dependencies(force) == {length: 1, time: -2, mass: 1}\n    assert SI.get_dimensional_dependencies(meter) == {length: 1}\n    assert SI.get_dimensional_dependencies(second) == {time: 1}\n    assert SI.get_dimensional_dependencies(length + time) == {length: 1, time: 1}\n", "def test_add_constant_and_dimensional():\n    const = PhysicalConstant('const')\n    dim = Dimension(time)\n    const.set_global_relative_scale_factor(S(10), dim)\n\n    assert isinstance(const + meter, Add)\n    assert (const + meter).args == (10 * meter, S(0))\n    assert (const + meter).args[0].args[0].is_physical_constant\n\n    assert isinstance(const + dim, Add)\n    assert (const + dim).args == (10 * dim, S(0))\n    assert (const + dim).args[0].args[0].is_physical_constant\n\n    assert isinstance(const * meter, Mul)\n    assert (const * meter).args == (10 * meter, S(1))\n    assert (const * meter).args[0].args[0].is_physical_constant\n\n    assert isinstance(const * dim, Mul)\n    assert (const * dim).args == (10 * dim, S(1))\n    assert (const * dim).args[0].args[0].is_physical_constant\n\n    with warns_deprecated_sympy():\n        assert isinstance(const - meter, Add)\n        assert (const - meter).args == (10 * meter, S(-1))\n        assert (const - meter).args[0].args[0].is_physical_constant\n\n        assert isinstance(const - dim, Add)\n        assert (const - dim).args == (10 * dim, S(-1))\n        assert (const - dim).args[0].args[0].is_physical_constant\n\n        assert isinstance(const / meter, Mul)\n        assert (const / meter).args == (10 * meter, S(-1))\n        assert (const / meter).args[0].args[0].is_physical_constant\n\n        assert isinstance(const / dim, Mul)\n        assert (const / dim).args == (10 * dim, S(-1))\n        assert (const / dim).args[0].args[0].is_physical_constant\n", "def test_unit_system():\n    si = SI\n    assert si.name == 'SI'\n    assert si.get_dimensional_expr(si.meter) == length.name\n    assert si.get_dimensional_expr(si.kilo) * si.kilo == si.megagram\n    assert si.get_units_non_prefixed() == {\n        m,\n        s,\n        joule,\n        coulomb,\n        kelvin,\n        mole,\n        ampere,\n        candela,\n        ohm,\n        siemens,\n        farad,\n        weber,\n        tesla,\n        henry,\n        pascal,\n        volt,\n        watt,\n        joule_per_kilogram,\n        newton,\n        pascal_second,\n        hertz,\n        siemens_per_meter,\n        ampere_per_meter,\n        ohm_meter,\n        siemens_per_meter_squared,\n        volt_per_meter,\n        watt_per_millimeter_squared,\n        joule_per_kilogram_per_cubic_meter,\n        kilogram_per_cubic_meter,\n        kilogram_per_cubic_meter_per_second,\n        kilogram_per_cubic_meter_per_second_squared,\n        kilogram_per_cubic_meter_per_second_cubed,\n        meter_per_second,\n        meter_per_second_squared,\n        meter_per_second_cubed,\n        meter_per_second_fourth,\n        meter_per_second_fifth,\n        meter_per_second_sixth,\n        meter_per_second_seventh,\n        meter_per_second_eighth,\n        meter_per_second_ninth,\n        meter_per_second_tenth,\n        meter_per_second_eleventh,\n        meter_per_second_twelfth,\n        meter_per_second_thirteenth,\n        meter_per_second_fourteenth,\n        meter_per_second_fifteenth,\n        meter_per_second_sixteenth,\n        meter_per_second_seventeenth,\n        meter_per_second_eighteenth,\n        meter_per_second_nineteenth,\n        meter_per_second_twentieth,\n        kilogram_per_milliliter,\n        kilogram_per_liter,\n        kilogram_per_deciliter,\n        kilogram_per_decilitre,\n        kilogram_per_hectoliter,\n        kilogram_per_centiliter,\n        kilogram_per_centilitre,\n        kilogram_per_milliliter,\n        kilogram_per_millilitre,\n        kilogram_per_ton,\n        kilogram_per_tonne,\n        kilogram_per_atomic_mass_unit,\n        kilogram_per_micromass_unit,\n        kilogram_per_millimass_unit,\n        kilogram_per_n"], "sample_59": ["    def test_initial_forms_not_counted_as_empty(self):\n        data = {\n            \"choices-TOTAL_FORMS\": \"2\",\n            \"choices-INITIAL_FORMS\": \"2\",\n            \"choices-MIN_NUM_FORMS\": \"0\",\n            \"choices-0-choice\": \"Zero\",\n            \"choices-0-votes\": \"0\",\n            \"choices-1-choice\": \"One\",\n            \"choices-1-votes\": \"1\",\n        }\n        ChoiceFormSet = formset_factory(Choice, min_num=1, validate_min=True)\n        formset = ChoiceFormSet(data, auto_id=False, prefix=\"choices\")\n        self.assertTrue(formset.is_valid())\n", "    def test_empty_management_form(self):\n        \"\"\"\n        A ManagementForm with no data is always invalid.\n        \"\"\"\n        formset = FavoriteDrinksFormSet()\n        self.assertFalse(formset.management_form.is_valid())\n        self.assertEqual(\n            formset.management_form.errors,\n            {\"TOTAL_FORMS\": [\"This field is required.\"], \"INITIAL_FORMS\": [\"This field is required.\"]},\n        )\n", "    def test_renderer_override(self):\n        \"\"\"\n        If a formset_factory() is called with a renderer, that renderer is\n        passed to the rendered formset.\n        \"\"\"\n        class CustomRenderer(TemplatesSetting):\n            formset_template_name = \"a/custom/formset/template.html\"\n\n        ChoiceFormSet = formset_factory(Choice, renderer=CustomRenderer)\n        formset = ChoiceFormSet()\n        self.assertEqual(formset.renderer, CustomRenderer())\n        formset = ChoiceFormSet(renderer=CustomRenderer())\n        self.assertEqual(formset.renderer, CustomRenderer())\n\n        # Test that the renderer is not passed to the forms in the formset\n        form = formset.forms[0]\n        self.assertNotEqual(form.renderer, CustomRenderer())\n", "def test_formset_repr_change(self):\n    \"\"\"\n    The repr() method of a formset reflects its internal state, including the\n    formset's validity.\n    \"\"\"\n    ChoiceFormSet = formset_factory(Choice, extra=2)\n    valid_formset = ChoiceFormSet([(\"Zero\", \"0\"), (\"One\", \"1\")])\n    self.assertTrue(valid_formset.is_valid())\n    invalid_formset = ChoiceFormSet([(\"Zero\", \"\"), (\"One\", \"\")])\n    self.assertFalse(invalid_formset.is_valid())\n    partially_invalid_formset = ChoiceFormSet([(\"Zero\", \"0\"), (\"One\", \"\")])\n    self.assertFalse(partially_invalid_formset.is_valid())\n    formset = ChoiceFormSet()\n    self.assertFalse(formset.is_valid())\n    cases = [\n        (valid_formset, \"<ChoiceFormSet: bound=True valid=True total_forms=2>\"),\n        (invalid_formset, \"<ChoiceFormSet: bound=True valid=False total_forms=2>\"),\n        (partially_invalid_formset, \"<ChoiceFormSet: bound=True valid=False total_forms=2>\"),\n        (formset, \"<ChoiceFormSet: bound=False valid=Unknown total_forms=2>\"),\n    ]\n    for formset, expected_repr in cases:\n        with self.subTest(expected_repr=expected_repr):\n            self.assertEqual(repr(formset), expected_repr)\n", "    def test_valid(self):\n        # Start out with a some duplicate data.\n        data = {\n            \"drinks-TOTAL_FORMS\": \"2\",  # the number of forms rendered\n            \"drinks-INITIAL_FORMS\": \"0\",  # the number of forms with initial data\n            \"drinks-MIN_NUM_FORMS\": \"0\",  # min number of forms\n            \"drinks-MAX_NUM_FORMS\": \"0\",  # max number of forms\n            \"drinks-0-name\": \"Gin and Tonic\",\n            \"drinks-1-name\": \"Gin and Tonic\",\n        }\n        formset = FavoriteDrinksFormSet(data, prefix=\"drinks\")\n        self.assertFalse(formset.is_valid())\n        # Any errors raised by formset.clean() are available via the\n        # formset.non_form_errors() method.\n        self.assertEqual(len(formset.non_form_errors()), 1)\n        # The clean() hook should run after the clean() hook on each form.\n        # It should run after the clean() hook on each form even if any of\n        # the forms were invalid.\n        self.assertEqual(\n            formset.non_form_errors(), [\"You may only specify a drink once.\"]\n        )\n        # The valid case still works.\n        data[\"drinks-1-name\"] = \"Bloody Mary\"\n        formset = FavoriteDrinksFormSet(data, prefix=\"drinks\")\n        self.assertTrue(formset.is_valid())\n        self.assertEqual(formset.non_form_errors(), [])\n"], "sample_60": ["    def test_register_serializer_with_collision(self):\n        \"\"\"\n        Test that registering a serializer with the same type as one already\n        registered raises an exception.\n        \"\"\"\n        class ExistingSerializer(BaseSerializer):\n                return \"existing(%r)\" % self.value, {}\n\n        MigrationWriter.register_serializer(int, ExistingSerializer)\n        with self.assertRaisesMessage(\n            ValueError, \"Serializer already registered for type <class 'int'>\"\n        ):\n            MigrationWriter.register_serializer(int, ExistingSerializer)\n", "    def test_serialize_negatives(self):\n        self.assertSerializedResultEqual(\n            decimal.Decimal(\"-1.3\"),\n            (\"Decimal('-1.3')\", {\"from decimal import Decimal\"}),\n        )\n        self.assertSerializedResultEqual(\n            decimal.Decimal(\"-1.3e5\"),\n            (\"Decimal('-1.3e+05')\", {\"from decimal import Decimal\"}),\n        )\n", "    def test_serialize_missing_import(self):\n        \"\"\"\n        Test that a serializer that tries to import something that is not\n        available raises an error.\n        \"\"\"\n        class MissingSerializer(BaseSerializer):\n                return \"missing(%r)\" % self.value, {\"import non.existent.module\"}\n\n        MigrationWriter.register_serializer(missing, MissingSerializer)\n        with self.assertRaisesModuleNotFoundError:\n            self.assertSerializedEqual(missing(\"missing_value\"))\n        MigrationWriter.unregister_serializer(missing)\n", "    def test_serialize_path_like_with_symlink(self):\n        \"\"\"\n        Ticket #30613: makemigrations should be able to serialize symlinks.\n        \"\"\"\n        with os.scandir(os.path.dirname(__file__)) as entries:\n            path_like = list(entries)[0]\n        if not hasattr(path_like, \"is_symlink\"):\n            self.skipTest(\"No symlinks found in test dir.\")\n        expected = (repr(path_like.path), {})\n        self.assertSerializedResultEqual(path_like, expected)\n\n        field = models.FilePathField(path=path_like)\n        string = MigrationWriter.serialize(field)[0]\n        self.assertEqual(string, \"models.FilePathField(path=%r)\" % path_like.path)\n", "    def test_serializer_for_custom_class(self):\n        \"\"\"\n        Test serialization of a custom class that doesn't inherit from BaseSerializer.\n        \"\"\"\n        class CustomClass:\n                return \"custom_class(%r)\" % self.value, {}\n\n        MigrationWriter.register_serializer(type(None), CustomClass)\n        self.assertSerializedEqual(CustomClass(1))\n        MigrationWriter.unregister_serializer(type(None))\n        with self.assertRaisesMessage(ValueError, \"Cannot serialize: 1\"):\n            self.assertSerializedEqual(1)\n"], "sample_102": ["def test_iforest_offset():\n    \"\"\"Check offset_ attribute.\"\"\"\n    # toy sample (the last two samples are outliers)\n    X = [[-2, -1], [-1, -1], [-1, -2], [1, 1], [1, 2], [2, 1], [6, 3], [-4, 7]]\n\n    # Test IsolationForest with contamination='auto'\n    clf = IsolationForest(contamination='auto').fit(X)\n    # assert offset_ is -0.5\n    assert_equal(clf.offset_, -0.5)\n\n    # Test IsolationForest with contamination='auto' and behaviour='new'\n    clf = IsolationForest(contamination='auto', behaviour='new').fit(X)\n    # assert offset_ is -0.5\n    assert_equal(clf.offset_, -0.5)\n\n    # Test IsolationForest with contamination='auto' and behaviour='new'\n    clf = IsolationForest(contamination='auto', behaviour='new').fit(X)\n    # assert offset_ is -0.5\n    assert_equal(clf.offset_, -0.5)\n\n    # Test IsolationForest with contamination=0.1 and behaviour='old'\n    clf = IsolationForest(contamination=0.1, behaviour='old').fit(X)\n    # assert offset_ is -0.5\n    assert_equal(clf.offset_, -0.5)\n\n    # Test IsolationForest with contamination=0.1 and behaviour='new'\n    clf = IsolationForest(contamination=0.1, behaviour='new').fit(X)\n    # assert offset_ is calculated\n    assert_not_equal(clf.offset_, -0.5)\n", "def test_iforest_decision_function():\n    \"\"\"Check decision_function attribute.\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    X_test = [[2., 2.]]\n    clf = IsolationForest(behaviour='new', random_state=rng, contamination=0.1).fit(X_train)\n\n    assert_array_equal(clf.decision_function(X_test), clf.score_samples(X_test) - clf.offset_)\n\n    assert_warns_message(UserWarning,\n                         \"behaviour=\\\"old\\\" is deprecated and will be removed \"\n                         \"in version 0.22.\",\n                         IsolationForest, behaviour='old')\n", "def test_max_features_attribute():\n    \"\"\"Test max_features attribute.\"\"\"\n    X = iris.data\n    clf = IsolationForest(max_features=10).fit(X)\n    assert_equal(clf.max_features_, 10)\n    # check that max_features is scaled\n    clf = IsolationForest(max_features=0.5).fit(X)\n    assert_equal(clf.max_features_, 0.5 * X.shape[1])\n    # test max_features type\n    with pytest.raises(ValueError):\n        IsolationForest(max_features='a')\n    with pytest.raises(ValueError):\n        IsolationForest(max_features=-0.1)\n    with pytest.raises(ValueError):\n        IsolationForest(max_features=1.1)\n", "def test_iforest_max_features():\n    \"\"\"Test max_features for various settings.\"\"\"\n    # Test max_features when int\n    X_train = np.array([[0, 1], [1, 2], [1, 3], [1, 4]])\n    X_test = np.array([[1, 1], [2, 1], [2, 2]])\n    for max_features in [1, 2, 3]:\n        clf = IsolationForest(max_features=max_features).fit(X_train)\n        assert_array_equal(clf.predict(X_test), 3 * [-1])\n        assert_array_equal(clf.predict(X_test[:, :max_features]), 3 * [-1])\n\n    # Test max_features when float\n    X_train = np.array([[0, 1], [1, 2], [1, 3], [1, 4], [1, 5]])\n    X_test = np.array([[1, 1], [2, 1], [2, 2]])\n    for max_features in [0.5, 1.0, 1.5]:\n        clf = IsolationForest(max_features=max_features).fit(X_train)\n        assert_array_equal(clf.predict(X_test), 3 * [-1])\n        assert_array_equal(clf.predict(X_test[:, :int(max_features * X_train.shape[1])]),\n                          3 * [-1])\n", "def test_iforest_contamination():\n    \"\"\"Test contamination parameter works as expected.\"\"\"\n    X_train = [[1, 1], [1, 2], [2, 1]]\n    clf = IsolationForest(contamination=0.2, behaviour='new')\n    clf.fit(X_train)\n\n    decision_func = -clf.decision_function([[2., 2.]])\n    assert_greater(decision_func, 0.8)\n    assert_greater(0.8, clf.offset_)\n\n    # Check decision_function threshold when behaviour='new'\n    # assuming that inliers are 2 samples and outliers are 1 sample\n    assert clf.threshold_ == 0\n\n    # Test case with 1 outlier\n    X_train = [[1, 1], [1, 2], [2, 1], [1, 10]]\n    clf = IsolationForest(contamination=0.25, behaviour='new')\n    clf.fit(X_train)\n\n    decision_func = -clf.decision_function([[1., 1.], [2., 2.]])\n    assert_greater(decision_func[0], 0.8)\n    assert_greater(decision_func[1], 0.8)\n\n    # Test case with 2 outliers\n    X_train = [[1, 1], [1, 2], [2, 1], [1, 10], [1, 20]]\n    clf = IsolationForest(contamination=0.2, behaviour='new')\n    clf.fit(X_train)\n\n    decision_func = -clf.decision_function([[1., 1.], [2., 2.], [1., 20.]])\n    assert_greater(decision_func[0], 0.8)\n    assert_greater(decision_func[1], 0.8)\n    assert_greater(decision_func[2], 0.8)\n"], "sample_109": ["def test_cross_validator_params():\n    n_samples = 4\n    n_unique_groups = 4\n    p = 2\n    n_shuffle_splits = 10  # (the default value)\n\n    X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n    X_1d = np.array([1, 2, 3, 4])\n    y = np.array([1, 1, 2, 2])\n    groups = np.array([1, 2, 3, 4])\n\n    # Test with default parameters\n    loo = LeaveOneOut()\n    lpo = LeavePOut(p)\n    kf = KFold()\n    skf = StratifiedKFold()\n    lolo = LeaveOneGroupOut()\n    lopo = LeavePGroupsOut(p)\n    ss = ShuffleSplit(random_state=0)\n    ps = PredefinedSplit([1, 1, 2, 2])  # n_splits = np of unique folds = 2\n\n    # Test with custom parameters\n    loo_custom = LeaveOneOut(n_repeats=3, shuffle=True)\n    lpo_custom = LeavePOut(p, n_repeats=3, shuffle=True)\n    kf_custom = KFold(n_splits=2, shuffle=True)\n    skf_custom = StratifiedKFold(n_splits=2, shuffle=True)\n    lolo_custom = LeaveOneGroupOut(n_repeats=3, shuffle=True)\n    lopo_custom = LeavePGroupsOut(p, n_repeats=3, shuffle=True)\n    ss_custom = ShuffleSplit(test_size=0.33, random_state=0, n_repeats=3)\n    ps_custom = PredefinedSplit(test_fold=[2, 2, 1, 1])  # n_splits = np of unique folds = 2\n\n    n_splits_expected = [n_samples, comb(n_samples, p), 2, 2,\n                         n_unique_groups, comb(n_unique_groups, p),\n                         n_shuffle_splits, 2]\n\n    for i, (cv, cv_repr) in enumerate(zip(\n            [loo_custom, lpo_custom, kf_custom, skf_custom, lolo_custom,\n             lopo_custom, ss_custom, ps_custom],\n            ['LeaveOneOut(n_repeats=3, random_state=None, shuffle=True)',\n             '", "def test_predefined_split():\n    # Check that PredefinedSplit can generate the same train-test splits as KFold.\n    folds = np.full(10, -1)\n    kf = KFold(3, shuffle=True)\n    for i, (train, test) in enumerate(kf.split(X)):\n        folds[test] = i\n    ps = PredefinedSplit(folds)\n    ps_train, ps_test = zip(*ps.split())\n    assert_array_equal(ps_train, list(train for _, train, _ in kf.split(X)))\n    assert_array_equal(ps_test, list(test for _, train, test in kf.split(X)))\n\n    # Check that PredefinedSplit can generate the same train-test splits as StratifiedKFold.\n    y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n    X = np.ones_like(y)\n    skf = StratifiedKFold(n_splits=3, shuffle=True)\n    skf_train, skf_test = zip(*skf.split(X, y))\n    ps = PredefinedSplit(folds=np.array([0, 0, 1, 1, 2, 2, 0, 0, 1, 1]))\n    ps_train, ps_test = zip(*ps.split())\n    assert_array_equal(ps_train, list(train for _, train, _ in skf.split(X, y)))\n    assert_array_equal(ps_test, list(test for _, train, test in skf.split(X, y)))\n\n    # Check that PredefinedSplit can generate the same train-test splits as GroupKFold.\n    groups = np.array([0, 0, 1, 1, 2, 2, 0, 0, 1, 1])\n    X = y = np.ones(len(groups))\n    gkf = GroupKFold(n_splits=3)\n    gkf_train, gkf_test = zip(*gkf.split(X, y, groups))\n    ps = PredefinedSplit(folds=groups)\n    ps_train, ps_test = zip(*ps.split())\n    assert_array_equal(ps_train, list(train for _, train, _ in gkf.split(X, y, groups)))\n    assert_array_equal(ps_test, list(test for _, train, test in gkf.split(X, y, groups)))\n\n    # Check that PredefinedSplit can reproduce a split generated by GroupShuffleSplit.\n    groups =", "def test_repeated_cross_validator():\n    # Check that RepeatedCrossValidator can be used with any cv\n    rng = np.random.RandomState(42)\n\n    # Check that get_n_splits returns the correct number of splits\n    for cv in (KFold(), StratifiedKFold(), GroupKFold(n_splits=2),\n               ShuffleSplit(), StratifiedShuffleSplit(),\n               GroupShuffleSplit()):\n        repeated_cv = _RepeatedSplits(cv, n_repeats=5)\n        assert repeated_cv.get_n_splits() == 5 * cv.get_n_splits()\n\n    # Test that repeated splits are deterministic if the random_state is set\n    # to an integer\n    for cv in (KFold(), StratifiedKFold(), GroupKFold(n_splits=2),\n               ShuffleSplit(), StratifiedShuffleSplit(),\n               GroupShuffleSplit()):\n        repeated_cv = _RepeatedSplits(cv, n_repeats=5, random_state=42)\n        first_split = next(repeated_cv.split(X=np.ones(10)))\n        for _ in range(4):\n            next(repeated_cv.split(X=np.ones(10)))\n        second_split = next(repeated_cv.split(X=np.ones(10)))\n        assert_array_equal(first_split, second_split)\n\n    # Test that repeated splits are not deterministic if the random_state is not\n    # set\n    for cv in (KFold(), StratifiedKFold(), GroupKFold(n_splits=2),\n               ShuffleSplit(), StratifiedShuffleSplit(),\n               GroupShuffleSplit()):\n        repeated_cv = _RepeatedSplits(cv, n_repeats=5)\n        first_split = next(repeated_cv.split(X=np.ones(10)))\n        for _ in range(4):\n            next(repeated_cv.split(X=np.ones(10)))\n        second_split = next(repeated_cv.split(X=np.ones(10)))\n        assert not array_equal(first_split, second_split)\n\n    # Test that RepeatedKFold and RepeatedStratifiedKFold classes work as\n    # expected\n    repeated_cv = RepeatedKFold(n_splits=3, n_repeats=5)\n    assert repeated_cv.get_n_splits() == 15\n    repeated_cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=5)\n    assert repeated_cv.get_n_splits() == 15\n", "def test_check_cv_incorrect_input():\n    with pytest.raises(ValueError):\n        check_cv(cv='cv')\n\n", "def test_time_series_cv_train_size():\n    # Test that the max_train_size parameter works with TimeSeriesSplit\n    n_samples = 10\n    X = np.zeros((n_samples, 1))\n    cv = TimeSeriesSplit(n_splits=3, max_train_size=2)\n    splits = cv.split(X)\n    for i, (train, test) in enumerate(splits):\n        if i < 2:\n            assert len(train) <= 2\n        else:\n            assert len(train) == n_samples - len(test)\n\n    # Test that train-size and test-size work together\n    cv = TimeSeriesSplit(n_splits=3, max_train_size=None, test_size=1, train_size=2)\n    splits = cv.split(X)\n    for train, test in splits:\n        assert len(test) == 1\n        assert len(train) == 2\n\n    # Test that max_train_size and test_size work together\n    cv = TimeSeriesSplit(n_splits=3, max_train_size=2, test_size=1)\n    splits = cv.split(X)\n    for train, test in splits:\n        assert len(train) <= 2\n        assert len(test) == 1\n\n    # Test that max_train_size is respected when it is larger than the\n    # sample size\n    cv = TimeSeriesSplit(n_splits=3, max_train_size=15)\n    splits = cv.split(X)\n    for train, test in splits:\n        assert len(train) <= 10\n\n    # Test that the max_train_size parameter works with TimeSeriesSplit when\n    # the max_train_size is larger than the sample size\n    cv = TimeSeriesSplit(n_splits=3, max_train_size=10)\n    splits = cv.split(X)\n    for train, test in splits:\n        assert len(train) <= 10\n\n    # Test that TimeSeriesSplit raises a ValueError when n_splits is larger than\n    # n_samples\n    cv = TimeSeriesSplit(n_splits=n_samples + 1)\n    with pytest.raises(ValueError):\n        next(cv.split(X))\n\n    # Test that TimeSeriesSplit raises a ValueError when n_folds is larger than\n    # n_samples\n    cv = TimeSeriesSplit(max_train_size=n_samples + 1)\n    with pytest.raises(ValueError):\n        next(cv.split(X))\n"], "sample_110": ["def test_affinity_propagation_max_iter_non_convergence():\n    # In case of non-convergence of affinity_propagation(), the cluster\n    # centers should be an empty array and training samples should be labelled\n    # as noise (-1)\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n\n    # Force non-convergence by setting max_iter lower than convergence_iter\n    af = AffinityPropagation(preference=-10, max_iter=10, convergence_iter=20)\n\n    assert_warns(ConvergenceWarning, af.fit, X)\n    assert_array_equal(np.empty((0, 2)), af.cluster_centers_)\n    assert_array_equal(np.array([-1, -1, -1]), af.labels_)\n", "def test_affinity_propagation_zero_preferences():\n    X = np.array([[0, 0], [1, 1], [-2, -2]])\n    S = -euclidean_distances(X, squared=True)\n    cluster_center_indices, labels = assert_warns_message(\n        UserWarning, \"mutually equal\", affinity_propagation, S, preference=0)\n\n    assert_array_equal(cluster_center_indices, [0])\n    assert_array_equal(labels, [0, 0, 0])\n\n    af = AffinityPropagation(preference=0, affinity=\"precomputed\")\n    labels_precomputed = af.fit(S).labels_\n\n    af = AffinityPropagation(preference=0)\n    labels = af.fit(X).labels_\n\n    assert_array_equal(labels, labels_precomputed)\n", "def test_affinity_propagation_precomputed_affinity():\n    # Test precomputed affinity matrix\n    X = np.array([[1, 2], [1, 4], [1, 0],\n                  [4, 2], [4, 4], [4, 0]])\n    S = np.dot(X, X.T)\n    preference = np.median(S) * 10\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    cluster_centers_indices, labels = af.fit(S).cluster_centers_indices_, af.labels_\n\n    af = AffinityPropagation(preference=preference, affinity=\"precomputed\")\n    labels_precomputed = af.fit_predict(S)\n\n    assert_array_equal(labels, labels_precomputed)\n\n    # Test also with no copy\n    _, labels_no_copy = affinity_propagation(S, preference=preference, copy=False)\n    assert_array_equal(labels, labels_no_copy)\n\n    # Test input validation\n    with pytest.raises(ValueError):\n        affinity_propagation(S[:, :-1])\n    with pytest.raises(ValueError):\n        affinity_propagation(S, damping=0)\n    af = AffinityPropagation(affinity=\"unknown\")\n    with pytest.raises(ValueError):\n        af.fit(S)\n", "def test_affinity_propagation_convergence_iter():\n    # In case of convergence, the number of iterations should be equal to the\n    # convergence_iter parameter when the algorithm does not converge before max_iter.\n    X = np.array([[1, 1], [1, 1], [-2, -2]])\n    af = AffinityPropagation(preference=-10, max_iter=100, convergence_iter=5)\n    assert_warns(ConvergenceWarning, af.fit, X)\n    assert af.n_iter_ == 0\n\n    # In case of convergence, the number of iterations should be equal to the\n    # max_iter parameter when the algorithm converges.\n    af = AffinityPropagation(preference=np.median(euclidean_distances(X)), max_iter=10, convergence_iter=5)\n    af.fit(X)\n    assert af.n_iter_ == 5\n", "def test_affinity_propagation_copy_parameter():\n    # Test copy parameter\n    X = np.array([[1, 2], [3, 4]])\n\n    # Test default behavior\n    af = AffinityPropagation()\n    af.fit(X)\n\n    # Create a copy of the input data\n    af_copy = AffinityPropagation()\n    af_copy.fit(X.copy())\n\n    # Check if the two instances have the same parameters\n    assert af.cluster_centers_indices_ == af_copy.cluster_centers_indices_\n    assert af.labels_ == af_copy.labels_\n    assert af.n_iter_ == af_copy.n_iter_\n\n    # Test the copy parameter\n    af_copy = AffinityPropagation(copy=False)\n    af_copy.fit(X)\n\n    # Check if the two instances have the same parameters\n    assert af.cluster_centers_indices_ is not af_copy.cluster_centers_indices_\n    assert af.labels_ is not af_copy.labels_\n    assert af.n_iter_ == af_copy.n_iter_\n\n    # Test the copy parameter when affinity is precomputed\n    S = np.dot(X, X.T)\n    af = AffinityPropagation(affinity=\"precomputed\")\n    af.fit(S)\n    af_copy = AffinityPropagation(affinity=\"precomputed\", copy=False)\n    af_copy.fit(S)\n\n    # Check if the two instances have the same parameters\n    assert af.affinity_matrix_ is not af_copy.affinity_matrix_\n"], "sample_39": ["    def test_lazy_reverse_object_coercion_by_reverse(self):\n        \"\"\"\n        Verifies lazy object returned by reverse_lazy is coerced to text by\n        resolve(). Previous to #21043, this would raise a TypeError.\n        \"\"\"\n        urls = 'urlpatterns_reverse.named_urls'\n        proxy_url = reverse_lazy('named-url1', urlconf=urls)\n        resolver = get_resolver(urls)\n        resolver.resolve(proxy_url)\n", "    def test_resolve_error_handler(self):\n        resolver = URLResolver(RegexPattern(r'^$'), 'urlpatterns_reverse.urls')\n        self.assertEqual(resolver.resolve_error_handler(400), empty_view)\n        self.assertEqual(resolver.resolve_error_handler(404), empty_view)\n        self.assertEqual(resolver.resolve_error_handler(500), empty_view)\n        with self.assertRaisesMessage(ViewDoesNotExist, \"The custom handler500 view 'handler500' could not be imported.\"):\n            resolver.resolve_error_handler(200)\n        with self.assertRaisesMessage(ViewDoesNotExist, \"The custom handler400 view 'handler400' could not be imported.\"):\n            resolver.resolve_error_handler(400, 'handler400')\n        with self.assertRaisesMessage(ViewDoesNotExist, \"The custom handler404 view 'handler404' could not be imported.\"):\n            resolver.resolve_error_handler(404, 'handler404')\n", "    def test_custom_pattern(self):\n        # Test a custom pattern with a custom converter.\n        resolver = get_resolver('urlpatterns_reverse.custom_pattern_urls')\n        test_urls = [\n            ('custom-pattern', [1, 'a'], {'captured': 'a', 'str': '1'}),\n            ('custom-pattern', ['a', 1], {}),\n            ('custom-pattern', [1, 2, 'a'], {'str': '1', 'int': '2', 'captured': 'a'}),\n        ]\n        for name, args, kwargs in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(resolve(name).kwargs, kwargs)\n", "    def test_nested_namespace_resolution(self):\n        test_urls = [\n            ('nested_urls:View1', [], {}, '/nested_urls/View1/'),\n            ('nested_urls:View1', [37, 42], {}, '/nested_urls/View1/37/42/'),\n            ('nested_urls:View1', [], {'arg1': 42, 'arg2': 37}, '/nested_urls/View1/42/37/'),\n            ('nested_urls:View2', [], {'arg1': 42, 'arg2': 37}, '/nested_urls/View2/42/37/'),\n            ('nested_urls:View3', [], {'arg1': 42, 'arg2': 37}, '/nested_urls/View3/42/37/'),\n            ('nested_urls:View3', [], {'arg3': 37}, '/nested_urls/View3/42/37/'),\n            ('nested_urls:View3', [42, 37], {'arg3': 'wrong'}, '/nested_urls/View3/42/37/'),\n            ('nested_urls:View3', [42, 37, 'wrong'], {}, '/nested_urls/View3/42/37/wrong/'),\n            ('nested_urls:View3', [42, 37, 'wrong'], {'arg4': 42}, '/nested_urls/View3/42/37/wrong/'),\n            ('nested_urls:View3', [42, 37, 'wrong'], {'arg4': 42, 'arg3': 'wrong'}, '/nested_urls/View3/42/37/wrong/'),\n            ('nested_urls:View3', [42, 37, 'wrong'], {'arg5': 42}, '/nested_urls/View3/42/37/wrong/'),\n        ]\n        for name, args, kwargs, expected in test_urls:\n            with self.subTest(name=name, args=args, kwargs=kwargs):\n                self.assertEqual(reverse(name, args=args, kwargs=kwargs), expected)\n", "    def test_nested_pattern_with_variables(self):\n        test_urls = [\n            ('inner-extra', {'extra': 'foo', 'outer': '42'}, '/outer/42/extra/foo/'),\n            ('inner-extra', {'extra': 'inner'}, '/outer/42/extra/inner/'),\n        ]\n        for name, kwargs, expected in test_urls:\n            with self.subTest(name=name, kwargs=kwargs):\n                self.assertEqual(reverse(name, kwargs=kwargs), expected)\n"]}, "raw_scored_generations": {"sample_3": [false, false, false, false, false], "sample_1": [false, false, false, false, false], "sample_0": [true, false, false, false, false], "sample_2": [false, false, false, false, false], "sample_6": [false, false, false, false, false], "sample_4": [false, false, false, false, true], "sample_5": [false, true, true, false, true], "sample_7": [false, false, false, false, false], "sample_9": [true, false, true, false, false], "sample_8": [false, false, false, false, false], "sample_10": [true, true, true, false, true], "sample_11": [true, false, false, true, false], "sample_13": [true, false, true, false, true], "sample_12": [true, true, true, false, true], "sample_14": [true, true, true, false, false], "sample_15": [false, true, true, true, true], "sample_16": [false, false, false, false, false], "sample_17": [false, false, false, false, false], "sample_18": [false, true, false, false, true], "sample_19": [false, false, false, false, false], "sample_21": [false, false, false, false, false], "sample_20": [true, true, true, true, true], "sample_23": [true, false, true, true, true], "sample_22": [true, true, true, true, false], "sample_24": [true, true, false, false, true], "sample_25": [true, true, true, true, true], "sample_28": [false, false, false, false, false], "sample_26": [false, false, false, false, true], "sample_27": [true, true, true, false, true], "sample_29": [false, false, false, false, false], "sample_31": [false, true, false, false, true], "sample_30": [true, false, false, false, true], "sample_32": [true, false, false, false, false], "sample_33": [false, false, false, true, false], "sample_34": [false, true, false, false, false], "sample_35": [true, false, false, false, false], "sample_36": [true, true, true, false, true], "sample_37": [false, false, false, false, false], "sample_39": [true, false, false, false, false], "sample_38": [false, false, false, false, false], "sample_40": [true, false, true, true, true], "sample_42": [true, false, true, false, false], "sample_41": [false, false, false, false, false], "sample_43": [true, true, true, false, false], "sample_45": [false, false, false, false, false], "sample_44": [false, true, false, true, true], "sample_46": [false, false, false, false, false], "sample_48": [true, false, false, false, false], "sample_47": [true, true, true, false, true], "sample_50": [true, true, false, false, false], "sample_49": [false, false, true, false, false], "sample_51": [true, true, true, true, false], "sample_52": [false, true, true, true, true], "sample_53": [true, true, true, true, false], "sample_55": [true, true, true, true, true], "sample_54": [true, false, true, true, true], "sample_56": [false, false, false, false, false], "sample_58": [true, true, true, true, true], "sample_57": [false, false, false, false, false], "sample_60": [false, false, false, true, false], "sample_59": [true, false, false, true, true], "sample_61": [true, true, true, true, false], "sample_63": [true, false, false, false, false], "sample_62": [false, false, false, false, false], "sample_64": [false, false, false, false, false], "sample_66": [true, false, true, false, true], "sample_65": [false, false, false, false, false], "sample_68": [true, true, true, false, true], "sample_67": [false, false, false, false, true], "sample_69": [true, true, false, false, false], "sample_70": [true, false, false, false, false], "sample_71": [true, false, false, true, false], "sample_72": [false, false, false, false, false], "sample_73": [false, false, false, false, false], "sample_74": [false, false, true, false, false], "sample_75": [false, true, false, false, false], "sample_76": [false, false, false, false, false], "sample_77": [false, false, false, false, false], "sample_78": [false, false, false, false, false], "sample_79": [false, false, false, false, false], "sample_80": [false, false, false, false, false], "sample_82": [false, false, false, false, false], "sample_81": [false, false, false, true, false], "sample_83": [false, false, false, false, false], "sample_84": [true, true, true, false, true], "sample_85": [false, false, false, false, false], "sample_86": [false, false, false, false, false], "sample_87": [true, true, true, true, true], "sample_88": [false, false, false, false, false], "sample_89": [false, false, false, false, false], "sample_90": [false, false, false, false, false], "sample_91": [false, false, true, true, true], "sample_92": [true, true, false, true, false], "sample_93": [false, true, false, true, false], "sample_95": [false, false, true, true, false], "sample_94": [false, false, false, false, true], "sample_96": [true, false, true, false, false], "sample_97": [false, false, false, false, false], "sample_98": [false, false, false, false, false], "sample_99": [false, false, false, false, false], "sample_100": [true, false, false, false, false], "sample_101": [false, false, false, false, false], "sample_102": [false, false, false, false, false], "sample_103": [false, false, false, false, false], "sample_104": [false, false, false, false, false], "sample_105": [false, false, false, false, false], "sample_107": [false, false, false, false, false], "sample_106": [false, false, false, false, false], "sample_108": [true, false, false, false, false], "sample_109": [false, false, false, true, false], "sample_110": [true, false, true, false, false], "sample_111": [false, true, false, false, false], "sample_112": [true, false, false, false, false], "sample_113": [false, false, false, false, false], "sample_114": [false, false, false, false, false], "sample_115": [true, false, false, false, false], "sample_116": [false, false, false, false, false], "sample_117": [true, false, false, false, false], "sample_118": [false, false, false, false, false], "sample_119": [false, false, false, false, false], "sample_120": [false, false, false, false, true], "sample_121": [false, false, false, false, false], "sample_122": [false, false, false, false, false], "sample_123": [false, false, false, false, false], "sample_124": [false, false, false, false, false], "sample_125": [true, false, false, false, false], "sample_126": [false, false, false, false, false], "sample_127": [true, true, false, false, false], "sample_128": [false, false, false, false, false], "sample_129": [true, false, false, false, false], "sample_130": [true, false, true, false, false], "sample_131": [true, false, false, false, false], "sample_132": [false, false, false, false, false], "sample_133": [false, false, false, false, false], "sample_134": [true, true, true, true, true], "sample_135": [false, false, false, false, false], "sample_136": [false, false, false, true, false], "sample_137": [false, false, false, false, false], "sample_138": [false, false, false, false, false], "sample_139": [false, false, false, false, false], "sample_140": [false, false, false, false, true], "sample_142": [false, false, false, false, false], "sample_141": [false, false, false, false, false], "sample_143": [false, false, false, false, false], "sample_144": [false, false, false, false, false], "sample_145": [true, true, false, false, false], "sample_146": [false, false, false, false, true], "sample_147": [false, false, false, false, false], "sample_148": [false, false, false, false, false], "sample_149": [false, false, false, false, false], "sample_150": [false, false, false, false, false], "sample_151": [false, false, false, false, true], "sample_152": [true, false, false, false, false], "sample_153": [false, false, false, false, false], "sample_154": [false, false, false, true, false], "sample_155": [false, false, false, false, false], "sample_156": [false, false, false, false, false], "sample_157": [true, false, false, false, false], "sample_158": [false, false, false, false, false], "sample_159": [false, false, false, false, false]}}